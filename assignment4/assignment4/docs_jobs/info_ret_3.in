<=====doc_Id=====>:3
<=====title=====>:
Information needs
<=====text=====>:
information need is an uncertainty that arises in an individual, and which the individual is believed to be satisfied by information.{{more footnotes|date=June 2015}}
The concept '''Information need''' is seldom, if ever, mentioned in the general literature about [[needs]], but is a common term in the literature of [[information science]]. According to Hjørland (1997) it is closely related to the concept of [[relevance]]: If something is relevant for a person in relation to a given task, we might say that the person needs the information for that task. 

It is often understood as an individual or group's desire to locate and obtain [[information]] to satisfy a conscious or unconscious [[need]]. The ‘information’ and ‘need’ in ‘information need’ are an inseparable interconnection. Needs and interests call forth information. The objectives of studying information needs are:
# The explanation of observed phenomena of information use or expressed need;
# The prediction of instances of information uses;
# The control and thereby improvement of the utilization of information manipulation of essentials conditions.

Information needs are related to, but distinct from [[information requirements]].  An example is that a need is hunger; the requirement is food.

== Background ==

The concept of information needs was coined by an American information journalist [http://www.libsci.sc.edu/BOB/ISP/taylor2.htm Robert S. Taylor] in his article [http://doi.wiley.com/10.1002/asi.5090130405 "The Process of Asking Questions"] published in American Documentation (renamed Journal of the American Society of Information Science and Technology).

In this paper, Taylor attempted to describe how an inquirer obtains an answer from an [[information system]], by performing the process consciously or unconsciously; also he studied the reciprocal influence between the inquirer and a given system.

According to Taylor, information need has four levels:
# The conscious and unconscious need for information not existing in the remembered experience of the investigator. In terms of the query range, this level might be called the “ideal question” — the question which would bring from the ideal system exactly what the inquirer, if he could state his need. It is the actual, but unexpressed, need for information
# The conscious mental description of an ill-defined area of in decision. In this level, the inquirer might talk to someone else in the field to get an answer.
# A researcher forms a rational statement of his question. This statement is a rational and unambiguous description of the inquirer’s doubts.
# The question as presented to the information system.

There are variables within a system that influence the question and its formation. Taylor divided them into five groups: general aspects (physical and geographical factors); system input (What type of material is put into the system, and what is the unit item?); internal organization (classification, indexing, subject heading, and similar access schemes); question input (what part do human operators play in the total system?); output (interim feedback).

Herbert Menzel preferred demand studies to preference studies. Requests for information or documents that were actually made by scientists in the course of their activities form the data for demand studies. Data may be in the form of records of orders placed for bibliographics, calls for books from an interlibrary loan system, or inquires addressed to an information center or service. Menzel also investigated user study and defined information seeking behaviour from three angles:
# When approached from the point of view of the scientist or technologists, these are studies of scientists’ communication behaviour;
# When approached from the point of view of any communication medium, they are use studies;
# When approached from the science communication system, they are studies in the flow of information among scientists and technologists.

William J. Paisley moved from information needs/uses toward strong guidelines for information system. He studied the theories of information-processing behavior that will generate propositions concerning channel selection; amount of seeking; effects on productivity of information quality, quantity, currency, and diversity; the role of motivational and personality factors, etc. He investigated a concentric conceptual framework for user research. In the framework, he places the information users at the centre of ten systems, which are:
# The scientist within his culture.
# The scientist within a political system.
# The scientist within a membership group.
# The scientist within a reference group.
# The scientist within an invisible college.
# The scientist within a formal organization.
# The scientist within a work team.
# The scientist within his own head.
# The scientist within a legal/economical system.
# The scientist within a formal.

==See also==
* [[Information retrieval]]
* [[Needs]]

==References==
* Hjørland, Birger (1997). Information seeking and subject representation. An activity-theoretical approach to information science. Westport, CO: Greenwood Press. 
* Menzel, Herbert. “Information Needs and Uses in Science and Technology.” Annual Review of Information Science and Technology, Vol. 1, Interscience Publishers 1966, pp 41-69.
* Paisley, William J. “Information Needs and Uses.” Annual Review of Information Science and Technology, Vol.3, Encyclopædia Britannica, Inc. Chicago 1968, pp.1-30.
* Taylor, Robert S. “The Process of Asking Questions” American Documentation, Vol.13, No. 4, October 1962, pp.391-396, DOI: 10.1002/asi.5090130405.
* Wilson, T.D. “On User Studies and Information Needs.” Journal of Documentation, Vol. 37, No. 1, 1981, pp.3-15

[[Category:Information retrieval]]
<=====doc_Id=====>:6
<=====title=====>:
Category:Directories
<=====text=====>:
A directory maintains a list for reference or commercial purposes.  This category contains articles about directories.
{{Cat main|Directories}}
{{Commons cat|Directories}}

[[Category:Telephony]]
[[Category:Reference works]]
[[Category:Data management]]
[[Category:Information retrieval]]
<=====doc_Id=====>:9
<=====title=====>:
Category:Information retrieval organizations
<=====text=====>:
[[Category:Information retrieval]]
[[Category:Organizations by subject]]
<=====doc_Id=====>:12
<=====title=====>:
Category:Information retrieval techniques
<=====text=====>:
[[Category:Information retrieval]]
<=====doc_Id=====>:15
<=====title=====>:
Dwell time (information retrieval)
<=====text=====>:
{{one source|date=October 2015}}

In [[information retrieval]], '''dwell time''' denotes the time which a user spends viewing a document after clicking a link on a [[Search engine results page|search engine results page (SERP)]]. Dwell time is the duration between when a user clicks on a [[search engine]] result, and when the user returns from that result, or the user is otherwise seen to have left the result. Dwell time is a [[Relevance (information retrieval)|relevance]] indicator of the search result correctly satisfying the [[Information needs|intent]] of the user. Short dwell times indicate the user's query intent was not satisfied by viewing the result. Long dwell times indicate the user's query intent was satisfied.<ref>{{Cite web|url=https://blogs.bing.com/webmaster/2011/08/02/how-to-build-quality-content/|title=How To Build Quality Content|publisher=Bing blogs}}</ref> 

==References==
<references />2. [https://www.impression.co.uk/blog/4004/dwell-time/ "Understanding dwell time and its impact on search rankings"] Impression Digital
[[Category:Information retrieval]]
[[Category:Information retrieval evaluation]]
[[Category:Internet search engines]]

{{web-software-stub}}
<=====doc_Id=====>:18
<=====title=====>:
Wiener connector
<=====text=====>:
{{Orphan|date=February 2016}}

In mathematics applied to the study of networks, the '''Wiener connector''', named in honor of chemist [[Harry Wiener]] who first introduced the [[Wiener Index]], is a means of maximizing efficiency in connecting specified "query vertices" in a network. Given a [[connected graph|connected]], [[undirected graph]] and a set of query vertices in a graph, the '''minimum Wiener connector''' is an [[induced subgraph]] that connects the query vertices and minimizes the sum of [[shortest path]] distances among all pairs of vertices in the subgraph. In [[combinatorial optimization]], the '''minimum Wiener connector problem''' is the problem of finding the minimum Wiener connector. It can be thought of as a version of the classic [[Steiner tree problem]] (one of [[Karp's 21 NP-complete problems]]), where instead of minimizing the size of the tree, the objective is to minimize the distances in the subgraph.<ref name="steiner">{{cite journal|last1=Hwang|first1=Frank|last2=Richards|first2=Dana|last3=Winter|first3=Dana|last4=Winter|first4=Pawel|title=The Steiner Tree Problem|journal=Annals of Discrete Mathematics|date=1992|url=http://www.sciencedirect.com/science/bookseries/01675060/53}}</ref><ref name="dimacs">[http://dimacs11.cs.princeton.edu/ DIMACS Steiner Tree Challenge]</ref>

The minimum Wiener connector was first presented by Ruchansky, et al. in 2015.<ref name="sigmod">{{cite journal|last2=Bonchi|first2=Francesco|last3=Garcia-Soriano|first3=David|last4=Gullo|first4=Francesco|last5=Kourtellis|first5=Nicolas|date=2015|year=|title=The Minimum Wiener Connector|url=https://arxiv.org/abs/1504.00513|journal=SIGMOD|volume=|pages=|via=|last1=Ruchansky|first1=Natali}}</ref>

The minimum Wiener connector has applications in many domains where there is a graph structure and an interest in learning about connections between sets of individuals. For example, given a set of patients infected with a viral disease, which other patients should be checked to find the culprit? Or given a set of proteins of interest, which other proteins participate in pathways with them?

==Problem definition==
The [[Wiener index]] is the sum of shortest path distances in a (sub)graph. Using <math>d(u,v)</math> to denote the shortest path between <math>u</math> and <math>v</math>, the Wiener index of a (sub)graph <math>S</math>, denoted <math>W(S)</math>, is defined as
: <math>W(S) = \sum_{(u, v) \in S} d(u,v)</math>.

The minimum Wiener connector problem is defined as follows. Given an undirected and unweighted graph with vertex set <math>V</math> and edge set <math>E</math> and a set of query vertices <math>Q\subseteq V</math>, find a connector <math>H\subseteq V</math> of minimum Wiener index. More formally, the problem is to compute
: <math>\operatorname*{arg\,min}_H W(H\cup Q)</math>,
that is, find a connector <math>H</math> that minimizes the sum of shortest paths in <math>H</math>.

==Relationship to Steiner tree==
[[File:SteinerExample nicer.pdf|thumb|upright=2.0|The optimal solutions to the Steiner tree problem and the minimum Wiener connector can differ. Define the set of query vertices ''Q'' by ''Q'' = {''v''<sub>1</sub>, &hellip;, ''v''<sub>10</sub>}. The unique optimal solution to the Steiner tree problem is ''Q'' itself, which has Wiener index 165, whereas the optimal solution for the minimum Wiener connector problem is ''Q'' ∪ {''r''<sub>1</sub>, ''r''<sub>2</sub>}, which has Wiener index 142.]]
The minimum Wiener connector problem is related to the [[Steiner tree problem]]. In the former, the [[objective function]] in the minimization is the Wiener index of the connector, whereas in the latter, the objective function is the sum of the weights of the edges in the connector. The optimum solutions to these problems may differ, given the same graph and set of query vertices. In fact, a solution for the Steiner tree problem may be arbitrarily bad for the minimum Wiener connector problem; the graph on the right provides an example.

== Computational complexity ==

===Hardness===
The problem is [[NP-hard]], and does not admit a [[polynomial-time approximation scheme]] unless [[P = NP|'''P''' = '''NP''']].<ref name="sigmod"/> This can be proven using the [[inapproximability]] of [[vertex cover]] in bounded degree graphs.<ref name="dinursafra">{{cite journal|last1=Dinur|first1=Irit|last2=Safra|first2=Samuel|title=On the hardness of approximating minimum vertex cover|journal=Annals of Mathematics|date=2005}}</ref> Although there is no polynomial-time approximation scheme, there is a polynomial-time constant-factor approximation—an algorithm that finds a connector whose Wiener index is within a constant multiplicative factor of the Wiener index of the optimum connector. In terms of [[complexity class]]es, the minimum Wiener connector problem is in '''[[APX]]''' but is not in '''PTAS''' unless '''P''' = '''NP'''.

=== Exact algorithms ===
An exhaustive search over all possible subsets of vertices to find the one that induces the connector of minimum Wiener index yields an algorithm that finds the optimum solution in <math>2^{O(n)}</math> time (that is, [[exponential time]]) on graphs with ''n'' vertices. In the special case that there are exactly two query vertices, the optimum solution is the [[shortest path]] joining the two vertices, so the problem can be solved in [[polynomial time]] by computing the shortest path. In fact, for any fixed constant number of query vertices, an optimum solution can be found in polynomial time.

=== Approximation algorithms ===
There is a constant-factor approximation algorithm for the minimum Wiener connector problem that runs in time <math>O(q (m \log n + n \log^2 n))</math> on a graph with ''n'' vertices, ''m'' edges, and ''q'' query vertices, roughly the same time it takes to compute shortest-path distances from the query vertices to every other vertex in the graph.<ref name="sigmod"/> The central approach of this algorithm is to reduce the problem to the vertex-weighted Steiner tree problem, which admits a constant-factor approximation in particular instances related to the minimum Wiener connector problem.

==Behavior==

The minimum Wiener connector behaves like [[Centrality#Betweenness centrality|betweenness centrality]].

When the query vertices belong to the same community, the non-query vertices that form the minimum Wiener connector tend to belong to the same community and have high centrality within the community.  Such vertices are likely to be [[influential]] vertices playing leadership roles in the community. In a [[social network]], these influential vertices might be good users for spreading information or to target in a viral marketing campaign.<ref name="viral">{{cite journal | first1=Oliver | last1=Hinz | first2=Bernd | last2=Skiera | first3=Christian | last3=Barrot | first4=Jan U. | last4=Becker | title=Seeding Strategies for Viral Marketing: An Empirical Comparison | journal=Journal of Marketing | volume=75 | number=6 | pages=55–71 | year = 2011 | doi=10.1509/jm.10.0088}}</ref>

When the query vertices belong to different communities, the non-query vertices that form the minimum Wiener connector contain vertices adjacent to edges that bridge the different communities. These vertices span a [[Social Network#Structural holes|structural hole]] in the graph and are important.<ref name="structhole">{{cite conference|last1=Lou|first1=Tiancheng|last2=Tang|first2=Jie|title=Mining Structural Hole Spanners Through Information Diffusion in Social Networks|booktitle=Proceedings of the 22nd International Conference on World Wide Web|date=2013|isbn=9781450320351|location=Rio de Janeiro, Brazil|pages=825–836|url=http://dl.acm.org/citation.cfm?id=2488388.2488461|publisher=International World Wide Web Conferences Steering Committee}}</ref>

==Applications==
The minimum Wiener connector is useful in applications in which one wishes to learn about the relationship between a set of vertices in a graph. For example,
* in [[biology]], it provides insight into how a set of proteins in a [[protein–protein interaction]] network are related,
* in [[social network]]s (like [[Twitter]]), it demonstrates the communities to which a set of users belong and how these communities are related,
* in [[computer network]]s, it may be useful in identifying an efficient way to route a [[multicast]] message to a set of destinations.

==References==
{{reflist}}

[[Category:NP-complete problems]]
[[Category:Trees (graph theory)]]
[[Category:Computational problems in graph theory]]
[[Category:Geometric algorithms]]
[[Category:Geometric graphs]]
[[Category:Graph algorithms]]
[[Category:Data mining]]
[[Category:Social networks]]
[[Category:Computational biology]]
[[Category:Computer science]]
[[Category:Algorithms]]
[[Category:Information retrieval]]
__INDEX__
<=====doc_Id=====>:21
<=====title=====>:
Navigational database
<=====text=====>:
{{Refimprove|date=July 2007}}
A '''navigational database''' is a type of [[database]] in which [[Record (computer science)|records]] or [[Object (computer science)|objects]] are found primarily by following references from other objects. They were a common type of database in the era when data was stored on [[magnetic tape]]; the navigational references told the computer where the next record on the tape was stored, allowing fast-forwarding (and in some cases, reversing) through the records without having to read every record along the way to see if it matched a given criterion.

The introduction of low-cost [[hard drive]]s that provided semi-random access to data led to new models of database storage better suited to these devices. Among these, the [[relational database]] and especially [[SQL]] became the canonical solution from the 1980s through to about 2010. At that time a reappraisal of the entire database market began, the various [[NoSQL]] concepts, which has led to the navigational model being reexamined. Offshoots of the concept, especially the [[graph database]], are finding new uses in modern [[transaction processing]] workloads.

==Description==
Navigational interfaces are usually procedural, though some modern systems like [[XPath]] can be considered to be simultaneously navigational and declarative. 

Navigational access is traditionally associated with the [[network model]] and [[hierarchical model]] of [[database]] interfaces, and some have even acquired set-oriented features.<ref>{{cite book | author = Błażewicz, Jacek |author2=Królikowski, Zbyszko |author3=Morzy, Tadeusz | title = Handbook on Data Management in Information Systems  | publisher = Springer  | year = 2003  | location =  | page = 18  | url = https://books.google.com/books?id=AvLziHKyuLcC&pg=PA18&dq=%22Navigational+database%22+-wikipedia+network+model+and+hierarchical+model&ie=ISO-8859-1| doi =  | isbn = 3-540-43893-9 }}</ref> Navigational techniques use "pointers" and "paths" to navigate among data records (also known as "nodes"). This is in contrast to the [[relational model]] (implemented in [[relational database]]s), which strives to use "declarative" or [[logic programming]] techniques that ask the system for ''what'' to fetch instead of ''how'' to navigate to it.  

For example, to give directions to a house, the navigational approach would resemble something like "Get on highway 25 for 8 miles, turn onto Horse Road, left at the red barn, then stop at the 3rd house down the road", whereas the declarative approach would resemble "Visit the green house(s) within the following coordinates...."

Hierarchical models are also considered navigational because one "goes" up (to parent), down (to leaves), and there are "paths", such as the familiar file/folder paths in hierarchical file systems. In general, navigational systems will use combinations of paths and prepositions such as "next", "previous", "first", "last", "up", "down", "owner", etc.

"Paths" are often formed by concatenation of [[Node (computer science)|node]] names or node addresses. Example:

[[File:6n-graf.svg|thumb|250px|Sample database nodes: A labeled graph on 6 vertices and 7 edges. (Numbers are used for illustration purposes only. In practice more meaningful names are often used. Other potential attributes are not shown.)]]

  Node6.Node4.Node5.Node1

Or

  Node6/Node4/Node5/Node1

If there is no link between given nodes, then an error condition is usually triggered with a message such as "Invalid Path".  The path "Node6.Node2.Node1" would be invalid in most systems because there is no direct link between Node 6 and Node 2.

The usage of the term "navigational" allegedly is derived from a statement by [[Charles Bachman]] in which he describes the "programmer as navigator" while accessing his favored type of database.<ref>{{cite web|url=http://portal.acm.org/citation.cfm?id=362534&coll=portal&dl=ACM |title=The programmer as navigator |doi=10.1145/355611.362534 |publisher=Portal.acm.org |accessdate=2012-10-01}}</ref>

Except for hierarchical file systems (which some consider a form of database), navigational techniques fell out of favor by the 1980s. However, [[object oriented programming]] and [[XML]] have kindled a renewed, but controversial interest in navigational techniques.

Critics of navigational techniques view them as "unstructured spaghetti messes", and liken them to the "[[Goto (command)|goto]]" of pre-[[structured programming]]. In other words, they are allegedly to data organization what goto's were to behavior flow. In this view, relational techniques provide improved discipline and consistency to data organization and usage because of its roots in [[set theory]] and [[predicate calculus]]. 

Some also suggest that navigational database engines are easier to build and take up less memory (RAM) than relational equivalents. However, the existence of relational or relational-based products of the late 1980s that possessed small engines (by today's standards) because they didn't use SQL suggest this is not necessarily the case. Whatever the reason, navigational techniques are still the preferred way to handle smaller-scale structures.

A current example of navigational structuring can be found in the [[Document Object Model]] (DOM) often used in web browsers and closely associated with [[JavaScript]]. The DOM "engine" is essentially a light-weight navigational database. The [[World Wide Web]] itself and Wikipedia could potentially be considered forms of navigational databases, though they focus on human-readable text rather than data (on a large scale, the Web is a network model and on smaller or local scales, such as domain and URL partitioning, it uses hierarchies).  In contrast, the [[Linked Data]] facet of the [[Semantic Web]] is specifically concerned with network-scale [[machine-readable data]], and follows precisely the 'follow your nose' paradigm implied by the navigational idea.

A new kind of navigational databases{{fact|date=August 2015}} has recently{{when|date=August 2015}} emerged, the [[graph databases]]. This category of databases is often included as one of the four family of the [[NoSQL]] databases.

==Examples==
* [[IBM Information Management System]]
* [[IDMS]]

==See also==
* [[CODASYL]]
* [[Graph database]]
* [[Network database]]
* [[Object database]]
* [[Relational database]]

==References==
{{Reflist}}

==External links==
* [http://db-engines.com/en/ranking/navigational+dbms DB-Engines Ranking of Navigational DBMS] by popularity, updated by month


[[Category:Data management]]
[[Category:Types of databases]]
<=====doc_Id=====>:24
<=====title=====>:
Category:Concurrency control
<=====text=====>:
{{Cat main|Concurrency control}}

[[Category:Data management]]
[[Category:Synchronization]]
[[Category:Operating system technology]]
[[Category:Concurrency (computer science)]]
[[Category:Distributed computing problems]]
<=====doc_Id=====>:27
<=====title=====>:
Enterprise Objects Framework
<=====text=====>:
The '''Enterprise Objects Framework''', or more commonly simply '''EOF''', was introduced by [[NeXT]] in 1994 as a pioneering [[object-relational mapping]] product for its [[NeXTSTEP]] and [[OpenStep]] development platforms. EOF abstracts the process of interacting with a [[relational database]], mapping database rows to [[Java (programming language)|Java]] or [[Objective-C]] [[Object (computer science)|objects]]. This largely relieves developers from writing low-level [[SQL]] code. EOF enjoyed some niche success in the mid-1990s among financial institutions who were attracted to the rapid application development advantages of NeXT's object-oriented platform. Since [[Apple Inc]]'s merger with NeXT in 1996, EOF has evolved into a fully integrated part of [[WebObjects]], an application server also originally from NeXT.

== History ==
In the early 1990s [[NeXT]] Computer recognized that connecting to databases was essential to most businesses and yet also potentially complex.  Every data source has a different data-access language (or [[Application programming interface|API]]), driving up the costs to learn and use each vendor's product. The NeXT engineers wanted to apply the advantages of [[object-oriented programming]], by getting objects to "talk" to relational databases. As the two technologies are very different, the solution was to create an abstraction layer, insulating developers from writing the low-level procedural code ([[SQL]]) specific to each data source.

The first attempt came in 1992 with the release of Database Kit (DBKit), which wrapped an object-oriented framework around any database. Unfortunately, [[NEXTSTEP]] at the time was not powerful enough and DBKit had serious design flaws.

NeXT's second attempt came in 1994 with the Enterprise Objects Framework (EOF) version 1, a [[Rewrite (programming)|complete rewrite]] that was far more modular and [[OpenStep]] compatible. EOF 1.0 was the first product released by [[NeXT]] using the Foundation Kit and introduced autoreleased objects to the developer community. The development team at the time was only four people: Jack Greenfield, Rich Williamson, Linus Upson and Dan Willhite. EOF 2.0, released in late 1995, further refined the architecture, introducing the editing context. At that point, the development team consisted of Dan Willhite, [[Craig Federighi]], Eric Noyau and Charly Kleissner.

EOF achieved a modest level of popularity in the financial programming community in the mid-1990s, but it would come into its own with the emergence of the [[World Wide Web]] and the concept of [[web application]]s. It was clear that EOF could help companies plug their legacy databases into the Web without any rewriting of that data. With the addition of frameworks to do state management, load balancing and dynamic HTML generation, NeXT was able to launch the first object-oriented Web application server, [[WebObjects]], in 1996, with EOF at its core.

In 2000, Apple Inc. (which had merged with NeXT) officially dropped EOF as a standalone product, meaning that developers would be unable to use it to create desktop applications for the forthcoming [[macOS|Mac OS X]].  It would, however, continue to be an integral part of a major new release of WebObjects.  WebObjects 5, released in 2001, was significant for the fact that its frameworks had been ported from their native [[Objective-C]] programming language to the [[Java (programming language)|Java]] language.  Critics of this change argue that most of the power of EOF was a side effect of its Objective-C roots, and that EOF lost the beauty or simplicity it once had.  Third-party tools, such as [[EOGenerator]], help fill the deficiencies introduced by Java (mainly due to the loss of [[Objective-C#Categories|categories]]).

The Objective-C code base was re-introduced with some modifications to desktop application developers as [[Core Data]], part of Apple's [[Cocoa (API)|Cocoa API]], with the release of [[Mac OS X Tiger]] in April 2005.

==How EOF works==

Enterprise Objects provides tools and frameworks for object-relational mapping. The technology specializes in providing mechanisms to retrieve data from various data sources, such as relational databases via JDBC and JNDI directories, and mechanisms to commit data back to those data sources. These mechanisms are designed in a layered, abstract approach that allows developers to think about data retrieval and commitment at a higher level than a specific data source or data source vendor.

<!--  Commented out because image was deleted: [[Image:EoModeler.png|frame|right|EOModeler application icon (Mac OS X)]] -->Central to this mapping is a model file (an "EOModel") that you build with a visual tool &mdash; either EOModeler, or the EOModeler plug-in to [[Xcode]]. The mapping works as follows:

* Database tables are mapped to classes.
* Database columns are mapped to class attributes.
* Database rows are mapped to objects (or class instances).

You can build data models based on existing data sources or you can build data models from scratch, which you then use to create data structures (tables, columns, joins) in a data source. The result is that database records can be transposed into Java objects.

The advantage of using data models is that applications are isolated from the idiosyncrasies of the data sources they access. This separation of an application's business logic from database logic allows developers to change the database an application accesses without needing to change the application.

EOF provides a level of database transparency not seen in other tools and allows the same model to be used to access different vendor databases and even allows relationships across different vendor databases without changing source code.  

Its power comes from exposing the underlying data sources as managed graphs of persistent objects.  In simple terms, this means that it organizes the application's model layer into a set of defined in-memory data objects.  It then tracks changes to these objects and can reverse those changes on demand, such as when a user performs an undo command.  Then, when it is time to save changes to the application's data, it archives the objects to the underlying data sources.

===Using Inheritance===

In designing Enterprise Objects developers can leverage the object-oriented feature known as [[Inheritance (computer science)|inheritance]]. A Customer object and an Employee object, for example, might both inherit certain characteristics from a more generic Person object, such as name, address, and phone number. While this kind of thinking is inherent in object-oriented design, relational databases have no explicit support for inheritance. However, using Enterprise Objects, you can build data models that reflect object hierarchies. That is, you can design database tables to support inheritance by also designing enterprise objects that map to multiple tables or particular views of a database table.

==What is an Enterprise Object (EO)? ==

An Enterprise Object is analogous to what is often known in object-oriented programming as a [[Business object (computer science)|business object]] &mdash; a class which models a physical or [[conceptual object]] in the business domain (e.g. a customer, an order, an item, etc.). What makes an EO different from other objects is that its instance data maps to a data store. Typically, an enterprise object contains key-value pairs that represent a row in a relational database. The key is basically the column name, and the value is what was in that row in the database. So it can be said that an EO's properties persist beyond the life of any particular running application.

More precisely, an Enterprise Object is an instance of a class that implements the com.webobjects.eocontrol.EOEnterpriseObject interface.

An Enterprise Object has a corresponding model (called an EOModel) that defines the mapping between the class's object model and the database schema. However, an enterprise object doesn't explicitly know about its model. This level of abstraction means that database vendors can be switched without it affecting the developer's code. This gives Enterprise Objects a high degree of reusability.

== EOF and Core Data ==

Despite their common origins, the two technologies diverged, with each technology retaining a subset of the features of the original Objective-C code base, while adding some new features.

=== Features Supported Only by EOF ===

EOF supports custom SQL; shared editing contexts; nested editing contexts; and pre-fetching and batch faulting of relationships, all features of the original Objective-C implementation not supported by Core Data.  Core Data also does not provide the equivalent of an EOModelGroup—the NSManagedObjectModel class provides methods for merging models from existing models, and for retrieving merged models from bundles.

=== Features Supported Only by Core Data ===

Core Data supports fetched properties; multiple configurations within a managed object model; local stores; and store aggregation (the data for a given entity may be spread across multiple stores); customization and localization of property names and validation warnings; and the use of predicates for property validation.  These features of the original Objective-C implementation are not supported by the Java implementation.

== External links ==
* [http://www.linuxjournal.com/article.php?sid=7101&mode=thread&order=0&thold=0 article in linuxjournal about GDL2]

[[Category:Data management]]
[[Category:NeXT]]
[[Category:Apple Inc. software]]
<=====doc_Id=====>:30
<=====title=====>:
QuickPar
<=====text=====>:
{{Merge to |Parchive |date=March 2014}}

{{Infobox software
| logo                   = 
| screenshot             = [[File:QuickPar Screenshot.png|250px]]
| caption                = QuickPar 0.9 checking a series of [[RAR (file format)|RAR]] files for integrity.
| collapsible            = 
| author                 = 
| developer              = Peter Clements
| released               = 0.1, (February 5, 2003)<ref>{{cite web|url=http://www.quickpar.org.uk/ReleaseNotes2.htm|title=QuickPar - Old Release Notes|accessdate=2010-11-19}}</ref>
| latest release version = 0.9.1
| latest release date    = {{Start date and age|2004|07|04}}<ref>{{cite web |url=http://www.quickpar.org.uk/ |title=QuickPar for Windows |accessdate=2009-09-27}}</ref>
| latest preview version = 
| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD}} -->
| frequently updated     = 
| programming language   = 
| operating system       = [[Microsoft Windows]]
| platform               = [[x86]]
| size                   = 
| language               = 
| status                 = 
| genre                  = [[Data recovery]]
| license                = [[Proprietary software|Proprietary]], [[Freeware]]
| website                = {{URL|www.quickpar.org.uk}}
}}

'''QuickPar''' is a computer program that creates [[parchive]]s used as verification and recovery information for a file or group of files, and uses the recovery information, if available, to attempt to reconstruct the originals from the damaged files and the PAR volumes.

Designed for the [[Microsoft Windows]] [[operating system]], it is often used to recover damaged or missing files that have been downloaded through [[Usenet]].<ref>{{cite book
| last        = Wang
| first       = Wallace
| authorlink  = 
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 – 167
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}</ref> QuickPar may also be used under [[Linux]] via [[Wine (software)|Wine]].<ref>{{cite book
| last        = Petersen
| first       = Richard
| title       = Ubuntu 9.04 Desktop Handbook
| url         = https://books.google.com/books?id=-XLrpiHDYoQC&pg=PT224
| accessdate  = 2009-09-27
| date        = 2009-05-01
| publisher   = Surfing Turtle Press
| location    = [[Los Angeles, California]]
| isbn        = 0-9820998-4-3
| page        = 224
| chapter     = Internet Applications
}}</ref>
[[Image:QuickPar Protect Screenshot.png|thumb|right|Par2 file creation screen]]

There are two main versions of [[Parchive|PAR files]]: PAR and PAR2. The PAR2 file format lifts many of its previous restrictions.<ref>{{cite web
| url         = http://www.quickpar.org.uk/AboutPAR2.htm
| title       = QuickPar - About PAR2
| accessdate  = 2009-09-27
}}</ref> QuickPar is [[freeware]] but not [[open source]]. It uses the [[Reed-Solomon error correction]] algorithm internally to create the error correcting information.<ref name="newsgroups">{{Cite journal | last1 = Fellows | first1 = G. | title = Newsgroups reborn – the binary posting renaissance | doi = 10.1016/j.diin.2006.04.006 | journal = Digital Investigation | volume = 3 | issue = 2 | pages = 73–78 | year = 2006 | pmid =  | pmc = }}</ref>

==Abandonware==
Though QuickPar works well, it is currently considered [[abandonware]], since there have been no updates for it in {{age|2004|07|04}} years.  The software, [[Parchive#Windows| MultiPar]], is actively being developed by another author named Yutaka Sawada, who is adding support for the new PAR3 file format.

==See also==
* [[:nl:Data Archiving and Networked Services|DANS; has some similar software]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.quickpar.org.uk/}}
* [http://www.quickpar.org.uk/Tutorials.htm QuickPar tutorial referencing Usenet downloads]
* [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar], successor to QuickPar, supports PAR2, PAR3 and multicore cpu's

[[Category:Data management]]


{{storage-software-stub}}
<=====doc_Id=====>:33
<=====title=====>:
Universal Data Element Framework
<=====text=====>:
The '''Universal Data Element Framework''' ('''UDEF''') provides the foundation for building an enterprise-wide [[controlled vocabulary]]. It is a standard way of indexing enterprise information that can produce big cost savings. UDEF simplifies information management through consistent classification and assignment of a global standard identifier to the data names and then relating them to similar data element concepts defined by other organizations. Though this approach is a small part of the overall picture, it is potentially a crucial enabler of semantic [[interoperability]].

== How UDEF works ==
UDEF provides semantic links, through assigning an intelligent, derived ID as an attribute of the data element, essentially labeling the element as a specific data element concept. When this UDEF ID exists in both source and target formats, it can then be used as an easy analysis point via a [[match report]], and then as the primary pivot point for transformations between source and target.

UDEF takes a list of high-level root object classes and assigns an integer to each class plus alpha characters to each specialization modifier.  It then also assigns integers to property word plus integers to each specialization modifier.  These object class alpha-integers are concatenated together with the property integers to form a dewey-decimal like code for each data element concept.

===Examples===
For the following examples, go to http://www.opengroup.org/udefinfo/htm/en_defs.htm and expand the applicable UDEF object and property trees

Assuming an application used by a hospital needs to map the data element concepts to the UDEF, the last name and first name (within UDEF you will find Family Name and Given Name under the UDEF property Name) of several people that are likely to appear on a medical record that could include the following example data element concepts –

*Patient Person Family Name – find the word “Patient” under the UDEF object “Person” and find the word “Family” under the UDEF property “Name”
*Patient Person Given Name – find the word “Patient” under the UDEF object “Person” and find the word “Given” under the UDEF property “Name”
*Doctor Person Family Name – find the word “Doctor” under the UDEF object “Person” and find the word “Family” under the UDEF property “Name”
*Doctor Person Given Name – find the word “Doctor” under the UDEF object “Person” and find the word “Given” under the UDEF property “Name”

The associated UDEF IDs for the above are derived by walking up each tree respectively and using an underscore to separate the object from the property. For the examples above, the following data element concepts are available within the current UDEF – see http://www.opengroup.org/udefinfo/htm/en_ob5.htm and http://www.opengroup.org/udefinfo/htm/en_pr10.htm

*“Patient Person Family Name” the UDEF ID is “au.5_11.10”
*“Patient Person Given Name” the UDEF ID is “au.5_12.10”
*“Doctor Person Family Name” the UDEF ID is “aq.5_11.10”
*“Doctor Person Given Name” the UDEF ID is “aq.5_12.10”

== Six basic steps to map enterprise data to the UDEF ==

There are six basic steps to follow when mapping data element concepts to the UDEF.

1. Identify the applicable UDEF property word that characterizes the dominant attribute (property) of the data element concept. For example: Name, Identifier, Date, etc.

2. Identify the dominant UDEF object word that the dominant property (selected in step 1) is describing. For example, Person_Name, Product_Identifier, Document_Date, etc.

3. By reviewing the UDEF tree for the selected property identified in step 1, identify applicable qualifiers that are necessary to describe the property word term unambiguously. For example, Family Name.

4. By reviewing the UDEF tree for the selected object identified in step 2, identify applicable qualifiers that are necessary to describe the object word term unambiguously. For example, Customer Person.

5. Concatenate the object term and the property term to create a UDEF naming convention compliant name where it is recognized that the name may seem artificially long. For example, Customer Person_Family Name.

6. Derive a structured ID based on the UDEF taxonomy that carries the UDEF inherited indexing scheme. For example <CustomerPersonFamilyName UDEFID=”as.5_11.10”>.

== The Open Group UDEF Project objectives ==

The UDEF Project aims to establish the Universal Data Element Framework (UDEF) as the universally-used classification system for data element concepts. It focuses on developing and maintaining the UDEF as an open standard, advocating and promoting it, putting in place a technical infrastructure to support it, implementing a Registry for it, and setting up education programs to train information professionals in its use.

Organizations that implement UDEF will likely realize the greatest benefit by defining their controlled vocabulary based on the UDEF. To help an organization manage its UDEF based controlled vocabulary, it should seriously consider a metadata registry that is based on ISO/IEC 11179-5.

== History of UDEF ==

Ron Schuldt, Sr. Enterprise Data Architect, Lockheed Martin, originated the UDEF concept based on [[ISO/IEC 11179]] Metadata standards in the early 1990s. Currently, he is a Senior Partner with Femto-Data LLC

== Ownership of UDEF intellectual property ==

The Open Group assumed from the [[Association for Enterprise Information]] (AFEI) the right to grant public use licensing of the UDEF.

The Supplier Management Council Electronic Enterprise Working Group of the [[Aerospace Industry Association]] (AIA) supports the UDEF as the naming convention solution to [[XML]] interoperability between standards that include all functions throughout a product's life-cycle and is working through a well defined process to obtain approval of this position from AIA and its member companies.

== Criticism ==

Classification in UDEF is sometimes hampered by ad hoc decisions that might produce problems. 
Example: 
* b.be.5 is "United-Kingdom Citizen Person" and
* c.be.5 is "European Union Citizen Person"
As the United Kingdom is part of the European Union, the classification is not unique.
Response:
The UDEF is flexible and is designed to match the semantics and behaviour of existing systems. Therefore, if one system has a table for United Kingdom Citizens and a different system has a table for European Union Citizens, the UDEF can handle both situations.

Some of the concepts in UDEF are not as universal as it is claimed. They show a lot of bias to Anglo-American tradition and way of thinking and are not easily transferable to other languages.
Example: The following part of the hierarchy shows the concept of an officer.

* j.5        Officer.Person
* a.j.5      Contracting.Officer.Person
* a.a.j.5    Procuring.Contracting.Officer.Person
* a.a.a.j.5  Government.Procuring.Contracting.Officer.Person
* b.a.j.5    Administrative.Contracting.Officer.Person
* b.j.5      Police.Officer.Person
* c.j.5      Military.Officer.Person
In many cultures, the part of the tree below "a.j.5 Contracting Officer Person" would not be placed under j.5 (see [[Officer (disambiguation)|officer]]) as b.j.5 (see [[Law enforcement officer]]) or c.j.5 (see [[Officer (armed forces)]]).

==See also==
* [[Data integration]]
* [[ISO/IEC 11179]]
* [[National Information Exchange Model]]
* [[Metadata]]
* [[Naming conventions (programming)]]
* [[Semantics]]
* [[Semantic web]]
* [[Semantic equivalency]]
* [[Data element]]
* [[Representation term]]
* [[Controlled vocabulary]]

== External links ==
* [http://www.opengroup.org/udefinfo/ UDEF Project of The Open Group]
* [https://www.youtube.com/embed/y6hID5qzAzQ YouTube - UDEF Tutorial, Part 1]
* [https://www.youtube.com/embed/d6dH_U8TqhY YouTube - UDEF Tutorial, Part 2]
* [http://www.opengroup.org/udefinfo/faq.htm UDEF Frequently Asked Questions]
* [https://udef-it.com/UDEF_Tools.html - Obtain Enhanced UDEF Gap Analysis Tool in English, Dutch, or French]

== Further reading ==
* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt |publisher=CreateSpace |date=November 15, 2011 |isbn=978-1-4664-6762-0 |url=https://www.createspace.com/3711806}}
* {{cite book |title=UDEF - Six Steps to Cost Effective Data Integration |author=Ronald Schuldt and Roberta Shauger |publisher=Amazon Digital Services |date=January 16, 2012 |isbn=1-4664-6762-2 |url=http://www.amazon.com/dp/B006YK6YOQ}}
* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger |publisher=CreateSpace |date=December 20, 2011 |isbn=978-1-4681-1483-6 |url=https://www.createspace.com/3753707}}
* {{cite book |title=UDEF Concepts Defined - Reference Guide |author=Roberta Shauger and Ronald Schuldt |publisher=Amazon Digital Services |date=January 14, 2012 |isbn=1-4681-1483-2 |url=http://www.amazon.com/dp/B006XXMLQE}}

[[Category:Data management]]
[[Category:Interoperability]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Open Group standards]]
[[Category:Software that uses Motif]]
[[Category:Technical communication]]
<=====doc_Id=====>:36
<=====title=====>:
Savepoint
<=====text=====>:
{{For|save points in video games|Saved game}}
{{refimprove|date=September 2014}}

A '''savepoint''' is a way of implementing subtransactions (also known as [[nested transaction]]s) within a [[relational database management system]] by indicating a point within a [[database transaction|transaction]] that can be "[[Rollback (data management)|rolled back to]]" without affecting any work done in the transaction before the savepoint was created. Multiple savepoints can exist within a single transaction. Savepoints are useful for implementing complex error recovery in database applications. If an error occurs in the midst of a multiple-statement transaction, the application may be able to recover from the error (by rolling back to a savepoint) without needing to abort the entire transaction.

A savepoint can be declared by issuing a <code>SAVEPOINT ''name''</code> statement. All changes made after a savepoint has been declared can be undone by issuing a <code>ROLLBACK TO SAVEPOINT ''name''</code> command. Issuing <code>RELEASE SAVEPOINT ''name''</code> will cause the named savepoint to be discarded, but will not otherwise affect anything. Issuing the commands <code>ROLLBACK</code> or <code>COMMIT</code> will also discard any savepoints created since the start of the main transaction.[http://docs.oracle.com/cd/B19306_01/appdev.102/b14261/savepoint_statement.htm]

Savepoints are supported in some form or other in database systems like [[PostgreSQL]], [[Oracle database|Oracle]], [[Microsoft SQL Server]], [[MySQL]], [[IBM DB2|DB2]], [[SQLite]] (since 3.6.8), [[Firebird (database server)|Firebird]], [[H2_(DBMS)|H2 Database Engine]], and [[Informix]] (since version 11.50xC3). Savepoints are also defined in the [[SQL#Interoperability_and_standardization|SQL standard]].

{{databases}}

[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:39
<=====title=====>:
Serializability
<=====text=====>:
{{About|serializability of database transactions|serialization of objects in object-oriented languages|serialization}}

In [[concurrency control]] of [[database]]s,<ref name=Bernstein87>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx ''Concurrency Control and Recovery in Database Systems''] (free PDF download), Addison Wesley Publishing Company, ISBN 0-201-10715-5</ref><ref name=Weikum01>[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8</ref> [[transaction processing]] (transaction management), and various [[Database transaction|transactional]] applications (e.g., [[transactional memory]]<ref name=Herlihy1993>[[Maurice Herlihy]] and J. Eliot B. Moss. ''Transactional memory: architectural support for lock-free data structures.'' Proceedings of the 20th annual international symposium on Computer architecture (ISCA '93). Volume 21, Issue 2, May 1993.</ref> and [[software transactional memory]]), both centralized and [[Distributed computing|distributed]], a transaction [[Schedule (computer science)|schedule]] is '''serializable''' if its outcome (e.g., the resulting database state) is equal to the outcome of its transactions executed serially, i.e., sequentially without overlapping in time. Transactions are normally executed concurrently (they overlap), since this is the most efficient way. Serializability is the major correctness criterion for concurrent transactions' executions. It is considered the highest level of [[isolation (computer science)|isolation]] between [[Database transaction|transactions]], and plays an essential role in [[concurrency control]]. As such it is supported in all general purpose database systems. ''[[Two-phase locking|Strong strict two-phase locking]]'' (SS2PL) is a popular serializability mechanism utilized in most of the database systems (in various variants) since their early days in the 1970s.

'''Serializability theory''' provides the formal framework to reason about and analyze serializability and its techniques. Though it is [[Mathematics|mathematical]] in nature, its fundamentals are informally (without mathematics notation) introduced below.

==Database transaction==
{{main|Database transaction}}

A '''database transaction''' is a specific intended run (with specific parameters, e.g., with transaction identification, at least) of a computer program (or programs) that accesses a database (or databases). Such a program is written with the assumption that it is running in ''isolation'' from other executing programs, i.e., when running, its accessed data (after the access) are not changed by other running programs. Without this assumption the transaction's results are unpredictable and can be wrong. The same transaction can be executed in different situations, e.g., in different times and locations, in parallel with different programs. A ''live'' transaction (i.e., exists in a computing environment with already allocated computing resources; to distinguish from a ''transaction request'', waiting to get execution resources) can be in one of three states, or phases:
#''Running'' - Its program(s) is (are) executing.
#''Ready'' - Its program's execution has ended, and it is waiting to be ''Ended (Completed)''.
#''Ended'' (or ''Completed'') - It is either ''Committed'' or ''Aborted (Rolled-back)'', depending whether the execution is considered a success or not, respectively . When committed, all its ''recoverable'' (i.e., with states that can be controlled for this purpose), ''durable'' resources (typically ''database data'') are put in their ''final'' states, states after running. When aborted, all its recoverable resources are put back in their ''initial'' states, as before running.

A failure in transaction's computing environment before ending typically results in its abort. However, a transaction may be aborted also for other reasons as well (e.g., see below).

Upon being ended (completed), transaction's allocated computing resources are released and the transaction disappears from the computing environment. However, the effects of a committed transaction remain in the database, while the effects of an aborted (rolled-back) transaction disappear from the database. The concept of ''atomic transaction'' ("all or nothing" semantics) was designed to exactly achieve this behavior, in order to control correctness in complex faulty systems.

==Correctness==

===Serializability===
'''Serializability''' is used to keep the data in the data item in a consistent state.  Serializability is a property of a transaction [[Schedule (computer science)|schedule]] (history). It relates to the ''[[Isolation (database systems)|isolation]]'' property of a [[database transaction]].
:'''Serializability''' of a schedule means equivalence (in the outcome, the database state, data values) to a ''serial schedule'' (i.e., sequential with no transaction overlap in time) with the same transactions. It is the major criterion for the correctness of concurrent transactions' schedule, and thus supported in all general purpose database systems.

:'''The rationale behind serializability''' is the following:
:If each transaction is correct by itself, i.e., meets certain integrity conditions, then a schedule that comprises any ''serial'' execution of these transactions is correct (its transactions still meet their conditions): "Serial" means that transactions do not overlap in time and cannot interfere with each other, i.e, complete ''isolation'' between each other exists. Any order of the transactions is legitimate, if no dependencies among them exist, which is assumed (see comment below). As a result, a schedule that comprises any execution (not necessarily serial) that is equivalent (in its outcome) to any serial execution of these transactions, is correct.

Schedules that are not serializable are likely to generate erroneous outcomes. Well known examples are with transactions that debit and credit accounts with money: If the related schedules are not serializable, then the total sum of money may not be preserved. Money could disappear, or be generated from nowhere. This and violations of possibly needed other [[invariant (computer science)|invariant]] preservations are caused by one transaction writing, and "stepping on" and erasing what has been written by another transaction before it has become permanent in the database. It does not happen if serializability is maintained.

If any specific order between some transactions is requested by an application, then it is enforced independently of the underlying serializability mechanisms. These mechanisms are typically indifferent to any specific order, and generate some unpredictable [[partial order]] that is typically compatible with multiple serial orders of these transactions. This partial order results from the scheduling orders of concurrent transactions' data access operations, which depend on many factors.

A major characteristic of a database transaction is ''[[Atomicity (database systems)|atomicity]]'', which means that it either ''commits'', i.e., all its operations' results take effect in the database, or ''aborts'' (rolled-back), all its operations' results do not have any effect on the database ("all or nothing" semantics of a transaction). In all real systems transactions can abort for many reasons, and serializability by itself is not sufficient for correctness. Schedules also need to possess the ''[[Schedule (computer science)#Recoverable|recoverability]]'' (from abort) property. '''Recoverability''' means that committed transactions have not read data written by aborted transactions (whose effects do not exist in the resulting database states). While serializability is currently compromised on purpose in many applications for better performance (only in cases when application's correctness is not harmed), compromising recoverability would quickly violate the database's integrity, as well as that of transactions' results external to the database. A schedule with the recoverability property (a ''recoverable'' schedule) "recovers" from aborts by itself, i.e., aborts do not harm the integrity of its committed transactions and resulting database. This is false without recoverability, where the likely integrity violations (resulting incorrect database data) need special, typically manual, corrective actions in the database.

Implementing recoverability in its general form may result in ''cascading aborts'': Aborting one transaction may result in a need to abort a second transaction, and then a third, and so on. This results in a waste of already partially executed transactions, and may result also in a performance penalty. '''[[Schedule (computer science)#Avoids cascading aborts (rollbacks)|Avoiding cascading aborts]]''' (ACA, or Cascadelessness) is a special case of recoverability that exactly prevents such phenomenon. Often in practice a special case of ACA is utilized: '''[[Schedule (computer science)#Strict|Strictness]]'''. Strictness allows an efficient database recovery from failure.

Note that the ''recoverability'' property is needed even if no database failure occurs and no database ''recovery'' from failure is needed. It is rather needed to correctly automatically handle aborts, which may be unrelated to database failure and recovery from failure.

===Relaxing serializability===

In many applications, unlike with finances, absolute correctness is not needed. For example, when retrieving a list of products according to specification, in most cases it does not matter much if a product, whose data was updated a short time ago, does not appear in the list, even if it meets the specification. It will typically appear in such a list when tried again a short time later. Commercial databases provide concurrency control with a whole range of [[isolation (computer science)#Isolation levels|isolation levels]] which are in fact (controlled) serializability violations in order to achieve higher performance. Higher performance means better transaction execution rate and shorter average transaction response time (transaction duration). ''[[Snapshot isolation]]'' is an example of a popular, widely utilized efficient relaxed serializability method with many characteristics of full serializability, but still short of some, and unfit in many situations.

Another common reason nowadays for [[Serializability#Distributed serializability|distributed serializability]] relaxation (see below) is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large-scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.<ref name=Gray1996>{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 | coauthors = Helland, P.; [[Patrick O'Neil|O’Neil, P.]]; [[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | title = The dangers of replication and a solution
 | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173–182
 | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}</ref>
Consequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while serializability is relaxed and compromised for [[eventual consistency]]. Again in this case, relaxation is done only for applications that are not expected to be harmed by this technique.

Classes of schedules defined by ''relaxed serializability'' properties either contain the serializability class, or are incomparable with it.

==View and conflict serializability ==

Mechanisms that enforce serializability need to execute in [[Real-time computing|real time]], or almost in real time, while transactions are running at high rates. In order to meet this requirement special cases of serializability, sufficient conditions for serializability which can be enforced effectively, are utilized.

Two major types of serializability exist: ''view-serializability'', and ''conflict-serializability''. View-serializability matches the general definition of serializability given above. Conflict-serializability is a broad special case, i.e., any schedule that is conflict-serializable is also view-serializable, but not necessarily the opposite. Conflict-serializability is widely utilized because it is easier to determine and covers a substantial portion of the view-serializable schedules. Determining view-serializability of a schedule is an [[NP-complete]] problem (a class of problems with only difficult-to-compute, excessively time-consuming known solutions).

:'''View-serializability''' of a schedule is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that respective transactions in the two schedules read and write the same data values ("view" the same data values).

:'''Conflict-serializability''' is defined by equivalence to a serial schedule (no overlapping transactions) with the same transactions, such that both schedules have the same sets of respective chronologically ordered pairs of conflicting operations (same precedence relations of respective conflicting operations).

Operations upon data are ''read'' or ''write'' (a write: either ''insert'' or ''modify'' or ''delete''). Two operations are ''conflicting'', if they are of different transactions, upon the same datum (data item), and at least one of them is ''write''. Each such pair of conflicting operations has a ''conflict type'': It is either a ''read-write'', or ''write-read'', or a ''write-write'' conflict. The transaction of the second operation in the pair is said to be ''in conflict'' with the transaction of the first operation. A more general definition of conflicting operations (also for complex operations, which may consist each of several "simple" read/write operations) requires that they are [[noncommutative]] (changing their order also changes their combined result). Each such operation needs to be atomic by itself (by proper system support) in order to be considered an operation for a commutativity check. For example, read-read operations are commutative (unlike read-write and the other possibilities) and thus read-read is not a conflict. Another more complex example: the operations ''increment'' and ''decrement'' of a ''counter'' are both ''write'' operations (both modify the counter), but do not need to be considered conflicting (write-write conflict type) since they are commutative (thus increment-decrement is not a conflict; e.g., already has been supported in the old [[IBM Information Management System|IBM's IMS "fast path"]]). Only precedence (time order) in pairs of conflicting (non-commutative) operations is important when checking equivalence to a serial schedule, since different schedules consisting of the same transactions can be transformed from one to another by changing orders between different transactions' operations (different transactions' interleaving), and since changing orders of commutative operations (non-conflicting) does not change an overall operation sequence result, i.e., a schedule outcome (the outcome is preserved through order change between non-conflicting operations, but typically not when conflicting operations change order). This means that if a schedule can be transformed to any serial schedule without changing orders of conflicting operations (but changing orders of non-conflicting, while preserving operation order inside each transaction), then the outcome of both schedules is the same, and the schedule is conflict-serializable by definition.

Conflicts are the reason for blocking transactions and delays (non-materialized conflicts), or for aborting transactions due to serializability violations prevention. Both possibilities reduce performance. Thus reducing the number of conflicts, e.g., by commutativity (when possible), is a way to increase performance.

A transaction can issue/request a conflicting operation and be ''in conflict'' with another transaction while its conflicting operation is delayed and not executed (e.g., blocked by a [[Lock (computer science)|lock]]). Only executed (''materialized'') conflicting operations are relevant to ''conflict serializability'' (see more below).

==Enforcing conflict serializability==

===Testing conflict serializability===

Schedule compliance with conflict serializability can be tested with the [[precedence graph]] (''serializability graph'', ''serialization graph'', ''conflict graph'') for committed transactions of the schedule. It is the [[directed graph]] representing precedence of transactions in the schedule, as reflected by precedence of conflicting operations in the transactions.

:In the '''[[precedence graph]]'''  transactions are nodes and precedence relations are directed edges. There exists an edge from a first transaction to a second transaction, if the second transaction is ''in conflict'' with the first (see Conflict serializability above), and the conflict is '''materialized''' (i.e., if the requested conflicting operation is actually executed: in many cases a requested/issued conflicting operation by a transaction is delayed and even never executed, typically by a [[Lock (computer science)|lock]] on the operation's object, held by another transaction, or when writing to a transaction's temporary private workspace and materializing, copying to the database itself, upon commit; as long as a requested/issued conflicting operation is not executed upon the database itself, the conflict is '''non-materialized'''; non-materialized conflicts are not represented by an edge in the precedence graph).

:'''Comment:''' In many text books only ''committed transactions'' are included in the precedence graph. Here all transactions are included for convenience in later discussions.

The following observation is a '''key characterization of conflict serializability''':

:A schedule is ''conflict-serializable'' [[if and only if]] its precedence graph of ''committed transactions'' (when only ''committed'' transactions are considered) is ''[[directed acyclic graph|acyclic]]''. This means that a cycle consisting of committed transactions only is generated in the (general) precedence graph, if and only if conflict-serializability is violated.

Cycles of committed transactions can be prevented by aborting an ''undecided'' (neither committed, nor aborted) transaction on each cycle in the precedence graph of all the transactions, which can otherwise turn into a cycle of committed transactions (and a committed transaction cannot be aborted). One transaction aborted per cycle is both required and sufficient number to break and eliminate the cycle (more aborts are possible, and can happen in some mechanisms, but unnecessary for serializability). The probability of cycle generation is typically low, but nevertheless, such a situation is carefully handled, typically with a considerable overhead, since correctness is involved. Transactions aborted due to serializability violation prevention are ''restarted'' and executed again immediately.

Serializability enforcing mechanisms typically do not maintain a precedence graph as a data structure, but rather prevent or break cycles implicitly (e.g., SS2PL below).

===Common mechanism - SS2PL===
{{main|Two-phase locking}}

''Strong strict two phase locking'' (SS2PL) is a common mechanism utilized in database systems since their early days in the 1970s (the "SS" in the name SS2PL is newer though) to enforce both conflict serializability and ''[[Schedule (computer science)#Strict|strictness]]'' (a special case of recoverability which allows effective database recovery from failure) of a schedule. In this mechanism each datum is locked by a transaction before accessing it (any read or write operation): The item is marked by, associated with a ''[[lock (computer science)|lock]]'' of a certain type, depending on operation (and the specific implementation; various models with different lock types exist; in some models locks may change type during the transaction's life). As a result, access by another transaction may be blocked, typically upon a conflict (the lock delays or completely prevents the conflict from being materialized and be reflected in the precedence graph by blocking the conflicting operation), depending on lock type and the other transaction's access operation type. Employing an SS2PL mechanism means that all locks on data on behalf of a transaction are released only after the transaction has ended (either committed or aborted).

SS2PL is the name of the resulting schedule property as well, which is also called ''rigorousness''. SS2PL is a special case ([[proper subset]]) of [[Two-phase locking]] (2PL)

Mutual blocking between transactions results in a ''deadlock'', where execution of these transactions is stalled, and no completion can be reached. Thus deadlocks need to be resolved to complete these transactions' execution and release related computing resources. A deadlock is a reflection of a potential cycle in the precedence graph, that would occur without the blocking when conflicts are materialized. A deadlock is resolved by aborting a transaction involved with such potential cycle, and breaking the cycle. It is often detected using a ''[[wait-for graph]]'' (a graph of conflicts blocked by locks from being materialized; it can be also defined as the graph of non-materialized conflicts; conflicts not materialized are not reflected in the precedence graph and do not affect serializability), which indicates which transaction is "waiting for" lock release by which transaction, and a cycle means a deadlock. Aborting one transaction per cycle is sufficient to break the cycle. Transactions aborted due to deadlock resolution are ''restarted'' and executed again immediately.

===Other enforcing techniques===

Other known mechanisms include:
* [[Precedence graph]] (or Serializability graph, Conflict graph) cycle elimination
* [[Two-phase locking]] (2PL)
* [[Timestamp-based concurrency control|Timestamp ordering]] (TO)
* [[Snapshot isolation#Making Snapshot Isolation Serializable|Serializable snapshot isolation]]<ref name=Cahill08>Michael J. Cahill, Uwe Röhm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], ''Proceedings of the 2008 ACM SIGMOD international conference on Management of data'', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award)</ref> (SerializableSI)

The above (conflict) serializability techniques in their general form do not provide recoverability. Special enhancements are needed for adding recoverability.

====Optimistic versus pessimistic techniques====

Concurrency control techniques are of three major types:
# ''Pessimistic'': In Pessimistic concurrency control a transaction blocks data access operations of other transactions upon conflicts, and conflicts are ''non-materialized'' until blocking is removed. This is done to ensure that operations that may violate serializability (and in practice also recoverability) do not occur.
# ''Optimistic'': In [[Optimistic concurrency control]] data access operations of other transactions are not blocked upon conflicts, and conflicts are immediately ''materialized''. When the transaction reaches the ''ready'' state, i.e., its ''running'' state has been completed, possible serializability (and in practice also recoverability) violation by the transaction's operations (relatively to other running transactions) is checked: If violation has occurred, the transaction is typically ''aborted'' (sometimes aborting ''another'' transaction to handle serializability violation is preferred). Otherwise it is ''committed''.
# ''Semi-optimistic'': Mechanisms that mix blocking in certain situations with not blocking in other situations and employ both materialized and non-materialized conflicts

The main differences between the technique types is the conflict types that are generated by them. A pessimistic method blocks a transaction operation upon conflict and generates a non-materialized conflict, while an optimistic method does not block and generates a materialized conflict. A semi-optimistic method generates both conflict types. Both conflict types are generated by the chronological orders in which transaction operations are invoked, independently of the type of conflict. A cycle of committed transactions (with materialized conflicts) in the ''[[precedence graph]]'' (conflict graph) represents a serializability violation, and should be avoided for maintaining serializability. A cycle of (non-materialized) conflicts in the ''[[wait-for graph]]'' represents a deadlock situation, which should be resolved by breaking the cycle. Both cycle types result from conflicts, and should be broken. At any technique type conflicts should be detected and considered, with similar overhead for both materialized and non-materialized conflicts (typically by using mechanisms like locking, while either blocking for locks, or not blocking but recording conflict for materialized conflicts). In a blocking method typically a [[context switch]]ing occurs upon conflict, with (additional) incurred overhead. Otherwise blocked transactions' related computing resources remain idle, unutilized, which may be a worse alternative. When conflicts do not occur frequently, optimistic methods typically have an advantage. With different transactions loads (mixes of transaction types) one technique type (i.e., either optimistic or pessimistic) may provide better performance than the other.

Unless schedule classes are ''inherently blocking'' (i.e., they cannot be implemented without data-access operations blocking; e.g., 2PL, SS2PL and SCO above; see chart), they can be implemented also using optimistic techniques (e.g., Serializability, Recoverability).

====Serializable multi-version concurrency control====

:See also [[Multiversion concurrency control]] (partial coverage)
:and [[Snapshot isolation#Serializable Snapshot Isolation|Serializable_Snapshot_Isolation]] in [[Snapshot isolation]]

'''Multi-version concurrency control''' (MVCC) is a common way today to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object), depending on scheduling method. MVCC can be combined with all the serializability techniques listed above (except SerializableSI which is originally MVCC based). It is utilized in most general-purpose DBMS products.

MVCC is especially popular nowadays through the ''relaxed serializability'' (see above) method ''[[Snapshot isolation]]'' (SI) which provides better performance than most known serializability mechanisms (at the cost of possible serializability violation in certain cases). [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI]], which is an efficient enhancement of SI to make it serializable, is intended to provide an efficient serializable solution. [[Snapshot isolation#Making Snapshot Isolation Serializable|SerializableSI has been analyzed]]<ref name=Cahill08/><ref name=fekete2009>Alan Fekete (2009), [http://www.it.usyd.edu.au/~fekete/teaching/serializableSI-Fekete.pdf "Snapshot Isolation and Serializable Execution"], Presentation, Page 4, 2009, The university of Sydney (Australia). Retrieved 16 September 2009</ref> via a general theory of MVCC

==Distributed serializability==

===Overview===

'''Distributed serializability''' is the serializability of a schedule of a transactional [[distributed system]] (e.g., a [[distributed database]] system). Such system is characterized by ''[[distributed transaction]]s'' (also called ''global transactions''), i.e., transactions that span computer processes (a process abstraction in a general sense, depending on computing environment; e.g., [[operating system]]'s [[Thread (computer science)|thread]]) and possibly network nodes. A distributed transaction comprises more than one ''local sub-transactions'' that each has states as described above for a [[Serializability#Database transaction|database transaction]]. A local sub-transaction comprises a single process, or more processes that typically fail together (e.g., in a single [[processor core]]). Distributed transactions imply a need in [[Atomic commit]] protocol to reach consensus among its local sub-transactions on whether to commit or abort. Such protocols can vary from a simple (one-phase) hand-shake among processes that fail together, to more sophisticated protocols, like [[Two-phase commit]], to handle more complicated cases of failure (e.g., process, node, communication, etc. failure). Distributed serializability is a major goal of [[distributed concurrency control]] for correctness. With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s) the need for effective distributed serializability techniques to ensure correctness in and among distributed applications seems to increase.

Distributed serializability is achieved by implementing distributed versions of the known centralized techniques.<ref name=Bernstein87 /><ref name=Weikum01 /> Typically all such distributed versions require utilizing conflict information (either of materialized or non-materialized conflicts, or equivalently, transaction precedence or blocking information; conflict serializability is usually utilized) that is not generated locally, but rather in different processes, and remote locations. Thus information distribution is needed (e.g., precedence relations, lock information, timestamps, or tickets). When the distributed system is of a relatively small scale, and message delays across the system are small, the centralized concurrency control methods can be used unchanged, while certain processes or nodes in the system manage the related algorithms. However, in a large-scale system (e.g., ''Grid'' and ''Cloud''), due to the distribution of such information, substantial performance penalty is typically incurred, even when distributed versions of the methods (Vs. centralized) are used, primarily due to computer and communication [[latency (engineering)|latency]]. Also, when such information is distributed, related techniques typically do not scale well. A well-known example with scalability problems is a [[distributed lock manager]], which distributes lock (non-materialized conflict) information across the distributed system to implement locking techniques.

==See also==

*[[Two-phase locking|Strong strict two-phase locking]] (SS2PL or Rigorousness).
*[[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]]<ref name=Cahill08 /> in [[Snapshot isolation]].
*[[Global serializability]], where the ''Global serializability problem'' and its proposed solutions are described.
* [[Linearizability]], a more general concept in [[concurrent computing]]

==Notes==
{{reflist}}

==References==
{{more footnotes|date=November 2011}}
<!--Supposed sources for most of the material about centralized (vs. distributed) serializability:-->
*[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, ISBN 0-201-10715-5
*[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8

[[Category:Data management]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]
[[Category:Distributed computing problems]]

[[el:Σειριοποιησιμότητα Συγκρούσεων]]
<=====doc_Id=====>:42
<=====title=====>:
World Wide Molecular Matrix
<=====text=====>:
{{Refimprove|date=December 2010}}
{{No footnotes|date=December 2010}}

The '''World Wide Molecular Matrix''' ('''WWMM''') is an electronic [[Disciplinary repository|repository]] for unpublished chemical [[data]]. First proposed in 2002 by [[Peter Murray-Rust]] and his colleagues in the [[chemistry]] department at the [[University of Cambridge]] in the [[United Kingdom]], WWMM provides a free, easily searchable [[database]] for information about thousands of complicated [[molecules]], data that would otherwise remain inaccessible to [[scientists]].

Murray-Rust, a chemical [[cheminformatics|informatics]] specialist, has estimated that 80% of the results produced by chemists around the world is never published in [[scientific journals]]. Most of this data is not ground-breaking, yet it could conceivably be of use to scientists doing related projects—if they could access it. The WWMM was proposed as a solution to this problem. It would house the results of experiments on over 100,000 molecules in [[physical chemistry]], [[organic chemistry]], [[biochemistry]] and medicinal chemistry.

In other scientific fields, the need for a similar depository to house inaccessible information could be more acute. In a presentation at the "[[CERN]] Workshop on Innovations in Scholarly Communications ([[Open Archives Initiative|OAI4]])", Murray-Rust said that chemistry actually leads other fields in published data. He estimated that as much as 99% of the data in some scientific fields never reaches publication.{{Citation needed|date=December 2010|reason=This is not found in external link, OAI4 - Paine Ellsworth, ed.}}

Although scientific in nature, the WWMM is part of the broader [[Open Archives Initiative|open archives]] and [[open source]] movements, pushes to make more and more information freely available to any user via the [[Internet]] or [[www|World Wide Web]]. In his [[CERN]] presentation, Murray-Rust stated that the WWMM was a "response to the expense of [scientific] journals", and he asked the rhetorical question, "Can we win the war to make data open, or will it be absorbed into the [[publishing]] and pseudo-publishing world?" Murray-Rust and his colleagues are also responsible for the development of the Chemical Mark-up Language ([[Chemical Markup Language|CML]]), a variant of [[XML]] intended for [[chemists]].

==See also==
* [[Open Archives Initiative|The open archives initiative (OAI)]]
* [[Informatics (academic field)|The science of Informatics]]
* [[Chemical Markup Language|Chemical Mark-up language (CML)]]

==External links==
*[http://www.ch.cam.ac.uk/person/pm286 The home page of Dr. Peter Murray-Rust at the University of Cambridge]
*[http://www.escience.cam.ac.uk/projects/mi/mi_call.html The Cambridge Center for molecular informatics]
*[http://www.nesc.ac.uk/events/ahm2003/AHMCD/pdf/157.pdf An outline of the WWMM]
*[http://oai4.web.cern.ch/OAI4/ CERN Workshop on Innovations in Scholarly Communication (OAI4)]{{verify source|type=application to WWMM|date=December 2010}}

[[Category:Data management]]
<=====doc_Id=====>:45
<=====title=====>:
Three-phase commit protocol
<=====text=====>:
In [[computer networking]] and [[database]]s, the '''three-phase commit protocol''' ('''3PC''')<ref name=3PC>{{cite journal
 | last = Skeen
 | first = Dale
 | title = A Formal Model of Crash Recovery in a Distributed System
 | journal = IEEE Transactions on Software Engineering
 | volume = 9
 | issue = 3
 |date=May 1983
 | pages = 219–228
 | doi = 10.1109/TSE.1983.236608
 | last2 = Stonebraker
 | first2 = M.
}}</ref> is a [[distributed algorithm]] which lets all nodes in a [[distributed system]] agree to [[Commit (data management)|commit]] a [[database transaction|transaction]].  Unlike the [[two-phase commit protocol]] (2PC) however, 3PC is non-blocking.  Specifically, 3PC places an upper bound on the amount of time required before a transaction either commits or [[Abort (computing)|aborts]].  This property ensures that if a given transaction is attempting to commit via 3PC and holds some [[lock (computer science)|resource locks]], it will release the locks after the timeout.

==Protocol Description==
In describing the protocol, we use terminology similar to that used in the [[two-phase commit protocol]].  Thus we have a single coordinator site leading the transaction and a set of one or more cohorts being directed by the coordinator.

<center>[[Image:Three-phase commit diagram.png]]</center>

===Coordinator===
#The coordinator receives a transaction request.  If there is a failure at this point, the coordinator aborts the transaction (i.e. upon recovery, it will consider the transaction aborted).  Otherwise, the coordinator sends a canCommit? message to the cohorts and moves to the waiting state.
#If there is a failure, timeout, or if the coordinator receives a No message in the waiting state, the coordinator aborts the transaction and sends an abort message to all cohorts.  Otherwise the coordinator will receive Yes messages from all cohorts within the time window, so it sends preCommit messages to all cohorts and moves to the prepared state.
#If the coordinator succeeds in the prepared state, it will move to the commit state.  However if the coordinator times out while waiting for an acknowledgement from a cohort, it will abort the transaction.  In the case where an acknowledgement is received from the majority of cohorts, the coordinator moves to the commit state as well.

===Cohort===
#The cohort receives a canCommit? message from the coordinator.  If the cohort agrees it sends a Yes message to the coordinator and moves to the prepared state.  Otherwise it sends a No message and aborts.  If there is a failure, it moves to the abort state.
#In the prepared state, if the cohort receives an abort message from the coordinator, fails, or times out waiting for a commit, it aborts.  If the cohort receives a  preCommit message, it sends an '''[[acknowledgement (data networks)|ACK]]''' message back and awaits a final commit or abort.
#If, after a cohort member receives a  preCommit  message, the coordinator fails or times out, the cohort member goes forward with the commit.

==Motivation==
A [[Two-phase commit protocol]] cannot dependably recover from a failure of both the coordinator and a cohort member during the '''Commit phase'''.  If only the coordinator had failed, and no cohort members had received a commit message, it could safely be inferred that
no commit had happened.  If, however, both the coordinator and a cohort member
failed, it is possible that the failed cohort member was the first to be notified, and had
actually done the commit.  Even if a new coordinator is selected, it cannot 
confidently proceed with the operation until it has received an agreement from
all cohort members ... and hence must block until all cohort members respond.

The Three-phase commit protocol eliminates this problem by introducing the Prepared to commit
state.  If the coordinator fails before sending preCommit messages, the cohort will
unanimously agree that the operation was aborted.  The coordinator will not send out a doCommit
message until all cohort members have '''ACK'''ed that they are '''Prepared to commit'''. 
This eliminates the possibility that any cohort member actually completed the 
transaction before all cohort members were aware of the decision to do so 
(an ambiguity that necessitated indefinite blocking in the [[Two-phase commit protocol]]).

==Disadvantages==
The main disadvantage to this algorithm is that it cannot recover in the event the network is segmented in any manner. The original 3PC algorithm assumes a fail-stop model, where processes fail by crashing and crashes can be
accurately detected, and does not work with network partitions or asynchronous communication.

Keidar and Dolev's E3PC<ref name=E3PC>{{cite journal|last=Keidar|first=Idit|author2=Danny Dolev |title=Increasing the Resilience of Distributed and Replicated Database Systems|journal=Journal of Computer and System Sciences (JCSS)|volume=57|issue=3|date=December 1998|pages=309–324|
url=http://webee.technion.ac.il/~idish/Abstracts/jcss.html|doi=10.1006/jcss.1998.1566}}</ref> algorithm eliminates this disadvantage.

The protocol requires at least 3 round trips to complete, needing a minimum of 3 round trip times (RTTs). This is potentially a long latency to complete each transaction.

==References==
{{Reflist}}

==See also==
*[[Two-phase commit protocol]]

[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:48
<=====title=====>:
Online complex processing
<=====text=====>:
'''Online complex processing''' ('''OLCP''') is a class of realtime data processing involving complex queries, lengthy queries and/or simultaneous reads and writes to the same records.
{{software-stub}}
==Sources==
*http://www.pcmag.com/encyclopedia_term/0,2542,t=online+complex+processing&i=48345,00.asp

==See also==
*[[Online transaction processing]] 
*[[OLAP]]
*[[Transaction processing]]


[[Category:Data management]]
[[Category:Databases]]
<=====doc_Id=====>:51
<=====title=====>:
Master data
<=====text=====>:
'''Master data''' represents the business objects which are agreed on and shared across the enterprise.<ref>[http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202010/Papers/2B1_EnterpriseMasterDataArchitecture.pdf "ENTERPRISE MASTER DATA ARCHITECTURE: DESIGN DECISIONS AND OPTIONS"], Boris Otto & Alexander Schmidt, Institute of Information Management, University of St. Gallen</ref> It  can cover relatively static reference data, [[Dynamic data|transactional]], [[Unstructured data|unstructured]], analytical, [[Hierarchical database model|hierarchical]] and [[Metadata|meta]] data.<ref>"What Is Master Data?", Roger Wolter and Kirk Haselden, Microsoft Corporation, [http://msdn.microsoft.com/en-us/library/bb190163.aspx "The What, Why, and How of Master Data Management"], November 2006</ref> It is the primary focus of the [[Information Technology]] (IT) discipline of [[Master Data Management]] (MDM). 

While master data is often non-transactional in nature, it is not limited to non-transactional data, and often ''supports'' transactional processes and operations. For example, Master data may be about: customers, products, employees, materials, suppliers, and vendors, and it may also cover: sales, documents and aggregated sales.

==Types of master data ==

'''Reference Data''' is the set of permissible values to be used by other (master or transaction) data fields. Reference data normally changes slowly, reflecting changes in the modes of operation of the business, rather than changing in the normal course of business.

'''Master Data''' is a ''single source'' of common business data used across '''multiple''' systems, applications, and/or processes.

'''Enterprise Master Data''' is the ''single source'' of common business data used across '''all''' systems, applications, and processes for an entire enterprise (all departments, divisions, companies, and countries).

'''Market Master Data''' is the ''single source'' of common business data for an entire marketplace. Market master data is used among enterprises within the value chain. An example of Market Master Data is the UPC (Universal Product Code) found on consumer products.

Market Master Data is compatible with enterprise-specific and domain-specific systems, compliant with or linked to industry standards, and incorporated within market research analytics. Market master data also facilitates integration of multiple data sources and literally puts everyone in the market on the same page.

Excerpted from ''Master Data Management for Media: A Call to Action for Business Leaders in Marketing, Advertising, and the Media,'' a Microsoft White Paper by Scott Taylor and Robin Laylin, January 2010

==Master data and Master reference data==
Master data is also called '''Master reference data'''. This is to avoid confusion with the usage of the term Master data for '''[[original data]]''', like an original recording (see also: [[Master Tape]]). Master data is nothing but unique data, i.e., there are no duplicate values.{{Citation needed|date=January 2011}}

'''Material Master Data''' is a specific data set holding structured information about spare parts, raw materials and products within Enterprise Resource Planning (ERP) software. The data is held centrally and used across organisations.

'''Vendor Master''' refers to the centralised location of information pertinent to the Vendor. Often this will include the Legal entity name, Tax identification and contact information.

==Master Data Management==
{{main|Master Data Management}}
Curating and managing master data is key to ensuring master data quality. Analysis and reporting is greatly dependent on the quality of an organization's master data. Master data may either be stored in a central repository, sourced from one or more systems, or referenced centrally using an index. However, when it is used by several functional groups it may be distributed and redundantly stored in different applications across an organization and this copy data may be inconsistent (and if so, inaccurate).<ref>[http://blogs.gartner.com/andrew_white/2014/01/14/the-elephant-in-the-room-master-data-and-application-data/#comments "The Elephant in the Room – Master Data and Application Data"], Andrew White, Gartner, 14 January 2014</ref> Thus Master Data should have an agreed-upon view that is shared across the organization. Care should be taken to properly version Master Data, if the need arises to modify it, to avoid issues with distributed copies.

==See also==
*[[Master data management]]
*[[Customer data integration]]
*[[Product information management]]
*[[Data governance]]

== External links==
{{reflist}}
* [http://www.stibosystems.com/Files/Billeder/Stibo%20Systems%20images/UK_Resource_Library_Images/What-is-Master-Data-Management_EN.png What is Master Data?]
* [http://www.semarchy.com/overview/what-is-master-data/ Semarchy: What is Master Data?]
* [http://www.orchestranetworks.com/rdm/ Managing Reference Data (RDM)]
* [https://www.youtube.com/watch?v=2tzVUqWAovg Aaron Zornes: Understanding Reference Data]

[[Category:Data management]]

[[fr:Données de référence]]
<=====doc_Id=====>:54
<=====title=====>:
DAMA
<=====text=====>:
{{about|the association||Dama (disambiguation)}}

== About DAMA ==

'''DAMA''' (the [http://dama.org Data Management Association]) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and [[data resource management]] (DRM).

DAMA's primary purpose is to promote the understanding, development and practice of managing information and data as a key enterprise asset. The group is organized as a set of more than 40 chapters and members-at-large around the world, with an International Conference held every year. 

== Chapters, Chapter Structure, and Central Membership ==

DAMA International is organised through a Chapter structure, with each Chapter being a separate legal entity that formally affiliates with DAMA International. There are over 40 chapters established in over 16 countries around the world. The United States is disproportionately represented in the number of Chapters due to the number of city-based chapters as opposed to country-level in other jurisdictions.

In 2015 DAMA introduced a "Central" membership to help support and develop member services in a more consistent manner internationally and to provide a clear rallying point for members world wide who may lack a local chapter structure.

A full listing of Chapters can be found on the [http://www.dama.org/browse-chapters DAMA International website]. This list does not include "chapters in formation" which have yet to meet the criteria for recognition as Chapters and formal affiliation with DAMA-I.

== The Data Management Body of Knowledge (DMBOK) ==

The DAMA Guide to the [[Data management|Data Management]] Body of Knowledge" (DAMA-DMBOK Guide) was first published in April 5th, 2009.  

It defines ten knowledge domains which are at the core of Information and Data Management. 
* Data Governance (the central knowledge domain that connects all the others) 
* Data Architecture Management 
* Data Development 
* Data Operations Management 
* Data Security Management 
* Reference and Master Data Management 
* Data Warehousing and Business Intelligence 
* Document and Content Management 
* Metadata Management 
* Data Quality Management 
The DMBOK is copyright DAMA International.

== CDMP ==

DAMA International is the owner of the [[Certified Data Management Professional]] certification. This Certification is based on a range of learning objectives derived from the DMBOK.

In October 2015, DAMA International terminated its relationship with ICCP who had provided administrative services for the delivery of the CDMP certification. 

== DAMA Awards ==

From its inception in 1989 through to 2002, The DAMA Individual Achievement Awards have recognized a data professional who has made significant, demonstrable contributions to the information resource management industry consistent with DAMA International's vision.  From 2003 to 2015, DAMA created additional categories to recognize more people who have made special contributions to the world of data management.  The awards categories are:
* Academic Achievement Award: To a member from academia for outstanding research or theoretical contributions in the area of IRM/DRM
* DAMA Community Award : To a member of the DAMA community who has gone beyond the call of volunteer service to enhance the efforts of providing exceptional benefits to the DAMA Membership.
* Government Achievement Award : To a member of the leadership populace for instituting the inclusion and adherence to DRM/IRM principles.
* Professional Achievement Award : To a member from the ‘industry’ (business, discipline, specialist) who has made significant, demonstrable contributions to the IRM/DRM.
* Lifetime Achievement and Contribution Award : This special award has been presented to John Zachman in 2002, Michael Brackett in 2006 and Catherine Nolan in 2015. 

Starting in 2016, DAMA International created the DAMA International Award for Data Management Excellence.  This award will be presented to organizations or individuals who have made contributions to data management principles.  The first awards under this new structure will be given in April, 2016.

=== List of Award Winners ===

A list of award winners can be found on the [https://www.dama.org/content/award-results DAMA website].

=== Speakers Bureau ===
DAMA International provides a speakers bureau service to connect conference and event organisers with internationally regarded expert speakers.

A full listing of speakers can be found [http://www.dama.org/speakers here].

== References ==
<references />

==External links==
* [http://www.dama.org/ DAMA International]

{{prof-assoc-stub}}
[[Category:Data management]]
<=====doc_Id=====>:57
<=====title=====>:
Content Engineering
<=====text=====>:
{{Unreferenced|date=May 2009}}
'''Content Engineering''' is a term applied to an engineering speciality dealing with the issues around the use of [[Content (media and publishing)|content]] in computer-facilitated environments.  Content production, [[content management]], content modelling, content conversion, and content use and repurposing are all areas involving this speciality.  It is not a speciality with wide industry recognition and is often performed on an ad hoc basis by members of software development or content production staff, but is beginning to be recognized as a necessary function in any complex content-centric project involving both content production as well as software system development.

Content engineering tends to bridge the gap between groups involved in the production of content ([[Publishing]] and [[Editing|Editorial staff]], [[Marketing]], [[Sales]], [[Human resources|HR]]) and more technologically oriented departments such as [[Software Development]], or [[Information technology|IT]] that put this content to use in web or other software-based environments, and requires an understanding of the issues and processes of both sides.

Typically, Content Engineering involves extensive use of Embedded,  [[XML]] technologies, XML being the most widespread language for representing structured content. [[Content_management_system|Content Management Systems]] are often key technology used in this practice though frequently Content Engineering fills the gap where no formal CMS has been put into place.

[[Category:Data management]]
<=====doc_Id=====>:60
<=====title=====>:
Scriptella
<=====text=====>:
{{Multiple issues|
{{unreferenced|date=March 2012}}
{{Notability|Products|date=March 2012}}
}}

{{Infobox Software
| name = Scriptella
| logo = [[File:Scriptella logo.png|160px|Scriptella logo]]
| latest_release_version = 1.1
| latest_release_date = 28 December 2012
| operating_system = [[Cross-platform]]
| genre = [[Extract transform load|ETL]], [[Data migration]] and [[SQL]].
| license = [[Apache Software License]]
| website = [http://scriptella.org http://scriptella.org]
}}

'''Scriptella''' is an open source [[Extract transform load|ETL (Extract-Transform-Load)]] and script execution tool written in Java. Its primary focus is simplicity. It doesn't require the user to learn another complex XML-based language to use it, but allows the use of SQL or another scripting language suitable for the data source to perform required transformations. Scriptella does not offer any [[graphical user interface]].

==Typical use==
* Database migration.
* Database creation/update scripts.
* Cross-database ETL operations, import/export.
* Alternative for Ant <sql> task.
* Automated database schema upgrade.

==Features==
* '''Simple XML syntax''' for scripts. Add dynamics to your existing SQL scripts by creating a thin wrapper XML file:<source lang="xml">
      <!DOCTYPE etl SYSTEM "http://scriptella.javaforge.com/dtd/etl.dtd">
      <etl>
          <connection driver="$driver" url="$url" user="$user" password="$password"/>
          <script>
              <include href="PATH_TO_YOUR_SCRIPT.sql"/>
              -- And/or directly insert SQL statements here
          </script>
      </etl>
</source>
* Support for '''multiple datasources''' (or multiple connections to a single database) in an ETL file.
* Support for many useful '''[[Java Database Connectivity|JDBC]] features''', e.g. parameters in SQL including file blobs and JDBC escaping.
* '''Performance.''' Performance and low memory usage are one of the primary goals.
* Support for '''evaluated expressions and properties''' (JEXL syntax)
* Support for '''cross-database ETL scripts''' by using <dialect> elements
* '''Transactional execution'''
* '''Error handling''' via <onerror> elements
* '''Conditional scripts/queries execution''' (similar to Ant if/unless attributes but more powerful)
* '''Easy-to-Use''' as a standalone tool or Ant task. No deployment/installation required.
* '''Easy-To-Run''' ETL files directly from Java code.
* '''Built-in adapters for popular databases''' for a tight integration. Support for any database with JDBC/[[Open Database Connectivity|ODBC]] compliant driver.
* Service Provider Interface (SPI) for interoperability with non-JDBC DataSources and integration with scripting languages. Out of the box support for [[JSR 223|JSR 223 (Scripting for the Java Platform)]] compatible languages.
* Built-In [[Comma-separated values|CSV]], TEXT, [[XML]], [[Lightweight Directory Access Protocol|LDAP]], [[Apache Lucene|Lucene]], [[Apache Velocity|Velocity]], JEXL and Janino providers. Integration with [[Java EE]], [[Spring framework|Spring Framework]], [[Java Management Extensions|JMX]] and [[JNDI]] for enterprise ready scripts.

==External links==
* [http://scriptella.org Scriptella ETL Site]
* [https://github.com/scriptella/scriptella-etl GitHub Page]
* [http://groups.google.com/group/scriptella/ Discussion forum]
* [http://www.javaforge.com/proj/forum/browseForum.do?forum_id=3126 Discussion forum(deprecated)]
* [http://jroller.com/page/ejboy Scriptella ETL Author's Blog]
* {{Ohloh project|id=4526|name=Scriptella ETL}}

[[Category:Extract, transform, load tools]]
[[Category:Data warehousing products]]
[[Category:Data management]]


{{compu-stub}}
<=====doc_Id=====>:63
<=====title=====>:
Category:Transaction processing
<=====text=====>:
{{Cat main|Transaction processing}}

[[Category:Concurrency control]]
[[Category:Data management]]
[[Category:Utility software by type]]
[[Category:Databases]]
<=====doc_Id=====>:66
<=====title=====>:
Category:Recording
<=====text=====>:
{{Commons category|Recording}}
{{Cat main|Recording}}

[[Category:Data management]]
[[Category:Information storage]]
<=====doc_Id=====>:69
<=====title=====>:
Global concurrency control
<=====text=====>:
{{POV|Commitment ordering|date=November 2011}}
'''Global concurrency control''' typically pertains to the [[concurrency control]] of a system comprising several components, each with its own concurrency control. The overall concurrency control of the whole system, the ''Global concurrency control'', is determined by the concurrency control of its components, [[Modular programming|module]]s. In this case also the term '''Modular concurrency control''' is used.

In many cases a system may be distributed over a communication network. In this case we deal with [[distributed concurrency control]] of the system, and the two terms sometimes overlap. However, distributed concurrency control typically relates to a case where the distributed system's components do not have each concurrency control of its own, but rather are involved with a concurrency control mechanism that spans several components in order to operate. For example, as typical in a [[distributed database]].

In ''[[database systems]]'' and ''[[transaction processing]]'' (''transaction management'') global concurrency control relates to the concurrency control of a ''multidatabase system'' (for example, a [[Federated database]]; other examples are [[Grid computing]] and [[Cloud computing]] environments). It deals with the properties of the ''global [[schedule (computer science)|schedule]]'', which is the unified schedule of the multidatabase system, comprising all the individual schedules of the [[database system]]s and possibly other [[transactional object]]s in the system. A major goal for global concurrency control is ''[[Global serializability]]'' (or ''Modular serializability''). The problem of achieving global serializability in a [[heterogeneous]] environment had been [[open problem|open]] for many years, until an effective solution based on [[Commitment ordering]] (CO) has been proposed (see [[Global serializability]]). Global concurrency control deals also with [[global serializability#Relaxing global serializability|relaxed]] forms of global serializability which compromise global serializability (and in many applications also correctness, and thus are avoided there). While local (to a database system) [[Serializability#Relaxing serializability|relaxed serializability methods]] compromise serializability for performance gain (utilized when the application allows), it is unclear that the various proposed relaxed global serializability methods provide any performance gain over CO, which guarantees global serializability.


==See also==
*[[Concurrency control]]
*[[Global serializability]]
*[[Commitment ordering]]
*[[Distributed concurrency control]]

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]
<=====doc_Id=====>:72
<=====title=====>:
Dynamic knowledge repository
<=====text=====>:
{{refimprove|date=August 2007}}

The '''dynamic knowledge repository''' ('''DKR''') is a concept developed by [[Douglas C. Engelbart]] as a primary strategic focus for allowing humans to address complex problems.{{when|date=September 2011}} Doug has proposed that a DKR will enable us to develop a collective [[IQ]] greater than any individual's IQ. References and discussion of Engelbart's DKR concept are available at the [[Doug Engelbart Institute]].<ref>{{cite web | url=http://www.dougengelbart.org/about/dkrs.html|title=About Dynamic Knowledge Repositories – an introduction | accessdate=September 15, 2011 | author=Christina Engelbart}}</ref>

==Definition==
A knowledge repository is a computerized system that systematically captures, organizes and categorizes an organization's knowledge. The repository can be searched and data can be quickly retrieved.

The effective knowledge repositories include factual, conceptual, procedural and meta-cognitive techniques. The key features of knowledge repositories include communication forums.

A knowledge repository can take many forms to "contain" the knowledge it holds. A customer database is a knowledge repository of customer information and insights – or electronic explicit knowledge. A Library is a knowledge repository of books – physical explicit knowledge. A community of experts is a knowledge repository of tacit knowledge or experience. The nature of the repository only changes to contain/manage the type of knowledge it holds. A repository (as opposed to an archive) is designed to get knowledge out. It should therefore have some rules of structure, classification, taxonomy, record management, etc., to facilitate user engagement.

==References==
{{Reflist|2}}

== External links==
* [http://dougengelbart.org/ Doug Engelbart Institute]

[[Category:Knowledge representation]]
[[Category:Data management]]


{{compu-storage-stub}}
<=====doc_Id=====>:75
<=====title=====>:
Client-side persistent data
<=====text=====>:
'''Client-side persistent data''' or CSPD is a term used in [[computing]] for storing data required by [[web application |web applications]] to complete internet tasks on the [[client-side]] as needed rather than exclusively on the [[Server (computing) |server]]. As a framework it is one solution to the needs of [[Occasionally connected computing]] or OCC.

A major challenge for [[HTTP]] as a [[Stateless server |stateless]] [[Protocol (computing)|protocol]] has been asynchronous tasks. The [[Ajax (programming)|AJAX]] pattern using [[XMLHttpRequest]] was first introduced by [[Microsoft]] in the context of the [[Outlook Web App|Outlook]] e-mail product.

The first CSPD were the [[HTTP cookie |'cookies']] introduced by the [[Netscape]] [[Netscape (web browser)|Navigator]].  [[ActiveX]] components which have entries in the [[Windows registry]] can also be viewed as a form of [[client-side]] [[Persistence (computer science)|persistence]].

==See also==
* [[Occasionally connected computing]]
* [[Curl (programming_language)]]
* [[Ajax (programming)|AJAX]]
* [[HTTP]]
* [[Web storage]]

==External links==
* [http://www.curl.com/developer/faq/cspd/ CSPD]
* [http://safari.ciscopress.com/0596101996/jscript5-CHP-19-SECT-6 Safari] preview
* [http://wp.netscape.com/newsref/std/cookie_spec.html Netscape] on persistent client state

[[Category:Clients (computing)]]
[[Category:Data management]]
[[Category:Web applications]]
<=====doc_Id=====>:78
<=====title=====>:
Distributed concurrency control
<=====text=====>:
{{POV|Commitment ordering|date=November 2011}}
'''Distributed concurrency control''' is the [[concurrency control]] of a system [[Distributed computing|distributed]] over a [[computer network]] ([[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]). 

In ''[[database systems]]'' and ''[[transaction processing]]'' (''transaction management'') distributed concurrency control refers primarily to the concurrency control of a [[distributed database]]. It also refers to the concurrency control in a multidatabase (and other multi-[[transactional object]]) environment (e.g., [[federated database]], [[grid computing]], and [[cloud computing]] environments. A major goal for distributed concurrency control is distributed [[serializability]] (or [[global serializability]] for multidatabase systems). Distributed concurrency control poses special challenges beyond centralized one, primarily due to communication and computer [[latency (engineering)|latency]]. It often requires special techniques, like [[distributed lock manager]] over fast [[computer network]]s with low latency, like [[switched fabric]] (e.g., [[InfiniBand]]). [[commitment ordering]] (or commit ordering) is a general serializability technique that achieves distributed serializability (and global serializability in particular) effectively on a large scale, without concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets), and thus without performance penalties that are typical to other serializability techniques ([[#Raz92|Raz 1992]]).

The most common distributed concurrency control technique is ''strong strict two-phase locking'' ([[two phase locking#Strong strict two-phase locking|SS2PL]], also named ''rigorousness''), which is also a common centralized concurrency control technique. SS2PL provides both the ''serializability'', ''[[schedule (computer science)#Strict|strictness]]'', and ''commitment ordering'' properties. Strictness, a special case of recoverability, is utilized for effective recovery from failure, and commitment ordering allows participating in a general solution for global serializability. For large-scale distribution and complex transactions, distributed locking's typical heavy performance penalty (due to delays, latency) can be saved by using the [[atomic commitment]] protocol, which is needed in a distributed database for (distributed) transactions' [[Atomicity (database systems)|atomicity]] (e.g., [[Two-phase commit protocol|two-phase commit]], or a simpler one in a reliable system), together with some local commitment ordering variant (e.g., local [[two phase locking#Strong strict two-phase locking|SS2PL]]) instead of distributed locking, to achieve global serializability in the entire system. All the commitment ordering theoretical results are applicable whenever atomic commitment is utilized over partitioned, distributed recoverable (transactional) data, including automatic ''distributed deadlock'' resolution. Such technique can be utilized also for a large-scale [[parallel database]], where a single large database, residing on many nodes and using a distributed lock manager, is replaced with a (homogeneous) multidatabase, comprising many relatively small databases (loosely defined; any process that supports transactions over partitioned data and participates in atomic commitment complies), fitting each into a single node, and using commitment ordering (e.g., SS2PL, strict CO) together with some appropriate atomic commitment protocol (without using a distributed lock manager).

==See also==
*[[Global concurrency control]]

==References==
*<cite id=Bern87>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 </cite>
*<cite id=Weikum01>[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8 </cite>
*<cite id=Raz92>[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  ''Proceedings of the Eighteenth International Conference on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) </cite>

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]
<=====doc_Id=====>:81
<=====title=====>:
Category:Data modeling
<=====text=====>:
{{Commons cat|Data modeling}}
In information system design, '''[[data modeling]]''' is the analysis and design of the information in the system, concentrating on the logical entities and the logical dependencies between these entities

{{catdiffuse}}

<!--  -->

[[Category:Computer-aided software engineering tools]]
[[Category:Data management|Modeling]]
[[Category:Scientific modeling]]
[[Category:Software design]]
<=====doc_Id=====>:84
<=====title=====>:
Category:Data-centric programming languages
<=====text=====>:
{{Cat main|Data-centric programming language}}

This [[Wikipedia:Category|category]] lists those [[programming languages]] that are data-centric, with significant built-in functionality for data storage and manipulation.

[[Category:Data management|Programming languages]]
[[Category:Persistent programming languages]]
<=====doc_Id=====>:87
<=====title=====>:
Virtual facility
<=====text=====>:
{{Advert|article|date=May 2013}}
[[Image:VirtualFacility.jpg|thumb|250px|A Virtual Facility snapshot created with 6SigmaDC software.|right]]
A '''Virtual Facility''' (VF) is a highly realistic digital representation of a [[data center]] (primarily). The term virtual in Virtual Facility refers to the use of the word as in [[Virtual reality|Virtual Reality]] rather than the abstraction of computer resources as in [[platform virtualization]]. The VF mirrors the characteristics of the physical facility over time and allows modeling all relevant characteristics of a physical data center with a high degree of precision. 

==VF Model includes==

* Three-dimensional physical facility layout
* Network connectivity of facility equipment
* Full inventory of facility equipment, including electronics and electrical systems such as [[Power distribution unit|Power Distribution Units]] (PDU’s) and [[Uninterruptible power supplies|Uninterruptible Power Supplies]] (UPS’s)
* Full air conditioning system (ACU’s) and controls within the room

The term Virtual Facility was introduced by Future Facilities, a data centre design consultancy focused on delivering Design and Operational solutions to address the emerging environmental problems facing the modern Mission Critical Facility (MCF). The concept is in essence a convergence of the fields of [[Virtual reality|Virtual Reality]] (VR), [[Computer simulation|Computer Simulation]] and [[Expert systems|Expert Systems]], applied to the specific domain of facilities.

The VF type of computer simulation allows detailed analysis and prototyping of air flow in the data center by making use of [[Computational fluid dynamics|Computational Fluid Dynamics]] (CFD) techniques. This in turn allows the air flow and temperatures of the facility to be analyzed visually ([[Scientific visualization|Scientific Visualisation]]) and numerically to study and predict what will happen in the real facility. The importance of scientific methods in design of mission critical facilities has become a necessity, since the performance gains predicted by [[Moore’s law|Moore's Law]] go hand in hand with a rise in power and heat dissipated by equipment. Rules of thumb have proven to be no longer adequate.

==VF design purposes==

* Green field design
* Asset management
* Troubleshooting existing data centers
* Making existing data centers more resilient
* Making existing data centers more energy efficient
* Cost prediction
* Staff training
* Capacity planning
* Load growth management
 
The VF is now being employed by many large organizations as a way of virtually assessing a situation before having to spend huge sums of money trying to solve a problem in the real facility.

It is essential to know whether adding new equipment or changing equipment will cause a logistical or thermal problem.  The VF allows the designer or operator to assess the best course of action and gives in depth understanding on unintuive behaviours.

==References==
{{reflist}}
* {{Citation
  | last = Seymour
  | first = Mark
  | title = Virtual Data Centre Design. A blueprint for success
  | url=http://www.futurefacilities.com/newsarticles/articles/commerzbankarticlesummerZDTjournal.pdf
  | accessdate = 2007-09-26}}

[[Category:Data management]]
<=====doc_Id=====>:90
<=====title=====>:
Integration competency center
<=====text=====>:
{{ad|date=March 2014}}
{{peacock|date=March 2014}}
The '''integration competency center''' (ICC), sometimes referred to as an '''integration center of excellence''' (COE), is a [[shared services|shared service]] function within an organization, particularly large corporate enterprises as well as public sector institutions, for performing methodical [[data integration]], [[system integration]] or [[enterprise application integration]]. 

[[Data integration]] allows companies to access their enterprise data and functions, fragmented across disparate systems, in order to create a combined, accurate, and consistent view of their core information as well as process assets and leverage them across the enterprise to drive business decisions and operations.  System integration is the bringing together of component subsystems into one system and ensuring that they function together effectively. Enterprise application integration enables efficient information exchanges and business process automation across separate computer applications in a [[Cohesion (computer science)|cohesive]] fashion.

== Overview ==
The term may be better understood by examining each of the three words that comprise the acronym. '''Integration''' refers to the objective of the ICC to take a holistic perspective and optimize certain qualities such as cost efficiency, organizational agility and effectiveness, operational risk, customer (internal or external) experience, etc. across multiple functional groups. '''Competency''' refers to the expertise, knowledge or capability that the ICC offers as services.  '''Center''' means that the service is managed or coordinated from a common (central) point independent from the functional areas that it supports. 

Large organizations are usually sub-divided into functional areas such as marketing, sales, distribution, finance, human resources to name just a few. These functional groups have separate operations and are ''vertically integrated'' and are therefore sometimes referred to as "silos" or "stovepipes".  From an organizational perspective, an ICC is a group of people with special skills, who are centrally coordinated, and offer services to accomplish a mission that requires separate functional areas to work together. 

Key objectives of an ICC are:

* Lead and support enterprise integration (data, system and process) projects with the cooperation/coordination of subject matter experts
* Promote Enterprise integration as a formal discipline. For example, data integration will include [[data warehousing]], [[data migration]], [[data quality]] management, data integration for [[service oriented architecture]] deployments, and [[data synchronization]]. Similar system integration will include common messaging services, business service virtualization etc.
* Develop staff specialists in integration processes and operations and leverage their expertise company-wide
* Assess and select integration technology and tools from the marketplace
* Manage integration pilots and projects across the organization
* Optimize integration investments across the enterprise level
* Leverage economies of scale for the integration tools portfolio at enterprise level

ICCs allow companies to:
* Optimize scarce resources by combining integration skills, resources, and processes into one group
* Reduce project delivery times and development and maintenance costs through effectiveness and efficiency
* Improve ROI through creation and reuse of enterprise assets like source definitions, application interfaces, and codified business rules
* Decrease duplication of integration related effort across the enterprise
* Build on past successes instead of reinventing the wheel with each project
* Lower total technology cost of ownership by leveraging technology investments across multiple projects

An ICC may be a temporary group in support of a [[program management|program]] or a permanent part of the organization.  Furthermore, ICC’s can be established at various scales or levels; within a division of a company, at the enterprise level, or across multiple companies in a supply chain.

== History ==
The term "integration competency center" and its acronym ICC was popularized by Roy Schulte of [[Gartner]] in a series of articles and conference presentations beginning in 2001 with ''The Integration Competency Center'' {{citation missing|date=February 2013}}<!--[Ref SPA-14-0456] mostly useless, cannot be found anywhere-->. He picked up the term from one of his colleagues, Gary Long, who found some of his clients using it (they took the established term “competency center” and applied it to integration). Prior to that (from 1997 to 2001) Gartner had been referring to it as the ''central integration team''. The concept itself (even before it was given a label) goes back to 1996 in one of Gartner’s first reports on integration. {{citation missing|date=February 2013}}

A major milestone was the publication in 2005 of the first book on the topic: ''Integration Competency Center: An Implementation Methodology''<ref name="ICC">John G. Schmidt and David Lyle (2005), ''Integration Competency Center, An Implementation Methodology,'' ISBN 0-9769163-0-4</ref> by ''John G. Schmidt'' and ''David Lyle''. The book introduced five ICC organizational models and explored the people, process and technology dimensions of ICC’s.  Several reviews of the book can be found at [http://blogs.ittoolbox.com/eai/business/archives/soa-competency-center-5731  IT Toolbox] and at [http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304 Amazon]. The concept of integration as a competency in the IT domain has now survived for over 10 years and appears to be picking up momentum and broad-based acceptance. 

These days, ICC’s are often called, integration center of excellence, SOA center of excellence, the data management center of excellence and other variants. The most advanced ICC's are using [[Lean Integration]] practices to optimize end-to-end processes and to drive continuous improvements. Universities are also beginning to include integration topics in their MBA programs and computer science curricula. For example, The College of Information Sciences and Technology at Penn State University has established a [http://ist.psu.edu/facultyresearch/facilities/eii/  Enterprise Informatics and Integration Center] with the following mission:

"''The Enterprise Informatics and Integration Center (EI²) will actively engage industry, non-profit, and government agency leaders to address critical issues in enterprise processes, knowledge management, and decision making.''"

== Operating models ==
There are a number of ways an ICC can be organized and a wide range of responsibilities with which it can be chartered. The ICC book<ref name="ICC"/> introduced five ICC organizational models and explored the people, process and technology dimensions of ICCs. They include:

=== Best practices ICC ===
The primary function of this ICC model is to document best practices. It does not include a central support or development team to implement those standards across projects, and probably not metadata either. To implement a best practices ICC, companies need a flexible development environment that supports diverse teams and that enables the team to enhance and extend existing systems and processes. Such a team might be a subset of an existing enterprise architecture capability and generally consists of a small number of staff (1-5).

=== Standard services ICC ===
A standard services ICC provides the same knowledge leverage as a best practices ICC, but enforces technical consistency in software development and hardware choices. A standard services ICC focuses on processes, including standardizing and enforcing naming conventions, establishing metadata standards, instituting change management procedures, and providing standards training. This type of ICC also reviews emerging technologies, selects vendors, and manages hardware and software systems.  This style of ICC is often tightly linked with the enterprise architecture team and may be slightly larger than a typical best practices ICC.

=== Shared services ICC ===
A shared services ICC provides a supported technical environment and services ranging from development support all the way through to a help desk for projects in production. This type of ICC is significantly more complex than a best practices or Standard Services model. It establishes processes for knowledge management, including product training, standards enforcement, technology benchmarking, and metadata management, and it facilitates impact analysis, software quality, and effective use of developer resources across projects. The organizational structure of a Shared Services ICC is sometimes referred to as a hybrid or federated model which often includes a small central coordinating team plus dotted-line reporting relationships with multiple distributed teams.

=== Central services ICC ===
A central services ICC controls integration across the enterprise. It carries out the same processes as the other models, but in addition usually has its own budget and a charge-back methodology. It also offers more support for development projects, providing management, development resources, [[data profiling]], data quality, and unit testing. Because a central services ICC is more involved in development activities than the other models, it requires a production operator and a data integration developer.  The staff in a central services ICC does not necessarily need to be a central location and may be distributed geographically; the important distinction is that the staffs have a solid-line reporting relationship to the ICC Director.  The size of these teams can vary and may be as large as 10%-15% of the IT staff in an organization.

=== Self service ICC ===
The self-service ICC represents the highest level of maturity in an organization.  The ICC itself may be almost invisible in that its functions are so ingrained in the day-to-day systems development life-cycle and its operations are so tightly integrated with the infrastructure that it may require only small central team to sustain itself.  This ICC model achieves both a highly efficient operation and provides an environment where independent development and innovation can flourish. This goal is achieved by strict enforcement of a set of application integration standards through automated processes enabled by tools and systems.

== Key challenges ==
ICC as a concept is fairly simple. It is embodiment of the IT management best practices to deliver shared services. However, being an organizational concept, it is far more challenging to implement in practice than the conceptual view because every organization has different DNA and it takes specific personalization/customization effort for ICC that makes the ICC initiative successful. Here are some of the common challenges in ICC establishment journey:
* Change management in terms of technology, processes, organization structure
* Ability of the organization to deal with the pace and quantum of change
* Alignment of stakeholders and process owners for ICC strategy
* Inappropriate ownership level for ICC program and lack of senior management sponsorship
* Highly tactical focus and business program level constraints
* Ignoring foundation elements and jumping to implementation directly
* Inappropriate funding

These issues are important to consider when embarking on the ICC investment since the last leg of the implementation of ICC that's what matters most. Intellectual definition of ICC that is not implemented in the organisation has no real value for the enterprise.

== See also ==
* [[Lean Integration]]

== References ==
<references/>

== Further reading ==
* Maurizio Lenzerini (2002). "Data Integration: A Theoretical Perspective". PODS 2002: 243-246.

== External links ==
* Integration Competency Center book: (http://www.amazon.com/Integration-Competency-Center-Implementation-Methodology/dp/0976916304)
* [[Informatica]] ICC Blog: (http://blogs.informatica.com/perspectives/category/data-integration/integration-competency-centers/)
* Gartner paper  (http://www.ebizq.net/topics/tech_in_biz/features/5360.html)
* Integration Consortium: (http://www.integrationconsortium.org)
* Infosys ICC Blogs (http://www.infosysblogs.com/bpm-eai/integration_competency_center_icc)
* ICC Handbook (http://www.unthink.fi/Global/PDF/ICC-Handbook.pdf)
* Integration Warstories - article about avoiding ICC pitfalls (http://integrationwarstories.com/2013/10/25/avoiding-pitfalls-of-integration-competency-centers/)

[[Category:Data management]]
[[Category:Software development philosophies]]
[[Category:Information technology]]
<=====doc_Id=====>:93
<=====title=====>:
Operational database
<=====text=====>:
{{more footnotes|date=March 2013}}

Operational database management systems (also referred to as [[OLTP]] On Line Transaction Processing databases), are used to manage dynamic data in real-time. These types of databases allow you to do more than simply view archived data. Operational databases allow you to modify that data (add, change or delete data), doing it in [[Real-time computing|real-time]].

Since the early 90's, the operational database software market has been largely taken over by [[SQL]] engines. Today, the operational [[DBMS]] market (formerly [[OLTP]]) is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. Operational databases are increasingly supporting [[distributed database]] architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.

Recognizing the growing role of operational databases in the IT industry that is fast moving from legacy databases to real-time operational databases capable to handle distributed web and mobile demand and to address [[Big data]] challenges, in October 2013 [[Gartner]] started to publish the [[Magic Quadrant]] for Operational Database Management Systems.<ref name="Gartner Magic Quadrant for Operational Database Management Systems">{{cite web|url=https://www.gartner.com/doc/2610218/magic-quadrant-operational-database-management |title=Gartner Magic Quadrant for Operational Database Management Systems|publisher=Gartner.com}}</ref>

== List of Operational Databases ==

{| style="text-align: left;" class="wikitable sortable"
|-
! Database platform !! Database model !! [[SQL]] Support !! [[NoSQL]] Support !! Managed objects !! ACID-transactions
|-
| [[Aerospike database|Aerospike]] || Key–Value Store ||  No || '''Yes''' || key-value pairs || None
|-
| [[Altibase]] || Relational database || '''Yes''' || NO || tabular data || Real-time ACID transactions
|-
| [[Apache Cassandra]] || Key-value store || No || '''Yes''' || key-value pairs || None
|-
| [[Cloudant]] || Document-Oriented Database || No || '''Yes''' || JSON || None
|-
| [[Clusterpoint]] || Document-Oriented Database || '''Yes''' (essential SQL)  || '''Yes''' || XML, JSON, text data || Distributed ACID-transactions 
|-
| [[Clustrix]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-transactions 
|-
| [[Couchbase]] || Document-Oriented Database || '''Yes''' (N1QL) || '''Yes''' || JSON || None
|-
| [[CouchDB]] || Document-Oriented Database || No || '''Yes''' || JSON || None 
|-
| [[EnterpriseDB]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[FoundationDB]] || Key-value store || '''Yes''' || No || key-value pairs || ACID-transactions 
|-
| [[IBM DB2]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[Ingres_(database)|Ingres]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[MarkLogic]] || Document-Oriented Database || No || '''Yes''' (XQuery) || XML || ACID-transactions
|-
| [[Microsoft SQL Server]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions
|-
| [[MongoDB]] || Document-Oriented Database || No || '''Yes''' || BSON || None
|-
| [[NuoDB]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-compliant
|-
| [[Oracle Database|Oracle]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions
|-
| [[OrientDB]] || Document-oriented Database || '''Yes''' || Yes || key-value pairs || ACID-transactions<ref>http://orientdb.com/docs/last/Transactions.html</ref>
|-
| [[Riak]] || Key-value store || No || '''Yes''' || key-value pairs || None
|-
| [[SAP HANA]] || Relational Database || '''Yes''' || No || tabular data || ACID-transactions 
|-
| [[VoltDB]] || Relational Database || '''Yes''' (newSQL) || No || tabular data || ACID-transactions

|}

== Use in business ==

Operational databases are used to store, manage and track real-time business information. For example, a company might have an operational database used to track warehouse/stock quantities. As customers order products from an online web store, an operational database can be used to keep track of how many items have been sold and when the company will need to reorder stock.  An '''operational database''' stores information about the activities of an [[organization]], for example [[customer relationship management]] transactions or financial operations, in a computer [[database]].

Operational databases allow a business to enter, gather, and retrieve large quantities of specific information, such as company legal data, financial data, call data records, personal employee information, sales data, customer data, data on assets and many other information.  An important feature of storing information in an operational database is the ability to share information across the company and over the Internet.  Operational databases can be used to manage mission-critical business data, to monitor activities, to audit suspicious transactions, or to review the history of dealings with a particular customer.  They can also be part of the actual process of making and fulfilling a purchase, for example in [[e-commerce]].

==Data warehouse terminology==

In [[Data warehouse|data warehousing]], the term is even more specific: the operational database is the one which is accessed by an [[operational system]] (for example a customer-facing website or the application used by the customer service department) to carry out regular operations of an organization. Operational databases usually use an [[online transaction processing]] database which is optimized for faster transaction processing ([[create, read, update and delete]] operations).

== See also ==
* [[Document database|Document-oriented databases]]
* [[NewSQL|NewSQL databases]]
* [[NoSQL|NoSQL databases]]
* [[XML|XML databases]]
* [[SQL|SQL databases]]
* [[Distributed database]]s

== References ==
{{Reflist|33em}}
* O’Brien, Jason., and Marakas, Gorila., (2008).  Management Information Technology Systems.  Computer Software (pp.&nbsp;185). New York, New York:  McGraw-Hill

[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Business intelligence]]
[[Category:Types of databases]]
<=====doc_Id=====>:96
<=====title=====>:
Data proliferation
<=====text=====>:
'''Data proliferation''' refers to the prodigious amount of [[data]], [[structured data|structured]] and unstructured, that businesses and governments continue to generate at an unprecedented rate and the [[usability]] problems that result from attempting to store and manage that data. While originally pertaining to problems associated with paper [[documentation]], data proliferation has become a major problem in primary and secondary [[data storage device|data storage]] on computers.

While digital storage has become cheaper, the associated costs, from raw power to maintenance and from metadata to search engines, have not kept up with the proliferation of data. Although the power required to maintain a unit of data has fallen, the cost of facilities which house the digital storage has tended to rise.<ref>{{cite web |url =http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm
|title=Downsizing the digital attic |work=Deloitte Technology Predictions |archiveurl=https://web.archive.org/web/20110722194032/http://www.deloitte.co.uk/TMTPredictions/technology/Downsizing-digital-attic-data-storage.cfm |archivedate=July 22, 2011}}</ref>

{{rquote|right| At the simplest level, company [[e-mail]] systems spawn large amounts of data. Business e-mail – some of it important to the enterprise, some much less so – is estimated to be growing at a rate of 25-30% annually. And whether it’s relevant or not, the load on the system is being magnified by practices such as multiple addressing and the attaching of large text, audio and even [[video file formats|video file]]s.|IBM Global Technology Services<ref name=IBM>[https://web.archive.org/web/20090206010415/http://www-03.ibm.com/systems/resources/systems_storage_solutions_pdf_toxic_tb.pdf “The Toxic [[Terabyte]]”, IBM Global Technology Services, July 2006]</ref>}}

Data proliferation has been documented as a problem for the [[U.S. military]] since August 1971, in particular regarding the excessive documentation submitted during the acquisition of major weapon systems.<ref name=DODPP>[http://stinet.dtic.mil/oai/oai?&verb=getRecord&metadataPrefix=html&identifier=AD0892652 Evolution of the Data Proliferation Problem within Major Air Force Acquisition Programs.]</ref> Efforts to mitigate data proliferation and the problems associated with it are ongoing.<ref>[http://www.thic.org/pdf/Jun02/dod.rroderique.020612.pdf Data Proliferation: Stop That]</ref>

==Problems caused==
The problem of data proliferation is affecting all areas of commerce as the result of the availability of relatively inexpensive data storage devices. This has made it very easy to dump data into secondary storage immediately after its window of usability has passed. This masks problems that could gravely affect the profitability of businesses and the efficient functioning of health services, police and security forces, local and national governments, and many other types of organizations.<ref name=IBM /> Data proliferation is problematic for several reasons:
*Difficulty when trying to find and retrieve information. At [[Xerox]], on average it takes employees more than one hour per week to [[document retrieval|find]] hard-copy documents, costing $2,152 a year to manage and store them. For businesses with more than 10 employees, this increases to almost two hours per week at $5,760 per year.<ref>[http://www.itbusiness.ca/it/client/en/home/News.asp?id=40615&cid=13 “Dealing with data proliferation”; Vawn Himmelsbach. it business.ca: Canadian Technology News, September 19, 2006]</ref> In large [[storage network|networks]] of primary and secondary data storage, problems finding electronic data are analogous to problems finding hard copy data.
*[[Data loss]] and legal liability when data is disorganized, not properly replicated, or cannot be found in a timely manner. In April 2005, the [[TD Ameritrade|Ameritrade Holding Corporation]] told 200,000 current and past customers that a [[Magnetic tape data storage|tape]] containing confidential information had been lost or destroyed in transit. In May of the same year, [[Time Warner Incorporated]] reported that 40 tapes containing personal data on 600,000 current and former employees had been lost en route to a storage facility. In March 2005, a Florida judge hearing a $2.7 billion lawsuit against Morgan Stanley issued an "[[adverse inference]] order" against the company for "willful and gross abuse of its discovery obligations." The judge cited Morgan Stanley for repeatedly finding misplaced tapes of e-mail messages long after the company had claimed that it had turned over all such tapes to the court.<ref>[http://www.computerworld.com/printthis/2005/0,4814,103541,00.html “Data: Lost, Stolen or Strayed”, Computer World, Security]</ref>
*Increased manpower requirements to manage increasingly chaotic data storage resources.
*Slower networks and application performance due to excess traffic as users search and search again for the material they need.<ref name=IBM />
*High cost in terms of the energy resources required to operate storage hardware. A 100 terabyte system will cost up to $35,040 a year to run—not counting cooling costs.<ref>[http://findarticles.com/p/articles/mi_m0BRZ/is_10_23/ai_111062988 "Power and storage: the hidden cost of ownership”, Computer Technology Review, October 2003]</ref>

==Proposed solutions==
*Applications that better utilize modern technology
*Reductions in duplicate data (especially as caused by data movement)
*Improvement of [[metadata]] structures
*Improvement of file and storage transfer structures
*User education and discipline<ref name=DODPP />
*The implementation of [[Information Lifecycle Management]] solutions to eliminate low-value information as early as possible before putting the rest into actively managed long-term storage in which it can be quickly and cheaply accessed.<ref name=IBM />

==See also==
*[[Backup]]
* [[Digital Asset Management]]
*[[Disk storage]]
*[[Document management system]]
*[[Hierarchical storage management]]
*[[Information Lifecycle Management]]
*[[Information repository]]
*[[Magnetic tape data storage]]
*[[Retention period|Retention schedule]]

==References==
{{reflist}}

[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Data management]]
<=====doc_Id=====>:99
<=====title=====>:
Long-lived transaction
<=====text=====>:
{{Multiple issues|
{{Orphan|date=February 2009}}
{{No sources|date=October 2015}}
}}

A '''long-lived transaction''' is a [[Database transaction|transaction]] that spans multiple database transactions. The transaction is considered "long-lived" because its boundaries must, by necessity of business logic, extend past a single database transaction. A long-lived transaction can be thought of as a sequence of database transactions grouped to achieve a single atomic result.

A common example is a multi-step sequence of requests and responses of an interaction with a user through a web client.

A long-lived transaction creates challenges of [[concurrency control]] and [[scalability]].

A chief strategy in designing long-lived transactions is [[optimistic concurrency control]] with [[Version control|versioning]].

So much research work related to these long lived transactions was carried out by several professors from the Oxford University and Michigan State University and the Central University of Hyderabad. Dr. James from the Oxford University created several hypotheses for long-lived transactions. Dr Copperfield of the Michigan State University was regarded highly for his contributions in this field. Dr A B Sagar of Hyderabad Central University has also done very creative work in relating long-lived transactions with financial transactions in Microfinance.

However the study is not complete and is still open to challenges and research issues.

==See also==
*[[Long-running transaction]]

[[Category:Data management]]
[[Category:Transaction processing]]


{{software-eng-stub}}
<=====doc_Id=====>:102
<=====title=====>:
Schema crosswalk
<=====text=====>:
A '''schema crosswalk''' is a table that shows equivalent elements (or "fields") in more than one [[database schema]]. It maps the elements in one schema to the equivalent elements in another schema.

Crosswalk tables are often employed within or in parallel to [[enterprise systems]], especially when multiple systems are interfaced or when the system includes [[legacy system]] data. In the context of Interfaces, they function as a sort of internal [[Extract, Transform, Load|ETL]] mechanism.

For example, this is a [[metadata]] crosswalk from [[MARC standards|MARC]] to [[Dublin Core]]:

<center>
{| class="wikitable"
|-
! MARC field
! 
! Dublin Core element
|-
| $260c (Date of publication, distribution, etc.)
| →
| Date.Created
|-
| 522 (Geographic Coverage Note)
| →
| Coverage.Spatial
|-
| $300a (Physical Description)
| →
| Format.Extent
|}
</center>

Crosswalks show people where to put the data from one scheme into a different scheme. They are often used by libraries, archives, museums, and other cultural institutions to translate data to or from [[MARC standards|MARC]], [[Dublin Core]], [[Text Encoding Initiative|TEI]], and other metadata schemes.  For example, say an archive has a MARC record in their catalog describing a manuscript.  If the archive makes a digital copy of that manuscript and wants to display it on the web along with the information from the catalog, it will have to translate the data from the MARC catalog record into a different format such as [[Metadata Object Description Schema|MODS]] that is viewable in a webpage.  Because MARC has different fields than MODS, decisions must be made about where to put the data into MODS. This type of "translating" from one format to another is often called "metadata mapping" or "field mapping," and is related to "[[data mapping]]", and "[[Semantic mapper|semantic mapping]]".

Crosswalks also have several technical capabilities.  They help databases using different metadata schemes to share information. They help metadata harvesters create union catalogs.  They enable search engines to search multiple databases simultaneously with a single query.

== Challenges for crosswalks ==

One of the biggest challenges for crosswalks is that no two metadata schemes are 100% equivalent.  One scheme may have a field that doesn't exist in another scheme, or it may have a field that is split into two different fields in another scheme; this is why you often lose data when mapping from a complex scheme to a simpler one.  For example, when mapping from MARC to Simple Dublin Core, you lose the distinction between types of titles:

<center>
{| class="wikitable"
|-
! MARC field
! 
! Dublin Core element
|-
| 210 Abbreviated Title
| →
| Title
|-
| 222 Key Title
| →
| Title
|-
| 240 Uniform Title
| → 
| Title
|-
| 242 Translated Title
| →
| Title
|-
| 245 Title Statement
| →
| Title
|-
| 246 Variant Title
| →
| Title
|}
</center>

Simple Dublin Core only has one single "Title" element so all of the different types of MARC titles get lumped together without any further distinctions.  This is called "many-to-one" mapping. This is also why, once you've translated these titles into Simple Dublin Core you can't translate them back into MARC.  Once they're Simple Dublin Core you've lost the MARC information about what types of titles they are so when you map from Simple Dublin Core back to MARC, all the data in the "Title" element maps to the basic MARC 245 Title Statement field.<ref>[http://www.loc.gov/marc/dccross.html "Dublin Core to MARC Crosswalk,"] Network Development and MARC Standards Office, Library of Congress</ref>

<center>
{| class="wikitable"
|-
! Dublin Core element
!
! MARC field
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|-
| Title
| →
| 245 Title Statement
|}
</center>

This is why crosswalks are said to be "lateral" (one-way) mappings from one scheme to another.  Separate crosswalks would be required to map from scheme A to scheme B and from scheme B to scheme A.<ref>{{Cite book|title=Metadata fundamentals for all librarians|last=Caplan|first=Priscilla|publisher=American Library Association|year=2003|isbn=0838908470|location=Chicago|pages=39|quote=|via=}}</ref>

===Difficulties in mapping===
Other mapping problems arise when:

*One scheme has one element that needs to be split up with different parts of it placed in multiple other elements in the second scheme ("one-to-many" mapping)
*One scheme allows an element to be repeated more than once while another only allows that element to appear once with multiple terms in it
*Schemes have different data formats (e.g. ''John Doe'' or ''Doe, John'')
*An element in one scheme is indexed but the equivalent element in the other scheme is not
*Schemes may use different controlled vocabularies
*Schemes change their standards over time

Some of these problems are simply not fixable. As Karen Coyle says in "''Crosswalking Citation Metadata: The University of California's Experience,''"

<blockquote>"The more metadata experience we have, the more it becomes clear that metadata perfection is not attainable, and anyone who attempts it will be sorely disappointed.  When metadata is crosswalked between two or more unrelated sources, there will be data elements that cannot be reconciled in an ideal manner.  The key to a successful metadata crosswalk is intelligent flexibility.  It is essential to focus on the important goals and be willing to compromise in order to reach a practical conclusion to projects."<ref><u>in</u> "Metadata in Practice" Diane I. Hillmann and Elaine L. Westbrooks, eds., American Library Association, Chicago, 2004, p. 91.</ref></blockquote>

==Examples==

MARC to Dublin Core (Library of Congress) 
http://loc.gov/marc/marc2dc.html

Dublin Core to MARC21 (Library of Congress) 
http://www.loc.gov/marc/dccross.html

Dublin Core to UNIMARC (UKOLN)
http://www.ukoln.ac.uk/metadata/interoperability/dc_unimarc.html

TEI to and from MARC
http://purl.oclc.org/NET/teiinlibraries

FGDC to USMARC (Alexandria) 
http://www.alexandria.ucsb.edu/public-documents/metadata/fgdc2marc.html

ONIX to MARC21 (LC) 
http://www.loc.gov/marc/onix2marc.html

VRA to MARC (Indiana University) 
http://php.indiana.edu/%7Efryp/marcmap.html

Metadata Mappings (MIT Library)
http://web.archive.org/web/20080720134522/http://libraries.mit.edu/guides/subjects/metadata/mappings.html

Mapping Between Metadata formats (UKOLN) 
http://www.ukoln.ac.uk/metadata/interoperability/

International Metadata Standard Mappings (Academia Sinica) 
http://www.sinica.edu.tw/%7Emetadata/standard/mapping-foreign_eng.htm

JATS to MARC
http://webservices.itcs.umich.edu/mediawiki/jats/index.php/JATS-to-MARC_mapping

== See also ==
* [[Meta tag]]
* [[Metadata]]
* [[Database]]

==References==
 {{Reflist}}

==External links==
* [http://www.oclc.org/research/researchworks/schematrans/default.htm "Metadata Crosswalk Depository" (SchemaTrans)](OCLC)
* [http://www.ukoln.ac.uk/metadata/interoperability "Mapping Between Metadata Formats"] (UKOLN)
* [http://www.getty.edu/research/conducting_research/standards/intrometadata/path.html "Crosswalks the Path to Universal Access?"] (Getty)
* [http://www.dlib.org/dlib/june06/chan/06chan.html "Metadata Interoperability and Standardization - A Study of Methodology Part I"] (D-Lib)

[[Category:Data management]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata]]
[[Category:Technical communication]]
<=====doc_Id=====>:105
<=====title=====>:
British Oceanographic Data Centre
<=====text=====>:
{{Infobox organization
|name         = British Oceanographic Data Centre
|image        = Bodc logo.jpg
|image_border = 
|size         = 150px
|alt          = British Oceanographic Data Centre
|caption      = 
|map          =
|msize        = 
|malt         = 
|mcaption     = 
|abbreviation = 
|motto        = 
|formation    = 1969
|extinction   = 
|type         = 
|status       = 
|purpose      = 
|headquarters = 
|location     = [[Liverpool]], [[UK]]<br>
[[UK postcodes|L3 5DA]]
|region_served =
|membership   = 
|language     =
|leader_title = Head of BODC
|leader_name  = Dr Graham Allen
|main_organ   = 
|parent_organization = [[Natural Environment Research Council]] (NERC)
|affiliations = 
|num_staff    = approx. 500
|num_volunteers =
|budget       = 
|website      = {{URL|http://www.bodc.ac.uk/}}
|remarks      =
}}
The '''British Oceanographic Data Centre''' ('''BODC''') is a national facility for looking after and distributing [[data]] about the [[marine (ocean)|marine]] environment. BODC is the designated marine science data centre for the [[UK]] and part of the [[Natural Environment Research Council]] (NERC). The centre provides a resource for science, education and industry, as well as the general public. BODC is hosted by the [[National Oceanography Centre]] (NOC) — primarily at its facility in [[Liverpool]], with small number of its staff in [[Southampton]].

[[File:Bidston Observatory.jpg|250px|right|thumb|Bidston Observatory, home of BODC from 1975 to 2004.]]
[[File:Joseph Proudman Building.jpg|250px|right|thumb|Joseph Proudman Building, Liverpool.]]

== History ==
The origins of BODC go back to 1969 when NERC created the '''British Oceanographic Data Service''' ('''BODS'''). Located at the National Institute of [[Oceanography]], [[Wormley, Surrey|Wormley]] in [[Surrey]], its purpose was to: 
* Act as the UK's National Oceanographic Data Centre
* Participate in the international exchange of data as part of the [[Intergovernmental Oceanographic Commission]] (IOC) network of national data centres

In 1975 BODS was transferred to [[Bidston]] Observatory on the [[Wirral Peninsula|Wirral]], near Liverpool, as part of the newly formed Institute of Oceanographic Sciences. The following year BODS became the Marine Information and Advisory Service (MIAS)[http://www.soton.ac.uk/library/about/nol/mias.html]. Its primary activity was to manage the data collected from weather ships, [[oil rigs]] and [[Data Buoys|data buoys]].
The data banking component of MIAS was restructured to form BODC in April 1989. Its mission was to 'operate as a world-class data centre in support of UK marine science'. BODC pioneered a start to finish approach to marine data management. This involved:
* Assisting in the collection of data at sea
* Quality control of data
* Assembling the data for use by the scientists
* The publication of data sets on [[CD-ROM]]
In December 2004, BODC moved to the purpose-built [[Joseph Proudman]] Building on the campus of the [[University of Liverpool]]. A small number of its staff are based in the [[National Oceanography Centre]] (NOC), Southampton.

== Aims ==
* Work alongside scientists during marine research projects
* Provide quality control and archiving of oceanographic data
* Maintain an online source of information and improve public access to marine data
* Provide innovative marine data products

== National role ==
[[File:Current meter inventory.jpg|250px|right|thumb|BODC [[current meter]] data holdings from around the UK.]]
BODC is one of six designated data centres that manage NERC's environmental data and has a number of national roles and responsibilities:
* Performing data management for NERC-funded marine projects
* Maintaining and developing its archive of marine data, the '''National Oceanographic Database''' ('''NODB''')
* Managing, checking and archiving data from [[tide gauge]]s around the UK coast for the [[UK National Tide Gauge Network|National Tide Gauge Network]], which aims to obtain high quality [[tidal]] information and to provide warning of possible flooding of coastal areas around the British Isles. This  is part of the [[National Tidal and Sea Level Facility|National Tidal & Sea Level Facility]] (NTSLF)
* Hosting the Marine Environmental Data and Information Network ([http://www.oceannet.org/ MEDIN])
* Working in partnership with other NERC marine research centres:
** [[British Antarctic Survey]] (BAS)
** [[National Oceanography Centre]] (NOC), Liverpool, formerly [[Proudman Oceanographic Laboratory]] (POL)
** [[National Oceanography Centre]] (NOC), Southampton
** [[Plymouth Marine Laboratory]] (PML)
** [[Scottish Association for Marine Science]] (SAMS)
** [[Sea Mammal Research Unit]] (SMRU)

== International role ==
BODC's international roles and responsibilities include:
* Contributing to the [[International Council for the Exploration of the Sea]] (ICES) Marine Data Management
* Creating, maintaining and publishing the [[General Bathymetric Chart of the Oceans]] (GEBCO) Digital Atlas
* BODC is one of over 60 national oceanographic data centres that form part of the IOC [[International Oceanographic Data and Information Exchange]] (IODE)

==Projects and initiatives==
The following are a selection of the projects that BODC is or has been involved with:
:[[Image:RAPID mooring.JPG|250px|right|thumb|Servicing of a RAPID mooring.]]
* '''Atlantic Meridional Transect (AMT)'''
:The AMT programme [http://www.bodc.ac.uk/projects/uk/amt/] undertook a twice yearly [[transect]] between the UK and the [[Falkland Islands]] to study the factors determining the [[ecological]] and [[biogeochemical]] variability in the [[planktonic]] [[ecosystems]].
* '''Autosub Under Ice (AUI)'''
:The AUI programme [http://www.bodc.ac.uk/projects/uk/aui/] investigated the role of sub-ice shelf processes in the [[climate]] system. The marine environment beneath floating [[ice shelves]] was explored using Autosub, an [[Autonomous_underwater_vehicle|AUV]].
* '''Marine Productivity (MarProd)'''
:MarProd [http://www.bodc.ac.uk/projects/uk/marprod/] helped to develop coupled [[Computer simulation|models]] and observation systems for the [[pelagic]] ecosystem, with emphasis on the physical factors affecting [[zooplankton]] dynamics.
* '''Rapid Climate Change (RAPID)'''
:The RAPID programme [http://www.bodc.ac.uk/projects/uk/rapid/] aimed to improve understanding of the causes of sudden changes in the Earth's climate.
* '''Ocean Margin Exchange (OMEX)'''
:The OMEX project [http://www.bodc.ac.uk/projects/european/omex/] studied, measured and modelled the physical, chemical and biological processes and fluxes at the ocean margin - the interface between the open [[Atlantic ocean]] and the European [[continental shelf]].
* '''SeaDataNet'''
:[[SeaDataNet]] [http://www.bodc.ac.uk/projects/european/seadatanet/] aims to develop a [[standardised]], distributed system providing transparent access to marine data sets and data products from countries in and around [[Europe]].
*'''System of Industry Metocean data for the Offshore and Research Communities (SIMORC)'''
:SIMORC [http://www.bodc.ac.uk/projects/european/simorc/] aimed to create a central index and database of [[metocean]] data sets collected globally by the oil and gas industry.
*'''Vocabulary Server'''
:BODC operates the NERC Vocabulary Server Web Service [http://www.bodc.ac.uk/products/web_services/vocab/], which provides access to [[Controlled_vocabulary|controlled vocabularies]] of relevance to the scientific community.

== External links ==
* [http://www.bodc.ac.uk BODC homepage]
* [http://www.bodc.ac.uk/about/news_and_events/ BODC News and events]
* [http://www.nerc.ac.uk Natural Environment Research Council (NERC) homepage]
* [http://www.nerc.ac.uk/research/sites/data/ NERC Data centres]
* [http://ndg.nerc.ac.uk/discovery NERC Data Discovery Service]
* [http://www.ntslf.org/ National Tidal and Sea Level Facility (NTSLF)]

{{coord|53|24|27.5|N|2|58|8.2|W|type:landmark|display=title}}

[[Category:Oceanographic organizations]]
[[Category:Scientific organisations based in the United Kingdom]]
[[Category:Data management]]
[[Category:Oceanography]]
[[Category:Marine biology]]
[[Category:Marine geology]]
[[Category:Environmental science]]
[[Category:Environment of the United Kingdom]]
[[Category:Public bodies and task forces of the United Kingdom government]]
[[Category:1969 establishments in the United Kingdom]]
[[Category:Scientific organizations established in 1969]]
[[Category:Organisations based in Liverpool]]
<=====doc_Id=====>:108
<=====title=====>:
Data exchange
<=====text=====>:
{{more footnotes|date=February 2014}}
'''Data exchange''' is the process of taking [[data]] structured under a ''source'' [[Database schema|schema]] and transforming it into data structured under a ''target'' schema, so that the target data is an accurate representation of the source data.<ref>A. Doan, A. Halevy, and Z. Ives. "Principles of data integration", Morgan Kaufmann, 2012 pp. 276</ref> Data exchange allows data to be [[cross-platform|shared between]] different [[computer program]]s. It is similar to the related concept of [[data integration]] except that data is actually restructured (with possible loss of content) in data exchange. There may be no way to transform an [[Instance (computer science)|instance]] given all of the constraints. Conversely, there may be numerous ways to transform the instance (possibly infinitely many), in which case a "best" choice of solutions has to be identified and justified.

== Single-domain data exchange ==

Often there are a few dozen different source and target schema (proprietary data formats) in some specific domain.
Often people develop an '''exchange format''' or '''interchange format''' for some single domain, and then write a few dozen different routines to (indirectly) translate each and every source schema to each and every target schema by using the interchange format as an intermediate step.
That requires a lot less work than writing and debugging the hundreds of different routines that would be required to directly translate each and every source schema directly to each and every target schema.
(For example,
[[Standard Interchange Format]] for geospatial data,
[[Data Interchange Format]] for spreadsheet data,
[[GPS eXchange Format]] or [[Keyhole Markup Language]] for indicating GPS coordinates on the globe,
[[Quicken Interchange Format]] for financial data,
[[GDSII]] for integrated circuit layout,
etc.){{Citation needed|date=September 2016}}

== Data exchange languages == <!-- redirect target, if you change this, fix the redirect, too! -->
{{merge to|Modeling language|date=May 2016}}
A data exchange language{{citation needed|date=May 2016}} is a language that is domain-independent and can be used for any kind of data. Its semantic expression capabilities and qualities are largely determined by comparison with the capabilities of natural languages. The term is also applied to any [[file format]] that can be read by more than one program, including proprietary formats such as [[Microsoft Office]] documents. However, a file format is not a real language as it lacks a grammar and vocabulary.

Practice has shown that certain types of [[formal language]]s are better suited for this task than others, since their specification is driven by a formal process instead of a particular software implementation needs. For example, [[XML]] is a [[markup language]] that was designed to enable the creation of dialects (the definition of domain-specific sublanguages) and a popular choice now in particular on the internet. However, it does not  contain domain specific dictionaries or fact types. Beneficial to a reliable data exchange is the availability of standard dictionaries-taxonomies and tools libraries such as [[parser]]s, schema [[validator]]s and transformation tools.{{Citation needed|date=September 2016}}

=== Popular languages used for data exchange ===
The following is a partial list of popular generic languages used for data exchange in multiple domains.

<!-- this currently is very rough and ad-hoc - feel free to extend and change it! -->
<!-- Please verify definitions for the column headers of the table! -->
{| class="wikitable sortable" style="font-size: 85%; text-align: center; width: auto;"
!
! Schemas
! Flexible
! Semantic verification
! Dictionary
! Information Model
! Synonyms and homonyms
! Dialecting
! Web standard
! Transformations
! Lightweight
! Human readable
! Compatibility
|-
| {{rh}}|[[Resource Description Framework|RDF]]
| {{yes}}{{Ref label|feat-rdf|1}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{partial}}
| Subset of [[Semantic web]]
|-
| {{rh}}|[[XML]]
| {{yes}}{{Ref label|feat-schema|1}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| subset of [[SGML]], [[HTML]]
|-
| {{rh}}|[[Atom (file format)|Atom]]
| {{yes}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| [[XML]] dialect
|-
| {{rh}}|[[JSON]]
| {{no}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| subset of [[YAML]]
|-
| {{rh}}|[[YAML]]
| {{no}}{{Ref label|feat-ext|2}}
| {{unk}}
| {{unk}}
| {{unk}}
| {{no}}
| {{unk}}
| {{no}}
| {{no}}
| {{no}}{{Ref label|feat-ext|2}}
| {{yes}}
| {{yes}}{{Ref label|feat-yaml-readable|3}}
| superset of [[JSON]]
|-
| {{rh}}|[[REBOL]]
| {{yes}}{{Ref label|feat-rebol-parse|6}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}{{Ref label|feat-rebol-parse|6}}
| {{yes}}
| {{yes}}{{Ref label|feat-rebol-readable|4}}
| 
|-
| {{rh}}|[[Gellish]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}{{Ref label|feat-gellish-dict|7}}
| {{no}}
| {{yes}}
| {{yes}}
| ISO
| {{no}}
| {{yes}}
| {{partial}}{{Ref label|feat-gellish-readable|5}}
| SQL, RDF/XML, OWL
|}

'''Nomenclature'''
* Schemas - Whether the language definition is available in a computer interpretable form.
* Flexible - Whether the language enables extension of the semantic expression capabilities without modifying the schema.
* Semantic verification - Whether the language definition enables semantic verification of the correctness of expressions in the language.
* Dictionary-Taxonomy - Whether the language includes a dictionary and a taxonomy (subtype-supertype hierarchy) of concepts with inheritance.
* Synonyms and homonyms - Whether the language includes and supports the use of synonyms and homonyms in the expressions.
* Dialecting - Whether the language definition is available in multiple natural languages or dialects.
* Web or ISO standard - Organization that endorsed the language as a standard.
* Transformations - Whether the language includes a translation to other standards.
* Lightweight - Whether a lightweight version is available, in addition to a full version.
* Human readable - Whether expressions in the language are [[human-readable]]—readable by humans without training.{{Citation needed|date=September 2016}}
* Compatibility - Which other tools are possible or required when using the language.{{Citation needed|date=September 2016}}

'''Notes:'''

# {{note|feat-rdf}} RDF is a schema flexible language.
# {{note|feat-schema}} The schema of XML contains a very limited grammar and vocabulary.
# {{note|feat-ext}} Available as extension.
# {{note|feat-yaml-readable}} in the default format, not the compact syntax.
# {{note|feat-rebol-readable}} the syntax is fairly simple (the language was designed to be human readable); the dialects may require domain knowledge.
# {{note|feat-gellish-readable}} the standardized fact types are denoted by standardized English phrases, which interpretation and use needs some training.
# {{note|feat-rebol-parse}} the [[REBOL#parse|Parse dialect]] is used to specify, validate, and transform dialects.
# {{note|feat-gellish-dict}} the English version includes a Gellish English Dictionary-Taxonomy that also includes standardized fact types (= kinds of relations).

=== XML for data exchange ===
The popularity of [[XML]] for data exchange on the [[World Wide Web]] has several reasons. First of all, it is closely related to the preexisting standards [[Standard Generalized Markup Language]] (SGML) and [[Hypertext Markup Language]] (HTML), and as such a parser written to support these two languages can be easily extended to support XML as well. For example, [[XHTML]] has been defined as a format that is formal XML, but understood correctly by most (if not all) HTML parsers. This led to quick adoption of XML support in web browsers and the toolchains used for generating web pages.{{Citation needed|date=September 2016}}

=== YAML for data exchange ===
[[YAML]] is a language that was designed to be human-readable (and as such to be easy to edit with any standard text editor). Its notion often is similar to [[reStructuredText]] or a Wiki syntax, who also try to be readable both by humans and computers. YAML 1.2 also includes a shorthand notion that is compatible with JSON, and as such any JSON document is also valid YAML; this however does not hold the other way.{{Citation needed|date=September 2016}}

=== REBOL for data exchange ===

[[REBOL]] is a language that was designed to be human-readable and easy to edit using any standard text editor. To achieve that it uses a simple free-form syntax with minimal punctuation, and a rich set of datatypes. REBOL datatypes like URLs, e-mails, date and time values, tuples, strings, tags, etc. respect the common standards. REBOL is designed to not need any additional meta-language, being designed in a metacircular fashion. The metacircularity of the language is the reason why e.g. the Parse dialect used (not exclusively) for definitions and transformations of REBOL dialects is also itself a dialect of REBOL. REBOL was used as a source of inspiration by the designer of JSON.{{Citation needed|date=September 2016}}

=== Gellish for data exchange ===
[[Gellish English]] is a formalized subset of natural English, which includes a simple grammar and a large extensible [[English dictionary|English Dictionary-Taxonomy]] that defines the general and domain specific terminology (terms for concepts), whereas the concepts are arranged in a subtype-supertype hierarchy (a Taxonomy), which supports inheritance of knowledge and requirements. The Dictionary-Taxonomy also includes standardized fact types (also called relation types). The terms and relation types together can be used to create and interpret expressions of facts, knowledge, requirements and other information. Gellish can be used in combination with [[SQL]], [[RDF/XML]], [[Web Ontology Language|OWL]] and various other meta-languages. The Gellish standard is being adopted as ISO 15926-11.{{Citation needed|date=September 2016}}

== See also ==
* [[Atom (file format)]]
* [[Lightweight markup language]]
* [[RSS]]

== References ==

{{reflist}}

{{refbegin}}

*R. Fagin, P. Kolaitis, R. Miller, and L. Popa. "Data exchange: semantics and query answering." Theoretical Computer Science, 336(1):89–124, 2005.
*P. Kolaitis. "Schema mappings, data exchange, and metadata management." Proceedings of the twenty- fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 61–75, 2005
{{refend}}

{{Data Exchange}}

[[Category:Data management]]
<=====doc_Id=====>:111
<=====title=====>:
UI data binding
<=====text=====>:
{{Refimprove|date=February 2015}}

'''UI data binding''' is a [[Design pattern (computer science)|software design pattern]] to simplify development of [[GUI]] applications.  UI [[data binding]] binds UI elements to an application [[domain model]]. Most frameworks employ the [[Observer pattern]] as the underlying binding mechanism.  To work efficiently, UI data binding has to address [[Data validation|input validation]] and data type mapping.

A ''bound control'' is a [[GUI widget|widget]] whose value is tied or [[data binding|bound]] to a field in a [[recordset]] (e.g., a [[column (database)|column]] in a [[row (database)|row]] of a [[table (database)|table]]).  Changes made to data within the control are automatically saved to the database when the control's exit [[event trigger]]s.

== Data binding frameworks and tools ==

=== [[Embarcadero Delphi|Delphi]] ===
* [[DSharp (data binding)|DSharp]] 3rd party Data Binding tool{{cn|date=December 2016}}
* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd party Visual Data Binding tool

=== Java ===
* [[JFace]] Data Binding
* [[JavaFX]] Property<ref>https://docs.oracle.com/javafx/2/binding/jfxpub-binding.htm</ref>

=== .NET ===
* [[Windows Forms]] data binding overview
* [[Windows Presentation Foundation|WPF]] data binding overview
* Unity 3D data binding framework (available in modifications for NGUI, iGUI and EZGUI libraries){{cn|date=December 2016}}

=== JavaScript ===
* [[AngularJS]]
* [[Backbone.js]]
* [[Ember.js]]
* Datum.js<ref>{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}</ref>
* [[knockout.js]]
* [[Meteor (web framework)|Meteor]], via its ''Blaze'' live update engine<ref>{{cite web|title=Meteor Blaze|url=https://www.meteor.com/blaze|quote=Meteor Blaze is a powerful library for creating live-updating user interfaces. Blaze fulfills the same purpose as Angular, Backbone, Ember, React, Polymer, or Knockout, but is much easier to use. We built it because we thought that other libraries made user interface programming unnecessarily difficult and confusing.}}</ref>
* [[OpenUI5]]
* [[React (JavaScript library)|React]]

==See also==
*[[Data binding]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Software design patterns]]


{{compu-prog-stub}}
{{database-stub}}
<=====doc_Id=====>:114
<=====title=====>:
Metadata controller
<=====text=====>:
'''Metadata controller''' (or MDC) is a [[storage area network]] (SAN) technology for managing [[file locking]], space allocation and data access authorization.
This is needed when several clients are given block level access to the same disk volume, [[Computer data storage|data storage]] sharing.

The abstract for the patent describing this technology can be read [http://www.freepatentsonline.com/7448077.html here]

[[Category:Data management]]
[[Category:Telecommunications engineering]]
[[Category:Storage area networks]]
[[Category:Local area networks]]

{{compu-storage-stub}}
<=====doc_Id=====>:117
<=====title=====>:
Write–read conflict
<=====text=====>:
In [[computer science]], in the field of [[database]]s, '''write–read conflict''', also known as '''reading uncommitted data''', is a computational anomaly associated with interleaved execution of transactions.

Given a schedule S

:<math>S = \begin{bmatrix}
T1 & T2 \\
R(A) &  \\
W(A) & \\
 & R(A) \\
 & W(A)\\
 & R(B) \\
 & W(B) \\
 & Com. \\
R(B) & \\
W(B) & \\
Com. & \end{bmatrix}</math>

T2 could read a database object A, modified by T1 which hasn't committed. This is a '''''dirty read'''''.

T1 may write some value into A which makes the database inconsistent.  It is possible that interleaved execution can expose this inconsistency and lead to inconsistent final database state, violating [[ACID]] rules.

[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T2 out from performing a Read/Write on A.  Note however that [[Strict two-phase locking|Strict 2PL]] can have a number of drawbacks, such as the possibility of [[deadlock]]s.

== See also ==

* [[Concurrency control]]
* [[Read–write conflict]]
* [[Write–write conflict]]

==References==
{{reflist}}
{{Unreferenced|date=August 2009}}

{{DEFAULTSORT:Write-Read Conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:120
<=====title=====>:
Mobile content management system
<=====text=====>:
A '''Mobile Content Management system''' ('''MCMs''') is a type of [[content management system]] (CMS) capable of storing and delivering content and services to mobile devices, such as mobile phones, smart phones, and PDAs. Mobile content management systems may be discrete systems, or may exist as features, modules or add-ons of larger content management systems capable of multi-channel content delivery. Mobile content delivery has unique, specific constraints including widely variable device capacities, small screen size, limited wireless bandwidth, small storage capacity, and comparatively weak device processors.<ref>[http://www.insight-corp.com/%5CExecSummaries%5Ccontent08ExecSum.pdf Content Management for Wireless Networks, 2008-2013 - Insight Research Report]</ref>

Demand for mobile content management increased as mobile devices became increasingly ubiquitous and sophisticated. MCMS technology initially focused on the business to consumer (B2C) mobile market place with ringtones, games, text-messaging, news, and other related content. Since, mobile content management systems have also taken root in business-to-business (B2B) and business-to-employee (B2E) situations, allowing companies to provide more timely information and functionality to business partners and mobile workforces in an increasingly efficient manner. A 2008 estimate put global revenue for mobile content management at US$8 billion.<ref>[http://www.wirelessweek.com/Content-Management-Systems-Mobile-Embrace.aspx Content Management Systems’ Mobile Embrace, By Evan Koblentz, WirelessWeek, August 28, 2008]</ref>

==Key features==

===Multi-channel content delivery===
Multi-channel content delivery capabilities allow users to manage a central content repository while simultaneously delivering that content to mobile devices such as mobile phones, smartphones, tablets and other mobile devices. Content can be stored in a raw format (such as Microsoft Word, Excel, PowerPoint, PDF, Text, HTML etc.) to which device-specific presentation styles can be applied.<ref>[http://www.apoorv.info/2007/05/26/content-management-for-mobile-delivery/ Content Management for Mobile Delivery, Posted by Apoorv, PCM.Blog, May 26, 2007]</ref>

===Content access control===
Access control includes authorization, authentication, access approval to each content.  In many cases the access control also includes download control, wipe-out for specific user, time specific access.  For the authentication, MCM shall have basic authentication which has user ID and password.  For higher security many MCM supports IP authentication and mobile device authentication.

===Specialized templating system===
While traditional web content management systems handle templates for only a handful of web browsers, mobile CMS templates must be adapted to the very wide range of target devices with different capacities and limitations. There are two approaches to adapting templates: multi-client and multi-site. The multi-client approach makes it possible to see all versions of a site at the same domain (e.g. sitename.com), and templates are presented based on the device client used for viewing. The multi-site approach displays the mobile site on a targeted sub-domain (e.g. mobile.sitename.com).

===Location-based content delivery===
Location-based content delivery provides targeted content, such as information, advertisements, maps, directions, and news, to mobile devices based on current physical location. Currently, GPS (global positioning system)  navigation systems offer the most popular [[location-based service]]s. Navigation systems are specialized systems, but incorporating mobile phone functionality makes greater exploitation of location-aware content delivery possible.

==See also==
*[[Mobile Web]]
*[[Content management]]
*[[Web content management system]]
*[[Enterprise content management]]
*[[Apache Mobile Filter]]

==References==
<references/>

[[Category:Content management systems]]
[[Category:Mobile Web]]
[[Category:Data management]]
<=====doc_Id=====>:123
<=====title=====>:
Semantic warehousing
<=====text=====>:
{{Multiple issues|
{{unreferenced|date=November 2008}}
{{confusing|date=August 2009}}
{{cleanup-rewrite|date=August 2009}}
}}

In [[data management]],  '''semantic warehousing''' is a methodology of digitalized text data using similar functions to [[Data warehousing]] (DW), such as ETL([[Extract, transform, load]]), ODS([[Operational data store]]), and MODEL. [[Value (computer science)|Key value]] operation is less useful for the digitalized text. Semantic warehousing is different from DW in that semantic information base from text(semantic) data.

Semantic warehousing is different from search engine in that semantic information base from text data is stored in the database.([[DBMS]])

Though data is most important word in computing era, it can not explain human knowledge well yet.
Data(numeric data) is key element of computing systems for certain organization (especially companies, enterprises), but no performance oriented organization needs something to gather and use knowledge or human feeling.
Semantic warehousing will be equally or more important than data warehousing in the future.

==Definition==
Semantic warehousing is a conceptual and functional term meaning to gather from a source, semantically defining and providing information from digitalized text type data.

==Background==

Data warehousing (DW) is popular these days. Gathering data from systems that generate transactions, data warehouses become a base of [[information]]. Key of data warehouse is a model (called [[datamart]]) and that model is made up of dimensions(key) and measures(value). Users get information from the models by doing certain operations. [[Online analytical processing]] (OLAP) is most the important operation for the users to get information from the DW models. Handling dimensions with pivoting, drilling, slice & dice operations users get numeric values like sales amounts, growth rates, etc.
Various areas of this world defined and appeared on the World Wide Web(Internet), eager to present their contents in a semantic way. 
Briefly speaking semantic warehousing has datawarehousing boby and search head and ontology features.

Data warehousing contributed to companies' business values and lots of solutions and tools are commercially successful. Analysis of internal data delivers a certain level of business values, on the contrary to this Semantic warehousing environment has not yet matured. Capacity of social data is increasing rapidly and various efforts of finding value from that data are made widely known as Big data, etc. Semantic warehousing can be the mainstream of treat data and intelligence of social world in the future though it is defined with other keywords.

At the Big data era, semantic processing is going to become major IT process. Semantic warehousing is digital infra of Intelligence.

== Practices ==

'''▣ Medical area (Clinical Information)'''

Some hospital implement semantic warehousing for [[clinic]]al information (SWCI). Medical information is now knowledge network level. [[UMLS]] define semantic knowledge network of medical language. Currently medical information stored in database and not fully used for clinic. Semantic warehousing is next stage of digitalized medical information.

SWCI is a name of conceptual system of clinical information.<br />
Named by Juhan Kim (SNUH, [[Seoul National University Hospital]]) and Bohyon Hwang, YongChan Keum in 2008.

Defined architecture on SWCI ;<br />
1. Semantic-oriented cleansing<br />
2. Semantic-oriented meta management<br />
3. Clinical(Medical) knowledge basement<br />
4. Semantic-oriented user intelligence

'''▣ Intelligence Area'''

At the point of Big data usage, intelligence reporting can be valuable results.

1. Source information<br />
2. Manage intelligence & Semantic data<br />
3. Intelligence service & use

http://www.globalintelligence.kr/gibigdata/

== Connected area ==

- [[Big data]] <br />
- [[Semantic web]] <br />
- [[Ontology]] <br />
- [[Knowledge]] <br />
- [[Medical]] and [[healthcare]] : EMR [[Electronic medical record|(Electronic Medical Record)]], EHR [[Electronic health record|(Electronic Health Record)]]<br />
- [[Data warehouse]] <br />
- AI ([[artificial intelligence]])

== References ==
*[http://www.snubi.org/ '''BI''' Laboratory of Seoul National University Hospital]
*Smith, Barry Kumar, Anand and Schulze-Kremer, Steffen (2004) [http://ontology.buffalo.edu/medo/UMLS_SN.pdf Revising the UMLS Semantic Network], in M. Fieschi, et al. (eds.), Medinfo 2004, Amsterdam: IOS Press, 1700.
* Foundations of Data Warehouse Quality :
  Data Quality article mentioning that semantically rich DW.
  http://www.cs.brown.edu/courses/cs227/Papers/Projects/iq97_dwq.pdf
* An Integrative and Uniform Model for Metadata Management in Data Warehousing Environment.
  Semantic metadata and technical metadata.
  http://ftp.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-19/paper12.pdf

* Effective Query Expansion using Condensed UMLS Metathesaurus for Medical Information Retrieval
http://www.e-hir.org/journal/view.html?uid=201&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=

* A Study of Effective Unified Medical Language System Concept Indexing in Radiology Reports
http://www.e-hir.org/journal/view.html?uid=226&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=

* Developing a Reference Terminology Model for Health Care Using an Object-Oriented Approach
http://www.e-hir.org/journal/view.html?uid=311&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=

* UMLS(Unified Medical Language System)의 증상용어와 국내의무기록에서 사용되는 증상용어와의 비교연구
http://www.e-hir.org/journal/view.html?uid=922&start=&sort=&scale=&key=all&oper=&key_word=UMLS&year1=&year2=&Vol=&Num=&PG=&book=&mod=vol&sflag=&sub_box=Y&aut_box=Y&sos_box=&pub_box=Y&key_box=&abs_box=&year=

[[Category:Data management]]
<=====doc_Id=====>:126
<=====title=====>:
Vocabulary-based transformation
<=====text=====>:
{{unreferenced|date=July 2013}}
In [[metadata]], a '''vocabulary-based transformation (VBT)''' is a transformation aided by the use of a [[semantic equivalence]] statements within a [[controlled vocabulary]].

Many organizations today require communication between two or more computers.  Although many standards exist to exchange data between computers such as [[HTML]] or [[email]], there is still much structured information that needs to be exchanged between computers that is not standardized.  The process of mapping one source of data into another is often a slow and labor-intensive process.

VBT is a possible way to avoid much of the time and cost of manual data mapping using traditional [[Extract, transform, load]] technologies.

== History ==

The term ''vocabulary-based transformation'' was first defined by Roy Shulte of the [[Gartner Group]] around May 2003 and appeared in annual "[[hype cycle|hype]]-cycle" for [[data integration|integration]].

== Application ==
VBT allows computer systems integrators to more automatically "look up" the definitions of data elements in a centralized [[data dictionary]] and use that definition and the equivalent mappings to transform that data element into a foreign [[namespace]].

The [[Web Ontology Language]] (OWL) language also support three [[semantic equivalence]] statements.

== Companies or products ==
* [[IONA Technologies]]
* [http://liaison.com/products/transform Contivo and Delta] by [http://liaison.com/ Liaison Technologies]
* enLeague Systems
* ItemField
* Unicorn Solutions
* Vitria Technology
* Zonar

== See also ==
* [[metadata]]
* [[Controlled vocabulary]]
* [[Data dictionary]]
* [[Semantic spectrum]]
* [[Semantic equivalence]]
* [[XSLT]]
* [[Enterprise Application Integration]]

==External links==
* [http://www.gartner.com/6_help/glossary/GlossaryV.jsp Gartner Glossary of Terms] Gartner definition Vocabulary-based transformation
* [http://www.sun.com/service/openwork/analyst/Gartner_Hype_Cycle.pdf Gartner Hype Cycle 2003]

{{DEFAULTSORT:Vocabulary-Based Transformation}}
[[Category:Data management]]
<=====doc_Id=====>:129
<=====title=====>:
Enterprise information system
<=====text=====>:
An '''enterprise information system''' ('''EIS''') is any kind of [[information system]] which improves the functions of an enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of [[data]] and capable of supporting some large and possibly complex [[organization]] or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.<ref name="eidvtai" />

The word ''enterprise'' can have various connotations. Frequently the term is used only to refer to very large organizations such as multi-national companies or public-sector organizations. However, the term may be used to mean virtually anything, by virtue of it having become the latest corporate-speak [[buzzword]].{{Citation needed|date=June 2016}}

==Purpose==
Enterprise information systems provide a technology platform that enables organizations to [[Enterprise integration|integrate]] and coordinate their [[business processes]] on a robust foundation. An EIS is currently used in conjunction with [[customer relationship management]] and [[supply chain management]] to automate business processes.<ref name="eidvtai" />  An enterprise information system provides a single system that is central to the organization that ensures information can be shared across all functional levels and management [[hierarchies]].

An EIS can be used to increase business [[productivity]] and reduce service cycles, [[product development]] cycles and marketing life cycles.<ref name="eidvtai">{{cite book |title=Enterprise Information Systems: Contemporary Trends and Issues |last=Olson |first=David L. |author2=Subodh Kesharwani |year=2010 |publisher=World Scientific |isbn=9814273163 |pages=2, 13–16 |url=https://books.google.com/books?id=-AwDAp7Fe2UC |accessdate=20 August 2013}}</ref>  It may be used to amalgamate existing applications. Other outcomes include higher [[operational efficiency]] and cost savings.<ref name="eidvtai" />

Financial value is not usually a direct outcome from the implementation of an enterprise information system.<ref name="eiscmta">{{cite book |title=Enterprise Information Systems: Concepts, Methodologies, Tools and Applications |author=Information Resources Management Association |year=2010 |publisher=Idea Group Inc |isbn=1616928530 |pages=38, 43 |url=https://books.google.com/books?id=hpc6-SfS2scC |accessdate=20 August 2013}}</ref>

==Design stage==
At the design stage the main characteristic of EIS efficiency evaluation is the probability of timely delivery of various messages such as command, service, etc.<ref>{{cite journal |title=ОЦЕНКА ХАРАКТЕРИСТИК ФУНКЦИОНИРОВАНИЯ КОРПОРАТИВНЫХ ИНФОРМАЦИОННЫХ СИСТЕМ С НЕОДНОРОДНОЙ НАГРУЗКОЙ |trans-title=Efficiency Evaluation of Enterprise Information Systems with Non-uniform Load |language=ru |url=http://ntv.ifmo.ru/en/article/13881/ocenka_harakteristik_funkcionirovaniya_korporativnyhinformacionnyh_sistem_s_neodnorodnoy_nagruzkoy.htm |author1=Kalinin I.V. |author2=Maharevs E. |author3=Muravyeva-Vitkovskaya L.A. |journal=Scientific and Technical Journal of Information Technologies, Mechanics and Optics |volume=15 |issue=5 |pages=863–868 |year=2015 |doi=10.17586/2226-1494-2015-15-5-863-868}}</ref>

==Information systems==
{{main|Information systems}}
Enterprise systems create a standard [[data structure]] and are invaluable in eliminating the problem of information fragmentation caused by multiple information systems within an organization.  An EIS differentiates itself from [[legacy system]]s in that it self-transactional, self-helping and adaptable to general and specialist conditions.<ref name="eidvtai" />  Unlike an enterprise information system, legacy systems are limited to department wide communications.<ref name="eiscmta" />

A typical enterprise information system would be housed in one or more [[data center]]s, would run [[enterprise software]], and could include applications that typically cross organizational borders such as [[content management systems]].

==See also==
{{Portal|Computing|Business}}
*[[Executive information system]]
*[[Management information system]]
*[[Enterprise planning systems]]
*[[Enterprise software]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Enterprise modelling]]
[[Category:Website management]]
<=====doc_Id=====>:132
<=====title=====>:
Modular concurrency control
<=====text=====>:
#REDIRECT [[Global concurrency control]]

Modular concurrency control

[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]
<=====doc_Id=====>:135
<=====title=====>:
Operational data store
<=====text=====>:
An '''operational data store''' (or "'''ODS'''") is a [[database]] designed to [[data integration|integrate data]] from multiple sources for additional operations on the data. Unlike a master data store, the data is not passed back to [[operational system]]s. It may be passed for further operations and to the [[data warehouse]] for reporting. 

Because the [[data]] originate from multiple sources, the integration often involves [[data cleaning|cleaning]], resolving redundancy and checking against [[business rule]]s for [[data integrity|integrity]].  An ODS is usually designed to contain low-level or atomic (indivisible) data (such as transactions and prices) with limited history that is captured "real time" or "near real time" as opposed to the much greater volumes of data stored in the data warehouse generally on a less-frequent basis.

==General Use==
The general purpose of an ODS is to integrate data from disparate source systems in a single structure, using [[data integration]] technologies like [[Data Virtualization|data virtualization]], [[Federated database system|data federation]], or [[Extract, transform, load|extract, transform, and load]]. This will allow operational access to the data for operational reporting, [[Master Data|master data or reference data management]].

An ODS is not a replacement or substitute for a [[data warehouse]] but in turn could become a source.

==See also==
* Some examples of ODS Architecture Patterns can be found in the article [[Architectural pattern (computer science)#Examples|Architecture Patterns]].

==Publications==
* {{cite book |last1=Inmon |first1=William |author1-link=Bill Inmon |title=Building the Operational Data Store |edition=2nd |location=New York |publisher=[[John Wiley & Sons]] |year=1999 |isbn=0-471-32888-X}}

==External links==
*[[Architectural pattern (computer science)#Examples|ODS Architecture Patterns (EA Reference Architecture)]]
* [http://www.dmreview.com/issues/19980701/469-1.html Bill Inmon Information Management article on ODS]
* [http://www.information-management.com/issues/20000101/1749-1.html Bill Inmon Information Management article on the five classes of ODS]
* [http://www.intelsols.com/documents/Imhoff_10-02.pdf Claudia Imhoff Information Management article on ODS] PDF

{{Data warehouse}}

== See also ==
{{Wikipedia books|Enterprise Architecture}}
* [[Enterprise architecture]]

{{DEFAULTSORT:Operational Data Store}}
[[Category:Data management]]
[[Category:Data warehousing]]

{{database-stub}}
<=====doc_Id=====>:138
<=====title=====>:
Data conditioning
<=====text=====>:
{{Multiple issues|
{{POV|date=August 2015}}
{{One source|date=August 2015}}
{{Notability|date=August 2015}}
}}

'''Data conditioning''' is the use of data management and optimization techniques which result in the intelligent routing, optimization and protection of data for storage or [[data movement]] in a computer system.  Data conditioning features enable enterprise and cloud [[data center]]s to dramatically improve system utilization and increase application performance lowering both [[capital expenditures]] and [[operating cost]]s.

Data conditioning technologies delivered through a Data Conditioning Platform optimize data as it moves through a computer’s I/O ([[Input/Output]]) path or I/O bus—the data path between the main processor complex and storage subsystems.  The functions of a Data Conditioning Platform typically reside on a storage controller add-in card inserted into the [[PCI-e]] slots of a server.  This enables easy integration of new features in a server or a whole data center.

Data conditioning features delivered via a Data Conditioning Platform are designed to simplify system integration, and minimize implementation risks associated with deploying new technologies by ensuring seamless compatibility with all leading server and  storage hardware, operating systems and applications, and meeting all current commercial/off-the-shelf (COTS) standards.  By delivering optimization features via a Data Conditioning Platform, data center managers can improve system efficiency and reduce cost with minimal disruption and avoid the need to modify existing applications or operating systems, and leverage existing hardware systems.

== Summary ==

Data conditioning builds on existing data storage functionality delivered in the I/O path including [[RAID]] (Redundant Arrays of Inexpensive Disks), intelligent I/O-based [http://www.adaptec.com/en-us/_common/greenpower?refURL=/greenpower/ power management], and [[Solid-state drive|SSD]] (Solid-State Drive) performance caching techniques.  Data conditioning is enabled both by advanced [[ASIC]] controller technology and intelligent software.  New data conditioning capabilities can be designed into and delivered via storage controllers in the I/O path  or to achieve the data center’s technical and business goals.

Data Conditioning strategies can also be applied to improving server and storage utilization and for better managing a wide range of hardware and system-level capabilities.

== Background and Purpose ==

Data conditioning principles can be applied to any demanding computing environment to create significant cost, performance and system utilization efficiencies, and are typically deployed by data center managers, system integrators, and storage and server OEMs seeking to optimize hardware and software utilization, simplified, non-intrusive technology integration, and minimal risks and performance hits traditionally associated with incorporating new data center technologies.

== References ==

[http://www.adaptec/maxIQ Adaptec MaxIQ]

[[Category:Data management]]
<=====doc_Id=====>:141
<=====title=====>:
Concurrency control
<=====text=====>:
{{pp-pc1}}
In [[information technology]] and [[computer science]],  especially in the fields of [[computer programming]], [[operating systems]], [[multiprocessor]]s, and [[database]]s, '''concurrency control''' ensures that correct results for [[Concurrent computing|concurrent]] operations are generated, while getting those results as quickly as possible.

Computer systems, both [[software]] and [[computer hardware|hardware]], consist of modules, or components. Each component is designed to operate correctly, i.e., to obey or to meet certain consistency rules. When components that operate concurrently interact by messaging or by sharing accessed data (in [[Computer memory|memory]] or [[Computer data storage|storage]]), a certain component's consistency may be violated by another component. The general area of concurrency control provides rules, methods, design methodologies, and [[Scientific theory|theories]] to maintain the consistency of components operating concurrently while interacting, and thus the consistency and correctness of the whole system. Introducing concurrency control into a system means applying operation constraints which typically result in some performance reduction. Operation consistency and correctness should be achieved with as good as possible efficiency, without reducing performance below reasonable levels. Concurrency control can require significant additional complexity and overhead in a [[concurrent algorithm]] compared to the simpler [[sequential algorithm]].

For example, a failure in concurrency control can result in [[data corruption]] from [[Torn data-access operation|torn read or write operations]].

==Concurrency control in databases==
'''Comments:'''
# This section is applicable to all transactional systems, i.e., to all systems that use ''[[database transaction]]s'' (''atomic transactions''; e.g., transactional objects in [[Systems management]] and in networks of [[smartphone]]s which typically implement private, dedicated database systems), not only general-purpose [[database management system]]s (DBMSs).
# DBMSs need to deal also with concurrency control issues not typical just to database transactions but rather to operating systems in general. These issues (e.g., see ''[[Concurrency control#Concurrency control in operating systems|Concurrency control in operating systems]]'' below) are out of the scope of this section.

Concurrency control in [[Database management system]]s (DBMS; e.g., [[#Bern87|Bernstein et al. 1987]], [[#Weikum01|Weikum and Vossen 2001]]), other [[database transaction|transactional]] objects, and related distributed applications (e.g., [[Grid computing]] and [[Cloud computing]]) ensures that ''[[database transaction]]s'' are performed [[Concurrency (computer science)|concurrently]] without violating the [[data integrity]] of the respective [[database]]s. Thus concurrency control is an essential element for correctness in any system where two database transactions or more, executed with time overlap, can access the same data, e.g., virtually in any general-purpose database system. Consequently, a vast body of related research has been accumulated since database systems emerged in the early 1970s. A well established concurrency control [[Scientific theory|theory]] for database systems is outlined in the references mentioned above: [[Serializability|serializability theory]], which allows to effectively design and analyze concurrency control methods and mechanisms. An alternative theory for concurrency control of atomic transactions over [[abstract data type]]s is presented in ([[#Lynch1993|Lynch et al. 1993]]), and not utilized below. This theory is more refined, complex, with a wider scope, and has been less utilized in the Database literature than the classical theory above. Each theory has its pros and cons, emphasis and [[insight]]. To some extent they are complementary, and their merging may be useful.

To ensure correctness, a DBMS usually guarantees that only ''[[Serializability|serializable]]'' transaction [[Schedule (computer science)|schedule]]s are generated, unless ''serializability'' is [[Serializability#Relaxing serializability|intentionally relaxed]] to increase performance, but only in cases where application correctness is not harmed. For maintaining correctness in cases of failed (aborted) transactions (which can always happen for many reasons) schedules also need to have the ''[[Serializability#Correctness - recoverability|recoverability]]'' (from abort) property. A DBMS also guarantees that no effect of ''committed'' transactions is lost, and no effect of ''aborted'' ([[Rollback (data management)|rolled back]]) transactions remains in the related database. Overall transaction characterization is usually summarized by the [[ACID]] rules below. As databases have become [[Distributed database|distributed]], or needed to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990, and [[Cloud computing]] currently), the effective distribution of concurrency control mechanisms has received special attention.

===Database transaction and the ACID rules===
{{main|Database transaction|ACID}}
The concept of a ''database transaction'' (or ''atomic transaction'') has evolved in order to enable both a well understood database system behavior in a faulty environment where crashes can happen any time, and ''recovery'' from a crash to a well understood database state. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands). Every database transaction obeys the following rules (by support in the database system; i.e., a database system is designed to guarantee them for the transactions it runs):
*'''[[Atomicity (database systems)|Atomicity]]''' - Either the effects of all or none of its operations remain ("all or nothing" semantics) when a [[database transaction|transaction]] is completed (''committed'' or ''aborted'' respectively). In other words, to the outside world a committed transaction appears (by its effects on the database) to be indivisible (atomic), and an aborted transaction does not affect the database at all.
*'''[[Consistency (database systems)|Consistency]]''' - Every transaction must leave the database in a consistent (correct) state, i.e., maintain the predetermined integrity rules of the database (constraints upon and among the database's objects). A transaction must transform a database from one consistent state to another consistent state (however, it is the responsibility of the transaction's programmer to make sure that the transaction itself is correct, i.e., performs correctly what it intends to perform (from the application's point of view) while the predefined integrity rules are enforced by the DBMS). Thus since a database can be normally changed only by transactions, all the database's states are consistent.
*'''[[Isolation (database systems)|Isolation]]''' - Transactions cannot interfere with each other (as an end result of their executions). Moreover, usually (depending on concurrency control method) the effects of an incomplete transaction are not even visible to another transaction. Providing isolation is the main goal of concurrency control.
*'''[[Durability (database systems)|Durability]]''' - Effects of successful (committed) transactions must persist through [[Crash (computing)|crash]]es (typically by recording the transaction's effects and its commit event in a [[non-volatile memory]]).

The concept of atomic transaction has been extended during the years to what has become [[Business transaction management|Business transactions]] which actually implement types of [[Workflow]] and are not atomic. However also such enhanced transactions typically utilize atomic transactions as components.

===Why is concurrency control needed?===
If transactions are executed ''serially'', i.e., sequentially with no overlap in time, no transaction concurrency exists. However, if concurrent transactions with interleaving operations are allowed in an uncontrolled manner, some unexpected, undesirable result may occur, such as:
# The lost update problem: A second transaction writes a second value of a data-item (datum) on top of a first value written by a first concurrent transaction, and the first value is lost to other transactions running concurrently which need, by their precedence, to read the first value. The transactions that have read the wrong value end with incorrect results.
# The dirty read problem: Transactions read a value written by a transaction that has been later aborted. This value disappears from the database upon abort, and should not have been read by any transaction ("dirty read"). The reading transactions end with incorrect results.
# The incorrect summary problem: While one transaction takes a summary over the values of all the instances of a repeated data-item, a second transaction updates some instances of that data-item. The resulting summary does not reflect a correct result for any (usually needed for correctness) precedence order between the two transactions (if one is executed before the other), but rather some random result, depending on the timing of the updates, and whether certain update results have been included in the summary or not.

Most high-performance transactional systems need to run transactions concurrently to meet their performance requirements. Thus, without concurrency control such systems can neither provide correct results nor maintain their databases consistent.

===Concurrency control mechanisms===

====Categories====
The main categories of concurrency control mechanisms are:
* '''[[Optimistic concurrency control|Optimistic]]''' - Delay the checking of whether a transaction meets the isolation and other integrity rules (e.g., [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]]) until its end, without blocking any of its (read, write) operations ("...and be optimistic about the rules being met..."), and then abort a transaction to prevent the violation, if the desired rules are to be violated upon its commit. An aborted transaction is immediately restarted and re-executed, which incurs an obvious overhead (versus executing it to the end only once). If not too many transactions are aborted, then being optimistic is usually a good strategy.
* '''Pessimistic''' - Block an operation of a transaction, if it may cause violation of the rules, until the possibility of violation disappears. Blocking operations is typically involved with performance reduction.
*'''Semi-optimistic''' - Block operations in some situations,  if they may cause violation of some rules, and do not block in other situations while delaying rules checking (if needed) to transaction's end, as done with optimistic.

Different categories provide different performance, i.e., different average transaction completion rates (''throughput''), depending on transaction types mix, computing level of parallelism, and other factors. If selection and knowledge about trade-offs are available, then category and method should be chosen to provide the highest performance.

The mutual blocking between two transactions (where each one blocks the other) or more results in a [[deadlock]], where the transactions involved are stalled and cannot reach completion. Most non-optimistic mechanisms (with blocking) are prone to deadlocks which are resolved by an intentional abort of a stalled transaction (which releases the other transactions in that deadlock), and its immediate restart and re-execution. The likelihood of a deadlock is typically low.

Blocking, deadlocks, and aborts all result in performance reduction, and hence the trade-offs between the categories.

====Methods====
Many methods for concurrency control exist. Most of them can be implemented within either main category above. The major methods,<ref name=Bern2009>[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (page 145)</ref> which have each many variants, and in some cases may overlap or be combined, are:
#Locking (e.g., '''[[Two-phase locking]]''' - 2PL) - Controlling access to data by [[Lock (computer science)|locks]] assigned to the data. Access of a transaction to a data item (database object) locked by another transaction may be blocked (depending on lock type and access operation type) until lock release.
#'''Serialization [[Serializability#Testing conflict serializability|graph checking]]''' (also called Serializability, or Conflict, or Precedence graph checking) - Checking for [[Cycle (graph theory)|cycles]] in the schedule's [[Directed graph|graph]] and breaking them by aborts.
#'''[[Timestamp-based concurrency control|Timestamp ordering]]''' (TO) - Assigning timestamps to transactions, and controlling or checking access to data by timestamp order.
#'''[[Commitment ordering]]''' (or Commit ordering; CO) - Controlling or checking transactions' chronological order of commit events to be compatible with their respective [[Serializability#Testing conflict serializability|precedence order]].

Other major concurrency control types that are utilized in conjunction with the methods above include:

* '''[[Multiversion concurrency control]]''' (MVCC) - Increasing concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object) depending on scheduling method.
* '''[[Index locking|Index concurrency control]]''' - Synchronizing access operations to [[Index (database)|index]]es, rather than to user data. Specialized methods provide substantial performance gains.
* '''Private workspace model''' ('''Deferred update''') - Each transaction maintains a private workspace for its accessed data, and its changed data become visible outside the transaction only upon its commit (e.g., [[#Weikum01|Weikum and Vossen 2001]]). This model provides a different concurrency control behavior with benefits in many cases.

The most common mechanism type in database systems since their early days in the 1970s has been ''[[Two-phase locking|Strong strict Two-phase locking]]'' (SS2PL; also called ''Rigorous scheduling'' or ''Rigorous 2PL'') which is a special case (variant) of both [[Two-phase locking]] (2PL) and [[Commitment ordering]] (CO). It is pessimistic. In spite of its long name (for historical reasons) the idea of the '''SS2PL''' mechanism is simple: "Release all locks applied by a transaction only after the transaction has ended." SS2PL (or Rigorousness) is also the name of the set of all schedules that can be generated by this mechanism, i.e., these are SS2PL (or Rigorous) schedules, have the SS2PL (or Rigorousness) property.

===Major goals of concurrency control mechanisms===
Concurrency control mechanisms firstly need to operate correctly, i.e., to maintain each transaction's integrity rules (as related to concurrency; application-specific integrity rule are out of the scope here) while transactions are running concurrently, and thus the integrity of the entire transactional system. Correctness needs to be achieved with as good performance as possible. In addition, increasingly a need exists to operate effectively while transactions are [[Distributed transaction|distributed]] over [[Process (computing)|processes]], [[computer]]s, and [[computer network]]s. Other subjects that may affect concurrency control are [[Data recovery|recovery]] and [[Replication (computer science)|replication]].

====Correctness====

=====Serializability=====
{{Main|Serializability}}

For correctness, a common major goal of most concurrency control mechanisms is generating [[Schedule (computer science)|schedule]]s with the ''[[Serializability]]'' property. Without serializability undesirable phenomena may occur, e.g., money may disappear from accounts, or be generated from nowhere. '''Serializability''' of a schedule means equivalence (in the resulting database values) to some ''serial'' schedule with the same transactions (i.e., in which transactions are sequential with no overlap in time, and thus completely isolated from each other: No concurrent access by any two transactions to the same data is possible). Serializability is considered the highest level of [[isolation (database systems)|isolation]] among [[database transaction]]s, and the major correctness criterion for concurrent transactions. In some cases compromised, [[serializability#Relaxing serializability|relaxed forms]] of serializability are allowed for better performance (e.g., the popular ''[[Snapshot isolation]]'' mechanism) or to meet [[availability]] requirements in highly distributed systems (see ''[[Eventual consistency]]''), but only if application's correctness is not violated by the relaxation (e.g., no relaxation is allowed for [[money]] transactions, since by relaxation money can disappear, or appear from nowhere).

Almost all implemented concurrency control mechanisms achieve serializability by providing ''[[Serializability#View and conflict serializability|Conflict serializablity]]'', a broad special case of serializability (i.e., it covers, enables most serializable schedules, and does not impose significant additional delay-causing constraints) which can be implemented efficiently.

=====Recoverability=====
:See ''[[Serializability#Correctness - recoverability|Recoverability]]'' in ''[[Serializability]]''

'''Comment:''' While in the general area of systems the term "recoverability" may refer to the ability of a system to recover from failure or from an incorrect/forbidden state, within concurrency control of database systems this term has received a specific meaning.

Concurrency control typically also ensures the ''[[Serializability#Correctness - recoverability|Recoverability]]'' property of schedules for maintaining correctness in cases of aborted transactions (which can always happen for many reasons). '''Recoverability''' (from abort) means that no committed transaction in a schedule has read data written by an aborted transaction. Such data disappear from the database (upon the abort) and are parts of an incorrect database state. Reading such data violates the consistency rule of ACID. Unlike Serializability, Recoverability cannot be compromised, relaxed at any case, since any relaxation results in quick database integrity violation upon aborts. The major methods listed above provide serializability mechanisms. None of them in its general form automatically provides recoverability, and special considerations and mechanism enhancements are needed to support recoverability. A commonly utilized special case of recoverability is ''[[Schedule (computer science)#Strict|Strictness]]'', which allows efficient database recovery from failure (but excludes optimistic implementations; e.g., [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]] cannot have an optimistic implementation, but [[The History of Commitment Ordering#Semi-optimistic database scheduler|has semi-optimistic ones]]).

'''Comment:''' Note that the ''Recoverability'' property is needed even if no database failure occurs and no database ''recovery'' from failure is needed. It is rather needed to correctly automatically handle transaction aborts, which may be unrelated to database failure and recovery from it.

====Distribution====
With the fast technological development of computing the difference between local and distributed computing over low latency [[Computer network|networks]] or [[Bus (computing)|buses]] is blurring. Thus the quite effective utilization of local techniques in such distributed environments is common, e.g., in [[computer cluster]]s and [[multi-core processor]]s. However the local techniques have their limitations and use multi-processes (or threads) supported by multi-processors (or multi-cores) to scale. This often turns transactions into distributed ones, if they themselves need to span multi-processes. In these cases most local concurrency control techniques do not scale well.

=====Distributed serializability and Commitment ordering=====
{{POV-section|Commitment ordering|date=November 2011}}
:See ''[[Serializability#Distributed serializability|Distributed serializability]]'' in ''[[Serializability]]''
{{Main|Global serializability}} {{Main|Commitment ordering}}
As database systems have become [[Distributed database|distributed]], or started to cooperate in distributed environments (e.g., [[Federated database]]s in the early 1990s, and nowadays [[Grid computing]], [[Cloud computing]], and networks with [[smartphone]]s), some transactions have become distributed. A [[distributed transaction]] means that the transaction spans [[Process (computing)|processes]], and may span [[computer]]s and geographical sites. This generates a need in effective [[distributed concurrency control]] mechanisms. Achieving the Serializability property of a distributed system's schedule (see ''[[Serializability#Distributed serializability|Distributed serializability]]'' and ''[[Global serializability]]'' (''Modular serializability'')) effectively poses special challenges typically not met by most of the regular serializability mechanisms, originally designed to operate locally. This is especially due to a need in costly distribution of concurrency control information amid communication and computer [[latency (engineering)|latency]]. The only known general effective technique for distribution is Commitment ordering, which was disclosed publicly in 1991 (after being [[patent]]ed). '''[[Commitment ordering]]''' (Commit ordering, CO; [[#Raz92|Raz 1992]]) means that transactions' chronological order of commit events is kept compatible with their respective [[Serializability#Testing conflict serializability|precedence order]]. CO does not require the distribution of concurrency control information and provides a general effective solution ([[Reliability engineering|reliable]], high-performance, and [[Scalability|scalable]]) for both distributed and global serializability, also in a heterogeneous environment with database systems (or other transactional objects) with different (any) concurrency control mechanisms.<ref name=Bern2009/> CO is indifferent to which mechanism is utilized, since it does not interfere with any transaction operation scheduling (which most mechanisms control), and only determines the order of commit events. Thus, CO enables the efficient distribution of all other mechanisms, and also the distribution of a mix of different (any) local mechanisms, for achieving distributed and global serializability. The existence of such a solution has been considered "unlikely" until 1991, and by many experts also later, due to misunderstanding of the [[Commitment ordering#Summary|CO solution]] (see [[Global serializability#Quotations|Quotations]] in ''[[Global serializability]]''). An important side-benefit of CO is [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|automatic distributed deadlock resolution]]. Contrary to CO, virtually all other techniques (when not combined with CO) are prone to [[Deadlock#Distributed deadlock|distributed deadlocks]] (also called global deadlocks) which need special handling. CO is also the name of the resulting schedule property: A schedule has the CO property if the chronological order of its transactions' commit events is compatible with the respective transactions' [[Serializability#Testing conflict serializability|precedence (partial) order]].

[[Two-phase locking|SS2PL]] mentioned above is a variant (special case) of CO and thus also effective to achieve distributed and global serializability. It also provides automatic distributed deadlock resolution (a fact overlooked in the research literature even after CO's publication), as well as Strictness and thus Recoverability. Possessing these desired properties together with known efficient locking based implementations explains SS2PL's popularity. SS2PL has been utilized to efficiently achieve Distributed and Global serializability since the 1980, and has become the [[de facto standard]] for it. However, SS2PL is blocking and constraining (pessimistic), and with the proliferation of distribution and utilization of systems different from traditional database systems (e.g., as in [[Cloud computing]]), less constraining types of CO (e.g., [[Commitment ordering#Distributed optimistic CO (DOCO)|Optimistic CO]]) may be needed for better performance.

'''Comments:'''
# The ''Distributed conflict serializability'' property in its general form is difficult to achieve efficiently, but it is achieved efficiently via its special case ''Distributed CO'': Each local component (e.g., a local DBMS) needs both to provide some form of CO, and enforce a special ''vote ordering strategy'' for the ''[[Two-phase commit protocol]]'' (2PC: utilized to commit [[distributed transaction]]s). Differently from the general Distributed CO, ''Distributed SS2PL'' [[Commitment ordering#Strong strict two phase locking (SS2PL)|exists automatically when all local components are SS2PL based]] (in each component CO exists, implied, and the vote ordering strategy is now met automatically). This fact has been known and utilized since the 1980s (i.e., that SS2PL exists globally, without knowing about CO) for efficient Distributed SS2PL, which implies Distributed serializability and strictness (e.g., see [[#Raz92|Raz 1992]], page 293; it is also implied in [[#Bern87|Bernstein et al. 1987]], page 78). Less constrained Distributed serializability and strictness can be efficiently achieved by Distributed [[Commitment ordering#Strict CO (SCO)|Strict CO (SCO)]], or by a mix of SS2PL based and SCO based local components.
# About the references and Commitment ordering: ([[#Bern87|Bernstein et al. 1987]]) was published before the discovery of CO in 1990. The CO schedule property is called ''[[The History of Commitment Ordering#Dynamic atomicity|Dynamic atomicity]]'' in ([[#Lynch1993|Lynch et al. 1993]], page 201). CO is described in ([[#Weikum2001|Weikum and Vossen 2001]], pages 102, 700), but the description is partial and misses [[Commitment ordering#Summary|CO's essence]]. ([[#Raz92|Raz 1992]]) was the first refereed and accepted for publication article about CO algorithms (however, publications about an equivalent Dynamic atomicity property can be traced to 1988). Other [[Commitment ordering#References|CO articles]] followed. (Bernstein and Newcomer 2009)<ref name=Bern2009/> note CO as one of the four major concurrency control methods, and CO's ability to provide interoperability among other methods.

=====Distributed recoverability=====
Unlike Serializability, ''Distributed recoverability'' and ''Distributed strictness'' can be achieved efficiently in a straightforward way, similarly to the way Distributed CO is achieved: In each database system they have to be applied locally, and employ a vote ordering strategy for the [[Two-phase commit protocol]] (2PC; [[#Raz92|Raz 1992]], page 307).

As has been mentioned above, Distributed [[Two-phase locking|SS2PL]], including Distributed strictness (recoverability) and Distributed [[commitment ordering]] (serializability), automatically employs the needed vote ordering strategy, and is achieved (globally) when employed locally in each (local) database system (as has been known and utilized for many years; as a matter of fact locality is defined by the boundary of a 2PC participant ([[#Raz92|Raz 1992]]) ).

====Other major subjects of attention====
The design of concurrency control mechanisms is often influenced by the following subjects:

=====Recovery=====
{{Main|Data recovery}}
All systems are prone to failures, and handling ''[[Data recovery|recovery]]'' from failure is a must. The properties of the generated schedules, which are dictated by the concurrency control mechanism, may affect the effectiveness and efficiency of recovery. For example, the Strictness property (mentioned in the section [[Concurrency control#Recoverability|Recoverability]] above) is often desirable for an efficient recovery.

=====Replication=====
{{Main|Replication (computer science)}}
For high availability database objects are often ''[[Replication (computer science)|replicated]]''. Updates of replicas of a same database object need to be kept synchronized. This may affect the way concurrency control is done (e.g., Gray et al. 1996<ref name=Gray1996>{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 |author2=Helland, P. |author3=[[Patrick O'Neil|O'Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | conference = The dangers of replication and a solution
 | title = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173–182
 | conference-url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}</ref>).

=== See also ===
* [[Schedule (computer science)|Schedule]]
* [[Isolation (computer science)]]
* [[Distributed concurrency control]]
* [[Global concurrency control]]

===References===
*<cite id=Bern87>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''] (free PDF download), Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5 </cite>
*<cite id=Weikum01>[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, ISBN 1-55860-508-8 </cite>
*<cite id=Lynch1993>[[Nancy Lynch]], Michael Merritt, William Weihl, Alan Fekete (1993): [http://www.elsevier.com/wps/find/bookdescription.cws_home/680521/description#description ''Atomic Transactions in Concurrent and Distributed Systems ''], Morgan Kauffman (Elsevier), August 1993, ISBN 978-1-55860-104-8, ISBN 1-55860-104-X </cite>
*<cite id=Raz92>[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html  "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment."]  ([http://www.vldb.org/conf/1992/P292.PDF  PDF]), ''Proceedings of the Eighteenth International Conference on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) </cite>

===Footnotes===
{{Reflist}}

== Concurrency control in operating systems ==
{{Expand section|date=December 2010}}
[[Computer multitasking|Multitasking]] operating systems, especially [[real-time operating system]]s, need to maintain the illusion that all tasks running on top of them are all running at the same time, even though only one or a few tasks really are running at any given moment due to the limitations of the hardware the operating system is running on. Such multitasking is fairly simple when all tasks are independent from each other. However, when several tasks try to use the same resource, or when tasks try to share information, it can lead to confusion and inconsistency. The task of [[concurrent computing]] is to solve that problem. Some solutions involve "locks" similar to the locks used in databases, but they risk causing problems of their own such as [[deadlock]]. Other solutions are [[Non-blocking algorithm]]s and [[Read-copy-update]].

=== See also ===
* [[Linearizability]]
* [[Mutual exclusion]]
* [[Semaphore (programming)]]
* [[Lock (computer science)]]
* [[Software transactional memory]]
* [[Transactional Synchronization Extensions]]

===References===
*  Andrew S. Tanenbaum, Albert S Woodhull (2006): ''Operating Systems Design and Implementation, 3rd Edition'', [[Prentice Hall]], ISBN 0-13-142938-8
* {{cite book | last = Silberschatz | first = Avi |author2=Galvin, Peter |author3=Gagne, Greg | title = Operating Systems Concepts, 8th edition | publisher = [[John Wiley & Sons]] | year = 2008 | isbn = 0-470-12872-0 }}

{{Databases}}

{{DEFAULTSORT:Concurrency Control}}
[[Category:Concurrency control| ]]
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
<=====doc_Id=====>:144
<=====title=====>:
Database transaction
<=====text=====>:
{{Refimprove|date=August 2010}}

A '''transaction''' symbolizes a unit of work performed within a [[database management system]] (or similar system) against a database, and treated in a coherent and reliable way independent of other transactions. A transaction generally represents any change in database. Transactions in a database environment have two main purposes:

# To provide reliable units of work that allow correct recovery from failures and keep a database consistent even in cases of system failure, when execution stops (completely or partially) and many operations upon a database remain uncompleted, with unclear status.
# To provide isolation between programs accessing a database concurrently. If this isolation is not provided, the programs' outcomes are possibly erroneous.

A database transaction, by definition, must be [[Atomicity (database systems)|atomic]], [[Consistency (database systems)|consistent]], [[Isolation (database systems)|isolated]] and [[Durability (database systems)|durable]].<ref>[http://msdn.microsoft.com/en-us/library/aa366402(VS.85).aspx A transaction is a group of operations that are atomic, consistent, isolated, and durable (ACID).]</ref> Database practitioners often refer to these properties of database transactions using the acronym [[ACID]].

Transactions provide an "all-or-nothing" proposition, stating that each work-unit performed in a database must either complete in its entirety or have no effect whatsoever. Further, the system must isolate each transaction from other transactions, results must conform to existing constraints in the database, and transactions that complete successfully must get written to durable storage.

==Purpose==
[[Database]]s and other data stores which treat the [[data integrity|integrity]] of data as paramount often include the ability to handle transactions to maintain the integrity of data. A single transaction consists of one or more independent units of work, each reading and/or writing information to a database or other data store. When this happens it is often important to ensure that all such processing leaves the database or data store in a consistent state.

Examples from [[Double-entry bookkeeping system|double-entry accounting systems]] often illustrate the concept of transactions. In double-entry accounting every debit requires the recording of an associated credit. If one writes a check for $100 to buy groceries, a transactional double-entry accounting system must record the following two entries to cover the single transaction:

# Debit $100 to Groceries Expense Account
# Credit $100 to Checking Account

A transactional system would make both entries pass or both entries would fail. By treating the recording of multiple entries as an atomic transactional unit of work the system maintains the integrity of the data recorded. In other words, nobody ends up with a situation in which a debit is recorded but no associated credit is recorded, or vice versa.

==Transactional databases==
A '''transactional database''' is a [[DBMS]] where write transactions on the database are able to be rolled back if they are not completed properly (e.g. due to power or connectivity loss).

Most {{As of|2008|alt=modern}}  [[relational database management system]]s fall into the category of databases that support transactions.

In a database system a transaction might consist of one or more data-manipulation statements and queries, each reading and/or writing information in the database. Users of [[database system]]s consider [[Data consistency|consistency]] and [[data integrity|integrity]] of data as highly important. A simple transaction is usually issued to the database system in a language like [[Structured Query Language|SQL]] wrapped in a transaction, using a pattern similar to the following:

# Begin the transaction
# Execute a set of data manipulations and/or queries
# If no errors occur then commit the transaction and end it
# If errors occur then rollback the transaction and end it

If no errors occurred during the execution of the transaction then the system commits the transaction. A transaction commit operation applies all data manipulations within the scope of the transaction and persists the results to the database. If an error occurs during the transaction, or if the user specifies a [[Rollback (data management)|rollback]] operation, the data manipulations within the transaction are not persisted to the database. In no case can a partial transaction be committed to the database since that would leave the database in an inconsistent state.

Internally, multi-user databases store and process transactions, often by using a transaction [[identifier|ID]] or XID.

There are multiple varying ways for transactions to be implemented other than the simple way documented above. [[Nested transaction]]s, for example, are transactions which contain statements within them that start new transactions (i.e. sub-transactions). ''Multi-level transactions'' are a variant of nested transactions where the sub-transactions take place at different levels of a layered system architecture (e.g., with one operation at the database-engine level, one operation at the operating-system level) <ref>Beeri, C., Bernstein, P.A., and Goodman, N. A model for concurrency in nested transactions systems. Journal of the ACM, 36(1):230-269, 1989</ref> Another type of transaction is the [[compensating transaction]].

===In SQL===
Transactions are available in most SQL database implementations, though with varying levels of robustness. (MySQL, for example, does not support transactions in the [[MyISAM]] storage engine, which was its default storage engine before version 5.5.)

A transaction is typically started using the command <code>BEGIN</code> (although the SQL standard specifies <code>START TRANSACTION</code>). When the system processes a <code>[[Commit (SQL)|COMMIT]]</code> statement, the transaction ends with successful completion.  A <code>[[Rollback (data management)|ROLLBACK]]</code> statement can also end the transaction, undoing any work performed since <code>BEGIN TRANSACTION</code>. If [[autocommit]] was disabled using <code>START TRANSACTION</code>, autocommit will also be re-enabled at the transaction's end.

One can set the [[Isolation (database systems)|isolation level]] for individual transactional operations as well as globally. At the READ COMMITTED level, the result of any work done after a transaction has commenced, but before it has ended, will remain invisible to other database-users until it has ended. At the lowest level (READ UNCOMMITTED), which may occasionally be used to ensure high concurrency, such changes will be visible.

==Object databases==
Relational databases traditionally comprise tables with fixed size fields and thus records. Object databases comprise variable sized [[Binary large object|blobs]] (possibly incorporating a [[mime-type]] or [[Serializable (databases)|serialized]]). The fundamental similarity though is the start and the [[Commit (data management)|commit]] or [[Rollback (data management)|rollback]].

After starting a transaction, database records or objects are locked, either read-only or read-write. Actual reads and writes can then occur. Once the user (and application) is happy, any changes are committed or rolled-back [[Atomicity (database systems)|atomically]], such that at the end of the transaction there is no [[Consistency (database systems)|inconsistency]].

==Distributed transactions==
Database systems implement [[distributed transaction]]s as transactions against multiple applications or hosts. A distributed transaction enforces the ACID properties over multiple systems or data stores, and might include systems such as databases, file systems, messaging systems, and other applications. In a distributed transaction a coordinating service ensures that all parts of the transaction are applied to all relevant systems. As with database and other transactions, if any part of the transaction fails, the entire transaction is rolled back across all affected systems.

==Transactional filesystems==
The [[Namesys]] [[Reiser4]] filesystem for [[Linux]]<ref>[http://namesys.com/v4/v4.html#committing namesys.com<!-- Bot generated title -->]</ref> supports transactions, and as of [[Microsoft]] [[Windows Vista]], the Microsoft [[NTFS]] filesystem<ref>{{cite web|url=http://msdn.microsoft.com/library/default.asp?url=/library/en-us/fileio/fs/portal.asp|title=MSDN Library|publisher=|accessdate=16 October 2014}} {{dead link|date=May 2014}}</ref> supports [[distributed transaction]]s across networks.

==See also==
* [[Concurrency control]]

==References==
{{reflist}}

==Further reading==
* <cite id=Bern2009>[[Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4 </cite>
* Gerhard Weikum, Gottfried Vossen (2001), ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, ISBN 1-55860-508-8

==External links==
* [[c2:TransactionProcessing]]
* https://docs.oracle.com/database/121/CNCPT/transact.htm#CNCPT016
* https://docs.oracle.com/cd/B28359_01/server.111/b28318/transact.htm

{{Databases}}

{{DEFAULTSORT:Database Transaction}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:147
<=====title=====>:
Category:Structured storage
<=====text=====>:
{{Cat main|Structured storage}}

[[Category:Data management]]
<=====doc_Id=====>:150
<=====title=====>:
Archive site
<=====text=====>:
{{refimprove|date=January 2016}}

In [[web archiving]], an '''archive site''' is a [[website]] that stores information on webpages from the past for anyone to view.

==Common techniques==
Two common techniques for archiving web sites are using a [[web crawler]] or soliciting user submissions:

# '''Using a [[web crawler]]''': By using a web crawler (e.g., the [[Internet Archive]]) the service will not depend on an active community for its content, and thereby can build a larger database faster. However, web crawlers are only able to index and archive information the public has chosen to post to the Internet, or that is available to be crawled, as web site developers and system administrators have the ability to block web crawlers from accessing [certain] web pages (using a [[Robots Exclusion Standard|robots.txt]]).
# '''User submissions''': While it can be difficult to start user submissions services due to potentially low rates of user submission, this system can yield some of the best results. By crawling web pages one is only able to obtain the information the public has chosen to post online; however, potential content providers may not bother to post certain information, assuming no one would be interested in it, because they lack a proper venue in which to post it, or because of copyright concerns.<ref>{{cite news|url=http://dlib.org/dlib/march12/niu/03niu1.html|journal=D-Lib Magazine| date=March–April 2012 |
 volume =18| number =3/4| title=An Overview of Web Archiving |author=Jinfang Niu,  University of South Florida|doi=10.1045/march2012-niu1}}</ref> However, users who see someone wants their information may be more apt to submit it.

==Examples==

===Google Groups===
On February 12, 2001, [[Google]] acquired the [[usenet]] discussion group archives from [[Deja.com]] and turned it into their [[Google Groups]] service.<ref>{{cite web |title=Google Acquires Usenet Discussion Service and Significant Assets from Deja.com |work= |date=February 12, 2001 |url=https://googlepress.blogspot.com/2001/02/google-acquires-usenet-discussion.html }}</ref> They allow users to search old discussions with Google's search technology, while still allowing users to post to the [[mailing list]]s.

===Internet Archive===
The [[Internet Archive]] is building a compendium of websites and [[digital media]]. Starting in 1996, the Archive has been employing a web crawler to build up their database. It is one of the best known archive sites.

===NBCUniversal Archives===
[[NBCUniversal Archives]] offer access to exclusive content from [[NBCUniversal]] and its subsidiaries. Their NBCUniversal Archives website provides easy viewing of past and recent news clips, and it is a prime example of a news archive.<ref>[http://www.nbcuniversalarchives.com/nbcuni/home.do NBCUniversal Archives]</ref>

===Nextpoint===
[[Nextpoint]] offers an automated [[Cloud computing|cloud]]-based, [[Software as a service|SaaS]] for marketing, compliance, and litigation related needs including electronic discovery.

===PANDORA Archive===
PANDORA ([[Pandora Archive]]), founded in 1996 by the National Library of [[Australia]], stands for Preserving and Accessing Networked Documentary Resources of Australia, which encapsulates their mission. They provide a long-term catalog of select online publications and web sites authored by Australians or that are of an Australian topic. They employ their PANDAS (PANDORA Digital Archiving System) when building their catalog.

===textfiles.com===
[[textfiles.com]] is a large library of old text files maintained by [[Jason Scott Sadofsky]]. Its mission is to archive the old documents that had floated around the [[bulletin board systems]] (BBS) of his youth and to document other people's experiences on the bulletin board systems.

==See also==
* [[Internet Archive]]
* [[Pandora Archive]]
* [[WebCite]]
* [[Web archiving]]

== References ==
{{reflist}}

{{DEFAULTSORT:Archive Site}}
[[Category:Data management]]
[[Category:Online archives]]
[[Category:Web archiving initiatives]]
<=====doc_Id=====>:153
<=====title=====>:
Linear medium
<=====text=====>:
{{Unreferenced|date=December 2009}}
A '''linear medium''' is any medium which is intended to be written to or accessed in a [[linear]] fashion, literally meaning ''in a line''. 

This means that the information is written to or read from the medium in a given order, so for example a book containing a [[novel]] is intended to be read from front to back, beginning to end, and is therefore a linear medium. It may be written in the same way, but would not necessarily need to be, to be considered a linear medium. 
A book containing an [[encyclopedia]] however is a non-linear medium, as it is not necessary for the articles to be accessed (or written) in any particular order. Even though both non-linear and linear mediums have perimeters to which they are restricted, linear mediums have a set path of how to get from point A to point B, whereas non-linear mediums do not. 

Examples in technology are a pre-recorded [[videocassette]] which is usually accessed one item after another, compared with a pre-recorded [[DVD]] which can be accessed in any order.

==Types of linear medium==
* [[Scroll]]
* [[Magnetic tape data storage]]
* [[Paper tape]]
* [[Photographic film]]
* [[Novel|story book]]s
* [[Compact cassette]]s

==See also==
*[[Sequential access]] 
*[[Random access]]

{{DEFAULTSORT:Linear Medium}}
[[Category:Data management]]
<=====doc_Id=====>:156
<=====title=====>:
Reference table
<=====text=====>:
{{Unreferenced|auto=yes|date=December 2009}}

A '''reference table''' (or table of reference) may mean a set of references that an author may have cited or gained inspiration from whilst writing an article, similar to a [[bibliography]].

It can also mean an [[Table (information)|information table]] that is used as a quick and easy reference for things that are difficult to remember such as comparing [[Imperial unit|imperial]] with [[SI|metric]] measurements. This kind of data is known as [[reference data]].

In the context of [[database design]] a reference table is a table into which an [[enumeration|enumerated]] set of possible values of a certain field data type is divested. For example, in a [[relational model|relational database model]] of a warehouse the entity 'Item' may have a field called 'status' with a predefined set of values such as 'sold', 'reserved', 'out of stock'. In a purely designed database these values would be divested into an extra entity or Reference Table called 'status' in order to achieve [[database normalisation]]. The entity 'status' in this case has no true representative in the real world but rather would an exceptional case where the attribute of a certain database entity is divested into its own table. The advantage of doing this is that internal functionality and optional conditions within the database and the software which utilizes it are easier to modify and extend on that particular aspect.  Establishing an enterprise-wide view of reference tables is called [[master data management]].

{{DEFAULTSORT:Reference Table}}
[[Category:Data management]]

{{Publish-stub}}
<=====doc_Id=====>:159
<=====title=====>:
Nonlinear medium
<=====text=====>:
{{Distinguish|Non-linear media}}
{{Unreferenced|date=December 2009}}
A '''nonlinear medium''' is one which is intended to be accessed in a nonlinear fashion. It is the opposite of a [[Linear_medium|Linear Medium]]. 

Examples include:
* a [[hard drive]]
* a [[newspaper]]
* a [[phone book]]
* a [[dictionary]]

==See also==
*[[nonlinear]]
*[[linear medium]]
*[[Random access]]

{{DEFAULTSORT:Nonlinear Medium}}
[[Category:Data management]]


{{Tech-stub}}
<=====doc_Id=====>:162
<=====title=====>:
Point-in-time recovery
<=====text=====>:
{{Redirect|PITR|other uses|Pitr (disambiguation){{!}}Pitr}}
{{Unreferenced stub|auto=yes|date=December 2009}}

'''Point-in-time recovery''' ('''PITR''') in the context of [[computer]]s involves systems whereby an administrator can restore or recover a set of data or a particular setting from a time in the past. Note for example [[Windows XP]]'s capability to restore operating-system settings from a past date (before data corruption occurred, for example).  [[Time Machine (OS X)|Time Machine]] for Mac OS X provides another example of point-in-time recovery.

Once PITR logging starts for a PITR-capable [[database]], a [[database administrator]] can restore that database from [[backup]]s to the state that it had at any time since.

==External links==
* [http://blog.ganneff.de/blog/2008/02/15/postgresql-continuous-archivin.html PostgreSQL Continuous Archiving and Point-In-Time Recovery (PITR) blog/article]
* [http://dev.mysql.com/doc/refman/5.5/en/point-in-time-recovery.html MySQL 5.5 Point in Time Recovery]

{{DEFAULTSORT:Point-In-Time Recovery}}
[[Category:Data management]]


{{Compu-stub}}
<=====doc_Id=====>:165
<=====title=====>:
Automated tiered storage
<=====text=====>:
'''Automated tiered storage''' (also '''automated storage tiering''') is the automated progression or demotion of data across different tiers (types) of storage devices and media. The movement of data takes place in an automated way with the help of a software or embedded firmware and is assigned to the related media according to performance and capacity requirements. More advanced implementations include the ability to define rules and policies that dictate if and when data can be moved between the tiers, and in many cases provides the ability to pin data to tiers permanently or for specific periods of time. Implementations vary, but are classed into two broad categories: pure software based implementations that run on general purpose processors supporting most forms of general purpose storage media and embedded automated tiered storage controlled by firmware as part of a closed embedded storage system such as a SAN disk array. Software Defined Storage architectures commonly include a component of tiered storage as part of their primary functions.

In the most general definition, Automated Tiered Storage is a form of Hierarchical Storage Management. However, the term automated tiered storage has emerged to accommodate newer forms of real-time performance optimized [[data migration]] driven by the proliferation of solid state disks and storage class memory. Furthermore, where traditional HSM systems act on files and move data between storage tiers in a batch, scheduled like fashion, automated storage tiered systems are capable of operating at sub-file level both in batch and real-time modes. In the case of the latter, data is moved almost as soon as it enters the storage system or relocated based on its activity levels within seconds of data being accessed, whereas more traditional tiering tends to operate on an hourly, daily or even weekly schedule. Some more background on the relative differences between HSM, ILM and automated tiered storage is available at SNIA web site.<ref>http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf</ref> A general comparison of different approaches can also be found in this 'comparison article on auto tiered storage'[http://searchstorage.techtarget.co.uk/feature/Automated-storage-tiering-product-comparison].

==OS and Software Based Automated Tiered Storage==
Most server oriented software automated tiered storage vendors offer tiering as a component of a general storage virtualization stack offering, an example being [[Microsoft]] with their Tiered Storage Spaces.<ref>https://redmondmag.com/articles/2013/08/30/windows-storage-tiering.aspx?m=1</ref> However, automated tiering is now becoming a common part of industry standard operating systems such as Linux and Microsoft Windows, and in the case of consumer PCs, Apple OSX with its Fusion Drive.<ref>[http://www.apple.com/imac/performance/ "Apple iMac Performance Website"] October 24, 2012.</ref> This solution allowed a single SSD and hard disk drive to be combined into a single automated tiered storage drive that ensured that the most frequently accessed data was stored on the SSD portion of the virtual disk. A more OS agnostic version was introduced by Enmotus which supports real-time tiering with its FuzeDrive product for Linux and Windows operating systems, extending support to storage class memory offerings such as NVDIMM and NVRAM devices.<ref>http://cdn2.hubspot.net/hub/486631/file-2586107985-pdf/PDFs/20111129_S2-102_Mills.pdf?t=1447892865729</ref>

==SAN Based Tiered Storage==
An example of automated tiered storage in a hardware storage array is a feature called Data Progression from Compellent Technologies. Data Progression has the capability to transparently move blocks of data between different drive types and RAID groups such as RAID 10 and RAID 5. The blocks are part of the "same virtual volume even as they span different RAID groups and drive types.  Compellent can do this because they keep metadata about every block -- which allows them to keep track of each block and its associations.".<ref>[http://blogs.computerworld.com/compellent_ilm  Tony Asaro, Computerworld. "Compellent-Intelligent Tiered Storage."] January 19, 2009.</ref> Another strong example of SAN based tiering is DotHill's Autonomous Tiered Storage which moves data between tiers of storage within the SAN disk array with decisions made every few seconds".<ref>[https://www.dothill.com/solutions/tiered-data-storage/ "Hybrid Data Storage Solution with SSD and HDD Tiers"]</ref>

== Automated Tiered Storage vs. SSD Caching ==
While tiering solutions and caching may look the same on the surface, the fundamental differences lie in the way the faster storage is utilized and the algorithms used to detect and accelerate frequently accessed data. SSD caching operates much like SRAM-DRAM caches do i.e. they make a copy of frequently accessed blocks of data, for example in 4K cache page sizes, and store the copy in the SSD and use this copy instead of the original data source on the slower backend storage. Every time a storage IO occurs, the caching software look to see if a copy of this data already exists using a variety of algorithms and service the host request from the SSD if it is found. The SSD is used in this case as a lookaside device as it is not part of the primary storage. While some good caching algorithms can demonstrate native SSD performance on reads and short bursts of writes, caching typically operates well below the maximum sustainable rate of the underlying SSD devices as overhead CPU cycles are introduced during the host IO commands that increasingly impact performance as the amount of data cached grows. Tiering on the other hand operates very differently. Using the specific case of SSDs, once data is identified as frequently used, the identified blocks of data are moved in the background to the SSD and not copied as the SSD is being utilized as a primary storage tier, not a look aside copy area. When the data is subsequently accessed, the IOs occur at or near the native performance of the SSDs as there area are few if any CPU cycles needed to do the simpler virtual to physical addressing translations.<ref>[http://searchsolidstatestorage.techtarget.com/tip/Tiering-vs-caching-in-flash-based-storage-systems] "Tiering vs. caching in flash-based storage systems"</ref>

== See also ==
* [[Hierarchical storage management]]
* [[Tiered storage]]

== References ==
* Russ Taddiken – Senior Storage Architect (2006). Automating Data Movement Between Storage Tiers. Retrieved from the UW Records Management Web site: http://www.compellent.com/
<references/>

== External links ==
* http://www.snia.org/sites/default/education/tutorials/2012/spring/storman/LarryFreeman_What_Old_Is_New_Again.pdf

[[Category:Data management]]
<=====doc_Id=====>:168
<=====title=====>:
Jenks natural breaks optimization
<=====text=====>:
The '''Jenks optimization method''', also called the '''Jenks natural breaks classification method''', is a [[data clustering]] method designed to determine the best arrangement of values into different classes. This is done by seeking to minimize each class’s average deviation from the class mean, while maximizing each class’s deviation from the means of the other groups. In other words, the method seeks to reduce the [[variance]] within classes and maximize the variance between classes.<ref name="Jenks">Jenks, George F. 1967. "The Data Model Concept in Statistical Mapping", International Yearbook of Cartography 7: 186–190.</ref><ref name="McMaster">McMaster, Robert, "In Memoriam: George F. Jenks (1916–1996)". Cartography and Geographic Information Science. 24(1) p.56-59.</ref>

==History==

=== George Jenks ===
George Frederick Jenks was a 20th Century American [[cartography|cartographer]]. Graduating with his Ph.D. in agricultural geography from [[Syracuse University]] in 1947, Jenks began his career under the tutelage of [[Richard Edes Harrison|Richard Harrison]], cartographer for [[Time (magazine)|TIME]] and Fortune magazine.<ref name="McMaster2">McMaster, Robert and McMaster, Susanna. 2002. “A History of Twentieth-Century American Academic Cartography”, Cartography and Geographic Information Science. 29(3) p.312-315.</ref> He joined the faculty of the [[University of Kansas]] in 1949 and began to build the cartography program. During his 37-year tenure at KU, Jenks developed the Cartography program into one of three programs renowned for their graduate education in the field; the others being the [[University of Wisconsin]] and the [[University of Washington]]. Much of his time was spent developing and promoting improved cartographic training techniques and programs. He also spent significant time investigating three-dimensional maps, eye-movement research, [[thematic map]] communication, and [[geostatistics]].<ref name="McMaster" /><ref name="McMaster2" /><ref name="CSUN">CSUN Cartography Specialty Group, [http://www.csun.edu/~hfgeg003/csg/winter97.html Winter 1997 Newsletter]</ref>

===Development===
Jenks was a cartographer by profession. His work with [[statistics]] grew out of a desire to make [[choropleth map]]s more visually accurate for the viewer. In his paper, ''The Data Model Concept in Statistical Mapping'', he claims that by visualizing data in a three dimensional model cartographers could devise a “systematic and rational method for preparing choroplethic maps”.<ref name="Jenks" /> Jenks used the analogy of a “blanket of error” to describe the need to use elements other than the mean to generalize data. The three dimensional models were created to help Jenks visualize the difference between data classes. His aim was to generalize the data using as few planes as possible and maintain a constant “blanket of error”.

==Method==
The method requires an iterative process. That is, calculations must be repeated using different breaks in the dataset to determine which set of breaks has the smallest in-class [[variance]]. The process is started by dividing the ordered data into groups. Initial group divisions can be arbitrary. There are four steps that must be repeated:
#Calculate the sum of squared deviations between classes (SDBC).
#Calculate the sum of squared deviations from the array mean (SDAM).
#Subtract the SDBC from the SDAM (SDAM-SDBC). This equals the sum of the squared deviations from the class means (SDCM).
#After inspecting each of the SDBC, a decision is made to move one unit from the class with the largest SDBC toward the class with the lowest SDBC.

New class deviations are then calculated, and the process is repeated until the sum of the within class deviations reaches a minimal value.<ref name="Jenks" /><ref name="ESRI">ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&d=26442 What is the Jenks Optimization method]</ref>

Alternatively, all break combinations may be examined, SDCM calculated for each combination, and the combination with the lowest SDCM selected. Since all break combinations are examined, this guarantees that the one with the lowest SDCM is found.

Finally, the GVF statistic (goodness of variance fit) is calculated. GVF is defined as (SDAM - SDCM) / SDAM. GVF ranges from 0 (worst fit) to 1 (perfect fit).

==Uses==

{{main article|Choropleth map}}
Jenks’ goal in developing this method was to create a map that was absolutely accurate, in terms of the representation of data’s spatial attributes. By following this process, Jenks claims, the “blanket of error” can be uniformly distributed across the mapped surface. He developed this with the intention of using relatively few data classes, less than seven, because that was the limit when using monochromatic shading on a choroplethic map.<ref name="Jenks" />

==Alternative methods==
{{Main article|Cluster analysis}}

Other methods of data classification include [[Head/tail Breaks]], Natural Breaks (without Jenks Optimization), Equal Interval, Quantile, and Standard Deviation.

==See also==
* [[k-means clustering]], a generalization for multivariate data (Jenks natural breaks optimization seems to be one dimensional k-means<ref>[http://www.quantdec.com/SYSEN597/GTKAV/section1/chapter_9.htm]</ref>).

==References==
{{Reflist}}

==External links==
* ESRI FAQ, [http://support.esri.com/index.cfm?fa=knowledgebase.techarticles.articleShow&d=26442 What is the Jenks Optimization method]
* Volunteered Geographic Information, Daniel Lewis, [http://danieljlewis.org/2010/06/07/jenks-natural-breaks-algorithm-in-python/ Jenks Natural Breaks Algorithm with an implementation in python]
* Object Vision wiki, [http://wiki.objectvision.nl/index.php/Fisher%27s_Natural_Breaks_Classification Fisher's Natural Breaks Classification, a O(k*n*log(n)) algorithm]
* [http://www.ehdp.com/vitalnet/breaks-1.htm What is Jenks Natural Breaks?]

[[Category:Data management]]
[[Category:Cartography]]
<=====doc_Id=====>:171
<=====title=====>:
Data independence
<=====text=====>:
{{multiple issues|
{{Cleanup|date=January 2008}}
{{Unreferenced|date=December 2009}}
}}

'''Data independence''' is the type of [[data]] transparency that matters for a centralised [[Database management system|DBMS]]. It refers to the immunity of user [[application software|applications]] to changes made in the definition and organization of data.

Physical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues, since there is no difference in the operation carried out against the data.

The data independence and operation independence together gives the feature of [[data abstraction]]. There are two levels of data independence.

==First Level of Data Independence==
The [[logical]] structure of the data is known as the 'schema definition'. In general, if a user application operates on a subset of the [[Attribute (computing)|attributes]] of a [[Relation (database)|relation]], it should not be affected later when new attributes are added to the same relation.
Logical data independence indicates that the conceptual schema can be changed without affecting the existing schemas.

==Second Level of Data Independence==
The physical structure of the data is referred to as "physical data description". Physical data independence deals with hiding the details of the storage structure from user applications. The application should not be involved with these issues since, conceptually, there is no difference in the operations carried out against the data. There are three types of data independence:
# Logical data independence: The ability to change the logical (conceptual) schema without changing the External schema (User View) is called logical data independence. For example, the addition or removal of new entities, attributes, or relationships to the conceptual schema should be possible without having to change existing external schemas or having to rewrite existing application programs.
# Physical data independence: The ability to change the physical schema without changing the logical schema is called physical data independence. For example, a change to the internal schema, such as using different file organization or storage structures, storage devices, or indexing strategy, should be possible without having to change the conceptual or external schemas.
#View level data independence: always independent no effect, because there doesn't exist any other level above view level.

===Data Independence===

Data independence can be explained as follows: Each higher level of the data architecture is immune to changes of the next lower level of the architecture.

The logical scheme stays unchanged even though the storage space or type of some data is changed for reasons of optimization or reorganization. In this external schema does not change. In this internal schema changes may be required due to some physical schema were reorganized here.  Physical data independence is present in most databases and file environment in which hardware storage of encoding, exact location of data on disk,merging of records, so on this are hidden from user.

One of the biggest advantage of databases is data independence. It means we can change the conceptual schema at one level without affecting the data at another level. It also means we can change the structure of a database without affecting the data required by users and programs. This feature was not available in the file oriented approach.

==Data Independence Types==

The ability to modify schema definition in one level without affecting schema definition in the next higher level is called data independence. There are two levels of data independence, they are Physical data independence and Logical data independence.

# Physical data independence is the ability to modify the physical schema without causing application programs to be rewritten. Modifications at the physical level are occasionally necessary to improve performance. It means we change the physical storage/level without affecting the conceptual or external view of the data. The new changes are absorbed by mapping techniques.
# Logical data independence is the ability to modify the logical schema without causing application program to be rewritten. Modifications at the logical level are necessary whenever the logical structure of the database is altered (for example, when money-market accounts are added to banking system).  Logical Data independence means if we add some new columns or remove some columns from table then the user view and programs should not change. For example: consider two users A & B. Both are selecting the fields "EmployeeNumber" and "EmployeeName". If user B adds a new column (e.g. salary) to his table, it will not effect the external view for user A, though the internal schema of the database has been changed for both users A & B. 

Logical data independence is more difficult to achieve than physical data independence, since application programs are heavily dependent on the logical structure of the data that they access.

Physical data independence means we change the physical storage/level without affecting the conceptual or external view of the data. Mapping techniques absorbs the new changes.

==See also==
* [[Network transparency]]
* [[Replication transparency]]
* [[Codd's 12 rules]]
* [[ANSI-SPARC_Architecture]]


{{DEFAULTSORT:Data Independence}}
[[Category:Data management]]
<=====doc_Id=====>:174
<=====title=====>:
Content inventory
<=====text=====>:
A '''content inventory''' is the process and the result of cataloging the entire contents of a [[website]]. An allied practice—a [[content audit]]—is the process of ''evaluating'' that content.<ref name="Halverson">{{cite web |url= http://www.peachpit.com/articles/article.aspx?p=1388961 |title= Content Strategy for the Web: Why You Must Do a Content Audit |first=Kristina |last=Halvorson |date=August 2009 |accessdate=6 May 2010}}</ref><ref name="Baldwin">{{cite web |url= http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title= Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=29 April 2010}}</ref><ref name="Marsh">{{cite web |url=http://www.hilarymarsh.com/2012/03/12/how-to-do-a-content-audit/ |title=How to do a content audit |first=Hilary |last=Marsh |date=March 2012 |accessdate=2 May 2013}}</ref> A content inventory and a [[content audit]] are closely related concepts, and they are often conducted in tandem.

==Description==

A content inventory typically includes all information assets on a website, such as [[web page]]s (html), [[meta element]]s (e.g., keywords, description, page title), images, audio and video files, and document files (e.g., .pdf, .doc, .ppt).<ref name="Spencer2006">{{cite web |url=http://maadmob.net/donna/blog/2006/taking-a-content-inventory |title=Taking a content inventory |first=Donna |last=Spencer |date=January 2006 |accessdate=27 April 2010}}</ref><ref name="Doss2007">{{cite web |url=http://www.fatpurple.com/2010/02/26/content-inventory/ |title=Content Inventory: Sometimes referred to as Web Content Inventory or Web Audit |first=Glen |last=Doss |date=January 2007 |accessdate=27 April 2010}}</ref><ref name="Jones2009">{{cite web |url=http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php |title=Content Analysis: A Practical Approach |first=Colleen |last=Jones |date=August 2009 |accessdate=27 April 2010}}</ref><ref name="Leise2007">{{cite web |url=http://boxesandarrows.com/view/content-analysis |title=Content Analysis Heuristics |first=Fred |last=Leise |date=March 2007 |accessdate=27 April 2010}}</ref><ref name="Baldwin2010">{{cite web |url=http://nform.ca/blog/2010/01/doing-a-content-audit-or-inven |title=Doing a content audit or inventory |first=Scott |last=Baldwin |date=January 2010 |accessdate=27 April 2010}}</ref><ref name="Krozser">{{cite web |url=http://www.alttags.org/content-management/the-content-inventory-roadmap-to-a-succesful-cms-implementation/ |title= The Content Inventory: Roadmap to a Successful CMS Implementation |first=Kassia |last=Krozser |date=April 2005 |accessdate=27 April 2010}}</ref> A content inventory is a [[Quantitative research|quantitative analysis]] of a website. It simply logs what is on a website. The content inventory will answer the question: “What is there?” and can be the start of a website review.<ref name="GOSS Interactive">{{cite web |url=http://www.gossinteractive.com/community/whitepapers/conducting-a-website-review-and-implementing-results-for-increased-customer-engagement-and-conversions |title= Conducting a website review and implementing results for increased customer engagement and conversions()|first=GOSS Interactive|date=October 2011 |accessdate=8 October 2011}}</ref> A related (and sometimes confused term) is a [[content audit]], a [[Qualitative research|qualitative analysis]] of information assets on a website. It is the assessment of that content and its place in relationship to surrounding Web pages and information assets. The content audit will answer the question: “Is it any good?”<ref name="Baldwin"/><ref name="Marsh"/>

Over the years, techniques for creating and managing a content inventory have been developed and refined in the field of website [[content management]].<ref name="Halverson"/><ref name="Veen2002">{{cite web |url=http://www.adaptivepath.com/ideas/essays/archives/000040.php |title=Doing a Content Inventory (Or, A Mind-Numbingly Detailed Odyssey Through Your Web Site) |first=Jeffrey |last=Veen |date=June 2002 |accessdate=27 April 2010}}</ref><ref name="Bruns">{{cite web |url=http://donbruns.net/index.php/how-to-automatically-index-a-content-inventory/ |title= Automatically Index a Content Inventory with GetUXIndex() |first=Don |last=Bruns |date=March 2010 |accessdate=6 May 2010}}</ref>

A [[spreadsheet]] application (e.g., [[Microsoft Excel]] or [[LibreOffice Calc]]) is the preferred tool for keeping a content inventory; the data can be easily configured and manipulated. Typical categories in a content inventory include the following:

* Link — The [[URL]] for the page
* Format — For example, .[[html]], [[.pdf]], [[Microsoft Word|.doc]], [[Microsoft PowerPoint|.ppt]]
* Meta page title — Page title as it appears in the meta <title> tag
* Meta keywords — Keywords as they appear in the [[Meta tag#The keywords attribute|meta name="keywords" tag element]]
* Meta description — Text as it appears in the [[Meta tag#The description attribute|meta name="description" tag element]]
* Content owner — Person responsible for maintaining page content
* Date page last updated — Date of last page update
* Audit Comments (or Notes) — Audit findings and notes

There are other descriptors that may need to be captured on the inventory sheet. Content management experts advise capturing information that might be useful for both short- and long-term purposes. Other information could include:

* the overall topic or area to which the page belongs
* a short description of the information on the page
* when the page was created, date of last revision, and when the next page review is due
* pages this page links to
* pages that link to this page
* page status – keep, delete, revise, in revision process, planned, being written, being edited, in review, ready for posting, or posted
* rank of page on the website – is it a top 50 page? a bottom 50 page? Initial efforts might be more focused on those pages that visitors use the most and least.

Other tabs in the inventory workbook can be created to track related information, such as meta keywords, new Web pages to develop, website tools and resources, or content inventories for sub-areas of the main website. Creating a single, shared location for information related to a website can be helpful for all website content managers, writers, editors, and publishers.

Populating the spreadsheet is a painstaking task, but some up-front work can be automated with software, and other tools and resources can assist the audit work.

==Value==

A content inventory and a content audit are performed to understand what is on a website and why it is there. The inventory sheet, once completed and revised as the site is updated with new content and information assets, can also become a resource for help in maintaining [[website governance]].

For an existing website, the information cataloged in a content inventory and content audit will be a resource to help manage all of the information assets on the website.<ref name="Usability">{{cite web |url=http://www.usability.gov/methods/design_site/inventory.html |title=Content Inventory |date=26 May 2009 |publisher=U.S. Department of Health & Human Services |accessdate=4 May 2010}}</ref> The information gathered in the inventory can also be used to plan a website re-design or site migration to a [[web content management system]].<ref name="Krozser"/> When planning a new website, a content inventory can be a useful [[project management]] tool: as a guide to map [[information architecture]] and to track new pages, page revision dates, content owners, and so on.

==See also==

* [[Content audit]]
* [[Web content management system]]
* [[Design methods]]
* [[Information architecture]]
* [[Web design]]
* [[Website governance]]

==References==
{{Reflist}}

==Further reading==

* In his article [http://www.boxesandarrows.com/view/a-map-based-approach A Map-Based Approach to a Content Inventory], Patrick Walsh describes how to use [[Microsoft Access]] and Microsoft Excel to link a data attribute with a structural attribute to create “a tool that can be used throughout the lifetime of a website.”
* In the article [http://www.louisrosenfeld.com/home/bloug_archive/000448.html The Rolling Content Inventory], author Louis Rosenfeld argues that “ongoing, partial content inventories” are more cost-effective and realistic to implement.
* Colleen Jones writes from a UX design perspective in [http://www.uxmatters.com/mt/archives/2009/08/content-analysis-a-practical-approach.php Content Analysis: A Practical Approach].
* [http://xmlpress.net/content-strategy/audits-and-inventories/ Content Audits and Inventories: A Handbook] is a practical guide to conducting content inventories and audits.

==External links==
* [http://home.snafu.de/tilman/xenulink.html Xenu's Link Sleuth]
* [http://siteorbiter.com/ SiteOrbiter]
* [http://www.webconfs.com/similar-page-checker.php Similar Page Checker]
* [http://www.cryer.co.uk/resources/link_checkers.htm Link Checker Tools]
* [http://www.kevinpnichols.com/downloads/kpn_content_audit.xls Kevin P Nichols' Content Inventory and Audit Template]

{{DEFAULTSORT:Content Inventory}}
[[Category:Data management]]
[[Category:Website management]]
[[Category:Content management systems]]
<=====doc_Id=====>:177
<=====title=====>:
Data stream management system
<=====text=====>:
{{Use dmy dates|date=July 2013}}

A '''Data stream management system''' (DSMS) is a computer program to manage continuous data streams. It is similar to a [[database management system]] (DBMS), which is, however, designed for static data in conventional databases. A DSMS also offers a flexible query processing so that the information need can be expressed using queries. However, in contrast to a DBMS, a DSMS executes a ''continuous query'' that is not only performed once, but is permanently installed. Therefore, the query is continuously executed until it is explicitly uninstalled. Since most DSMS are data-driven, a continuous query produces new results as long as new data arrive at the system. This basic concept is similar to [[Complex event processing]] so that both technologies are partially coalescing.

== Functional principle ==

One of the most important features of a DSMS is the possibility to handle potentially infinite and rapidly changing data streams by offering flexible processing at the same time, although there are only limited resources such as main memory. The following table provides various principles of DSMS and compares them to traditional DBMS.

{| border="1"
! Database management system (DBMS)
! Data stream management system (DSMS)
|-
|Persistent data (relations)
|volatile data streams
|-
|Random access
|Sequential access
|-
|One-time queries 
|Continuous queries
|-
|(theoretically) unlimited secondary storage
|limited main memory
|-
|Only the current state is relevant
|Consideration of the order of the input
|-
|relatively low update rate
|potentially extremely high update rate
|-
|Little or no time requirements
|Real-time requirements
|-
|Assumes exact data 
|Assumes outdated/inaccurate data
|-
|Plannable query processing
|Variable data arrival and data characteristics
|}

== Processing and streaming models ==
One of the biggest challenges for a DSMS is to handle potentially infinite data streams using a fixed amount of memory and no random access to the data. There are different approaches to limit the amount of data in one pass, which can be divided into two classes. For the one hand, there are compression techniques that try to summarize the data and for the other hand there are window techniques that try to portion the data into (finite) parts.

=== Synopses ===
The idea behind compression techniques is to maintain only a synopsis of the data, but not all (raw) data points of the data stream. The algorithms range from selecting random data points called sampling to summarization using histograms, wavelets or sketching. One simple example of a compression is the continuous calculation of an average. Instead of memorizing each data point, the synopsis only holds the sum and the number of items. The average can be calculated by dividing the sum by the number. However, it should be mentioned that synopses cannot reflect the data accurately. Thus, a processing that is based on synopses may produce inaccurate results.

=== Windows ===
Instead of using synopses to compress the characteristics of the whole data streams, window techniques only look on a portion of the data. This approach is motivated by the idea that only the most recent data are relevant. Therefore, a window continuously cuts out a part of the data stream, e.g. the last ten data stream elements, and only considers these elements during the processing. There are different kinds of such windows like sliding windows that are similar to [[FIFO (computing and electronics)|FIFO]] lists or tumbling windows that cut out disjoint parts. Furthermore, the windows can also be differentiated into element-based windows, e.g., to consider the last ten elements, or time-based windows, e.g., to consider the last ten seconds of data. There are also different approaches to implementing windows. There are, for example, approaches that use timestamps or time intervals for system-wide windows or buffer-based windows for each single processing step. Sliding-window query processing is also suitable to being implemented in parallel processors by exploiting parallelism between different windows and/or within each window extent.<ref>{{cite journal|last1=De Matteis|first1=Tiziano|last2=Mencagli|first2=Gabriele|title=Parallel Patterns for Window-Based Stateful Operators on Data Streams: An Algorithmic Skeleton Approach|journal=International Journal of Parallel Programming|date=25 March 2016|doi=10.1007/s10766-016-0413-x}}</ref>

== Query Processing ==
Since there are a lot of prototypes, there is no standardized architecture. However, most DSMS are based on the [[Information retrieval|query]] processing in DBMS by using declarative languages to express queries, which are translated into a plan of operators. These plans can be optimized and executed. A query processing often consists of the following steps.

=== Formulation of continuous queries ===
The formulation of queries is mostly done using declarative languages like [[SQL]] in DBMS. Since there are no standardized query languages to express continuous queries, there are a lot of languages and variations. However, most of them are based on [[SQL]], such as the [[Continuous Query Language]] (CQL), [[StreamSQL]] and [[Event stream processing|EPL]]. There are also graphical approaches where each processing step is a box and the processing flow is expressed by arrows between the boxes.

The language strongly depends on the processing model. For example, if windows are used for the processing, the definition of a window has to be expressed. In [[StreamSQL]], a query with a sliding window for the last 10 elements looks like follows:
<source lang="sql">
SELECT AVG(price) FROM examplestream [SIZE 10 ADVANCE 1 TUPLES] WHERE value > 100.0
</source>
This stream continuously calculates the average value of "price" of the last 10 tuples, but only considers those tuples whose prices are greater than 100.0.

In the next step, the declarative query is translated into a logical query plan. A query plan is a directed graph where the nodes are operators and the edges describe the processing flow. Each operator in the query plan encapsulates the semantic of a specific operation, such as filtering or aggregation. In DSMSs that process relational data streams, the operators are equal or similar to the operators of the [[Relational algebra]], so that there are operators for selection, projection, join, and set operations. This operator concept allows the very flexible and versatile processing of a DSMS.

=== Optimization of queries ===
The logical query plan can be optimized, which strongly depends on the streaming model. The basic concepts for optimizing continuous queries are equal to those from [[Query optimizer|database systems]]. If there are relational data streams and the logical query plan is based on relational operators from the [[Relational algebra]], a query optimizer can use the algebraic equivalences to optimize the plan. These may be, for example, to push selection operators down to the sources, because they are not so computationally intensive like join operators.

Furthermore, there are also cost-based optimization techniques like in DBMS, where a query plan with the lowest costs is chosen from different equivalent query plans. One example is to choose the order of two successive join operators. In DBMS this decision is mostly done by certain statistics of the involved databases. But, since the data of a data streams is unknown in advance, there are no such statistics in a DSMS. However, it is possible to observe a data stream for a certain time to obtain some statistics. Using these statistics, the query can also be optimized later. So, in contrast to a DBMS, some DSMS allows to optimize the query even during runtime. Therefore, a DSMS needs some plan migration strategies to replace a running query plan with a new one.

=== Transformation of queries ===
Since a logical operator is only responsible for the semantics of an operation but does not consist of any algorithms, the logical query plan must be transformed into an executable counterpart. This is called a physical query plan. The distinction between a logical and a physical operator plan allows more than one implementation for the same logical operator. The join, for example, is logically the same, although it can be implemented by different algorithms like a [[Nested loop join]] or a [[Sort-merge join]]. Notice, these algorithms also strongly depend on the used stream and processing model.
Finally, the query is available as a physical query plan.

=== Execution of queries ===
Since the physical query plan consists of executable algorithms, it can be directly executed. For this, the physical query plan is installed into the system. The bottom of the graph (of the query plan) is connected to the incoming sources, which can be everything like connectors to sensors. The top of the graph is connected to the outgoing sinks, which may be for example a visualization. Since most DSMSs are data-driven, a query is executed by pushing the incoming data elements from the source through the query plan to the sink. Each time when a data element passes an operator, the operator performs its specific operation on the data element and forwards the result to all successive operators.

== Data Stream Management Systems ==
* [http://www.sqlstream.com/stream-processing/ SQLstream]
* [http://www-db.stanford.edu/stream STREAM] <ref name="StandfordStream">[http://ilpubs.stanford.edu:8090/641/ Arasu, A., et. al. ''STREAM: The Stanford Data Stream Management System.''  Technical Report. 2004, Stanford InfoLab.]</ref>
* [http://www.cs.brown.edu/research/aurora/ AURORA],<ref name="aurora">{{cite conference | author = Abadi | title = Aurora: A Data Stream Management System | conference = SIGMOD 2003 | citeseerx = 10.1.1.67.8671 |display-authors=etal}}</ref> [http://www.streambase.com/ StreamBase Systems, Inc.]
* [http://telegraph.cs.berkeley.edu/telegraphcq/ TelegraphCQ] <ref name="telegraphcq">[http://www.cs.berkeley.edu/~franklin/Papers/TCQcidr03.pdf Chandrasekaran, S. et al, "TelegraphCQ: Continuous Dataflow Processing for an Uncertain World." CIDR 2003.]</ref>
* [http://research.cs.wisc.edu/niagara/ NiagaraCQ],<ref name="niagaracq">[http://www.cs.wisc.edu/niagara/papers/NiagaraCQ.pdf Chen, J. et al, "NiagaraCQ: A Scalable Continuous Query System for Internet Databases." SIGMOD 2000.]</ref> 
* [http://wwwdb.inf.tu-dresden.de/research-projects/closed-projects/qstream/ QStream]
* [http://dbs.mathematik.uni-marburg.de/Home/Research/Projects/PIPES PIPES], [http://www.softwareag.com/de/products/wm/events/overview/default.asp webMethods Business Events]
* [http://www-db.in.tum.de/research/projects/StreamGlobe/index.shtml StreamGlobe]
* [http://odysseus.informatik.uni-oldenburg.de/ Odysseus]
* [http://www.microsoft.com/sqlserver/en/us/solutions-technologies/business-intelligence/streaming-data.aspx StreamInsight]
* [http://www-01.ibm.com/software/data/infosphere/streams/ InfoSphere Streams]
* [http://www.sas.com/en_us/software/data-management/event-stream-processing.html SAS Event Stream Processing Engine]
* [http://go.sap.com/uk/product/data-mgmt/complex-event-processing.html  SAP Event Stream Processor]
* [https://www.pipelinedb.com/ Pipeline DB]

== See also ==
* [[Complex Event Processing]]
* [[Event stream processing]]
* [[Relational data stream management system]]

== References ==
{{Reflist}}
* {{Cite book
|last=Aggarwal
|first=Charu C.
|authorlink=
|year=2007
|title=Data Streams: Models and Algorithms
|publisher=Springer
|location=New York
|id=
|isbn=978-0-387-47534-9
}}
* {{Cite book
|first1=Lukasz
|last1=Golab
|first2=M. Tamer
|last2=Özsu
|authorlink=
|year=2010
|title=Data Stream Management
|publisher=Morgan and Claypool
|location=Waterloo, USA
|id=
|isbn=978-1-608-45272-9
}}

==External links==
*[http://www.pam2004.org/papers/113.pdf Using Data Stream Management Systems for Traffic Analysis: A Case Study, last visited 2013-01-10]
*[http://infolab.stanford.edu/stream/ STREAM: Stanford Stream Data Manager, last visited 2013-01-10]
*[http://datalab.cs.pdx.edu/niagara/ NiagaraST: A Research Data Stream Management System at Portland State University, last visited 2013-01-10]
*[http://odysseus.informatik.uni-oldenburg.de Odysseus: An open source Java based framework for Data Stream Management Systems, last visited 2013-01-10]
*[http://home.dei.polimi.it/margara/papers/survey.pdf Processing Flows of Information: From Data Stream to Complex Event Processing] - Survey article on Data Stream and Complex Event Processing Systems, last visited 2013-01-10
*[http://www.streambase.com/developers/docs/latest/streamsql/index.html StreamSQL reference, last visited 2013-01-10]
*[https://web.archive.org/web/20140706215458/http://www.sqlstream.com/stream-processing-with-sql/ Stream processing with SQL] - Introduction to streaming data management with SQL

[[Category:Data management]]
<=====doc_Id=====>:180
<=====title=====>:
Novell Storage Manager
<=====text=====>:
{{Infobox software
|name                       = Novell Storage Manager
|logo                       =
|screenshot                 =
|caption                    =
|collapsible                =
|author                     =
|developer                  = [[Novell]]
|released                   = 2004 <!-- {{Start date|YYYY|MM|DD}} -->
|discontinued               =
|latest release version     = 4.1
|latest release date        = {{Start date|2014|10|07}}
|latest preview version     =
|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->
|frequently updated         =
|programming language       =
|operating system           =
|platform                   =
|size                       =
|language                   =
|status                     =
|genre                      = [[System Software]]
|license                    =
|website                    = [http://www.novell.com/products/storage-manager/ Novell Storage Manager]
}}

'''Novell Storage Manager''' is a [[system software]] package released by [[Novell]] in 2004 <ref>{{Citation | last=Greyzdorf| first=Noemi| title=Novell Delivers a New Way of Intelligently Managing Organizations' File-Based Information| journal=IDC #216013 | volume=1| issue=Storage Software: Technology Assessment| year=2009| pages=1–3| url=http://www.novell.com/docrep/2009/01/Novell%20Delivers%20a%20New%20Way%20of%20Intelligently%20Managing%20Organizations_%20File-Based%20Information_en.pdf}}</ref> that uses identity, policy and [[Novell eDirectory|directory]] events to automate full lifecycle management of file storage for individual users and organizational groups. By tying storage management to an organization's existing identity infrastructure, it has been pointed out,<ref>{{Citation | last=Greyzdorf| first=Noemi| title=Efficiently Delivering Enterprise-Class File-Based Storage| journal=IDC Spotlight | year=2010| pages=1–5}}</ref> Novell Storage Manager enables the administration of users across all file servers "as a single pool rather than [in] separate independently managed domains." Novell Storage Manager is a component of the [[Novell File Management Suite]].

==How It Works==

Novell Storage Manager dynamically manages and provisions storage based on user and group events that occur in the directory, including user creations, group assignments, moves, renames, and deletions. When a change happens in the directory that affects a user’s file storage needs or user storage policy, Storage Manager applies the appropriate policy and makes the necessary changes at the file system level to address those storage needs.<ref>{{citation| title=Novell Storage Manager for Novell eDirectory | year=2009 | page=4 | url=http://www.novell.com/docrep/2009/04/Novell_Storage_Manager_for_Novell_eDirectory_White_Paper_en.pdf}}</ref>

The following key components comprise Novell Storage Manager's identity and policy-driven [[state machine]] architecture: Directory services; Storage policies; Novell Storage Manager event monitors; Novell Storage Manager policy engine; Novell Storage Manager agents; and Action objects. This state machine architecture enables the engine to properly deal with transient waits with directory synchronization issues. It also allows recovery from failures involving network communications, a target server or a server running a component of Storage Manager—including the policy engine itself. If a failure or interruption occurs at any point during operation, Storage Manager will be able to successfully continue the operation from where it was when the interruption occurred.

==Reviews==

Jon Toigo called Novell Storage Manager "a robust and smart approach to corralling user files... into an organized and efficient management scheme".<ref>{{Citation| last = Toigo | first = Jon William | title = Novell Storage Manager Strikes Data Management Gold | url= http://esj.com/articles/2009/04/28/novell-storage-mgr.aspx | accessdate = 26 July 2010}}</ref> He also said it was "best in class" of the products he'd reviewed.<ref>{{Citation| last = Toigo | first = Jon William | title = Everything We Need to Know About How to Screw Up IT… | url=http://www.drunkendata.com/?p=2916 | accessdate = 30 July 2010}}</ref>

==References==
{{reflist}}

==External links==
*[http://www.novell.com/products/storage-manager/ Novell Storage Manager: Product homepage] - Overview, features, and technical information
*[http://www.storagemanagersupport.com/nsm/ Novell Storage Manager: Support]

{{Novell}}

[[Category:Novell]]
[[Category:Novell software]]
[[Category:Storage software]]
[[Category:Data management]]
[[Category:Identity management]]
<=====doc_Id=====>:183
<=====title=====>:
Virtual data room
<=====text=====>:
A '''virtual data room''' (sometimes called a '''VDR''') is an online repository of information that is used for the storing and distribution of documents.  In many cases, a virtual data room is used to facilitate the [[due diligence]] process during an [[M&A]] transaction, [[loan syndication]], or private equity and venture capital transactions.  This due diligence process has traditionally used a physical [[data room]] to accomplish the disclosure of documents. For reasons of cost, efficiency and security, virtual data rooms have widely replaced the more traditional physical data room.<ref>http://www.inc.com/best-industries-2013/jeremy-quittner/virtual-data-rooms.html</ref><ref>http://www.forbes.com/pictures/fghj45fjl/2-virtual-data-rooms/</ref>

A virtual data room is an [[extranet]] to which the bidders and their advisers are given access via the internet. An extranet is essentially a website with limited controlled access, using a secure log-on supplied by the vendor, which can be disabled at any time, by the vendor, if a bidder withdraws. Much of the information released is confidential and restrictions are applied to the viewer’s ability to release this to third parties (by means of forwarding, copying or printing). This can be effectively applied to protect the data using digital rights management.<ref>http://www.divestopedia.com/definition/836/virtual-data-room-vdr</ref> 

In the process of [[mergers and acquisitions]] the data room is set up as part of the central repository of data relating to companies or divisions being acquired or sold. The data room enables the interested parties to view information relating to the business in a controlled environment where confidentiality can be preserved. Conventionally this was achieved by establishing a supervised, physical data room in secure premises with controlled access. In most cases, with a physical data room, only one bidder team can access the room at a time. A virtual data room is designed to have the same advantages as a conventional data room (controlling access, viewing, copying and printing, etc.) with fewer disadvantages. Due to their increased efficiency, many businesses and industries have moved to using virtual data rooms instead of physical data rooms. In 2006, a spokesperson for a company which sets up virtual deal rooms was reported claiming that the process reduced the bidding process by about thirty days compared to physical data rooms.<ref>{{cite news|last1=Buckler|first1=Grant|title=A virtual smoke-filled room|url=http://www.theglobeandmail.com/technology/a-virtual-smoke-filled-room/article1110056/|accessdate=4 July 2016|work=The Globe and Mail|date=21 November 2006}}</ref>

==References==
{{Reflist}}


{{DEFAULTSORT:Virtual Data Room}}
[[Category:Data management]]
[[Category:Disclosure]]
[[Category:Mergers and acquisitions]]
<=====doc_Id=====>:186
<=====title=====>:
Document capture software
<=====text=====>:
{{primary sources|date=August 2009}}
'''Document Capture Software''' refers to applications that provide the ability and feature set to automate the process of [[Image scanner|scanning]] paper documents. Most scanning [[Personal computer hardware|hardware]], both scanners and [[copier]]s, provides the basic ability to scan to any number of [[image file formats]], including: [[PDF]], [[TIFF]], [[JPG]], [[BMP file format|BMP]], etc. This basic functionality is augmented by document capture software, which can add efficiency and standardization to the process.

==Typical features==
Typical features of Document Capture Software include:
* [[Barcode]] recognition
* Patch Code recognition
* Separation
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Optical mark recognition|Optical Mark Recognition (OMR)]]
* Quality Assurance
* Indexing
* Migration

===Goal for Implementation of a Document Capture Solution===
The goal for implementing a document capture solution is to reduce the amount of time spent in the scanning and capture process, and produce metadata along with an image file, and/or OCR text. This information is then migrated to a [[Document management system|Document Management]] or [[Enterprise content management|Enterprise Content Management]] system. These systems often provide a search function, allowing search of the assets based on the produced [[metadata]], and then viewed using [[document imaging]] software.

== Document Capture System Solutions - General ==

===Integration with Document Management System===
{{main|Enterprise content management}}
ECM (Enterprise Content management) and their DMS component (Document Management System) are being adopted by many organizations as a corporate document management system for all types of electronic files, e.g. MS word, PDF ... However, much of the information held by organisations is on paper and this needs to be integrated within the same document repository.

By converting paper documents into digital format through scanning companies can convert paper into image formats such as TIF and JPG and also extract valuable index information or business data from the document using OCR technology. Digital documents and associated metadata can easily be stored in the ECM in a variety of formats. The most popular of these formats is PDF which not only provides an accurate representation of the document but also allows all the OCR text in the document to be stored behind the PDF image. This format is known as PDF with hidden text or text-searchable PDF. This allows users to search for documents by using keywords in the metadata fields or by searching the content of PDF files across the repository.

====Advantages of scanning documents into a ECM/DMS====

Information held on paper is usually just as valuable to organisations as the electronic documents that are generated internally. Often this information represents a large proportion of the day to day correspondence with suppliers and customers. Having the ability to manage and share this information internally through a document management system such as [[SharePoint]] can improve collaboration between departments or employees and also eliminate the risk of losing this information through disasters such as floods or fire.

Organisations adopting an ECM/DMS  often implement electronic workflow which allows the information held on paper to be included as part of an electronic business process and incorporated into a customer record file along with other associated office documents and emails.
For business critical documents, such as purchase orders and supplier invoices, digitising documents can help speed up business transactions as well as reduce manual effort involved in keying data into business systems, such as CRM, ERP and Accounting. Scanned invoices can also be routed to managers for payment approval via email or an electronic workflow.

==Distributed Capture Solutions==

Distributed document capture is a technology which allows the scanning of documents into a central server through the use of individual capture stations. A variation of distributed capture is thin-client document capture in which documents are scanned into a central server through the use of web browser.  One of these web-based products was reviewed by AIIM.  They said, "(this product) is a thin-client distributed capture system that streamlines the process of acquiring and creating documents."<ref>Association for Information and Image Management [http://www.aiim.org/community/product-guide/Capture/Prevalent-Software-Quillix "Prevalent Software - Quillix"], accessed August 29, 2011.</ref>  The streamlining is a result of several factors including the lack of software which needs to be installed at every scanning station and the variety of input sources from which documents can be captured.  This includes things like email, fax, or a watched folder.

Jeff Shuey, Director of Business Development at Kodak, makes a distinction between distributed capture and what he calls "remote" capture.  In an article publishing in [[AIIM]], he said that the key difference between the two is whether or not the information that is captured from scanning needs to be sent to the centralized server. If, as he points out in his article, the document just needs to be scanned and committed to a [[SharePoint]] system and doesn't need to be sent to some other centralized server, this is just a remote capture situation.<ref>Association for Information and Image Management [http://www.aiim.org/community/blogs/expert/Remote-or-Distributed-Scanning-Are-they-Different "Remote or Distributed Scanning - Are They Different?"], accessed August 29, 2011.</ref>

There are Document Capture Software comparisons available, featuring some of the most relevant products (EMC Captiva, IBM Datacap, Artsyl Technologies or Ephesoft) and extracting performance facts and their most relevant features.

==References==
<references/>

{{DEFAULTSORT:Document Capture Software}}
[[Category:Artificial intelligence applications]]
[[Category:Optical character recognition]]
[[Category:Data management]]
[[Category:SharePoint]]
<=====doc_Id=====>:189
<=====title=====>:
Data binding
<=====text=====>:
'''Data binding''' is a general technique that binds data sources from the provider and consumer together and [[data synchronization|synchronizes]] them. This is usually done with two data/information sources with different languages as in [[XML data binding]]. In [[UI data binding]], data and information objects of the same language but different logic function are bound together (e.g. [[Java (programming language)|Java]] [[user interface|UI]] elements to Java objects).<ref>{{cite web |url=https://www.techopedia.com/definition/15652/data-binding|title=What is Data Binding? |work=Techopedia.com |accessdate=30 December 2015}}</ref>

In a data binding process, each data change is reflected automatically by the elements that are bound to the data. The term data binding is also used in cases where an outer representation of data in an element changes, and the underlying data is automatically updated to reflect this change. As an example, a change in a [[text box|<code>TextBox</code>]] element could modify the underlying data value.<ref>{{cite web |url=https://msdn.microsoft.com/en-us/library/ms752347(v=vs.110).aspx |title=Data Binding Overview |work=Microsoft Developer Network |publisher=Microsoft |access-date=29 December 2016}}</ref>

== Data binding frameworks and tools ==

=== [[Embarcadero Delphi|Delphi]] ===
* DSharp 3rd-party Data Binding tool
* [[OpenWire (library)|OpenWire]] Visual Live Binding - 3rd-party Visual Data Binding tool
* LiveBindings

=== [[C Sharp (programming language)|C#]] ===
* [[Windows Presentation Foundation]]

=== [[JavaScript]] ===
* [[AngularJS]]
* [[Backbone.js]]
* BindingJS
* Datum.js<ref>{{cite web |url=http://datumjs.com|title=Datum.js|accessdate=7 November 2016}}</ref>
* [[EmberJS]]
* Generic Data Binder
* [[KnockoutJS]]
* [[React (JavaScript library)]]
* SAP/OPEN UI5
* [[Vue.js]]

=== [[Java_(programming_language)|Java]] ===
* [[Google Web Toolkit]]

=== [[Objective-C]] ===
* AKABeacon iOS Data Binding framework

=== [[Scala (programming language)|Scala]] ===
* Binding.scala<ref>{{cite web |url=https://github.com/ThoughtWorksInc/Binding.scala|title=Binding.scala|accessdate=30 December 2016}}</ref> Reactive data-binding for Scala

==See also==
* [[Windows Presentation Foundation]]
* [[XML data binding]]
* [[UI data binding]]
* [[Bound property]]

==References==
<references/>

==Further reading==
*{{cite book |last=Noyes |first=Brian |title=Data Binding with Windows Forms 2.0: Programming Smart Client Data Applications with .NET |url=https://books.google.com/books?id=RxptHgJ5W2cC |date=12 January 2006 |publisher=Pearson Education |isbn=978-0-321-63010-0}}

{{DEFAULTSORT:Data Binding}}
<!--Categories-->
[[Category:Data management]]
<=====doc_Id=====>:192
<=====title=====>:
Learning object
<=====text=====>:
A '''learning object''' is "a collection of content items, practice items, and assessment items that are combined based on a single learning objective".<ref>{{citation|last=Cisco Systems|title=Reusable information object strategy|url=http://www.cisco.com/warp/public/779/ibs/solutions/learning/whitepapers/el_cisco_rio.pdf}}</ref>  The term is credited to Wayne Hodgins, and dates from a working group in 1994 bearing the name.<ref>{{citation|last=Gerard|first=R.W.|title="Shaping the mind: Computers in education", In N. A. Sciences, Applied Science and Technological Progress|url=https://books.google.com/books?id=BTcrAAAAYAAJ|year=1967}}</ref> The concept encompassed by 'Learning Objects' is known by numerous other terms, including: content objects, chunks, educational objects, information objects, intelligent objects, knowledge bits, knowledge objects, learning components, media objects, reusable curriculum components, nuggets, reusable information objects, reusable learning objects, testable reusable units of cognition, training components, and units of learning.

The core idea of the use of learning objects is characterized by the following: discoverability, reusability, and interoperability. To support discoverability, learning objects are described by Learning Object Metadata, formalized as IEEE 1484.12 [[Learning object metadata]].<ref>[http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf IEEE 1484.12 Learning Object Metadata]</ref> To support reusability, the IMS Consortium proposed a series of specifications such as the IMS [[Content package]]. And to support interoperability, the U.S. military's [[Advanced Distributed Learning]] organization created the [[Sharable Content Object Reference Model]].<ref>[http://legacy.adlnet.gov/Technologies/scorm/SCORMSDocuments/2004%204th%20Edition/Overview.aspx SCORM 2004 4th Edition Version 1.1 Overview]</ref> Learning objects were designed in order to reduce the cost of learning, standardize learning content, and to enable the use and reuse of learning content by learning management systems.<ref>[http://www.irrodl.org/index.php/irrodl/article/view/32/378|Stephen Downes|Learning Objects: Resources For Distance Education Worldwide|The International Review of Research in Open and Distributed Learning|Volume 2 Number 1 2001|Athabasca University Press]</ref>

==Definitions==
The [[Institute of Electrical and Electronics Engineers]] (IEEE) defines a learning object as "any entity, digital or non-digital, that may be used for learning, education or training".<ref>{{Harvnb|Learning Technology Standards Committee|2002|p=45}}</ref>

Chiappe defined Learning Objects as: "A digital self-contained and reusable entity, with a clear educational purpose, with at least three internal and editable components: content, learning activities and elements of context. The learning objects must have an external structure of information to facilitate their identification, storage and retrieval: the metadata."<ref>{{Harvnb|Chiappe|Segovia|Rincon|2007|p=8}}.</ref>

The following definitions focus on the relation between learning object and digital media.  RLO-CETL, a British inter-university Learning Objects Center, defines "reusable learning objects" as "web-based interactive chunks of e-learning designed to explain a stand-alone learning objective".<ref>{{citation|chapter= Learning Objects|url=http://www.rlo-cetl.ac.uk/joomla/index.php?option=com_content&task=view&id=235&Itemid=28|title=RLO-CETL: Reusable Learning Objects|accessdate=2008-04-29}}.</ref> Daniel Rehak and Robin Mason define it as "a digitized entity which can be used, reused or referenced during technology supported learning".<ref>http://129.115.100.158/txlor/docs/IEEE_LOM_1484_12_1_v1_Final_Draft.pdf</ref><ref>{{Harvnb|Rehak|Mason|2003|p=}}</ref>

Adapting a definition from the Wisconsin Online Resource Center, Robert J. Beck suggests that learning objects have the following key characteristics:

* Learning objects are a new way of thinking about learning content. Traditionally, content comes in a several hour chunk.  Learning objects are much smaller units of learning, typically ranging from 2 minutes to 15 minutes.
* Are self-contained – each learning object can be taken independently
* Are reusable – a single learning object may be used in multiple contexts for multiple purposes
* Can be aggregated – learning objects can be grouped into larger collections of content, including traditional course structures
* Are tagged with metadata – every learning object has descriptive information allowing it to be easily found by a search<ref name="beck">{{citation|last=Beck|first=Robert J.|chapter=What Are Learning Objects?|url=http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 | title=Learning Objects|publisher=Center for International Education, University of Wisconsin-Milwaukee|accessdate=2008-04-29}}</ref>

== Components ==
The following is a list of some of the types of information that may be included in a learning object and its metadata:
* General Course Descriptive Data, including: course identifiers, language of content (English, Spanish, etc.), subject area (Maths, Reading, etc.), descriptive text, descriptive keywords
* Life Cycle, including: version, status
* Instructional Content, including: text, web pages, images, sound, video
* Glossary of Terms, including: terms, definition, acronyms
* Quizzes and Assessments, including: questions, answers
* Rights, including: cost, copyrights, restrictions on Use
* Relationships to Other Courses, including prerequisite courses
* Educational Level, including: grade level, age range, typical learning time, and difficulty. [IEEE 1484.12.1:2002]
*Typology as defined by Churchill (2007): presentation, practice, simulation, conceptual models, information, and contextual representation <ref name="Churchill">Churchill, D. (2007). Towards a useful classification of learning objects. ''Educational Technology Research & Development, 55(5)'', 479-497.</ref>

==Metadata==
One of the key issues in using learning objects is their identification by search engines or content management systems.{{Citation needed|date=April 2008}}  This is usually facilitated by assigning descriptive [[learning object metadata]]. Just as a book in a library has a record in the [[card catalog]], learning objects must also be tagged with metadata.  The most important pieces of metadata typically associated with a learning object include:
# '''objective:''' The educational objective the learning object is instructing
# '''prerequisites:''' The list of skills (typically represented as objectives) which the learner must know before viewing the learning object
# '''topic:''' Typically represented in a taxonomy, the topic the learning object is instructing
# '''interactivity:''' The [[Interaction Model]] of the learning object.
# '''technology requirements:''' The required system requirements to view the learning object.

==Mutability==

A mutated learning object is, according to Michael Shaw, a learning object that has been "re-purposed and/or re-engineered, changed or simply re-used in some way different from its original intended design". Shaw also introduces the term "contextual learning object", to describe a learning object that has been "designed to have specific meaning and purpose to an intended learner".<ref>{{Harvnb|Shaw|2003}}</ref>

==Portability==
Before any institution invests a great deal of time and energy into building high-quality e-learning content (which can cost over $10,000 per classroom hour),<ref>Rumble, Greville. 2001. The Cost and Costing of Networked Learning. Journal of Asynchronous Learning Networks, Volume 5, Issue 2.</ref> it needs to consider how this content can be easily loaded into a [[Learning Management System]]. It is possible for example, to package learning objects with [[SCORM]] specification and load it in [[Moodle]] Learning Management System or [[Desire2Learn]] Learning Environment.

If all of the properties of a course can be precisely defined in a common format, the content can be serialized into a standard format such as [[XML]] and loaded into other systems.  When it is considered that some e-learning courses need to include video, mathematical equations using [[MathML]], chemistry equations using [[Chemical Markup Language|CML]] and other complex structures, the issues become very complex, especially if the systems needs to understand and validate each structure and then place it correctly in a database.{{Citation needed|date=April 2008}}

==Criticism==
In 2001, David Wiley criticized learning object theory in his paper, [https://web.archive.org/web/20041019162710/http:/rclt.usu.edu/whitepapers/paradox.html The Reusability Paradox] which is [http://www.darcynorman.net/2003/08/21/addressing-the-reusability-paradox/ summarized by D'Arcy Norman] as, ''If a learning object is useful in a particular context, by definition it is not reusable in a different context. If a learning object is reusable in many contexts, it isn’t particularly useful in any.'' 
In [http://www.learningspaces.org/papers/objections.html Three Objections to Learning Objects and E-learning Standards], Norm Friesen, Canada Research Chair in E-Learning Practices at Thompson Rivers University, points out that the word ''neutrality'' in itself implies ''a state or position that is antithetical ... to pedagogy and teaching.''

== See also ==
* [[Intelligent tutoring system]]
* [[North Carolina Learning Object Repository (NCLOR)]]
* [[Serious games]]

==References==
{{reflist|30em}}

==Further reading==
*{{citation|last=Beck|first=Robert J.|title="What Are Learning Objects?", Learning Objects, Center for International Education, University of Wisconsin-Milwaukee, |url= http://www4.uwm.edu/cie/learning_objects.cfm?gid=56 |year= 2009|accessdate= 2009-10-23}}.
*{{citation|last=Learning Technology Standards Committee|title=Draft Standard for Learning Object Metadata. IEEE Standard 1484.12.1|place=New York|publisher=Institute of Electrical and Electronics Engineers|year=2002|url=http://ltsc.ieee.org/wg12/files/LOM_1484_12_1_v1_Final_Draft.pdf| format=PDF| accessdate=2008-04-29}}.
*{{citation|last1=Rehak |first1=Daniel R.|first2=Robin |last2=Mason|chapter=Engaging with the Learning Object Economy|editor-first=Allison|editor-last=Littlejohn|title=Reusing Online Resources: A Sustainable Approach to E-Learning|place= London|publisher= Kogan Page| year=2003| pages=22–30| isbn=978-0-7494-3949-1}}.
*{{citation|last=Shaw|first=Michael|chapter=(Contextual and Mutated) Learning Objects in the Context of Design, Learning and (Re)Use| url=http://www.shawmultimedia.com/edtech_oct_03.html|title=Teaching and Learning with Technology|date=October 2003|accessdate=2008-04-29}}
*{{citation|first1=Andrés Chiappe |last1=Laverde |first2=Yasbley Segovia |last2=Cifuentes |first3=Helda Yadira Rincón |last3=Rodríguez|chapter=Toward an instructional design model based on learning objects|editor-first=Springer|editor-last=Boston|title=Educational Technology Research and Development |place=Boston|year= 2007|pages=671–81|publisher=Springer US |doi=10.1007/s11423-007-9059-0 |issue=6 |volume=55 |issn=1042-1629|id=(Print) {{ISSN|1556-6501}} (Online) |url= http://www.springerlink.com/content/u84w63873vq77h2h/?p=41be7fbeef9648ee9b554f1835112005&pi=6|accessdate=2008-08-21}} Spanish Draf: [http://andreschiappe.blogspot.com/2007/09/que-es-un-objeto-de-aprendizaje-what-is.html ''Blog de Andrés Chiappe - Objetos de Aprendizaje''].
*{{citation|last=Northrup|first=Pamela|title=Learning Objects for Instruction: Design and Evaluation |place=USA|publisher=Information Science Publishing |year=2007| format=Book}}.
*{{citation|last1=Hunt|first1=John P.|last2=Bernard|first2=Robert|title="An XML-based information architecture for learning content", IBM developerWorks, |url= http://www.ibm.com/developerworks/xml/library/x-dita9a |year= 2005|accessdate= 2005-08-05}}.
*Churchill, D. (2007). Towards a useful classification of learning objects.  ''Educational Technology Research & Development, 55(5)'', 479-497.
Innayah: Creating An Audio Script with Learning Object, unpublished, 2013.

==External links==
* The [http://www4.uwm.edu/cie/learning_objects.cfm?gid=55 Learning Objects] at Milwaukee's Center for International Education.

{{DEFAULTSORT:Learning Object}}
[[Category:Data management]]
[[Category:Educational materials]]
[[Category:Educational technology]]
<=====doc_Id=====>:195
<=====title=====>:
Dashboard (business)
<=====text=====>:
[[File:3 Dashboards.JPG|thumb|200px|Business Dashboards.]]

'''Dashboards''' often provide at-a-glance views of KPIs ([[key performance indicators]])  relevant to a particular objective or business process (e.g. [[sales]], [[marketing]], [[human resources]], or [[Production (economics)|production]]).<ref>Michael Alexander and John Walkenbach, ''Excel Dashboards and Reports'' (Wiley, 2010)</ref> In real-world terms, "dashboard" is another name for "progress report" or "report."

Often, the "dashboard" is displayed on a web page that is linked to a database which allows the report to be constantly updated. For example, a manufacturing dashboard may show numbers related to productivity such as number of parts manufactured, or number of failed quality inspections per hour. Similarly, a human resources dashboard may show numbers related to staff recruitment, retention and composition, for example number of open positions, or average days or cost per recruitment.<ref name="Briggs">{{cite web|url=http://www.targetdashboard.com/site/Dashboard-Best-Practice/Management-Report-and-Dashboard-best-practice-index.aspx|title=Management Reports & Dashboard Best Practice|last=Briggs|first=Jonathan|publisher=Target Dashboard|accessdate=18 February 2013}}</ref>

The term dashboard originates from the [[automobile]] [[dashboard]] where drivers   monitor the major functions at a glance via the instrument cluster. 

==Benefits==
Digital dashboards allow managers to monitor the contribution of the various departments in their organization. To gauge exactly how well an organization is performing overall, digital dashboards allow you to capture and report specific data points from each department within the organization, thus providing a "snapshot" of performance.

Benefits of using digital dashboards include:<ref name="Briggs" />
*Visual presentation of performance measures
*Ability to identify and correct negative trends
*Measure efficiencies/inefficiencies
*Ability to generate detailed reports showing new trends
*Ability to make more informed decisions based on collected [[business intelligence]]
*Align strategies and organizational goals
*Saves time compared to running multiple reports
*Gain total visibility of all systems instantly
*Quick identification of data outliers and correlations

==Classification==
Dashboards can be broken down according to role and are either [[strategic]], analytical, operational, or informational.<ref>Steven Few, ''Information Dashboard Design: The Effective Visual Communication of Data'' (O'Reilly, 2006)</ref> Strategic dashboards support managers at any level in an organization, and provide the quick overview that decision makers need to monitor the health and opportunities of the business. Dashboards of this type focus on high level measures of performance, and forecasts. Strategic dashboards benefit from static snapshots of data (daily, weekly, monthly, and quarterly) that are not constantly changing from one moment to the next. Dashboards for analytical purposes often include more context, comparisons, and history, along with subtler performance evaluators. Analytical dashboards typically support interactions with the data, such as drilling down into the underlying details. Dashboards for monitoring operations are often designed differently from those that support strategic decision making or data analysis and often require monitoring of activities and events that are constantly changing and might require attention and response at a moment's notice.

==Types of dashboards==

Digital dashboards may be laid out to track the flows inherent in the business processes that they monitor. Graphically, users may see the high-level processes and then [[data drilling|drill down]] into low level data. This level of detail is often buried deep within the corporate enterprise and otherwise unavailable to the senior executives.

Three main types of digital dashboard dominate the market today: stand alone software applications, web-browser based applications, and desktop applications also known as [[desktop widgets]]. The last are driven by a [[Software widget|widget engine]].

Specialized dashboards may track all corporate functions. Examples include [[human resources]], [[Recruitment|recruiting]], [[sales]], [[Business operations|operations]], [[security]], [[information technology]], [[project management]], [[customer relationship management]] and many more departmental dashboards. For a smaller organization like a startup a compact startup scorecard dashboard tracks important activities across lot of domains ranging from social media to sales.{{cn|date=September 2016}}

Digital dashboard projects involve business units as the driver and the information technology department as the enabler. The success of digital dashboard projects often depends on the [[measurement|metrics]] that were chosen for monitoring. [[Key performance indicator]]s, [[balanced scorecard]]s, and sales performance figures are some of the content appropriate on business dashboards.

==Dashboards and scoreboards==
Balanced Scoreboards and Dashboards have been linked together as if they were interchangeable. However, although both visually display critical information, the difference is in the format: Scoreboards can open the quality of an operation while dashboards provide calculated direction. 
A balanced scoreboard has what they called a "prescriptive" format. It should always contain these components (Active Strategy)...
*Perspectives – groupings of high level strategic areas
*Objectives – verb-noun phrases pulled from a strategy plan
*Measures – also called Metric or Key Performance Indicators (KPIs)
*Spotlight Indicators – red, yellow, or green symbols that provide an at-a-glance view of a measure’s performance.
Each of these sections ensures that a Balanced Scorecard is essentially connected to the businesses critical strategic needs.

The design of a dashboard is more loosely defined.  Dashboards are usually a series of graphics, charts, gauges and other visual indicators that can be monitored and interpreted.  Even when there is a strategic link, on a dashboard, it may not be noticed as such since objectives are not normally present on dashboards.  However, dashboards can be customized to link their graphs and charts to strategic objectives.<ref>ZSL Inc., ''Dashboards Vs Scorecards – An Insight'' ZSL Inc. (2006)</ref>

==Design==
Digital dashboard technology is available "out-of-the-box" from many software providers. Some companies however continue to do in-house development and maintenance of dashboard applications. For example, [[GE Aviation]] has developed a proprietary software/portal called "Digital Cockpit" to monitor the trends in aircraft spare parts business.

A good information design will clearly communicate key information to users and makes supporting information easily accessible.<ref>Stacey Barr, ''7 Small Business Dashboard Design Dos and Don'ts'' (Barr, 2010)</ref>

==Assessing the quality of dashboards==
There are four key elements to a good dashboard:.<ref>Victoria Hetherington, ''Dashboard Demystified: What is a Dashboard?'' (Hetherington, 2009)</ref>
# Simple, communicates easily
# Minimum distractions...it could cause confusion
# Supports organized business with meaning and useful data
# Applies human visual perception to visual presentation of information

==History==

The idea of digital dashboards followed the study of [[decision support system]]s in the 1970s. Early predecessors of the modern business dashboard were first developed in the 1980s in the form of [[Executive Information Systems]] (EISs). Due to problems primarily with data refreshing and handling, it was soon realized that the approach wasn’t practical as information was often incomplete, unreliable, and spread across too many disparate sources.<ref>Steven Few, ''Information Dashboard Design: The Effective Visual Communication of Data'' (O'Reilly, 2006)</ref> Thus, EISs hibernated until the 1990s when the information age quickened pace and data warehousing, and [[online analytical processing]] (OLAP) allowed dashboards to function adequately.{{fact|date=August 2014}} Despite the availability of enabling technologies, the dashboard use didn't become popular until later in that decade, with the rise of [[key performance indicators]] (KPIs), and the introduction of Robert S. Kaplan and David P. Norton's [[Balanced Scorecard]].<ref>[[Wayne W. Eckerson]], ''Performance Dashboards: Measuring, Monitoring, and Managing Your Business'' (Wiley , 2010)</ref> In the late 1990s, [[Microsoft]] promoted a concept known as the [[Digital Nervous System]] and "digital dashboards" were described as being one leg of that concept.<ref>{{cite web | url=http://www.kmworld.com/Articles/News/Breaking-News/Microsoft-refines-Digital-Dashboard-concept--12189.aspx | title=Microsoft refines Digital Dashboard concept | accessdate=2009-06-09}}</ref> Today, the use of dashboards forms an important part of Business Performance Management (BPM).

==See also==
* [[Business activity monitoring]]
* [[Complex event processing]]
* [[Corporate performance management]]
* [[Data presentation architecture]]
* [[Enterprise manufacturing intelligence]]
* [[Event stream processing]]
* [[Infographic|Information graphics]]
* [[Information design]]
* [[Scientific visualization]]

==References==
{{reflist}}

==Further reading==
* {{cite book
  |title=Information Dashboard Design
  |last=Few   |first=Stephen
  |publisher=O'Reilly
  |isbn=978-0-596-10016-2
  |date=2006
}}

* {{cite book
  |title=Performance Dashboards: Measuring, Monitoring, and Managing Your Business
  |last=Eckerson|first=Wayne W |author-link=
  |publisher=John Wiley & Sons
  |isbn=978-0-471-77863-9
  |date=2006
}}

{{Data warehouse}}

{{DEFAULTSORT:Dashboard (Business)}}
{{Use dmy dates|date=April 2011}}
[[Category:Business terms]]
[[Category:Computing terminology]]
[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Business software]]
[[Category:Information systems]]
<=====doc_Id=====>:198
<=====title=====>:
Data custodian
<=====text=====>:
{{more footnotes|date=August 2010}}
{{merge|Data steward|date=February 2016}}
In [[Data governance|Data Governance]] groups, responsibilities for data management are increasingly divided between the business process owners and information technology (IT) departments.  Two functional titles commonly used for these roles are [[Data steward|Data Steward]] and Data Custodian. 

Data Stewards are commonly responsible for data content, context, and associated business rules. Data Custodians are responsible for the safe custody, transport, storage of the data and implementation of business rules.<ref>Carnegie Mellon - Information Security Roles and Responsibilities, http://www.cmu.edu/iso/governance/roles/data-custodian.html</ref><ref>''Policies, Regulations and Rules: Data Management Procedures - REG 08.00.3 - Information Technology'', , NC State University, http://www.ncsu.edu/policies/informationtechnology/REG08.00.3.php</ref> Simply put, Data Stewards are responsible for what is stored in a data field, while Data Custodians are responsible for the technical environment and database structure. Common job titles for data custodians are Database Administrator (DBA), Data Modeler, and ETL Developer.

==Data Custodian Responsibilities==
A data custodian ensures:
# Access to the data is authorized and controlled
# Data stewards are identified for each data set
# Technical processes sustain data integrity
# Processes exist for data quality issue resolution in partnership with Data Stewards
# Technical controls safeguard data
# Data added to data sets are consistent with the common data model
# Versions of Master Data are maintained along with the history of changes
# Change management practices are applied in maintenance of the database
# Data content and changes can be audited

==See also==
* [[Data governance]]
* [[Data steward]]

==References==
<references></references>

==Related Links==
* ''Establishing data stewards'', by Jonathan G. Geiger, Teradata Magazine Online, September 2008, http://www.teradata.com/tdmo/v08n03/Features/EstablishingDataStewards.aspx
* '' A Rose By Any Other Name – Titles In Data Governance'', by Anne Marie Smith, Ph.D., EIMInstitute.ORG Archives, Volume 1, Issue 13, March 2008, http://www.eiminstitute.org/library/eimi-archives/volume-1-issue-13-march-2008-edition/a-rose-by-any-other-name-2013-titles-in-data-governance

{{DEFAULTSORT:Data Custodian}}
[[Category:Information technology governance]]
[[Category:Data management]]
[[Category:Knowledge representation]]
[[Category:Library occupations]]
[[Category:Metadata]]
[[Category:Technical communication]]

[[ar:ميتاداتا]]
[[cs:Metadata]]
[[da:Metadata]]
[[de:Data Steward]]
[[et:Metaandmed]]
[[es:Metadato]]
[[eo:Meta-dateno]]
[[fr:Métadonnée]]
[[it:Metadata]]
[[lv:Metadati]]
[[hu:Metaadat]]
[[nl:Metadata]]
[[ja:メタデータ]]
[[no:Metadata]]
[[pl:Metadane]]
[[pt:Metadados]]
[[ru:Метаданные]]
[[fi:Metatieto]]
[[sv:Metadata]]
[[th:เมทาดาตา]]
[[vi:Metadata]]
<=====doc_Id=====>:201
<=====title=====>:
Category:Storage area networks
<=====text=====>:
{{Commons category|Storage area networks}}
{{See also category|Computer storage buses}}
{{cat main|Storage area network}}

[[Category:Computer data storage]]
[[Category:Local area networks]]
[[Category:Data management]]
[[Category:Storage virtualization]]
<=====doc_Id=====>:204
<=====title=====>:
Category:Database theory
<=====text=====>:
{{Cat main|database theory}}

[[Category:Areas of computer science]]
[[Category:Databases|Theory]]
[[Category:Data management|Theory]]
<=====doc_Id=====>:207
<=====title=====>:
Storage model
<=====text=====>:
{{Unreferenced|date=December 2006}}
A '''storage model''' is a model that captures key ''physical'' aspects of data structure in a data store. 

On the other hand, a [[data model]] is a model that captures key ''logical'' aspects of data structure in a database.



[[Category:Data management]]


{{Compu-storage-stub}}
<=====doc_Id=====>:210
<=====title=====>:
Category:Data analysis
<=====text=====>:
{{Commons category|Data analysis}}
{{cat main|Data analysis}}

[[Category:Analysis]]
[[Category:Data management|Analysis]]
<=====doc_Id=====>:213
<=====title=====>:
Category:Data quality
<=====text=====>:
{{Cat main|Data quality}}
:''See also:'' [[:category:Data security]] ([[data loss]] prevention is in fact an assurance of data quality)


[[Category:Data management|Quality]]
[[Category:Quality]]
<=====doc_Id=====>:216
<=====title=====>:
Master data management
<=====text=====>:
{{refimprove|date=April 2012}}
In business, '''master data management''' ('''MDM''') comprises the processes, governance, policies, standards and tools that consistently define and manage the critical data of an [[organization]] to provide a single point of reference.<ref>"What is Master Data" SearchDataManagement, TechTarget, 22 November 2010, http://searchdatamanagement.techtarget.com/definition/master-data-management</ref>

The data that is mastered may include:

* [[reference data]] &ndash; the business objects for transactions, and the dimensions for analysis
* analytical data &ndash; supports decision making<ref>"Introduction to Master Data Management", Mark Rittman, Director, Rittman Mead Consulting, 9 May 2008 https://s3.amazonaws.com/rmc_docs/Introduction%20to%20Oracle%20Master%20Data%20Management.pdf</ref><ref>"[http://www.b-eye-network.com/view/2918 "Defining Master Data"], David Loshin, BeyeNetwork, May 2006</ref>

In [[computing]], a master data management tool can be used to support master data management by removing duplicates, standardizing data (mass maintaining), and incorporating rules to eliminate incorrect data from entering the system in order to create an authoritative source of master data. Master data are the products, accounts and parties for which the business transactions are completed. The root cause problem stems from business unit and product line segmentation, in which the same customer will be serviced by different product lines, with redundant data being entered about the customer (a.k.a. party in the role of customer) and account in order to process the transaction. The redundancy of party and account data is compounded in the front to back office life cycle, where the authoritative single source for the party, account and product data is needed but is often once again redundantly entered or augmented.

Master data management has the objective of providing processes for collecting, aggregating, matching, consolidating, quality-assuring, persisting and distributing such data throughout an organization to ensure consistency and control in the ongoing maintenance and application use of this information.

The term recalls the concept of a ''master file'' from an earlier computing era.

==Definition==
Master data management (MDM) is a comprehensive method of enabling an enterprise to link all of its critical data to one file, called a master file, that provides a common point of reference. When properly done, master data management streamlines data sharing among personnel and departments. In addition, master data management can facilitate computing in multiple system architectures, platforms and applications.<ref>{{cite web|title=Master data management|url=http://www.ibm.com/software/data/master-data-management/overview.html|publisher=[[IBM]]}}</ref>

At its core Master Data Management (MDM) can be viewed as a "discipline for specialized quality improvement"<ref>DAMA-DMBOK Guide,2010 DAMA International</ref> defined by the policies and procedures put in place by a data governance organization.  The ultimate goal being to provide the end user community with a "trusted single version of the truth" from which to base decisions.

==Issues==
At a basic level, master data management seeks to ensure that an organization does not use multiple (potentially [[Consistency (database systems)|inconsistent]]) versions of the same master data in different parts of its operations, which can occur in large organizations. A typical example of poor master data management is the scenario of a bank at which a [[customer]] has taken out a [[Mortgage loan|mortgage]] and the bank begins to send mortgage solicitations to that customer, ignoring the fact that the person already has a mortgage account relationship with the bank. This happens because the customer information used by the marketing section within the bank lacks integration with the customer information used by the customer services section of the bank.  Thus the two groups remain unaware that an existing customer is also considered a sales lead. The process of [[record linkage]] is used to associate different records that correspond to the same entity, in this case the same person.

Other problems include (for example) issues with the [[data quality|quality of data]], consistent [[classification]] and identification of data, and [[Data validation and reconciliation|data-reconciliation]] issues.  Master data management of disparate data systems requires [[data transformation]]s as the data extracted from the disparate source data system is transformed and loaded into the master data management hub.  To synchronize the disparate source master data, the managed master data extracted from the master data management hub is again transformed and loaded into the disparate source data system as the master data is updated.  As with other [[Extract, Transform, Load]]-based data movement, these processes are expensive and inefficient to develop and to maintain which greatly reduces the [[return on investment]] for the master data management product.

One of the most common reasons some large corporations experience massive issues with master data management is growth through [[merger]]s or [[Takeover|acquisitions]].  Any organizations which merge will typically create an entity with duplicate master data (since each likely had at least one master database of its own prior to the merger).  Ideally, [[database administrator]]s resolve this problem through [[Data deduplication|deduplication]] of the master data as part of the merger. In practice, however, reconciling several master data systems can present difficulties because of the dependencies that existing applications have on the master databases.  As a result, more often than not the two systems do not fully merge, but remain separate, with a special reconciliation process defined that ensures consistency between the data stored in the two systems.  Over time, however, as further mergers and acquisitions occur, the problem multiplies, more and more master databases appear, and data-reconciliation processes become extremely complex, and consequently unmanageable and unreliable. Because of this trend, one can find organizations with 10, 15, or even as many as 100 separate, poorly integrated master databases, which can cause serious operational problems in the areas of [[customer satisfaction]], operational efficiency, [[decision support]], and regulatory compliance.

Another problem concerns determining the proper degree of detail and normalization to include in the master data schema. For example, in a federated HR environment, the enterprise may focus on storing people data as a current status, adding a few fields to identify date of hire, date of last promotion, etc. However this simplification can introduce business impacting errors into dependent systems for planning and forecasting. The stakeholders of such systems may be forced to build a parallel network of new interfaces to track onboarding of new hires, planned retirements, and divestment, which works against one of the aims of master data management.  
==Solutions==
Processes commonly seen in master data management include source identification, data collection, [[data transformation]], [[database normalization|normalization]], rule administration, error detection and correction, data consolidation, [[data storage device|data storage]], data distribution, data classification, taxonomy services, item master creation, schema mapping, product codification, data enrichment and [[data governance]].

The selection of entities considered for master data management depends somewhat on the nature of an organization. In the common case of commercial enterprises, master data management may apply to such entities as customer ([[customer data integration]]), product ([[product information management]]), employee, and vendor. Master data management processes identify the sources from which to collect descriptions of these entities. In the course of transformation and normalization, administrators adapt descriptions to conform to standard formats and data domains, making it possible to remove duplicate instances of any entity. Such processes generally result in an organizational master data management repository, from which  all requests for a certain entity instance produce the same description, irrespective of the originating sources and the requesting destination.

The tools include [[data networks]], [[file systems]], a [[data warehouse]], [[data mart]]s, an [[operational data store]], [[data mining]], [[data analysis]], [[data visualization]], [[Federated database system|data federation]] and [[data virtualization]]. One of the newest tools, virtual master data management utilizes data virtualization and a persistent metadata server to implement a multi-level automated master data management hierarchy.

==Transmission of master data==
There are several ways in which master data may be collated and distributed to other systems.<ref>[http://dama-ny.com/images/meeting/101509/damanyc_mdmprint.pdf "Creating the Golden Record: Better Data Through Chemistry"], DAMA, slide 26, Donald J. Soulsby, 22 October 2009</ref> This includes:

* Data consolidation – The process of capturing master data from multiple sources and integrating into a single hub ([[operational data store]]) for replication to other destination systems.
* [[Federated database system|Data federation]] – The process of providing a single virtual view of master data from one or more sources to one or more destination systems.
* Data propagation – The process of copying master data from one system to another, typically through point-to-point interfaces in legacy systems.

==See also==
* [[Reference data]]
* [[Master data]]
* [[Record linkage]]
* [[Data steward]]
* [[Data visualization]]
* [[Customer data integration]]
* [[Data integration]]
* [[Product information management]]
* [[Identity resolution]]
* [[Enterprise information integration]]
* [[Linked data]]
* [[Semantic Web]]
* [[Data governance]]
* [[Operational data store]]
* [[Single customer view]]

==References==
{{reflist}}

==External links==
* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 Microsoft: The What, Why, and How of Master Data Management]
* [http://msdn.microsoft.com/en-us/library/bb410798.aspx Microsoft: Master Data Management (MDM) Hub Architecture]
* [http://mike2.openmethodology.org/wiki/Master_Data_Management_Solution_Offering Open Methodology for Master Data Management]
* [http://www.semarchy.com/overview/why-do-i-need-mdm/ Semarchy: Why do I Need MDM? (Video)]
* [http://www.mdmalliancegroup.com/ MDM Community]
* [http://www.stibosystems.com/Global/explore-stibo-systems/master-data-management.aspx Multidomain Master Data Management]
* [http://blogs.gartner.com/andrew_white/2014/06/05/reprise-when-is-master-data-and-mdm-not-master-data-or-mdm/ Reprise: When is Master Data and MDM Not Master Data or MDM?]
* [http://www.orchestranetworks.com/mdm/ Master Data Management (Multidomain)]

{{Data warehouse}}
{{databases}}

{{DEFAULTSORT:Master Data Management}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]
<=====doc_Id=====>:219
<=====title=====>:
Log trigger
<=====text=====>:
In [[relational database]]s, the '''Log trigger''' or '''History trigger''' is a mechanism for automatic recording of information about changes inserting or/and updating or/and deleting [[Row (database)|rows]] in a [[Table (database)|database table]].

It is a particular technique for [[Change data capture|change data capturing]], and in [[data warehousing]] for dealing with [[slowly changing dimension]]s.

== Definition ==

Suppose there is a [[Table (database)|table]] which we want to audit. This [[Table (database)|table]] contains the following [[Column (database)|columns]]:

<code>Column1, Column2, ..., Columnn</code>

The [[Column (database)|column]] <code>Column1</code> is assumed to be the [[primary key]].

These [[Column (database)|columns]] are defined to have the following types:

<code>Type1, Type2, ..., Typen</code>

The '''Log Trigger''' works writing the changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) on the [[Table (database)|table]] in another, '''history table''', defined as following:

<syntaxhighlight lang="sql">
CREATE TABLE HistoryTable (
   Column1   Type1,
   Column2   Type2,
      :        :
   Columnn   Typen,

   StartDate DATETIME,
   EndDate   DATETIME
)
</syntaxhighlight>

As shown above, this new [[Table (database)|table]] contains the same [[Column (database)|columns]] as the original [[Table (database)|table]], and additionally two new [[Column (database)|columns]] of type <code>DATETIME</code>: <code>StartDate</code> and <code>EndDate</code>. This is known as [[Tuple-versioning|tuple versioning]]. These two additional [[Column (database)|columns]] define a period of time of "validity" of the data associated with a specified entity (the entity of the [[primary key]]), or in other words, it stores how the data were in the period of time between the <code>StartDate</code> (included) and <code>EndDate</code> (not included).

For each entity (distinct [[primary key]]) on the original [[Table (database)|table]], the following structure is created in the history [[Table (database)|table]]. Data is shown as example.

[[File:example log trigger.png|center|example]]

Notice that if they are shown chronologically the <code>EndDate</code> [[Column (database)|column]] of any [[Row (database)|row]] is exactly the <code>StartDate</code> of its successor (if any). It does not mean that both [[Row (database)|rows]] are common to that point in time, since -by definition- the value of <code>EndDate</code> is not included.

There are two variants of the '''Log trigger''', depending how the old values (DELETE, UPDATE) and new values (INSERT, UPDATE) are exposed to the trigger (it is RDBMS dependent):

'''Old and new values as fields of a record data structure'''

<syntaxhighlight lang="sql">
CREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS
DECLARE @Now DATETIME
SET @Now = GETDATE()

/* deleting section */

UPDATE HistoryTable
   SET EndDate = @Now
 WHERE EndDate IS NULL
   AND Column1 = OLD.Column1

/* inserting section */

INSERT INTO HistoryTable (Column1, Column2, ...,Columnn, StartDate, EndDate) 
VALUES (NEW.Column1, NEW.Column2, ..., NEW.Columnn, @Now, NULL)
</syntaxhighlight>

'''Old and new values as rows of virtual tables'''

<syntaxhighlight lang="sql">
CREATE TRIGGER HistoryTable ON OriginalTable FOR INSERT, DELETE, UPDATE AS
DECLARE @Now DATETIME
SET @Now = GETDATE()

/* deleting section */

UPDATE HistoryTable
   SET EndDate = @Now
  FROM HistoryTable, DELETED
 WHERE HistoryTable.Column1 = DELETED.Column1
   AND HistoryTable.EndDate IS NULL

/* inserting section */

INSERT INTO HistoryTable
       (Column1, Column2, ..., Columnn, StartDate, EndDate)
SELECT (Column1, Column2, ..., Columnn, @Now, NULL)
  FROM INSERTED
</syntaxhighlight>

=== Compatibility notes ===

* The function <code>GetDate()</code> is used to get the system date and time, a specific [[Relational database management system|RDBMS]] could either use another function name, or get this information by another way.
* Several [[Relational database management system|RDBMS]] (DB2, MySQL) do not support that the same trigger can be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]). In such a case a trigger must be created for each operation; For an [[Insert (SQL)|INSERT]] operation only the ''inserting section'' must be specified, for a [[Delete (SQL)|DELETE]] operation only the ''deleting section'' must be specified, and for an [[Update (SQL)|UPDATE]] operation both sections must be present, just as it is shown above (the ''deleting section'' first, then the ''inserting section''), because an [[Update (SQL)|UPDATE]] operation is logically represented as a [[Delete (SQL)|DELETE]] operation followed by an [[Insert (SQL)|INSERT]] operation.
* In the code shown, the record data structure containing the old and new values are called <code>OLD</code> and <code>NEW</code>. On a specific [[Relational database management system|RDBMS]] they could have different names.
* In the code shown, the virtual tables are called <code>DELETED</code> and <code>INSERTED</code>. On a specific [[Relational database management system|RDBMS]] they could have different names. Another [[Relational database management system|RDBMS]] (DB2) even let the name of these logical tables be specified.
* In the code shown, comments are in C/C++ style, they could not be supported by a specific [[Relational database management system|RDBMS]], or a different syntax should be used.
* Several [[Relational database management system|RDBMS]] require that the body of the trigger is enclosed between <code>BEGIN</code> and <code>END</code> keywords.

=== [[Data warehousing]] ===

According with the [[slowly changing dimension]] management methodologies, The '''log trigger''' falls into the following:

* [[Slowly changing dimension#Type 2|Type 2]] ([[Tuple-versioning|tuple versioning]] variant)
* [[Slowly changing dimension#Type 4|Type 4]] (use of history tables)

== Implementation in common [[RDBMS]] ==

=== [[IBM DB2]]<ref>"Database Fundamentals" by Nareej Sharma et al. (First Edition, Copyright IBM Corp. 2010)</ref> ===

* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.
* The old and new values are exposed as fields of a record data structures. The names of these records can be defined, in this example they are named as <code>O</code> for old values and <code>N</code> for new values.

<syntaxhighlight lang="sql">
-- Trigger for INSERT
CREATE TRIGGER Database.TableInsert AFTER INSERT ON Database.OriginalTable
REFERENCING NEW AS N
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);
END;

-- Trigger for DELETE
CREATE TRIGGER Database.TableDelete AFTER DELETE ON Database.OriginalTable
REFERENCING OLD AS O
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   UPDATE Database.HistoryTable
      SET EndDate = Now
    WHERE Column1 = O.Column1
      AND EndDate IS NULL;
END;

-- Trigger for UPDATE
CREATE TRIGGER Database.TableUpdate AFTER UPDATE ON Database.OriginalTable
REFERENCING NEW AS N OLD AS O
FOR EACH ROW MODE DB2SQL
BEGIN
   DECLARE Now TIMESTAMP;
   SET NOW = CURRENT TIMESTAMP;

   UPDATE Database.HistoryTable
      SET EndDate = Now
    WHERE Column1 = O.Column1
      AND EndDate IS NULL;

   INSERT INTO Database.HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (N.Column1, N.Column2, ..., N.Columnn, Now, NULL);
END;
</syntaxhighlight>

=== [[Microsoft SQL Server]]<ref>"Microsoft SQL Server 2008 - Database Development" by Thobias Thernström et al. (Microsoft Press, 2009)</ref> ===

* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.
* Old and new values as rows of virtual tables named <code>DELETED</code> and <code>INSERTED</code>.

<syntaxhighlight lang="sql">
CREATE TRIGGER TableTrigger ON OriginalTable FOR DELETE, INSERT, UPDATE AS

DECLARE @NOW DATETIME
SET @NOW = CURRENT_TIMESTAMP

UPDATE HistoryTable
   SET EndDate = @now
  FROM HistoryTable, DELETED
 WHERE HistoryTable.ColumnID = DELETED.ColumnID
   AND HistoryTable.EndDate IS NULL

INSERT INTO HistoryTable (ColumnID, Column2, ..., Columnn, StartDate, EndDate)
SELECT ColumnID, Column2, ..., Columnn, @NOW, NULL
  FROM INSERTED
</syntaxhighlight>

=== [[MySQL]] ===

* A trigger cannot be attached to more than one operation ([[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], [[Update (SQL)|UPDATE]]), so a trigger must be created for each operation.
* The old and new values are exposed as fields of a record data structures called <code>Old</code> and <code>New</code>.

<syntaxhighlight lang="sql">
DELIMITER $$

/* Trigger  for INSERT */
CREATE TRIGGER HistoryTableInsert AFTER INSERT ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();
    
   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);
END;

/* Trigger for DELETE */
CREATE TRIGGER HistoryTableDelete AFTER DELETE ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();
    
   UPDATE HistoryTable
      SET EndDate = N
    WHERE Column1 = OLD.Column1
      AND EndDate IS NULL;
END;

/* Trigger for UPDATE */
CREATE TRIGGER HistoryTableUpdate AFTER UPDATE ON OriginalTable FOR EACH ROW BEGIN
   DECLARE N DATETIME;
   SET N = now();

   UPDATE HistoryTable
      SET EndDate = N
    WHERE Column1 = OLD.Column1
      AND EndDate IS NULL;

   INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate)
   VALUES (New.Column1, New.Column2, ..., New.Columnn, N, NULL);
END;
</syntaxhighlight>

=== [[Oracle Database|Oracle]] ===

* The same trigger can be attached to all the [[Insert (SQL)|INSERT]], [[Delete (SQL)|DELETE]], and [[Update (SQL)|UPDATE]] operations.
* The old and new values are exposed as fields of a record data structures called <code>:OLD</code> and <code>:NEW</code>.
* It is necessary to test the nullity of the fields of the <code>:NEW</code> record that define the [[primary key]] (when a [[Delete (SQL)|DELETE]] operation is performed), in order to avoid the insertion of a new row with null values in all columns.

<syntaxhighlight lang="sql">
CREATE OR REPLACE TRIGGER TableTrigger
AFTER INSERT OR UPDATE OR DELETE ON OriginalTable
FOR EACH ROW
DECLARE Now TIMESTAMP;
BEGIN
   SELECT CURRENT_TIMESTAMP INTO Now FROM Dual;

   UPDATE HistoryTable
      SET EndDate = Now
    WHERE EndDate IS NULL
      AND Column1 = :OLD.Column1;

   IF :NEW.Column1 IS NOT NULL THEN
      INSERT INTO HistoryTable (Column1, Column2, ..., Columnn, StartDate, EndDate) 
      VALUES (:NEW.Column1, :NEW.Column2, ..., :NEW.Columnn, Now, NULL);
   END IF;
END;
</syntaxhighlight>

== Historic information ==

Typically, [[Database dump|database backups]] are used to store and retrieve historic information. A [[Database dump|database backup]] is a security mechanism, more than an effective way to retrieve ready-to-use historic information.

A (full) [[Database dump|database backup]] is only a snapshot of the data in specific points of time, so we could know the information of each snapshot, but we can know nothing between them. Information in [[Database dump|database backups]] is discrete in time.

Using the '''log trigger''' the information we can know is not discrete but continuous, we can know the exact state of the information in any point of time, only limited to the granularity of time provided with the <code>DATETIME</code> data type of the [[Relational database management system|RDBMS]] used.

== Advantages ==

* It is simple.
* It is not a commercial product, it works with available features in common [[Relational database management system|RDBMS]].
* It is automatic, once it is created, it works with no further human intervention.
* It is not required to have good knowledge about the tables of the database, or the data model.
* Changes in current programming are not required.
* Changes in the current [[Table (database)|tables]] are not required, because log data of any [[Table (database)|table]] is stored in a different one.
* It works for both programmed and ad hoc statements.
* Only changes ([[Insert (SQL)|INSERT]], [[Update (SQL)|UPDATE]] and [[Delete (SQL)|DELETE]] operations) are registered, so the growing rate of the history tables are proportional to the changes.
* It is not necessary to apply the trigger to all the tables on database, it can be applied to certain [[Table (database)|tables]], or certain [[Column (database)|columns]] of a [[Table (database)|table]].

== Disadvantages ==

* It does not automatically store information about the user producing the changes (information system user, not database user). This information might be provided explicitly. It could be enforced in information systems, but not in ad hoc queries.

== Examples of use ==

=== Getting the current version of a table ===

<syntaxhighlight lang="sql">
SELECT Column1, Column2, ..., Columnn
  FROM HistoryTable
 WHERE EndDate IS NULL
</syntaxhighlight>

It should return the same resultset of the whole original [[Table (database)|table]].

=== Getting the version of a table in a certain point of time ===

Suppose the <code>@DATE</code> variable contains the point or time of interest.

<syntaxhighlight lang="sql">
SELECT  Column1, Column2, ..., Columnn
  FROM  HistoryTable
 WHERE  @Date >= StartDate
   AND (@Date < EndDate OR EndDate IS NULL)
</syntaxhighlight>

=== Getting the information of an entity in a certain point of time ===

Suppose the <code>@DATE</code> variable contains the point or time of interest, and the <code>@KEY</code> variable contains the [[primary key]] of the entity of interest.

<syntaxhighlight lang="sql">
SELECT  Column1, Column2, ..., Columnn
  FROM  HistoryTable
 WHERE  Column1 = @Key
   AND  @Date >= StartDate
   AND (@Date <  EndDate OR EndDate IS NULL)
</syntaxhighlight>

=== Getting the history of an entity ===

Suppose the <code>@KEY</code> variable contains the [[primary key]] of the entity of interest.

<syntaxhighlight lang="sql">
SELECT Column1, Column2, ..., Columnn, StartDate, EndDate
  FROM HistoryTable
 WHERE Column1 = @Key
 ORDER BY StartDate
</syntaxhighlight>

=== Getting when and how an entity was created ===

Suppose the <code>@KEY</code> variable contains the [[primary key]] of the entity of interest.

<syntaxhighlight lang="sql">
SELECT H2.Column1, H2.Column2, ..., H2.Columnn, H2.StartDate
  FROM HistoryTable AS H2 LEFT OUTER JOIN HistoryTable AS H1
    ON H2.Column1 = H1.Column1
   AND H2.Column1 = @Key
   AND H2.StartDate = H1.EndDate
 WHERE H2.EndDate IS NULL
</syntaxhighlight>

== Immutability of [[primary key]]s ==

Since the trigger requires that [[primary key]] being the same throughout time, it is desirable to either ensure or maximize its immutability, if a [[primary key]] changed its value, the entity it represents would break its own history.

There are several options to achieve or maximize the [[primary key]] immutability:

* Use of a [[Surrogate Key|surrogate key]] as a [[primary key]]. Since there is no reason to change a value with no meaning other than identity and uniqueness, it would never change.
* Use of an immutable [[natural key]] as a [[primary key]]. In a good database design, a [[natural key]] which can change should not be considered as a "real" [[primary key]].
* Use of a mutable [[natural key]] as a [[primary key]] (it is widely discouraged) where changes are propagated in every place where it is a [[foreign key]]. In such a case, the history table should be also affected.

=== Alternatives ===

Sometimes the [[Slowly changing dimension]] is used as a method, this diagram is an example:
[[File:Scd model.png|frame|right|Scd model]]

== See also ==

* [[RDBMS|Relational database]]
* [[Primary key]]
* [[Natural key]]
* [[Surrogate key]]
* [[Change data capture]]
* [[Slowly changing dimension]]
* [[Tuple-versioning|Tuple versioning]]

== Notes ==

The Log trigger was written by [[Laurence Ruiz Ugalde|Laurence R. Ugalde]] to automatically generate history of transactional databases.

==References==
<references />

{{DEFAULTSORT:Log Trigger}}
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Data warehousing]]
<=====doc_Id=====>:222
<=====title=====>:
Australian National Data Service
<=====text=====>:
The '''Australian National Data Service''' (ANDS) was established in 2008 to help address the challenges of storing and managing Australia's research data, and making it discoverable and accessible for validation and reuse. It is a joint collaboration between [[Monash University]], [[The Australian National University]] and [[CSIRO]].

==Background==
ANDS is funded by the [[Australian Department of Education]]. The funding has been provided through Australian Government's National Collaborative Research Infrastructure Strategy (NCRIS) as part of the Platforms for Collaboration Investment Plan.<ref>{{cite web|url=http://ncris.innovation.gov.au/Capabilities/Pages/PfC.aspx#ANDS |title=Platforms for Collaboration |accessdate=2011-06-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20110702094824/http://ncris.innovation.gov.au:80/Capabilities/Pages/PfC.aspx |archivedate=2011-07-02 |df= }}</ref> The NCRIS roadmap emphasized the vital importance of eResearch Infrastructure to Australian future research competitiveness.<ref>{{cite web|title=eResearch Infrastructure |url=http://www.pfc.org.au/bin/view/Main |publisher=NCRIS |accessdate=25 June 2011 }}{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> In mid-2009 ANDS was further funded by the Education Investment Fund (EIF) for the establishment of the Australian Research Data Commons under the Australian Government’s Super Science Initiative.<ref>{{cite web|title=Super Science Initiative |url=http://www.innovation.gov.au/SCIENCE/RESEARCHINFRASTRUCTURE/Pages/SuperScience.aspx |publisher=DIISR |accessdate=25 June 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110601193908/http://www.innovation.gov.au/Science/ResearchInfrastructure/Pages/SuperScience.aspx |archivedate=1 June 2011 |df= }}</ref>

==Research Data Australia==
''Research Data Australia'' (formerly the ''ANDS Collections Registry'') is an online discovery service run by ANDS.<ref>[http://researchdata.ands.org.au/home/about Research Data Australia]</ref><ref>[http://www.ands.org.au/resource/registry.html ANDS Collections Registry] {{webarchive |url=https://web.archive.org/web/20140302113241/http://www.ands.org.au/resource/registry.html |date=March 2, 2014 }}</ref> It allows researchers to publicise the existence of their research data and enable prospective users of that data to find it.

''Research Data Australia'' makes use of the [[ISO 2146]]-based [[RIF-CS]] metadata standard.<ref>[http://ands.org.au/guides/cpguide/cpgrifcs.html About RIF-CS]</ref>

== External links ==
* {{Official website|http://www.ands.org.au}}
* [http://researchdata.ands.org.au Research Data Australia]

==References==
<references />

[[Category:Data management]]


{{Australia-org-stub}}
<=====doc_Id=====>:225
<=====title=====>:
Cloud Data Management Interface
<=====text=====>:
{{Infobox standardref
| title             = Cloud Data Management Interface
| status            = Published
| year_started      = 2009
| version           = 1.1.1
| organization      = [[Storage Networking Industry Association]]
| base_standards    = [[Hypertext Transfer Protocol]]
| related_standards = [[Network File System]]
| abbreviation      = CDMI
| domain            = [[Cloud computing]]
| license           = 
| website           = [http://www.snia.org/cloud CDMI Technical Working Group]
}}

The '''Cloud Data Management Interface''' ('''CDMI''') is a [http://www.snia.org SNIA] standard that specifies a protocol for self-provisioning, administering and accessing [[cloud storage]].<ref>{{cite web|title=Cloud Data Management Interface|url=http://www.snia.org/cdmi|publisher=SNIA|accessdate=26 June 2011}}</ref>

CDMI defines [[REST]]ful [[HTTP]] operations for assessing the capabilities of the cloud storage system, allocating and accessing containers and objects, managing users and groups, implementing access control, attaching metadata, making arbitrary queries, using persistent queues, specifying retention intervals and holds for compliance purposes, using a logging facility, billing, moving data between cloud systems, and exporting data via other protocols such as [[iSCSI]] and [[Network File System (protocol)|NFS]]. Transport security is obtained via [[Transport Layer Security|TLS]].

==Capabilities==
Compliant implementations must provide access to a set of configuration parameters known as ''capabilities''.
These are either boolean values that represent whether or not a system supports things such as queues, export via other protocols, path-based storage and so on, or numeric values expressing system limits, such as how much metadata may be placed on an object.  As a minimal compliant implementation can be quite small, with few features, clients need to check the cloud storage system for a capability before attempting to use the functionality it represents.

==Containers==
A CDMI client may access objects, including containers, by either name or object id (OID), assuming the CDMI server supports both methods.  When storing objects by name, it is natural to use nested named containers; the resulting structure corresponds exactly to a traditional filesystem directory structure.

==Objects==
Objects are similar to files in a traditional file system, but are enhanced with an increased amount of and capacity for [[metadata]].  As with containers, they may be accessed by either name or OID.  When accessed by name, clients use [[Uniform Resource Locator|URLs]] that contain the full pathname of objects to [[create, read, update and delete]] them. When accessed by OID, the URL specifies an OID string in the '''cdmi-objectid''' container; this container presents a flat name space conformant with standard object storage system semantics.

Subject to system limits, objects may be of any size or type and have arbitrary user-supplied metadata attached to them. Systems that support query allow arbitrary queries to be run against the metadata.

==Domains, Users and Groups==
CDMI supports the concept of a ''domain'', similar in concept to a domain in the [[Windows]] [[Active Directory]] model. Users and groups created in a domain share a common administrative database and are known to each other on a "first name" basis, i.e. without reference to any other domain or system.

Domains also function as containers for usage and billing summary data.

==Access Control==
CDMI exactly follows the [[Access Control List|ACL]] and [[Access Control Entry|ACE]] model used for file authorization operations by [[NFSv4#NFSv4|NFSv4]]. This makes it also compatible with [[Microsoft Windows]] systems.

==Metadata==
CDMI draws much of its metadata model from the [[XAM]] specification. Objects and containers have "storage system metadata", "data system metadata" and arbitrary user specified metadata, in addition to the metadata maintained by an ordinary filesystem (atime etc.).

==Queries==
CDMI specifies a way for systems to support arbitrary queries against CDMI containers, with a rich set of comparison operators, including support for [[regular expression]]s.

==Queues==
CDMI supports the concept of persistent [[FIFO (computing and electronics)|FIFO]] (first-in, first-out) queues. These are useful for job scheduling,  order processing and other tasks in which lists of things must be processed in order.

==Compliance==
Both retention intervals and retention holds are supported by CDMI.  A retention interval consists of a start time and a retention period.  During this time interval, objects are preserved as immutable and may not be deleted. A retention hold is usually placed on an object because of judicial action and has the same effect: objects may not be changed nor deleted until all holds placed on them are removed.

==Logging==
CDMI clients can sign up for logging of system, security and object access events on servers that support it.  This feature allows clients to see events locally as the server logs them.

==Billing==
Summary information suitable for billing clients for on-demand services can be obtained by authorized users from systems that support it.

==Serialization==
Serialization of objects and containers allows export of all data and metadata on a system and importation of that data into another cloud system.

==Foreign Protocols==
CDMI supports export of containers as NFS or CIFS shares.  Clients that mount these shares see the container hierarchy as an ordinary filesystem directory hierarchy, and the objects in the containers as normal files. Metadata outside of ordinary filesystem metadata may or may not be exposed.

Provisioning of iSCSI LUNs is also supported.

== Client SDKs ==
* [http://www.snia.org/forums/csi/programs/CDMIportal CDMI Reference Implementation]
* [https://github.com/scality/Droplet Droplet]
* [https://github.com/livenson/libcdmi-java libcdmi-java]
* [https://github.com/livenson/libcdmi-python libcdmi-python]
* [https://github.com/projectpvg1/.net-SDK .NET SDK]

== See also ==
[[Comparison of CDMI server implementations]]

== References ==
{{reflist}}

== External links ==
* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=40874 ISO-8601]  International Organization for Standardization, "Data elements and interchange formats -- Information interchange -- Representation of dates and times”, ISO 8601:20044
* [http://www.itu.int/ITU-T/publications/recs.html ITU-T509]  International Telecommunications Union Telecommunication Standardization Sector (ITU-T), Recommendation X.509: Information technology - Open Systems Interconnection - The Directory: Public-key and attribute certificate frameworks, May 2000. Specification and technical corrigenda -
* [http://www.unix.org/version3/ieee_std.html POSIX ERE] The Open Group, Base Specifications Issue 6, IEEE Std 1003.1, 2004 Edition
* [http://www.cloudplugfest.org/ Cloud Interoperability Plugfest project]

[[Category:Cloud storage]]
[[Category:Data management]]
<=====doc_Id=====>:228
<=====title=====>:
H-Store
<=====text=====>:
{{Infobox software
| name                   = H-Store
| logo                   = [[File:H-Store-logo.png|80px|H-Store logo]]
| screenshot             =
| caption                =
| developer              = [[Brown University|Brown]], [[Carnegie Mellon University|CMU]], [[Massachusetts Institute of Technology|MIT]], [[Yale University|Yale]]
| latest release version = June 2016
| latest release date    = {{Start date and age|2016|06|03}}
| programming language   = [[C++]], [[Java (programming language)|Java]]
| operating system       = [[Linux]], [[Mac OS X]]
| genre                  = [[Database Management System]]
| license                = [[BSD License]], [[GPL]]
| website                = {{URL|hstore.cs.brown.edu}}
}}

'''H-Store''' is an experimental [[database management system]] (DBMS) designed for [[online transaction processing]] applications that is being developed by a team at [[Brown University]], [[Carnegie Mellon University]], the [[Massachusetts Institute of Technology]], and [[Yale University]].<ref>
{{cite web 
| url = http://hstore.cs.brown.edu
| title = H-Store - Next Generation OLTP DBMS Research
| accessdate = 2011-08-07
}}
</ref><ref>
{{cite web 
| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/
| title = Stonebraker's H-Store: There's something happenin' here
| first = David
| last = Van Couvering

| date = 2008-02-18
| <!-- is this one just plain wrong?:--> publication-date = 2011-03-11
| accessdate = 2012-07-18
}}
</ref>
The system's design was developed in 2007 by database researchers [[Michael Stonebraker]], [[Samuel Madden (MIT)|Sam Madden]], Andy Pavlo and Daniel Abadi.<ref>
{{cite conference
| authorlink = Michael Stonebraker
| first = Mike | last = Stonebraker
| title = The end of an architectural era: (it's time for a complete rewrite)
| booktitle = VLDB '07: Proceedings of the 33rd international conference on Very large data bases
| location = Vienna, Austria
| year = 2007
| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf
| format = PDF |display-authors=etal}}</ref><ref>
{{cite journal
| last1 = Kallman
| first1 = Robert
| last2 = Kimura
| first2 = Hideaki
| last3 = Natkins
| first3 = Jonathan
| last4 = Pavlo
| first4 = Andrew
| last5 = Rasin
| first5 = Alexander
| last6 = Zdonik
| first6 = Stanley
| authorlink6 = Stan Zdonik
| last7 = Jones
| first7 = Evan P. C.
| last8 = Madden
| first8 = Samuel
| authorlink8 = Samuel Madden (MIT)
| last9 = Stonebraker
| first9 = Michael
| authorlink9 = Michael Stonebraker
| last10 = Zhang
| first10 = Yang
| last11 = Hugg
| first11 = John
| last12 = Abadi
| first12 = Daniel J.
| title = H-Store: a high-performance, distributed main memory transaction processing system
| journal = Proc. VLDB Endowment
| year = 2008
| volume = 1
| series = 2
| pages = 1496–1499
| url = http://hstore.cs.brown.edu/papers/hstore-demo.pdf
| issn = 2150-8097
}}</ref><ref>
{{cite web 
| url = http://www.dbms2.com/2008/02/18/mike-stonebraker-calls-for-the-complete-destruction-of-the-old-dbms-order/
| title = Mike Stonebraker calls for the complete destruction of the old DBMS order
| first = Curt
| last = Monash
| year = 2008
| publication-date = 2008-02-18
| accessdate  = 2012-07-18
}}
</ref>

==Architecture==
The significance of the H-Store is that it is the first implementation of a new class of [[Parallel database|parallel database management systems]], called [[NewSQL]],<ref>{{cite web|url=http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |title=How Will The Database Incumbents Respond To NoSQL And NewSQL? |first=Matthew |last=Aslett |publisher=451 Group |publication-date=2011-04-04 |year=2010 |accessdate=2012-07-06 |deadurl=yes |archiveurl=https://web.archive.org/web/20120127202623/http://www.cs.brown.edu/courses/cs227/papers/newsql/aslett-newsql.pdf |archivedate=January 27, 2012 }}
</ref><!-- good link, just not supporting H-Store directly, is supporting [[VoltDB]] that is related, but doesn not state the connection: <ref>
{{cite web 
| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext
| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps
| first = Michael
| last = Stonebraker
| publisher = Communications of the ACM
| publication-date = 2011-06-16
| accessdate = 2012-07-06
}}
</ref> -->that provide the high-throughput and high-availability of [[NoSQL]] systems, but without giving up the [[ACID|transactional guarantees]] of a traditional DBMS.<ref>
{{cite web 
| url = http://preferisco.blogspot.com/2008/03/h-store-new-architectural-era-or-just.html
| title = H-Store - a new architectural era, or just a toy? 
| first = Nigel
| last = Thomas

| date = 2008-03-01
| accessdate = 2012-07-05
}}
</ref>
Such systems are able to scale out horizontally across multiple machines to improve throughput, as opposed to moving to a more powerful, more expensive machine for a single-node system.<ref>
{{cite web 
| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/
| title = Is H-Store the future of database management systems?
| first = Matthew
| last = Aslett

| date = 2008-03-04
| accessdate  = 2012-07-05
}}
</ref>

H-Store is able to execute [[transaction processing]] with high throughput by forgoing much of legacy architecture of [[IBM System R|System R]]-like systems. For example, H-Store was designed as a [[Parallel database|parallel]], row-storage relational DBMS that runs on a cluster of [[Shared nothing architecture|shared-nothing]], main memory executor nodes.<ref>
{{cite web 
| url = http://hstore.cs.brown.edu/documentation/architecture-overview/
| title = H-Store - Architecture Overview
| accessdate  = 2011-08-07
}}
</ref>
The database is [[Partition (database)|partitioned]] into disjoint subsets that are assigned to a single-threaded execution engine assigned to one and only one [[Multi-core processor|core]] on a node. Each engine has exclusive access to all of the data at its partition. Because it is single-threaded, only one transaction at a time is able to access the data stored at its partition. Thus, there are no physical locks or latches in the system, and no transaction will stall waiting for another transaction once it is started.<ref>
{{cite web 
| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055
| title = H-Store: Complete destruction of the old DBMS order?
| first = Larry
| last = Dignan
| year = 2008
| accessdate  = 2012-07-05
}}
</ref>

==Licensing==
H-Store is licensed under the [[BSD license]] and [[GPL]] licenses. The commercial version of H-Store's design is [[VoltDB]].<ref>
{{cite web
| url         = http://www.dbms2.com/2009/06/22/h-store-horizontica-voltdb/
| title       = H-Store is now VoltDB
| first       = Curt
| last        = Monash
| year        = 2009
| accessdate  = 2011-07-14
| postscript  = 
}}
</ref>

==See also==
{{Portal|Free software}}
*[[VoltDB]]
*[[C-Store]]
*[[Transaction processing]]

==References==
{{Reflist}}

==External links==

[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Free database management systems]]
[[Category:NewSQL]]
<=====doc_Id=====>:231
<=====title=====>:
Category:Data partitioning
<=====text=====>:
[[Category:Data management]]
<=====doc_Id=====>:234
<=====title=====>:
Category:Data mapping
<=====text=====>:
{{catmain|Data mapping}}

[[Category:Data management]]
<=====doc_Id=====>:237
<=====title=====>:
DMAIC
<=====text=====>:
{{sources|date=April 2012}}
'''DMAIC''' (an acronym for ''Define, Measure, Analyze, Improve and Control'') (pronounced ''də-MAY-ick'') refers to a data-driven improvement cycle used for improving, optimizing and stabilizing business processes and designs. The DMAIC improvement cycle is the core tool used to drive [[Six Sigma]] projects. However, DMAIC is not exclusive to Six Sigma and can be used as the framework for other improvement applications.

==Steps==
DMAIC is an abbreviation of the five improvement steps it comprises: Define, Measure, Analyze, Improve and Control. All of the DMAIC process steps are required and always proceed in the given order.
[[File:DMAICWebdingsII.png|thumbnail|right|400px|The five steps of DMAIC]]

===Define===
The purpose of this step is to clearly articulate the business problem, goal, potential resources, project scope and high-level project timeline.  This information is typically captured within project charter document.  Write down what you currently know. Seek to clarify facts, set objectives and form the project team. Define the following:

* A problem 
* The customer(s)
* [[Voice of the customer]] (VOC) and  [[Critical to Quality]] (CTQs) — what are the critical process outputs?

===Measure===
The purpose of this step is to objectively establish current baselines as the basis for improvement.  This is a data collection step, the purpose of which is to establish process performance baselines.  The performance metric baseline(s) from the Measure phase will be compared to the performance metric at the conclusion of the project to determine objectively whether significant improvement has been made.  The team decides on what should be measured and how to measure it. It is usual for teams to invest a lot of effort into assessing the suitability of the proposed measurement systems. Good data is at the heart of the DMAIC process:

===Analyze===
The purpose of this step is to identify, validate and select root cause for elimination.  A large number of potential root causes (process inputs, X) of the project problem are identified via root cause analysis (for example a [[Ishikawa diagram|fishbone diagram]]).  The top 3-4 potential root causes are selected using multi-voting or other consensus tool for further validation.  A data collection plan is created and data are collected to establish the relative contribution of each root causes to the project metric, Y.  This process is repeated until "valid" root causes can be identified.  Within Six Sigma, often complex analysis tools are used. However, it is acceptable to use basic tools if these are appropriate.  Of the "validated" root causes, all or some can be

* List and prioritize potential causes of the problem
* Prioritize the root causes (key process inputs) to pursue in the Improve step
* Identify how the process inputs (Xs) affect the process outputs (Ys).  Data are analyzed to understand the magnitude of contribution of each root cause, X, to the project metric, Y.  Statistical tests using p-values accompanied by Histograms, Pareto charts, and line plots are often used to do this.
* Detailed process maps can be created to help pin-point where in the process the root causes reside, and what might be contributing to the occurrence.

===Improve===
The purpose of this step is to identify, test and implement a solution to the problem; in part or in whole. This depends on the situation. Identify creative solutions to eliminate the key root causes in order to fix and prevent process problems. Use brainstorming or techniques like [[Six Thinking Hats]] and [[Random stimulus|Random Word]]. Some projects can utilize complex analysis tools like DOE ([[Design of Experiments]]), but try to focus on obvious solutions if these are apparent. However, the purpose of this step can also be to find solutions without implementing them.

* Create
* Focus on the simplest and easiest solutions
* Test solutions using [[PDCA|Plan-Do-Check-Act]] (PDCA) cycle
* Based on PDCA results, attempt to anticipate any avoidable risks associated with the "improvement" using [[Failure mode and effects analysis|FMEA]]
* Create a detailed implementation plan
* Deploy improvements

===Control===
The purpose of this step is to sustain the gains.  Monitor the improvements to ensure continued and sustainable success. Create a control plan. Update documents, business process and training records as required.

A [[Control chart]] can be useful during the Control stage to assess the stability of the improvements over time by serving as 1. a guide to continue monitoring the process and 2. provide a response plan for each of the measures being monitored in case the process becomes unstable.

===Replicate and thank the teams===
This is additional to the standard DMAIC steps but it should be considered. Think about replicating the changes in other processes. Share your new knowledge within and outside of your organization.  It is very important to always provide positive morale support to team members in an effort to maximize the effectiveness of DMAIC.

Replicating the improvements, sharing your success and thanking your team members helps build buy-in for future DMAIC or improvement initiatives.

===Additional Steps===
Some organizations add a '''''R'''ecognize'' step at the beginning, which is to recognize the right problem to work on, thus yielding an RDMAIC methodology.<ref name="WebberWallace2006p43">{{cite book | first1=Larry | last1=Webber | first2=Michael | last2=Wallace | title=Quality Control for Dummies | url=https://books.google.com/books?id=9BWkxto2fcEC&pg=PA43 | accessdate=2012-05-16 | date=15 December 2006 | publisher=For Dummies | isbn=978-0-470-06909-7 | pages=42–43 }}</ref>

==See also==
*[[Design for Six Sigma|DFSS]]
*[[Industrial engineering]]
*[[Kaizen]]
*[[PDCA]]
*[[Six Sigma]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Six Sigma]]
<=====doc_Id=====>:240
<=====title=====>:
Linked data
<=====text=====>:
In [[computing]], '''linked data''' (often capitalized as '''Linked Data''') is a method of publishing structured data so that it can be interlinked and become more useful through [[semantic query|semantic queries]]. It builds upon standard [[World Wide Web|Web]] technologies such as [[Hypertext Transfer Protocol|HTTP]], [[Resource Description Framework|RDF]] and [[uniform resource identifier|URIs]], but rather than using them to serve web pages for human readers, it extends them to share information in a way that can be read automatically by computers. This enables data from different sources to be connected and queried.<ref name=linkeddatastorysofar>{{Cite journal |url=http://tomheath.com/papers/bizer-heath-berners-lee-ijswis-linked-data.pdf
 |title=Linked Data&mdash;The Story So Far |last=Bizer |first=Christian |last2=Heath |first2=Tom |last3=Berners-Lee
 |first3=Tim |author3-link=Tim Berners-Lee |year=2009 |accessdate=2010-12-18 |doi=10.4018/jswis.2009081901 |issn=1552-6283
 |journal=International Journal on Semantic Web and Information Systems |volume=5 |issue=3 |pages=1–22}} Solving Semantic Interoperability Conflicts in Cross–Border E–Government Services.</ref>

[[Tim Berners-Lee]], director of the [[World Wide Web Consortium]] (W3C), coined the term in a 2006 design note about the [[Semantic Web]] project.<ref name=DesignIssues>{{cite web |url=http://www.w3.org/DesignIssues/LinkedData.html
 |title=Linked Data |work=Design Issues |author=Tim Berners-Lee |authorlink=Tim Berners-Lee |date=2006-07-27
 |publisher=[[W3C]] |accessdate=2010-12-18}}</ref>

== Principles ==
Tim Berners-Lee outlined four principles of linked data in his "Linked Data" note of 2006,<ref name=DesignIssues/> paraphrased along the following lines:

<blockquote>
# Use [[uniform resource identifier|URIs]] to name (identify) things.
# Use [[Hypertext Transfer Protocol|HTTP]] URIs so that these things can be looked up (interpreted, "dereferenced").
# Provide useful information about what a name identifies when it's looked up, using open standards such as [[Resource Description Framework|RDF]], [[SPARQL]], etc.
# Refer to other things using their HTTP URI-based names when publishing data on the Web.
</blockquote>

Tim Berners-Lee gave a presentation on linked data at the [[TED (conference)|TED]] 2009 conference.<ref>{{cite web |url=http://www.ted.com/talks/tim_berners_lee_on_the_next_web.html |title=Tim Berners-Lee on the next Web}}</ref>  In it, he restated the linked data principles as three "extremely simple" rules:

<blockquote>
# All kinds of conceptual things, they have names now that start with HTTP.
# If I take one of these HTTP names and I look it up...I will get back some data in a standard format which is kind of useful data that somebody might like to know about that thing, about that event.
# When I get back that information it's not just got somebody's height and weight and when they were born, its got relationships. And when it has relationships, whenever it expresses a relationship then the other thing that it's related to is given one of those names that starts with HTTP.
</blockquote>

== Components ==
* [[Uniform resource identifier|URI]]s
* [[HTTP]]
* [[Structured data]] using [[controlled vocabulary]] terms and dataset definitions expressed in [[Resource Description Framework]] [[serialization]] formats such as [[RDFa]], [[RDF/XML]], [[Notation 3|N3]], [[Turtle (syntax)|Turtle]], or [[JSON-LD]]
* [[Linked Data Platform]]

==Linked open data==
'''Linked open data''' is linked data that is [[open content]].<ref>{{cite web|url=http://linkeddata.org/faq|title=Frequently Asked Questions (FAQs) - Linked Data - Connect Distributed Data across the Web|publisher=}}</ref><ref>{{cite web|url=https://www.coar-repositories.org/activities/repository-observatory/second-edition-linked-open-data/7-things-you-should-know-about-open-data/|title=COAR »   7 things you should know about…Linked Data|publisher=}}</ref><ref>{{cite web|url=http://openorg.ecs.soton.ac.uk/wiki/Linked_Data_Basics_for_Techies#Open_Linked_Data|title=Linked Data Basics for Techies|publisher=}}</ref> Tim Berners-Lee gives the clearest definition of linked open data in differentiation with linked data. {{Quote |text=Linked Open Data (LOD) is Linked Data which is released under an open licence, which does not impede its reuse for free. |author=Tim Berners-Lee |title=Linked Data<ref name=DesignIssues /><ref>{{cite web|url=http://5stardata.info/en|title=5 Star Open Data}}</ref>}} Large linked open data sets include [[DBpedia]] and [[Freebase]].

=== History ===

The term "linked open data" has been in use since at least February 2007, when the "Linking Open Data" mailing list<ref>{{cite web|url=http://lists.w3.org/Archives/Public/public-lod/|title=public-lod@w3.org Mail Archives|publisher=}}</ref> was created.<ref>{{cite web|url=http://www.w3.org/wiki/SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|title=SweoIG/TaskForces/CommunityProjects/LinkingOpenData/NewsArchive|publisher=}}</ref> The mailing list was initially hosted by the [[SIMILE]] project<ref>{{cite web|url=http://simile.mit.edu/mail.html|title=SIMILE Project - Mailing Lists|publisher=}}</ref> at the [[Massachusetts Institute of Technology]].

=== Linking Open Data community project ===
[[File:LOD Cloud 2014.svg|thumb|400px|The above diagram shows which Linking Open Data datasets are connected, as of August 2014.  This was produced by the 
Linked Open Data Cloud project, which was started in 2007.  Some sets may include copyrighted data which is freely available.<ref>Linking open data cloud diagram 2014, by Max Schmachtenberg, Christian Bizer, Anja Jentzsch and Richard Cyganiak. http://lod-cloud.net/</ref>]]

The goal of the W3C Semantic Web Education and Outreach group's Linking Open Data community project is to extend the Web with a [[Knowledge commons|data commons]] by publishing various [[open knowledge|open]] [[dataset]]s as RDF on the Web and by setting [[Resource Description Framework|RDF]] links between data items from different data sources. In October 2007, datasets consisted of over two billion [[RDF triples]], which were interlinked by over two million RDF links.<ref>[http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data]</ref><ref>{{cite book |last1=Fensel |first1=Dieter |last2=Facca |first2= Federico Michele |last3=Simperl |first3=Elena |last4=Ioan |first4=Toma |title=Semantic Web Services |year=2011 |publisher=Springer|isbn=3642191924 |pages=99}}</ref>  By September 2011 this had grown to 31 billion RDF triples, interlinked by around 504 million RDF links.  A detailed statistical breakdown was published in 2014.<ref>http://linkeddatacatalog.dws.informatik.uni-mannheim.de/state/</ref>

=== European Union projects ===
There are a number of European Union projects{{Definition|date=June 2013}} involving linked data. These include the linked open data around the clock (LATC) project,<ref>[http://latc-project.eu/ Linked open data around the clock (LATC)]</ref> the PlanetData project,<ref>[http://planet-data.eu/ PlanetData]</ref> the DaPaaS (Data-and-Platform-as-a-Service) project,<ref>[http://project.dapaas.eu/ DaPaaS]</ref> and  the Linked Open Data 2 (LOD2) project.<ref>[http://lod2.eu/ Linking Open Data 2 (LOD2)]</ref><ref>{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&ACTION=D&CAT=PROJ&RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects – LOD2 |date=2010-04-20}}</ref><ref>{{cite web |url=http://static.lod2.eu/Deliverables/LOD2_D12.5.1_Project_Fact_Sheet_Version.pdf |title=LOD2 Project Fact Sheet – Project Summary |date=2010-09-01 |accessdate=2010-12-18}}</ref> Data linking is one of the main goals of the [[EU Open Data Portal]], which makes available thousands of datasets for anyone to reuse and link.

=== Datasets ===

* [[DBpedia]] – a dataset containing extracted data from Wikipedia; it contains about 3.4 million concepts described by 1 billion [[Semantic triple|triples]], including abstracts in 11 different languages
* [[FOAF (software)|FOAF]] – a dataset describing persons, their properties and relationships
* [[GeoNames]] provides RDF descriptions of more than {{formatnum:7500000}} geographical features worldwide.
* [[UMBEL]] – a lightweight reference structure of {{formatnum:20000}} subject concept classes and their relationships derived from [[OpenCyc]], which can act as binding classes to external data; also has links to 1.5 million named entities from DBpedia and [[YAGO (ontology)|YAGO]]
* [[Wikidata]] – a collaboratively-created linked dataset that acts as central storage for the structured data of its [[Wikimedia]] sister projects

=== Dataset instance and class relationships ===
Clickable diagrams that show the individual datasets and their relationships within the DBpedia-spawned LOD cloud (as shown by the figures to the right) are available.<ref>[http://www4.wiwiss.fu-berlin.de/bizer/pub/lod-datasets_2009-07-14.html Instance relationships amongst datasets]</ref><ref>[http://web.archive.org/web/20110828103804/http://umbel.org/sites/umbel.org/lod/lod_constellation.html Class relationships amongst datasets]</ref>

==See also==
* [[Authority control]] – about ''controlled headings'' in library catalogs
* [[Citation analysis]] – for citations between scholarly articles
* [[Hyperdata]]
* [[Linked data page]]
* [[Network model]] – an older type of database management system
* [[Schema.org]]
* [[Web Ontology Language]]

== References ==
{{reflist|30em}}

== Further reading ==
{{ref begin|2}}
* Ahmet Soylu, Felix Mödritscher, and Patrick De Causmaecker. 2012. [http://www.ahmetsoylu.com/wp-content/uploads/2013/10/soylu_ICAE2012.pdf “Ubiquitous Web Navigation through Harvesting Embedded Semantic Data: A Mobile Scenario.”] Integrated Computer-Aided Engineering 19 (1): 93–109.
* ''[http://linkeddatabook.com/book Linked Data: Evolving the Web into a Global Data Space]'' (2011) by Tom Heath and Christian Bizer, Synthesis Lectures on the Semantic Web: Theory and Technology, Morgan & Claypool <!-- note this resources supersedes the tutorial [http://www4.wiwiss.fu-berlin.de/bizer/pub/LinkedDataTutorial/ How to publish Linked Data on the Web] by Bizer, Cyganiak, and Heath -->
* [http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/LinkedDataTutorial/ How to Publish Linked Data on the Web], by Chris Bizer, Richard Cyganiak and Tom Heath, Linked Data Tutorial at Freie Universität Berlin, Germany, 27 July 2007.
* [http://www.scientificamerican.com/article.cfm?id=berners-lee-linked-data The Web Turns 20: Linked Data Gives People Power], part 1 of 4, by Mark Fischetti, ''[[Scientific American]]'' 2010 October 23
* [http://knoesis.wright.edu/library/publications/linkedai2010_submission_13.pdf Linked Data Is Merely More Data] – Prateek Jain, Pascal Hitzler, Peter Z. Yeh, Kunal Verma, and Amit P. Sheth. In: Dan Brickley, Vinay K. Chaudhri, Harry Halpin, and Deborah McGuinness: ''Linked Data Meets Artificial Intelligence''. Technical Report SS-10-07, AAAI Press, Menlo Park, California, 2010, pp.&nbsp;82–86.
* [http://knoesis.org/library/resource.php?id=1718 Moving beyond sameAs with PLATO: Partonomy detection for Linked Data] – Prateek Jain, Pascal Hitzler, Kunal Verma, Peter Z. Yeh, Amit Sheth. In:  Proceedings of the 23rd ACM Hypertext and Social Media conference (HT 2012), Milwaukee, WI, USA, June 25–28, 2012.
* Freitas, André, Edward Curry, João Gabriel Oliveira, and Sean O’Riain. 2012. [http://www.edwardcurry.org/publications/freitas_IC_12.pdf “Querying Heterogeneous Datasets on the Linked Data Web: Challenges, Approaches, and Trends.”] IEEE Internet Computing 16 (1): 24–33.
* [http://www2008.org/papers/pdf/p1265-bizer.pdf Linked Data on the Web] – Chris Bizer, Tom Heath, [[Kingsley Uyi Idehen]], [[Tim Berners-Lee]]. In Proceedings WWW2008, Beijing, China
* [http://sites.wiwiss.fu-berlin.de/suhl/bizer/pub/LinkingOpenData.pdf Interlinking Open Data on the Web] – Chris Bizer, Tom Heath, Danny Ayers, Yves Raimond. In Proceedings Poster Track, ESWC2007, Innsbruck, Austria
* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data] – Prateek Jain, Pascal Hitzler, Amit Sheth, Kunal Verma, Peter Z. Yeh. In proceedings of the 9th International Semantic Web Conference, ISWC 2010, Shanghai, China
* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3121711/ Linked open drug data for pharmaceutical research and development] - J Cheminform. 2011; 3: 19. Samwald, Jentzsch, Bouton, Kallesøe, Willighagen, Hajagos, Marshall, Prud'hommeaux, Hassenzadeh, Pichler, and Stephens (May 2011)
* [http://www.community-of-knowledge.de/beitrag/the-hype-the-hope-and-the-lod2-soeren-auer-engaged-in-the-next-generation-lod/ Interview with Sören Auer, head of the LOD2 project about the continuation of LOD2 in 2011], June 2011
* [http://www.semantic-web.at/LOD-TheEssentials.pdf Linked Open Data: The Essentials] - Florian Bauer and Martin Kaltenböck (January 2012)
* [http://semanticweb.com/the-flap-of-a-butterfly-wing_b26808 The Flap of a Butterfly Wing] - semanticweb.com Richard Wallis (February 2012)
{{ref end}}

== External links ==
* [http://www.w3.org/wiki/LinkedData LinkedData] at the W3C Wiki
* [http://linkeddata.org LinkedData.org]
* [http://virtuoso.openlinksw.com/white-papers/ OpenLink Software white papers]
* [http://demo.openlinksw.com/Demo/customers/CustomerID/ALFKI%23this Data from Northwind SQL schema as linked data], use case demo
* [http://nomisma.org/ Linked data for the discipline of numismatics], use case demo
* [http://en.lodlive.it Interactive LOD demo]
* [http://americanartcollaborative.org/ American Art Collaborative], consortium of US art museums committed to establishing a critical mass of linked open data on American art

{{Semantic Web}}
{{Open data navbox}}

{{Authority control}}

[[Category:Cloud standards]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]
[[Category:Hypermedia]]
[[Category:Internet terminology]]
[[Category:Open data]]
[[Category:World Wide Web]]
[[Category:Semantic Web]]
<=====doc_Id=====>:243
<=====title=====>:
Asset Description Metadata Schema
<=====text=====>:
[[File:ADMSmodelv1.png|thumb|300px|ADMS UML model version 1.00]]

The '''Asset Description Metadata Schema''' ('''ADMS''') is a common [[metadata]] vocabulary to describe standards, so-called interoperability assets, on the Web.

Used in concert with [[Web Syndication|web syndication technology]] ADMS helps people make sense of the complex multi-publisher environment around standards and in particular the ones which are semantic assets such as [[ontologies]], [[data model]]s, [[Data dictionary|data dictionaries]], code lists, [[XML]] and [[Resource Description Framework|RDF]] schemas. In spite of their importance, standards are not easily discoverable on the web via search engines because [[metadata]] about them is seldom available. Navigating on the websites of the different publishers of standards is not efficient either.

==Key terminology==
A '''semantic asset''' is a specific type of standard which involves:

 highly reusable metadata
 (e.g. xml schemata, generic data models)
 and/or reference data
 (e.g. code lists, taxonomies, dictionaries, vocabularies)

Organisations use semantic assets to share information and knowledge (within themselves and with others). Semantic assets are usually very valuable and reusable elements for the development of Information Systems, in particular, as part of machine-to-machine interfaces. As enablers to interoperable information exchange, semantic assets are usually created, published and maintained by standardisation bodies. Nonetheless, ICT projects and groups of experts also create such assets. There are therefore many publishers of semantic assets with different degrees of formalism.

==What is ADMS==
ADMS<ref name="ADMS">[http://joinup.ec.europa.eu/asset/adms/home], ADMS homepage on Joinup</ref> is a standardised metadata vocabulary created by the [[European Union|EU]]'s Interoperability Solutions for European Public Administrations (ISA) Programme<ref name="ISA">[http://ec.europa.eu/isa/], Interoperability Solutions for European Public Administrations (ISA) Programme</ref> of the [[European Commission]] to help publishers of standards document what their standards are about (their name, their status, theme, version, etc.) and where they can be found on the Web. ADMS descriptions can then be published on different websites while the standard itself remains on the website of its publisher (i.e. syndication of content). ADMS embraces the multi-publisher environment and, at the same time, it provides the means for the creation of aggregated catalogues of standards and single points of access to them based on ADMS descriptions. The Commission will offer a single point of access to standards described using ADMS via its collaborative platform, Joinup.<ref name="Joinup">[https://joinup.ec.europa.eu/], Link to Joinup</ref> The Federation<ref name="Federation">[https://joinup.ec.europa.eu/elibrary/document/adms-enabled-federation-semantic-asset-repositories-brochure], Link to the brochure of the Federation of Semantic Asset Repositories</ref> service will increase the visibility of standards described with ADMS on the web. This will also stimulate their reuse by Pan-European initiatives.

==ADMS Working Group==
More than 43 people of 20 EU Member States as well as from the US and Australia have participated in the [https://joinup.ec.europa.eu/asset/adms/document/adms-working-group ADMS Working Group]. Most of them were experts from standardisation bodies, research centres and the EU Commission. The working group used a methodology based on [[W3C]]’s processes and methods.<ref name="CoreVocsPM">{{cite web|url=http://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |title=Archived copy |accessdate=2012-04-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130430164940/https://joinup.ec.europa.eu/sites/default/files/D3.1-Process%20and%20Methodology%20for%20Core%20Vocabularies_v1.01.pdf |archivedate=2013-04-30 |df= }}, Link to ISA's Core Vocs methodology</ref>

==How to download ADMS==
ADMS version 1 was officially released in April 2012.<ref name="ADMSrelease">[https://joinup.ec.europa.eu/news/adms-v100-officially-released], ADMS V1 release announcement</ref> Version 1.00 of ADMS is available for download on Joinup:<ref name="Joinup"/>
https://joinup.ec.europa.eu/asset/adms/release/100<ref name="ADMS1">[https://joinup.ec.europa.eu/], Link to ADMS v1</ref>

ADMS is offered under ISA's Open Metadata Licence v1.1<ref name="OpenMetadataLicense">[https://joinup.ec.europa.eu/category/licence/isa-open-metadata-licence-v11], ISA Open Metadata Licence v1.1</ref>

==Related work==
The ADMS specification reuses existing [[metadata]] vocabularies and core vocabularies including:
* The [[Dublin Core]] Metadata Element Set (DCMES)<ref>http://dublincore.org/documents/dces/</ref>
* The [[Data Catalog Vocabulary]] (DCAT) <ref>http://www.w3.org/TR/vocab-dcat/</ref>
* The [[FOAF (software)|Friend of a Friend (FOAF) Ontology]]
* The [[vCard]] Ontology <ref>[http://www.w3.org/TR/vcard-rdf/], Representing vCard Objects in RDF</ref>

==The future of ADMS==
ADMS v1.00 will be contributed to<ref name="ADMScontributed">[https://joinup.ec.europa.eu/asset/adms/topic/adms-public-review-key-specifications-interoperability-developed-eus-isa-programme-], Announcement that key specifications for interoperability developed by the EU's ISA Programme will become W3C standards</ref> W3C’s Government Linked Data (GLD) Working Group.<ref name="W3C_GLD">[http://www.w3.org/2011/gld/wiki/Main_Page], W3 Government Linked Data (GLD) Working Group</ref> This means that ADMS will be published by the GLD Working Group as First Public Working Drafts for further consultation within the context of the typical W3C standardization process. The desired outcome of that process will be the publication of ADMS as a W3C Recommendation available under W3C's Royalty-Free License.

The ADMS RDFS Vocabulary already has a w3.org namespace: [http://www.w3.org/ns/adms http://www.w3.org/ns/adms#].

==References==
{{reflist|30em}}

[[Category:Data management]]
[[Category:Metadata]]
[[Category:Technical communication]]
<=====doc_Id=====>:246
<=====title=====>:
Classora
<=====text=====>:
{{Refimprove|date=June 2012}}

'''Classora''' is a [[knowledge base]] for the [[Internet]] oriented to [[data]] analysis. From a practical point of view, Classora is a [[digital repository]] that stores structured [[information]] and allows it to be displayed in multiple formats: analytically, graphically, geographically (through maps); as well as carry out [[OLAP]] analysis. The information contained in Classora comes from public sources<ref>[http://revista.mundo-r.com/contido/%E2%80%9Cclassora-evita-tener-que-bucear-entre-resultados-google-o-wikipedia%E2%80%9D Interview in R Technological Magazine (Spanish)]</ref> and is uploaded into the system through bots and [[Extract, transform, load|ETL]] processes. The [[Knowledge Base]] has a '''commercial API'''<ref>[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Classora API in Official Weblog]</ref> for semantic enhancement, and an '''open web'''<ref>[http://www.classora.com Open Web of Classora Knowledge Base]</ref> through which any user can access to part of the information collected (it also allows users to complete data and share opinions).

Internally, Classora is organized into '''Knowledge Units''' and '''Reports'''. A «Knowledge Unit» is any element of the World about which information may be stored and presented in the form of a data sheet (a person, a company, a country, etc.) A «Report» is a group of Knowledge Units: a ranking of companies, a sport classification table, a survey about people, etc. In fact, one of the technical capabilities of Classora is that it allows the comparison of reports and knowledge units gathered from different sources, thereby generating an added value for the media in which this information is published: digital media, interactive TV, etc.

== Key definitions ==

=== Knowledge unit ===
The '''units of knowledge''' (also known as ''entries'') in Classora are data sheets that have a certain semantic equivalence with the articles on the Wikipedia: they store information about any element of the world, be it a film, a country, a company or an animal. However, they differ from Wikipedia in that Classora stores structured information, enriched with a metadata layer; and therefore it is able to automatically interpret the meaning of each unit of knowledge.

=== Data report ===
A '''report''' is a group of units of knowledge in which the repetition of elements is not allowed. This definition includes any list, poll, ranking, etc.; and, in general, any consultation that involves more than one unit of knowledge. Classora excels at the reports management due to its visualization capabilities, being able to display data in the form of tables, graphs and maps. 

Types of reports:

* '''Sports scores''': Sports competitions results sanctioned by the competent institution.
* '''Rankings and lists''': All types of interesting and curious lists, whether they have an implicit order or not.
* '''Polls''': Units of knowledge that are ranked according to users’ votes.
* '''Queries to the Knowledge Base''': Questions from users using [[Contextual Query Language|CQL]].
* '''Networks of connections''': automatically calculated from the reports and the taxonomy of each Knowledge Unit.

=== Organizational taxonomy ===
An '''organizational taxonomy''' (also referred to as '''entry type''') is a data sheet that brings together the common attributes of a set of units of knowledge. For instance, the organizational taxonomy ''F1 Driver'' displays attributes such as date of debut, team, etc.; and the organizational taxonomy ''Football Club'' presents attributes such as city, stadium, etc.

In Classora, taxonomies are hierarchically organized, so that they inherit attributes from their parent taxonomies. For instance, ''F1 Driver'' is a subsidiary taxonomy of ''Sportsperson'', which is a subsidiary taxonomy of ''Person'', which in turn is a subsidiary taxonomy of ''Organism''.

The simplest type of entry in Classora is '''Classora Object'''. All the other taxonomies are its subsidiaries and inherit its attributes. In fact, the only attribute Classora Object possesses is ''name'' (all units of knowledge are required to have one name at least).

== Architecture of Classora ==

=== Data Extraction Module ===
The Data Extraction Module consists of a set of robots coordinated by software that also manages the potential incidents. Most of the information available in Classora is automatically uploaded through those robots, which connect to the main online public sources to gather all types of data. There are three categories of robots:
* '''Extraction robots''': responsible for the massive uploading of reports from official public sources (FIFA, CIA, IMF, Eurostat...). They are used for either absolute or incremental data uploading.
* '''Data scanner robots''': responsible for looking for and updating the data of a unit of knowledge. They use specific sources to perform this task: Wikipedia, IMDB, World Bank, etc.
* '''Content aggregators''':  they don’t connect to external sources. Instead, they generate new information using Classora’s internal database.

=== Participatory Module ===
In Classora’s Open Website, Internet users may participate providing their knowledge as they would on the Wikipedia. There are different ways to participate: adding or correcting data in the Knowledge Base, voting in surveys (participatory rankings) and creating new Knowledge Units and Data Reports.

=== Connectivity Module ===
The Knowledge Base is designed to be embedded in multi-platform, multi-channel systems, thus enabling its integration into mobile devices, tablets, interactive TV, etc. This integration may be carried out through specific plugins (for [[navigators]] or other devices) or an [[API]] [[REST]] that provides content in [[XML]] or [[JSON]] formats. The API is divided into three blocks of operations. The first one is the block of '''general utility tools''' (ranging from autosuggest components about geographical hierarchies to operations to obtain the list of today’s celebrity birthdays, using [[Contextual Query Language|CQL]]). The second one is the block of '''operations for widget generation''' (graphs, maps, rankings) using information from the knowledge base. Finally, there is a block of '''operations designed for the publication of free-source content'''.<ref>[http://blog.classora.com/2012/03/05/api-de-classora-para-desarrolladores/ Post about API in Classora official weblog]</ref>

== Project statistics ==
As of April 2012, 2,000,000 Knowledge Units, 15,000 Reports, around 10,000 Maps and several million potential Comparative Analyses had been added to Classora. According to the site of web metrics Alexa, Classora Open Website is ranked at 100,557 globally and at 2,880 in the Spanish traffic ranking.<ref>[http://www.alexa.com/siteinfo/http%3A%2F%2Fwww.classora.com Alexa metrics for Classora Open Web]</ref> Users spend an average of 9 ½ minutes in Classora.

== External links ==
* [http://www.classora.com Open Website of Classora Knowledge Base]

== References ==
<references/>

[[Category:Knowledge bases]]
[[Category:Data management]]
[[Category:Semantic Web]]
[[Category:Knowledge representation software]]
<=====doc_Id=====>:249
<=====title=====>:
Category:NewSQL
<=====text=====>:
{{Cat main|NewSQL}}

[[Category:Databases]] 
[[Category:Data management]]
<=====doc_Id=====>:252
<=====title=====>:
Data access
<=====text=====>:
{{Refimprove|date=September 2014}}
'''Data access''' typically refers to software and activities related to storing, retrieving, or acting on [[data]] housed in a [[database]] or other [[Information repository|repository]]. Two fundamental types of data access exist:

# [[sequential access]] (as in [[Magnetic tape data storage|magnetic tape]], for example)
# [[random access]] (as in indexed [[Digital media|media]])

Data access crucially involves [[authorization]] to access different data repositories. Data access can help distinguish the abilities of administrators and users. For example, administrators may have the ability to remove, edit and add data, while general users may not even have "read" rights if they lack access to particular information.

Historically, each repository (including each different database, [[file system]], etc.), might require the use of different [[Method (computer science)|methods]] and [[languages]], and many of these repositories stored their content in different and incompatible formats.

Over the years standardized languages, methods, and formats, have developed to serve as interfaces between the often proprietary, and always idiosyncratic, specific languages and methods.  Such standards include [[SQL]] (1974- ), [[ODBC]] (ca 1990- ), [[JDBC]], [[XQuery API for Java|XQJ]], [[ADO.NET]], [[XML]], [[XQuery]], [[XPath]] (1999- ), and [[Web Services]].

Some of these standards enable translation of data from [[unstructured data|unstructured]] (such as HTML or free-text files) to [[structured data|structured]] (such as [[XML]] or [[SQL]]).

Structures such as [[connection string]]s and DBURLs<ref>
{{cite web
| url           = http://www.quickprogrammingtips.com/java/connecting-to-oracle-database-in-java.html
| title         = Connecting to Oracle Database in Java
| accessdate    = 2014-07-18
| quote         = DBURL is of the form [...] jdbc:oracle:thin:@machinename:1521:databasename [...]
}}
</ref>
can attempt to standardise methods of [[Database connection|connecting to databases]].

== References ==
{{reflist}}

{{DEFAULTSORT:Data Access}}
[[Category:Data management]]
[[Category:Data access technologies| ]]


{{Database-stub}}
<=====doc_Id=====>:255
<=====title=====>:
SQL/PSM
<=====text=====>:
{{infobox programming language
| name                   = SQL/PSM
| logo                   =
| paradigm               = [[multi-paradigm programming language|Multi-paradigm]]
| year                   = 1996
| designer               =
| developer              =
| latest_release_version = [[SQL:2011]]
| latest_release_date    = 
| latest_preview_version = 
| latest_preview_date    = 
| turing-complete        = Yes
| typing                 = 
| implementations        = [[MySQL]] <br />IBM's [[SQL PL]]
| influenced_by          = [[Ada (programming language)|Ada]]<ref>{{Citation | url = http://ocelot.ca/blog/blog/2015/01/15/stored-procedures-critiques-and-defences/ | title = Stored Procedures: critiques and defences | year = 2015 | first1 = Peter | last1 = Gulutzan }}</ref>
| influenced             = 
| operating_system       = [[Cross-platform|Cross-platform (multi-platform)]]
| license                =
| website                = 
| file_ext               =
| dialects               =
| wikibooks              = 
}}

'''SQL/PSM''' ([[SQL]]/Persistent Stored Modules) is an [[ISO standard]] mainly defining an extension of SQL with a [[procedural language]] for use in [[stored procedure]]s. Initially published in 1996 as an extension of [[SQL-92]] (ISO/IEC 9075-4:1996, a version sometimes called PSM-96 or even SQL-92/PSM<ref>{{Cite journal | last1 = Eisenberg | first1 = A. | title = New standard for stored procedures in SQL | doi = 10.1145/245882.245907 | journal = ACM SIGMOD Record | volume = 25 | issue = 4 | pages = 81-88| year = 1996 | pmid =  | pmc = }}</ref>), SQL/PSM was later incorporated into the multi-part [[SQL:1999]] standard, and has been part 4 of that standard since then, most recently in [[SQL:2011]].  The SQL:1999 part 4 covered less than the original PSM-96 because the SQL statements for defining, managing, and invoking routines were actually incorporated into part 2 SQL/Foundation, leaving only the procedural language itself as SQL/PSM.<ref>{{cite book| first1 =Jim | last1 = Melton | first2 =Alan R | last2 = Simon | title = SQL: 1999|year=2002| publisher = Morgan Kaufmann|isbn= 978-1-55860-456-8 | pages = 541–42}}</ref> The SQL/PSM facilities are still optional as far as the SQL standard is concerned; most of them are grouped in Features P001-P008.

SQL/PSM standardizes syntax and semantics for [[control flow]], [[exception handling]] (called "condition handling" in SQL/PSM), local variables, assignment of expressions to variables and parameters, and (procedural) use of [[Cursor (databases)|cursors]]. It also defines an information schema ([[metadata]]) for stored procedures.  SQL/PSM is one language in which [[Method (computer programming) |methods]] for the SQL:1999 [[structured type]]s can be defined.  The other is Java, via [[SQL/JRT]].

In practice [[MySQL]]'s procedural language and IBM's [[SQL PL]] (used in DB2) are closest to the SQL/PSM standard.<ref name = "HarrisonFeuerstein2008">{{cite book | first1 = Guy | last1 = Harrison| first2 = Steven | last2 = Feuerstein|title=MySQL Stored Procedure Programming|url= https://books.google.com/books?id=YpeP0ok0cO4C&pg=PT75 | year=2008|publisher=O'Reilly |isbn = 978-0-596-10089-6 |page= 49}}</ref> 

SQL/PSM resembles and inspired by [[PL/SQL]], as well as [[PL/pgSQL]], so they are similar languages.  With [[PostgreSQL]] v9 some SQL/PSM features, like overloading of SQL-invoked functions and procedures<ref>{{Citation | publisher = PostgreSQL | title = SQL standard features | edition = 9 | contribution-url = http://www.postgresql.org/docs/9.0/static/features-sql-standard.html | contribution = feature T322}}.</ref> are now supported.  A [[PostgreSQL]] addon implements SQL/PSM<ref>{{Citation | url = https://github.com/okbob/plpsm0 | format = git | type = repository | title = plpsm0}}.</ref><ref>{{Citation | publisher = PostgreSQL | url = http://www.postgresql.org/message-id/1305291347.14548.13.camel@jara.office.nic.cz | date = May 2011 | title = Announce}}.</ref><ref>[http://www.postgresql.org/message-id/CAFj8pRDWFdcjNSnwQB_3j1-rMO6b8=TmLTNBvDCSpRrOW2Dfeg@mail.gmail.com 2012-2's Proposal PL/pgPSM announce]</ref><ref>{{Citation | title = SQL/PSM | format = wiki | url = http://postgres.cz/wiki/SQL/PSM_Manual | publisher = PostgreSQL | type = manual | year = 2008}}.</ref> (alongside its own procedural language), although it is not part of the core product.<ref>{{Citation | contribution-url = http://www.postgresql.org/docs/9.2/static/features.html | publisher = PostgreSQL | title = Documentation | edition = 9.2 | contribution = SQL Conformance}}.</ref>

==See also==
The following implementations adopt the standard, but they are not 100% compatible to SQL/PSM:

[[Open source]]:
* [[HSQLDB]] stored procedures and functions<ref name="SQL/PSM routines">http://hsqldb.org/doc/2.0/guide/sqlroutines-chapt.html#src_psm_routines</ref>
* [[MySQL]] stored procedures <ref name="HarrisonFeuerstein2008"/>
* [[PostgreSQL]] [[PL/pgSQL]]

Proprietary:
* Oracle [[PL/SQL]]
* Microsoft and Sybase [[Transact-SQL]]

==References==
{{reflist}}

==Further reading==
* Jim Melton, ''Understanding SQL's Stored Procedures: A Complete Guide to SQL/PSM'', Morgan Kaufmann Publishers, 1998, ISBN 1-55860-461-8

{{SQL}}

__NOTOC__

{{DEFAULTSORT:SQL PSM}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Data-centric programming languages]]
[[Category:Programming languages created in 1996]]


{{compu-lang-stub}}
{{database-stub}}
<=====doc_Id=====>:258
<=====title=====>:
Inverted index
<=====text=====>:
In [[computer science]], an '''inverted index''' (also referred to as '''postings file''' or '''inverted file''') is an [[index (database)|index data structure]] storing a mapping from content, such as words or numbers, to its locations in a [[Table (database)|database file]], or in a document or a set of documents (named in contrast to a [[Forward Index]], which maps from documents to content).  The purpose of an inverted index is to allow fast [[full text search]]es, at a cost of increased processing when a document is added to the database.  The inverted file may be the database file itself, rather than its [[Index (database)|index]]. It is the most popular data structure used in [[document retrieval]] systems,<ref>{{Harvnb |Zobel|Moffat|Ramamohanarao|1998| Ref=none }}</ref> used on a large scale for example in [[search engine]]s.  Additionally, several significant general-purpose [[Mainframe computer|mainframe]]-based [[database management systems]] have used inverted list architectures, including [[ADABAS]], [[DATACOM/DB]], and [[Model 204]].

There are two main variants of inverted indexes: A '''record-level inverted index''' (or '''inverted file index''' or just '''inverted file''') contains a list of references to documents for each word. A '''word-level inverted index''' (or '''full inverted index''' or '''inverted list''') additionally contains the positions of each word within a document.<ref name="isbn0-201-39829-X-p192">{{Harvnb |Baeza-Yates|Ribeiro-Neto|1999| p=192 | Ref=BYR99 }}</ref> The latter form offers more functionality (like [[phrase search]]es), but needs more processing power and space to be created.

==Applications==

The inverted index [[data structure]] is a central component of a typical [[Index (search engine)|search engine indexing algorithm]]. A goal of a search engine implementation is to optimize the speed of the query: find the documents where word X occurs. Once a [[Search engine indexing#The forward index|forward index]] is developed, which stores lists of words per document, it is next inverted to develop an inverted index. Querying the forward index would require sequential iteration through each document and to each word to verify a matching document. The time, memory, and processing resources to perform such a query are not always technically realistic.  Instead of listing the words per document in the forward index, the inverted index data structure is developed which lists the documents per word.

With the inverted index created, the query can now be resolved by jumping to the word ID (via [[random access]]) in the inverted index.

In pre-computer times, [[Concordance (publishing)|concordances]] to important books were manually assembled.  These were effectively inverted indexes with a small amount of accompanying commentary that required a tremendous amount of effort to produce.

In bioinformatics, inverted indexes are very important in the [[sequence assembly]] of short fragments of sequenced DNA. One way to find the source of a fragment is to search for it against a reference DNA sequence. A small number of mismatches (due to differences between the sequenced DNA and reference DNA, or errors) can be accounted for by dividing the fragment into smaller fragments—at least one subfragment is likely to match the reference DNA sequence. The matching requires constructing an inverted index of all substrings of a certain length from the reference DNA sequence. Since the human DNA contains more than 3 billion base pairs, and we need to store a DNA substring for every index and a 32-bit integer for index itself, the storage requirement for such an inverted index would probably be in the tens of gigabytes.

==See also==
* [[Index (search engine)]]
* [[Reverse index]]
* [[Vector space model]]

== Bibliography ==
*{{cite book |last= Knuth |first= D. E. |authorlink= Donald Knuth |title= [[The Art of Computer Programming]] |publisher= [[Addison-Wesley]] |edition= Third |year= 1997 |origyear= 1973 |location= [[Reading, Massachusetts]] |isbn= 0-201-89685-0 |ref= Knu97 |chapter= 6.5. Retrieval on Secondary Keys}}
*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |last3=Ramamohanarao |first3=Kotagiri |date=December 1998 |title= Inverted files versus signature files for text indexing |journal= ACM Transactions on Database Systems |volume= 23 |issue= 4 |pages=453–490 |publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/296854.277632 |url= |accessdate= }}
*{{cite journal|last1= Zobel |first1= Justin |last2=Moffat |first2=Alistair |date=July 2006 |title= Inverted Files for Text Search Engines |journal= ACM Computing Surveys |volume= 38 |issue= 2 |page=6|publisher= [[Association for Computing Machinery]] |location= New York |doi= 10.1145/1132956.1132959 |url= |accessdate= }}
*{{cite book |last= Baeza-Yates | first = Ricardo |authorlink=Ricardo Baeza-Yates |author2=Ribeiro-Neto, Berthier |title= Modern information retrieval |publisher= Addison-Wesley Longman |location= [[Reading, Massachusetts]] |year= 1999 |isbn= 0-201-39829-X |oclc= |doi= |ref= BYR99 |page= 192 }}
*{{cite journal |last= Salton | first = Gerard |author2=Fox, Edward A. |author3=Wu, Harry  |title= Extended Boolean information retrieval |publisher= ACM |year= 1983
|journal = Commun. ACM |volume = 26 |issue = 11 |doi= 10.1145/182.358466 |page=1022 }}
*{{cite book |title=Information Retrieval: Implementing and Evaluating Search Engines  |url=http://www.ir.uwaterloo.ca/book/ |publisher=MIT Press |year=2010 |location=Cambridge, Massachusetts |isbn= 978-0-262-02651-2 |author8=Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack}}

==References==
{{Reflist}}

==External links==
*[https://xlinux.nist.gov/dads/HTML/invertedIndex.html NIST's Dictionary of Algorithms and Data Structures: inverted index]
*[http://mg4j.di.unimi.it Managing Gigabytes for Java] a free full-text search engine for large document collections written in Java.
*[http://lucene.apache.org/java/docs/ Lucene] - Apache Lucene is a full-featured text search engine library written in Java.
*[http://sphinxsearch.com/ Sphinx Search] - Open source high-performance, full-featured text search engine library used by craigslist and others employing an inverted index.
*[http://rosettacode.org/wiki/Inverted_Index Example implementations] on [[Rosetta Code]]
* [http://www.vision.caltech.edu/malaa/software/research/image-search/ Caltech Large Scale Image Search Toolbox]: a Matlab toolbox implementing Inverted File Bag-of-Words image search.

[[Category:Data management]]
[[Category:Search algorithms]]
[[Category:Database index techniques]]
[[Category:Substring indices]]
<=====doc_Id=====>:261
<=====title=====>:
Content-oriented workflow models
<=====text=====>:
{{Orphan|date=May 2014}}

The goal of '''content-oriented workflow models''' is to articulate workflow progression by the presence of content units (like data-records/objects/documents).
Most content-oriented workflow approaches provide a life-cycle model for content units, such that workflow progression can be qualified by conditions on the state of the units.
Most approaches are research and work in progress and the content models and life-cycle models are more or less formalized.

The term ''content-oriented workflows'' is an umbrella term for several scientific workflow approaches, namely "data-driven", "resource-driven", "artifact-centric", "object-aware", and "document-oriented". Thus, the meaning of "content" ranges from simple data attributes to self-contained documents; the term "content-oriented workflows" appeared at first in <ref name="Neumann2010" /> as an umbrella term. Such general term, independent from a specific approach, is necessary to contrast the content-oriented modelling principle with traditional activity-oriented workflow models (like [[Petri net]]s or [[Business Process Model and Notation|BPMN]]) where a workflow is driven by a control flow and where the content production perspective is neglected or even missing.

The term "content" was chosen to subsume the different levels in granularity of the content units in the respective workflow models; it was also chosen to make associations with [[content management]]. Both terms "artifact-centric" and "data-driven" would also be good candidates for an umbrella term, but each is closely related to a specific approach of a single working group. The "artifact-centric" group itself (i.e. IBM Research) has generalized the characteristics of their approach and has used "information-centric" as an umbrella term in.<ref name="Kumaran2008" /> Yet, the term [[information]] is too unspecific in the context of computer science, thus, "content-orientated workflows" is considered as good compromise.

== Workflow Model Approaches ==

=== Data-driven ===

The data-driven process structures provides a sophisticated workflow model being specialized on hierarchical write-and-review-processes.
The approach provides interleaved synchronization of sub-processes and extends activity diagrams.
Unfortunately, the COREPRO prototype implementation is not publicly available.

Research on the project had been ceased. The general idea has been continued by Reichert in form of the [[#Object-aware]] approach.

; Synonyms
: data-driven process structures / data-driven modeling and coordination
;Protagonists
: Dr. Dominic Müller (University of Twente), Joachim Herbst (DaimlerChrysler Research), and Manfred Reichert (at this time [http://wwwhome.cs.utwente.nl/~reichertm/index01.htm Assoc. Prof. at Univ. of Twente], currently [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. at Ulm Univ.])
;Organization(s)
: University of Twente, DaimlerChrysler
;Period
: 2005 - 2007
;Selected publications
:<ref name="Mueller2006" /><ref name="Mueller2007" />
;Implementation
: [http://www.utwente.nl/ewi/is/research/completed_projects/completed_projects/corepro.doc/ COREPRO]

=== Resource-driven ===

The resource-driven workflow system is an early approach that considered workflows from a content-oriented perspective and emphasizes on the missing support for plain document-driven processes by traditional activity-oriented workflow engines.
The resource-driven approach demonstrated the application of database triggers for handling workflow events.
Still the system implementation is centralized and the workflow schema is statically defined.
The project appeared in 2005 but many aspects are considered future work by the authors.

Research did not continue on the project. Wang completed his PhD thesis in 2009, yet, his thesis does not mention the resource-driven approach to workflow modelling but is about discrete event simulation.

;Synonyms
: Resource-based Workflows / Document-Driven Workflow Systems
;Protagonists
: Jianrui Wang and [http://www.personal.psu.edu/axk41/ Prof. Akhil Kumar]
;Organization
: Pennsylvania State University
;Period
: 2005 - today
;Selected publications
:<ref name="Wang2005" /><ref name="Kumar2010" />
;Implementation
: N/A

=== Artifact-centric ===
{{See also|Artifact-centric business process model}}

The artifact-centric approach appears as a mature framework for general purpose content-oriented workflows.
The distribution of the enterprise application landscape with its business services is considered, yet, the workflow engine itself seems to be centralized.
The process enactment seems to be tightly coupled with a technically pre-integrated database management system infrastructure.
The latter makes it most suitable for manufacturing process or for organizational processes within a well-defined institutional scope.
The approach remains work in progress, still, it is a relatively old and established project on content-oriented workflows.
Funded by IBM, it has comparably high number of developers.
It is a promising approach.

;Synonyms
: artifact-centric business process models / artifact-based business process (ACP) / artifact-centric workflows
;Protagonists
: [http://domino.research.ibm.com/comm/research_people.nsf/pages/hull.index.html Richard Hull] and Dr. Kamal Bhattacharya as well as Cagdas E. Gerede and Jianwen Su
;Organization
: IBM (T.J. Watson Research Center, NY)
;Period
: 2007 - today
;Selected publications
:<ref name="Bhattacharya2007" /><ref name="Calvanese2009" />
;Implementation
: [http://domino.research.ibm.com/comm/research_projects.nsf/pages/artifact.index.html ArtiFact]

=== Object-aware ===

The object-aware approach manages a set of object types and generates forms for creating object instances.
The form completion flow is controlled by transitions between object configurations each describing a progressing set of mandatory attributes.
Each object configuration is named by an object state.
The data production flow is user-shifting and it is discrete by defining a sequence of object states.
The discussion is currently limited to a centralized system, without any workflows across different organizations.
However, the approach is of great relevance to many domains like concurrent engineering.
Finally, the object-aware approach and its PHILharmonicFlows system are going to provide general-purpose workflow systems for generic enactment of data production processes.

;Synonyms
: object-aware process management / datenorientiertes Prozess-Management-System
;Protagonists
: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/vera-kuenzle.html Vera Künzle] and [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/staff/manfred-reichert.html Prof. Manfred Reichert]
;Organization
: Ulm University
;Period
: 2009 - today
;Selected publications
:<ref name="Kuenzle2009" /><ref name="Kuenzle2010" />
;Implementation
: [http://www.uni-ulm.de/en/in/institute-of-databases-and-information-systems/research/projects/philharmonic-flows.html PHILharmonicFlows]

=== Distributed Document-oriented ===

Distributed document-oriented process management (dDPM) enables distributed case handling in heterogeneous system environments and it is based on document-oriented [[semantic integration|integration]].
The workflow model reflects the paper-based working practice in inter-institutional healthcare scenarios.
It targets distributed knowledge-driven ad-hoc workflows, wherein distributed information systems are required to coordinate work with initially unknown sets of actors and activities.

The distributed workflow engine supports process planning & process history as well as participant management and process template creation with import/export.
The workflow engine embeds a functional fusion of 1) group-based instant messaging 2) with a shared work list editor 3) with version control.
The software implementation of dDPM is α-Flow which is available as open source.
dDPM and α-Flow provide a content-oriented approach to schema-less workflows.

The complete distributed case handling application is provided in form of a single active Document ("&alpha;-Doc").
The α-Doc is a case file (as information carrier) with an embedded workflow engine (in form of active properties).
Inviting process participants is equivalent to providing them with a copy of an α-Doc, copying it like an ordinary desktop file.
All α-Docs that belong to the same case can synchronize each other, based on the participant management, electronic postboxes, store-and-forward messaging, and an offline-capable synchronization protocol.

;Synonyms
: distributed document-oriented process management (dDPM), distributed case handling via active documents
;Protagonists
: [http://www6.informatik.uni-erlangen.de/people/cpn/ Christoph P. Neumann] and [http://www6.informatik.uni-erlangen.de/people/lenz/ Prof. Richard Lenz]
;Organization
: Friedrich-Alexander-Universit&auml;t Erlangen-N&uuml;rnberg
;Period
: 2009 - 2012
;Selected Publications
:<ref name="Neumann2011" /><ref name="Neumann2012" /> and a PhD thesis <ref name="DissNeumann2012" />
;Implementation
: [https://github.com/cpnatwork/alphaflow_dev &alpha;-Flow (open source)]

== Related Concepts ==

=== Content Management ===

The bandwidth of [[Content management system]]s (CMS) reaches from [[Web content management system]]s (WCMS) and [[Document management system]] (DMS) to [[Enterprise Content Management]] (ECM). Mature DMS products support document production workflows in a basic form, primarily focusing on review cycle workflows concerning a single document. Market leaders are [[Alfresco (software)|Alfresco]] , [[eXo Platform]] and EMC with [[Documentum]].

=== Groupware and Computer-Supported Cooperative Work ===
[[Groupware]] focuses on messaging (like E-Mail, Chat, and Instant Messaging), shared calendars (e.g. Lotus Notes, Microsoft Outlook with Exchange Server), and conferencing (e.g. Skype).
Groupware overlaps with [[Computer-supported cooperative work]] (CSCW), that originated from shared multimedia editors (for live drawing/sketching) and synchronous multi-user applications like [[desktop sharing]]. The extensive conceptual claim of CSWC must be put into perspective by its actual solution scope, that is available as the [[CSCW#CSCW Matrix|CSCW Matrix]].

=== Case Handling ===

The case handling paradigm stems from Prof. van der Aalst and gained momentum in 2005. The core features are:
(a) provide all information available, i.e. present the case as a whole rather than showing bits and pieces,
(b) decide about activities on the basis of the information available rather than the activities already executed,
(c) separate work distribution from authorization and allow for additional types of roles, not just the execute role, and
(d) allow workers to view and add/modify data before or after the corresponding activities have been executed.

In healthcare, the flow of a patient between healthcare professionals is considered as a workflow - with activities that include all kinds of diagnostic or therapeutic treatments. The workflow is considered as a case, and workflow management in healthcare is to handle these cases.

Case handling is orthogonal to content-oriented workflows. Some content-oriented workflow approaches are not related to case handling, but, for example, to automated manufacturing. In contrast, systems that are considered to be case handling systems (CHS) but which do not apply a content-oriented workflow model are, for example, BPMone (formerly PROTOS and FLOWer) from Pallas Athena, ECHO from Digital, CMDT from ICL, and Vectus from London Bridge Group. In conclusion, those content-oriented workflow approaches that are tightly related to case handling are the [[#Resource-driven]] workflow model and the [[#Distributed Document-oriented]] workflow model.

;Protagonists
: [http://wwwis.win.tue.nl/~wvdaalst/ Prof. Wil van der Aalst] and Associate Professor Dr. [http://reijers.com/ Hajo Reijers] (with focus on healthcare)
;Organization
: Univ. of Technology, Eindhoven
;Period
: 2001 - today
;Selected publication
:<ref name="Reijers2003" /><ref name="Aalst2005" />

== See also ==
* [[Process philosophy]]
* [[Workflow]]
* [[Adam Smith]]
* [[Process control]]
* [[Process management]]
* [[Business process]]
* [[Business process automation]]
* [[Business process management]]
* [[Business Process Model and Notation]]
* [[Advanced case management]]
* [[Content management system]]
* [[Web content management system]]
* [[Document management system]]
* [[Enterprise Content Management]]

== References ==
<references>
<ref name="Neumann2010">Christoph P. Neumann and Richard Lenz: [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5541988 The alpha-Flow Use-Case of Breast Cancer Treatment - Modeling Inter-Institutional Healthcare Workflows by Active Documents]. In: Proc of the 8th Int'l Workshop on Agent-based Computing for Enterprise Collaboration (ACEC) at the 19th Int'l Workshops on Enabling Technologies: Infrastructures for Collaborative Enterprises (WETICE 2010), Larissa, Greece, June 2010. ([http://www6.informatik.uni-erlangen.de/publications/public/2010/acec2010_neumann_alphaUC.pdf PDF])</ref>
<ref name="Kumaran2008">Kumaran, S., R. Liu, and F. Wu. [http://www.springerlink.com/content/a837212173812011/ On the Duality of Information-Centric and Activity-Centric Models of Business Processes]. In: Advanced Information Systems Engineering. 2008. p. 32-47.</ref>
<ref name="Mueller2006">Dominic Müller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/kpm9454157514825/ Flexibility of Data-Driven Process Structures]. In: Business Process Management Workshops Lecture Notes in Computer Science, 2006, Volume 4103/2006, 181-192. ([http://dbis.eprints.uni-ulm.de/111/2/Mueller06-DPM.pdf PDF])</ref>
<ref name="Mueller2007">Dominic Müller, Manfred Reichert und Joachim Herbst. [http://www.springerlink.com/content/2771670210653747/ Data-Driven Modeling and Coordination of Large Process Structures]. On the Move to Meaningful Internet Systems 2007: CoopIS, DOA, ODBASE, GADA, and IS Lecture Notes in Computer Science, 2007, Volume 4803/2007, 131-149. ([http://dbis.eprints.uni-ulm.de/116/1/Mueller07-CoopIS.pdf PDF])</ref>
<ref name="Bhattacharya2007">Kamal Bhattacharya, Cagdas Gerede, Richard Hull, Rong Liu, and Jianwen Su. 2007. [http://dl.acm.org/citation.cfm?id=1793141 Towards formal analysis of artifact-centric business process models]. In Proceedings of the 5th international conference on Business process management (BPM'07), Gustavo Alonso, Peter Dadam, and Michael Rosemann (Eds.). Springer-Verlag, Berlin, Heidelberg, 288-304. (pages 289ff in: [http://alumni.cs.ucsb.edu/~gerede/research/papers/bghls-bpm07-Artifact.pdf PDF])</ref>
<ref name="Calvanese2009">Diego Calvanese, Giuseppe De Giacomo, Richard Hull und Jianwen Su. [http://www.springerlink.com/content/t58342lj86807111/ Artifact-Centric Workflow Dominance]. Lecture Notes in Computer Science, 2009, Volume 5900/2009, 130-143. ([http://www.dis.uniroma1.it/~degiacom/papers/2009/ICSOC09.pdf PDF])</ref>
<ref name="Kuenzle2009">Vera Künzle und Manfred Reichert. [http://www.springerlink.com/content/n6448q47g0474242/ Towards Object-Aware Process Management Systems: Issues, Challenges, Benefits]. In: Enterprise, Business-Process and Information Systems Modeling Lecture Notes in Business Information Processing, 2009, Volume 29, Part 1, Part 5, 197-210, DOI: 10.1007/978-3-642-01862-6_17 ([http://dbis.eprints.uni-ulm.de/526/1/BPMDS09_Kuenzle_Reichert.pdf PDF])</ref>
<ref name="Kuenzle2010">Künzle, Vera and Reichert, Manfred. 2010. [http://dbis.eprints.uni-ulm.de/647/ Herausforderungen bei der Integration von Benutzern in Datenorientierten Prozess-Management-Systemen]. EMISA Forum, 30 (1). pp. 11-28. ISSN 1610-3351 ([http://dbis.eprints.uni-ulm.de/647/2/KuRe10.pdf PDF])</ref>
<ref name="Wang2005">Wang, J. and A. Kumar. [http://www.springerlink.com/content/79k8v7terwchn5ct/ A Framework for Document-Driven Workflow Systems]. In: Business Process Management. 2005. p. 285-301. ([http://php.scripts.psu.edu/faculty/a/x/axk41/BPM05-jerry-reprint.pdf PDF])</ref>
<ref name="Kumar2010">Akhil Kumar und Jianrui Wang. [http://www.springerlink.com/content/n218t085521q1347/ A Framework for Designing Resource-Driven Workflows]. In: Handbook on Business Process Management 1, International Handbooks on Information Systems, 2010, Part III, 419-440.</ref>
<ref name="Neumann2011">Christoph P. Neumann, Peter K. Schwab, Andreas M. Wahl and Richard Lenz. alpha-Adaptive: Evolutionary Workflow Metadata in Distributed Document-Oriented Process Management. In: Proc of the 4th Int'l Workshop on Process-oriented Information Systems in Healthcare (ProHealth'11) in conjunction with the 9th Int'l Conf on Business Process Management (BPM'11), Clermont-Ferrand, France, August 2011. ([http://www6.informatik.uni-erlangen.de/publications/public/2011/prohealth2011_neumann.pdf PDF])</ref>
<ref name="Neumann2012">Christoph P. Neumann and Richard Lenz. The alpha-Flow Approach to Inter-Institutional Process Support in Healthcare. International Journal of Knowledge-Based Organizations. IGI Global, 2012.</ref>
<ref name="DissNeumann2012">Christoph P. Neumann. [http://www.dr.hut-verlag.de/978-3-8439-0919-8.html Distributed Case Handling]. PhD thesis (German 'Dissertation'). Friedrich-Alexander-Universit&auml;t Erlangen-N&uuml;rnberg. 2012.</ref>
<ref name="Reijers2003">Hajo Reijers , Jaap Rigter , Wil Van Der Aalst. [http://www.worldscinet.com/ijcis/12/1203/S0218843003000784.html The Case Handling Case]. International Journal of Cooperative Information Systems (IJCIS), Volume: 12, Issue: 3(2003) pp. 365-391. ([http://is.tm.tue.nl/staff/hreijers/H.A.%20Reijers%20Bestanden/chc.pdf PDF])</ref>
<ref name="Aalst2005">[[Wil M.P. van der Aalst]], [[Mathias Weske]], Dolf Grünbauer. [http://www.sciencedirect.com/science/article/pii/S0169023X04001296 Case handling: a new paradigm for business process support]. In: Data &amp; Knowledge Engineering, Volume 53, Issue 2, May 2005, Pages 129-162, ISSN 0169-023X, 10.1016/j.datak.2004.07.003. ([http://www.imamu.edu.sa/Scientific_selections/abstracts/AbstratctIT1/Case%20handling%20a%20new%20paradigm%20for%20business%20process%20support.pdf PDF])</ref>
</references>

{{DEFAULTSORT:Content-oriented Workflows}}
<!--Categories-->
[[Category:Workflow technology]]
[[Category:Data management]]
<=====doc_Id=====>:264
<=====title=====>:
Read–write conflict
<=====text=====>:
{{Unreferenced|date=June 2008}}
In [[computer science]], in the field of [[database]]s, '''read–write conflict''', also known as '''unrepeatable reads''', is a computational anomaly associated with interleaved execution of transactions. 

Given a schedule S

:<math>S = \begin{bmatrix}
T1 & T2 \\
R(A) &  \\
 & R(A) \\
 & W(A)\\
 & Com. \\
R(A) & \\
W(A) & \\
Com. & \end{bmatrix}</math>

In this example, T1 has read the original value of A, and is waiting for T2 to finish. T2 also reads the original value of A, overwrites A, and commits.

However, when T1 reads to A, it discovers two different versions of A, and T1 would be forced to [[Abort (computing)|abort]], because T1 would not know what to do. This is an unrepeatable read. This could never occur in a serial schedule. [[Strict two-phase locking]] (Strict 2PL) prevents this conflict.

== Real-world example==
[[Alice and Bob]] are using a website to book tickets for a specific show. Only one ticket is left for the specific show. Alice signs on first to see that only one ticket is left, and finds it expensive. Alice takes time to decide. Bob signs on and also finds one ticket left, and orders it instantly. Bob purchases and logs off. Alice decides to buy a ticket, to find there are no tickets. This is a typical read-write conflict situation.

== See also ==

* [[Concurrency control]]
* [[Write–read conflict]]
* [[Write–write conflict]]

{{DEFAULTSORT:Read-write conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:267
<=====title=====>:
XSA
<=====text=====>:
In computer science, '''XSA''' (better known as '''Cross-Server Attack''') is a networking security intrusion method which allows for a malicious client to compromise security over a website or service on a server by using implemented services on the server that may not be secure.

In general, XSA is demonstrated against websites, yet sometimes it is used in conjunction with other services located on the same server.

== Basics ==
XSA is a method that allows for a malicious client to use services that a remote server implements in order to attack another service on the same server or network.

Most website hosting companies that offer hosting for large or even little amounts of separate websites are vulnerable to this method of attack, because of the amount of access services such as [[PHP]] and the webserver itself give to a client that allows the client to access other website configurations, files, passwords and the like.

== History ==

The term 'XSA' was first coined by DeadlyData, a prominent [[Computer hacker]] during the early 2000s, over the voice communications software [[TeamSpeak]]. While he had not invented or pioneered this method of intrusion, he coined it as a shortened term to describe the act of performing Cross-Server Attacks (XSAs).

It was then used further in the community and now supports for most of the methods and subsets of the method that give both [[Computer hacker]] and malicious individuals the terminology to attack websites using software that is located on the same server.

== See also ==
{{Portal|Software Testing}}
*[[SQL Injection]] (SQLi)
*[[Cross-Site Scripting]] (XSS)
*[[Cross-Site Request Forgery]] (CSRF)
*[[Buffer Overflow]]

{{DEFAULTSORT:XSA}}
[[Category:Data management]]
[[Category:Computer security exploits]]
[[Category:Computer network security]]
[[Category:World Wide Web]]
[[Category:Web development]]
<=====doc_Id=====>:270
<=====title=====>:
Online analytical processing
<=====text=====>:
'''Online analytical processing''', or '''OLAP''' ({{IPAc-en|ˈ|oʊ|l|æ|p}}), is an approach to answering [[multi-dimensional analytical]] (MDA) queries swiftly in [[computing]], .<ref name=Codd1993>{{cite web
  |url=http://www.sgpnyc.com/us/products/dataquest/whitepapers/OLAP_wp_efcodd.pdf
  |title=Providing OLAP (On-line Analytical Processing) to User-Analysts: An IT Mandate
  |publisher=Codd & Date, Inc
  |author1=Codd E.F. |author2=Codd S.B. |author3=Salley C.T.  |last-author-amp=yes |year=1993
  |accessdate=2008-03-05 }}</ref> OLAP is part of the broader category of [[business intelligence]], which also encompasses [[relational database]], report writing and [[data mining]].<ref>{{cite book
  |url=https://books.google.com/books?id=M-UOE1Cp9OEC
  |title=Business Intelligence for Telecommunications
  |publisher=CRC Press
  |author=Deepak Pareek
  |year=2007
  |pages=294 pp
  |isbn=0-8493-8792-2
  |accessdate=2008-03-18
}}</ref>  Typical applications of OLAP include [[business reporting]] for sales, [[marketing]], management reporting, [[business process management]] (BPM),<ref>{{cite book
  |url=http://www.google.com/products?q=9783639222166
  |title=Business Process Management:A Data Cube To Analyze Business Process Simulation Data For Decision Making
  |publisher=[[VDM Verlag|VDM Verlag Dr. Müller e.K.]]
  |author=Apostolos Benisis
  |year=2010
  |pages=204 pp
  |isbn=978-3-639-22216-6
}}</ref> [[budget]]ing and [[forecasting|forecast]]ing, [[financial reporting]] and similar areas, with new applications coming up, such as [[agriculture]].<ref name=ahsan/> The term ''OLAP'' was created as a slight modification of the traditional database term [[online transaction processing]] (OLTP).<ref>{{cite web
  |url=http://www.symcorp.com/downloads/OLAP_CouncilWhitePaper.pdf
  |format=PDF|title=OLAP Council White Paper
  |publisher=OLAP Council
  |year=1997

  |accessdate=2008-03-18
}}</ref>

OLAP tools enable users to analyze multidimensional data interactively from multiple perspectives. OLAP consists of three basic analytical operations: consolidation (roll-up), drill-down, and slicing and dicing.<ref>O'Brien & Marakas, 2011, p. 402-403</ref> Consolidation involves the aggregation of data that can be accumulated and computed in one or more dimensions. For example, all sales offices are rolled up to the sales department or sales division to anticipate sales trends. By contrast, the drill-down is a technique that allows users to navigate through the details. For instance, users can view the sales by individual products that make up a region's sales. Slicing and dicing is a feature whereby users can take out (slicing) a specific set of data of the [[OLAP cube]] and view (dicing) the slices from different viewpoints.  These viewpoints are sometimes called dimensions (such as looking at the same sales by salesperson or by date or by customer or by product or by region, etc.)

[[Database]]s configured for OLAP use a multidimensional data model, allowing for complex analytical and [[ad hoc]] queries with a rapid execution time.<ref>{{cite web
  |url=http://www.dwreview.com/OLAP/Introduction_OLAP.html
  |title=Introduction to OLAP – Slice, Dice and Drill!
  |publisher=Data Warehousing Review
  |author=Hari Mailvaganam
  |year=2007  |accessdate=2008-03-18
}}</ref>  They borrow aspects of [[navigational database]]s, [[hierarchical database]]s and relational databases.

OLAP is typically contrasted to [[Online transaction processing|OLTP]] (online transaction processing), which is generally characterized by much less complex queries, in a larger volume, to process transactions rather than for the purpose of business intelligence or reporting. Whereas OLAP systems are mostly optimized for read, OLTP has to processes all kinds of queries (read, insert, update and delete).

== Overview of OLAP systems ==
At the core of any OLAP system is an [[OLAP cube]] (also called a 'multidimensional cube' or a [[hypercube]]). It consists of numeric facts called ''measures'' that are categorized by ''[[Dimension (data warehouse)|dimensions]]''. The measures are placed at the intersections of the hypercube, which is spanned by the dimensions as a [[vector space]]. The usual interface to manipulate an OLAP cube is a matrix interface, like [[Pivot table]]s in a spreadsheet program, which performs projection operations along the dimensions, such as aggregation or averaging.

The cube metadata is typically created from a [[star schema]] or [[snowflake schema]] or [[fact constellation]] of tables in a [[relational database]]. Measures are derived from the records in the [[fact table]] and dimensions are derived from the [[dimension table]]s.

Each ''measure'' can be thought of as having a set of ''labels'', or meta-data associated with it. A ''dimension'' is what describes these ''labels''; it provides information about the ''measure''.

A simple example would be a cube that contains a store's sales as a ''measure'', and Date/Time as a ''dimension''. Each Sale has a Date/Time ''label'' that describes more about that sale.

For example:
  Sale Fact Table
 +-------------+----------+
 | sale_amount | time_id  |
 +-------------+----------+            Time Dimension
 |      2008.10|     1234 |----+     +---------+-------------------+
 +-------------+----------+    |     | time_id | timestamp         |
                               |     +---------+-------------------+
                               +---->|   1234  | 20080902 12:35:43 |
                                     +---------+-------------------+

=== Multidimensional databases ===
Multidimensional structure is defined as "a variation of the relational model that uses multidimensional structures to organize data and express the relationships between data".<ref>O'Brien & Marakas, 2009, pg 177</ref>  The structure is broken into cubes and the cubes are able to store and access data within the confines of each cube. "Each cell within a multidimensional structure contains aggregated data related to elements along each of its dimensions".<ref>O'Brien & Marakas, 2009, pg 178</ref>  Even when data is manipulated it remains easy to access and continues to constitute a compact database format.  The data still remains interrelated.
Multidimensional structure is quite popular for analytical databases that use online analytical processing (OLAP) applications.<ref>(O'Brien & Marakas, 2009)</ref>  Analytical databases use these databases because of their ability to deliver answers to complex business queries swiftly.  Data can be viewed from different angles, which gives a broader perspective of a problem unlike other models.<ref>Williams, C., Garza, V.R., Tucker, S, Marcus, A.M. (1994, January 24). Multidimensional models boost viewing options. InfoWorld, 16(4)</ref>

=== Aggregations ===
It has been claimed that for complex queries OLAP cubes can produce an answer in around 0.1% of the time required for the same query on [[OLTP]] relational data.<ref>{{cite web
  | author=MicroStrategy, Incorporated
  | year=1995
  | title=The Case for Relational OLAP
  | url=http://www.cs.bgu.ac.il/~onap052/uploads/Seminar/Relational%20OLAP%20Microstrategy.pdf

  |format=PDF| accessdate=2008-03-20
}}</ref><ref>{{cite journal
  |author1=Surajit Chaudhuri  |author2=Umeshwar Dayal
   |lastauthoramp=yes | title = An overview of data warehousing and OLAP technology
  | journal = SIGMOD Rec.
  | publisher = [[Association for Computing Machinery|ACM]]
  | volume = 26
  | issue = 1
  | year = 1997

  | pages = 65
  | url = http://doi.acm.org/10.1145/248603.248616
  | doi = 10.1145/248603.248616

  | accessdate=2008-03-20
}}</ref>  The most important mechanism in OLAP which allows it to achieve such performance is the use of ''aggregations''. Aggregations are built from the fact table by changing the granularity on specific dimensions and aggregating up data along these dimensions. The number of possible aggregations is determined by every possible combination of dimension granularities.

The combination of all possible aggregations and the base data contains the answers to every query which can be answered from the data.<ref>{{cite journal
  | last1 = Gray | first1 = Jim
  | author1-link = Jim Gray (computer scientist)
  | last2 = Chaudhuri | first2 = Surajit
  | last3 = Layman | first3 = Andrew
  | last4 = Reichart | first4 = Don
  | last5 = Venkatrao | first5 = Murali
  | last6 = Pellow | first6 = Frank
  | last7 = Pirahesh | first7 = Hamid
  | title = Data Cube: {A} Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals
  | journal = J. Data Mining and Knowledge Discovery
  | volume = 1
  | issue = 1
  | pages = 29–53
  | year = 1997
  | url = http://citeseer.ist.psu.edu/gray97data.html

  | accessdate=2008-03-20
}}</ref>

Because usually there are many aggregations that can be calculated, often only a predetermined number are fully calculated; the remainder are solved on demand.  The problem of deciding which aggregations (views) to calculate is known as the view selection problem.  View selection can be constrained by the total size of the selected set of aggregations, the time to update them from changes in the base data, or both.  The objective of view selection is typically to minimize the average time to answer OLAP queries, although some studies also minimize the update time.  View selection is [[NP-Complete]]. Many approaches to the problem have been explored, including [[greedy algorithm]]s, randomized search, [[genetic algorithm]]s and [[A* search algorithm]].

==Types==
OLAP systems have been traditionally categorized using the following taxonomy.<ref name=Pendse2006>{{cite web|url=http://www.olapreport.com/Architectures.htm |title=OLAP architectures |publisher=OLAP Report |author=Nigel Pendse |date=2006-06-27 |accessdate=2008-03-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20080124155954/http://www.olapreport.com/Architectures.htm |archivedate=January 24, 2008 }}</ref>

===Multidimensional OLAP (MOLAP)===
MOLAP (multi-dimensional online analytical processing) is the classic form of OLAP and is sometimes referred to as just OLAP. MOLAP stores this data in an optimized multi-dimensional array storage, rather than in a relational database.

Some MOLAP tools require the [[pre-computation]] and storage of derived data, such as consolidations – the operation known as processing. Such MOLAP tools generally utilize a pre-calculated data set referred to as a data cube. The data cube contains all the possible answers to a given range of questions. As a result, they  have a very fast response to queries. On the other hand, updating can take a long time depending on the degree of pre-computation. Pre-computation can also lead to what is known as data explosion.

Other MOLAP tools, particularly those that implement the [[Functional Database Model|functional database model]] do not pre-compute derived data but make all calculations on demand other than those that were previously requested and stored in a cache.

'''Advantages of MOLAP'''
* Fast query performance due to optimized storage, multidimensional indexing and caching.
* Smaller on-disk size of data compared to data stored in [[relational database]] due to compression techniques.
* Automated computation of higher level aggregates of the data.
* It is very compact for low dimension data sets.
* Array models provide natural indexing.
* Effective data extraction achieved through the pre-structuring of aggregated data.

'''Disadvantages of MOLAP'''
* Within some MOLAP Solutions the processing step (data load) can be quite lengthy, especially on large data volumes. This is usually remedied by doing only incremental processing, i.e., processing only the data which have changed (usually new data) instead of reprocessing the entire data set.
* Some MOLAP methodologies introduce data redundancy.

====Products====
Examples of commercial products that use MOLAP are [[Cognos]] Powerplay, [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]], [[Microsoft Analysis Services]], [[Essbase]], [[Applix|TM1]], [[Jedox]], and [[icCube]].

===Relational OLAP (ROLAP)===
'''ROLAP''' works directly with relational databases and does not require pre-computation. The base data and the dimension tables are stored as relational tables and new tables are created to hold the aggregated information. It depends on a specialized schema design. This methodology relies on manipulating the data stored in the relational database to give the appearance of traditional OLAP's slicing and dicing functionality. In essence, each action of slicing and dicing is equivalent to adding a "WHERE" clause in the SQL statement. ROLAP tools do not use pre-calculated data cubes but instead pose the query to the standard relational database and its tables in order to bring back the data required to answer the question. ROLAP tools feature the ability to ask any question because the methodology does not limit to the contents of a cube.  ROLAP also has the ability to drill down to the lowest level of detail in the database.

While ROLAP uses a relational database source, generally the database must be carefully designed for ROLAP use.  A database which was designed for [[OLTP]] will not function well as a ROLAP database.  Therefore, ROLAP still involves creating an additional copy of the data.  However, since it is a database, a variety of technologies can be used to populate the database.

==== Advantages of ROLAP ====
<!--Note to editors:
Please review the discussion page before making changes to the advantages or disadvantages. Thank you.
-->

* ROLAP is considered to be more scalable in handling large data volumes, especially models with [[Dimension (data warehouse)|dimensions]] with very high [[cardinality]] (i.e., millions of members).
* With a variety of data loading tools available, and the ability to fine-tune the [[Extract, transform, load|ETL]] code to the particular data model, load times are generally much shorter than with the automated [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] loads.
* The data are stored in a standard [[relational database]] and can be accessed by any [[SQL]] reporting tool (the tool does not have to be an OLAP tool).
* ROLAP tools are better at handling ''non-aggregatable facts'' (e.g., textual descriptions).  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools tend to suffer from slow performance when querying these elements.
* By [[Decoupling (electronics)|decoupling]] the data storage from the multi-dimensional model, it is possible to successfully model data that would not otherwise fit into a strict dimensional model.
* The ROLAP approach can leverage [[database]] authorization controls such as row-level security, whereby the query results are filtered depending on preset criteria applied, for example, to a given user or group of users ([[SQL]] WHERE clause).

==== Disadvantages of ROLAP ====
<!--Note to editors:
Please review the discussion page before making changes to the advantages or disadvantages. Thank you.
-->

* There is a consensus in the industry that ROLAP tools have slower performance than MOLAP tools. However, see the discussion below about ROLAP performance.
* The loading of ''aggregate tables'' must be managed by custom [[Extract, transform, load|ETL]] code.  The ROLAP tools do not help with this task.  This means additional development time and more code to support.
* When the step of creating aggregate tables is skipped, the query performance then suffers because the larger detailed tables must be queried. This can be partially remedied by adding additional aggregate tables, however it is still not practical to create aggregate tables for all combinations of dimensions/attributes.
* ROLAP relies on the general purpose database for querying and caching, and therefore several special techniques employed by [[MOLAP]] tools are not available (such as special hierarchical indexing).  However, modern ROLAP tools take advantage of latest improvements in [[SQL]] language such as CUBE and ROLLUP operators, DB2 Cube Views, as well as other SQL OLAP extensions.  These SQL improvements can mitigate the benefits of the [[MOLAP]] tools.
* Since ROLAP tools rely on [[SQL]] for all of the computations, they are not suitable when the model is heavy on calculations which don't translate well into [[SQL]]. Examples of such models include budgeting, allocations, financial reporting and other scenarios.

==== Performance of ROLAP ====

In the OLAP industry ROLAP is usually perceived as being able to scale for large data volumes, but suffering from slower query performance as opposed to [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]]. The [http://www.olapreport.com/survey.htm OLAP Survey], the largest independent survey across all major OLAP products, being conducted for 6 years (2001 to 2006) have consistently found that companies using ROLAP report slower performance than those using MOLAP even when data volumes were taken into consideration.

However, as with any survey there are a number of subtle issues that must be taken into account when interpreting the results.
* The survey shows that ROLAP tools have 7 times more users than [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools within each company.  Systems with more users will tend to suffer more performance problems at peak usage times.
* There is also a question about complexity of the model, measured both in number of dimensions and richness of calculations. The survey does not offer a good way to control for these variations in the data being analyzed.

==== Downside of flexibility ====

Some companies select ROLAP because they intend to re-use existing relational database tables—these tables will frequently not be optimally designed for OLAP use.  The superior flexibility of ROLAP tools allows this less than optimal design to work, but performance suffers.  [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] tools in contrast would force the data to be re-loaded into an optimal OLAP design.

===Hybrid OLAP (HOLAP)===
The undesirable trade-off between additional [[Extract, transform, load|ETL]] cost and slow query performance has ensured that most commercial OLAP tools now use a "Hybrid OLAP" (HOLAP) approach, which allows the model designer to decide which portion of the data will be stored in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and which portion in ROLAP.

There is no clear agreement across the industry as to what constitutes "Hybrid OLAP", except that a database will divide data between relational and specialized storage.<ref name="ieee_cite">{{cite journal
  | last1 = Bach Pedersen | first1 = Torben
  | last2 = S. Jensen 
  | title = Multidimensional Database Technology
  | journal = Distributed Systems Online
  | volume = 
  | issue = 
  | issn = 0018-9162
  | pages = 40–46
  | publisher = [[IEEE]]
  | location = 
  | date = December 2001
  | url = http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=00970558
  | doi = 
  | id = 
  | accessdate = | first2 = Christian }}
</ref> For example, for some vendors, a HOLAP database will use relational tables to hold the larger quantities of detailed data, and use specialized storage for at least some aspects of the smaller quantities of more-aggregate or less-detailed data. HOLAP addresses the shortcomings of [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and [[#Relational_OLAP_.28ROLAP.29|ROLAP]] by combining the capabilities of both approaches. HOLAP tools can utilize both pre-calculated cubes and relational data sources.

==== Vertical partitioning ====

In this mode HOLAP stores ''aggregations'' in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and detailed data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]] to optimize time of cube ''processing''.

==== Horizontal partitioning ====

In this mode HOLAP stores some slice of data, usually the more recent one (i.e. sliced by Time dimension) in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] for fast query performance, and older data in [[#Relational_OLAP_.28ROLAP.29|ROLAP]]. Moreover, we can store some dices in [[#Multidimensional_OLAP_.28MOLAP.29|MOLAP]] and others in [[#Relational_OLAP_.28ROLAP.29|ROLAP]], leveraging the fact that in a large cuboid, there will be dense and sparse subregions.<ref>Owen Kaser and Daniel Lemire, [http://arxiv.org/abs/cs.DB/0702143 Attribute Value Reordering for Efficient Hybrid OLAP], Information Sciences, Volume 176, Issue 16, pages 2279-2438, 2006.</ref>

==== Products ====
The first product to provide HOLAP storage was [[Holos]], but the technology also became available in other commercial products such as [[Microsoft Analysis Services]], [[Oracle OLAP|Oracle Database OLAP Option]], [[MicroStrategy]] and [[SAP AG]] BI Accelerator. The hybrid OLAP approach combines ROLAP and MOLAP technology, benefiting from the greater scalability of ROLAP and the faster computation of MOLAP. For example, a HOLAP server may allow large volumes of detail data to be stored in a relational database, while aggregations are kept in a separate MOLAP store. The Microsoft SQL Server 7.0 OLAP Services supports a hybrid OLAP server

===Comparison===
Each type has certain benefits, although there is disagreement about the specifics of the benefits between providers.

* Some MOLAP implementations are prone to database explosion, a phenomenon causing vast amounts of storage space to be used by MOLAP databases when certain common conditions are met: high number of dimensions, pre-calculated results and sparse multidimensional data.
* MOLAP generally delivers better performance due to specialized indexing and storage optimizations. MOLAP also needs less storage space compared to ROLAP because the specialized storage typically includes [[Data compression|compression]] techniques.<ref name="ieee_cite"/>
* ROLAP is generally more scalable.<ref name="ieee_cite"/> However, large volume pre-processing is difficult to implement efficiently so it is frequently skipped.  ROLAP query performance can therefore suffer tremendously.
* Since ROLAP relies more on the database to perform calculations, it has more limitations in the specialized functions it can use.
* HOLAP encompasses a range of solutions that attempt to mix the best of ROLAP and MOLAP.  It can generally pre-process swiftly, scale well, and offer good function support.

===Other types===
The following acronyms are also sometimes used, although they are not as widespread as the ones above:

* '''WOLAP''' – Web-based OLAP
* '''DOLAP''' – [[Desktop computer|Desktop]] OLAP
* '''[[Rtolap|RTOLAP]]''' – Real-Time OLAP

==APIs and query languages==
Unlike [[relational databases]], which had SQL as the standard query language, and widespread [[Application programming interface|API]]s such as [[ODBC]], [[JDBC]] and [[OLEDB]], there was no such unification in the OLAP world for a long time. The first real standard API was [[OLE DB for OLAP]] specification from [[Microsoft]] which appeared in 1997 and introduced the [[Multidimensional Expressions|MDX]] query language. Several OLAP vendors – both server and client – adopted it. In 2001 Microsoft and [[Hyperion Solutions Corporation|Hyperion]] announced the [[XML for Analysis]] specification, which was endorsed by most of the OLAP vendors. Since this also used MDX as a query language, MDX became the de facto standard.<ref>{{cite web|url=http://www.olapreport.com/Comment_APIs.htm |title=Commentary: OLAP API wars |publisher=OLAP Report |author=Nigel Pendse |date=2007-08-23 |accessdate=2008-03-18 |deadurl=yes |archiveurl=https://web.archive.org/web/20080528220113/http://www.olapreport.com/Comment_APIs.htm |archivedate=May 28, 2008 }}</ref>
Since September-2011 [[LINQ]] can be used to query [[Microsoft Analysis Services|SSAS]] OLAP cubes from Microsoft .NET.<ref>{{cite web|url=http://www.agiledesignllc.com/Products|title=SSAS Entity Framework Provider for LINQ to SSAS OLAP}}</ref>

==Products==

===History===
The first product that performed OLAP queries was ''Express,'' which was released in 1970 (and acquired by [[Oracle Corporation|Oracle]] in 1995 from Information Resources).<ref>{{cite web|title=The origins of today's OLAP products |url=http://olapreport.com/origins.htm |publisher=OLAP Report |date=2007-08-23 |author=Nigel Pendse |accessdate=November 27, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20071221044811/http://www.olapreport.com/origins.htm |archivedate=December 21, 2007 }}</ref> However, the term did not appear until 1993 when it was coined by [[Edgar F. Codd]], who has been described as "the father of the relational database". Codd's paper<ref name=Codd1993/> resulted from a short consulting assignment which Codd undertook for former Arbor Software (later [[Hyperion Solutions]], and in 2007 acquired by Oracle), as a sort of marketing coup.  The company had released its own OLAP product, ''[[Essbase]]'', a year earlier. As a result, Codd's "twelve laws of online analytical processing" were explicit in their reference to Essbase. There was some ensuing controversy and when Computerworld learned that Codd was paid by Arbor, it retracted the article.
OLAP market experienced strong growth in late 90s with dozens of commercial products going into market. In 1998, Microsoft released its first OLAP Server – [[Microsoft Analysis Services]], which drove wide adoption of OLAP technology and moved it into mainstream.

===Product comparison===
{{Main|Comparison of OLAP Servers}}

===OLAP Clients===
OLAP clients include many spreadsheet programs like Excel, web application, sql,dashboard tools, etc.

===Market structure===
Below is a list of top OLAP vendors in 2006, with figures in millions of [[US Dollar]]s.<ref>{{cite web
  |url=http://www.olapreport.com/market.htm
  |title=OLAP Market
  |publisher=OLAP Report
  |author=Nigel Pendse
  |year=2006

  |accessdate=2008-03-17
}}</ref>
{| class="wikitable sortable"
|- bgcolor="#CCCCCC" align="center"
! Vendor !! Global Revenue  !! Consolidated company
|-
| [[Microsoft Corporation]] || 1,806   || Microsoft
|-
| [[Hyperion Solutions Corporation]] || 1,077  || Oracle
|-
| [[Cognos]] || 735  || IBM
|-
| [[Business Objects (company)|Business Objects]] || 416 || SAP
|-
| [[MicroStrategy]] || 416 || MicroStrategy
|-
| [[SAP AG]] || 330 || SAP
|-
| Cartesis ([[SAP AG|SAP]]) || 210  || SAP
|-
| [[Applix]] || 205  || IBM
|-
| [[Infor]] || 199  || Infor
|-
| [[Oracle Corporation]] || 159 || Oracle
|-
| Others || 152  || Others
|-
| '''Total''' || '''5,700'''
|}

=== Open-source ===
* [[Druid (open-source data store)]] is a popular [[open-source]] distributed data store for OLAP queries that is used at scale in production by various organizations.
* [[Apache Kylin]] is a distributed data store for OLAP queries originally developed by eBay.
* [[Cubes (OLAP server)]] is another light-weight [[open-source]] toolkit implementation of OLAP functionality in the [[Python (programming language)|Python programming language]] with built-in ROLAP.
* [[Linkedin Pinot]] is used at LinkedIn to deliver scalable real time analytics with low latency.<ref>{{cite news |last= Yegulalp |first=Serdar |date=2015-06-11 |title= LinkedIn fills another SQL-on-Hadoop niche |url=http://www.infoworld.com/article/2934506/olap/linkedins-pinot-fills-another-sql-on-hadoop-niche.html |magazine=InfoWorld |access-date=2016-11-19}}</ref> It can ingest data from offline data sources (such as Hadoop and flat files) as well as online sources (such as Kafka). Pinot is designed to scale horizontally.

== See also ==
{{portal|Computer science}}
* [[Comparison of OLAP Servers]]
* [[Data warehouse]]
* [[Online transaction processing]] (OLTP)
* [[Business analytics]]
* [[Predictive analytics]]
* [[Data Mining]]
* [[Thomsen Diagrams]]
* [[Functional Database Model]]

==Bibliography==
* {{cite web
  |url= http://www.daniel-lemire.com/OLAP/
  |title= Data Warehousing and OLAP-A Research-Oriented Bibliography
  |author= Daniel Lemire
  |date= December 2007
  }}

* {{cite book
  | title = OLAP Solutions: Building Multidimensional Information Systems, 2nd Edition
  | publisher = John Wiley & Sons
  | series =
  | year = 1997
  | isbn = 978-0-471-14931-6
  | author = Erik Thomsen. }}

* Ling Liu and Tamer M. Özsu (Eds.) (2009).  "[http://www.springer.com/computer/database+management+&+information+retrieval/book/978-0-387-49616-0 Encyclopedia of Database Systems], 4100 p.&nbsp;60 illus. ISBN 978-0-387-49616-0.
* O'Brien, J. A., & Marakas, G. M. (2009). Management information systems (9th ed.). Boston, MA: McGraw-Hill/Irwin.

==References==
{{Reflist|30em|refs=
<ref name=ahsan>
{{cite journal
|last1=Abdullah
|first1=Ahsan
|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool
|journal=Computers and Electronics in Agriculture |date=November 2009 |volume=69 |issue=1 |pages=59–72 |doi=10.1016/j.compag.2009.07.003
}}
</ref>
}}

{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Online Analytical Processing}}
[[Category:Online analytical processing| ]]
[[Category:Data management]]
[[Category:Information technology management]]
<=====doc_Id=====>:273
<=====title=====>:
Data thinking
<=====text=====>:
{{unreferenced|date=September 2013}}
'''Data thinking''' is the generic mental pattern observed during the processes of picking a subject to start with, identifying its parts or components, organizing and describing them in an informative fashion that is relevant to what motivated and initiated the whole processes.

The term was created by Mario Faria and Rogerio Panigassi in 2013 when they were writing a book about data science, [[data analysis|data analytics]], data management and how data practitioners were able to achieve their goals.

Mario Faria is one of the first [[Chief Data Officer]]s in the world.



[[Category:Data management]]
<=====doc_Id=====>:276
<=====title=====>:
Small data
<=====text=====>:

==Introduction==
The term small data did not exist before the word big data which came into use in the 1990's. What we once called data is now called small data.  Small data was not made useless by the advent of big data. Most data we see in our life is small data, and it should not be overlooked in any field. 

This article helps to define small data and to give examples in marketing and recruiting to help in understanding.

==Definition==
'''Small data''' (''sm’aē’āll DH(ə)ta'') is data that is 'small' enough for human comprehension.<ref>{{cite news|author=Rufus Pollock |url=https://www.theguardian.com/news/datablog/2013/apr/25/forget-big-data-small-data-revolution |title=Forget big data, small data is the real revolution &#124; News |newspaper=[[The Guardian]] |date= |accessdate=2016-10-02}}</ref><ref>{{cite web|url=http://jwork.org/main/node/18 |title="Small data". Never heard this term? |website=jWork.ORG |date= |accessdate=2016-10-02}}</ref> It is data in a volume and format that makes it accessible, informative and actionable.<ref>{{cite web|url=http://whatis.techtarget.com/definition/small-data |title=What is small data? - Definition from WhatIs.com |website=Whatis.techtarget.com |date=2016-08-18 |accessdate=2016-10-02}}</ref>

The term "big data" is about machines and "small data" is about people.<ref>{{cite web|author=Eric Lundquist |url=http://www.eweek.com/enterprise-apps/small-data-analysis-the-next-big-thing-advocates-assert.html/ |title='Small Data' Analysis the Next Big Thing, Advocates Assert |website=Eweek.com |date=2013-09-10 |accessdate=2016-10-02}}</ref> This is to say that eye witness observations or five pieces of related data could be small data. Small data is what we used to think of as data. The only way to comprehend [[Big data]] is to reduce the data into small, visually-appealing objects representing various aspects of large data sets (such as
[[histogram]], [[chart]]s, and scatter plots). So sometimes big data is simplified to be like small data. 

A formal definition of small data has been proposed by Allen Bonde, VP of Innovation at [[Actuate Corporation|Actuate]]: "Small data connects people with timely, meaningful insights (derived from big data and/or “local” sources), organized and packaged – often visually – to be accessible, understandable, and actionable for everyday tasks."<ref>{{cite web|url=http://smalldatagroup.com/2013/10/18/defining-small-data/ |title=Defining Small Data |publisher=Small Data Group |date= |accessdate=2016-10-02}}</ref>
 
Another definition of '''small data''' is:
* The small set of specific attributes produced by the [[Internet of Things]]. These are typically a small set of sensor data such as temperature, wind speed, vibration and status.<ref>{{cite web|author= |url=http://www.forbes.com/sites/mikekavis/2015/02/25/forget-big-data-small-data-is-driving-the-internet-of-things/#4a72ffad661b |title=Forget Big Data - Small Data Is Driving The Internet Of Things |website=Forbes.com |date= |accessdate=2016-10-02}}</ref>


==Some Examples of Uses in Business==

===Marketing===

Bonde has written extensively about the topic for Forbes,<ref>{{cite web|author= |url=http://www.forbes.com/sites/markfidelman/2012/10/30/these-smart-social-apps-bring-big-data-down-to-size/ |title=These Smart, Social Apps Bring Big Data Down to Size |website=Forbes.com |date= |accessdate=2016-10-02}}</ref> Direct Marketing News,<ref>{{cite web|url=http://www.dmnews.com/why-small-data-is-the-next-big-thing-for-marketers/article/308376/ |title=Why Small Data Is the Next Big Thing for Marketers - DMN |website=Dmnews.com |date=2013-08-22 |accessdate=2016-10-02}}</ref> CMO.com<ref>{{cite web|last=Bonde |first=Allen |url=http://www.cmo.com/features/articles/2013/11/20/think_small_time_for.html |title=Think Small: Time For Marketers To Move Beyond The Big Data Hype |website=Cmo.com |date=2013-12-12 |accessdate=2016-10-02}}</ref> and other publications.  

According to Martin Lindstrom, in his book, [[Small Data: The Tiny Clues that Uncover Huge Trends|Small Data:]] "{In customer research, small data is} Seemingly insignificant behavioral observations containing very specific attributes pointing towards an unmet customer need. Small data is the foundation for break through ideas or completely new ways to turnaround brands."<ref>{{cite web|url=https://www.martinlindstrom.com/small-data/ |title=Small Data - Martin Lindstrom - Bestselling Author |publisher=Martin Lindstrom |date= |accessdate=2016-10-02}}</ref>

==Conclusion==
Small Data is what we used to call data. The hype about Big Data should not cause us to look down on Small Data. Small Data's practicality and depth of insight is often better than big data.

==References==
{{Reflist}}

[[Category:Data management]]
<=====doc_Id=====>:279
<=====title=====>:
Geospatial metadata
<=====text=====>:
'''Geospatial metadata''' (also '''geographic metadata''', or simply  '''metadata''' when used in a geographic context) is a type of [[metadata]] that is applicable to objects that have an explicit or implicit [[Geography|geographic]] extent, i.e. are associated with some position on the surface of the [[globe]]. Such objects may be stored in a [[geographic information system]] (GIS) or may simply be documents, data-sets, images or other objects, services, or related items that exist in some other native environment but whose features may be appropriate to describe in a (geographic) metadata catalog (may also be known as a data directory, data inventory, etc.).

==Definition==
I'''SO 19115:2013 "Geographic Information - Metadata'''"<ref name=":0">{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:19115:-1:ed-1:v1:en|title=ISO 19115-1:2014(en)|last=International Organization for Standardization|first=|date=2014-04-01|website=ISO|publisher=|access-date=2016-04-01}}</ref> from [[ISO/TC 211]], the industry standard for geospatial metadata, describes its scope as follows:

"''[This standard] provides information about the identification, the extent, the quality, the spatial and temporal aspects, the content, the spatial reference, the portrayal, distribution, and other properties of digital geographic data and services"<ref name=":0" />''

ISO 19115:2013 also provides for non-digital mediums: ''"[t]hough this part of [https://www.iso.org/obp/ui/#iso:std:iso:19115:en ISO 19115] is applicable to digital data and services, its principles can be extended to many other types of resources such as maps, charts, and textual documents as well as non-geographic data.''" <ref name=":0" />

'''The U.S. Federal Geographic Data Committee (FGDC)''' describes geospatial metadata as follows:

"''A metadata record is a file of information, usually presented as an XML document, which captures the basic characteristics of a data or information resource. It represents the who, what, when, where, why and how of the resource. Geospatial metadata commonly document geographic digital data such as Geographic Information System (GIS) files, geospatial databases, and earth imagery but can also be used to document geospatial resources including data catalogs, mapping applications, data models and related websites. Metadata records include core library catalog elements such as Title, Abstract, and Publication Data; geographic elements such as Geographic Extent and Projection Information; and database elements such as Attribute Label Definitions and Attribute Domain Values.''" <ref>{{Cite web|url=http://www.fgdc.gov/metadata|title=Geospatial Metadata — Federal Geographic Data Committee|website=www.fgdc.gov|access-date=2016-04-01}}</ref>

==History==
The growing appreciation of the value of geospatial metadata through the 1980s and 1990s led to the development of a number of initiatives to collect metadata according to a variety of formats either within agencies, communities of practice, or countries/groups of countries. For example, [[NASA]]'s "DIF" metadata format was developed during an Earth Science and Applications Data Systems Workshop in 1987,<ref>[http://gcmd.nasa.gov/User/difguide/whatisadif.html Gene Major and Lola Olsen: "A short history of the DIF". On GCMD website, visited 16 October 2006]</ref> and formally approved for adoption in 1988. Similarly, the U.S. FGDC developed its geospatial metadata standard over the period 1992–1994.<ref>[http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Libraries Guide: "Federal Geographic Data Committee (FGDC) Metadata". On MIT Libraries website, visited 16 October 2006]
</ref> The Spatial Information Council of Australia and New Zealand (ANZLIC),<ref>
{{cite web
| url         = http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf
| title       = ANZLIC Metadata Profile Guidelines version 1.2 July 2011
| year        = 2011
| publisher   = ANZLIC
| accessdate  = 2011-04-11
| quote       = ANZLIC[:] The Spatial Information Council of Australia and New Zealand (formerly known as the Australia New Zealand Land Information Council)
}}
</ref> a combined body representing spatial data interests in Australia and New Zealand, released version 1 of its "metadata guidelines" in 1996.<ref>[http://anzlic.gov.au/resources/anzlic-metadata-profile ANZLIC Metadata Guidelines: Core metadata elements for geographic data in Australia and New Zealand, Version 2 (February 2001)]</ref> [[ISO/TC 211]] undertook the task of harmonizing the range of formal and ''de facto'' standards over the approximate period 1999–2002, resulting in the release of '''ISO 19115''' "'''Geographic Information - Metadata'''" in 2003 and a subsequent revision in 2013. {{As of | 2011}} individual countries, communities of practice, agencies, etc. have started re-casting their previously used metadata standards as "profiles" or recommended subsets of ISO 19115, occasionally with the inclusion of additional metadata elements as formal extensions to the ISO standard. The growth in popularity of Internet technologies and data formats, such as [[Extensible Markup Language]] (XML) during the 1990s led to the development of mechanisms for exchanging geographic metadata on the [[World Wide Web|web]]. In 2004, the [[Open Geospatial Consortium]] released the current version (3.1) of [[Geography Markup Language]] (GML), an XML grammar for expressing geospatial features and corresponding metadata. With the growth of the [[Semantic Web]] in the 2000s, the geospatial community has begun to develop [[Ontology (computer science)|ontologies]] for representing semantic geospatial metadata. Some examples include the [http://www.ordnancesurvey.co.uk/oswebsite/ontology/ Hydrology and Administrative ontologies] developed by the [[Ordnance Survey]] in the [[United Kingdom]].

==ISO 19115: Geographic information - Metadata==
ISO 19115 is a standard of the International Organization for Standardization (ISO).<ref>ISO 19115 Geographic Information - Metadata. International Organization for Standardization (ISO), Geneva, 2003</ref> The standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series). ISO 19115 and its parts define how to describe geographical information and associated services, including contents, spatial-temporal purchases, data quality, access and rights to use.

The objective of this International Standard is to provide a clear procedure for the description of digital geographic data-sets so that users will be able to determine whether the data in a holding will be of use to them and how to access the data. By establishing a common set of metadata terminology, definitions and extension procedures, this standard promotes the proper use and effective retrieval of geographic data.<ref>{{cite web|title=ISO 19115 Metadata Factsheet|url=http://www.isotc211.org/Outreach/Overview/Factsheet_19115.pdf|publisher=AG Outreach|accessdate=2012-11-22}}</ref>

ISO 19115 was revised in 2013 to accommodate growing use of the internet for metadata management, as well as add many new categories of metadata elements (referred to as codelists) and the ability to limit the extent of metadata use temporally or by user.<ref>{{Cite web|url=https://wiki.earthdata.nasa.gov/display/NASAISO/NASA+Metadata+and+the+New+ISO+19115-1+Capabilities|title=NASA Metadata and the New ISO 19115-1 Capabilities - NASA ISO for EOSDIS - Earthdata Wiki|website=wiki.earthdata.nasa.gov|access-date=2016-04-01}}</ref>

{{Expand section|date=June 2012}}

==ISO 19139 Geographic information Metadata XML schema implementation==
ISO 19139:2012 <ref>{{Cite web|url=https://www.iso.org/obp/ui/#iso:std:iso:ts:19139:-2:ed-1:v1:en|title=ISO/TS 19139-2:2012(en)|last=International Organization for Standardization|first=|date=2012-12-15|website=ISO|publisher=|access-date=2016-04-01}}</ref> provides the XML implementation schema for ISO 19115 specifying the metadata record format and may be used to describe, validate, and exchange geospatial metadata prepared in XML.<ref>[http://marinemetadata.org/references/iso19139 "ISO 19139 Geographic information Metadata XML schema implementation"], Marine Metadata Interoperability Project</ref>

The standard is part of the [[ISO/TC 211|ISO geographic information suite of standards]] (19100 series), and provides a spatial metadata XML (spatial metadata eXtensible Mark-up Language (smXML)) encoding, an XML schema implementation derived from ISO 19115, Geographic information – Metadata. The metadata includes information about the identification, constraint, extent, quality, spatial and temporal reference, distribution, lineage, and maintenance of the digital geographic data-set.

{{Expand section|date=June 2012}}

==Metadata directories==
Also known as metadata catalogues or data directories.

(need discussion of, and subsections on GCMD, FGDC metadata gateway, ASDD, European and Canadian initiatives, etc. etc.)
* [http://gisinventory.net GIS Inventory] – National GIS Inventory System which is maintained by the US-based [[National States Geographic Information Council|National States Geographic Information Council (NSGIC)]] as a tool for the entire US GIS Community. Its primary purpose is to track data availability and the status of geographic information system (GIS) implementation in state and local governments to aid the planning and building of statewide spatial data infrastructures (SSDI). The Random Access Metadata for Online Nationwide Assessment (RAMONA) database is a critical component of the GIS Inventory. RAMONA moves its FGDC-compliant metadata (CSDGM Standard) for each data layer to a web folder and a Catalog Service for the Web (CSW) that can be harvested by Federal programs and others. This provides far greater opportunities for discovery of user information. The GIS Inventory website was originally created in 2006 by NSGIC under award NA04NOS4730011 from the Coastal Services Center, National Oceanic and Atmospheric Administration, U.S. Department of Commerce. The Department of Homeland Security has been the principal funding source since 2008 and they supported the development of the Version 5 during 2011/2012 under Order Number HSHQDC-11-P-00177. The Federal Emergency Management Agency and National Oceanic and Atmospheric Administration have provided additional resources to maintain and improve the GIS Inventory. Some US Federal programs require submission of CSDGM-Compliant Metadata for data created under grants and contracts that they issue. The GIS Inventory provides a very simple interface to create the required Metadata. 
* [http://gcmd.nasa.gov GCMD] - Global Change Master Directory's goal is to enable users to locate and obtain access to Earth science data sets and services relevant to global change and Earth science research. The GCMD database holds more than 20,000 descriptions of Earth science data sets and services covering all aspects of Earth and environmental sciences.
* [http://earthdata.nasa.gov/echo ECHO] - The EOS Clearing House (ECHO) is a spatial and temporal metadata registry, service registry, and order broker. It allows users to more efficiently search and access data and services through the [http://reverb.earthdata.nasa.gov/echo Reverb Client] or Application Programmer Interfaces (APIs). ECHO stores metadata from a variety of science disciplines and domains, totalling over 3400 Earth science data sets and over 118&nbsp;million granule records.
* [http://www.gogeo.ac.uk/gogeo/ GoGeo] - GoGeo is a service run by [[EDINA]] (University of Edinburgh) and is supported by [[Jisc]]. GoGeo allows users to conduct geographically targeted searches to discover geospatial datasets. GoGeo searches many data portals from the HE and FE community and beyond. GoGeo also allows users to create standards compliant metadata through its Geodoc metadata editor.

==Geospatial metadata tools==
There are many commercial GIS or geospatial products that support metadata viewing and editing on GIS resources. For example, [[ESRI]]'s [[ArcGIS]] Desktop, [[SOCET GXP]], [[Autodesk]]'s AutoCAD Map 3D 2008, [[Arcitecta]]'s [[Mediaflux]] and [[Intergraph]]'s [[GeoMedia]] support geospatial metadata extensively.

[http://gisinventory.net GIS Inventory] is a free web-based tool that provides a very simple interface to create geospatial metadata. Participants create a profile and document their data layers through a survey-style interface. The GIS Inventory produces metadata that is compliant with the Federal Content Standard for Digital Geospatial Metadata (CSDGM). The GIS Inventory is also capably of ingesting already completed metadata through document upload and web server connectivity. Through the GIS Inventory web services, metadata are automatically shared with US Federal agencies.

[http://geonetwork-opensource.org GeoNetwork opensource] is a comprehensive [[Free and Open Source Software]] solution to manage and publish geospatial metadata and services based on international metadata and catalog standards. The software is part of the [[Open Source Geospatial Foundation]]'s software stack.

[http://geocat.net/bridge GeoCat Bridge] allows to edit, validate and directly publish metadata from [[ArcGIS]] Desktop to [http://geonetwork-opensource.org GeoNetwork] (and generic CSW catalogs) and publishes data as map services on [http://geoserver.org GeoServer]. Several metadata profiles are supported.

[[pycsw]] is an OGC CSW server implementation written in Python. pycsw fully implements the OpenGIS Catalogue Service Implementation Specification ([[Catalog Service for the Web|Catalogue Service for the Web]]). The project is certified OGC Compliant, and is an OGC Reference Implementation.

[http://catmdedit.sourceforge.net/ CATMDEdit]
terraCatalog
ArcCatalog
ArcGIS Server Portal
[http://geonetwork-opensource.org GeoNetwork opensource]
[http://www.conterra.de/en/products/sdi/terracatalog/index.shtm IME]
[http://www.intelec.ca/html/en/technologies/m3cat.html M3CAT MetaD]
[http://www.gigateway.org.uk/metadata/metagenie.html MetaGenie]
Parcs Canada Metadata Editor
Mapit/CADit
NOKIS Editor

{{Expand section|date=June 2008}}

==References==
<references/>
[http://anzlic.gov.au/sites/default/files/files/ANZLICmetadataProfileGuidelines_v1-2.pdf ANZLIC Metadata Profile Version 1.2 (viewed July 2011)]

==External links==
*[http://www.fgdc.gov/metadata FGDC metadata page]
*[http://gcmd.nasa.gov/ Global Change Master Directory(GCMD)]
*[http://wiki.milcord.com/wiki/Geospatial_Exploitation_of_Motion_Imagery Geospatial Exploitation of Motion Imagery] is a geospatially aware and integrated Intelligent Video Surveillance (IVS) software system targeted at real-time and forensic video analytic and mining applications that require low-resolution detection, tracking, and classification of moving objects (people and vehicles) in outdoor, wide-area scenes.
*[http://www.iso.org/iso/en/CatalogueDetailPage.CatalogueDetail?CSNUMBER=26020 ISO 19115:2003 Geographic information -- Metadata]
*[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32557 Geographic information -- Metadata -- XML schema implementation ]
*[http://www.earthdatamodels.org/designs/metadata_BGS.html EarthDataModels design for Metadata] is a logical data model and physical implementation of a Spatial Metadata Database, based on ISO19115 and is INSPIRE compliant.

{{use dmy dates|date=January 2011}}

[[Category:Data management]]
[[Category:Metadata]]
[[Category:Geographic data and information]]
<=====doc_Id=====>:282
<=====title=====>:
PhUSE
<=====text=====>:
{{unreferenced|date=June 2014}}
[[File:PhUSE Computational Science Symposium 2016 (26133831630).jpg|thumb|PhUSE Computational Science Symposium 2016]]
'''PhUSE''', or '''Pharmaceutical Users Software Exchange''' is an independent, [[not-for-profit]] organization, that started in Europe, but now which serves as forum and global platform for [[clinical data management]], [[biostatistics]], and [[eClinical]] [[information technology]] professionals. It provides three collaboration platforms for members, a set online suits which implements worldwide collaboration tools, a [[wiki]], a repository of videos (PhUSE Tube), a [[blog]], a [[webforum]] and an [[archive]].

PhUSE also publishes '''[[Pharmaceutical Programming]]''',  an [[academic journal]] focusing on programming for [[Drug regulation|drug regulation environments]] of the [[pharmaceutical]] industry, a quarterly [[newsletter]], '''PhUSE News'''. In addition, it organizes an annual [[Meeting|conference]].

==External links==

* [http://www.phuse.eu/ Official web site]
* [http://www.phusewiki.org/wiki/index.php?title=PhUSE_Wiki PhUSE Wiki]
* [http://www.phuse.eu/blog/blog.aspx Blog]
* [http://www.phuse.eu/Society-Newsletters.aspx Newsletter]
* [http://www.phuse.eu/forum.aspx Forum]
* [http://www.phuse.eu/archive.aspx Archive]
* [http://www.phuse.eu/phusetube.aspx PhUSE Tube]
* [http://www.phuse.eu/publications.aspx Publications]

[[Category:Pharmacy organizations]]
[[Category:Regulation|Therapeutic goods]]
[[Category:Biostatistics]]
[[Category:Data management]]
<=====doc_Id=====>:285
<=====title=====>:
Computer-aided software engineering
<=====text=====>:
[[File:Umbrello 1.png|320px|thumb|Example of a CASE tool.]]
'''Computer-aided software engineering''' ('''CASE''') is the domain of software tools used to design and implement applications. CASE tools are similar to and were partly inspired by [[computer-aided design]] (CAD) tools used for designing hardware products.  CASE tools are used for developing high-quality, defect-free, and maintainable software.<ref>Kuhn, D.L (1989). "Selecting and effectively using a computer aided software engineering tool". Annual Westinghouse computer symposium; 6–7 Nov 1989; Pittsburgh, PA (U.S.); DOE Project.</ref> CASE software is often associated with methods for the development of [[information system]]s together with automated tools that can be used in the [[software development process]].<ref>P. Loucopoulos and V. Karakostas (1995). ''System Requirements Engineerinuality software which will perform effectively.</ref>

== History ==
The Information System Design and Optimization System (ISDOS) project, started in 1968 at the University of Michigan, initiated a great deal of interest in the whole concept of using computer systems to help analysts in the very difficult process of analysing requirements and developing systems. Several papers by Daniel Teichroew fired a whole generation of enthusiasts with the potential of automated systems development. His Problem Statement Language / Problem Statement Analyzer (PSL/PSA) tool was a CASE tool although it predated the term.<ref>{{cite journal|last1=Teichroew|first1=Daniel|last2=Hershey|first2=Ernest Allen|title=PSL/PSA a computer-aided technique for structured documentation and analysis of information processing systems|journal=Proceeding ICSE '76 Proceedings of the 2nd international conference on Software engineering|date=1976|url=http://dl.acm.org/citation.cfm?id=807641|publisher=IEEE Computer Society Press}}</ref>

Another major thread emerged as a logical extension to the [[data dictionary]] of a [[database]]. By extending the range of [[metadata]] held, the attributes of an application could be held within a dictionary and used at runtime. This "active dictionary" became the precursor to the more modern [[model-driven engineering]] capability. However, the active dictionary did not provide a graphical representation of any of the metadata. It was the linking of the concept of a dictionary holding analysts' metadata, as derived from the use of an integrated set of techniques, together with the graphical representation of such data that gave rise to the earlier versions of CASE.<ref>{{cite book|last1=Coronel|first1=Carlos|last2=Morris|first2=Steven|title=Database Systems: Design, Implementation, & Management|date=February 4, 2014|publisher=Cengage Learning|isbn=1285196147|pages=695–700|url=https://books.google.com/books?id=QWLpAgAAQBAJ&pg=PA698&lpg=PA698&dq=case+tools+data+dictionary&source=bl&ots=eJt_GWYHGx&sig=MZEMesWkJrGdczKSEZ_6bnqdNAY&hl=en&sa=X&ei=HNF0VOvWDu3xigK5FQ&ved=0CFIQ6AEwCQ#v=onepage&q=case%20tools%20data%20dictionary&f=false|accessdate=25 November 2014}}</ref>

The term was originally coined by software company Nastec Corporation of Southfield, Michigan in 1982 with their original integrated graphics and text editor GraphiText, which also was the first microcomputer-based system to use hyperlinks to cross-reference text strings in documents&mdash;an early forerunner of today's web page link.  GraphiText's successor product, DesignAid, was the first microprocessor-based tool to logically and semantically evaluate software and system design diagrams and build a data dictionary.

Under the direction of [[Albert F. Case, Jr.]] vice president for product management and consulting, and Vaughn Frick, director of product management, the DesignAid product suite was expanded to support analysis of a wide range of [[Structured Analysis and Design Technique|structured analysis and design methodologies]], including those of [[Ed Yourdon]] and [[Tom DeMarco]], [[Chris Gane (computer scientist)|Chris Gane]] & [[Trish Sarson]], Ward-Mellor (real-time) SA/SD and [[Warnier-Orr]] (data driven).<ref>{{cite journal|last1=Case|first1=Albert|title=Computer-aided software engineering (CASE): technology for improving software development productivity|journal=ACM SIGMIS Database|date=Fall 1985|volume=17|issue=1|pages=35–43|url=http://dl.acm.org/citation.cfm?id=1040698}}</ref>

The next entrant into the market was Excelerator from Index Technology in Cambridge, Mass.  While DesignAid ran on Convergent Technologies and later Burroughs Ngen networked microcomputers, Index launched Excelerator on the IBM PC/AT platform. While, at the time of launch, and for several years, the IBM platform did not support networking or a centralized database as did the Convergent Technologies or Burroughs machines, the allure of IBM was strong, and Excelerator came to prominence. Hot on the heels of Excelerator were a rash of offerings from companies such as Knowledgeware (James Martin, [[Fran Tarkenton]] and Don Addington), Texas Instrument's [[Information Engineering Facility|IEF]] and [[Andersen Consulting|Andersen Consulting's]] FOUNDATION toolset (DESIGN/1, INSTALL/1, FCP).

CASE tools were at their peak in the early 1990s.<ref>{{cite news|last1=Yourdon|first1=Ed|title=Can XP Projects Grow?|url=https://books.google.com/books?id=_faqtO2fEbkC&pg=PA28&lpg=PA28&dq=CASE+tools+most+interest+90%27s&source=bl&ots=9WNDAYPU89&sig=vC_s1JtRyOwcHcCvyDici5H9Z7w&hl=en&sa=X&ei=lNd0VPr1De2rjAK6o4D4DA&ved=0CDIQ6AEwAw#v=onepage&q=CASE%20tools%20most%20interest%2090%27s&f=false|accessdate=25 November 2014|publisher=Computerworld|date=Jul 23, 2001}}</ref>  At the time [[IBM]] had proposed AD/Cycle, which was an alliance of software vendors centered on IBM's [[Software repository]] using [[IBM DB2]] in [[Mainframe computer|mainframe]] and [[OS/2]]:

:''The application development tools can be from several sources: from IBM, from vendors, and from the customers themselves. IBM has entered into relationships with [[Bachman Information Systems]], Index Technology Corporation, and [[KnowledgeWare|Knowledgeware]] wherein selected products from these vendors will be marketed through an IBM complementary marketing program to provide offerings that will help to achieve complete life-cycle coverage''.<ref name="ADC_SAaA">"AD/Cycle strategy and architecture", IBM Systems Journal, Vol 29, NO 2, 1990; p. 172.</ref>

With the decline of the mainframe, AD/Cycle and the Big CASE tools died off, opening the market for the mainstream CASE tools of today. Many of the leaders of the CASE market of the early 1990s ended up being purchased by [[Computer Associates]], including IEW, IEF, ADW, Cayenne, and Learmonth & Burchett Management Systems (LBMS).  The other trend that led to the evolution of CASE tools was the rise of object-oriented methods and tools. Most of the various tool vendors added some support for object-oriented methods and tools.  In addition new products arose that were designed from the bottom up to support the object-oriented approach. Andersen developed its project Eagle as an alternative to Foundation. Several of the thought leaders in object-oriented development each developed their own methodology and CASE tool set: Jacobsen, Rumbaugh, [[Grady Booch|Booch]], etc. Eventually, these diverse tool sets and methods were consolidated via standards led by the [[Object Management Group]] (OMG). The OMG's [[Unified Modelling Language]] (UML) is currently widely accepted as the industry standard for object-oriented modeling.

== CASE software ==
A. Fuggetta classified CASE software into 3 categories:<ref name="AF_93">{{cite journal
| author      = Alfonso Fuggetta
|date=December 1993
| title       = A classification of CASE technology
| journal     = Computer
| volume      = 26
| issue       = 12
| pages       = 25–38
| doi         = 10.1109/2.247645
| url         = http://www2.computer.org/portal/web/csdl/abs/mags/co/1993/12/rz025abs.htm
| accessdate  = 2009-03-14
}}
</ref>
# ''Tools'' support specific tasks in the [[software life-cycle]].
# ''Workbenches'' combine two or more tools focused on a specific part of the software life-cycle.
# ''Environments'' combine two or more tools or workbenches and support the complete software life-cycle.

=== Tools ===
CASE tools supports specific tasks in the software development life-cycle. They can be divided into the following categories:
# Business and Analysis modeling. Graphical modeling tools. E.g., E/R modeling, object modeling, etc.
# Development. Design and construction phases of the life-cycle. Debugging environments. E.g., [[GNU Debugger]].
# Verification and validation. Analyze code and specifications for correctness, performance, etc.  
# Configuration management. Control the check-in and check-out of repository objects and files. E.g., [[Source Code Control System|SCCS]], CMS.
# Metrics and measurement. Analyze code for complexity, modularity (e.g., no "go to's"), performance, etc. 
# Project management. Manage project plans, task assignments, scheduling.
Another common way to distinguish CASE tools is the distinction between Upper CASE and Lower CASE. Upper CASE Tools support business and analysis modeling. They support traditional diagrammatic languages such as [[ER diagram]]s, [[Data flow diagram]], [[Structure chart]]s, [[Decision Tree]]s, [[Decision table]]s, etc. Lower CASE Tools support development activities, such as physical design, debugging, construction, testing, component integration, maintenance, and reverse engineering. All other activities span the entire life-cycle and apply equally to upper and lower CASE.<ref>Software Engineering: Tools, Principles and Techniques by Sangeeta Sabharwal, Umesh Publications</ref>

=== Workbenches ===
Workbenches integrate two or more CASE tools and support specific software-process activities. Hence they achieve:
*a homogeneous and consistent interface (presentation integration).
*seamless integration of tools and tool chains (control and data integration).

An example workbench is Microsoft's [[Visual Basic]] programming environment. It incorporates several development tools: a GUI builder, smart code editor, debugger, etc. Most commercial CASE products tended to be such workbenches that seamlessly integrated two or more tools. Workbenches also can be classified in the same manner as tools; as focusing on Analysis, Development, Verification, etc. as well as being focused on upper case, lower case, or processes such as configuration management that span the complete life-cycle.

=== Environments ===
An environment is a collection of CASE tools or workbenches that attempts to support the complete software process. This contrasts with tools that focus on one specific task or a specific part of the life-cycle. CASE environments are classified by Fuggetta as follows:<ref name="AF_93" />
#Toolkits. Loosely coupled collections of tools. These typically build on operating system workbenches such as the Unix Programmer's Workbench or the VMS VAX set. They typically perform integration via piping or some other basic mechanism to share data and pass control. The strength of easy integration is also one of the drawbacks. Simple passing of parameters via technologies such as shell scripting can't provide the kind of sophisticated integration that a common repository database can. 
#Fourth generation. These environments are also known as 4GL standing for fourth generation language environments due to the fact that the early environments were designed around specific languages such as Visual Basic. They were the first environments to provide deep integration of multiple tools. Typically these environments were focused on specific types of applications. For example, user-interface driven applications that did standard atomic transactions to a relational database. Examples are Informix 4GL, and Focus.
#Language-centered. Environments based on a single often object-oriented language such as the Symbolics Lisp Genera environment or VisualWorks Smalltalk from Parcplace. In these environments all the operating system resources were objects in the object-oriented language. This provides powerful debugging and graphical opportunities but the code developed is mostly limited to the specific language. For this reason, these environments were mostly a niche within CASE. Their use was mostly for prototyping and R&D projects. A common core idea for these environments was the [[model-view-controller]] user interface that facilitated keeping multiple presentations of the same design consistent with the underlying model. The MVC architecture was adopted by the other types of CASE environments as well as many of the applications that were built with them. 
#Integrated. These environments are an example of what most IT people tend to think of first when they think of CASE. Environments such as IBM's AD/Cycle, Andersen Consulting's FOUNDATION, the ICL [[CADES]] system, and DEC Cohesion. These environments attempt to cover the complete life-cycle from analysis to maintenance and provide an integrated database repository for storing all artifacts of the software process. The integrated software repository was the defining feature for these kinds of tools. They provided multiple different design models as well as support for code in heterogenous languages. One of the main goals for these types of environments was "round trip engineering": being able to make changes at the design level and have those automatically be reflected in the code and vice versa. These environments were also typically associated with a particular methodology for software development. For example, the FOUNDATION CASE suite from Andersen was closely tied to the Andersen Method/1 methodology.
#Process-centered. This is the most ambitious type of integration. These environments attempt to not just formally specify the analysis and design objects of the software process but the actual process itself and to use that formal process to control and guide software projects. Examples are East, Enterprise II, Process Wise, Process Weaver, and Arcadia. These environments were by definition tied to some methodology since the software process itself is part of the environment and can control many aspects of tool invocation.

In practice, the distinction between workbenches and environments was flexible. Visual Basic for example was a programming workbench but was also considered a 4GL environment by many. The features that distinguished workbenches from environments were deep integration via a shared repository or common language and some kind of methodology (integrated and process-centered environments) or domain (4GL) specificity.<ref name="AF_93" />

== Major CASE Risk Factors ==
Some of the most significant risk factors for organizations adopting CASE technology include: 
* Inadequate standardization. Organizations usually have to tailor and adopt methodologies and tools to their specific requirements. Doing so may require significant effort to integrate both divergent technologies as well as divergent methods. For example, before the adoption of the UML standard the diagram conventions and methods for designing object-oriented models were vastly different among followers of Jacobsen, Booch, Rumbaugh, 
* Unrealistic expectations. The proponents of CASE technology—especially vendors marketing expensive tool sets—often hype expectations that the new approach will be a silver bullet that solves all problems. In reality no such technology can do that and if organizations approach CASE with unrealistic expectations they will inevitably be disappointed. 
* Inadequate training. As with any new technology, CASE requires time to train people in how to use the tools and to get up to speed with them. CASE projects can fail if practitioners are not given adequate time for training or if the first project attempted with the new technology is itself highly mission critical and fraught with risk. 
* Inadequate process control. CASE provides significant new capabilities to utilize new types of tools in innovative ways. Without the proper process guidance and controls these new capabilities can cause significant new problems as well.<ref>[http://ithandbook.ffiec.gov/it-booklets/development-and-acquisition/development-procedures/software-development-techniques/computer-aided-software-engineering.aspx Computer Aided Software Engineering]. In: ''FFIEC IT Examination Handbook InfoBase''. Retrieved 3 Mar 2012.</ref>

== See also ==
* [[Data modeling]]
* [[Domain-specific modeling]]
* [[Method engineering]]
* [[Model-driven architecture]]
* [[Modeling language]]
* [[Rapid application development]]
* [[Model-based architecture]]

== References ==
{{reflist}}

{{Authority control}}

[[Category:Computer-aided software engineering tools|*]]
[[Category:Data management]]
<=====doc_Id=====>:288
<=====title=====>:
NCSA Brown Dog
<=====text=====>:

'''NCSA Brown Dog''' is a research project to develop a method for easily accessing historic research data stored in order to maintain the long-term viability of large bodies of scientific research. It is supported by the [[National Center for Supercomputing Applications]] (NCSA) that is funded by the [[National Science Foundation]] (NSF).<ref name=bdweb>{{cite web|title=Brown Dog|url=http://browndog.ncsa.illinois.edu|website=NCSA Brown Dog|accessdate=31 July 2014}}</ref> 

==History==
Brown Dog is part of the [[Datanet|DataNet]] partners program funded by NSF in 2008. DataNet was conceived to address the increasingly digital and data-intensive nature of science, engineering and education. Brown Dog is part of a follow-on effort called [[Data Infrastructure Building Blocks (DIBBs)]], focused on building software to support DataNet. The project was proposed by researchers at NCSA and the [[University of Illinois Urbana-Champaign]] as well as researchers from [[Boston University]] and the [[University of North Carolina at Chapel Hill]].

==Unstructured, uncurated, long tail data==
Much scientific data is smaller, [[Unstructured data|unstructured]] and uncurated and thus not easily shared. Such data is sometimes referred to as "long tail" data. This borrows a term from statistics and refers to the tail of the distribution of project sizes. The majority of smaller projects lack the resources to properly steward the data they produce. This so-called “long tail” data, both past and present, has the potential to inform future research in many study areas. Much of this data has become inaccessible due to obsolete software and file formats. The resulting impossibility of reviewing data from older research disrupts the overall scientific research project.<ref>{{cite web|title=DataUp—Data Curation for the Long Tail of Science|url=http://blogs.msdn.com/b/msr_er/archive/2012/10/02/dataup-data-curation-for-the-long-tail-of-science.aspx|website=Microsoft Research Connections Blog|publisher=Microsoft Research Connections Team|accessdate=7 August 2014}}</ref>

==Approach==
Brown Dog describes itself as the “super mutt” of software<ref>{{cite web|last1=Woodie|first1=Alex|title=NCSA Project Aims to Create a DNS-Like Service for Data|url=http://www.datanami.com/2014/01/06/ncsa_project_aims_to_create_a_dns-like_service_for_data/|website=datanami|accessdate=7 August 2014}}</ref> (thus the name “Brown Dog”), serving as a low-level data infrastructure to interface digital data content across the internet.  Its approach is to use every possible source of automated help (i.e., software) in existence in a robust and provenance-preserving manner to create a service that can deal with as much of this data as possible.<ref>{{cite web|last1=Pletz|first1=John|title=U of I researchers get millions for 'super mutt' to sniff out big-data trends|url=http://www.chicagobusiness.com/article/20131202/blogs11/131129794/u-of-i-researchers-get-millions-for-super-mutt-to-sniff-out-big-data-trends|website=Chicago Business|publisher=Crain Communications, Inc.|accessdate=7 August 2014}}</ref> The project sees the broader impact of its work in its potential to serve the general public as a sort of “DNS for data”, with the goal of making all data and all file formats as accessible as webpages are today.

==Technology==
Brown Dog seeks to address problems involving the use of uncurated and unstructured data collections through the development of two services: the Data Access Proxy (DAP) to aid in the conversion of file formats and the Data Tilling Services (DTS) for the automatic extraction of metadata from file contents. Once developed, researchers and general public users will be able to download browser plugins and other tools from the Brown Dog tool catalog.<ref name="bdweb" /><ref>{{cite web|last1=Jewett|first1=Barbara|title=DATA SET FREE|url=http://www.ncsa.illinois.edu/news/stories/KentonMcHenry/|website=NCSA Access Magazine|publisher=NCSA|accessdate=7 August 2014}}</ref>

===Data Tilling Service===
Data Tilling Service (DTS) will allow users to search data collections using an existing file to discover other similar files in a collection. A DTS search field will be appended to configured browsers where example files can be dropped. This tells DTS to search all the files under a given [[URL]] for files similar to the dropped file. For example, while browsing an online image collection, a user could drop an image of three people into the search field, and the DTS would return all images in the collection that also contain three people. If DTS encounters a foreign file format, it will utilize DAP to make the file accessible. DTS also indexes the data and extract and appends metadata to files and collections enabling users to gain some sense of the type of data they are encountering.

This service runs on port 9443.

===Data Access Proxy===
Data Access Proxy (DAP) allows users to access data files that would otherwise be unreadable. Similar to an internet gateway or [[Domain Name System|Domain Name Service]], the DAP configuration would be entered into a user’s machine and browser settings. Data requests over [[HTTP]] would first be examined by DAP to determine if the native file format is readable on the client device. If not, DAP converts the file into the best available format readable by the client machine.  Alternatively, the user could specify the desired format themselves.

This service runs on port 8184.

==Use cases==
Brown Dog targets three [[use cases]] proposed by groups within the [http://earthcube.org EarthCube] research communities. Developers and researchers from these communities will work together on use cases that span [[geoscience]], [[engineering]], [[biology]] and [[social science]].

===Long tail vegetation data in ecology and global change biology===
This use case is led by [http://people.bu.edu/dietze/ '''Michael Dietze'''], [http://www.bu.edu/ Boston University]

<blockquote>Data on the abundance, species composition, and size structure of vegetation is critically important for a wide array of sub-disciplines in ecology, conservation, natural resource management, and global change biology. However, addressing many of the pressing questions in these disciplines will require that terrestrial biosphere and hydrologic models are able to assimilate the large amount of long-tail data that exists but is largely inaccessible. The Brown Dog team in cooperation with researches from Dietze's lab will facilitate the capture of a huge body of smaller research-oriented vegetation data sets collected over many decades and historical vegetation data embedded in Public Land Survey data dating back to 1785. This data will be used as initial conditions for models, to make sense of other large data sets and for model calibration and validation.<ref name=bdweb /><ref name=newswise>{{cite web|title=BU Scientist, Collaborators Get $10.5 Million Grant to Develop Software for un-Curated Data|url=http://www.newswise.com/articles/bu-scientist-collaborators-get-10-5-million-grant-to-develop-software-for-un-curated-data|website=www.newswise.com|publisher=Boston University College of Arts and Sciences|accessdate=7 August 2014}}</ref></blockquote>

===Designing green infrastructure considering storm water and human requirements===
This use case is led by [http://eisa.ncsa.illinois.edu/ Barbara Minsker], [http://illinois.edu University of Illinois at Urbana-Champaign];  [http://willsull.net/research/ William Sullivan], University of Illinois at Urbana-Champaign; [http://cee.illinois.edu/faculty/arthurschmidt Arthur Schmidt], University of Illinois at Urbana-Champaign

<blockquote>This case study involves developing novel green infrastructure design criteria and models that integrate requirements for storm water management and ecosystem and human health and well being. To address the scientific and social problems associated with the design of green spaces, data accessibility and availability is a major challenge.  This study will focus on identified areas of the Green Healthy Neighborhood Planning region within the City of Chicago where existing local sewer performance is most deficient and where changes in impervious area through green infrastructure would be beneficial to under served neighborhoods. Brown Dog will be used to extract long-tail experimental data on human landscape preferences and health impacts. This data will be used to develop a human health impacts model that will then be linked together with a terrestrial biosphere model and a storm water model using Brown Dog technology.<ref name=bdweb /></blockquote>

===Development and application for critical zone studies===
This use case is led by [http://hydrocomplexity.net/index.html Praveen Kumar], University of Illinois at Urbana-Champaign

<blockquote>[[Earth's Critical Zone|Critical Zone]] (CZ) is the “skin” of the earth that extends from the treetops to the bedrock that is created by life processes working at scales from microbes to biomes. The Critical Zone supports all terrestrial living systems. Its upper part is the bio-mantle. This is where terrestrial biota live, reproduce, use and expend energy, and where their wastes and remains accumulate and decompose. It encompasses the soil, which acts as a geomembrane through which water and solutes, energy, gases, solids, and organisms interact with the atmosphere, biosphere, hydrosphere, and lithosphere. A variety of drivers affect this bio-dynamic zone, ranging from climate and deforestation to agriculture, grazing and human development. Understanding and predicting these effects is central to managing and sustaining vital ecosystem services such as soil fertility, water purification, and production of food resources, and, at larger scales, global carbon cycling and carbon sequestration.
The CZ provides a unifying framework for integrating terrestrial surface and near-surface environments, and reflects an intricate web of biological and chemical processes and human impacts occurring at vastly different temporal and spatial scales. The nature of these data create significant challenges for inter-disciplinary studies of the CZ because integration of the variety and number of data products and models has been a barrier. On the other hand, CZ data provides an excellent opportunity for defining, testing and implementing Brown Dog technologies. In this context “unstructured” data is viewed broadly as consisting of a collection of heterogeneous data with formats that reflect temporal and disciplinary legacies, data from emerging low cost open hardware based sensors and embedded sensor networks that lack well defined metadata and sensor characteristics, as well as data that are available as maps, images and text.<ref name=bdweb /></blockquote>

==NSF Award==
CIF21 DIBBs: Brown Dog was awarded in the winter of 2013 with a start date of October 1, 2013. Estimated expiration date is September 30, 2018.<ref>{{cite web|title=Award#1261582 - CIF21 DIBBs: Brown Dog|url=http://www.nsf.gov/awardsearch/showAward?AWD_ID=1261582&HistoricalAwards=false|website=nsf.gov|accessdate=31 July 2014}}</ref>

The award amount was $10,519,716.00, the largest DIBB award. The principal investigator is Kenton McHenry of NCSA at the University of Illinois at Urbana-Champaign. Coleaders are Jong Lee NCSA/UIUC; Barbara Minsker, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Praveen Kumar, Civil and Environmental Engineering, University of Illinois at Urbana-Champaign; Michael Dietze, Department of Earth and Environment, Boston University.

==References==
{{reflist|30em}}

==External links==
* {{official website|http://browndog.ncsa.illinois.edu}}

[[Category:Data management]]
[[Category:National Science Foundation]]
[[Category:Research projects]]
<=====doc_Id=====>:291
<=====title=====>:
Semantic heterogeneity
<=====text=====>:
{{About|semantic differences in data|other uses|Heterogeneity (disambiguation)}}

'''Semantic heterogeneity''' is when [[database schema]] or [[Data set|datasets]] for the same domain are developed by independent parties, resulting in differences in meaning and interpretation of data values.<ref>{{cite journal |title=Why your data won't mix |author=Alon Halevy|journal=Queue |volume=3 |issue=8 |year=2005 |url=http://queue.acm.org/detail.cfm?id=1103836}}</ref> Beyond [[Data structure|structured data]], the problem of semantic heterogeneity is compounded due to the flexibility of [[semi-structured data]] and various [[Tag (metadata)|tagging]] methods applied to documents or [[unstructured data]]. Semantic heterogeneity is one of the more important sources of differences in [[Heterogeneous database system|heterogeneous datasets]].

Yet, for multiple data sources to [[Interoperability|interoperate]] with one another, it is essential to reconcile these [[Semantics|semantic]] differences. Decomposing the various sources of semantic heterogeneities provides a basis for understanding how to map and transform data to overcome these differences.

== Classification ==

One of the first known classification schemes applied to [[Semantic data model|data semantics]] is from William Kent more than two decades ago.<ref>{{cite conference |title=The many forms of a single fact |author=William Kent |conference=Proceedings of the IEEE COMPCON |date=February 27 – March 3, 1989 |location=San Francisco |number=HPL-SAL-88-8, Hewlett-Packard Laboratories, Oct. 21, 1988 | at=13 pp. |url=http://www.bkent.net/Doc/manyform.htm}}</ref> Kent's approach dealt more with structural [[Data mapping|mapping]] issues than differences in meaning, which he pointed to [[Data dictionary|data dictionaries]] as potentially solving.

One of the most comprehensive classifications is from Pluempitiwiriyawej and Hammer, "Classification Scheme for Semantic and Schematic Heterogeneities in XML Data Sources".<ref>{{cite news |title=A classification scheme for semantic and schematic heterogeneities in XML data sources |author=Charnyote Pluempitiwiriyawej and Joachim Hammer |publisher=University of Florida |at=Technical Report TR00-004 |location=Gainesville, Florida |date=September 2000 |url=https://cise.ufl.edu/tr/DOC/REP-2000-396.pdf}}</ref> They classify heterogeneities into three broad classes:

* ''[[Data structure|Structural]]'' conflicts arise when the schema of the sources representing related or overlapping data exhibit discrepancies. Structural conflicts can be detected when comparing the underlying schema. The class of structural conflicts includes generalization conflicts, aggregation conflicts, internal path discrepancy, missing items, element ordering, constraint and type mismatch, and naming conflicts between the element types and attribute names.
* ''[[Data domain|Domain]]'' conflicts arise when the semantics of the data sources that will be integrated exhibit discrepancies. Domain conflicts can be detected by looking at the information contained in the schema and using knowledge about the underlying data domains. The class of domain conflicts includes schematic discrepancy, scale or unit, precision, and data representation conflicts.
* ''[[Data]]'' conflicts refer to discrepancies among similar or related data values across multiple sources. Data conflicts can only be detected by comparing the underlying sources. The class of data conflicts includes ID-value, missing data, incorrect spelling, and naming conflicts between the element contents and the attribute values.

Moreover, mismatches or conflicts can occur between set elements (a "population" mismatch) or attributes (a "description" mismatch).

Michael Bergman expanded upon this schema by adding a fourth major explicit category of language, and also added some examples of each kind of semantic heterogeneity, resulting in about 40 distinct potential categories <ref>{{cite web |title=Sources and classification of semantic heterogeneities |author=M.K. Bergman |website=AI3:::Adaptive Information |date=6 June 2006 |accessdate=28 September 2014 |url=http://www.mkbergman.com/232/sources-and-classification-of-semantic-heterogeneities/}}</ref>
.<ref>{{cite web |title=Big structure and data interoperability |author=M.K. Bergman |website=AI3:::Adaptive Information |date=12 August 2014 |accessdate=28 September 2014 |url=http://www.mkbergman.com/1782/big-structure-and-data-interoperability/}}</ref> This table shows the combined 40 possible sources of semantic heterogeneities across sources:

{|  style="text-align: left; width: 100%;" border="1" cellpadding="3" cellspacing="0"
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Class
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Category
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Subcategory
|  style="vertical-align: middle; text-align: center; font-weight: bold; background: #EFEFEF" | Examples
|-
| rowspan="8" colspan="1" |
'''[[Language]]'''
| rowspan="4" colspan="1" |
[[Character encoding|Encoding]]
| Ingest Encoding Mismatch
|
For example, [[US-ASCII|ASCII]] ''v'' [[UTF-8]]
|-
| Ingest Encoding Lacking
| Mis-recognition of tokens because not being parsed with the proper encoding
|-
| Query Encoding Mismatch
| For example, ASCII ''v'' UTF-8 in search
|-
| Query Encoding Lacking
| Mis-recognition of search tokens because not being parsed with the proper encoding
|-
| rowspan="4" colspan="1" | Languages
| Script Mismatch
| Variations in how parsers handle, say, stemming, white spaces or hyphens
|-
| Parsing / Morphological Analysis Errors (many)
| Arabic languages (right-to-left) ''v'' Romance languages (left-to-right)
|-
| Syntactical Errors (many)
|
Ambiguous sentence references, such as ''I'm glad I'm a man, and so is Lola'' ([[Lola (song)|Lola]] by [[Ray Davies]] and the [[Kinks]])
|-
| Semantics Errors (many)
| River ''bank'' ''v'' money ''bank'' ''v'' billiards ''bank'' shot
|-
| rowspan="17" colspan="1" | '''Conceptual'''
| rowspan="5" colspan="1" | Naming
| Case Sensitivity
| Uppercase ''v'' lower case ''v'' Camel case
|-
|
[[Synonym]]s
| United States ''v'' USA ''v'' America ''v'' Uncle Sam ''v'' Great Satan
|-
|
[[Acronym]]s
| United States ''v'' USA ''v'' US
|-
|
[[Homonym]]s
| Such as when the same name refers to more than one concept, such as Name referring to a person ''v'' Name referring to a book
|-
| Misspellings
| As stated
|-
| rowspan="1" colspan="2" | Generalization / Specialization
| When single items in one schema are related to multiple items in another schema, or vice versa. For example, one schema may refer to "phone" but the other schema has multiple elements such as "home phone", "work phone" and "cell phone"
|-
| rowspan="2" colspan="1" | Aggregation
| Intra-aggregation
| When the same population is divided differently (such as, Census ''v'' Federal regions for states, England ''v'' Great Britain ''v'' United Kingdom, or full person names ''v'' first-middle-last)
|-
| Inter-aggregation
| May occur when sums or counts are included as set members
|-
| rowspan="1" colspan="2" | Internal Path Discrepancy
| Can arise from different source-target retrieval paths in two different schemas (for example, hierarchical structures where the elements are different levels of remove)
|-
| rowspan="4" colspan="1" | Missing Item
| Content Discrepancy
| Differences in set enumerations or including items or not (say, US territories) in a listing of US states
|-
| Missing Content
| Differences in scope coverage between two or more datasets for the same concept
|-
| Attribute List Discrepancy
| Differences in attribute completeness between two or more datasets
|-
| Missing Attribute
| Differences in scope coverage between two or more datasets for the same attribute
|-
| rowspan="2" colspan="2" | Item Equivalence
|
When two types (classes or sets) are asserted as being the same when the scope and reference are not (for example, [[Berlin]] the city ''v''  [[States of Germany#Subdivisions|Berlin]] the official city-state)
|-
|
When two individuals are asserted as being the same when they are actually distinct (for example, [[John F. Kennedy]] the president ''v''  [[USS John F. Kennedy (CV-67)|''John F. Kennedy'']] the aircraft carrier)
|-
| rowspan="1" colspan="2" | Type Mismatch
| When the same item is characterized by different types, such as a person being typed as an animal ''v'' human being ''v'' person
|-
| rowspan="1" colspan="2" | Constraint Mismatch
| When attributes referring to the same thing have different cardinalities or disjointedness assertions
|-
| rowspan="9" colspan="1" |
'''[[Domain of discourse|Domain]]'''
| rowspan="4" colspan="1" | Schematic Discrepancy
| Element-value to Element-label Mapping
| rowspan="4" colspan="1" | One of four errors that may occur when attribute names (say, Hair ''v'' Fur) may refer to the same attribute, or when same attribute names (say, Hair ''v'' Hair) may refer to different attribute scopes (say, Hair ''v'' Fur) or where values for these attributes may be the same but refer to different actual attributes or where values may differ but be for the same attribute and putative value. <br /><br /> Many of the other semantic heterogeneities herein also contribute to schema discrepancies
|-
| Attribute-value to Element-label Mapping
|-
| Element-value to Attribute-label Mapping
|-
| Attribute-value to Attribute-label Mapping
|-
| rowspan="2" colspan="1" | Scale or Units
| Measurement Type
| Differences, say, in the metric ''v'' English measurement systems, or currencies
|-
| Units
| Differences, say, in meters ''v'' centimeters ''v'' millimeters
|-
| rowspan="1" colspan="2" | Precision
| For example, a value of 4.1 inches in one dataset ''v'' 4.106 in another dataset
|-
| rowspan="2" colspan="1" |
[[Data representation]]
| Primitive Data Type
|
Confusion often arises in the use of literals ''v'' [[Uniform resource identifier|URIs]] ''v'' object types
|-
| Data Format
| Delimiting decimals by period ''v'' commas; various date formats; using exponents or aggregate units (such as thousands or millions)
|-
| rowspan="8" colspan="1" |
'''[[Data]]'''
| rowspan="5" colspan="1" | Naming
| Case Sensitivity
| Uppercase ''v'' lower case ''v'' Camel case
|-
| Synonyms
| For example, centimeters ''v'' cm
|-
| Acronyms
| For example, currency symbols ''v'' currency names
|-
| Homonyms
| Such as when the same name refers to more than one attribute, such as Name referring to a person ''v'' Name referring to a book
|-
| Misspellings
| As stated
|-
| rowspan="1" colspan="2" | ID Mismatch or Missing ID
| URIs can be a particular problem here, due to actual mismatches but also use of name spaces or not and truncated URIs
|-
| rowspan="1" colspan="2" | Missing Data
|
A common problem, more acute with closed world approaches than with [[Open world assumption|open world ones]]
|-
| rowspan="1" colspan="2" | Element Ordering
| Set members can be ordered or unordered, and if ordered, the sequences of individual members or values can differ
|}

A different approach toward classifying semantics and integration approaches is taken by [[Amit Sheth|Sheth]] et al.<ref>{{cite journal |title=Semantics for the semantic Web: the implicit, the formal and the powerful | author1=Amit P. Sheth|author2=Cartic Ramakrishnan|author3=Christopher Thomas|journal=Int’l Journal on Semantic Web & Information Systems |volume=1 |issue=1 |pages=1–18 |date=2005 |url=http://www.informatik.uni-trier.de/~ley/db/journals/ijswis/ijswis1.html}}</ref> Under their concept, they split semantics into three forms: implicit, formal and powerful. Implicit semantics are what is either largely present or can easily be extracted; formal languages, though relatively scarce, occur in the form of [[Ontology (information science)|ontologies]] or other [[description logic]]s; and powerful (soft) semantics are fuzzy and not limited to rigid set-based assignments. Sheth et al.'s main point is that [[first-order logic]] (FOL) or description logic is inadequate alone to properly capture the needed semantics.

== Relevant applications ==

Besides data interoperabiity, relevant areas in [[information technology]] that depend on reconciling semantic heterogeneities include [[data mapping]], [[semantic integration]], and [[enterprise information integration]], among many others. From the conceptual to actual data, there are differences in perspective, vocabularies, measures and conventions once any two data sources are brought together. Explicit attention to these semantic heterogeneities is one means to get the information to integrate or interoperate.

A mere twenty years ago, information technology systems expressed and stored data in a multitude of formats and systems. The Internet and Web protocols have done much to overcome these sources of differences. While there is a large number of categories of semantic heterogeneity, these categories are also patterned and can be anticipated and corrected. These patterned sources inform what kind of work must be done to overcome semantic differences where they still reside.

==See also==
* [[Data integration]]
* [[Data mapping]]
* [[Enterprise information integration]]
* [[Heterogeneous database system]]
* [[Interoperability]]
* [[Ontology-based data integration]]
* [[Schema matching]]
* [[Semantic integration]]
* [[Semantic matching]]
* [[Semantics]]

==References==
<references/>

==Further reading==
* [http://wiki.opensemanticframework.org/index.php/Classification_of_Semantic_Heterogeneity Classification of semantic heterogeneity]

[[Category:Data management]]
[[Category:Interoperability]]
[[Category:Knowledge management]]
[[Category:Semantics]]
<=====doc_Id=====>:294
<=====title=====>:
Category:Information privacy
<=====text=====>:
{{Cat main}}

[[Category:Data management|Privacy]]
[[Category:Computer law|Privacy]]
[[Category:Privacy]]
[[Category:Computer security|Privacy]]
[[Category:Information|Privacy]]
<=====doc_Id=====>:297
<=====title=====>:
Data management
<=====text=====>:
{{lead too short|date=October 2014}}
'''Data management''' comprises all the [[List of academic disciplines|disciplines]] related to managing [[data]] as a valuable resource.

== Overview ==
The official definition provided by [[DAMA]] International, the professional organization for those in the data management profession, is: "Data Resource Management is the development and execution of architectures, policies, practices and procedures that properly manage the full data lifecycle needs of an enterprise." This definition is fairly broad and encompasses a number of professions which may not have direct technical contact with lower-level aspects of data management, such as [[relational database]] management.

[[File:The Data Lifecycle.jpg|thumb|The data lifecycle]]

Alternatively, the definition provided in the DAMA Data Management Body of Knowledge (<ref>[https://technicspub.com/dmbok/ DAMA-DMBOK]</ref>) is:
"Data management is the development, execution and supervision of plans, policies, programs and practices that control, protect, deliver and enhance the value of data and information assets."<ref>"DAMA-DMBOK Guide (Data Management Body of Knowledge) Introduction & Project Status" (Note: PDF no longer available online at https://www.dama.org, current version available for purchase)</ref>

The concept of "Data Management" arose in the 1980s as technology moved from sequential processing (first cards, then tape) to [[random access]] processing.  Since it was now technically possible to store a single fact in a single place and access that using random access disk, those suggesting that "Data Management" was more important than "[[Process Management]]" used arguments such as "a customer's home address is stored in 75 (or some other large number) places in our computer systems."  During this period, random access processing was not competitively fast, so those suggesting "Process Management" was more important than "Data Management" used batch processing time as their primary argument.  As applications moved into real-time, [[interactive]] applications, it became obvious to most practitioners that both management processes were important.  If the data was not well defined, the data would be mis-used in applications.  If the process wasn't well defined, it was impossible to meet user needs.

==Corporate Data Quality Management==
[[Corporate Data Quality Management]] (CDQM) is, according to the [[EFQM|European Foundation for Quality Management]] and the Competence Center Corporate Data Quality (CC CDQ, University of St. Gallen), the whole set of activities intended to improve corporate data quality (both reactive and preventive). Main premise of CDQM is the business relevance of high-quality corporate data. CDQM comprises with following activity areas:.<ref>[https://benchmarking.iwi.unisg.ch/Framework_for_CDQM.pdf EFQM ; IWI-HSG: EFQM Framework for Corporate Data Quality Management. Brussels : EFQM Press, 2011]</ref>
* '''Strategy for Corporate Data Quality''': As CDQM is affected by various business drivers and requires involvement of multiple divisions in an  organization; it must be considered a company-wide endeavor.
* '''Corporate Data Quality Controlling''': Effective CDQM requires compliance with standards, policies, and procedures. Compliance is monitored according to previously defined metrics and performance indicators and reported to stakeholders.
* '''Corporate Data Quality Organization''': CDQM requires clear roles and responsibilities for the use of corporate data. The CDQM organization defines tasks and privileges for decision making for CDQM.
* '''Corporate Data Quality Processes and Methods''': In order to handle corporate data properly and in a standardized way across the entire organization and to ensure corporate data quality, standard procedures and guidelines must be embedded in company’s daily processes.
* '''Data Architecture for Corporate Data Quality''': The data architecture consists of the data object model - which comprises the unambiguous definition and the conceptual model of corporate data - and the data storage and distribution architecture.
* '''Applications for Corporate Data Quality''': Software applications support the activities of Corporate Data Quality Management. Their use must be planned, monitored, managed and continuously improved.

== Topics in Data Management ==
Topics in Data Management, grouped by the DAMA DMBOK Framework,<ref>[http://dama-dach.org/dama-dmbok-functional-framework DAMA-DMBOK Functional Framework v3]</ref> include:
{{colbegin|2}}
# [[Data governance]]
#* [[Data asset]]
#* [[Data governance]]
#* [[Data steward]]
# Data Architecture, Analysis and Design
#* [[Data analysis]]
#* [[Data architecture]]
#* [[Data modeling]]
# Database Management
#* [[Data maintenance]]
#* [[Database administration]]
#* [[Database management system]]
# Data Security Management
#* [[Data access]]
#* [[Data erasure]]
#* [[Data privacy]]
#* [[Data security]]
# Data Quality Management
#* [[Data cleansing]]
#* [[Data integrity]]
#* [[Data enrichment]]
#* [[Data quality]]
#* [[Data quality assurance]]
# Reference and Master Data Management
#* [[Data integration]]
#* [[Master data management]]
#* [[Reference data]]
# Data Warehousing and Business Intelligence Management
#* [[Business intelligence]]
#* [[Data mart]]
#* [[Data mining]]
#* Data movement ([[Extract, transform, load ]])
#* [[Data warehouse]]
# Document, Record and Content Management
#* [[Document management system]]
#* [[Records management]]
# Meta Data Management
#* [[Meta-data management]]
#* [[Metadata]]
#* [[Metadata discovery]]
#* [[Metadata publishing]]
#* [[Metadata registry]]
# Contact Data Management
#* [[Business continuity planning]]
#* [[Marketing operations]]
#* [[Customer data integration]]
#* [[Identity management]]
#* [[Identity theft]]
#* [[Data theft]]
#* [[ERP software]]
#* [[CRM software]]
#* [[Address (geography)]]
#* [[Postal code]]
#* [[Email address]]
#* [[Telephone number]]
{{colend}}

==Body of Knowledge==
The DAMA Guide to the Data Management Body of Knowledge" (DAMA-DMBOK Guide), under the guidance of a new DAMA-DMBOK Editorial Board. This publication is available from April 5, 2009.

==Usage==

In modern [[management fad|management usage]], one can easily discern a trend away from the term "data" in composite expressions to the term "[[information]]" or even "[[knowledge]]" when talking in a non-technical context. Thus there exists not only data management, but also [[information management]] and [[knowledge management]]. This is a misleading trend as it obscures that traditional data are managed or somehow [[data processing|processed]] on second looks.{{cn|date=June 2016}} The distinction between data and derived values can be seen in the [[information ladder]].{{cn|date=June 2016}} While data can exist as such, "information" and "knowledge" are always in the "eye" (or rather the brain) of the beholder and can only be measured in relative units.

Several organisations have established a data management centre (DMC)<ref>
For example: {{cite book
| last1                 = Kumar
| first1                = Sangeeth
| last2                 = Ramesh
| first2                = Maneesha Vinodini
| chapter               = Lightweight Management framework (LMF) for a Heterogeneous Wireless Network for Landslide Detection
| editor1-last          = Meghanathan
| editor1-first         = Natarajan
| editor2-last          = Boumerdassi
| editor2-first         = Selma
| editor3-last          = Chaki
| editor3-first         = Nabendu
| editor4-last          = Nagamalai
| editor4-first         = Dhinaharan
| title                 = Recent Trends in Networks and Communications: International Conferences, NeCoM 2010, WiMoN 2010, WeST 2010,Chennai, India, July 23-25, 2010. Proceedings
| url                   = https://books.google.com/books?id=8i5qCQAAQBAJ
| series                = Communications in Computer and Information Science
| volume                = 90
| publisher             = Springer
| publication-date      = 2010
| page                  = 466
| isbn                  = 9783642144936
| accessdate            = 2016-06-16
| quote                 = 4.4 Data Management Center (DMC)[:] The Data Management Center is the data center for all of the deployed cluster networks. Through the DMC, the LMF allows the user to list the services in any cluster member belonging to any cluster [...].
}}
</ref>
for their operations.

==Integrated data management==

'''Integrated data management''' (IDM) is a tools approach to facilitate data management and improve performance. IDM consists of an integrated, modular environment to manage enterprise application data, and optimize data-driven applications over its [[Information Lifecycle Management|lifetime]].<ref>[http://www.ibm.com/developerworks/data/library/techarticle/dm-0807hayes/?S_TACT=105AGX11&S_CMP=FP#ibm-content  Integrated Data Management: Managing data across its lifecycle] by Holly Hayes</ref><ref>[http://www.ibmsystemsmagmainframedigital.com/nxtbooks/ibmsystemsmag/mainframe_20090708/index.php#/34 Organizations thrive on Data] by Eric Naiburg</ref><ref>[http://download.boulder.ibm.com/ibmdl/pub/software/data/sw-library/data-management/optim/reports/fragmented.pdf Fragmented Management Across The Data Life Cycle Increases Cost And Risk] - A commissioned study conducted by Forrester Consulting on behalf of IBM October 2008</ref><ref>[http://publib.boulder.ibm.com/infocenter/idm/v2r1/index.jsp integrated IBM Data Management information center]</ref> IDM's purpose is to:
*Produce enterprise-ready applications faster
*Improve data access, speed iterative testing
*Empower collaboration between architects, developers and DBAs
*Consistently achieve service level targets
*Automate and simplify operations
*Provide contextual intelligence across the [[solution stack]]
*Support business growth
*Accommodate new initiatives without expanding infrastructure
*Simplify application upgrades, consolidation and retirement
*Facilitate alignment, consistency and governance
*Define business policies and standards up front;  share, extend, and apply throughout the lifecycle

==See also==
{{colbegin|2}}
* [[Open data]]
* [[Information architecture]]
* [[Information management]]
* [[Enterprise architecture]]
* [[Information design]]
* [[Information system]]
* [[Controlled vocabulary]]
* [[Data curation]]
* [[Data retention]]
* [[Data governance]]
* [[Data quality]]
* [[Data modeling]]
* [[Data management plan]]
* [[Information lifecycle management]]
* [[Computer data storage]]
* [[Data proliferation]]
* [[Digital preservation]]
* [[Digital perpetuation]]
* [[Document management]]
* [[Enterprise content management]]
* [[Hierarchical storage management]]
* [[Information repository]]
* [[Records management]]
* [[System integration]]
{{colend}}

== References ==
{{Reflist}}

==External links==
* {{dmoz|Computers/Software/Master_Data_Management/Articles/}}

{{DEFAULTSORT:Data Management}}
[[Category:Data management| ]]
[[Category:Information technology management]]
<=====doc_Id=====>:300
<=====title=====>:
Semantic query
<=====text=====>:
'''Semantic queries''' allow for queries and analytics of associative and contextual nature. Semantic queries enable the retrieval of both explicitly and implicitly derived information based on syntactic, semantic and structural information contained in data. They are designed to deliver precise results (possibly the distinctive selection of one single piece of information) or to answer more fuzzy and wide open questions through [[pattern matching]] and [[Reasoning system|digital reasoning]].

Semantic queries work on [[named graphs]], [[Linked Data|linked-data]] or [[Semantic triple|triples]]. This enables the query to process the actual relationships between information and ''infer'' the answers from the ''network of data''. This is in contrast to [[semantic search]], which uses [[semantics]] (the science of meaning) in [[Unstructured data|unstructured text]] to produce a better search result (see [[natural language processing]]).

From a technical point of view semantic queries are precise relational-type operations much like a [[SQL|database query]]. They work on structured data and therefore have the possibility to utilize comprehensive features like operators (e.g. >, < and =), namespaces, [[pattern matching]], [[Type inheritance|subclassing]], [[transitive relation]]s, [[Semantic Web Rule Language|semantic rules]] and contextual [[Full-text index|full text search]]. The [[semantic web]] technology stack of the [[W3C]] is offering [[SPARQL]]<ref name="XML.com">{{cite web|url=http://www.xml.com/pub/a/2005/11/16/introducing-sparql-querying-semantic-web-tutorial.html |title=Introducing SPARQL: Querying the Semantic Web |publisher=XML.com|date=2005}}</ref><ref name="W3C">{{cite web|url=http://www.w3.org/TR/rdf-sparql-query |title=SPARQL Query Language for RDF |publisher=W3C|date=2008}}</ref> to formulate semantic queries in a syntax similar to [[SQL]]. Semantic queries are used in [[triplestore]]s, [[graph databases]], [[semantic wiki]]s, natural language and artificial intelligence systems.

== Background ==

[[Relational database]]s contain all relationships between data in an ''implicit'' manner only.<ref name="ACM-DL">{{cite web|url=http://portal.acm.org/citation.cfm?id=1646157 |title=Semantic queries in databases: problems and challenges |publisher=ACM Digital Library|date=2009}}</ref><ref name="ESWC">{{cite web|url=http://2012.eswc-conferences.org/sites/default/files/eswc2012_submission_357.pdf |title=Karma: A System for Mapping Structured Sources into the Semantic Web |publisher=eswc-conferences.org|date=2012}}</ref> For example, the relationships between customers and products (stored in two content-tables and connected with an additional link-table) only come into existence in a query statement ([[SQL]] in the case of relational databases) written by a developer. Writing the query demands exact knowledge of the [[database schema]].<ref name="IEEE">{{cite web|url=http://www-scf.usc.edu/~taheriya/papers/taheriyan14-icsc-paper.pdf |title=A Scalable Approach to Learn Semantic Models of Structured Sources |publisher=8th IEEE International Conference on Semantic Computing|date=2014}}</ref><ref name="AAAI">{{cite web|url=http://www.isi.edu/integration/papers/knoblock13-sbd.pdf |title=Semantics for Big Data Integration and Analysis |publisher=AAAI Fall Symposium on Semantics for Big Data|date=2013}}</ref>

[[Linked Data|Linked-Data]] contain all relationships between data in an ''explicit'' manner. In the above example no query code needs to be written. The correct product for each customer can be fetched automatically. Whereas this simple example is trivial, the real power of linked-data comes into play when a ''network of information'' is created (customers with their geo-spatial information like city, state and country; products with their categories within sub- and super-categories). Now the system can automatically answer more complex queries and analytics that look for the connection of a particular location with a product category. The development effort for this query is omitted. Executing a semantic query is conducted by ''walking'' the network of information and finding matches (also called ''Data Graph Traversal'').

Another important aspect of semantic queries is that the type of the relationship can be used to incorporate intelligence into the system. The relationship between a customer and a product has a fundamentally different nature than the relationship between a neighbourhood and its city. The latter enables the semantic query engine to ''infer'' that a customer ''living in Manhattan is also living in New York City'' whereas other relationships might have more complicated patterns and "contextual analytics". This process is called inference or reasoning and is the ability of the software to derive new information based on given facts.

== Articles ==

* {{Cite web
| last =  Velez
| first = Golda
| year = 2008
| url = http://www.wallstreetandtech.com/data-management/showArticle.jhtml?articleID=208700210&pgno=2
| title = Semantics Help Wall Street Cope With Data Overload
| publisher = wallstreetandtech.com
}}
* {{Cite web
| last =  Zhifeng
| first = Xiao
| year = 2009
| url = http://adsabs.harvard.edu/abs/2009SPIE.7492E..60X
| title = Spatial information semantic query based on SPARQL
| publisher = International Symposium on Spatial Analysis
}}
* {{Cite web
| last = Aquin
| first = Mathieu
| year = 2010
| url = http://www.semantic-web-journal.net/sites/default/files/swj96_1.pdf
| title = Watson, more than a Semantic Web search engine
| publisher = Semantic Web Journal
}}
* {{Cite web
| last =  Prudhommeaux
| first = Eric
| year = 2010
| url = http://www.cambridgesemantics.com/semantic-university/sparql-vs-sql-intro
| title = SPARQL vs. SQL - Introduction
| publisher = Cambridge Semantics
}}
* {{Cite web
| last = Dworetzky
| first = Tom
| year = 2011
| url = http://www.ibtimes.com/how-siri-works-iphones-brain-comes-natural-language-processing-stanford-professors-teach-free-online
| title = How Siri Works: iPhone's 'Brain' Comes from Natural Language Processing
| publisher = International Business Times
}}
* {{Cite web
| last =  Horwitt
| first = Elisabeth
| year = 2011
| url = http://www.computerworld.com/s/article/9209118/The_semantic_Web_gets_down_to_businessarticleID=208700210&pgno=2
| title = The semantic Web gets down to business
| publisher = computerworld.com
}}
* {{Cite web
| last = Rodriguez
| first = Marko
| year = 2011
| url = http://markorodriguez.com/2011/06/15/graph-pattern-matching-with-gremlin-1-1/
| title = Graph Pattern Matching with Gremlin
| publisher = markorodriguez.com on Graph Computing
}}
* {{Cite web
| last = Sequeda
| first = Juan
| year = 2011
| url = http://www.cambridgesemantics.com/semantic-university/sparql-nuts-and-bolts
| title = SPARQL Nuts & Bolts
| publisher = Cambridge Semantics
}}
* {{Cite web
| last = Freitas
| first = Andre
| year = 2012
| url = https://www.deri.ie/sites/default/files/publications/freitas_ic_12.pdf
| title = Querying Heterogeneous Datasets on the Linked Data Web
| publisher = IEEE Internet Computing
}}
* {{Cite web
| last = Kauppinen
| first = Tomi
| year = 2012
| url = http://linkedscience.org/tools/sparql-package-for-r/tutorial-on-sparql-package-for-r/
| title = Using the SPARQL Package in R to handle Spatial Linked Data
| publisher = linkedscience.org
}}
* {{Cite web
| last = Lorentz
| first = Alissa
| year = 2013
| url = http://www.wired.com/2013/04/with-big-data-context-is-a-big-issue/
| title = With Big Data, Context is a Big Issue
| publisher = Wired
}}

== See also ==

* [[Dataspaces]]
* [[Knowledge Representation]]
* [[Linked Data]]
* [[Ontology alignment]]
* [[Semantic Integration]]
* [[Semantic publishing]]
* [[Semantics of Business Vocabulary and Business Rules]]
* [[SPARQL]]

== References ==
{{reflist}}

==External links==
* [http://www.w3.org/standards/semanticweb/query W3C Semantic Web Standards - Query]

[[Category:Data management]]
[[Category:Query languages]]
[[Category:Semantic Web]]
<=====doc_Id=====>:303
<=====title=====>:
Multi-model database
<=====text=====>:
Most database management systems are organized around a single [[Database model|data model]] that determines how data can be organized, stored, and manipulated. In contrast, a '''multi-model database''' is designed to support multiple data models against a single, integrated backend.<ref name="neither">[http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]</ref> Document, graph, relational, and key-value models are examples of data models that may be supported by a multi-model database.

== Background ==

The [[relational model|relational]] data model became popular after its publication by [[Edgar F. Codd]] in 1970. Due to increasing requirements for [[Scalability#Horizontal and vertical scaling|horizontal scalability]] and [[fault tolerance]], [[NoSQL]] databases became prominent after 2009. NoSQL databases use a variety of data models, with [[Document-oriented database|document]], [[Graph database|graph]], and key-value models being popular.<ref name="rise">[http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]</ref>

A Multi-model database is a database that can store, index and query data in more than one model. For some time, databases have primarily supported only one model, such as: [[Relational database]], [[Document-oriented database]], [[Graph database]] or [[Triplestore]]. A database that combines many of these is multi-model.

For some time, it was all but forgotten (or considered irrelevant) that there were any other database models besides Relational. The Relational model and notion of [[Third normal form]] were the de facto standard for all data storage. However, prior to the dominance of Relational data modeling from about 1980 to 2005 the [[Hierarchical database model]] was commonly used, and since 2000 or 2010, many [[NoSQL]] models that are non-relational including Documents, triples, key-value stores and graphs are popular. Arguably, geospatial data, temporal data and text data are also separate models, though indexed, queryable text data is generally termed a "[[search engine]]" rather than a database.

The first time the word "Multi-Model" has been associated to the databases was on May 30, 2012 in Cologne, Germany, during the Luca Garulli's key note "''NoSQL Adoption – What’s the Next Step?''".<ref>{{Cite journal|date=2012-06-01|title=Multi-Model storage 1/2 one product,|url=http://www.slideshare.net/lvca/no-sql-matters2012keynote/47-MultiModel_storage_12_one_product}}</ref><ref>{{Cite web|url=https://2012.nosql-matters.org/cgn/index.html?p=1202.html#luca_garulli_keynote|title=Nosql Matters Conference 2012 {{!}} NoSQL Matters CGN 2012|website=2012.nosql-matters.org|access-date=2017-01-12}}</ref> Luca Garulli envisioned the evolution of the 1st generation NoSQL products into new products with more features able to be used by multiple use cases.

A Multi-model database is most directly a response to the "[[Polyglot Persistence]]" approach of knitting together multiple database products, each handing a different model, to achieve a multi-model capability as described by Martin Fowler.<ref name="polyglot">[http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]</ref> This strategy has two major disadvantages: it leads to a significant increase in operational complexity, and there is no support for maintaining data consistency across the separate data stores, so Multi-model databases have begun to fill in this gap.

Some stories of overcomplicated systems from un-necessary "frankenbeast" database integrations are found on the web.<ref name="frankenbeast">[http://www.marklogic.com/blog/polyglot-persistence-done-right/ MarkLogic, "Avoiding the Frankenbeast"]</ref><ref name="boring">[http://mcfunley.com/choose-boring-technology McKinley, "Choose Boring Technology"]</ref>

Multi-model databases are intended to offer the data modeling advantages of [[Polyglot Persistence]],<ref name="polyglot"/> without its disadvantages. Operational complexity, in particular, is reduced through the use of a single data store.<ref name="rise"/>

== Databases ==
Multi-model databases include (in alphabetic order):
* [[ArangoDB]] - Document (JSON), Graph, Key-value
* [[CouchBase]] - Relational (SQL), Document
* [[CrateDB]] - Relational (SQL), Document (Lucene)
* [[MarkLogic]] - Document (XML and JSON), Graph (RDF with OWL/RDFS), text, geospatial, binary, SQL
* [[OrientDB]] - Document (JSON), Graph, Key-value, Text, Geospatial, Binary, SQL, Reactive

Note that the level of support for the various models varies widely, including the ability to query across models, fully index the internal structure of a model, transactional support, and optimization or query planning across models.
The first multi-model database was [[OrientDB]], created in 2010 as an answer to the fragmented NoSQL environment, with the goal of providing one product to replace multiple NoSQL databases.

== Architecture ==

The main difference between the available multi-model databases is related to their architectures. Multi-model databases can support different models either within the engine or via different layers on top of the engine. Some products may provide an engine which supports documents and graphs while others provide layers on top of a key-key store.<ref>[http://blog.foundationdb.com/7-things-that-make-google-f1-and-the-foundationdb-sql-layer-so-strikingly-similar, "layer"]</ref> With a layered architecture, each data model is provided via its own [[Component-based software engineering|component]].

== User-defined data models ==

In addition to offering multiple data models in a single data store, some databases allow developers to easily define custom data models. This capability is enabled by ACID transactions with high performance and scalability. In order for a custom data model to support concurrent updates, the database must be able to synchronize updates across multiple keys. ACID transactions, if they are sufficiently performant,  allow such synchronization.<ref name="multiple">[http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]</ref> JSON documents, graphs, and relational tables can all be implemented in a manner that inherits the horizontal scalability and fault-tolerance of the underlying data store.

== See also ==
<!-- please do not list specific implementations here -->
* [[Comparison of multi-model databases]]
* [[ACID]]
* [[NoSQL]]
* [[Comparison of structured storage software]]
* [[Database transaction]]
* [[Distributed database]]
* [[Distributed transaction]]
* [[Document-oriented database]]
* [[Graph database]]
* [[Relational model]]

== References ==
{{Reflist|2}}

== External links ==
* [http://www.orientechnologies.com/docs/last/orientdb.wiki/Tutorial-Document-and-graph-model.html OrientDB Document and Graph Model]
* [https://www.arangodb.com/key-features ArangoDB Key Features]
* [https://foundationdb.com/try/multi-model FoundationDB Multi-Model Architecture]
* [http://martinfowler.com/bliki/PolyglotPersistence.html Polyglot Persistence]
* [http://blogs.the451group.com/information_management/2013/02/08/neither-fish-nor-fowl/ The 451 Group, "Neither Fish Nor Fowl: The Rise of Multi-Model Databases"]
* [http://www.odbms.org/blog/2013/10/on-multi-model-databases-interview-with-martin-schonert-and-frank-celler/ ODBMS, "On Multi-Model Databases. Interview with Martin Schönert and Frank Celler."]
* [http://www.odbms.org/wp-content/uploads/2014/04/Multiple-Data-Models.pdf ODBMS, "Polyglot Persistence or Multiple Data Models?"]
* [http://www.infoworld.com/article/2861579/database/the-rise-of-the-multimodel-database.html Infoworld, "The Rise of the Multi-Model Database"]
* [https://crate.io/docs/reference/storage_consistency.html, Crate.IO Storage and Consistency]
* [http://www.marklogic.com/blog/tag/multi-model-database/, MarkLogic on Multi-model databases]

{{DEFAULTSORT:Multi-model Database}}
[[Category:Applications of distributed computing]]
[[Category:Databases]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]
[[Category:Distributed data stores]]
[[Category:NoSQL]]
[[Category:Structured storage]]
[[Category:Transaction processing]]
<=====doc_Id=====>:306
<=====title=====>:
Media aggregation platform
<=====text=====>:
A '''Media Aggregation Platform''' or '''Media Aggregation Portal''' (MAP) is an over-the-top service for distributing web-based streaming media content from multiple sources to a large audience. MAPs consist of networks of sources who host their own content which viewers can choose and access directly from a larger variety of content to choose from than a single source can offer.<ref>{{cite web | url=https://medium.com/@bmobley/over-the-top-of-ott-need-a-map-9931096775c2 | title=Over the Top of OTT… Need a MAP? | publisher=[[Medium.com]] | accessdate=23 March 2015}}</ref><ref>{{cite book | title=Building Next-Generation Converged Networks: Theory and Practice | publisher=CRC Press |author1=Al-Sakib Khan Pathan |author2=Muhammad Mostafa Monowar |author3=Zubair Md. Fadlullah | year=2013 | isbn=1466507616}}</ref> The service is used by content providers, looking to extend the reach of their content.

Unlike multichannel video programming distributor ([[MVPD]]) or multiple-system operator (MSO), MAPs rely on the Internet rather than cables or satellite. As more network television channels have moved online in the early 21st century,<ref>{{cite web | url=http://www.exchange4media.com/59377_ott-platforms-to-be-key-growth-area-for-tv-broadcasters.html | title=OTT platforms to be key growth area for TV broadcasters | accessdate=23 March 2015}}</ref> joining web-native channels like [[Netflix]], MAPs aggregates content the way that MSOs and MVPDs have used cable, and to a lesser extent satellite and IPTV infrastructure. There are companies that offer a similar service for free, including [[Yidio]] and TV.com, while others charge a subscription fee like as [[FreeCast Inc]]'s Rabbit TV Plus.<ref>{{cite web | url=http://rabbittvplus.com/ | title=FreeCast Inc Rabbit TV Plus | accessdate=23 March 2015}}</ref> When compared with MSOs and MVPDs, MPAs network have much lower cost due to lack of physical infrastructure. The majority of revenues from their services is retained by the content creators and revenues are from advertisements, [[pay-per-view]], and subscription-based content offerings instead of by licensing and reselling content. MAPs service consumers directly with the content source and they purchase content directly from its source, without the markup added by a middleman.<ref>{{cite web | url=https://www.ncta.com/industry-data | title=NCTA Industry Data | accessdate=23 March 2015}}</ref>

==See also==
* [[Multichannel video programming distributor|Multichannel Video Programming Distributor (MVPD)]]
* [[Multiple system operator|Multiple System Operator (MSO)]]
* [[Internet protocol television|Internet Protocol Television (IPTV)]]
* [[Over-the-top content|Over-the-top (OTT)]]
* [[Over-the-air television|Over-the-air (OTA)]]
* [[Video on demand|Video on demand (VOD)]] 
* [[Broadcast networks|Broadcast Networks]] 
* [[Internet Television]]
* [[Streaming Media]]
* [[Pay TV]]

==References==
{{reflist}}

[[Category:Data management]]
<=====doc_Id=====>:309
<=====title=====>:
COMMIT (SQL)
<=====text=====>:
{{Unreferenced|date=April 2015}}
A <code>COMMIT</code> statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a <code>[[Begin work (SQL)|BEGIN WORK]]</code> statement, one or more SQL statements, and then the <code>COMMIT</code> statement. Alternatively, a <code>[[Rollback (data management)|ROLLBACK]]</code> statement can be issued, which undoes all the work performed since <code>BEGIN WORK</code> was issued. A <code>COMMIT</code> statement will also release any existing [[savepoint]]s that may be in use.

In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].

==See also==
* [[Commit (data management)]]
* [[Atomic commit]]
* [[Two-phase commit protocol]]
* [[Three-phase commit protocol]]

{{databases}}

{{DEFAULTSORT:Commit (Data Management)}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Transaction processing]]

{{comp-sci-stub}}
<=====doc_Id=====>:312
<=====title=====>:
Author Name Disambiguation
<=====text=====>:
{{Multiple issues|
{{Underlinked|date=May 2016}}
{{Orphan|date=May 2016}}
{{refimprove|date=April 2015}}
}}

'''Author name disambiguation''' is a type of [[Record linkage]] that is applied to scholarly documents where the goal is to find all mentions of the same author and cluster them together. Authors of scholarly documents often share names which makes it hard to distinguish each author's work. Hence, author name disambiguation aims to find all publications that belong to a given author and distinguish them from publications of other authors who share the same name.

There are multiple reasons that cause author names to be ambiguous, among which: individuals may publish under multiple names for variety of reasons including different spelling, misspelling, name change due to marriage, or the use of middle names and initials.<ref>{{cite journal
 | authorlink = Smalheiser, Neil R and Torvik, Vetle I
 | title = Author name disambiguation
 | journal = [[Annual Review of Information Science and Technology]]
 | url = http://onlinelibrary.wiley.com/doi/10.1002/aris.2009.1440430113/full
 | accessdate = 2015-04-20
 | doi = 10.1002/aris.2009.1440430113
}}</ref>

Typical approach for author name disambiguation rely on information about the authors such as their affiliations, email addresses, year of publication, co-authors, topic information to distinguish between authors. These information can be used to learn a machine learning classifier that decides whether two mentions refer to the same author or not. Other approaches utilized heuristics to distinguish between authors.

==References==
{{Reflist}}

[[Category:Metadata]]
[[Category:Data management]]
<=====doc_Id=====>:315
<=====title=====>:
Comparison of CDMI server implementations
<=====text=====>:

{| class="wikitable"
|-
! Implementation !! SNIA Reference Implementation !! CDMI-Serve !! CDMI-Proxy !! CDMI for OpenStack's Swift !! CDMI-z !! onedata
|-
| Version || [http://www.snia.org/forums/csi/programs/CDMIportal 1.0e] || [https://github.com/koenbollen/cdmi-serve 238c28fc7c] || [https://github.com/livenson/vcdm 0.1] || [https://github.com/osaddon/cdmi f0e3ad9bac] || 1 || [http://packages.onedata.org/oneprovider-Linux.rpm 2.0]
|-
| [[CDMI]] Version || 1.0.2 || ? || 1.0.1 || ? || 1.0.2 || 1.0.2
|-
| colspan="7" align="center" | '''HTTP features'''
|-
| [[HTTPS]] || ? || ? || {{Yes}} || ? || ? || {{Yes}}
|-
| [[Basic authentication]] || ? || ? || {{Yes}} || ? || ? || ?
|-
| [[Digest authentication]] || ? || ? || {{Yes}} || ? || ? || ?
|-
| [[X.509|X.509 authentication]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| X.509-VOMS authentication || ? || ? || ? || ? || ? || {{Yes}}
|-
| Token based authentication || ? || ? || ? || ? || ? || {{Yes}}
|-
| colspan="7" align="center" | '''Data access methods'''
|-
| [[Filesystem in Userspace|FUSE]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| [[GridFTP]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[iSCSI]] || {{Yes}} || ? || ? || ? || ? || {{No}}
|-
| [[WebDAV]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[Network File System|NFS]] || ? || ? || ? || ? || ? || {{No}}
|-
| [[Browser user interface|BUI]] || ? || ? || ? || ? || ? || {{Yes}}
|-
| colspan="7" align="center" | '''System-Wide CDMI Capabilities'''
|-
| cdmi_domains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false}}
|-
| cdmi_export_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_dataobjects || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_export_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_occi_iscsi || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_metadata_maxitems || 1024 || ? || ? || ? || 4096 || 1024
|-
| cdmi_metadata_maxsize || 4096 || ? || ? || ? || 4096 || 4096
|-
| cdmi_metadata_maxtotalsize || ∞ || ? || ? || ? || 1048576 || 1048576
|-
| cdmi_notification || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_logging || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_regex || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_contains || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_tags || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_query_value || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_queues || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_security_access_control || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_security_audit || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_data_integrity || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_encryption || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_immutability || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_security_sanitization || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialization_json || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_snapshots || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_references || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_object_move_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_from_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_move_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_copy_from_local || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_object_copy_from_remote || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_object_access_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_post_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_post_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_deserialize_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_dataobject_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_domain_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_container_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_queue_to_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_dataobject_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_queue_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_create_reference_by_ID || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Data Object Capabilities'''
|-
| cdmi_read_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_value || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}|| {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_value_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Container Capabilities'''
|-
| cdmi_list_children || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_list_children_range || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_read_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_metadata || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_modify_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_snapshot || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_serialize_domain || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_create_dataobject || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_post_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_post_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_create_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_create_queue || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_create_reference || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_cifs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_nfs || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_iscsi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_occi || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_export_container_webdav || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_container || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}} || {{Yes|"true"}}
|-
| cdmi_move_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_copy_container || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_move_dataobject || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_copy_dataobject" || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acl || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_size || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_ctime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_atime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_mtime || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{Yes|"true"}}
|-
| cdmi_acount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Domain Object Capabilities'''
|-
| cdmi_create_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_delete_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_domain_summary || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_domain_members || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_list_children || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_deserialize_domain || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| colspan="7" align="center" | '''Queue Object Capabilities'''
|-
| cdmi_read_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_read_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_value || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_metadata || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_modify_deserialize_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_delete_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{Yes|"true"}} || {{No|"false"}}
|-
| cdmi_move_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_copy_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_reference_queue || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acl || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_size || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_ctime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_atime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mtime || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_acount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_mcount || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_assignedsize || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_data_dispersion || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_retention || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_autodelete || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_data_holds || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_encryption || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_geographic_placement || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_immediate_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_infrastructure_redundancy || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|""}} || {{No|"false"}}
|-
| cdmi_latency || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RPO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_RTO || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_sanitization_method || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|-
| cdmi_throughput || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|"false"}} || {{No|"false"}}
|-
| cdmi_value_hash || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|N/A}} || {{No|[]}} || {{No|"false"}}
|}

{{DEFAULTSORT:CDMI server implementation comparison}}
[[Category:Cloud storage]]
[[Category:Data management]]
<=====doc_Id=====>:318
<=====title=====>:
Database administrator
<=====text=====>:
{{Infobox Occupation
|caption= Database Administrator
|official_names= Database administrator, database analyst
|activity_sector=[[Information technology]], [[information system]]s
|competencies= [[Database design|Databases design and implementation]], [[Computer programming|programming]] skills, [[database theory]], [[Computer network|networking]] basics, [[analytical skill]]s, [[critical thinking]]
|formation=At least a [[Academic certificate|certificate]] with experience.
}}

'''Database administrators''' ('''DBAs''') use specialized software to store and organize data.<ref name="BLS-DBA">{{cite web | url=http://www.bls.gov/ooh/computer-and-information-technology/database-administrators.htm | title=Database Administrators | publisher=Bureau Of Labor Statistics | work=11/04/2015 | accessdate=4 November 2015}}</ref>

The role may include [[capacity planning]], [[Installation (computer programs)|installation]], [[Computer configuration|configuration]], [[database design]], [[Data migration|migration]], performance monitoring, [[Computer security|security]], [[troubleshooting]], as well as [[backup]] and [[data recovery]].<ref name="techrepublic">{{cite web | url=http://www.techrepublic.com/blog/the-enterprise-cloud/what-does-a-dba-do-all-day/ | title=What does a DBA do all day? | publisher=techrepublic.com | work=11/04/2015 | accessdate=4 November 2015}}</ref>

==Skills==
List of skills required to become database administrators are:<ref>{{cite web|last1=Spenik|first1=Mark|last2=Sledge|first2=Orryn|date=2001-03-20|url=http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|title=What is a Database Administrator? (DBA)|publisher=Developer.com|accessdate=2012-02-06|archiveurl=https://web.archive.org/web/20110613101702/http://www.developer.com/db/article.php/718491/What-Is-a-Database-Administrator.htm|archivedate=2011-06-13}}</ref><ref>http://www.dba-oracle.com/oracle_tips_dba_job_skills.htm</ref><ref>http://www.orafaq.com/wiki/Roles_and_Responsibilities</ref>
* [[Communication]] skills
* Knowledge of [[database theory]]
* Knowledge of [[database design]]
* Knowledge about the [[Relational database management system|RDBMS]] itself, e.g. [[Microsoft SQL Server]] or [[MySQL]]
* Knowledge of [[SQL|structured query language]] (SQL), e.g. [[SQL/PSM]] or [[Transact-SQL]]
* General [[understanding]] of [[Distributed computing|distributed computing architectures]], e.g. [[Client–server model]]
* General understanding of [[operating system]], e.g. [[Microsoft Windows|Windows]] or [[Linux]]
* General understanding of [[Computer data storage|storage technologies]] and [[Computer network|networking]]
* General understanding of routine maintenance, recovery, and handling failover of a database

Database administrators benefit from a [[bachelor's degree]] or [[master's degree]] in [[computer science]]. An [[associate degree]] or a [[Academic certificate|certificate]] may be sufficient with work experience.<ref name="study.com">{{cite web | url=http://study.com/articles/Database_Administrator_Job_Description_and_Requirements.html | title=Database Administrator: Job Description and Requirements | publisher=study.com | work=11/4/2015 | accessdate=4 November 2015}}</ref>

===Certification===
There are many certifications available for becoming a certified database administrator. Many of these certifications are offered by database vendors themselves. By passing a series of tests and sometimes other requirements, you can earn a database administrator certification. Schools offering Database Administration degrees can also be found.<ref name="learn.org">{{cite web | url=http://learn.org/articles/How_Do_I_Become_a_Certified_Database_Administrator.html | title=How Do I Become a Certified Database Administrator? | publisher=learn.org | work=learn.org | accessdate=4 November 2015}}</ref>

For example:
* IBM Certified Advanced Database Administrator - DB2 10.1 for Linux, Unix and Windows<ref name="ibm.com">{{cite web |url=http://www-03.ibm.com/certify/index.shtml |title=IBM Professional Certification Program |work=ibm.com |publisher=[[IBM]] |accessdate=2014-08-10}}</ref>
* IBM Certified Database Administrator - DB2 10.1 for Linux, Unix, and Windows<ref name="ibm.com"/>
* Oracle Database 11g Administrator Certified Professional<ref>{{cite web |url=http://education.oracle.com/pls/web_prod-plq-dad/db_pages.getpage?page_id=143&p_org_id=1001&lang=US |title=Oracle Certification Program |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2011-06-18}}</ref>
* Oracle MySQL 5.6 Database Administrator Certified Professional<ref>{{cite web |url=https://education.oracle.com/pls/web_prod-plq-dad/ou_product_category.getPage?p_cat_id=159&p_org_id=15941&lang=US#tabs-3 |title=Oracle Certified Professional, MySQL 5.6 Database Administrator |work=oracle.com |publisher=[[Oracle Corporation]] |accessdate=2016-09-18}}</ref>
* MCSA SQL Server 2012<ref name=MCSASQL>{{cite web |url=https://www.microsoft.com/en-us/learning/mcsa-sql-certification.aspx |title=MCSA: SQL Server |work=microsoft.com |publisher=[[Microsoft]] |accessdate=2015-11-03}}</ref>
* MCSE Data Platform Solutions Expert <ref name="microsoftsolutionsexpert">{{cite web | url=https://www.microsoft.com/en-us/learning/mcse-sql-data-platform.aspx | title=MCSE: Data Platform | publisher=microsoft.com | work=11/4/2015 | accessdate=4 November 2015}}</ref>

==Duties==
A database administrator's responsibilities can include the following tasks:<ref>{{cite web |url=http://docs.oracle.com/cd/B10501_01/server.920/a96521/dba.htm#852 |title=Oracle DBA Responsibilities |work=[[Oracle Corporation]] |accessdate=2012-02-06}}</ref>
* [[Installation (computer programs)|Installing]] and [[upgrade|upgrading]] the database server and application tools
* Allocating system storage and [[planning]] future storage requirements for the database system
* Modifying the database structure, as necessary, from information given by application developers
* Enrolling users and maintaining system [[Computer security|security]]
* Ensuring compliance with database vendor [[license|license agreement]]
* Controlling and [[System Monitoring|monitoring]] [[user (computing)|user]] access to the database
* Monitoring and [[Program optimization|optimizing]] the performance of the database
* Planning for [[backup]] and recovery of database information
* Maintaining [[archive]]d data
* Backing up and restoring databases
* Contacting database [[vendor]] for [[technical support]]
* Generating various reports by querying from database as per need

==See also==
* [[Comparison of database tools]]

==References==
{{Reflist}}

==External links==
"Database Administrators"

{{Database}}

{{Use British English|date=June 2012}}
{{Use dmy dates|date=June 2012}}

{{DEFAULTSORT:Database Administrator}}
[[Category:Computer occupations]]
[[Category:Data management]]
[[Category:Database specialists| ]]
<=====doc_Id=====>:321
<=====title=====>:
Big data
<=====text=====>:
{{About|large collections of data|the band|Big Data (band)}}
[[File:Hilbert InfoGrowth.png|thumb|right|400px|Growth of and digitization of global information-storage capacity<ref>{{cite web|url= http://www.martinhilbert.net/WorldInfoCapacity.html|title= The World’s Technological Capacity to Store, Communicate, and Compute Information|work= MartinHilbert.net|accessdate= 13 April 2016}}</ref>]]

'''''Big data''''' is a term for [[data set]]s that are so large or complex that traditional [[data processing]] applications are inadequate to deal with them.  Challenges include [[Data analysis|analysis]], capture, [[data curation]], search, [[Data sharing|sharing]], [[Computer data storage|storage]], [[Data transmission|transfer]], [[Data visualization|visualization]], [[Query language|querying]], updating and [[information privacy]]. The term "big data" often refers simply to the use of [[predictive analytics]], [[user behavior analytics]], or certain other advanced data analytics methods that extract value from data, and seldom to a particular size of data set.<ref>{{Cite book|url= http://link.springer.com/10.1007/978-3-319-21569-3 |title= New Horizons for a Data-Driven Economy – Springer|doi= 10.1007/978-3-319-21569-3}}</ref> "There is little doubt that the quantities of data now available are indeed large, but that’s not the most relevant characteristic of this new data ecosystem."<ref>{{cite journal |last1=boyd |first1=dana |last2=Crawford |first2=Kate |title=Six Provocations for Big Data |journal=Social Science Research Network: A Decade in Internet Time: Symposium on the Dynamics of the Internet and Society |date=September 21, 2011 |doi=10.2139/ssrn.1926431}}</ref>

Analysis of data sets can find new correlations to "spot business trends, prevent diseases, combat crime and so on".{{r|Economist}} Scientists, business executives, practitioners of medicine, advertising and [[Government database|governments]] alike regularly meet difficulties with large data-sets in areas including [[Web search engine|Internet search]], finance, [[urban informatics]], and [[business informatics]].  Scientists encounter limitations in [[e-Science]] work, including [[meteorology]], [[genomics]],<ref>{{cite journal |title= Community cleverness required |journal= Nature |volume= 455 |issue= 7209 |page= 1 |date= 4 September 2008 |doi= 10.1038/455001a |url= http://www.nature.com/nature/journal/v455/n7209/full/455001a.html}}</ref> [[connectomics]], complex physics simulations, biology and environmental research.<ref>{{cite journal |last1= Reichman |first1= O.J. |last2= Jones |first2= M.B. |last3= Schildhauer |first3= M.P. |title= Challenges and Opportunities of Open Data in Ecology |journal= Science |volume= 331 |issue= 6018 |pages= 703–5 |year= 2011 |doi= 10.1126/science.1197962 |pmid= 21311007 }}</ref>

Data sets grow rapidly - in part because they are increasingly gathered by cheap and numerous information-sensing [[mobile device]]s, aerial ([[remote sensing]]), software logs, [[Digital camera|cameras]], microphones, [[radio-frequency identification]] (RFID) readers and [[wireless sensor networks]].<ref>{{cite web |author= Hellerstein, Joe |title= Parallel Programming in the Age of Big Data |date= 9 November 2008 |work= Gigaom Blog |url= http://gigaom.com/2008/11/09/mapreduce-leads-the-way-for-parallel-programming/}}</ref><ref>{{cite book |first1= Toby |last1= Segaran |first2= Jeff |last2= Hammerbacher |title= Beautiful Data: The Stories Behind Elegant Data Solutions |url= https://books.google.com/books?id=zxNglqU1FKgC |year= 2009 |publisher= O'Reilly Media |isbn= 978-0-596-15711-1 |page= 257}}</ref> The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s;<ref name="martinhilbert.net">{{cite journal | last1 = Hilbert | first1 = Martin | first2 = Priscila |last2=López | title = The World's Technological Capacity to Store, Communicate, and Compute Information | journal = Science | volume = 332 | issue = 6025 | pages = 60–65 | year = 2011 | doi = 10.1126/science.1200970 | pmid = 21310967 | url= http://martinhilbert.net/WorldInfoCapacity.html | ref= harv}}</ref> {{As of|2012|lc=on}}, every day 2.5 [[exabyte]]s (2.5×10<sup>18</sup>) of data are generated.<ref>{{cite web|url= http://www.ibm.com/big-data/us/en/ |title= IBM What is big data? – Bringing big data to the enterprise |publisher= www.ibm.com |accessdate= 2013-08-26}}</ref> One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.<ref>Oracle and FSN, [http://www.fsn.co.uk/channel_bi_bpm_cpm/mastering_big_data_cfo_strategies_to_transform_insight_into_opportunity#.UO2Ac-TTuys "Mastering Big Data: CFO Strategies to Transform Insight into Opportunity"], December 2012</ref>

[[Relational database management system]]s and desktop statistics- and visualization-packages often have difficulty handling big data. The work may require "massively parallel software running on tens, hundreds, or even thousands of servers".<ref>{{cite web |author= Jacobs, A. |title= The Pathologies of Big Data |date= 6 July 2009 |work= ACMQueue |url= http://queue.acm.org/detail.cfm?id=1563874}}</ref> What counts as "big data" varies depending on the capabilities of the users and their tools, and expanding capabilities make big data a moving target. "For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration."<ref>
{{cite journal 
|last1= Magoulas |first1= Roger 
|last2= Lorica |first2= Ben 
|title= Introduction to Big Data 
|journal= Release 2.0 
|issue= 11 |date= February 2009 
|url= http://radar.oreilly.com/r2/release2-0-11.html 
|publisher= O'Reilly Media 
|location= Sebastopol CA
}}
</ref>

== Definition ==
[[File:Viegas-UserActivityonWikipedia.gif|thumb|Visualization of daily Wikipedia edits created by IBM. At multiple [[terabyte]]s in size, the text and images of Wikipedia are an example of big data.]]
The term has been in use since the 1990s, with some giving credit to [[John Mashey]] for coining or at least making it popular.<ref>{{Cite web |title=  Big Data ... and the Next Wave of InfraStress |author= John R. Mashey |date= 25 April 1998 |publisher= Usenix |work= Slides from invited talk |url= http://static.usenix.org/event/usenix99/invited_talks/mashey.pdf |accessdate= 28 September 2016 }}</ref><ref>{{cite web|title=The Origins of ‘Big Data’: An Etymological Detective Story |author=Steve Lohr |date= 1 February 2013 |url=http://bits.blogs.nytimes.com/2013/02/01/the-origins-of-big-data-an-etymological-detective-story/ |publisher= [[New York Times]] |accessdate= 28 September 2016 }}</ref>
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to [[data acquisition|capture]], [[data curation|curate]], manage, and process data within a tolerable elapsed time.<ref name="Editorial">{{cite journal | last1 = Snijders | first1 = C. | last2 = Matzat | first2 = U. | last3 = Reips | first3 = U.-D. | year = 2012 | title = 'Big Data': Big gaps of knowledge in the field of Internet | url = http://www.ijis.net/ijis7_1/ijis7_1_editorial.html | journal = International Journal of Internet Science | volume = 7 | issue = | pages = 1–5 }}</ref> Big data "size" is a constantly moving target, {{As of|2012|lc=on}} ranging from a few dozen terabytes to many [[petabyte]]s of data.
Big data requires a set of techniques and technologies with new forms of integration to reveal insights from datasets that are diverse, complex, and of a massive scale.<ref>{{cite journal | last1 = Ibrahim | first1 =  | last2 = Targio Hashem | first2 = Abaker | last3 = Yaqoob | first3 = Ibrar | last4 = Badrul Anuar | first4 = Nor | last5 = Mokhtar | first5 = Salimah | last6 = Gani | first6 = Abdullah | last7 = Ullah Khan | first7 = Samee | year = 2015 | title = big data" on cloud computing: Review and open research issues | url = | journal = Information Systems | volume = 47 | issue = | pages = 98–115 | doi = 10.1016/j.is.2014.07.006 }}</ref>

In a 2001 research report<ref>{{cite web |first=Douglas |last=Laney |title=3D Data Management: Controlling Data Volume, Velocity and Variety |url=http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf |publisher=Gartner |accessdate = 6 February 2001}}</ref> and related lectures, [[META Group]] (now [[Gartner]]) analyst [[Doug Laney]] defined data growth challenges and opportunities as being three-dimensional, i.e. increasing [[volume]] (amount of data), [[velocity]] (speed of data in and out), and {{linktext|variety}} (range of data types and sources). Gartner, and now much of the industry, continue to use this "3Vs" model for describing big data.<ref>{{cite web |last=Beyer |first=Mark |title=Gartner Says Solving 'Big Data' Challenge Involves More Than Just Managing Volumes of Data |url=http://www.gartner.com/it/page.jsp?id=1731916 |publisher=Gartner |accessdate = 13 July 2011| archiveurl= https://web.archive.org/web/20110710043533/http://www.gartner.com/it/page.jsp?id=1731916| archivedate= 10 July 2011 | deadurl= no}}</ref> In 2012, [[Gartner]] updated its definition as follows: "Big data is high volume, high velocity, and/or high variety information assets that require new forms of processing to enable enhanced decision making, insight discovery and process optimization." Gartner's definition of the 3Vs is still widely used, and in agreement with a consensual definition that states that "Big Data represents the Information assets characterized by such a High Volume, Velocity and Variety to require specific Technology and Analytical Methods for its transformation into Value".<ref name="Big Data Definition">{{cite journal | last1 = De Mauro | first1 = Andrea | last2 = Greco | first2 = Marco | last3 = Grimaldi | first3 = Michele | year = 2016 | title = A Formal definition of Big Data based on its essential Features | url = http://www.emeraldinsight.com/doi/abs/10.1108/LR-06-2015-0061 | journal = Library Review | volume = 65| issue = | pages = 122–135 | doi=10.1108/LR-06-2015-0061}}</ref> Additionally, a new V "Veracity" is added by some organizations to describe it,<ref>{{cite web|title=What is Big Data?|url=http://www.villanovau.com/university-online-programs/what-is-big-data/|publisher=[[Villanova University]]}}</ref> revisionism challenged by some industry authorities.<ref>{{cite web|last=Grimes|first=Seth|title=Big Data: Avoid 'Wanna V' Confusion|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-avoid-wanna-v-confusion/d/d-id/1111077?|publisher=[[InformationWeek]]|accessdate = 5 January 2016}}</ref> The 3Vs have been expanded to other complementary characteristics of big data:<ref name="BD4D">{{cite web |last=Hilbert |first=Martin |title=Big Data for Development: A Review of Promises and Challenges. Development Policy Review. |url=http://www.martinhilbert.net/big-data-for-development |work=martinhilbert.net |accessdate=2015-10-07}}</ref><ref name="WhatIsBigData" />
* Volume: big data doesn't sample; it just observes and tracks what happens
* Velocity: big data is often available in real-time
* Variety: big data draws from text, images, audio, video; plus it completes missing pieces through [[data fusion]]
* [[Machine Learning]]: big data often doesn't ask why and simply detects patterns<ref>Mayer-Schönberger, V., & Cukier, K. (2013). Big data: a revolution that will transform how we live, work and think. London: John Murray.</ref>
* [[Digital footprint]]: big data is often a cost-free byproduct of digital interaction<ref name="WhatIsBigData">{{cite av media|url=https://www.youtube.com/watch?v=XRVIh1h47sA&index=51&list=PLtjBSCvWCU3rNm46D3R85efM0hrzjuAIg|title=DT&SC 7-3: What is Big Data?|date=12 August 2015|publisher=|via=YouTube}}</ref><ref>{{cite web|url=https://canvas.instructure.com/courses/949415|title=Digital Technology & Social Change|publisher=}}</ref>

The growing maturity of the concept more starkly delineates the difference between big data and [[Business Intelligence]]:<ref>http://www.bigdataparis.com/presentation/mercredi/PDelort.pdf?PHPSESSID=tv7k70pcr3egpi2r6fi3qbjtj6#page=4</ref>
* Business Intelligence uses [[descriptive statistics]] with data with high information density to measure things, detect trends, etc..
* Big data uses [[inductive statistics]] and concepts from [[nonlinear system identification]]<ref name="SAB1">Billings S.A. "Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains". Wiley, 2013</ref>  to infer laws (regressions, nonlinear relationships, and causal effects) from large sets of data with low information density<ref>{{cite web|url=http://www.andsi.fr/tag/dsi-big-data/|title=le Blog ANDSI   » DSI Big Data|publisher=}}</ref> to reveal relationships and dependencies, or to perform predictions of outcomes and behaviors.<ref name="SAB1" /><ref>{{cite web|url=http://lecercle.lesechos.fr/entrepreneur/tendances-innovation/221169222/big-data-low-density-data-faible-densite-information-com|title=Les Echos – Big Data car Low-Density Data ? La faible densité en information comme facteur discriminant – Archives|author=Les Echos|date=3 April 2013|work=lesechos.fr}}</ref>

== Characteristics ==
Big data can be described by the following characteristics:<ref name="BD4D" /><ref name="WhatIsBigData" />

;Volume: The quantity of generated and stored data. The size of the data determines the value and potential insight- and whether it can actually be considered big data or not.

;Variety: The type and nature of the data. This helps people who analyze it to effectively use the resulting insight.

;Velocity: In this context, the speed at which the data is generated and processed to meet the demands and challenges that lie in the path of growth and development.

;Variability: Inconsistency of the data set can hamper processes to handle and manage it.

;Veracity: The quality of captured data can vary greatly, affecting accurate analysis.

Factory work and [[Cyber-physical system]]s may have a 6C system:
* Connection (sensor and networks)
* Cloud (computing and data on demand)<ref>Wu, D., Liu. X., Hebert, S., Gentzsch, W., Terpenny, J. (2015). Performance Evaluation of Cloud-Based High Performance Computing for Finite Element Analysis. Proceedings of the ASME 2015 International Design Engineering Technical Conference & Computers and Information in Engineering Conference (IDETC/CIE2015), Boston, Massachusetts, U.S.</ref><ref>{{cite journal | last1 = Wu | first1 = D. | last2 = Rosen | first2 = D.W. | last3 = Wang | first3 = L. | last4 = Schaefer | first4 = D. | year = 2015 | title = Cloud-Based Design and Manufacturing: A New Paradigm in Digital Manufacturing and Design Innovation | url = | journal = Computer-Aided Design | volume = 59 | issue = 1| pages = 1–14 | doi = 10.1016/j.cad.2014.07.006 }}</ref>
* Cyber (model and memory)
* Content/context (meaning and correlation)
* Community (sharing and collaboration)
* Customization (personalization and value)

Data must be processed with advanced tools (analytics and algorithms) to reveal meaningful information. For example, to manage a factory one must consider both visible and invisible issues with various components. Information generation algorithms must detect and address invisible issues such as machine degradation, component wear, etc. on the factory floor.<ref name=INDIN2014>{{cite journal|last1=Lee|first1=Jay|last2=Bagheri|first2=Behrad|last3=Kao|first3=Hung-An|title=Recent Advances and Trends of Cyber-Physical Systems and Big Data Analytics in Industrial Informatics|journal=IEEE Int. Conference on Industrial Informatics (INDIN) 2014|date=2014|url=https://www.researchgate.net/profile/Behrad_Bagheri/publication/266375284_Recent_Advances_and_Trends_of_Cyber-Physical_Systems_and_Big_Data_Analytics_in_Industrial_Informatics/links/542dc0100cf27e39fa948a7d?origin=publication_detail}}</ref><ref name=MfgLetters>{{cite journal|last1=Lee|first1=Jay|last2=Lapira|first2=Edzel|last3=Bagheri|first3=Behrad|last4=Kao|first4=Hung-an|title=Recent advances and trends in predictive manufacturing systems in big data environment|journal=Manufacturing Letters|volume=1|issue=1|pages=38–41|doi=10.1016/j.mfglet.2013.09.005|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114}}</ref>

== Architecture ==

In 2000, Seisint Inc. (now [[LexisNexis|LexisNexis Group]]) developed a C++-based distributed file-sharing framework for data storage and query. The system stores and distributes structured, semi-structured, and [[unstructured data]] across multiple servers. Users can build queries in a C++ [[Dialect (computing)|dialect]] called [[ECL programming language|ECL]]. ECL uses an "apply schema on read" method to infer the structure of stored data when it is queried, instead of when it is stored. In 2004, LexisNexis acquired Seisint Inc.<ref>{{cite web|url=http://www.washingtonpost.com/wp-dyn/articles/A50577-2004Jul14.html|title=LexisNexis To Buy Seisint For $775 Million|publisher=Washington Post|accessdate=15 July 2004}}</ref> and in 2008 acquired [[ChoicePoint|ChoicePoint, Inc.]]<ref>{{cite web|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/02/21/AR2008022100809.html|title=LexisNexis Parent Set to Buy ChoicePoint|publisher=Washington Post|accessdate=22 February 2008}}</ref> and their high-speed parallel processing platform. The two platforms were merged into [[HPCC]] (or High-Performance Computing Cluster) Systems and in 2011, HPCC was open-sourced under the Apache v2.0 License. [[Quantcast File System]] was available about the same time.<ref>{{cite web|url=http://www.datanami.com/2012/10/01/quantcast_opens_exabyte_ready_file_system/|title=Quantcast Opens Exabyte-Ready File System|publisher=www.datanami.com|accessdate=1 October 2012}}</ref>

In 2004, [[Google]] published a paper on a process called [[MapReduce]] that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the Map step). The results are then gathered and delivered (the Reduce step). The framework was very successful,<ref>Bertolucci, Jeff [http://www.informationweek.com/big-data/news/software-platforms/hadoop-from-experiment-to-leading-big-d/240157176 "Hadoop: From Experiment To Leading Big Data Platform"], "Information Week", 2013. Retrieved on 14 November 2013.</ref> so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named [[Apache Hadoop|Hadoop]].<ref>Webster, John. [http://research.google.com/archive/mapreduce-osdi04.pdf "MapReduce: Simplified Data Processing on Large Clusters"], "Search Storage", 2004. Retrieved on 25 March 2013.</ref>

[[MIKE2.0 Methodology|MIKE2.0]] is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled "Big Data Solution Offering".<ref>{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Solution_Offering|title=Big Data Solution Offering|publisher=MIKE2.0|accessdate=8 December 2013}}</ref> The methodology addresses handling big data in terms of useful [[permutation]]s of data sources, [[complexity]] in interrelationships, and difficulty in deleting (or modifying) individual records.<ref>{{cite web|url=http://mike2.openmethodology.org/wiki/Big_Data_Definition|title=Big Data Definition|publisher=MIKE2.0|accessdate=9 March 2013}}</ref>

2012 studies showed that a multiple-layer architecture is one option to address the issues that big data presents. A [[List of file systems#Distributed parallel file systems|distributed parallel]] architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end user by using a front-end application server.<ref>{{cite journal|last=Boja|first=C|author2=Pocovnicu, A |author3=Bătăgan, L. |title=Distributed Parallel Architecture for Big Data|journal=Informatica Economica|year=2012|volume=16|issue=2|pages=116–127}}</ref>

Big data analytics for manufacturing applications is marketed as a 5C architecture (connection, conversion, cyber, cognition, and configuration).<ref>{{cite web|url=http://www.imscenter.net/cyber-physical-platform|title=IMS_CPS — IMS Center|publisher=|accessdate=16 June 2016}}</ref>

The [[data lake]] allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.<ref>http://www.hcltech.com/sites/default/files/solving_key_businesschallenges_with_big_data_lake_0.pdf</ref><ref>{{ cite web| url=https://secplab.ppgia.pucpr.br/files/papers/2015-0.pdf | title= Method for testing the fault tolerance of MapReduce frameworks | publisher=Computer Networks | year=2015}}</ref>

== Technologies ==
{{see|Enablers of big data}}
A 2011 [[McKinsey & Company|McKinsey Global Institute]] report characterizes the main components and ecosystem of big data as follows:<ref name="McKinsey">{{cite journal
 | last1 = Manyika
 | first1 = James
 | first2=Michael |last2=Chui |first3=Jaques |last3=Bughin |first4=Brad |last4=Brown |first5=Richard |last5=Dobbs |first6=Charles |last6=Roxburgh |first7=Angela Hung |last7=Byers
 | title = Big Data: The next frontier for innovation, competition, and productivity
 | publisher = McKinsey Global Institute
 | date = May 2011
 | url =  http://www.mckinsey.com/Insights/MGI/Research/Technology_and_Innovation/Big_data_The_next_frontier_for_innovation
|accessdate=January 16, 2016
}}</ref>
* Techniques for analyzing data, such as [[A/B testing]], [[machine learning]] and [[natural language processing]]
* Big data technologies, like [[business intelligence]], [[cloud computing]] and databases
* Visualization, such as charts, graphs and other displays of the data

Multidimensional big data can also be represented as [[tensor]]s, which can be more efficiently handled by tensor-based computation,<ref>{{cite web |title=Future Directions in Tensor-Based Computation and Modeling |date=May 2009|url=http://www.cs.cornell.edu/cv/tenwork/finalreport.pdf}}</ref> such as [[multilinear subspace learning]].<ref name="MSLsurvey">{{cite journal
 |first=Haiping |last=Lu
 |first2=K.N. |last2=Plataniotis
 |first3=A.N. |last3=Venetsanopoulos
 |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf
 |title=A Survey of Multilinear Subspace Learning for Tensor Data
 |journal=Pattern Recognition
 |volume=44 |number=7 |pages=1540–1551 |year=2011
 |doi=10.1016/j.patcog.2011.01.004
}}</ref> Additional technologies being applied to big data include massively parallel-processing ([[Massive parallel processing|MPP]]) databases, [[search-based application]]s, [[data mining]],<ref>{{cite web|last1=Pllana|first1=Sabri|last2=Janciak|first2=Ivan|last3=Brezany|first3=Peter|last4=Wöhrer|first4=Alexander|title=A Survey of the State of the Art in Data Mining and Integration Query Languages|url=http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6041580|website=2011 International Conference on Network-Based Information Systems (NBIS 2011)|publisher=IEEE Computer Society|accessdate=2 April 2016}}</ref> [[distributed file system]]s, [[distributed database]]s, [[cloud computing|cloud-based]] infrastructure (applications, storage and computing resources) and the Internet.{{Citation needed|date=September 2011}}

Some but not all [[Massive parallel processing|MPP]] relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the [[RDBMS]].<ref>{{cite web |author=Monash, Curt |title=eBay's two enormous data warehouses |date=30 April 2009 |url=http://www.dbms2.com/2009/04/30/ebays-two-enormous-data-warehouses/}}<br />{{cite web |author=Monash, Curt |title=eBay followup&nbsp;– Greenplum out, Teradata > 10 petabytes, Hadoop has some value, and more |date=6 October 2010 |url=http://www.dbms2.com/2010/10/06/ebay-followup-greenplum-out-teradata-10-petabytes-hadoop-has-some-value-and-more/}}</ref>

[[DARPA]]'s [[Topological Data Analysis]] program seeks the fundamental structure of massive data sets and in 2008 the technology went public with the launch of a company called [[Ayasdi]].<ref>{{cite web|url=http://www.ayasdi.com/resources/|title=Resources on how Topological Data Analysis is used to analyze big data|publisher=Ayasdi}}</ref>

The practitioners of big data analytics processes are generally hostile to slower shared storage,<ref>{{cite web |title=Storage area networks need not apply |author=CNET News |date=1 April 2011 |url=http://news.cnet.com/8301-21546_3-20049693-10253464.html}}</ref> preferring direct-attached storage ([[Direct-attached storage|DAS]]) in its various forms from solid state drive ([[Ssd]]) to high capacity [[Serial ATA|SATA]] disk buried inside parallel processing nodes. The perception of shared storage architectures—[[Storage area network]] (SAN) and [[Network-attached storage]] (NAS) —is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.

Real or near-real time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in memory is good—data on spinning disk at the other end of a [[Fiber connector|FC]] [[Storage area network|SAN]] connection is not. The cost of a [[Storage area network|SAN]] at the scale needed for analytics applications is very much higher than other storage techniques.

There are advantages as well as disadvantages to shared storage in big data analytics, but big data analytics practitioners {{As of|2011|lc=on}} did not favour it.<ref>{{cite web |title=How New Analytic Systems will Impact Storage |date=September 2011 |url=http://www.evaluatorgroup.com/document/big-data-how-new-analytic-systems-will-impact-storage-2/}}</ref>

== Applications ==
[[File:2013-09-11 Bus wrapped with SAP Big Data parked outside IDF13 (9730051783).jpg|thumb|Bus wrapped with [[SAP AG|SAP]] Big data parked outside [[Intel Developer Forum|IDF13]].]]
Big data has increased the demand of information management specialists so much so that [[Software AG]], [[Oracle Corporation]], [[IBM]], [[Microsoft]], [[SAP AG|SAP]], [[EMC Corporation|EMC]], [[Hewlett-Packard|HP]] and [[Dell]] have spent more than $15&nbsp;billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100&nbsp;billion and was growing at almost 10&nbsp;percent a year: about twice as fast as the software business as a whole.{{r|Economist}}

Developed economies increasingly use data-intensive technologies. There are 4.6&nbsp;billion mobile-phone subscriptions worldwide, and between 1&nbsp;billion and 2&nbsp;billion people accessing the internet.{{r|Economist}} Between 1990 and 2005, more than 1&nbsp;billion people worldwide entered the middle class, which means more people became more literate, which in turn lead to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 [[petabytes]] in 1986, 471 [[petabytes]] in 1993, 2.2 exabytes in 2000, 65 [[exabytes]] in 2007<ref name="martinhilbert.net"/> and predictions put the amount of internet traffic at 667 exabytes annually by 2014.{{r|Economist}} According to one estimate, one third of the globally stored information is in the form of alphanumeric text and still image data,<ref name="HilbertContent">{{cite web|url=http://www.tandfonline.com/doi/abs/10.1080/01972243.2013.873748|title=An Error Occurred Setting Your User Cookie|publisher=}}</ref> which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).

While many vendors offer off-the-shelf solutions for big data, experts recommend the development of in-house solutions custom-tailored to solve the company's problem at hand if the company has sufficient technical capabilities.<ref>{{cite web |url=http://www.kdnuggets.com/2014/07/interview-amy-gershkoff-ebay-in-house-BI-tools.html |title=Interview: Amy Gershkoff, Director of Customer Analytics & Insights, eBay on How to Design Custom In-House BI Tools |last1=Rajpurohit |first1=Anmol |date=11 July 2014 |website= KDnuggets|accessdate=2014-07-14|quote=Dr. Amy Gershkoff: "Generally, I find that off-the-shelf business intelligence tools do not meet the needs of clients who want to derive custom insights from their data. Therefore, for medium-to-large organizations with access to strong technical talent, I usually recommend building custom, in-house solutions."}}</ref>

=== Government ===
The use and adoption of big data within governmental processes is beneficial and allows efficiencies in terms of cost, productivity, and innovation,<ref>{{cite web|url=http://www.computerworld.com/article/2472667/government-it/the-government-and-big-data--use--problems-and-potential.html |title=The Government and big data: Use, problems and potential |date=21 March 2012 |publisher=Computerworld |access-date=12 September 2016}}</ref> but does not come without its flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. Below are some examples of initiatives the governmental big data space.

==== United States of America ====
* In 2012, the [[Presidency of Barack Obama|Obama administration]] announced the Big Data Research and Development Initiative, to explore how big data could be used to address important problems faced by the government.<ref name=WH_Big_Data>{{cite web|last=Kalil|first=Tom|title=Big Data is a Big Deal|url=http://www.whitehouse.gov/blog/2012/03/29/big-data-big-deal|publisher=White House|accessdate=26 September 2012}}</ref> The initiative is composed of 84 different big data programs spread across six departments.<ref>{{cite web|last=Executive Office of the President|title=Big Data Across the Federal Government|url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_fact_sheet_final_1.pdf|publisher=White House|accessdate=26 September 2012 |date=March 2012}}</ref>
* Big data analysis played a large role in [[Barack Obama]]'s successful [[Barack Obama presidential campaign, 2012|2012 re-election campaign]].<ref name=infoworld_bigdata>{{cite web|last=Lampitt|first=Andrew|title=The real story of how big data analytics helped Obama win|url=http://www.infoworld.com/d/big-data/the-real-story-of-how-big-data-analytics-helped-obama-win-212862|work=[[Infoworld]]|accessdate=31 May 2014}}</ref>
* The [[United States Federal Government]] owns six of the ten most powerful [[supercomputer]]s in the world.<ref>{{cite web |last=Hoover |first=J. Nicholas |title=Government's 10 Most Powerful Supercomputers |url=http://www.informationweek.com/government/enterprise-applications/image-gallery-governments-10-most-powerf/224700271 |work=Information Week |publisher=UBM |accessdate=26 September 2012}}</ref>
* The [[Utah Data Center]] has been constructed by the United States [[National Security Agency]]. When finished, the facility will be able to handle a large amount of information collected by the NSA over the Internet. The exact amount of storage space is unknown, but more recent sources claim it will be on the order of a few [[exabyte]]s.<ref>{{cite news | last=Bamford|first=James|title=The NSA Is Building the Country's Biggest Spy Center (Watch What You Say)|url=http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/1|work=Wired Magazine|accessdate=2013-03-18|date=15 March 2012}}</ref><ref>{{cite web|url=http://www.nsa.gov/public_info/press_room/2011/utah_groundbreaking_ceremony.shtml|title=Groundbreaking Ceremony Held for $1.2 Billion Utah Data Center|publisher=National Security Agency Central Security Service|accessdate=2013-03-18}}</ref><ref>{{cite news | last=Hill|first=Kashmir|title=TBlueprints of NSA's Ridiculously Expensive Data Center in Utah Suggest It Holds Less Info Than Thought|url=http://www.forbes.com/sites/kashmirhill/2013/07/24/blueprints-of-nsa-data-center-in-utah-suggest-its-storage-capacity-is-less-impressive-than-thought/|work=Forbes|accessdate=2013-10-31}}</ref>

==== India ====
* Big data analysis was in part responsible for the [[Bharatiya Janata Party|BJP]] to win the [[Indian general election, 2014|Indian General Election 2014]].<ref>{{cite web|url = http://www.livemint.com/Industry/bUQo8xQ3gStSAy5II9lxoK/Are-Indian-companies-making-enough-sense-of-Big-Data.html|title = News: Live Mint|date = 23 June 2014|accessdate = 2014-11-22|website = Are Indian companies making enough sense of Big Data?|publisher = Live Mint}}</ref>
* The [[Government of India|Indian government]] utilizes numerous techniques to ascertain how the Indian electorate is responding to government action, as well as ideas for policy augmentation.<ref>{{cite web|url=http://decipherias.com/currentaffairs/big-data-whats-so-big-about-it/|title=Big Data- What’s so big about it?|date=18 March 2016|publisher=Decipher IAS|access-date=12 September 2016}}</ref>

==== United Kingdom ====
Examples of uses of big data in public services:
* Data on prescription drugs: by connecting origin, location and the time of each prescription, a research unit was able to exemplify the considerable delay between the release of any given drug, and a UK-wide adaptation of the [[National Institute for Health and Care Excellence]] guidelines. This suggests that new or most up-to-date drugs take some time to filter through to the general patient.<ref>{{cite web|url=https://www.ijedr.org/papers/IJEDR1504022.pdf|title=Survey on Big Data Using Data Mining|date=2015|publisher=International Journal of Engineering Development and Research|access-date=14 September 2016}}</ref>
* Joining up data: a local authority blended data about services, such as road gritting rotas, with services for people at risk, such as 'meals on wheels'. The connection of data allowed the local authority to avoid any weather related delay.<ref>{{cite web|url=https://www.researchgate.net/publication/297762848_Recent_advances_delivered_by_mobile_cloud_computing_and_Internet_of_Things_for_Big_data_applications_A_Survey|title=Recent advances delivered by Mobile Cloud Computing and Internet of Things for Big Data applications: a survey|date=11 March 2016|publisher=International Journal of Network Management|access-date=14 September 2016}}</ref>

=== International development ===
Research on the effective usage of [[information and communication technologies for development]] (also known as [[ICT4D]]) suggests that big data technology can make important contributions but also present unique challenges to [[International development]].<ref>{{cite web|url=http://www.unglobalpulse.org/projects/BigDataforDevelopment|title=White Paper: Big Data for Development: Opportunities & Challenges (2012) – United Nations Global Pulse|publisher=|accessdate=13 April 2016}}</ref><ref>{{cite web|title=WEF (World Economic Forum), & Vital Wave Consulting. (2012). Big Data, Big Impact: New Possibilities for International Development|work= World Economic Forum|accessdate=24 August 2012|url= http://www.weforum.org/reports/big-data-big-impact-new-possibilities-international-development}}</ref> Advancements in big data analysis offer cost-effective opportunities to improve decision-making in critical development areas such as health care, employment, [[economic productivity]], crime, security, and [[natural disaster]] and resource management.<ref name="HilbertBigData2013" /><ref>{{cite web|url=http://blogs.worldbank.org/ic4d/four-ways-to-talk-about-big-data/|title=Elena Kvochko, Four Ways To talk About Big Data (Information Communication Technologies for Development Series)|publisher=worldbank.org|accessdate=2012-05-30}}</ref><ref>{{cite web|title=Daniele Medri: Big Data & Business: An on-going revolution|url=http://www.statisticsviews.com/details/feature/5393251/Big-Data--Business-An-on-going-revolution.html|publisher=Statistics Views |date=21 October 2013}}</ref> Additionally, user-generated data offers new opportunities to give the unheard a voice.<ref>{{cite web|title=Responsible use of data|author=Tobias Knobloch and Julia Manske|work= D+C, Development and Cooperation|date=11 January 2016|url= http://www.dandc.eu/en/article/opportunities-and-risks-user-generated-and-automatically-compiled-data}}</ref> However, longstanding challenges for developing regions such as inadequate technological infrastructure and economic and human resource scarcity exacerbate existing concerns with big data such as privacy, imperfect methodology, and interoperability issues.<ref name="HilbertBigData2013" />

=== Manufacturing ===
Based on TCS 2013 Global Trend Study, improvements in supply planning and product quality provide the greatest benefit of big data for manufacturing.<ref name="TCS Big Data Study – Manufacturing">{{cite web|url=http://sites.tcs.com/big-data-study/manufacturing-big-data-benefits-challenges/# |title=Manufacturing: Big Data Benefits and Challenges |work= TCS Big Data Study|publisher=[[Tata Consultancy Services Limited]] |location=Mumbai, India |accessdate=2014-06-03}}</ref> Big data provides an infrastructure for transparency in manufacturing industry, which is the ability to unravel uncertainties such as inconsistent component performance and availability. Predictive manufacturing as an applicable approach toward near-zero downtime and transparency requires vast amount of data and advanced prediction tools for a systematic process of data into useful information.<ref>{{cite journal|last=Lee|first=Jay|author2=Wu, F. |author3=Zhao, W. |author4=Ghaffari, M. |author5= Liao, L |title=Prognostics and health management design for rotary machinery systems—Reviews, methodology and applications|journal=Mechanical Systems and Signal Processing|date=January 2013|volume=42|issue=1}}</ref> A conceptual framework of predictive manufacturing begins with data acquisition where different type of sensory data is available to acquire such as acoustics, vibration, pressure, current, voltage and controller data. Vast amount of sensory data in addition to historical data construct the big data in manufacturing. The generated big data acts as the input into predictive tools and preventive strategies such as [[Prognostics]] and Health Management (PHM).<ref>{{cite web|url=https://www.phmsociety.org/events/conference/phm/europe/16/tutorials|title=Tutorials|publisher=PHM Society|accessdate=27 September 2016}}</ref><ref>{{cite web|url=https://www.itri.org.tw/eng/Content/MSGPic01/contents.aspx?&SiteID=1&MmmID=620651706136357202&CatID=620653256103620163&MSID=654532365564567545|title=Prognostic and Health Management Technology for MOCVD Equipment|publisher=Industrial Technology Research Institute|accessdate=27 September 2016}}</ref>

==== Cyber-physical models ====
Current PHM implementations mostly use data during the actual usage while analytical algorithms can perform more accurately when more information throughout the machine's lifecycle, such as system configuration, physical knowledge and working principles, are included. There is a need to systematically integrate, manage and analyze machinery or process data during different stages of machine life cycle to handle data/information more efficiently and further achieve better transparency of machine health condition for manufacturing industry.

With such motivation a cyber-physical (coupled) model scheme has been developed. The coupled model is a digital twin of the real machine that operates in the cloud platform and simulates the health condition with an integrated knowledge from both data driven analytical algorithms as well as other available physical knowledge. It can also be described as a 5S systematic approach consisting of sensing, storage, synchronization, synthesis and service. The coupled model first constructs a digital image from the early design stage. System information and physical knowledge are logged during product design, based on which a simulation model is built as a reference for future analysis. Initial parameters may be statistically generalized and they can be tuned using data from testing or the manufacturing process using parameter estimation. After that step, the simulation model can be considered a mirrored image of the real machine—able to continuously record and track machine condition during the later utilization stage. Finally, with the increased connectivity offered by cloud computing technology, the coupled model also provides better accessibility of machine condition for factory managers in cases where physical access to actual equipment or machine data is limited.<ref name="MfgLetters" />

=== Healthcare ===
Big data analytics has helped healthcare improve by providing personalized medicine and prescriptive analytics, clinical risk intervention and predictive analytics, waste and care variability reduction, automated external and internal reporting of patient data, standardized medical terms and patient registries and fragmented point solutions.<ref name="ref135">{{cite journal|doi=10.1016/j.ijrobp.2015.10.060|title=Impending Challenges for the Use of Big Data }}</ref> Some areas of improvement are more aspirational than actually implemented. The level of data generated within healthcare systems is not trivial. With the added adoption of mHealth, eHealth and wearable technologies the volume of data will continue to increase. This includes electronic health record data, imaging data, patient generated data, sensor data, and other forms of difficult to process data. There is now an even greater need for such environments to pay greater attention to data and information quality.<ref>{{cite journal|url=http://doi.acm.org/10.1145/2378016.2378021|title=Data Management Within mHealth Environments: Patient Sensors, Mobile Devices, and Databases|first1=John|last1=O'Donoghue|first2=John|last2=Herbert|date=1 October 2012|publisher=|volume=4|issue=1|pages=5:1–5:20|accessdate=16 June 2016|via=ACM Digital Library|doi=10.1145/2378016.2378021}}</ref> "Big data very often means `dirty data' and the fraction of data inaccuracies increases with data volume growth." Human inspection at the big data scale is impossible and there is a desperate need in health service for intelligent tools for accuracy and believability control and handling of information missed.<ref name="Mirkes2016">{{cite journal | last1 = Mirkes| first1 =E.M.|last2 = Coats|first2 =T.J.|last3 = Levesley|first3 =J.|last4 = Gorban|first4 = A.N.| title = Handling missing data in large healthcare dataset: A case study of unknown trauma outcomes|url =  https://www.researchgate.net/publication/300400110_Handling_missing_data_in_large_healthcare_dataset_A_case_study_of_unknown_trauma_outcomes| journal = Computers in Biology and Medicine| volume = 75| issue = | pages = 203–216| year = 2016| doi = 10.1016/j.compbiomed.2016.06.004}}</ref> While extensive information in healthcare is now electronic, it fits under the big data umbrella as most is unstructured and difficult to use.<ref>{{Cite journal|last=Murdoch|first=Travis B.|last2=Detsky|first2=Allan S.|date=2013-04-03|title=The Inevitable Application of Big Data to Health Care|url=http://jamanetwork.com/journals/jama/article-abstract/1674245|journal=JAMA|language=en|volume=309|issue=13|doi=10.1001/jama.2013.393|issn=0098-7484}}</ref>

=== Education ===
A [[McKinsey & Company|McKinsey Global Institute]] study found a shortage of 1.5 million highly trained data professionals and managers<ref name="McKinsey"/> and a number of universities<ref>{{cite news
| url=http://www.forbes.com/sites/jmaureenhenderson/2013/07/30/degrees-in-big-data-fad-or-fast-track-to-career-success/?
|access-date=2016-02-21
|newspaper=Forbes
|title=Degrees in Big Data: Fad or Fast Track to Career Success}}</ref> including [[University of Tennessee]] and [[UC Berkeley]], have created masters programs to meet this demand.  Private bootcamps have also developed programs to meet that demand, including free programs like [[The Data Incubator]] or paid programs like [[General Assembly]].<ref>{{cite news
|title=NY gets new bootcamp for data scientists: It’s free, but harder to get into than Harvard
|newspaper=Venture Beat
|access-date=2016-02-21
|url=http://venturebeat.com/2014/04/15/ny-gets-new-bootcamp-for-data-scientists-its-free-but-harder-to-get-into-than-harvard/
}}</ref>

=== Media ===
To understand how the media utilises big data, it is first necessary to provide some context into the mechanism used for media process. It has been suggested by Nick Couldry and Joseph Turow that [[wikt:practitioner|practitioners]] in Media and Advertising approach big data as many actionable points of information about millions of individuals. The industry appears to be moving away from the traditional approach of using specific media environments such as newspapers, magazines, or television shows and instead taps into consumers with technologies that reach targeted people at optimal times in optimal locations.  The ultimate aim is to serve, or convey, a message or content that is (statistically speaking) in line with the consumer's mindset. For example, publishing environments are increasingly tailoring messages (advertisements) and content (articles) to appeal to consumers that have been exclusively gleaned through various [[data-mining]] activities.<ref>{{cite journal|last1=Couldry|first1=Nick|last2=Turow|first2=Joseph|title=Advertising, Big Data, and the Clearance of the Public Realm: Marketers’ New Approaches to the Content Subsidy|journal=International Journal of Communication|date=2014|volume=8|pages=1710–1726}}</ref>
* Targeting of consumers (for advertising by marketers)
* Data-capture
* [[Data journalism]]: publishers and journalists use big data tools to provide unique and innovative insights and infographics.

==== Internet of Things (IoT) ====
{{tone|section|date=September 2016}}

{{Main article|Internet of Things}}
Big data and the IoT work in conjunction.  From a media perspective, data is the key derivative of device inter-connectivity and allows accurate targeting.  The [[Internet of Things]], with the help of big data, therefore transforms the media industry, companies and even governments, opening up a new era of economic growth and competitiveness. The intersection of people, data and intelligent algorithms have far-reaching impacts on media efficiency. The wealth of data generated allows an elaborate layer on the present targeting mechanisms of the industry.

==== Technology ====
* [[eBay.com]] uses two data warehouses at 7.5 [[petabytes]] and 40PB as well as a 40PB [[Hadoop]] cluster for search, consumer recommendations, and merchandising.<ref>{{cite web | last=Tay | first=Liz |url=http://www.itnews.com.au/news/inside-ebay8217s-90pb-data-warehouse-342615 | title=Inside eBay’s 90PB data warehouse | publisher=ITNews | accessdate=2016-02-12}}</ref>
* [[Amazon.com]] handles millions of back-end operations every day, as well as queries from more than half a million third-party sellers. The core technology that keeps Amazon running is Linux-based and as of 2005 they had the world's three largest Linux databases, with capacities of 7.8 TB, 18.5 TB, and 24.7 TB.<ref>{{cite web|last=Layton |first=Julia |url=http://money.howstuffworks.com/amazon1.htm |title=Amazon Technology |publisher=Money.howstuffworks.com |accessdate=2013-03-05}}</ref>
* [[Facebook]] handles 50&nbsp;billion photos from its user base.<ref>{{cite web|url=https://www.facebook.com/notes/facebook-engineering/scaling-facebook-to-500-million-users-and-beyond/409881258919 |title=Scaling Facebook to 500 Million Users and Beyond |publisher=Facebook.com |accessdate=2013-07-21}}</ref>
* As of August 2012, [[Google]] was handling roughly 100&nbsp;billion searches per month.<ref>{{cite web|url=http://searchengineland.com/google-1-trillion-searches-per-year-212940|title=Google Still Doing at Least 1 Trillion Searches Per Year|date=16 January 2015|work=Search Engine Land|accessdate=15 April 2015}}</ref>
* [[Oracle NoSQL Database]] has been tested to past the 1M ops/sec mark with 8 shards and proceeded to hit 1.2M ops/sec with 10 shards.<ref>{{cite web |last=Lamb |first=Charles |url=https://blogs.oracle.com/charlesLamb/entry/oracle_nosql_database_exceeds_1 |title=Oracle NoSQL Database Exceeds 1 Million Mixed YCSB Ops/Sec}}</ref>

=== Private sector ===

=== Information Technology ===
Especially since 2015, big data has come to prominence within [[Business Operations]] as a tool to help employees work more efficiently and streamline the collection and distribution of [[Information Technology]] (IT). The use of big data to attack IT and data collection issues within an enterprise is called [[IT Operations Analytics]] (ITOA).<ref name="ITOA1">{{cite web|last1=Solnik|first1=Ray|title=The Time Has Come: Analytics Delivers for IT Operations|url=http://www.datacenterjournal.com/time-analytics-delivers-operations/|website=Data Center Journal|accessdate=June 21, 2016}}</ref> By applying big data principles into the concepts of [[machine intelligence]] and [[deep computing]], IT departments can predict potential issues and move to provide solutions before the problems even happen.<ref name="ITOA1" /> In this time, ITOA businesses were also beginning to play a major role in [[systems management]] by offering platforms that brought individual [[data silos]] together and generated insights from the whole of the system rather than from isolated pockets of data.

==== Retail ====
* [[Walmart]] handles more than 1 million customer transactions every hour, which are imported into databases estimated to contain more than 2.5 petabytes (2560 terabytes) of data—the equivalent of 167 times the information contained in all the books in the US [[Library of Congress]].{{r|Economist}}

==== Retail banking ====
* FICO Card Detection System protects accounts worldwide.<ref name="fico.com">{{cite web|url=http://www.fico.com/en/Products/DMApps/Pages/FICO-Falcon-Fraud-Manager.aspx |title=FICO® Falcon® Fraud Manager |publisher=Fico.com |accessdate=2013-07-21}}</ref>
* The volume of business data worldwide, across all companies, doubles every 1.2 years, according to estimates.<ref name="KnowWPCarey.com">{{cite web|url=http://research.wpcarey.asu.edu/managing-it/ebay-study-how-to-build-trust-and-improve-the-shopping-experience |title=eBay Study: How to Build Trust and Improve the Shopping Experience |publisher=Knowwpcarey.com |date=8 May 2012 |accessdate=2015-12-20}}</ref><ref>[http://www.statista.com/statistics/280444/global-leading-priorities-for-big-data-according-to-business-and-it-executives/ Leading Priorities for Big Data for Business and IT]. eMarketer. October 2013. Retrieved January 2014.</ref>

==== Real estate ====
* [[Windermere Real Estate]] uses anonymous GPS signals from nearly 100 million drivers to help new home buyers determine their typical drive times to and from work throughout various times of the day.<ref>{{cite news|last=Wingfield |first=Nick |url=http://bits.blogs.nytimes.com/2013/03/12/predicting-commutes-more-accurately-for-would-be-home-buyers/ |title=Predicting Commutes More Accurately for Would-Be Home Buyers – NYTimes.com |publisher=Bits.blogs.nytimes.com |date=12 March 2013 |accessdate=2013-07-21}}</ref>

=== Science ===
The [[Large Hadron Collider]] experiments represent about 150 million sensors delivering data 40&nbsp;million times per second. There are nearly 600&nbsp;million collisions per second. After filtering and refraining from recording more than 99.99995%<ref>{{cite web|last1=Alexandru|first1=Dan|title=Prof|url=https://cds.cern.ch/record/1504817/files/CERN-THESIS-2013-004.pdf|website=cds.cern.ch|publisher=CERN|accessdate=24 March 2015}}</ref> of these streams, there are 100 collisions of interest per second.<ref>{{cite web |title=LHC Brochure, English version. A presentation of the largest and the most powerful particle accelerator in the world, the Large Hadron Collider (LHC), which started up in 2008. Its role, characteristics, technologies, etc. are explained for the general public. |url=http://cds.cern.ch/record/1278169?ln=en |work=CERN-Brochure-2010-006-Eng. LHC Brochure, English version. |publisher=CERN |accessdate=20 January 2013}}</ref><ref>{{cite web |title=LHC Guide, English version. A collection of facts and figures about the Large Hadron Collider (LHC) in the form of questions and answers. |url=http://cds.cern.ch/record/1092437?ln=en |work=CERN-Brochure-2008-001-Eng. LHC Guide, English version. |publisher=CERN |accessdate=20 January 2013}}</ref><ref name="nature">{{cite news |title=High-energy physics: Down the petabyte highway |work= Nature |date= 19 January 2011 |first=Geoff |last=Brumfiel |doi= 10.1038/469282a |volume= 469 |pages= 282–83 |url= http://www.nature.com/news/2011/110119/full/469282a.html }}</ref>
* As a result, only working with less than 0.001% of the sensor stream data, the data flow from all four LHC experiments represents 25 petabytes annual rate before replication (as of 2012). This becomes nearly 200 petabytes after replication.
* If all sensor data were recorded in LHC, the data flow would be extremely hard to work with. The data flow would exceed 150 million petabytes annual rate, or nearly 500 [[exabyte]]s per day, before replication. To put the number in perspective, this is equivalent to 500 [[quintillion]] (5×10<sup>20</sup>) bytes per day, almost 200 times more than all the other sources combined in the world.

The [[Square Kilometre Array]] is a radio telescope built of thousands of antennas. It is expected to be operational by 2024. Collectively, these antennas are expected to gather 14 exabytes and store one petabyte per day.<ref>http://www.zurich.ibm.com/pdf/astron/CeBIT%202013%20Background%20DOME.pdf</ref><ref>{{cite web|url=http://arstechnica.com/science/2012/04/future-telescope-array-drives-development-of-exabyte-processing/|title=Future telescope array drives development of exabyte processing|work=Ars Technica|accessdate=15 April 2015}}</ref> It is considered one of the most ambitious scientific projects ever undertaken.<ref>{{cite web|url=http://theconversation.com/australias-bid-for-the-square-kilometre-array-an-insiders-perspective-4891|title=Australia’s bid for the Square Kilometre Array – an insider’s perspective|date=1 February 2012|publisher=[[The Conversation (website)|The Conversation]]|accessdate=27 September 2016}}</ref>

==== Science and research ====
{{Expand section|date=December 2016}}
* When the [[Sloan Digital Sky Survey]] (SDSS) began to collect astronomical data in 2000, it amassed more in its first few weeks than all data collected in the history of astronomy previously. Continuing at a rate of about 200&nbsp;GB per night, SDSS has amassed more than 140 terabytes of information.<ref name="Economist">{{cite news |title=Data, data everywhere |url=http://www.economist.com/node/15557443 |newspaper=The Economist |date=25 February 2010 |accessdate=9 December 2012}}</ref> When the [[Large Synoptic Survey Telescope]], successor to SDSS, comes online in 2020, its designers expect it to acquire that amount of data every five days.{{r|Economist}}
* Decoding the [[Human Genome Project|human genome]] originally took 10 years to process, now it can be achieved in less than a day. The DNA sequencers have divided the sequencing cost by 10,000 in the last ten years, which is 100 times cheaper than the reduction in cost predicted by [[Moore's Law]].<ref>[http://www.oecd.org/sti/ieconomy/Session_3_Delort.pdf#page=6 Delort P., OECD ICCP Technology Foresight Forum, 2012.]</ref>
* The [[NASA]] Center for Climate Simulation (NCCS) stores 32 petabytes of climate observations and simulations on the Discover supercomputing cluster.<ref>{{cite web|url=http://www.nasa.gov/centers/goddard/news/releases/2010/10-051.html|title=NASA – NASA Goddard Introduces the NASA Center for Climate Simulation|publisher=|accessdate=13 April 2016}}</ref><ref>{{cite web|last=Webster|first=Phil|title=Supercomputing the Climate: NASA's Big Data Mission|url=http://www.csc.com/cscworld/publications/81769/81773-supercomputing_the_climate_nasa_s_big_data_mission|work=CSC World|publisher=Computer Sciences Corporation|accessdate=2013-01-18}}</ref>
* Google's DNAStack compiles and organizes DNA samples of genetic data from around the world to identify diseases and other medical defects. These fast and exact calculations eliminate any 'friction points,' or human errors that could be made by one of the numerous science and biology experts working with the DNA. DNAStack, a part of Google Genomics, allows scientists to use the vast sample of resources from Google's search server to scale social experiments that would usually take years, instantly.<ref>{{cite web|url=http://www.theglobeandmail.com/life/health-and-fitness/health/these-six-great-neuroscience-ideas-could-make-the-leap-from-lab-to-market/article21681731/|title=These six great neuroscience ideas could make the leap from lab to market|date=20 November 2014|publisher=[[The Globe and Mail]]|accessdate=1 October 2016}}</ref><ref>{{cite web|url=https://cloud.google.com/customers/dnastack/|title=DNAstack tackles massive, complex DNA datasets with Google Genomics|publisher=Google Cloud Platform |accessdate=1 October 2016}}</ref>
* [[23andme]]'s [[DNA database]] contains genetic information of over 1,000,000 people worldwide.<ref>{{cite web|title=23andMe - Ancestry|url=https://www.23andme.com/en-int/ancestry/|website=23andme.com|accessdate=29 December 2016}}</ref> The company explores selling the "anonymous aggregated genetic data" to other researchers and pharmaceutical companies for research purposes if patients give their consent.<ref name=verge1>{{cite web|last1=Potenza|first1=Alessandra|title=23andMe wants researchers to use its kits, in a bid to expand its collection of genetic data|url=http://www.theverge.com/2016/7/13/12166960/23andme-genetic-testing-database-genotyping-research|publisher=The Verge|accessdate=29 December 2016|date=13 July 2016}}</ref><ref>{{cite web|title=This Startup Will Sequence Your DNA, So You Can Contribute To Medical Research|url=https://www.fastcompany.com/3066775/innovation-agents/this-startup-will-sequence-your-dna-so-you-can-contribute-to-medical-resea|publisher=Fast Company|accessdate=29 December 2016|date=23 December 2016}}</ref><ref>{{cite web|last1=Seife|first1=Charles|title=23andMe Is Terrifying, but Not for the Reasons the FDA Thinks|url=https://www.scientificamerican.com/article/23andme-is-terrifying-but-not-for-the-reasons-the-fda-thinks/|publisher=Scientific American|accessdate=29 December 2016}}</ref><ref>{{cite web|last1=Zaleski|first1=Andrew|title=This biotech start-up is betting your genes will yield the next wonder drug|url=http://www.cnbc.com/2016/06/22/23andme-thinks-your-genes-are-the-key-to-blockbuster-drugs.html|publisher=CNBC|accessdate=29 December 2016|date=22 June 2016}}</ref><ref>{{cite web|last1=Regalado|first1=Antonio|title=How 23andMe turned your DNA into a $1 billion drug discovery machine|url=https://www.technologyreview.com/s/601506/23andme-sells-data-for-drug-search/|publisher=MIT Technology Review|accessdate=29 December 2016}}</ref> Ahmad Hariri, professor of psychology and neuroscience at [[Duke University]] who has been using 23andMe in his research since 2009 states that the most important aspect of the company's new service is that it makes genetic research accessible and relatively cheap for scientists.<ref name=verge1/> A study that identified 15 genome sites linked to depression in 23andMe's database lead to a surge in demands to access the repository with 23andMe fielding nearly 20 requests to access the depression data in the two weeks after publication of the paper.<ref>{{cite web|title=23andMe reports jump in requests for data in wake of Pfizer depression study {{!}} FierceBiotech|url=http://www.fiercebiotech.com/it/23andme-reports-jump-requests-for-data-wake-pfizer-depression-study|website=fiercebiotech.com|accessdate=29 December 2016}}</ref>

=== Sports ===
Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.<ref>{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&view=article&id=147241|title=Data scientists predict Springbok defeat
|author=Admire Moyo|work=www.itweb.co.za|accessdate=12 December 2015}}</ref>
Future performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.<ref>{{cite web|url=http://www.itweb.co.za/index.php?option=com_content&view=article&id=147852|title= Predictive analytics, big data transform sports
 |author=Regina Pazvakavambwa|work=www.itweb.co.za|accessdate=12 December 2015}}</ref>

The movie [[Moneyball (film)|''MoneyBall'']] demonstrates how big data could be used to scout players and also identify undervalued players.<ref>{{cite web|url=http://www.datacenterknowledge.com/archives/2011/09/23/the-lessons-of-moneyball-for-big-data-analysis/|title= The Lessons of Moneyball for Big Data Analysis|author=Rich Miller|work=www.datecenterknowledge.com|accessdate=12 December 2015}}</ref>

In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency. Then, this data is transferred to team headquarters in United Kingdom through fiber optic cables that could carry data at the speed of light.<ref>{{cite web|url=http://www.huffingtonpost.com/dave-ryan/sports-where-big-data-fin_b_8553884.html|title= Sports: Where Big Data Finally Makes Sense |author=Dave Ryan|work=www.huffingtonpost.com|accessdate=12 December 2015}}</ref>
Based on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.<ref>{{cite web|url=http://www.forbes.com/sites/frankbi/2014/11/13/how-formula-one-teams-are-using-big-data-to-get-the-inside-edge//|title= How Formula One Teams Are Using Big Data To Get The Inside Edge|author=Frank Bi|work=www.forbes.com|accessdate=12 December 2015}}</ref>

== Research activities ==
Encrypted search and cluster formation in big data was demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at ''Tackling the challenges of Big Data'' by [[MIT Computer Science and Artificial Intelligence Laboratory]] and Dr. Amir Esmailpour at UNH Research Group investigated the key features of big data as formation of clusters and their interconnections. They focused on the security of big data and the actual orientation of the term towards the presence of different type of data in an encrypted form at cloud interface by providing the raw definitions and real time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.<ref>{{cite conference |url=http://asee-ne.org/proceedings/2014/Student%20Papers/210.pdf |title=Encrypted Search & Cluster Formation in Big Data |last1=Siwach |first1=Gautam |last2=Esmailpour |first2=Amir |date=March 2014 |year= |conference=ASEE 2014 Zone I Conference |conference-url=http://ubconferences.org/ |location=[[University of Bridgeport]], [[Bridgeport, Connecticut]], US }}</ref>

In March 2012, The White House announced a national "Big Data Initiative" that consisted of six Federal departments and agencies committing more than $200&nbsp;million to big data research projects.<ref>{{cite web |title=Obama Administration Unveils "Big Data" Initiative:Announces $200 Million In New R&D Investments|publisher=The White House |url=http://www.whitehouse.gov/sites/default/files/microsites/ostp/big_data_press_release_final_2.pdf}}</ref>

The initiative included a National Science Foundation "Expeditions in Computing" grant of $10 million over 5 years to the AMPLab<ref>{{cite web|url=http://amplab.cs.berkeley.edu |title=AMPLab at the University of California, Berkeley |publisher=Amplab.cs.berkeley.edu |accessdate=2013-03-05}}</ref> at the University of California, Berkeley.<ref>{{cite web |title=NSF Leads Federal Efforts in Big Data|date=29 March 2012|publisher=National Science Foundation (NSF) |url=http://www.nsf.gov/news/news_summ.jsp?cntn_id=123607&org=NSF&from=news}}</ref> The AMPLab also received funds from [[DARPA]], and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion<ref>{{cite conference|url=https://amplab.cs.berkeley.edu/publication/scaling-the-mobile-millennium-system-in-the-cloud-2/|author1=Timothy Hunter|date=October 2011|author2=Teodor Moldovan|author3=Matei Zaharia|author4=Justin Ma|author5=Michael Franklin|author6=Pieter Abbeel|author7=Alexandre Bayen|title=Scaling the Mobile Millennium System in the Cloud}}</ref> to fighting cancer.<ref>{{cite news|title=Computer Scientists May Have What It Takes to Help Cure Cancer|author=David Patterson|publisher=The New York Times|date=5 December 2011|url=http://www.nytimes.com/2011/12/06/science/david-patterson-enlist-computer-scientists-in-cancer-fight.html?_r=0}}</ref>

The White House Big Data Initiative also included a commitment by the  Department of Energy to provide $25 million in funding over 5 years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute,<ref>{{cite web|title=Secretary Chu Announces New Institute to Help Scientists Improve Massive Data Set Research on DOE Supercomputers |publisher="energy.gov" |url=http://energy.gov/articles/secretary-chu-announces-new-institute-help-scientists-improve-massive-data-set-research-doe}}</ref> led by the Energy Department’s [[Lawrence Berkeley National Laboratory]]. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the Department's supercomputers.

The U.S. state of [[Massachusetts]] announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions.<ref>{{cite web |title=Governor Patrick announces new initiative to strengthen Massachusetts' position as a World leader in Big Data |publisher=Commonwealth of Massachusetts |url=http://www.mass.gov/governor/pressoffice/pressreleases/2012/2012530-governor-announces-big-data-initiative.html}}</ref>  The [[Massachusetts Institute of Technology]] hosts the Intel Science and Technology Center for Big Data in the [[MIT Computer Science and Artificial Intelligence Laboratory]], combining government, corporate, and institutional funding and research efforts.<ref>{{cite web|url=http://bigdata.csail.mit.edu/ |title=Big Data @ CSAIL |publisher=Bigdata.csail.mit.edu |date=22 February 2013 |accessdate=2013-03-05}}</ref>

The European Commission is funding the 2-year-long Big Data Public Private Forum through their [[Seventh Framework Program]] to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for [[Horizon 2020]], their next [[Framework Programmes for Research and Technological Development|framework program]].<ref>{{cite web|url=http://cordis.europa.eu/search/index.cfm?fuseaction=proj.document&PJ_RCN=13267529 |title=Big Data Public Private Forum |publisher=Cordis.europa.eu |date=1 September 2012 |accessdate=2013-03-05}}</ref>

The British government announced in March 2014 the founding of the [[Alan Turing Institute]], named after the computer pioneer and code-breaker, which will focus on new ways to collect and analyse large data sets.<ref>{{cite news|url=http://www.bbc.co.uk/news/technology-26651179|title=Alan Turing Institute to be set up to research big data|publisher=[[BBC News]]|accessdate=2014-03-19|date=19 March 2014}}</ref>

At the [[University of Waterloo Stratford Campus]] Canadian Open Data Experience (CODE) Inspiration Day, participants demonstrated how using data visualization can increase the understanding and appeal of big data sets and communicate their story to the world.<ref>{{cite web|url=http://www.betakit.com/event/inspiration-day-at-university-of-waterloo-stratford-campus/|title=Inspiration day at University of Waterloo, Stratford Campus |publisher=betakit.com/|accessdate=2014-02-28}}</ref>

To make manufacturing more competitive in the United States (and globe), there is a need to integrate more American ingenuity and innovation into manufacturing ; Therefore, National Science Foundation has granted the Industry University cooperative research center for Intelligent Maintenance Systems (IMS) at [[university of Cincinnati]] to focus on developing advanced predictive tools and techniques to be applicable in a big data environment.<ref>{{cite journal|last=Lee|first=Jay|author2=Lapira, Edzel |author3=Bagheri, Behrad |author4= Kao, Hung-An |title=Recent Advances and Trends in Predictive Manufacturing Systems in Big Data Environment|journal=Manufacturing Letters|year=2013|volume=1|issue=1|url=http://www.sciencedirect.com/science/article/pii/S2213846313000114|DOI=10.1016/j.mfglet.2013.09.005 |pages=38–41}}</ref> In May 2013, IMS Center held an industry advisory board meeting focusing on big data where presenters from various industrial companies discussed their concerns, issues and future goals in big data environment.

Computational social sciences&nbsp;– Anyone can use Application Programming Interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences.<ref name=pigdata>{{cite journal|last=Reips|first=Ulf-Dietrich|author2=Matzat, Uwe |title=Mining "Big Data" using Big Data Services |journal=International Journal of Internet Science|year=2014|volume=1|issue=1|pages=1–8 | url=http://www.ijis.net/ijis9_1/ijis9_1_editorial_pre.html}}</ref> Often these APIs are provided for free.<ref name="pigdata" /> [[Tobias Preis]] ''et al.'' used [[Google Trends]] data to demonstrate that Internet users from countries with a higher per capita gross domestic product (GDP) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators.<ref>{{cite journal |first1=Tobias |last1=Preis |first2=Helen Susannah |last2=Moat, |first3=H. Eugene |last3=Stanley |first4=Steven R. |last4=Bishop |title=Quantifying the Advantage of Looking Forward |journal=Scientific Reports |volume= 2 |page=350 |year=2012 |doi=10.1038/srep00350 |pmid=22482034 |pmc=3320057}}</ref><ref>{{cite web | url=http://www.newscientist.com/article/dn21678-online-searches-for-future-linked-to-economic-success.html | title=Online searches for future linked to economic success |first=Paul |last=Marks |work=New Scientist | date=5 April 2012 | accessdate=9 April 2012}}</ref><ref>{{cite web | url=http://arstechnica.com/gadgets/news/2012/04/google-trends-reveals-clues-about-the-mentality-of-richer-nations.ars | title=Google Trends reveals clues about the mentality of richer nations |first=Casey |last=Johnston |work=Ars Technica | date=6 April 2012 | accessdate=9 April 2012}}</ref> The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year ('2011') to the volume of searches for the previous year ('2009'), which they call the '[[future orientation index]]'.<ref>{{cite web | url = http://www.tobiaspreis.de/bigdata/future_orientation_index.pdf | title = Supplementary Information: The Future Orientation Index is available for download | author = Tobias Preis | date = 24 May 2012 | accessdate = 2012-05-24}}</ref> They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. The results hint that there may potentially be a relationship between the economic success of a country and the information-seeking behavior of its citizens captured in big data.

[[Tobias Preis]] and his colleagues [[Helen Susannah Moat]] and [[H. Eugene Stanley]] introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends.<ref>{{cite web | url=http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=[[Philip Ball]] | work=Nature | date=26 April 2013 | accessdate=9 August 2013}}</ref> Their analysis of [[Google]] search volume for 98 terms of varying financial relevance, published in ''[[Scientific Reports]]'',<ref>{{cite journal | author=Tobias Preis, Helen Susannah Moat and H. Eugene Stanley | title=Quantifying Trading Behavior in Financial Markets Using Google Trends | journal=[[Scientific Reports]] | volume= 3 | pages=1684 | year=2013 | doi=10.1038/srep01684 | pmid=23619126 | pmc=3635219}}</ref> suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.<ref>{{cite news | url=http://bits.blogs.nytimes.com/2013/04/26/google-search-terms-can-predict-stock-market-study-finds/ | title= Google Search Terms Can Predict Stock Market, Study Finds | author=Nick Bilton | work=[[New York Times]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://business.time.com/2013/04/26/trouble-with-your-investment-portfolio-google-it/ | title=Trouble With Your Investment Portfolio? Google It! | author=Christopher Matthews | work=[[TIME Magazine]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite web | url= http://www.nature.com/news/counting-google-searches-predicts-market-movements-1.12879 | title=Counting Google searches predicts market movements | author=Philip Ball |work=[[Nature (journal)|Nature]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite web | url=http://www.businessweek.com/articles/2013-04-25/big-data-researchers-turn-to-google-to-beat-the-markets | title='Big Data' Researchers Turn to Google to Beat the Markets | author=Bernhard Warner | work=[[Bloomberg Businessweek]] | date=25 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://www.independent.co.uk/news/business/comment/hamish-mcrae/hamish-mcrae-need-a-valuable-handle-on-investor-sentiment-google-it-8590991.html | title=Hamish McRae: Need a valuable handle on investor sentiment? Google it | author=Hamish McRae | work=[[The Independent]] | date=28 April 2013 | accessdate=9 August 2013 | location=London}}</ref><ref>{{cite web | url=http://www.ft.com/intl/cms/s/0/e5d959b8-acf2-11e2-b27f-00144feabdc0.html | title= Google search proves to be new word in stock market prediction | author=Richard Waters | work=[[Financial Times]] | date=25 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://www.forbes.com/sites/davidleinweber/2013/04/26/big-data-gets-bigger-now-google-trends-can-predict-the-market/ | title=Big Data Gets Bigger: Now Google Trends Can Predict The Market | author=David Leinweber | work=[[Forbes]] | date=26 April 2013 | accessdate=9 August 2013}}</ref><ref>{{cite news | url=http://www.bbc.co.uk/news/science-environment-22293693 | title=Google searches predict market moves | author=Jason Palmer | work=[[BBC]] | date=25 April 2013 | accessdate=9 August 2013}}</ref>

Big data sets come with algorithmic challenges that previously did not exist. Hence, there is a need to fundamentally change the processing ways.<ref>E. Sejdić, "Adapt current tools for use with big data," ''Nature,'' vol. vol. 507, no. 7492, pp. 306, Mar. 2014.</ref>

The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data.<ref>
Stanford.
[http://web.stanford.edu/group/mmds/ "MMDS. Workshop on Algorithms for Modern Massive Data Sets"].
</ref>

=== Sampling big data ===
An important research question that can be asked about big data sets is whether you need to look at the full data to draw certain conclusions about the properties of the data or is a sample good enough. The name big data itself contains a term related to size and this is an important characteristic of big data. But [[Sampling (statistics)]] enables the selection of right data points from within the larger data set to estimate the characteristics of the whole population. For example, there are about 600 million tweets produced every day. Is it necessary to look at all of them to determine the topics that are discussed during the day? Is it necessary to look at all the tweets to determine the sentiment on each of the topics? In manufacturing different types of sensory data such as acoustics, vibration, pressure, current, voltage and controller data are available at short time intervals. To predict down-time it may not be necessary to look at all the data but a sample may be sufficient.  Big Data can be broken down by various data point categories such as demographic, psychographic, behavioral, and transactional data.  With large sets of data points, marketers are able to create and utilize more customized segments of consumers for more strategic targeting.

There has been some work done in Sampling algorithms for big data. A theoretical formulation for sampling Twitter data has been developed.<ref>{{cite conference |author1=Deepan Palguna |author2=Vikas Joshi |author3=Venkatesan Chakaravarthy |author4=Ravi Kothari |author5=L. V. Subramaniam |last-author-amp=yes | title=Analysis of Sampling Algorithms for Twitter | journal=[[International Joint Conference on Artificial Intelligence]] | year=2015 }}</ref>

== Critique ==
Critiques of the big data paradigm come in two flavors, those that question the implications of the approach itself, and those that question the way it is currently done.<ref>{{cite journal | doi = 10.1002/joe.21642 | title = Big Data and Business Intelligence: Debunking the Myths | journal = Global Business and Organizational Excellence| volume = 35 | issue = 1 | pages = 23–34 | year = 2015 | last1 = Kimble | first1 = C. | last2 = Milolidakis | first2 = G. }}</ref> One approach to this criticism is the field of [[Critical data studies]].

=== Critiques of the big data paradigm ===
"A crucial problem is that we do not know much about the underlying empirical micro-processes that lead to the emergence of the[se] typical network characteristics of Big Data".<ref name="Editorial" /> In their critique, Snijders, Matzat, and [[Ulf-Dietrich Reips|Reips]] point out that often very strong assumptions are made about mathematical properties that may not at all reflect what is really going on at the level of micro-processes. Mark Graham has leveled broad critiques at [[Chris Anderson (writer)|Chris Anderson]]'s assertion that big data will spell the end of theory:<ref>{{cite web|url=http://www.wired.com/science/discoveries/magazine/16-07/pb_theory|title=The End of Theory: The Data Deluge Makes the Scientific Method Obsolete|author=Chris Anderson|date=23 June 2008|work=WIRED}}</ref> focusing in particular on the notion that big data must always be contextualized in their social, economic, and political contexts.<ref>{{cite news |author=Graham M. |title=Big data and the end of theory? |newspaper=The Guardian |url=https://www.theguardian.com/news/datablog/2012/mar/09/big-data-theory |location=London |date=9 March 2012}}</ref> Even as companies invest eight- and nine-figure sums to derive insight from information streaming in from suppliers and customers, less than 40% of employees have sufficiently mature processes and skills to do so. To overcome this insight deficit, big data, no matter how comprehensive or well analysed, must be complemented by "big judgment," according to an article in the Harvard Business Review.<ref>{{cite web|title=Good Data Won't Guarantee Good Decisions. Harvard Business Review|url=http://hbr.org/2012/04/good-data-wont-guarantee-good-decisions/ar/1|work=Shah, Shvetank; Horne, Andrew; Capellá, Jaime;|publisher=HBR.org|accessdate=8 September 2012}}</ref>

Much in the same line, it has been pointed out that the decisions based on the analysis of big data are inevitably "informed by the world as it was in the past, or, at best, as it currently is".<ref name="HilbertBigData2013">{{cite web|url=http://papers.ssrn.com/abstract=2205145|title=Big Data for Development: From Information- to Knowledge Societies|publisher=}}</ref>  Fed by a large number of data on past experiences, algorithms can predict future development if the future is similar to the past.<ref name="HilbertTEDx">[https://www.youtube.com/watch?v=UXef6yfJZAI Big Data requires Big Visions for Big Change.], Hilbert, M. (2014). London: TEDxUCL, x=independently organized TED talks</ref> If the systems dynamics of the future change (if it is not a [[stationary process]]), the past can say little about the future. In order to make predictions in changing environments, it would be necessary to have a thorough understanding of the systems dynamic, which requires theory.<ref name="HilbertTEDx"/>  As a response to this critique it has been suggested to combine big data approaches with computer simulations, such as [[agent-based model]]s<ref name="HilbertBigData2013" /> and [[Complex Systems]]. Agent-based models are increasingly getting better in predicting the outcome of social complexities of even unknown future scenarios through computer simulations that are based on a collection of mutually interdependent algorithms.<ref>{{cite web|url=http://www.theatlantic.com/magazine/archive/2002/04/seeing-around-corners/302471/|title=Seeing Around Corners|author=Jonathan Rauch|date=1 April 2002|work=The Atlantic}}</ref><ref>Epstein, J. M., & Axtell, R. L. (1996). Growing Artificial Societies: Social Science from the Bottom Up. A Bradford Book.</ref> In addition, use of multivariate methods that probe for the latent structure of the data, such as [[factor analysis]] and [[cluster analysis]], have proven useful as analytic approaches that go well beyond the bi-variate approaches (cross-tabs) typically employed with smaller data sets.

In health and biology, conventional scientific approaches are based on experimentation. For these approaches, the limiting factor is the relevant data that can confirm or refute the initial hypothesis.<ref>[http://www.bigdataparis.com/documents/Pierre-Delort-INSERM.pdf#page=5 Delort P., Big data in Biosciences, Big Data Paris, 2012]</ref>
A new postulate is accepted now in biosciences: the information provided by the data in huge volumes ([[omics]]) without prior hypothesis is complementary and sometimes necessary to conventional approaches based on experimentation.<ref>{{cite web|url=http://www.cs.cmu.edu/~durand/03-711/2011/Literature/Next-Gen-Genomics-NRG-2010.pdf|title=Next-generation genomics: an integrative approach|date=July 2010|publisher=nature|accessdate=18 October 2016}}</ref><ref>{{cite web|url=https://www.researchgate.net/publication/283298499_BIG_DATA_IN_BIOSCIENCES|title=BIG DATA IN BIOSCIENCES|date=October 2015|publisher=ResearchGate|accessdate=18 October 2016}}</ref> In the massive approaches it is the formulation of a relevant hypothesis to explain the data that is the limiting factor.<ref>{{cite web|url=https://next.ft.com/content/21a6e7d8-b479-11e3-a09a-00144feabdc0|title=Big data: are we making a big mistake?|date=28 March 2014|publisher=Financial Times|accessdate=20 October 2016}}</ref> The search logic is reversed and the limits of induction ("Glory of Science and Philosophy scandal", [[C. D. Broad]], 1926) are to be considered.{{Citation needed|date=April 2015}}

[[Consumer privacy|Privacy]] advocates are concerned about the threat to privacy represented by increasing storage and integration of [[personally identifiable information]]; expert panels have released various policy recommendations to conform practice to expectations of privacy.<ref>{{cite web |first=Paul |last=Ohm |title=Don't Build a Database of Ruin |publisher=Harvard Business Review |url=http://blogs.hbr.org/cs/2012/08/dont_build_a_database_of_ruin.html}}</ref><ref>Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/12/03/iron-cagebook/ Iron Cagebook – The Logical End of Facebook's Patents],'' [[Counterpunch.org]], 2013.12.03</ref><ref>Darwin Bond-Graham, ''[http://www.counterpunch.org/2013/09/11/inside-the-tech-industrys-startup-conference/  Inside the Tech industry’s Startup Conference],'' [[Counterpunch.org]], 2013.09.11</ref>

=== Critiques of big data execution ===
Big data has been called a "fad" in scientific research and its use was even made fun of as an absurd practice in a satirical example on "pig data".<ref name="pigdata" /> Researcher [[Danah Boyd]] has raised concerns about the use of big data in science neglecting principles such as choosing a [[Sampling (statistics)|representative sample]] by being too concerned about actually handling the huge amounts of data.<ref name="danah">{{cite web | url=http://www.danah.org/papers/talks/2010/WWW2010.html | title=Privacy and Publicity in the Context of Big Data | author=[[danah boyd]] | work=[[World Wide Web Conference|WWW 2010 conference]] | date=29 April 2010 | accessdate = 2011-04-18}}</ref> This approach may lead to results [[Bias (statistics)|bias]] in one way or another. Integration across heterogeneous data resources—some that might be considered big data and others not—presents formidable logistical as well as analytical challenges, but many researchers argue that such integrations are likely to represent the most promising new frontiers in science.<ref>{{cite journal |last1=Jones |first1=MB |last2=Schildhauer |first2=MP |last3=Reichman |first3=OJ |last4=Bowers |first4=S |title=The New Bioinformatics: Integrating Ecological Data from the Gene to the Biosphere |journal=Annual Review of Ecology, Evolution, and Systematics |volume=37 |issue=1 |pages=519–544 |year=2006 |doi=10.1146/annurev.ecolsys.37.091305.110031 |url=http://www.pnamp.org/sites/default/files/Jones2006_AREES.pdf |format=PDF}}</ref>
In the provocative article "Critical Questions for Big Data",<ref name="danah2">{{cite journal | doi = 10.1080/1369118X.2012.678878| title = Critical Questions for Big Data| journal = Information, Communication & Society| volume = 15| issue = 5| pages = 662–679| year = 2012| last1 = Boyd | first1 = D. | last2 = Crawford | first2 = K. }}</ref> the authors title big data a part of [[mythology]]: "large data sets offer a higher form of intelligence and knowledge [...], with the aura of truth, objectivity, and accuracy". Users of big data are often "lost in the sheer volume of numbers", and "working with Big Data is still subjective, and what it quantifies does not necessarily have a closer claim on objective truth".<ref name="danah2" /> Recent developments in BI domain, such as pro-active reporting especially target improvements in usability of big data, through automated [[Filter (software)|filtering]] of non-useful data and correlations.<ref name="Big Decisions White Paper">[http://www.fortewares.com/Administrator/userfiles/Banner/forte-wares--pro-active-reporting_EN.pdf Failure to Launch: From Big Data to Big Decisions], Forte Wares.</ref>

Big data analysis is often shallow compared to analysis of smaller data sets.<ref name="kdnuggets-berchthold">{{cite web|url=http://www.kdnuggets.com/2014/08/interview-michael-berthold-knime-research-big-data-privacy-part2.html|title=Interview: Michael Berthold, KNIME Founder, on Research, Creativity, Big Data, and Privacy, Part 2|date=12 August 2014|author=Gregory Piatetsky|authorlink=Gregory I. Piatetsky-Shapiro|publisher=KDnuggets|accessdate=2014-08-13}}</ref> In many big data projects, there is no large data analysis happening, but the challenge is the [[extract, transform, load]] part of data preprocessing.<ref name="kdnuggets-berchthold" />

Big data is a [[buzzword]] and a "vague term",<ref>{{cite web|last1=Pelt|first1=Mason|title="Big Data" is an over used buzzword and this Twitter bot proves it|url=http://siliconangle.com/blog/2015/10/26/big-data-is-an-over-used-buzzword-and-this-twitter-bot-proves-it/|website=siliconangle.com|publisher=SiliconANGLE|accessdate=4 November 2015}}</ref><ref name="ft-harford">{{cite web |url=http://www.ft.com/cms/s/2/21a6e7d8-b479-11e3-a09a-00144feabdc0.html |title=Big data: are we making a big mistake? |last1=Harford |first1=Tim |date=28 March 2014 |website=[[Financial Times]] |publisher=[[Financial Times]] |accessdate=2014-04-07}}</ref> but at the same time an "obsession"<ref name="ft-harford" /> with entrepreneurs, consultants, scientists and the media. Big data showcases such as [[Google Flu Trends]] failed to deliver good predictions in recent years, overstating the flu outbreaks by a factor of two. Similarly, [[Academy awards]] and election predictions solely based on Twitter were more often off than on target.
Big data often poses the same challenges as small data; and adding more data does not solve problems of bias, but may emphasize other problems. In particular data sources such as Twitter are not representative of the overall population, and results drawn from such sources may then lead to wrong conclusions. [[Google Translate]]—which is based on big data statistical analysis of text—does a good job at translating web pages. However, results from specialized domains may be dramatically skewed.
On the other hand, big data may also introduce new problems, such as the [[multiple comparisons problem]]: simultaneously testing a large set of hypotheses is likely to produce many false results that mistakenly appear significant.
Ioannidis argued that "most published research findings are false"<ref name="Ioannidis">{{cite journal | last1 = Ioannidis | first1 = J. P. A. | authorlink1 = John P. A. Ioannidis| title = Why Most Published Research Findings Are False | journal = PLoS Medicine | volume = 2 | issue = 8 | pages = e124 | year = 2005 | pmid = 16060722 | pmc = 1182327 | doi = 10.1371/journal.pmed.0020124}}</ref> due to essentially the same effect: when many scientific teams and researchers each perform many experiments (i.e. process a big amount of scientific data; although not with big data technology), the likelihood of a "significant" result being actually false grows fast – even more so, when only positive results are published.
<!-- sorry, this started overlapping with above section more and more... merging is welcome; I already dropped the intended subheadline "Hype cycle and inflated expectations". -->
Furthermore, big data analytics results are only as good as the model on which they are predicated.  In an example, big data took part in attempting to predict the results of the 2016 U.S. Presidential Election<ref>{{Cite news|url=http://www.nytimes.com/2016/11/10/technology/the-data-said-clinton-would-win-why-you-shouldnt-have-believed-it.html|title=How Data Failed Us in Calling an Election|last=Lohr|first=Steve|date=2016-11-10|last2=Singer|first2=Natasha|newspaper=The New York Times|issn=0362-4331|access-date=2016-11-27}}</ref> with varying degrees of success.  Forbes predicted "If you believe in ''Big Data'' analytics, it’s time to begin planning for a Hillary Clinton presidency and all that entails.".<ref>{{Cite news|url=http://www.forbes.com/sites/jonmarkman/2016/08/08/big-data-and-the-2016-election/#4802f20846d7|title=Big Data And The 2016 Election|last=Markman|first=Jon|newspaper=Forbes|access-date=2016-11-27}}</ref>

== See also ==
{{portal|Information technology}}
{{Category see also|LABEL=For a list of companies, and tools, see also|Big data}}
<!-- NO COMPANIES OR TOOL SPAM HERE. That would be an endless list! "See also" concepts, not linked above. -->
* [[Big memory]]
* [[Datafication]]
* [[Data defined storage]]
* [[Data journalism]]
* [[Data lineage]]
* [[Data philanthropy]]
* [[Data science]]
* [[Machine learning]]
* [[Statistics]]
* [[Small data]]
* [[Urban informatics]]
* [[List of buzzwords]]

== References ==
{{Reflist|30em}}

==Further reading==
*{{cite magazine|editors=Peter Kinnaird, Inbal Talgam-Cohen|series=[[XRDS (magazine)|XRDS: Crossroads, The ACM Magazine for Students]]|title=Big Data|issue=19 (1)|date=2012|publisher=[[Association for Computing Machinery]]|issn=1528-4980 |oclc=779657714 |url=http://dl.acm.org/citation.cfm?id=2331042}}
*{{cite book|title=Mining of massive datasets|author1=[[Jure Leskovec]]|author2=[[Anand Rajaraman]]|author3=[[Jeffrey D. Ullman]]|year=2014|publisher=Cambridge University Press|url=http://mmds.org/|isbn=9781107077232 |oclc=888463433}}
*{{cite book|author1=[[Viktor Mayer-Schönberger]]|author2=[[Kenneth Cukier]]|title=Big Data: A Revolution that Will Transform how We Live, Work, and Think|date=2013|publisher=Houghton Mifflin Harcourt|isbn=9781299903029 |oclc=828620988}}
*{{cite web |url=http://www.forbes.com/sites/gilpress/2013/05/09/a-very-short-history-of-big-data |title=A Very Short History Of Big Data |first=Gil |last=Press |work=forbes.com |date=2013-05-09 |accessdate=2016-09-17 |publisher=[[Forbes Magazine]] |location=Jersey City, NJ}}

== External links ==
*{{Commonsinline}}
* {{Wiktionary-inline|big data}}

{{Use dmy dates|date=December 2015}}
{{Authority control}}

[[Category:Big data| ]]
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]
<=====doc_Id=====>:324
<=====title=====>:
Data processing system
<=====text=====>:
{{other uses2|Data processing}}
{{refimprove|date=July 2013}}

A '''data processing system''' is a combination of machines, <!-- "data processing" is specific to machines, there is no data processing in nature, see the OED --> people, and processes that for a set of inputs produces a defined set of outputs.<ref>The first machines used for data processing were [[Unit record equipment|punched card machines]], now [[Computer]]s are used.</ref>  The inputs and outputs are interpreted as data, facts, information, ... depending on the interpreter's relation to the system. A common synonymous term is "[[Information systems#Types of information systems|information system]]".<ref name=Ralston>{{cite book|title=Encyclopedia of Computer Science 4th ed.|author=Anthony Ralston et al (ed.)|year=2000|publisher=Nature Publishing Group|page=865}}</ref>

A data processing system may involve some combination of:
* [[Data conversion|Conversion]] converting data to another format.
* [[Data validation|Validation]] &ndash; Ensuring that supplied data is "clean, correct and useful."
* [[Sorting]] &ndash; "arranging items in some sequence and/or in different sets."
* [[Summary statistic|Summarization]] &ndash; reducing detail data to its main points.
* [[Aggregate data|Aggregation]] &ndash; combining multiple pieces of data.
* [[Statistical analysis|Analysis]] &ndash; the "collection, organization, analysis, interpretation and presentation of data.".
* Reporting &ndash; list detail or summary data or computed information.

==Types of data processing systems==

===By application area===

====Scientific data processing====
Scientific data processing "usually involves a great deal of computation (arithmetic and comparison operations) upon a relatively small amount of input data, resulting in a small volume of output." <ref name=Reddy>{{cite book|last=Reddy|first=R.J.|title=Business Data Processing & Computer Applications|year=2004|publisher=A P H Publishing Corporation|location=New Dehli|isbn=8176486493|page=17|url=https://books.google.com/books?id=FLKoXCts9ssC&lpg=PA17&dq=%22scientific%20data%20processing%22&pg=PA17#v=onepage&q=%22scientific%20data%20processing%22&f=false}}</ref>

====Commercial data processing====
Commercial data processing "involves a large volume of input data, relatively few computational operations, and a large volume of output."<ref name=Reddy />  Accounting programs are the prototypical examples of data processing applications. [[Information systems|Information systems (IS)]] is the field that studies such organizational computer systems.

====Data analysis====
"[[Data analysis]] is a body of methods that help to describe facts, detect patterns,
develop explanations, and test hypotheses."<ref>{{cite web|last=Dartmouth College|title=Introduction: What Is Data Analysis?|url=http://www.dartmouth.edu/~mss/data%20analysis/Volume%20I%20pdf%20/006%20Intro%20%28What%20is%20the%20weal.pdf|accessdate=July 5, 2013}}</ref>  For example, data analysis might be used to look at sales and customer data to "identify connections between products to allow for cross selling campaigns."<ref>{{cite book|last1=Berthold|first1=M.R.|last2=Borgelt|first2=C|last3=Hőppner|first3=F.|last4=Klawonn|first4=F|title=Guide to Intelligent Data Analysis|year=2010|publisher=Springer|isbn=978-1-84882-260-3|page=15}}</ref>

===By service type<ref name=Ralston />=== 

* [[Transaction processing system|Transaction processing systems]]
* [[Information retrieval|Information storage and retrieval systems]]
* Command and control systems
* Computing service systems
* [[Control system|Process control systems]]
* Message switching systems

==Examples==
===Simple example===
A very simple example of a data processing system is the process of maintaining a check register.  Transactions&mdash; checks and deposits&mdash; are recorded as they occur and the transactions are summarized to determine a current balance.  Monthly the data recorded in the register is reconciled with a hopefully identical list of transactions processed by the bank.

A more sophisticated record keeping system might further identify the transactions&mdash; for example deposits by source or checks by type, such as charitable contributions.  This information might be used to obtain information like the total of all contributions for the year.

The important thing about this example is that it is a ''system'', in which, all transactions are recorded consistently, and the same method of bank reconciliation is used each time.

===Real-world example===
This is a [[flowchart]] of a data processing system combining manual and computerized processing to handle [[accounts receivable]], billing, and [[general ledger]]

[[File:Stockbridge system flowchart example.jpg]]
<ref>the highest acceleration of data processing the point of software</ref>

==References==
{{Reflist}}

== See also ==
* [[Data processing]]
* [[Electronic data processing]]
* [[Computational science|Scientific computing]]
* [[Information processing system]] (broader term)

== Further reading ==
* Bourque, Linda B.; Clark, Virginia A. (1992) Processing Data: The Survey Example. (Quantitative Applications in the Social Sciences, no. 07-085). Sage Publications. ISBN 0-8039-4741-0


[[Category:Data management]]
[[Category:Data processing]]
<=====doc_Id=====>:327
<=====title=====>:
Online transaction processing
<=====text=====>:
'''Online transaction processing''', or '''OLTP''', is a class of [[information system]]s that facilitate and manage transaction-oriented applications, typically for data entry and retrieval [[transaction processing]].

The term is somewhat ambiguous; some understand a "transaction" in the context of computer or [[database transactions]], while others (such as the [[Transaction Processing Performance Council]]) define it in terms of business or [[financial transaction|commercial transactions]].<ref>[http://www.tpc.org/ Transaction Processing Performance Council website]</ref> OLTP has also been used to refer to processing in which the system responds immediately to user requests. An [[automated teller machine]] (ATM) for a bank is an example of a commercial transaction processing application. Online transaction processing applications are high throughput and insert or update-intensive in database management. These applications are used concurrently by hundreds of users. The key goals of OLTP applications are availability, speed, concurrency and recoverability.<ref>[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]</ref> Reduced paper trails and the faster, more accurate forecast for revenues and expenses are both examples of how OLTP makes things simpler for businesses. However, like many modern online information technology solutions, some systems require offline maintenance, which further affects the cost–benefit analysis of on line transaction processing system.

OLTP is typically contrasted to [[Online analytical processing|OLAP]] (online analytical processing), which is generally characterized by much more complex queries, in a smaller volume, for the purpose of business intelligence or reporting rather than to process transactions. Whereas OLTP systems process all kinds of queries (read, insert, update and delete), OLAP is generally optimized for read only and might not even support other kinds of queries.

==Overview==
OLTP system is a popular data processing system in today's enterprises.  Some examples of OLTP systems include order entry, retail sales, and financial transaction systems.<ref>What is an OLTP System[http://docs.oracle.com/cd/E11882_01/server.112/e25523/part_oltp.htm]</ref>  On line transaction processing system increasingly requires support for transactions that span a network and may include more than one company. For this reason, modern on line transaction processing software use client or server processing and brokering software that allows transactions to run on different computer platforms in a network.

In large applications, efficient OLTP may depend on sophisticated transaction management software (such as [[CICS]]) and/or [[database]] optimization tactics to facilitate the processing of large numbers of concurrent updates to an OLTP-oriented database.

For even more demanding decentralized database systems, OLTP brokering programs can distribute transaction processing among multiple computers on a [[computer network|network]]. OLTP is often integrated into [[service-oriented architecture]] (SOA) and [[Web service]]s.

On line transaction processing (OLTP) involves gathering input information, processing the information and updating existing information to reflect the gathered and processed information. As of today, most organizations use a database management system to support OLTP. OLTP is carried in a client server system.

On line transaction process concerns about concurrency and atomicity.  Concurrency controls guarantee that two users accessing the same data in the database system will not be able to change that data or the user has to wait until the other user has finished processing, before changing that piece of data.  Atomicity controls guarantee that all the steps in transaction are completed successfully as a group. That is, if any steps between the transaction fail, all other steps must fail also.<ref>
[http://technet.microsoft.com/en-us/library/ms187669(v=sql.105).aspx On line Transaction Processing vs. Decision Support]</ref>

==Systems design==
To build an OLTP system, a designer must know that the large number of concurrent users does not interfere with the system's performance.  To increase the performance of OLTP system, designer must avoid the excessive use of indexes and clusters.

The following elements are crucial for the performance of OLTP systems:<ref>[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76992/ch3_eval.htm#2680 Application and System Performance Characteristics]</ref>
*Rollback segments
:Rollback segments are the portions of database that record the actions of transactions in the event that a transaction is rolled back.  Rollback segments provide read consistency, roll back transactions, and recover the database.<ref>[http://docs.oracle.com/cd/A87860_01/doc/server.817/a76956/rollbak.htm Rollback]</ref>
*Clusters
:A cluster is a [[Database schema|schema]] that contains one or more tables that have one or more columns in common.  Clustering tables in database improves the performance of [[Join (SQL)|join]] operation.<ref>[http://www.iselfschooling.com/mc4articles/mc4cluster.htm cluster table]</ref>
*Discrete transactions
:All changes to the data are deferred until the transaction commits during a discrete transaction.  It can improve the performance of short, non-distributed transaction.<ref>[http://docs.oracle.com/cd/A57673_01/DOC/server/doc/A48506/transac.htm Discrete Transactions]</ref>
*[[Block (data storage)]] size
:The data block size should be a multiple of the operating system's block size within the maximum limit to avoid unnecessary I/O.<ref>[http://docs.oracle.com/cd/B10500_01/server.920/a96524/c03block.htm Data Block]</ref>
*[[Buffer cache]] size
:To avoid unnecessary resource consumption, tune [[SQL]] statements to use the database buffer cache.<ref>[http://docs.oracle.com/cd/E16655_01/server.121/e15857/tune_buffer_cache.htm#TGDBA294 Database buffer Cache]</ref>
*[[Dynamic allocation]] of space to tables and rollback segments
*[[Transaction processing]] monitors and the multi-threaded server
:A transaction processing monitor is used for coordination of services.  It is like an operating system and does the coordination at a high level of granularity and can span multiple computing devices.<ref>[http://c2.com/cgi/wiki?TransactionProcessingMonitor Transaction processing monitor]</ref>
*[[Partition (database)]]
:Partition increases performance for sites that have regular transactions while still maintain availability and security.<ref>[[Partition (database)|Partition]]</ref>
*[[Database tuning]]
:With database tuning, OLTP system can maximize its performance as efficiently and rapidly as possible.

==Contrasted to==
*[[Batch processing]]
*[[Grid computing]]

==See also==
*[[On line analytical processing]] (OLAP)
*[[Transaction processing]]
*[[Database transaction]]

==References==
<references />

==External links==
{{Wiktionary|OLTP}}
*[http://hstore.cs.brown.edu H-Store Project] (architectural and application shifts affecting OLTP performance)
*[http://www.ibm.com/cics IBM CICS official website]
*[http://www.tpc.org/ Transaction Processing Performance Council]
*[http://dbms.knowledgehills.com/What-is-Online-Transaction-Processing-(OLTP)-Schema/a32p2 OLTP Schema]
*[http://www.amazon.com/dp/1558601902 Transaction Processing: Concepts & Techniques Management]

{{Databases}}

{{DEFAULTSORT:On line Transaction Processing}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
<=====doc_Id=====>:330
<=====title=====>:
Astroinformatics
<=====text=====>:
'''Astroinformatics''' is an interdisciplinary field of study involving the combination of [[astronomy]], [[data science]], [[informatics]], and [[Information technology|information]]/[[Communications technologies|communications]] technologies.<ref name="astroinfo" /><ref name=pdf>[http://www.math.bas.bg/~nkirov/zip/SEEDI_astro_presentation.pdf Astroinformatics and digitization of astronomical heritage], Nikolay Kirov. The fifth SEEDI International Conference Digitization of cultural and scientific heritage, May 19–20, 2010, Sarajevo. Retrieved 1 November 2012.</ref>

==Background==

Astroinformatics is primarily focused on developing the tools, methods, and applications of [[computational science]], [[data science]], and [[statistics]] for research and education in data-oriented astronomy.<ref name="astroinfo">{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy Research and Education|url=http://link.springer.com/article/10.1007%2Fs12145-010-0055-2|website=Journal of Earth Science Informatics, June 2010, Volume 3, Issue 1, pp 5-17|publisher=Springer Link, Netherlands|accessdate=11 January 2016}}</ref> Early efforts in this direction included [[data discovery]], [[metadata standards]] development, [[data modeling]], astronomical [[data dictionary]] development, [[data access]], [[information retrieval]],<ref>{{cite arXiv|last1=Borne|first1=Kirk|title=Science User Scenarios for a Virtual Observatory Design Reference Mission: Science Requirements for Data Mining|arxiv=astro-ph/0008307}}</ref> [[data integration]], and [[data mining]]<ref>{{cite web|last1=Borne|first1=Kirk|title=Scientific Data Mining in Astronomy|url=https://www.crcpress.com/Next-Generation-of-Data-Mining/Kargupta-Han-Yu-Motwani-Kumar/9781420085860|website=CRC Press, pp. 91-114|publisher=Taylor & Francis Group|accessdate=11 January 2016}}</ref> in the astronomical [[Virtual Observatory]] initiatives.<ref>{{cite web|last1=Borne|first1=Kirk|title=Distributed Data Mining in the National Virtual Observatory|url=http://proceedings.spiedigitallibrary.org/proceeding.aspx?articleid=764620|website=SPIE Digital Library|publisher=SPIE|accessdate=11 January 2016}}</ref><ref name="VOdm" /><ref>{{cite web|last1=Laurino|first1=O.|title=Astroinformatics of galaxies and quasars: a new general method for photometric redshifts estimation|url=http://mnras.oxfordjournals.org/content/418/4/2165|website=Monthly Notices of the Royal Astronomical Society, vol.418, pp. 2165-2195|publisher=Oxford Journals|accessdate=12 January 2016|display-authors=etal}}</ref> Further development of the field, along with astronomy community endorsement, was presented to the [[National Research Council (United States)]] in 2009 in the Astroinformatics "State of the Profession" Position Paper for the 2010 [[Astronomy and Astrophysics Decadal Survey]].<ref>{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics: A 21st Century Approach to Astronomy|url=http://adsabs.harvard.edu/abs/2009astro2010P...6B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}</ref> That position paper provided the basis for the subsequent more detailed exposition of the field in the Informatics Journal paper '''Astroinformatics: Data-Oriented Astronomy Research and Education'''.<ref name="astroinfo" />

Astroinformatics as a distinct field of research was inspired by work in the fields of [[Bioinformatics]] and [[Geoinformatics]], and through the [[eScience]] work<ref>{{cite web|title='Online Science'|url=http://research.microsoft.com/en-us/um/people/gray/JimGrayTalks.htm|website=Talks by Jim Gray|publisher=Microsoft Research|accessdate=11 January 2015}}</ref> of [[Jim Gray (computer scientist)]] at [[Microsoft Research]], whose legacy was remembered and continued through the Jim Gray eScience Awards.<ref>{{cite web|title=Jim Gray eScience Award|url=http://research.microsoft.com/en-us/collaboration/focus/escience/jim-gray-award.aspx|website=Microsoft Research}}</ref>

Though the primary focus of Astroinformatics is on the large worldwide distributed collection of digital astronomical databases, image archives, and research tools, the field recognizes the importance of legacy data sets as well—using modern technologies to preserve and analyze historical astronomical observations. Some Astroinformatics practitioners help to [[Digital data|digitize]] historical and recent astronomical observations and images in a large [[database]] for efficient retrieval through [[World wide web|web]]-based interfaces.<ref name=pdf/><ref>[http://www.casca.ca/lrp2010/Docs/LRPReports/astroinformatics_lrp.pdf Astroinformatics in Canada], Nicholas M. Ball, David Schade. Retrieved 1 November 2012.</ref> Another aim is to help develop new methods and software for astronomers, as well as to help facilitate the process and analysis of the rapidly growing amount of data in the field of astronomy.<ref>{{cite web|title='Astroinformatics' helps Astronomers explore the sky|url=http://phys.org/news/2013-10-astroinformatics-astronomers-exploring-sky.html|website=Phys.org|publisher=Heidelberg University|accessdate=11 January 2015}}</ref>

Astroinformatics is described as the '''Fourth Paradigm''' of astronomical research.<ref>{{cite web|title=The Fourth Paradigm: Data-Intensive Scientific Discovery|url=https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/|website=Microsoft Research}}</ref> There are many research areas involved with astroinformatics, such as data mining, machine learning, statistics, visualization, scientific data management, and semantic science.<ref name="VOdm">{{cite web|last1=Borne|first1=Kirk|title=Virtual Observations, Data Mining, and Astroinformatics|url=http://link.springer.com/referenceworkentry/10.1007/978-94-007-5618-2_9|website=Planets, Stars and Stellar Systems, Volume 2: Astronomical Techniques, Software, and Data, pp.403-443|publisher=Springer Link, Netherlands|accessdate=11 January 2015}}</ref> [[Data mining]] and [[machine learning]] play significant roles in Astroinformatics as a [[Scientific method|scientific research]] discipline due to their focus on "knowledge discovery from data" ([[data mining|KDD]]) and "learning from data".<ref>{{cite web|last1=Ball|first1=N.M.|last2=Brunner|first2=R.J.|title=Data Mining and Machine Learrning in Astronomy|url=http://www.worldscientific.com/doi/abs/10.1142/S0218271810017160|website=International Journal of Modern Physics D|publisher=World Scientific Publishing|accessdate=12 January 2016}}</ref><ref>{{cite web|last1=Borne|first1=Kirk|title=The LSST Data Mining Research Agenda|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059074|website=Classification and Discovery in Large Astronomical Surveys, pp.347-351|publisher=American Institute of Physics|accessdate=12 January 2016}}</ref>

The amount of data collected from astronomical sky surveys has grown from gigabytes to terabytes throughout the past decade and is predicted to grow in the next decade into hundreds of petabytes with the [[Large Synoptic Survey Telescope]] and into the exabytes with the [[Square Kilometre Array]].<ref>{{cite web|last1=Ivezić|first1=Ž.|title=Parametrization and Classification of 20 Billion LSST Objects|url=http://scitation.aip.org/content/aip/proceeding/aipcp/10.1063/1.3059076|website=Classification and Discovery in Large Astronomical Surveys, pp.359-365|publisher=American Institute of Physics|accessdate=12 January 2016|display-authors=etal}}</ref> This plethora of new data both enables and challenges effective astronomical research. Therefore, new approaches are required. In part, due to this data-driven science is becoming a recognized academic discipline. Consequently, astronomy (and other scientific disciplines) are developing sub-disciplines information and data intensive to an extent that these sub-disciplines are now becoming (or have already become) stand alone research disciplines and full-fledged academic programs. While many institutes of education do not boast an astroinformatics program, the most likely will in the near future.

[[Informatics]] has been recently defined as "the use of digital data, information, and related services for research and knowledge generation". However the usual, or commonly used definition is "informatics is the discipline of organizing, accessing, integrating, and mining data from multiple sources for discovery and decision support." Therefore, the discipline of astroinformatics includes many naturally-related specialties including data modeling, data organization, etc. It may also include transformation and normalization methods for data integration and information visualization, as well as knowledge extraction, indexing techniques, information retrieval and data mining methods. Classification schemes (e.g., [[taxonomy (general)|taxonomies]], [[ontology (information science)|ontologies]], [[folksonomy|folksonomies]], and/or collaborative [[Tag (metadata)|tagging]]<ref>{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of the ASIS&T|publisher=American Society for Information Science and Technology|accessdate=11 January 2016}}</ref>) plus '''[[Astrostatistics]]''' will also be heavily involved. '''[[Citizen science]]''' projects (such as [[Galaxy Zoo]]) also contribute highly valued novelty discovery, feature meta-tagging, and object characterization within large astronomy data sets. All of these specialties enable scientific discovery across varied massive data collections, collaborative research, and data re-use, in both research and learning environments.

In 2012, two position papers<ref>{{cite web|last1=Borne|first1=Kirk|title=Astroinformatics in a Nutshell|url=https://asaip.psu.edu/Articles/astroinformatics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}</ref><ref>{{cite web|last1=Feigelson|first1=Eric|title=Astrostatistics in a Nutshell|url=https://asaip.psu.edu/Articles/astrostatistics-in-a-nutshell|website=asaip.psu.edu|publisher=The Astrostatistics and Astroinformatics Portal, Penn State University|accessdate=11 January 2016}}</ref> were presented to the Council of the [[American Astronomical Society]] that led to the establishment of formal working groups in Astroinformatics and [[Astrostatistics]] for the profession of [[astronomy]] within the USA and elsewhere.<ref>{{cite arXiv|last1=Feigelson|first1=E.|last2=Ivezić|first2=Ž.|last3=Hilbe|first3=J.|last4=Borne|first4=K.|title=New Organizations to Support Astroinformatics and Astrostatistics|arxiv=1301.3069}}</ref>

Astroinformatics provides a natural context for the integration of education and research.<ref>{{cite web|last1=Borne|first1=Kirk|title=The Revolution in Astronomy Education: Data Science for the Masses|url=http://adsabs.harvard.edu/abs/2009astro2010P...7B|website=Astrophysics Data System|publisher=SAO/NASA|accessdate=11 January 2016}}</ref> The experience of research can now be implemented within the classroom to establish and grow '''[[Data Literacy]]''' through the easy re-use of data.<ref>{{cite web|title=Using Data in the Classroom|url=http://serc.carleton.edu/usingdata/index.html|website=Science Education Resource Center at Carleton College|publisher=National Science Digital Library|accessdate=11 January 2016}}</ref> It also has many other uses, such as repurposing archival data for new projects, literature-data links, intelligent retrieval of information, and many others.<ref>{{cite book|last1=Borne|first1=Kirk|title=Astroinformatics: Data-Oriented Astronomy|location=George Mason University, USA|url=http://www.iccs-meeting.org/iccs2009/PosterPapers/Poster-paper18.pdf|accessdate=January 21, 2015}}</ref>

==Conferences==

{| class="wikitable"
|-
! Year
! Place
! Link
|-
| 2016
| [[Sorrento]], [[Italy]]
| [http://www.iau.org/science/meetings/future/symposia/1158/]
|-
| 2015
| [[Dubrovnik]], [[Dalmatia]]
| [http://iszd.hr/AstroInfo2015/]
|-
| 2014
| [[University of Chile]]
| [http://eventos.cmm.uchile.cl/astro2014/]
|-
| 2013
| [[Australia Telescope National Facility]], [[CSIRO]]
| [http://www.atnf.csiro.au/research/workshops/2013/astroinformatics/]
|-
| 2012
| [[Microsoft Research]]
| [http://www.astro.caltech.edu/ai12/]
|-
| 2011
| [[Sorrento]], [[Italy]]
| [http://dame.dsf.unina.it/astroinformatics2011.html]
|-
| 2010
| [[Caltech]]
| [http://www.astro.caltech.edu/ai10/]
|}

==See also==

*''[[Astronomy and Computing]]''
*[[Astrophysics Data System]]
*[[Astrophysics Source Code Library]]
*[[Astrostatistics]]
*[[Galaxy Zoo]]
*[[International Astrostatistics Association]]
*[[International Virtual Observatory Alliance]] (IVOA)
*[[MilkyWay@home]]
*[[Virtual Observatory]]
*[[WorldWide Telescope]]
*[[Zooniverse (citizen science project)|Zooniverse]]

== External links ==

* [http://www.adass.org/ Astronomical Data Analysis Software and Systems] (ADASS)
* [https://asaip.psu.edu/ Astrostatistics and Astroinformatics Portal]
* [https://asaip.psu.edu/organizations/iaa/iaa-working-group-of-cosmostatistics/ Cosmostatistics Initiative] (COIN)
* [http://www.iau.org/science/scientific_bodies/commissions/B3/ Astroinformatics and Astrostatistics Commission of the International Astronomical Union]

==References==
{{reflist}}

[[Category:Astronomy]]
[[Category:Astrophysics]]
[[Category:Big data]]
[[Category:Computational astronomy]]
[[Category:Data management]]
[[Category:Information science by discipline]]
[[Category:Applied statistics]]
[[Category:Computational fields of study]]
<=====doc_Id=====>:333
<=====title=====>:
Sedona Canada Principles
<=====text=====>:
{{orphan|date=January 2016}}
<!-- Don't mess with this line! --><!-- Write your article below this line -->
The '''Sedona Canada Principles''' are a set of authoritative guidelines published by The Sedona Conference ® to aid members of the Canadian legal community involved in the identification, collection, preservation, review and production of [[electronically stored information]] (ESI).  The principles were drafted by a small group of lawyers, judges and technologists called the Sedona Working Group 7 or ''Sedona Canada''.  Sedona Canada is an offshoot of The Sedona Conference ® which is an American “non-profit…research and educational institute dedicated to the advanced study of law and policy in the areas of antitrust law, complex litigation, and intellectual property rights.”<ref>{{cite web|url=https://thesedonaconference.org/|title=The Sedona Conference® - "Moving the law forward in a reasoned and just way."|work=thesedonaconference.org}}</ref>

==Background==
[[Civil procedure in Canada]] is jurisdictional with each province following its own rules of civil procedure.<ref>{{cite web|url=https://en.wikibooks.org/wiki/Canadian_Civil_Procedure/Rules_by_Province|title=Canadian Civil Procedure/Rules by Province|work=wikibooks.org}}</ref> However, each province must address the fact that due to the advancement of technology the discovery process enshrined in the rules of civil procedure can be potentially derailed due to the sheer volume of [[electronically stored information]] (ESI). <ref name="mccarthy.ca">{{cite web|url=http://www.mccarthy.ca/article_detail.aspx?id=4068|title=McCarthy Tétrault - Taming the Beast of Electronic Discovery with Sedona Canada Principles - Article Detail|work=mccarthy.ca}}</ref> When dealing with litigation matters that involve [[electronically stored information]] (ESI), the discovery process is commonly called [[electronic discovery|e-discovery]].  The problems associated with [[electronic discovery|e-discovery]] in Canada led to the creation of the Sedona Canada Principles. <ref name="mccarthy.ca"/> Rule 29.1.03(4) of the [[wikibooks:Ontario Rules of Civil Procedure]] specifically refers to the Sedona Canada Principles in referencing Principles re Electronic Discovery although it has been reported that this rule has been largely ignored in practice.<ref name="canlii.org">{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2010/2010onsc3670/2010onsc3670.html|title=CanLII - 2010 ONSC 3670 (CanLII)|work=canlii.org}}</ref>

==Summary==
The Sedona Canada Principles largely refer to the processes found in the Electronic Discovery Reference Model.<ref>{{cite web|url=http://www.edrm.net/resources/edrm-stages-explained|title=EDRM Stages|work=edrm.net}}</ref>

The principles urge proportionality due to the potentially enormous volumes of documents that may be discoverable when dealing with ESI.  They also encourage [[good faith]] in the document preservation stage and regular meetings between parties to discuss the scope of the litigation.  Parties are urged to be aware of the potential costs involved in producing relevant ESI but are advised that only reasonably accessible ESI need be produced.  The principles stipulate that parties should not be required to search for or collect deleted material unless there is an agreement or court order related to those terms.  The use of electronic tools and processes such as data sampling and web harvesting are acceptable practices.  Parties are encouraged to agree early in the litigation process on production format required for the exchange of relevant documents as part of the discovery process (native files, [[pdf]], [[Tagged Image File Format|tiff]], [[metadata]] requirements etc).  Agreements or direction should be sought, if necessary, with respect to [[wikt:privilege|privilege]] or other confidential information related to production of electronic documents and data.  Parties should be aware that legal precedents can be formed as a result of [[e-discovery]] practices and sanctions can be considered for a party’s failure to meet their discovery obligations unless it can be demonstrated that the failure was not intentional.  All parties must bear the “reasonable” costs associated with [[e-discovery]] but other arrangements can be agreed upon by the parties or by court order.<ref>{{cite web|url=https://www.canlii.org/en/commentary/sedonacanada/principles_en.html|title=CanLII - The Sedona Canada Principles Addressing Electronic Discovery (Jan. 2008)|work=canlii.org}}</ref>

==Caselaw==

In ''Warman v. National Post Company'' proportionality was at issue in a case where the plaintiff was suing the defendant for libel.  A motion was brought by the defendant to have the plaintiff provide a mirror image of his hard drive in an effort to prove an internet article was indeed authored by the plaintiff.  Issues of proportionality and the work of the Sedona Conference and Sedona Canada Principles were factored in to the decision to grant the defendant only limited access to the hard drive.<ref name="canlii.org"/>

In ''Innovative Health Group Inc. v. Calgary Health Region'' the plaintiff’s legal obligation to produce imaged hard drives is in question.  Justice Conrad refers to the advice of Sedona Canada on proportionality and problems associated with time and expense related to the difficulties associated with electronically stored information.<ref>{{cite web|url=http://www.canlii.org/en/ab/abca/doc/2008/2008abca219/2008abca219.html|title=CanLII - 2008 ABCA 219 (CanLII)|work=canlii.org}}</ref>

In ''York University v. Michael Markicevic'' Justice Brown specifically refers to the need for the parties to agree upon a formal e-discovery plan to be drafted in consultation with Sedona Canada Principles.<ref name="canlii.org1">{{cite web|url=http://www.canlii.org/en/on/onsc/doc/2013/2013onsc378/2013onsc378.html|title=CanLII - 2013 ONSC 378 (CanLII)|work=canlii.org}}</ref>

In ''Friends of Lansdowne v. Ottawa'' Master MacLeod refers to the need for Sedona Canada principles and states “This is particularly true in the current information age when e-mail is ubiquitous and multiple copies or variants of messages may be held on various kinds of data storage devices including individual hard drives, e-mail and Blackberry servers.  Even documents that ultimately exist in paper form normally begin their life on computers and negotiations frequently involve exchanges of electronic drafts.  To find every scrap of paper and every electronic trace of relevant information has become a nightmarish task that threatens to render any kind of litigation extravagantly expensive.”<ref name="canlii.org1"/>

==Criticism==

Critics of the Sedona Canada Principles believe they should address [[system integrity]] and that the true history of any file preserved cannot be identified without proof of the integrity of the electronic record systems management it comes from.<ref>{{cite web|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2530515|title=The Sedona Canada Principles are Very Inadequate on Records Management and for Electronic Discovery|work=ssrn.com}}</ref>

Other criticism is more directed to the Sedona Canada working group and complaints that it is insular and irrelevant<ref>{{cite web|url=http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html|title=Sedona Canada is alive and well|author=Colin Campbell and James Swanson|work=canadianlawyermag.com}}</ref>

==External links==

[https://www.canlii.org/en/commentary/sedonacanada/principles_en.html/The Sedona Canada Principles]

[http://www.canadianlawyermag.com/5078/Sedona-Canada-is-alive-and-well.html/ Sedona Canada is alive and well]

[https://www.highbeam.com/doc/1G1-181488000.html/ Taming the beast of eDiscovery with Sedona Canada Principles]

[https://www.highbeam.com/doc/1G1-400332555.html/ 2014 eDiscovery Year in Review includes Sedona Canada Principles]

[http://www.canadianlawyermag.com/legalfeeds/469/ontario-judge-slams-dark-ages-court-system.html/Ontario Courts discuss Sedona Canada Principles]

[http://www.canadianlawyermag.com/3988/What-is-predictive-coding-and-can-it-help-me.html/ Sedona Canada Principles and predictive coding]

[http://www.canadianlawyermag.com/5019/Alternative-routes.html/ Document review using Sedona Canada Principles]


==References==

{{reflist}}
<!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. -->
*
*
*
*

<!-- STOP! Be warned that by using this process instead of Articles for Creation, this article is subject to scrutiny. As an article in "mainspace", it will be DELETED if there are problems, not just declined. If you wish to use AfC, please return to the Wizard and continue from there. -->



[[Category:Data management]]
<=====doc_Id=====>:336
<=====title=====>:
GB & Smith
<=====text=====>:
{{ad|date=August 2016}}
{{Orphan|date=April 2016}}
{{Use dmy dates|date=September 2016}}
{{Infobox company
| name = GB & Smith
| logo = GB & Smith.png
| caption =
| type = Private
| traded_as =
| successor =
| foundation = 2007
| founder =  Sebastien Goiffon and Alexandre Biegala
| defunct =
| location_city =
| location_country =
| location =
| locations =
| area_served =
| key_people =
| industry = Software
| products =
| services =
| revenue =
| operating_income =
| net_income =
| owner =
| num_employees = 70
| parent =
| divisions =
| subsid =
| homepage = {{url|gbandsmith.com}}
| footnotes =
| intl =
}}

'''GB & Smith''' is an independent [[software]] editor which provides layer independent matrix-based console allowing instant visual review on any supported [[computing platform]].

== History ==

GB & Smith was founded in 2007 by Sebastien Goiffon and Alexandre Biegala.<ref>{{cite web|title=Price Entrepreneur of the Year 2014: Winners North region|url=http://business.lesechos.fr/entrepreneurs/success-stories/prix-de-l-entrepreneur-de-l-annee-2014-les-laureats-region-nord-103724.php|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=20 March 2016}}</ref><ref>{{cite web|title=S'implanter à Londres ? Les clés du succès selon GB & Smith|url=http://www.lesechos.fr/02/04/2015/lesechos.fr/0204265266931_s-implanter-a-londres---les-cles-du-succes-selon-gb---smith.htm|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=19 March 2016}}</ref>

As of 2016 company is becoming agnostic gradually offering it is security administration solutions for [[Microsoft]], [[Oracle Corporation|Oracle]], [[IBM]], [[Tableau Software|Tableau]].<ref name=les>{{cite web|title=GB & Smith secures corporate data|url=http://www.lesechos.fr/26/12/2013/LesEchos/21592-075-ECH_gb---smith-securise-les-donnees-d-entreprise.htm|publisher=[[Les Échos (newspaper)|Les Échos]]|accessdate=20 March 2016}}</ref>

Alexandre Biegala and Sebastien Goiffon invented a method around [[identity access management]] (IAM) to easily review and administer security rights of various applications and over multiple technologies through a single User interface.<ref name=les /><ref>{{cite web|title=GB & Smith Lille : + 761% de croissance en cinq ans!|url=http://www.lavoixdunord.fr/economie/gb-smith-lille-761-de-croissance-en-cinq-ans-ia0b0n1806557|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}</ref>

GbandSmith was granted a patent for a solution on security administration of rights and has become the Security Administration company and known to be a pioneer in Administration Intelligence and real Self-service security administration.<ref>{{cite web|title=US Patent Issued to GB & Smith on Feb. 10 for "Matrix Security Management System for Managing User Accounts and Security Settings"|url=https://www.highbeam.com/doc/1P3-3585409721.html|website=Highbeam.com|accessdate=19 March 2016}}</ref>

[[BusinessObjects]] co-founder Denis Payre joined GB & Smith on 1 April 2016. In 1996, Denis Payre and his partner, Bernard Liautaud were ranked by ''Business Week'' among the "Best Entrepreneurs", alongside [[Steve Jobs]] and [[Steven Spielberg]].<ref>{{cite web|title=GB&SMITH Announces Denis Payre, Co-Founder of Business Objects, to Join its Board of Directors|url=http://www.bizjournals.com/prnewswire/press_releases/2016/04/01/NE60812|website=[[The Business Journals]]|accessdate=1 April 2016}}</ref>

==Solutions==

=== 365Suite Agnostic Self-Service Security Administration ===
365Suite is a set of agnostic tools solutions focused on Security administrations such as access rights management, security policy audits and related metadata. 365Suite enables centralizing security administration into a single console managing multiple applications. 365 runs on top of solutions such as Microsoft [[SharePoint]], [[Microsoft Active Directory]], SAP [[SAP BusinessObjects|BO]], [[SAP HANA|Hana]], Oracle [[Oracle Business Intelligence Suite Enterprise Edition|OBIEE,]] [[Oracle Database|ODB]], [[Tableau Software|Tableau]], etc.<ref>{{cite web|title=Le Comparateur assurance remporte le premier prix du Fast50 avec une croissance de + 1 562% en 4 ans|url=http://www.lavoixdunord.fr/economie/le-comparateur-assurance-remporte-le-premier-prix-du-fast50-ia0b0n3165825|publisher=[[La Voix du Nord (daily)|La Voix du Nord]]|accessdate=20 March 2016}}</ref>

365 solutions consists in two solutions:
* 365View: Single security administration console to operate multiple IT solutions simultaneously (sharepoint, Oarcle BI).
* 365Eyes: Centralized Metadata repository focused on security administration with ability to operate, monitor, restore and compare metadata from multiple IT solutions.

=== 360Suite ===

360Suite consists in a suite of eight solutions focused around [[SAP SE|SAP]] BusinessObjects:  
* 360Plus: Backup, incremental backup,Promotion, including ability to restore deleted files.<ref>{{cite web|title=Le Français GB & Smith invente le concept prometteur d'administration intelligence|url=http://www.channelnews.fr/le-francais-gb-a-smith-invente-le-concept-prometteur-dadministration-intelligence-21842|publisher=ChannelNews|accessdate=19 March 2016}}</ref>
* 360View: Security administration, via a security matrix crossing Ressources and Users, Bulk updates (UNV to UNX, unbounded documents)<ref>{{cite web|title=GB & Smith, un esprit de conquête et un esprit libre|url=http://www.lopinion.fr/2-decembre-2014/gb-smith-esprit-conquete-esprit-libre-18979|publisher=[[L'Opinion (newspaper)|L'Opinion]]|accessdate=19 March 2016}}</ref>
* 360Cast: Schedule and burst dynamically reports.<ref>{{cite web|title=La Société Ugitech Choisit Les Solutions 360suite De Gb & Smith Pour Administrer Sa Plateforme Sap Businessobjects Bi 4.0|url=http://www.decideo.fr/La-Societe-UGITECH-choisit-les-solutions-360suite-de-GB-SMITH-pour-administrer-sa-plateforme-SAP-BusinessObjects-BI-4-0_a6323.html|publisher=Decideo|accessdate=19 March 2016}}</ref>
* 360Eyes: Explore and analyze BO [[metadata]] and perform impact analysis.
* 360Eyes compliance: Compliance to ensure BO compliance.
* 360Vers: Facilitate and monitor BO versioning.
* 360Bind: Automate BO Non regression tests. With ability to compare results and pixels from Webi, Deski and Crystal reports.
* 360Init: Initialize and import your BO security.

== Recognition ==

* 2013-15 [[Deloitte Fast 500#Fast 500 EMEA|Deloitte EMEA technology Fast 500]].<ref>{{cite web|title=Technology Fast 500 EMEA 2013 Ranking|url=http://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/dttl_TMT-Event-Fast-500-2013-winners-ranking.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}</ref><ref>{{cite web|title=2015 Technology Fast 500TM Europe, Middle East & Africa (EMEA) Ranking|url=https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Technology-Media-Telecommunications/gx-deloitte-tmt-emea-fast500-2015-rankings.pdf|publisher=[[Deloitte Fast 500]]|accessdate=19 March 2016}}</ref><ref>{{cite web|title=Technology Fast 50|url=http://www2.deloitte.com/content/dam/Deloitte/fr/Documents/technology%20fast%2050/Deloitte_Palmar%C3%A8s-Fast50_2014.pdf|publisher=[[Deloitte]].com|accessdate=19 March 2016}}</ref>
* 2014 [[Ernst & Young]] emerging company.<ref>{{cite web|title=Sébastien GOIFFON et Alexandre BIEGALA reçoivent le Prix de l'Entreprise d'Avenir de l'Année 2014 pour le Nord de France|url=http://blog.gbandsmith.com/wp-content/uploads/2014/10/cp_resultats_ceremonie_nord_de_france_2014_gbs2_0.pdf|publisher=[[Ey.com]]|accessdate=19 March 2016}}</ref>

==References==
{{reflist}}

== External links ==
*{{Official website|http://www.gbandsmith.com}}

{{DEFAULTSORT:GB and Smith}}
[[Category:Computer access control]]
[[Category:Business intelligence companies]]
[[Category:Identity management]]
[[Category:Data analysis software]]
[[Category:Data management]]
<=====doc_Id=====>:339
<=====title=====>:
Data discovery
<=====text=====>:
'''Data discovery''' is a [[business intelligence]] architecture aimed at creating and using interactive reports and explorable [[data]] from multiple sources. According to the [[United States]] information technology research and advisory firm [[Gartner]] "Data discovery has become a mainstream architecture in 2012".<ref>Kern, J., (2013), [http://www.information-management.com/news/data-discovery-saas-lead-bi-market-review-10024484-1.html Data Discovery, SaaS Lead BI Market Review], ''Information Management''/</ref>

==Definition==
Data discovery is a user-driven process of searching for patterns or specific items in a data set.  Data discovery applications use visual tools such as geographical maps, pivot-tables, and heat-maps to make the process of finding patterns or specific items rapid and intuitive.  Data discovery may leverage statistical and [[data mining]] techniques to accomplish these goals.

==Data discovery and business intelligence (BI)==
Data discovery is a type of [[business intelligence]] in that they both provide the end-user with an application that visualizes [[data]].  Traditional BI covered dashboards, static and parameterized reports, and pivot tables.  Visualization of data in traditional BI incorporated standard charting, KPIs, and limited graphical representation and interactivity. BI is undergoing transformation in capabilities it offers, with a focus on end-user data analysis and discovery, access to larger volumes of data and an ability to create high fidelity presentations of information. 

==Players==
Data Discovery vendors include: [[Tableau_Software|Tableau]], [[Qlik]],  [[TIBCO_Software|TIBCO Spotfire]], Microsoft Power BI, [[MicroStrategy]], SAP (Lumira), Platfora, Datameer, ClearStory Data, [[AnswerRocket]], and Datawatch.<ref>The Rise of Data Discovery Has Set the Stage for a Major Strategic Shift in the BI and Analytics Platform Market 
15 June 2015 G00277789 
Analyst(s): Josh Parenteau | Rita L. Sallam | Cindi Howson 
</ref>

==See also==
* [[Business intelligence]]
* [[Business intelligence tools]]

==References==
{{Reflist|30em}}

[[Category:Business intelligence]]
[[Category:Data management]]
<=====doc_Id=====>:342
<=====title=====>:
Small Data: The Tiny Clues that Uncover Huge Trends
<=====text=====>:
{{npov|date=May 2016}}

{{Infobox book
| name = Small Data: The Tiny Clues That Uncover Huge Trends
| author = Martin Lindstrom
| language = English
| country = United States
| publisher = St. Martins
| isbn = 978-1250080684
}}
Small Data: the Tiny Clues that Uncover Huge Trends is [[Martin Lindstrom]]'s seventh book. It chronicles his work as a branding expert, working with consumers across the world to better understand their behavior. The theory behind the book is that businesses can better create products and services based on observing consumer behavior in their homes, as opposed to relying solely on [[big data]].

== Content ==
The book is based on a several year period of consumer studies for major corporations across the globe.<ref>{{Cite web|url=https://www.martinlindstrom.com/small-data/|title=Small Data - Martin Lindstrom - Bestselling Author|website=MartinLindstrom.com - Martin Lindstrom - Branding Expert & Consultant|language=en-US|access-date=2016-03-27}}</ref> It features case studies of the author's work interviewing consumers in their homes and using his observations to create hypotheses as to why they use products the way that they do.

== Public Reception ==

The book was a New York Times Bestseller<ref>{{Cite web|url=http://www.nytimes.com/best-sellers-books/2016-03-13/advice-how-to-and-miscellaneous/list.html|title=Best Sellers - The New York Times|website=www.nytimes.com|access-date=2016-03-27}}</ref> upon release and was positively reviewed on several websites, Including [[Entrepreneur (magazine)|Entrepreneur]]<ref>{{Cite web|url=http://www.entrepreneur.com/article/271992|title=From an Elon Musk Bio to Malcolm Gladwell's 'Blink', These 9 Books Are Must-Reads|last=Agius|first=Aaron|website=Entrepreneur|access-date=2016-03-27}}</ref> and [[Forbes]]<ref>{{Cite web|url=http://www.forbes.com/sites/davidburkus/2016/01/10/16-must-read-business-books-for-2016/#4ce073648bae|title=16 Must-Read Business Books For 2016|website=Forbes|access-date=2016-03-27}}</ref>

==References==
{{Reflist}}



[[Category:2016 books]]
[[Category:2016 non-fiction books]]
[[Category:Data management]]
<=====doc_Id=====>:345
<=====title=====>:
Cambridge Semantics
<=====text=====>:
{{Infobox company
|name             = Cambridge Semantics
|logo             =
|logo_size        =200
|type             = [[Private company|Private]]
|genre            =
|fate             =
|predecessor      =
|successor        =
|foundation       = 2007
|founder          = Sean Martin<br>Lee Feigenbaum<br>Simon Martin<br>Emmett Eldred
|defunct          =
|location_city    = [[Boston, MA]]
|location_country = [[United States]]
|locations        = (2) [[Boston, MA]] & [[San Diego, CA]]
|area_served      =
|key_people       =  Chuck Pieper (CEO)<br>Alok Prasad (President)
|industry         =  [[Computer Software]]
|products         =
|production       =
|services         =
|revenue          = 
|operating_income =
|net_income       =
|aum              =
|assets           =
|equity           =
|slogan           = The Smart Data Company
|owner            =
|num_employees    =
|parent           =
|divisions        =
|subsid           =
|homepage         =  {{URL|CambridgeSemantics.com}} 
|footnotes        =
|intl             =
}}

'''Cambridge Semantics''' is a privately held company headquartered in [[Boston, Massachusetts]] with a West Coast office in [[San Diego, California]]. The company develops and sells a suite of smart data products for Data Management, Data Discovery and Enterprise Analytics.

==History==

Cambridge Semantics was founded in 2007 by Sean Martin, Lee Feigenbaum, Simon Martin, Rouben Meschian and Emmett Eldred who all previously worked at [[IBM]]'s Advanced Technology Internet Group.<ref>{{cite web|last1=Lynch|first1=Brendan|website=[[Boston Business Journal]]|title=Ex-IBMers aim at better search tech|url=http://www.bizjournals.com/boston/blog/mass-high-tech/2008/03/ex-ibmers-aim-at-better-search-tech.html|accessdate=27 April 2016}}</ref><ref>{{cite web|last1=Resende|first1=Patricia|title=With explosion of big data comes big growth for Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2015/02/with-explosion-of-big-data-comes-big-growth-for.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}</ref>

In 2012, Cambridge Semantics appointed Chuck Pieper as Chief Executive Officer. Prior to joining Cambridge Semantics, Pieper was Vice Chairman of Alternative Investments and Managing Director of [[Credit Suisse]] within the Asset Management Division.<ref>{{cite web|last1=Seiffert|first1=Don|title=Chuck Pieper named CEO at Cambridge Semantics|url=http://www.bizjournals.com/boston/blog/techflash/2012/12/chuck-pieper-named-ceo-at-cambridge.html|website=[[Boston Business Journal]]|accessdate=27 April 2016}}</ref>

In 2015, Cambridge Semantics formed an alliance with [[MarkLogic]].<ref>{{cite web|title=Cambridge Semantics and MarkLogic Partner to Advance Semantic-Driven Data Management|url=http://www.dbta.com/Editorial/News-Flashes/Cambridge-Semantics-and-MarkLogic-Partner-to-Advance-Semantic-Driven-Data-Management-106569.aspx|website=Dbta.com|accessdate=27 April 2016|language=en-US|date=24 September 2015}}</ref><ref>{{cite web|title=MarkLogic, Cambridge Semantics partner for NoSQL|url=http://www.kmworld.com/Articles/News/News/MarkLogic-Cambridge-Semantics-partner-for-NoSQL-106568.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=24 September 2015}}</ref>

In January 2016, Cambridge Semantics acquired SPARQL City and its [[graph database]] [[intellectual property]].<ref>{{cite web|last1=Leopold|first1=George|title=Cambridge Semantics Buys Graph Database Specialist|url=http://www.datanami.com/2016/01/14/cambridge-semantics-buys-graph-database-specialist/|website=Datanami|accessdate=27 April 2016|date=14 January 2016}}</ref>

==Products==
* Anzo Smart Data Platform is a platform for building unified information solutions based on a set of open data standards implemented using [[Semantic Web |Semantic Web Technologies]].<ref>{{cite web|last1=Bertolucci|first1=Jeff|title=Big Data + Semantic Web: Love At First Terabyte? - InformationWeek|url=http://www.informationweek.com/big-data/big-data-analytics/big-data-+-semantic-web-love-at-first-terabyte/d/d-id/1107520?|website=[[InformationWeek]]|accessdate=28 April 2016}}</ref><ref>{{cite web|last1=Shacklett|first1=Mary|title=A start to solving the enterprise data usage problem - TechRepublic|url=http://www.techrepublic.com/article/a-start-to-solving-the-enterprise-data-usage-problem/|website=[[TechRepublic]]|accessdate=28 April 2016}}</ref> It allows IT departments and their business users to quickly and flexibly access all of their diverse data for breakthrough insights.<ref>{{cite web|last1=Lawson|first1=Loraine|title=Cambridge Semantics Offers New Integration Tool|url=http://www.itbusinessedge.com/blogs/integration/cambridge-semantics-offers-new-integration-tool.html|website=IT Business Edge|accessdate=27 April 2016}}</ref><ref>{{cite web|title=Cambridge Semantics Launches Anzo Smart Data Integration|url=http://www.econtentmag.com/Articles/News/News-Item/Cambridge-Semantics-Launches-Anzo-Smart-Data-Integration-98007.htm|website=EContent Magazine|accessdate=27 April 2016|language=en-US|date=3 July 2014}}</ref><ref>{{cite web|title=The time for Smart Data has finally arrived: Cambridge Semantics Inc.|url=http://thesiliconreview.com/magazines/the-time-for-smart-data-has-finally-arrived-cambridge-semantics-inc/|website=The Silicon Review|accessdate=27 April 2016|language=en-US}}</ref><ref>{{cite web|last1=Kutz|first1=Erin|title=Cambridge Semantics, Looking to Put Microsoft Excel "On Steroids," Brings Intelligent Data Sorting to Non-Techies|url=http://www.xconomy.com/boston/2010/07/08/cambridge-semantics-looking-to-put-microsoft-excel-on-steroids-brings-intelligent-data-sorting-to-non-techies/|website=[[Xconomy]]|accessdate=27 April 2016|language=en-US|date=8 July 2010}}</ref><ref>{{cite web|last1=McNamara|first1=Paul|title=Book of Odds opening eyes to new probabilities|url=http://www.networkworld.com/article/2231870/data-center/book-of-odds-opening-eyes-to-new-probabilities.html|website=[[Network World]]|accessdate=28 April 2016}}</ref>
* Anzo Smart Data Manager
* Anzo Graph Query Engine
* Anzo Smart Data Lake

==Awards and recognition==

* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2016 finalist.<ref>{{cite web|title=2016 Finalists|url=https://www.siia.net/codie/2016-Finalists|website=Siia.net|accessdate=27 April 2016}}</ref>
* Cambridge Semantics named [[KMWorld]]’s 2016 ‘100 Companies That Matter in Knowledge Management’<ref>{{cite web|title=KMWorld 100 COMPANIES That Matter in Knowledge Management|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-109344.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 March 2016}}</ref> and [[KMWorld]] Trend-Setting Products of 2015.<ref>{{cite web|last1=McKellar|first1=Hugh|title=KMWorld Trend-Setting Products of 2015|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx|website=[[KMWorld|KMWorld Magazine]]|accessdate=27 April 2016|language=en-US|date=1 September 2015}}</ref>
* Cambridge Semantics named  2016 Bio-IT World Best of Show People's Choice Award Contenders<ref>{{cite web|title=2016 Bio-IT World Best of Show People's Choice Award Contenders|url=http://www.bio-itworld.com/2016/3/29/2016-best-of-show-peoples-choice-award-contenders.asp|website=Bio-IT World|accessdate=27 April 2016}}</ref> and 2015 Bio-IT best of show finalist.<ref>{{cite web|title=Bio-IT World Recognizes 2015 Best of Show Winners|url=http://www.bio-itworld.com/2015/4/27/bio-it-world-recognizes-2015-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}</ref>
* CIO Review Recognizes Cambridge Semantics as 2015 Top 20 Tech Solution Provider for [[Pharmaceutical industry|Pharma and Life Sciences Industry]].<ref>{{cite web|title=20  Most Promising Pharma and Life Sciences Tech Solution Providers  20 15|url=http://pharma-life-sciences.cioreview.com/vendors/2015/20special1|website=CIOReview|accessdate=27 April 2016}}</ref><ref>{{cite web|title=Cambridge Semantics:Smart Data Management and Advanced Analytics for Pharma and Life Sciences|url=http://pharma-life-sciences.cioreview.com/vendor/2015/cambridge_semantics|website=CIOReview|accessdate=27 April 2016}}</ref>
* Anzo Insider Trading Investigation and Surveillance named 2015 [[CODiE Award]] finalist.<ref>{{cite web|title=Finalists - 2015 SIIA CODiE Awards|url=https://www.siia.net/archive/codies/2015/finalists.asp|website=Siia.net|accessdate=27 April 2016}}</ref>
* Cambridge Semantics Selected as Finalist for 2014 [[MIT Sloan]] CIO Symposium's Innovation Showcase.<ref>{{cite web|title=Lead Your Digital Enterprise Mit Sloan Cio|url=http://www.mitcio.com/wp-content/uploads/2015/12/mitcio_2014.pdf|accessdate=27 April 2016}}</ref>
* Cambridge Semantics named [[Software and Information Industry Association|SIIA]] [[CODiE Award]] 2014 finalist.<ref>{{cite web|title=Finalists - 2014 SIIA CODiE Awards |url=http://archive.siia.net/codies/2014/finalist_detail.asp?ID=3 |website=Siia.net |accessdate=27 April 2016 }}{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>
* Cambridge Semantics Win 2013 [[Software and Information Industry Association|SIIA]] [[CODiE Award]] for best business intelligence and analytics solution.<ref>{{cite web|title=2013 CODiE Award Winners|url=http://www.siia.net/archive/codies/2015/pw_2013.asp|website=Siia.net|accessdate=27 April 2016}}</ref>
* Cambridge Semantics wins [[KMWorld]] 2012 Promise Award.<ref>{{cite web|title=KMWorld Promise Award Winner|url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-2012-Promise-and-Reality-award-winners-and-finalists-85829.aspx|website=KMWorld Magazine|accessdate=27 April 2016|language=en-US|date=30 October 2012}}</ref>
* Cambridge Semantics wins Best of Show at 2012 Bio-IT World Conference.<ref>{{cite web|title=2012 Best of Show Winners|url=http://www.bio-itworld.com/2012/04/26/2012-best-of-show-winners.html|website=Bio-IT World|accessdate=27 April 2016}}</ref>

==References==
{{reflist|2}}

==External links==
* [https://www.cambridgesemantics.com/ Official website]

[[Category:Software companies based in Massachusetts]]
[[Category:Companies established in 2007]]
[[Category:Data management]]
<=====doc_Id=====>:348
<=====title=====>:
Category:Information governance
<=====text=====>:
{{catmain}}

[[Category:Data management]]
[[Category:Data security]]
[[Category:Information technology management]]
<=====doc_Id=====>:351
<=====title=====>:
Imprima iRooms
<=====text=====>:
{{Orphan|date=January 2017}}

{{Infobox company
| name = Imprima iRooms
| logo = Imprima-logo-hiRes-300dpi.jpg
| logo_size = 
| logo_alt = Imprima-logo-hiRes-300dpi.jpg
| logo_caption = 
| type = [[Private company|Private]]
| industry = [[Virtual Data Room]], [[Technology]]
| founded = 1910<br/>2001 <small>(relaunched)</small>
| founder = 
| hq_location = London, United Kingdom
| area_served = Worldwide
| website = {{URL|www.imprima.com}}
}}
'''Imprima iRooms''' is a [[private company]] headquartered in London. It provides [[Virtual Data Room]] services to organisations worldwide, including the likes of [[Morgan Stanley]], [[HSBC]] and others.<ref name="printweek">{{cite news|last1=Francis|first1=Jo|title=MBO at Imprima print operation {{!}} PrintWeek|url=http://www.printweek.com/print-week/news/1148840/mbo-imprima-print-operation|accessdate=22 August 2016|publisher=[[PrintWeek]]}}</ref> It also has offices in Paris, Frankfurt, Amsterdam and New York.<ref name="growthbusiness">{{cite news|title=The 21st century virtual data room: A how-to guide|url=http://www.growthbusiness.co.uk/growing-a-business/technology-for-business/2471012/the-21st-century-virtual-data-room-a-howto-guide.thtml|accessdate=22 August 2016|publisher=Growth Business UK}}</ref>

==History==
Imprima was founded over 100 years ago and during this time has served the financial sector in a variety of different capacities.<ref>{{cite web|title=Imprima de Bussy Limited: Private Company Information|url=http://www.bloomberg.com/research/stocks/private/snapshot.asp?privcapId=613207|publisher=[[Bloomberg Businessweek]]|accessdate=22 August 2016}}</ref> By 1990, the company was a leading provider of Financial Print solutions, involved in the publication and delivery of sensitive and business-critical communications for their clients.<ref>{{cite web|title=Companies and products...|url=http://www.ukauthority.com/market-report/news/4614/companies-and-products|publisher=UK Authority|accessdate=25 August 2016}}</ref> In doing so, Imprima amassed an impressive customer list featuring some of the world’s most reputable financial advisors, law firms and corporations.<ref name="printweek" /><ref name="cloudnewsdaily">{{cite web|title=Virtual Data Room Providers|url=http://cloudnewsdaily.com/virtual-data-room/|publisher=Cloud News Daily|accessdate=22 August 2016}}</ref>

==iRooms==
In 2001, Imprima launched their [[Virtual Data Room]] platform, iRooms.<ref name="teletrader" /> iRooms is used by organisations worldwide for Projects requiring secure online file storage and collaboration. Key use cases include [[Mergers & Acquisitions]] (M&A) activity and [[Real estate transaction|Real estate transactions]].<ref name="Francis">{{cite news|title=Rebrand for Imprima Financial Print|url=http://www.printweek.com/print-week/news/1154511/rebrand-for-imprima-financial-print|accessdate=22 August 2016|work=www.printweek.com|publisher=[[PrintWeek]]}}</ref> In 2012, iRooms software was completely revamped and receives regular upgrades.<ref name="cloudnewsdaily" /><ref name="Francis" />

==New Ownership==
In 2014, iRoom was acquired by its current owner, OTM Participation. At that time, Imprima operated two product lines: Financial Print and iRooms (Virtual Data Rooms).<ref name="growthbusiness" /> In November 2014, OTM Participation took the decision to divest away the Financial Print division, whose directors carried out an MBO.<ref name="Francis" /><ref name="teletrader">{{cite web|title=Imprima Adds Multiple Language Interfaces To New iRooms Release|url=http://www.teletrader.com/news/details/6743031?ts=1471881604044|publisher=www.teletrader.com|accessdate=22 August 2016}}</ref>

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:1910 establishments in the United Kingdom]]
[[Category:Companies based in London]]
[[Category:Technology companies established in 1910]]
[[Category:Technology companies of the United Kingdom]]
<=====doc_Id=====>:354
<=====title=====>:
Intelligence Engine
<=====text=====>:
'''Intelligence Engines''' are a type of [[enterprise information management]] that combine [[Business rule management system|business rule management]], [[predictive analytics|predictive]] and [[prescriptive analytics]] to form a unified information access platform that provides real-time intelligence through [[Web search engine|search technologies]], [[Dashboard (management information systems)|dashboards]] and/or existing business infrastructure.  Intelligence Engines are process and/or business problem specific, resulting in industry and/or function-specific [[marketing]] [[trademark]]s associated with them.  They can be differentiated from [[enterprise resource planning]] (ERP) software in that intelligence engines include organization-level business rules and proactive [[decision management]] functionality.

==History==
The first intelligence engine application appears to have been introduced in 2001 by [[Sonus Networks|Sonus Networks, Inc.]] in their patent US6961334 B1.<ref name="sonus">{{cite patent | country = US | number = 6961334 | status = patent | title = Intelligence engine | gdate = 2005-11-01 | invent1 = Kaczmarczyk, Casimer M}}</ref>  Applied to the field of telecommunications systems, the intelligence engine was composed of a database queried by a data distributor layer, received by a telephony management layer and acted upon by a facility management command & control layer.<ref name="sonus" />  This combined standalone business intelligence tools like a [[data warehouse]], reporting and querying software and a [[decision support system]].

The concept was reinforced in 2002 in patent application US20030236689 A1<ref name="2002patent">{{cite patent | country = US | number = 20030236689 | status = application | title = Analyzing decision points in  business processes | pubdate = 2003-12-25 | invent1 = Casati, Fabio | invent2 = Sayal, Mehmet | invent3 = Guadalupe Castellanos, Maria | invent4 = Gunopulos, Dimitrios | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&NR=2003236689&KC=A1&FT=E&locale=en_EP}}</ref> which applied predictive quantitative models to data and used rules to correlate context data at different stages of the business process with business process outcomes to be presented to end users.<ref name="2002patent" />

[[LogRhythm|LogRhythm Inc.]] advanced the concept in 2010 by adding event managers to the end of the intelligence engine's process to determine reporting, remediation and other outcomes.<ref name="LogRhythm">{{cite patent | country = US | number = 2012131185 | status = application | title = Advanced Intelligence Engine | pubdate = 2012-05-24 | invent1 = Petersen , Chris | invent2 = Villella, Phillip | invent3 = Aisa, Brad | url = https://worldwide.espacenet.com/publicationDetails/biblio?CC=US&NR=2012131185A1&KC=A1&FT=D}}</ref>

In 2016, professional service company [[KPMG]] continued to advance the concept by commercializing intelligence engines with the introduction of Third Party Intelligence, which is differentiated from past intelligence engines in its increased use of embedded intellectual property, diversity of global data inputs and focus on predictive analytics to mitigate risk and yield cost savings.<ref name="CIOReview">{{cite web|url=http://www.cioreview.com/news/kmpg-launches-third-party-intelligence-intelligence-engine-to-anticipate-thirdparty-disruptions-nid-14407-cid-78.html |title=KMPG Launches Third Party Intelligence: Intelligence Engine to Anticipate Third-party Disruptions|accessdate=2016-07-22}}</ref>

==Traits==
As a system that combines human intelligence, data inputs, automated decision-making and unified information access, intelligence engines are an advancement in business intelligence tools because they: 
* integrate structured data and unstructured content in a single index<ref name="itbiz">{{cite web|url=http://www.itbusinessedge.com/blogs/it-unmasked/attivio-applies-predictive-analytics-to-indexed-data.html |title=Attivio Applies Predictive Analytics to Indexed Data |accessdate=2016-07-22}}</ref>
* provide advanced workflow automation that can trigger multiple business processes<ref name="salesforce">{{cite press release|url=http://www.salesforce.com/company/news-press/press-releases/2015/03/150309.jsp |title=Salesforce Unveils Service Cloud Intelligence Engine—Fueling Smarter Customer Service for the Connected World |publisher=Salesforce.com|accessdate=2016-07-22}}</ref>
* project future impact of data<ref name="inboundlog">{{cite web|url=http://resources.inboundlogistics.com/digital/issues/il_digital_may2016.pdf |title=Inbound Logistics Magazine May 2016|accessdate=2016-07-22}}</ref> such as supply chain threats <ref name="3pie">{{cite web|url=https://www.kpmgspectrum.com/3pie/index.html |title=KPMG Spectrum |accessdate=2016-07-22}}</ref>
* recommend best actions<ref name="custmatrix">{{cite web|url=http://www.customermatrix.com/news-and-press-releases/press-releases/145-customermatrix-unveils-first-ever-cognitive-intelligence-engine-for-crm-2 |title=CustomerMatrix Unveils First-Ever Cognitive Intelligence Engine for CRM |accessdate=2016-07-22}}</ref> / highlight opportunities for process improvement<ref name="parasoft">{{cite web|url=https://www.parasoft.com/capability/process-intelligence-engine/ |title=Process Intelligence Engine (PIE) |accessdate=2016-07-22}}</ref>
* leverage business intelligence from a variety of experts<ref name="inboundlog" /> 
* combine human expertise with the power of technology to deliver actionable intelligence<ref name="CIOReview" />
* scale data visualization capabilities with the number of users<ref name="armanta">{{cite web|url=http://www.armanta.com/product/technology/intelligence-engine/ |title=A Big Data User Experience |accessdate=2016-07-22}}</ref>

==Applications==
* Attivio Active Intelligence Engine®<ref name="attivo-pr">{{cite web|url=http://info.attivio.com/rs/attivio/images/Attivio-Customer-Succes-Story-General-Electric.pdf |title=Active Intelligence Engine&reg; (AIE&reg;) Case Study: General Electric |accessdate=2016-07-22}}</ref>
* [[KPMG]] Spectrum Intelligence Engine(s)<ref name="KPMGIE">{{cite web|url=https://www.kpmgspectrum.com/3pie/about.html |title=KPMG Spectrum: Action through Intelligence |accessdate=2016-07-22}}</ref>
* [[Salesforce.com|Salesforce]] Service Cloud Intelligence Engine<ref name="salesforce" /> 
* [[FireEye]] Threat Intelligence Engine<ref name="fireeye">{{cite web|url=https://www.fireeye.com/products/dynamic-threat-intelligence/threat-intelligence-engine.html |title=FIREEYE THREAT INTELLIGENCE ENGINE |accessdate=2016-07-22}}</ref>
* [[Factiva]] Intelligence Engine<ref name="factiva">{{cite web|url=http://solutions.dowjones.com/collateral/files/dj-factivacom-brochure-F-3465.pdf |title=Factiva&reg; - The Intelligence Engine |accessdate=2016-07-22}}</ref>
* [[Parasoft]] Process Intelligence Engine<ref name="parasoft" />

==See also==
* [[Business Intelligence]] (BI)
* [[Business intelligence tools]] 
* [[Business Rule Management System]]
* [[Enterprise Information Management]] 
* [[Predictive Analytics]] 
* [[Prescriptive analytics]]
* [[Decision Management]] 
* [[Data Science]]
* [[Data Mining]]

==References==

{{Reflist}}

{{DEFAULTSORT:Intelligence Engine}}
[[Category:Data management]]
[[Category:Information management]]
[[Category:Big data]]
[[Category:Business terms]]
[[Category:Business intelligence]]
[[Category:Information systems]]
[[Category:Supply chain management terms]]
<=====doc_Id=====>:357
<=====title=====>:
Data exhaust
<=====text=====>:
{{Multiple issues|
{{Orphan|date=October 2016}}
{{refimprove|date=October 2016}}
}}

'''Data exhaust''' refers to the trail of [[data]] left by the activities of an [[Internet]] user during his/her online activity. An enormous amount of often raw data are created. These data (which can take the form of [[Cookie (computing)|cookies]], temporary files, [[log file]]s etc.) can help to improve the online experience (for example through customized content). But they can also compromise privacy, as they offer a valuable insight into the user’s habits. It can be used to improve tracking trends and studying data exhaust also improves the user interface and the layout design. <ref name=techtarget>{{cite web|url=http://whatis.techtarget.com/definition/data-exhaust|title=What is data exhaust? - Definition from WhatIs.com|publisher=}}</ref>

Unlike primary content, these data are not purposefully created by the user, who is often unaware of their very existence. A bank for example would consider as [[primary data]] information concerning the sums and parties of a transaction, whilst [[secondary data]] might include the percentage of transactions carried out at a [[cash machine]] instead of a real bank.<ref>{{cite web|url=http://www.pcworld.com/article/3069507/5-things-you-need-to-know-about-data-exhaust.html|title=5 things you need to know about data exhaust|publisher=}}</ref>

==References==
<references />


[[Category:Data management]]

{{internet-stub}}
<=====doc_Id=====>:360
<=====title=====>:
Atomicity (database systems)
<=====text=====>:
{{Other uses|Atomicity (disambiguation)}}

In [[database system]]s, '''atomicity''' (or '''atomicness'''{{Citation needed|date=February 2016}}; from [[Greek language|Greek]] ''atomos'', ''undividable'') is one of the [[ACID]] [[database transaction|transaction]] properties. An '''atomic transaction''' is an ''indivisible'' and ''irreducible'' series of database operations such that either ''all'' occur, or ''nothing'' occurs.<ref>{{cite web
| accessdate = 2011-03-23
| location = http://www.webopedia.com/
| publisher = Webopedia
| title = atomic operation
| quote = An operation during which a processor can simultaneously read a location and write it in the same bus operation. This prevents any other processor or I/O device from writing or reading memory until the operation is complete.
| url = http://www.webopedia.com/TERM/A/atomic_operation.html}}</ref> A guarantee of atomicity prevents updates to the database occurring only partially, which can cause greater problems than rejecting the whole series outright. As a consequence, the transaction cannot be observed to be in progress by another database client. At one moment in time, it has not yet happened, and at the next it has already occurred in whole (or nothing happened if the transaction was cancelled in progress).

An example of an atomic transaction is a monetary transfer from bank account A to account B. It consists of two operations, withdrawing the money from account A and saving it to account B. Performing these operations in an atomic transaction ensures that the database remains in a [[Data consistency|consistent state]], that is, money is not lost nor created if either of those two operations fail.<ref>{{cite web
| url = http://archive.oreilly.com/pub/a/onjava/2001/11/07/atomic.html
| title = Atomic File Transactions, Part 1 - O'Reilly Media
| last = Amsterdam
| first = Jonathan
| website = archive.oreilly.com
| access-date = 2016-02-28
}}</ref>

==Orthogonality==
Atomicity does not behave completely [[Orthogonal (computing)|orthogonally]] with regard to the other [[ACID]] properties of the transactions. For example, [[Isolation (database systems)|isolation]] relies on atomicity to roll back changes in the event of isolation failures such as [[deadlock]]; [[Consistency (database systems)|consistency]] also relies on rollback in the event of a consistency-violation by an illegal transaction. Finally, atomicity itself relies on [[Durability (database systems)|durability]] to ensure the atomicity of transactions even in the face of external failures.

As a result of this, failure to detect errors and roll back the enclosing transaction may cause failures of isolation and consistency.

==Implementation==
Typically, systems implement Atomicity by providing some mechanism to indicate which transactions have started and which finished; or by keeping a copy of the data before any changes occurred ([[read-copy-update]]).  Several filesystems have developed methods for avoiding the need to keep multiple copies of data, using journaling (see [[journaling file system]]). Databases usually implement this using some form of logging/journaling to track changes. The system synchronizes the logs (often the [[metadata]]) as necessary once the actual changes have successfully taken place. Afterwards, crash recovery simply ignores incomplete entries. Although implementations vary depending on factors such as concurrency issues, the principle of atomicity — i.e. complete success or complete failure — remain.

Ultimately, any application-level implementation relies on [[operating system|operating-system]] functionality.  At the file-system level, [[POSIX]]-compliant systems provide [[system call]]s such as <code>open(2)</code> and <code>flock(2)</code> that allow applications to atomically open or lock a file. At the process level, [[POSIX Threads]] provide adequate synchronization primitives.

The hardware level requires [[linearizability|atomic operations]] such as [[Test-and-set]], [[Fetch-and-add]], [[Compare-and-swap]], or [[Load-Link/Store-Conditional]], together with [[memory barrier]]s.  Portable operating systems cannot simply block interrupts to implement synchronization, since hardware that lacks actual concurrent execution such as [[hyper-threading]] or [[multi-processing]] is now extremely rare.{{Citation needed|date=December 2016}}

In [[NoSQL (concept)|NoSQL]] [[data store]]s with eventual consistency, the atomicity is also weaker specified than in relational database systems, and exists only in ''row''s (i.e. [[Column family|column families]]).<ref>{{cite web
| accessdate = 2011-03-23
| author = Olivier Mallassi
| date = 2010-06-09
| location = http://blog.octo.com/en/
| publisher = OCTO Talks!
| title = Let’s play with Cassandra… (Part 1/3)
| quote = Atomicity is also weaker than what we are used to in the relational world. Cassandra guarantees atomicity within a <code>ColumnFamily</code> so for all the columns of a row.
| url = http://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/}}</ref>

==See also==
* [[Atomic operation]]
* [[Transaction processing]]
* [[Long-running transaction]]
* [[Read-copy-update]]

==References==
{{reflist}}

{{DEFAULTSORT:Atomicity (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:363
<=====title=====>:
Lambda architecture
<=====text=====>:
[[File:Diagram of Lambda Architecture (generic).png|thumb|Flow of data through the processing and serving layers of a generic lambda architecture]]
'''Lambda architecture''' is a [[data processing|data-processing]] architecture designed to handle massive quantities of data by taking advantage of both [[batch processing|batch]]- and [[stream processing|stream-processing]] methods. This approach to architecture attempts to balance [[latency (engineering)|latency]], [[throughput]], and [[fault-tolerance]] by using batch processing to provide comprehensive and accurate views of batch data, while simultaneously using real-time stream processing to provide views of online data. The two view outputs may be joined before presentation. The rise of lambda architecture is correlated with the growth of [[big data]], real-time analytics, and the drive to mitigate the latencies of [[map-reduce]].<ref>{{cite web|last1=Schuster|first1=Werner|title=Nathan Marz on Storm, Immutability in the Lambda Architecture, Clojure|url=http://www.infoq.com/interviews/marz-lambda-architecture|website=www.infoq.com}} Interview with Nathan Marz, 6 April 2014</ref>

Lambda architecture depends on a data model with an append-only, immutable data source that serves as a system of record.<ref name=bijnens-slide>Bijnens, Nathan. [http://lambda-architecture.net/architecture/2013-12-11-a-real-time-architecture-using-hadoop-and-storm-devoxx/ "A real-time architecture using Hadoop and Storm"]. 11 December 2013.</ref>{{rp|32}} It is intended for ingesting and processing timestamped events that are appended to existing events rather than overwriting them. State is determined from the natural time-based ordering of the data.

==Overview==
Lambda architecture describes a system consisting of three layers: batch processing, speed (or real-time) processing, and a serving layer for responding to queries.<ref name=big-data>Marz, Nathan; Warren, James. ''Big Data: Principles and best practices of scalable realtime data systems''. Manning Publications, 2013.</ref>{{rp|13}} The processing layers ingest from an immutable master copy of the entire data set.

===Batch layer===
The batch layer precomputes results using a distributed processing system that can handle very large quantities of data. The batch layer aims at perfect accuracy by being able to process ''all'' available data when generating views. This means it can fix any errors by recomputing based on the complete data set, then updating existing views. Output is typically stored in a read-only database, with updates completely replacing existing precomputed views.<ref name=big-data />{{rp|18}}

[[Hadoop|Apache Hadoop]] is the de facto standard batch-processing system used in most high-throughput architectures.<ref>Kar, Saroj. [http://cloudtimes.org/2014/05/28/hadoop-sector-will-have-annual-growth-of-58-for-2013-2020/ "Hadoop Sector will Have Annual Growth of 58% for 2013-2020"], 28 May 2014. ''Cloud Times''.</ref>

===Speed layer===
[[File:Diagram of Lambda Architecture (named components).png|thumb|Diagram showing the flow of data through the processing and serving layers of lambda architecture. Example named components are shown.]]
The speed layer processes data streams in real time and without the requirements of fix-ups or completeness. This layer sacrifices throughput as it aims to minimize latency by providing real-time views into the most recent data. Essentially, the speed layer is responsible for filling the "gap" caused by the batch layer's lag in providing views based on the most recent data. This layer's views may not be as accurate or complete as the ones eventually produced by the batch layer, but they are available almost immediately after data is received, and can be replaced when the batch layer's views for the same data become available.<ref name=big-data />{{rp|203}}

Stream-processing technologies typically used in this layer include [[Storm (event processor)|Apache Storm]], [[Sqlstream|SQLstream]] and [[Apache Spark]]. Output is typically stored on fast NoSQL databases.<ref name=kinley>Kinley, James. [http://jameskinley.tumblr.com/post/37398560534/the-lambda-architecture-principles-for-architecting "The Lambda architecture: principles for architecting realtime Big Data systems"], retrieved 26 August 2014.</ref><ref>Ferrera Bertran, Pere. [http://www.datasalt.com/2014/01/lambda-architecture-a-state-of-the-art/ "Lambda Architecture: A state-of-the-art"]. 17 January 2014, Datasalt.</ref>

===Serving layer===
[[File:Diagram of Lambda Architecture (Druid data store).png|thumb|Diagram showing a lambda architecture with a Druid data store.]]
Output from the batch and speed layers are stored in the serving layer, which responds to ad-hoc queries by returning precomputed views or building views from the processed data.

Examples of technologies used in the serving layer include [[Druid (open-source data store)|Druid]], which provides a single cluster to handle output from both layers.<ref name=metamarkets-lambda>Yang, Fangjin, and Merlino, Gian. [https://speakerdeck.com/druidio/real-time-analytics-with-open-source-technologies-1 "Real-time Analytics with Open Source Technologies"]. 30 July 2014.</ref> Dedicated stores used in the serving layer include [[Apache Cassandra]] or [[Apache HBase]] for speed-layer output, and [https://github.com/nathanmarz/elephantdb Elephant DB] or [[Cloudera Impala]] for batch-layer output.<ref name=bijnens-slide />{{rp|45}}<ref name=kinley />

==Optimizations==
To optimize the data set and improve query efficiency, various rollup and aggregation techniques are executed on raw data,<ref name=metamarkets-lambda />{{rp|23}} while estimation techniques are employed to further reduce computation costs.<ref>Ray, Nelson. [https://metamarkets.com/2013/histograms/ "The Art of Approximating Distributions: Histograms and Quantiles at Scale"]. 12 September 2013. Metamarkets.</ref> And while expensive full recomputation is required for fault tolerance, incremental computation algorithms may be selectively added to increase efficiency, and techniques such as ''partial computation'' and resource-usage optimizations can effectively help lower latency.<ref name=big-data />{{rp|93,287,293}}

==Lambda architecture in use==
Metamarkets, which provides analytics for companies in the programmatic advertising space, employs a version of the lambda architecture that uses [[Druid (open-source data store)|Druid]] for storing and serving both the streamed and batch-processed data.<ref name=metamarkets-lambda />{{rp|42}}

For running analytics on its advertising data warehouse, [[Yahoo]] has taken a similar approach, also using [[Storm (event processor)|Apache Storm]], [[Hadoop|Apache Hadoop]], and [[Druid (open-source data store)|Druid]].<ref name=yahoo-lambda>Rao, Supreeth; Gupta, Sunil. [http://www.slideshare.net/Hadoop_Summit/interactive-analytics-in-human-time?next_slideshow=1 "Interactive Analytics in Human Time"]. 17 June 2014</ref>{{rp|9,16}}

The [[Netflix]] Suro project has separate processing paths for data, but does not strictly follow lambda architecture since the paths may be intended to serve different purposes and not necessarily to provide the same type of views.<ref name=netflix>Bae, Jae Hyeon; Yuan, Danny; Tonse, Sudhir. [http://techblog.netflix.com/2013/12/announcing-suro-backbone-of-netflixs.html "Announcing Suro: Backbone of Netflix's Data Pipeline"], ''[[Netflix]]'', 9 December 2013</ref> Nevertheless, the overall idea is to make selected real-time event data available to queries with very low latency, while the entire data set is also processed via a batch pipeline. The latter is intended for applications that are less sensitive to latency and require a map-reduce type of processing.

==Criticism==
Criticism of lambda architecture has focused on its inherent complexity and its limiting influence. The batch and streaming sides each require a different code base that must be maintained and kept in sync so that processed data produces the same result from both paths. Yet attempting to abstract the code bases into a single framework puts many of the specialized tools in the batch and real-time ecosystems out of reach.<ref>{{cite web|last1=Kreps|first1=Jay|title=Questioning the Lambda Architecture|url=http://radar.oreilly.com/2014/07/questioning-the-lambda-architecture.html|website=radar.oreilly.com|publisher=Oreilly|accessdate=15 August 2014|ref=kreps}}</ref>

In a technical discussion over the merits of employing a pure streaming approach, it was noted that using a flexible streaming framework such as [[Apache Samza]] could provide some of the same benefits as batch processing without the latency.<ref>[https://news.ycombinator.com/item?id=7976785 Hacker News] retrieved 20 August 2014</ref> Such a streaming framework could allow for collecting and processing arbitrarily large windows of data, accommodate blocking, and handle state.

== References ==
<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using <ref></ref> tags, these references will then appear here automatically -->
{{Reflist}}

== External links ==
* [http://lambda-architecture.net/ Repository of Information on Lambda of Architecture]

<!--- Categories --->
[[Category:Articles created via the Article Wizard]]
[[Category:Data processing]]
[[Category:Big data]]
[[Category:Data management]]
[[Category:Free software projects]]
[[Category:Software architecture]]
<=====doc_Id=====>:366
<=====title=====>:
5 Ways of Conceptualizing Data
<=====text=====>:
{{multiple issues|
{{essay-like|date=December 2016}}
{{notability|date=December 2016}}
{{original research|date=December 2016}}
}}
{{Orphan|date=December 2016}}

[[Data]] can be viewed as a measurement of numbers, and characters that are set in a way to understand a certain subject. However, there are many different ways to view data; such as conceptualizing data. These are five ways of conceptualizing data. They all have positive and negative points to each technique. Although they are different, they all bring up questions and concerns with data collection and what happens with the information afterward. Another concern is what is the goal with the data that has been collected depending on the category. The five ways of conceptualizing data are technically, ethically, politically and economically, spatially/ temporal, and philosophically. Typically viewed by critical data scholars, they have all of these ways of viewing data because it is important to see the different ways that data can be viewed and to see if there may be any bias. Not only is it important to see if there is any bias, however, it is also important to understand what all the data will mean in the bigger picture. The way that this is normally done is by understanding raw data, then placing them into categories that will help with the better understanding and creating new knowledge.

==Technically==
Technically viewing data concerns the knowledge about the quality of data, if it is reliable, if it is authentic, if it is valid. It is also about knowing how the data is structured, shared, processed, and analyzed.<ref>(Kitchin, p.12)</ref> There are views about the concerns around data such as the representativeness, how it is uncertain, the reliability of it, the chances of any errors, the likelihood of any bias, and around the measuring of the research design and the execution of it.<ref>(Kitchin, p.13)</ref> There are also questions around if this form of scientific technique is going to bring the data that is wanted and needed.<ref>(Kitchin, p.13)</ref> Other reliability concerns go with this technical view about data such as Quixotic reliability, Diachronic reliability, and Synchronic reliability. Quixotic reliability concern is where there is one observation method which produces unvarying measurements.<ref>(Kitchin, p.13-14)</ref> Diachronic reliability is the stability of an observation through time. Lastly, Synchronic reliability is the similarity of observations within the same time period.<ref>(Kitchin,p.14)</ref> With it being technology, there are many different ways that errors could arise, such as, missing data, mistakes, misunderstaning's, bias’, and uncertainty.<ref>(Kitchin, p.14)</ref>

==Ethically==
The [[ethics|ethical]] view of data is more about the idea of why the data is generated, and what use the data is going to be placed in. There are concerns around how the data will be shared, protected, traded, and to how they are employed”.<ref>(Kitchin, p.15)</ref> This also deals with the issue of sensitivity. Some data is low when it comes to sensitivity, such as the traffic. However, some are a lot higher, such as speaking to survivors of crime.<ref>(Kitchin, p. 15)</ref> With the sensitivity scale, there comes privacy issues, how someone may be treated, and the issue of human rights.<ref>(Kitchin, p. 15)</ref> It is helpful to know that some companies have a data protection act and have privacy laws.<ref>(Kitchin, p.15)</ref> Other components that add to the category of Ethics are the question of equality, fairness, justice, honesty, respect, entitlements, rights, and care of the information that is provided and towards those that give the information.<ref>(Kitchin, p.14)</ref> The honesty, respect and the care of the information can also be misinformed to the subject that is giving the data willingly. Causing ethical concerns for how long the information will be kept, or what the information will be used for. This is an ethical concern in the exchange of the subject and the researcher.<ref>(Jacob)</ref>

==Politically and economically==
[[Politics|politically]] and [[economic]]ally viewed data is seen to how the data could be viewed or theorized as public goods, intellectual property, political capital, and how they are traded and how they are regulated.<ref>(Kitchin, p.16)</ref> Economically there are many decisions when funding data researching, as well as investing in data researching. Data could be used to manage goals and raise the profits and values to those that invest in it.<ref>(Kitchin, p.15)</ref> Such as the multi-billion-dollar data marketplace, where many companies are trading and using that data to help themselves make a profit. It is positively effecting due to the production of knowledge.<ref>(Kitchin, p. 15)</ref> The more that the company knows about what the people want, and how to market to them, the more that they may profit and gain off of the data, due to them giving what the people want. However, there still is the political side to this. Although the data can make a profit and is economically great, there is also the competition which want to influence opinions and make the data terrain greater.<ref>(Kitchin, p.16)</ref> It is also political because the difference between publicly good data, which is shared with anyone that can have access to it, is much different than business data. This is because business data is wanting to keep the data that they have found and use it to their advantage, such as the “production of knowledge.” <ref>(Kitchin, p.16)</ref> The publicly good data is free to anyone that wants to view it, which would not be helpful in any way to any business strategies or marketing.<ref>(Kitchin, p. 16)</ref>

==Spatially/temporal==
Spatial and [[temporal]] views data around technical, ethical, political, and economic [[Regime]] with the production of the data.<ref>(Kitchin, p.17)</ref> The way that the terms spatial and temporal can be viewed is around how the data is developed and changed across time and space. Although, depending on the time and where this data is being collected, the process, the analysis, the storage of some information, yet not of others will be different, just due to a time frame and area will be different than others because of the different history that has happened and the different geographical locations. As noted the process of taking in data changes over time, however, they are never sudden changes. These changes happen slowly over time due to different laws that come in place around how data is handled or protected, the different forms of organizing, the improvements around administration, if any new technology has formed, when the methods of data sorting have changed, along with the methods of sorting the data, the geographical statistics that vary and the new techniques of statistics.<ref>(Kitchin, p.17)</ref> Not only does the geographical location change how the assemblage of data is taken is, but it can also be different depending on the person due to how they manage the data, or how they produce it.<ref>(Kitchin, p.17)</ref>

Looking over data temporally can bring forth either questions or patterns depending on what the data is about. An example of this is looking at graphs that have time in them. They present inclines and declines in a pattern about the data over time.<ref>(Whitney)</ref> Spatial data, on the other hand, looks more towards the geographical sense in the data. The information that is gathered could be about the location, the size or the shape of a particular object. A system that uses spatial data is [[GIS]] (Geographical Information System) <ref>(Rouse)</ref>

==Philosophical==
[[Philosophy]] brings forth views around the areas of epistemology and ontology. In this view of data, there is no interpretations, opinions, importance, or relevance of the data that has been found and processed. The data is simply measured for what it is. Which brings forth to how it is viewed. The data that is viewed philosophically is also viewed in an objective way which means that the data is fixed in some way to prove a specific point. Although the data may be truthful, how the data was provided and how it is placed makes the difference. The data is also viewed in a realist view such as how things truly are. No information is changed, everything is the way that it is and is seen for that.<ref>(Kitchin, p.17-19)</ref> This view also brings up issues around property rights.<ref>(Liu, p.61)</ref> Who would own what and who can have the right to take things.

==References==
{{Reflist|20em}}

===Works cited===
{{refbegin}}
* {{cite book |last=Kitchin |first=Rob |year=2014 |title=The data revolution: Big data, open data, data infrastructures & their consequences |place=London |publisher=Sage |chapter=Conceptualising data |pp=1–26 |chapter-url=http://www.uk.sagepub.com/upm-data/63923_Kitchin_CH1.pdf |format=pdf}}
* Metcalf, Jacob, Emily F. Keller, and danah boyd. 2016. “Perspectives on Big Data, Ethics, and Society.” Council for Big Data, Ethics, and Society.
* Rouse, Margaret. (2013). "What Is Spatial Data? - Definition from WhatIs.com." ''SearchSQLServer''. TechTarget.
* Liu, Hong. (2016). "Philosophical Reflections on Data. " Philosophical Reflections on Data. Science Direct.
* Whitney, Hunter. (2014). "It's About Time." It's About Time: Visualizing Temporal Data to Reveal Patterns and Stories | UX Magazine. UX Magazine.
{{refend}}



[[Category:Data management]]
<=====doc_Id=====>:369
<=====title=====>:
Category:Statistical data agreements
<=====text=====>:
[[Category:Statistical data|Agreements]]
[[Category:Agreements]]
[[Category:Data management]]
<=====doc_Id=====>:372
<=====title=====>:
Data recovery
<=====text=====>:
{{Use dmy dates|date=June 2016}}
{{Multiple issues|
{{Refimprove|date=February 2012}}
{{Manual|date=April 2016}}
}}

In [[computing]], '''data recovery''' is a process of salvaging (retrieving) inaccessible, lost, corrupted, damaged or formatted data from [[secondary storage]], [[removable media]] or [[Computer file|files]], when the data stored in them cannot be accessed in a normal way. The data is most often salvaged from storage media such as internal or external [[hard disk drive]]s (HDDs), [[solid-state drive]]s (SSDs), [[USB flash drive]]s, [[Magnetic tape data storage|magnetic tapes]], [[CD]]s, [[DVD]]s, [[RAID]] subsystems, and other [[electronic devices]]. Recovery may be required due to physical damage to the storage devices or logical damage to the [[file system]] that prevents it from being [[Mount (computing)|mounted]] by the host [[operating system]] (OS).

The most common data recovery scenario involves an operating system failure, malfunction of a storage device, logical failure of storage devices, accidental damage or deletion, etc. (typically, on a single-drive, single-[[disk partition|partition]], single-OS system), in which case the ultimate goal is simply to copy all important files from the damaged media to another new drive. This can be easily accomplished using a [[Live CD]], many of which provide a means to [[Mount (computing)|mount]] the system drive and backup drives or removable media, and to move the files from the system drive to the backup media with a [[file manager]] or [[optical disc authoring software]]. Such cases can often be mitigated by [[disk partition]]ing and consistently storing valuable data files (or copies of them) on a different partition from the replaceable OS system files.

Another scenario involves a drive-level failure, such as a compromised [[file system]] or drive partition, or a [[hard disk drive failure]]. In any of these cases, the data is not easily read from the media devices. Depending on the situation, solutions involve repairing the logical file system, partition table or [[master boot record]],or updating the firmware or drive recovery techniques ranging from software-based recovery of corrupted data, hardware- and software-based recovery of damaged service areas (also known as the hard disk drive's "firmware"), to hardware replacement on a physically damaged drive which involves changes the parts of the damaged drive to make the data in a readable form and can be copied to a new drive. If a drive recovery is necessary, the drive itself has typically failed permanently, and the focus is rather on a one-time recovery, salvaging whatever data can be read.

In a third scenario, files have been accidentally "[[file deletion|deleted]]" from a storage medium by the users. Typically, the contents of deleted files are not removed immediately from the physical drive; instead, references to them in the directory structure are removed, and thereafter space they deleted data occupy is made available for later data overwriting. In the mind of [[end user]]s, deleted files cannot be discoverable through a standard file manager, but the deleted data still technically exists on the physical drive. In the meantime, the original file contents remain, often in a number of disconnected [[File system fragmentation|fragments]], and may be recoverable if not overwritten by other data files.

The term "data recovery" is also used in the context of [[Computer forensics|forensic]] applications or [[espionage]], where data which have been [[Encryption|encrypted]] or hidden, rather than damaged, are recovered. Sometimes data present in the computer gets encrypted or hidden due to reasons like virus attack which can only be recovered by some computer forensic experts. 

==Physical damage==
{{See also|Data recovery hardware}}

A wide variety of failures can cause physical damage to storage media, which may result from human errors and natural disasters. [[CD-ROM]]s can have their metallic substrate or dye layer scratched off; hard disks can suffer any of several mechanical failures, such as [[head crash]]es and failed motors; [[tape drive|tapes]] can simply break. Physical damage always causes at least some data loss, and in many cases the logical structures of the file system are damaged as well. Any logical damage must be dealt with before files can be salvaged from the failed media.

Most physical damage cannot be repaired by end users. For example, opening a hard disk drive in a normal environment can allow airborne dust to settle on the platter and become caught between the platter and the [[read/write head]], causing new head crashes that further damage the platter and thus compromise the recovery process. Furthermore, end users generally do not have the hardware or technical expertise required to make these repairs. Consequently, data recovery companies are often employed to salvage important data with the more reputable ones using [[Cleanroom#Cleanroom classifications|class 100]] dust- and static-free [[cleanroom]]s.<ref>{{cite web|last=Vasconcelos|first=Pedro|title=DIY data recovery could mean "bye-bye"|url=http://blog.ontrackdatarecovery.co.uk/data-recovery-realities/diy-data-recovery-could-mean-bye-bye/|work=The Ontrack Data Recovery Blog|publisher=Kroll Ontrack UK|accessdate=23 May 2013}}</ref>

===Recovery techniques===
Recovering data from physically damaged hardware can involve multiple techniques. Some damage can be repaired by replacing parts in the hard disk. This alone may make the disk usable, but there may still be logical damage. A specialized disk-imaging procedure is used to recover every readable bit from the surface. Once this image is acquired and saved on a reliable medium, the image can be safely analyzed for logical damage and will possibly allow much of the original file system to be reconstructed.

==== {{Anchor|SERVICE-AREA}}Hardware repair ====
[[File:HD with toasty PCB.jpg|thumb|right|250px|Media that has suffered a catastrophic electronic failure requires data recovery in order to salvage its contents.]]

A common misconception is that a damaged [[printed circuit board]] (PCB) may be simply replaced during recovery procedures by an identical PCB from a healthy drive. While this may work in rare circumstances on hard disk drives manufactured before 2003, it will not work on newer drives.  Electronics boards of modern drives usually contain drive-specific [[adaptation data]] required for accessing their system areas, so the related componentry needs to be either reprogrammed (if possible) or unsoldered and transferred between two electronics boards.<ref>{{cite web
 | url = http://www.donordrives.com/pcb-replacement-guide
 | title = Hard Drive Circuit Board Replacement Guide or How To Swap HDD PCB
 | accessdate = 27 May 2015
 | website = donordrives.com
}}</ref><ref>{{cite web
 | url = http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap
 | archiveurl = https://web.archive.org/web/20130329021847/http://www.pcb4you.com/pages/firmware-adaptation-service-rom-swap
 | title = Firmware Adaptation Service - ROM Swap
 | accessdate = 27 May 2015 | archivedate = 29 March 2013
 | website = pcb4you.com
}}</ref>

Each hard disk drive has what is called a ''system area'' or ''service area''; this portion of the drive, which is not directly accessible to the [[end user]], usually contains drive's [[firmware]] and adaptive data that helps the drive operate within normal parameters.<ref>{{cite web
 | url = http://www.recover.co.il/SA-cover/SA-cover.pdf
 | title = Hiding Data in Hard Drive's Service Areas
 | date = 14 February 2013 | accessdate = 23 January 2015
 | author = Ariel Berkman | website = recover.co.il
 | format = PDF
}}</ref>  One function of the system area is to log defective sectors within the drive; essentially telling the drive where it can and cannot write data.

The sector lists are also stored on various chips attached to the PCB, and they are unique to each hard disk drive. If the data on the PCB do not match what is stored on the platter, then the drive will not calibrate properly.<ref>[https://web.archive.org/web/20130416232748/http://datarecoveryreport.com/#Swapping_PCB_Logic_Board#Swapping_PCB_Logic_Board Swapping Data Recovery Report]</ref> In most cases the drive heads will click because they are unable to find the data matching what is stored on the PCB.

==Logical damage==
{{See also|List of data recovery software}}
[[Image:Data loss of image file.JPG|thumb|Result of a failed data recovery from a hard disk drive.]]

The term "logical damage" refers to situations in which the error is not a problem in the hardware and requires software-level solutions.

===Corrupt partitions and file systems, media errors===
In some cases, data on a hard disk drive can be unreadable due to damage to the [[partition table]] or [[file system]], or to (intermittent) media errors. In the majority of these cases, at least a portion of the original data can be recovered by repairing the damaged partition table or file system using specialized data recovery software such as [[Testdisk]]; software like [[dd rescue]] can image media despite intermittent errors, and image raw data when there is partition table or file system damage. This type of data recovery can be performed by people without expertise in drive hardware, as it requires no special physical equipment or access to platters.

Sometimes data can be recovered using relatively simple methods and tools;<ref>[http://www.recover-computerdata.com/ Data Recovery Software]</ref> more serious cases can require expert intervention, particularly if parts of files are irrecoverable. [[File carving|Data carving]] is the recovery of parts of damaged files using knowledge of their structure.

===Overwritten data===
{{See also|Data erasure}}

After data has been physically overwritten on a hard disk drive, it is generally assumed that the previous data are no longer possible to recover. In 1996, [[Peter Gutmann (computer scientist)|Peter Gutmann]], a computer scientist, presented a paper that suggested overwritten data could be recovered through the use of [[magnetic force microscope]].<ref>[http://www.cs.auckland.ac.nz/~pgut001/pubs/secure_del.html ''Secure Deletion of Data from Magnetic and Solid-State Memory''], Peter Gutmann, Department of Computer Science, University of Auckland</ref> In 2001, he presented another paper on a similar topic.<ref>[http://www.cypherpunks.to/~peter/usenix01.pdf ''Data Remanence in Semiconductor Devices''], Peter Gutmann, IBM T.J. Watson Research Center</ref>  To guard against this type of data recovery, Gutmann and Colin Plumb designed a method of irreversibly scrubbing data, known as the [[Gutmann method]] and used by several disk-scrubbing software packages.

Substantial criticism has followed, primarily dealing with the lack of any concrete examples of significant amounts of overwritten data being recovered.<ref>{{cite web | last = Feenberg | first = Daniel | title = Can Intelligence Agencies Read Overwritten Data? A response to Gutmann. | publisher = National Bureau of Economic Research | date = 14 May 2004 | url = http://www.nber.org/sys-admin/overwritten-data-guttman.html | accessdate = 21 May 2008}}</ref> Although Gutmann's theory may be correct, there is no practical evidence that overwritten data can be recovered, while research has shown to support that overwritten data cannot be recovered.{{specify|date=June 2013}}<ref>{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough/ |title=Disk Wiping –  One Pass is Enough |date=17 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20120902011743/http://www.anti-forensics.com:80/disk-wiping-one-pass-is-enough |archivedate=2 September 2012 |df=dmy }}</ref><ref>{{cite web|url=https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |title=Disk Wiping –  One Pass is Enough –  Part 2 (this time with screenshots) |date=18 March 2009 |website=anti-forensics.com |deadurl=yes |archiveurl=https://web.archive.org/web/20121127130830/https://www.anti-forensics.com/disk-wiping-one-pass-is-enough-part-2-this-time-with-screenshots/ |archivedate=27 November 2012 |df=dmy }}</ref><ref>{{cite web
 | url = http://blogs.sans.org/computer-forensics/2009/01/15/overwriting-hard-drive-data/
 | title = Overwriting Hard Drive Data
 | date = 15 January 2009
 | first = Dr. Craig | last = Wright
}}</ref>

[[Solid-state drive]]s (SSD) overwrite data differently from [[hard disk drive]]s (HDD) which makes at least some of their data easier to recover. Most SSDs use [[flash memory]] to store data in pages and blocks, referenced by logical block addresses (LBA) which are managed by the flash translation layer (FTL). When the FTL modifies a sector it writes the new data to another location and updates the map so the new data appear at the target LBA. This leaves the pre-modification data in place, with possibly many generations, and recoverable by data recovery software.


===Lost, Deleted & Formatted Data ===

Sometimes, data present in the physical drives (Internal/External Hard disk, Pen Drive, etc) gets lost, deleted and formatted due to circumstances like virus attack, accidental deletion or accidental use of SHIFT+DELETE. In these cases, data recovery software are used to recover/restore the data files. 

===Logical Bad Sector ===

In the list of logical failures of Hard disk, Logical bad sector is the most common in which data files can't be retrieved from particular sector of the media drives. To resolve this, software are incorporated to correct the logical sectors of media drive and is this is not enough, then there is need for replacement of hardware parts to make the logical bad sectors to be OK.

==Remote data recovery==
Recovery experts do not always need to have physical access to the damaged hardware.  When the lost data can be recovered by software techniques, they can often perform the recovery using remote access software over the Internet, LAN or other connection to the physical location of the damaged media.  The process is essentially no different from what the end user could perform by themselves.<ref>{{Cite web|url = http://datarecovery-overinternet.datarecoverydigest.com/|title = Data Recovery Over the Internet|date = 17 December 2012|accessdate = 29 April 2015|website = Data Recovery Digest|publisher = |last = Barton|first = Andre}}</ref>

Remote recovery requires a stable connection with an adequate bandwidth. However, it is not applicable where access to the hardware is required, as in cases of physical damage.

==Four phases of data recovery==
Usually, there are four phases when it comes to successful data recovery, though that can vary depending on the type of data corruption and recovery required.<ref>{{cite web
 | url = http://www.dolphindatalab.com/the-four-phases-of-data-recovery/
 | title = [Infographic] Four Phases Of Data Recovery
 | date = 28 December 2012 | accessdate = 23 March 2015
 | author = Stanley Morgan | website = dolphindatalab.com
}}</ref>

; Phase 1: Repair the hard disk drive
: Repair the hard disk drive so it is running in some form, or at least in a state suitable for reading the data from it. For example, if heads are bad they need to be changed; if the PCB is faulty then it needs to be fixed or replaced; if the spindle motor is bad the platters and heads should be moved to a new drive.

; Phase 2: Image the drive to a new drive or a disk image file
: When a hard disk drive fails, the importance of getting the data off the drive is the top priority. The longer a faulty drive is used, the more likely further data loss is to occur. Creating an image of the drive will ensure that there is a secondary copy of the data on another device, on which it is safe to perform testing and recovery procedures without harming the source.

; Phase 3: Logical recovery of files, partition, MBR and filesystem structures
: After the drive has been cloned to a new drive, it is suitable to attempt the retrieval of lost data. If the drive has failed logically, there are a number of reasons for that. Using the clone it may be possible to repair the partition table or [[master boot record]] (MBR) in order to read the file system's data structure and retrieve stored data.

; Phase 4: Repair damaged files that were retrieved
: Data damage can be caused when, for example, a file is written to a sector on the drive that has been damaged. This is the most common cause in a failing drive, meaning that data needs to be reconstructed to become readable. Corrupted documents can be recovered by several software methods or by manually reconstructing the document using a hex editor.

== See also ==
{{Portal|Computer security|Computing}}

{{Div col||20em}}
* [[Backup]]
* [[Cleanroom]]
* [[Comparison of file systems]]
* [[Computer forensics]]
* [[Continuous data protection]]
* [[Data archaeology]]
* [[Data loss]]
* [[Error detection and correction]]
* [[File carving]]
* [[Hidden file and hidden directory]]
* [[Undeletion]]
{{Div col end}}

== References ==
{{Reflist|30em}}

==Further reading==
* Tanenbaum, A. & Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}


{{Data erasure}}

{{DEFAULTSORT:Data recovery}}
[[Category:Data recovery| ]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Transaction processing]]
[[Category:Hard disk software|*]]
[[Category:Backup|Recovery]]
<=====doc_Id=====>:375
<=====title=====>:
DataverseNL
<=====text=====>:
'''DataverseNL''' is data management service built on top of [[Dataverse]] data repository and jointly offered by participating institutions to the research community of the Netherlands. It's developed by the Dataverse Team at the Institute for Quantitative Social Science ([[IQSS]]) at [[Harvard University]] and [[Data Archiving and Networked Services]] (DANS) and housed and maintained by DANS team.

With DataverseNL, researchers and lecturers can store, share and register research data online, both during research and for up to ten years afterwards. Dataverse platform is used worldwide. In the Netherlands, DataverseNL was installed at the Universiteit Utrecht in 2010, after which it developed into a shared service of over other institutions. The data management is in the hands of the institutions; DANS has been managing the network since May 2014.

As of January 2017, DataverseNL offers access to more than 300 published studies.

== Persistent identifier ==
At DataverseNL, each deposited dataset receives its own persistent identifier that allows information to remain accessible in the long term, even if its location changes. [[DANS]] has developed this functionality for Dataverse repository to use handle system for its persistent identifiers.

== Open access ==
Besides storing data, DataverseNL allows you to share them with other scientists. Researchers themselves determine who gets access to which materials and what their access rights are (user, contributor or curator). Researchers may choose any license for their data, including [[CC0]] (CC Zero Waiver) or [[ODBL]] (Open Database License).

== How to use DataverseNL ==
'''Depositing data'''<br />
'''1''' Check to see if your university or institution is participating in DataverseNL.<br />
'''2''' Prepare your data: select the relevant data files and check for any privacy-sensitive aspects. <br />
'''3''' Log in at https://dataverse.nl. New users must first sign up for an account.<br />
'''4''' Upload your data to studies within your dataverse.<br />
'''5''' Describe your data and determine who gets access. <br />
'''6''' Share the data by publishing them or allowing others access to your dataverse or studies.<br />

'''Downloading data'''<br />
'''1''' Search or browse, and if necessary refine your search results, until you have found the dataset you are looking for at https://dataverse.nl. <br />
'''2''' Look at the metadata to determine if the dataset meets your requirements. <br />
'''3''' Depending on the access category, you can go to the Data & Analysis tab and download the data. <br />
'''4''' Check the ‘Data citation’ header to see the correct method for citing the data. <br />

== Participating institutions ==
DataverseNL is a shared service provided by the participating institutions and DANS. DANS performs back office tasks, including server and software maintenance and administrative support.
The participating institutions are responsible for managing the deposited data.

At the moment there are following participating institutions:
* [[4TU]] Data lab
*[[ 4TU.Centre for Research Data]]
* [[Tilburg University]]
* [[TiU]]
* [[Universiteit Utrecht]]
* [[Vrije Universiteit Amsterdam]]
* [[Maastricht University]]
* [[Protestantse Theologische Universiteit]]
* [[Avans Hogeschool]]
* [[Nederlands Instituut voor Ecologie]]
* [[Rijksuniversiteit Groningen]]
* [[Erasmus University Rotterdam]]
* [[Hogeschool Windesheim]]

==External links==
*[https://www.dataverse.nl The DataverseNL Repository (Netherlands)]
*[https://www.dans.knaw.nl Data Archiving and Networked Services]
*[http://dataverse.org/ The Dataverse Project]

[[Category:Data management]]
[[Category:Open science]]
[[Category:Open data]]
[[Category:Open-access archives]]
[[Category:Open access (publishing)]]
[[Category:Academic publishing]]
[[Category:Data publishing]]
[[Category:Scholarly communication]]
<=====doc_Id=====>:378
<=====title=====>:
Category:Information architecture
<=====text=====>:
{{cat main}}

Concepts, methodologies and topics related to the practice and theory of information architecture.

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Information architects]]
[[Category:Information governance]]
[[Category:Information science]]
[[Category:Information technology management]]
[[Category:Information technology]]
[[Category:Records management]]
[[Category:Technical communication]]
<=====doc_Id=====>:381
<=====title=====>:
Category:Video storage
<=====text=====>:
[[Category:Video|Storage]]
[[Category:Electronic documents]]
[[Category:Information storage]]
[[Category:Storage media]]
<=====doc_Id=====>:384
<=====title=====>:
Electronic article
<=====text=====>:
'''Electronic articles''' are [[Article (publishing)|article]]s in [[academic journal|scholarly journal]]s or [[magazine]]s  that can be accessed via electronic transmission. They are a specialized form of [[electronic document]], with a specialized content, purpose, format, [[metadata]], and availability&ndash;they consist of individual articles from scholarly journals or  magazines (and now sometimes popular magazines), they have the purpose of providing material for academic [[research]] and study, they are formatted approximately like printed journal articles, the metadata is entered into specialized databases, such as the [[Directory of Open Access Journals]] as well as the databases for the discipline, and they are predominantly available through [[academic library|academic libraries]] and special [[library|libraries]], generally at a fixed charge. 

Electronic articles can be found in [[online and offline|online]]-only journals (par excellence), but in the 21st century they have also become common as online versions of articles that also appear in printed journals. The practice of [[Electronic publishing|publishing of an electronic version]] of an article before it later appears in print is sometimes called '''epub ahead of print''', particularly in [[PubMed]].<ref>{{cite web |url=http://www.nlm.nih.gov/services/ldepubahead.html |title=FAQ: Loansome Doc Article Ordering Service - Epub Ahead of Print |work= |accessdate=2010-10-23}}</ref><ref>{{cite web |url=http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |title=Himmelfarb Library Blog: Epub ahead of print… What does this mean?? |format= |work= |archiveurl=https://web.archive.org/web/20100119081653/http://www.gwumc.edu/library/blog/client/index.cfm/2007/11/26/Epub-ahead-of-print-What-does-this-mean |archivedate=2010-01-19 |deadurl=yes }}</ref>

The term can also be used for the electronic versions of less formal publications, such as online archives, working paper archives from universities, government agencies, private and public think tanks and institutes and private websites. In many academic areas, specialized [[bibliographic database]]s are available to find their online content.

Most commercial sites are [[subscription business model|subscription]]-based, or allow pay-per-view access. Many universities subscribe to electronic journals to provide access to their students and faculty, and it is generally also possible for individuals to subscribe. An increasing number of journals are now available with open access, requiring no subscription. Most working paper archives and articles on personal homepages are free, as are collections in [[institutional repository|institutional repositories]] and [[disciplinary repository|subject repositories]].

The most common formats of transmission are [[HTML]], [[Portable Document Format|PDF]] and, in specialized fields like mathematics and physics, [[TeX]] and [[PostScript]].

==See also==
* [[Academic publishing]]
* [[Eprint]]
* [[Electronic journal]]
* [[Scholarly article]]

== References ==
{{reflist}}

[[Category:Academic publishing]]
[[Category:Electronic publishing]]
[[Category:Electronic documents]]
<=====doc_Id=====>:387
<=====title=====>:
Xplor International
<=====text=====>:
{{about|the trade association|the software package|X-PLOR}}
{{Refimprove|date=June 2008}}
'''Xplor International'''<ref>'''Technology Trends: Xplor Directions Survey''',
Dr. Keith Davidson PhD, edp, ''Enterprise Journal, 1998''. Example of Xplor industry research on [[transaction documents]] and [[laser printing]] trends http://esj.com/article.aspx?ID=10229843143PM</ref> also known as '''The Electronic Document Systems Association''' is an international [[trade association]] specifically focused on the issues of [[transaction document]]s. Transaction documents are legally relevant documents that are either printed and mailed or are electronically delivered e.g. [[Bill (payment)|bills]], [[bank statements]], [[insurance policies]] etc.

The acronym XPLOR was derived from ''Xerox Printer Liaison ORganization'', the original association name. Xplor expanded its mission in 1983 to include other vendors' technology and adopted the acronym as the organization's name.

== History ==
Xplor International was founded in 1980 as a trade association specifically for transaction document applications, due to the difference in emphasis on variable data. Originally a user group for the [[Xerox 9700]] laser printer, they reshaped its mission in the early 1980s to address the entire transaction document industry. Hardware companies like [[IBM]], [[Siemens]] (later [[Océ]]), [[Hewlett Packard]], [[Pitney Bowes]], [[Bell & Howell]], and [[Xerox]] have been actively involved as have software companies like Image Sciences (later Docucorp International), Document Sciences, [[Cincom Systems]], [[GMC Software AG|GMC Software Technology]], Xenos, Crawford Technologies, supported Xplor in order to promote a venue for the issues that are unique to the creation of transaction documents.

In the 1990s, Xplor began to shift from solely document “printing” to document “printing and presentation”, as transaction documents came to be presented on the Web.

==Membership==
Xplor’s membership of users and vendors is worldwide, with approximately 45% of the membership in the early 2000s being outside the US.<ref>William J. 'Bill' McCalpin edp, former General Manager of Xplor International, 2008</ref>

== Xplor honours and awards ==
Xplor awards various technology providers with awards each year, including:

*The Technology Application Award is presented to an individual, a company, or an organization to recognize outstanding achievement in the imaginative application of current technology and/or unique implementation of existing [[electronic document]] systems.
*The Innovator of the Year Award honors an individual, company, or organization that has conceived and developed an original concept leading to a significant advancement in the industry. The "Innovator" has advanced a new program product or technology that notably enhances the capabilities of [[electronic document]] systems.
*The Xplorer of the Year is Xplor International's most prestigious award; it honors significant service to the Association, dedication to the Xplor mission, and notable achievement promoting the interest of the [[electronic document]] systems industry.
*The Brian Platte Lifetime Achievement Award, established in 2007, is given to an individual whose efforts and contributions have significantly changed the course and development of the digital document industry.

=== Electronic Document Professional ===

Xplor manages the [[Electronic Document Professional]] (EDP) certification program for people experienced in [[electronic document]] systems and/or application development.

==Associations in related fields==
* [[Association for Information and Image Management]], the association for electronic content management
* [[Association of Records Managers and Administrators]], the association for records management professionals

==External links==
* [http://www.xplor.org Xplor International] website

== References ==

{{Reflist}}

[[Category:Electronic documents]]
[[Category:International trade associations]]
<=====doc_Id=====>:390
<=====title=====>:
Category:Office software
<=====text=====>:
{{Commons category|Office suites}}
[[Category:Office work]]
[[Category:Business software]]
[[Category:Electronic documents]]
<=====doc_Id=====>:393
<=====title=====>:
Digital object identifier
<=====text=====>:
{{Selfref|For the use of digital object identifiers on Wikipedia, see [[Wikipedia:Digital Object Identifier]].}}
{{Use dmy dates|date=February 2011}}
{{Infobox identifier
| name          = Digital object identifier
| image         = DOI logo.svg
| image_size    = 130px
| image_caption = 
| image_alt     = 
| image_border  = no
| full_name     = 
| acronym       = DOI
| number        = 
| start_date    = {{Start date|2000}}
| organisation  = International DOI Foundation
| digits        = 
| check_digit   = 
| example       = 
| website       = {{URL|doi.org}}
}}
In computing, a '''Digital Object Identifier''' or '''DOI''' is a [[persistent identifier]] or [[handle (computing)|handle]] used to uniquely identify objects. An implementation of the [[Handle System]]<ref>{{cite web|url= http://handle.net/|title=The Handle System}}</ref><ref>{{cite web |url=http://www.doi.org/factsheets.html |title=Factsheets}}</ref> and standardized by the [[ISO]], DOIs are in wide use mainly to identify academic, professional, and government information, such as journal articles, research reports and data sets, and official publications though they can be used to identify other objects, such as commercial videos.

DOI means "digital identifier of an object" rather than "identifier of a digital object".<ref name = "iso">{{cite web | url = https://www.iso.org/obp/ui/#iso:std:iso:26324:ed-1:v1:en | title = ISO 26324:2012(en), Information and documentation — Digital object identifier system | publisher = [[ISO]] | date = | accessdate = 2016-04-20 | quote = DOI is an acronym for 'digital object identifier', meaning a 'digital identifier of an object' rather than an 'identifier of a digital object'.}} "Introduction", paragraph 2.</ref> Thus ''DOI'' stands for "digital object-identifier" rather than "digital-object identifier".

[[Metadata]] about the object is stored in association with the DOI name. It may include a location, such as a [[URL]], indicating where the object can be found. The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI provides more stable linking than simply using its URL, because if its URL changes, the publisher only needs to update the metadata for the DOI to link to the new URL.<ref>{{cite book|author= Witten, Ian H.|author2= David Bainbridge|author3= David M. Nichols|last-author-amp= yes |date= 2010|title= How to Build a Digital Library|edition= 2nd|location= Amsterdam; Boston|publisher= Morgan Kaufmann|pages= 352–253|isbn= 978-0-12-374857-7}}</ref><ref>{{Cite journal|first1= Marc|last1= Langston|first2= James|last2= Tyler|title= Linking to journal articles in an online teaching environment: The persistent link, DOI, and OpenURL|journal= The Internet and Higher Education|volume= 7|issue= 1|date= 2004|pages= 51–58|doi= 10.1016/j.iheduc.2003.11.004}}</ref><ref>{{Cite journal |url=http://www.bloomberg.com/bw/stories/2001-07-22/online-extra-how-the-digital-object-identifier-works |title= How the 'Digital Object Identifier' works |date= 23 July 2001 |work= BusinessWeek |accessdate= 20 April 2010 |quote= Assuming the publishers do their job of maintaining the databases, these centralized references, unlike current Web links, should never become outdated or broken. |publisher= [[BusinessWeek]]}}</ref>

A DOI name differs from standard identifier registries such as the [[ISBN]] and [[ISRC]]. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable.

The DOI system began in 2000 and is managed by the International DOI Foundation.<ref>{{citation|last= Paskin|first= Norman|chapter= Digital Object Identifier (DOI®) System|title= Encyclopedia of Library and Information Sciences|date= 2010|publisher= Taylor and Francis|pages= 1586–1592|edition= 3rd}}</ref> Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs.<ref name="dd">{{Cite journal|title= Digital Object Identifiers: Promise and problems for scholarly publishing|first1= Lloyd A.|last1= Davidson|first2= Kimberly|last2= Douglas|date= December 1998|journal= Journal of Electronic Publishing|volume= 4|issue= 2|doi= 10.3998/3336451.0004.203}}</ref> The DOI system is implemented through a federation of registration agencies coordinated by the International DOI Foundation,<ref>{{cite web|url= https://doi.org/ |title= Welcome to the DOI System |publisher= Doi.org |date= 28 June 2010 |accessdate= 7 August 2010}}</ref> which developed and controls the system. The DOI system has been developed and implemented in a range of publishing applications since 2000; by late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations.<ref>{{Cite web|url= https://doi.org/news/DOINewsApr11.html#1 |title= DOI® News, April 2011: 1. DOI System exceeds 50 million assigned identifiers |publisher= Doi.org |date= 20 April 2011 |accessdate= 3 July 2011}}</ref> By April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.

==Nomenclature==
A DOI name takes the form of a [[character string]] divided into two parts, a prefix and a suffix, separated by a slash. The prefix identifies the registrant of the name, and the suffix is chosen by the registrant and identifies the specific object associated with that DOI. Most legal [[Unicode]] characters are allowed in these strings, which are interpreted in a [[case-insensitive]] manner. The prefix usually takes the form <code>10.NNNN</code>, where <code>NNNN</code> is a series of at least 4 numbers greater than or equal to <code>1000</code>, whose limit depends only on the total number of registrants.<ref name="CrossRefDOI">{{cite web |url=http://www.crossref.org/01company/15doi_info.html |access-date=10 June 2016 |title=doi info & guidelines |website=CrossRef.org |publisher=Publishers International Linking Association, Inc. |date=2013 |quote=All DOI prefixes begin with "10" to distinguish the DOI from other implementations of the Handle System followed by a four-digit number or string (the prefix can be longer if necessary).}}</ref><ref name="DOIKeyFacts">{{cite web |url=https://doi.org/factsheets/DOIKeyFacts.html |access-date=10 June 2016 |title=Factsheet—Key Facts on Digital Object Identifier System |website=doi.org |publisher=International DOI Foundation |date=June 6, 2016 |quote=Over 18,000 DOI name prefixes within the DOI System}}</ref> The prefix may be further subdivided with periods, like <code>10.NNNN.N</code>.<ref name="2.2.2">{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.2.2 |access-date=10 June 2016 |title=DOI Handbook—2 Numbering |website=doi.org |publisher=International DOI Foundation |date=February 1, 2016 |quote=The registrant code may be further divided into sub-elements for administrative convenience if desired. Each sub-element of the registrant code shall be preceded by a full stop.}}</ref>

For example, in the DOI name <code>10.1000/182</code>, the prefix is <code>10.1000</code> and the suffix is <code>182</code>. The "10." part of the prefix identifies the DOI registry,{{efn-ua|Other registries are identified by other strings at the start of the prefix. Handle names that begin with "100." are also in use, as for example in the following citation: {{cite journal|hdl=100.2/ADA013939 |url=http://handle.dtic.mil/100.2/ADA013939 |title=Development of a Transmission Error Model and an Error Control Model l |journal=<!-- --> |volume=<!-- --> | date=May 1975 |last1=Hammond |first1=Joseph L., Jr. |last2=Brown |first2=James E. |last3=Liu |first3=Shyan-Shiang S. |bibcode=1975STIN...7615344H|publisher=Rome Air Development Center|series=Technical Report RADC-TR-75-138}}}} and the characters <code>1000</code> in the prefix identify the registrant; in this case the registrant is the International DOI Foundation itself. <code>182</code> is the suffix, or item ID, identifying a single object (in this case, the latest version of the ''DOI Handbook'').

DOI names can identify creative works (such as texts, images, audio or video items, and software) in both electronic and physical forms, [[performance]]s, and abstract works<ref name=doifaq2>{{Cite journal |url=https://doi.org/faq.html#1 |title=Frequently asked questions about the DOI system: 2. What can be identified by a DOI name? | accessdate = 23 April 2010 | date = 17 February 2010|origyear=update of earlier version |publisher=International DOI Foundation}}
</ref> such as licenses, parties to a transaction, etc.

The names can refer to objects at varying levels of detail: thus DOI names can identify a journal, an individual issue of a journal, an individual article in the journal, or a single table in that article. The choice of level of detail is left to the assigner, but in the DOI system it must be declared as part of the metadata that is associated with a DOI name, using a data dictionary based on the [[indecs Content Model]].

===Display===
The official ''DOI Handbook'' explicitly states that DOIs should display on screens and in print in the format "doi:10.1000/182".<ref name="C4WDefault-2811140">{{cite web |url=https://doi.org/doi_handbook/2_Numbering.html#2.6.1 |title=DOI Handbook – Numbering |date=13 February 2014 |accessdate=30 June 2014 |work=doi.org |author1=<!--Staff writer(s); no by-line.--> |archiveurl=https://web.archive.org/web/20140630181440/http://www.doi.org/doi_handbook/2_Numbering.html |archivedate=30 June 2014 |deadurl=no |at=Section 2.6.1 Screen and print presentation}}</ref> Contrary to the ''DOI Handbook'', [[CrossRef]], a major DOI registration agency, recommends displaying a URL (for example, <code><nowiki>https://doi.org/10.1000/182</nowiki></code>) instead of the officially specified format (for example, <code>[https://doi.org/10.1000/182 doi:10.1000/182]</code>)<ref>{{Cite web| title=DOI Display Guidelines|url=http://www.crossref.org/02publishers/doi_display_guidelines.html}}</ref><ref>{{Cite web| title=New Crossref DOI display guidelines are on the way|url=http://blog.crossref.org/2016/09/new-crossref-doi-display-guidelines.html}}</ref> This URL provides the location of an [[HTTP proxy]] server which will redirect web accesses to the correct online location of the linked item.<ref name="dd"/><ref>{{Cite journal | first=Andy |last=Powell |title=Resolving DOI Based URNs Using Squid: An Experimental System at UKOLN |journal = D-Lib Magazine|date=June 1998|url=http://www.dlib.org/dlib/june98/06powell.html| issn=1082-9873}}</ref> This recommendation is primarily based on the assumption that the DOI is being displayed without being hyper-linked to its appropriate URL – the argument being that without the hyperlink it is not as easy to copy-and-paste the full URL to actually bring up the page for the DOI, thus the entire URL should be displayed, allowing people viewing the page containing the DOI to copy-and-paste the URL, by hand, into a new window/tab in their browser in order to go to the appropriate page for the document the DOI represents.

==Applications==
Major applications of the DOI system currently include:
* [[Scientific literature|scholarly materials]] (journal articles, books, ebooks, etc.) through [[CrossRef]], a consortium of around 3,000 publishers;
* research datasets through [[DataCite]], a consortium of leading research libraries, technical information providers, and scientific data centers;
* [[European Union]] official publications through the [[Publications Office (European Union)|EU publications office]];
* Permanent global identifiers for commercial video content through the Entertainment ID Registry, commonly known as [[EIDR]].

In the [[Organisation for Economic Co-operation and Development]]'s publication service [[OECD iLibrary]], each table or graph in an OECD publication is shown with a DOI name that leads to an Excel file of data underlying the tables and graphs. Further development of such services is planned.<ref>{{cite journal|doi=10.1787/603233448430 |title=We Need Publishing Standards for Datasets and Data Tables |date=2009|journal=Research Information |last=Green|first=T.}}</ref>

A multilingual European DOI registration agency activity, [http://www.mEDRA.org ''m''EDRA], Traditional Chinese content thru [http://doi.airiti.com/ Airiti Inc.] and a Chinese registration agency, [http://www.wanfangdata.com/ Wanfang Data], are active in non-English language markets. Expansion to other sectors is planned by the International DOI Foundation.{{Citation needed|date=May 2010}}

==Features and benefits==
The DOI system was designed to provide a form of [[Persistent identifier|persistent identification]], in which each DOI name permanently and unambiguously identifies the object to which it is associated. And, it associates [[metadata]] with objects, allowing it to provide users with relevant pieces of information about the objects and their relationships. Included as part of this metadata are network actions that allow DOI names to be resolved to web locations where the objects they describe can be found. To achieve its goals, the DOI system combines the [[Handle System]] and the [[indecs Content Model]] with a social infrastructure.

The Handle System ensures that the DOI name for an object is not based on any changeable attributes of the object such as its physical location or ownership, that the attributes of the object are encoded in its metadata rather than in its DOI name, and that no two objects are assigned the same DOI name. Because DOI names are short character strings, they are human-readable, may be copied and pasted as text, and fit into the [[Uniform Resource Identifier|URI]] specification. The DOI name resolution mechanism acts behind the scenes, so that users communicate with it in the same way as with any other web service; it is built on [[open architecture]]s, incorporates [[Computational trust|trust mechanisms]], and is engineered to operate reliably and flexibly so that it can be adapted to changing demands and new applications of the DOI system.<ref>{{cite web|url=http://arstechnica.com/science/2010/03/dois-and-their-discontents-1/|title=DOIs and their discontents|last=Timmer|first=John|date=6 March 2010|work=[[Ars Technica]]|accessdate=5 March 2013}}</ref> DOI name resolution may be used with [[OpenURL]] to select the most appropriate among multiple locations for a given object, according to the location of the user making the request.<ref>{{Cite journal|first1=Susanne|last1=DeRisi|first2=Rebecca|last2=Kennison|first3=Nick|last3=Twyman|title=Editorial: The what and whys of DOIs|journal=[[PLoS Biology]]|volume=1|issue=2|page=e57|date=2003|doi=10.1371/journal.pbio.0000057|pmid=14624257|pmc=261894}} {{open access}}</ref> However, despite this ability, the DOI system has drawn criticism from librarians for directing users to non-free copies of documents that would have been available for no additional fee from alternative locations.<ref>{{Cite book|contribution=Open access to scientific and technical information: the state of the art|first=Jack|last=Franklin|title=Open access to scientific and technical information: state of the art and future trends|editor1-first=Herbert|editor1-last=Grüttemeier|editor2-first=Barry|editor2-last=Mahon|publisher=IOS Press|date=2003|page=74|url=https://books.google.com/?id=2X3gW1lUvN4C&pg=PA74#v=onepage&q|isbn=978-1-58603-377-4}}</ref>

The [[indecs Content Model]] is used within the DOI system to associate metadata with objects. A small kernel of common metadata is shared by all DOI names and can be optionally extended with other relevant data, which may be public or restricted. Registrants may update the metadata for their DOI names at any time, such as when publication information changes or when an object moves to a different URL.

The International DOI Foundation (IDF) oversees the integration of these technologies and operation of the system through a technical and social infrastructure. The social infrastructure of a federation of independent registration agencies offering DOI services was modelled on existing successful federated deployments of identifiers such as [[GS1]] and [[ISBN]].

==Comparison with other identifier schemes==
A DOI name differs from commonly used Internet pointers to material, such as the [[Uniform Resource Locator]] (URL), in that it identifies an object itself as a [[First class (computing)|first-class entity]], rather than the specific place where the object is located at a certain time. It implements the [[Uniform Resource Identifier]] ([[Uniform Resource Name]]) concept and adds to it a data model and social infrastructure.<ref>{{cite web|url=https://doi.org/factsheets/DOIIdentifierSpecs.html |title=DOI System and Internet Identifier Specifications |publisher=Doi.org |date=18 May 2010 |accessdate=7 August 2010}}</ref>

A DOI name also differs from standard identifier registries such as the [[International Standard Book Number|ISBN]], [[International Standard Recording Code|ISRC]], etc. The purpose of an identifier registry is to manage a given collection of identifiers, whereas the primary purpose of the DOI system is to make a collection of identifiers actionable and interoperable, where that collection can include identifiers from many other controlled collections.<ref>{{cite web|url=https://doi.org/factsheets/DOIIdentifiers.html |title=DOI System and standard identifier registries |publisher=Doi.org |accessdate=7 August 2010}}</ref>

The DOI system offers persistent, [[Semantic interoperability|semantically-interoperable]] resolution to related current data and is best suited to material that will be used in services outside the direct control of the issuing assigner (e.g., public citation or managing content of value). It uses a managed registry (providing social and technical infrastructure). It does not assume any specific business model for the provision of identifiers or services and enables other existing services to link to it in defined ways. Several approaches for making identifiers persistent have been proposed. The comparison of persistent identifier approaches is difficult because they are not all doing the same thing. Imprecisely referring to a set of schemes as "identifiers" doesn't mean that they can be compared easily. Other "identifier systems" may be enabling technologies with low barriers to entry, providing an easy to use labeling mechanism that allows anyone to set up a new instance (examples include [[Persistent Uniform Resource Locator]] (PURL), URLs, [[Globally Unique Identifier]]s (GUIDs), etc.), but may lack some of the functionality of a registry-controlled scheme and will usually lack accompanying metadata in a controlled scheme. The DOI system does not have this approach and should not be compared directly to such identifier schemes. Various applications using such enabling technologies with added features have been devised that meet some of the features offered by the DOI system for specific sectors (e.g., [[Archival Resource Key|ARK]]).

A DOI name does not depend on the object's location and, in this way, is similar to a Uniform Resource Name (URN) or PURL but differs from an ordinary URL. URLs are often used as substitute identifiers for documents on the Internet (better characterised as Uniform Resource Identifiers) although the same document at two different locations has two URLs. By contrast, persistent identifiers such as DOI names identify objects as first class entities: two instances of the same object would have the same DOI name.

==Resolution==
DOI name resolution is provided through the [[Handle System]], developed by [[Corporation for National Research Initiatives]], and is freely available to any user encountering a DOI name. Resolution redirects the user from a DOI name to one or more pieces of typed data: URLs representing instances of the object, services such as e-mail, or one or more items of metadata. To the Handle System, a DOI name is a handle, and so has a set of values assigned to it and may be thought of as a record that consists of a group of fields. Each handle value must have a data type specified in its <code><type></code> field, which defines the syntax and semantics of its data.

To resolve a DOI name, it may be input to a DOI resolver (e.g. [https://doi.org/ doi.org]) or may be represented as an HTTP string by preceding the DOI name by the string <code><nowiki>https://doi.org/</nowiki></code> (preferred)<ref>{{cite web|author1=International DOI Foundation|title=Resolution|url=https://doi.org/doi_handbook/3_Resolution.html#3.7.3|website=DOI Handbook|accessdate=19 March 2015|date=2014-08-07}}</ref> or <code><nowiki>https://dx.doi.org/</nowiki></code>. For example, the DOI name <code>10.1000/182</code> can be resolved at the address "<nowiki>https://doi.org/10.1000/182</nowiki>". Web pages or other hypertext documents can include hypertext links in this form. Some browsers allow the direct resolution of a DOI (or other handles) with an add-on, e.g., [http://www.handle.net/hs-tools/extensions/firefox_hdlclient.html CNRI Handle Extension for Firefox]. The CNRI Handle Extension for Firefox enables the browser to access handle or DOI URIs like hdl:4263537/4000 or doi:10.1000/1 using the native Handle System protocol. It will even replace references to web-to-handle proxy servers with native resolution.

Alternative DOI resolvers include http://hdl.handle.net, http://doi.medra.org, https://doi.pangaea.de/ and http://doai.io. The last is unusual in that it tries to find a non-paywalled version of a title and redirects you to that instead of the publisher's version.<ref>{{cite web|url=http://doai.io/|title=DOAI|publisher=CAPSH (Committee for the Accessibility of Publications in Sciences and Humanities)|accessdate=6 August 2016}}</ref><ref>{{Cite web| last = Schonfeld| first = Roger C.| title = Co-opting 'Official' Channels through Infrastructures for Openness |work = The Scholarly Kitchen| accessdate = 2016-10-17| date = 2016-03-03| url = https://scholarlykitchen.sspnet.org/2016/03/03/coopting-official-channels/}}</ref>

==Organizational structure==
The International DOI Foundation (IDF), a non-profit organisation created in 1998, is the governance body of the DOI system.<ref>{{cite book|url=https://doi.org/doi_handbook/7_IDF.html#7.5 |title=DOI Handbook |chapter=Chapter 7: The International DOI Foundation |publisher=Doi.org |accessdate=8 July 2015}}</ref> It safeguards all [[intellectual property|intellectual property rights]] relating to the DOI system, manages common operational features, and supports the development and promotion of the DOI system. The IDF ensures that any improvements made to the DOI system (including creation, maintenance, registration, resolution and policymaking of DOI names) are available to any DOI registrant. It also prevents third parties from imposing additional licensing requirements beyond those of the IDF on users of the DOI system.

The IDF is controlled by a Board elected by the members of the Foundation, with an appointed Managing Agent who is responsible for co-ordinating and planning its activities. Membership is open to all organizations with an interest in electronic publishing and related enabling technologies. The IDF holds annual open meetings on the topics of DOI and related issues.

Registration agencies, appointed by the IDF, provide services to DOI registrants: they allocate DOI prefixes, register DOI names, and provide the necessary infrastructure to allow registrants to declare and maintain metadata and state data. Registration agencies are also expected to actively promote the widespread adoption of the DOI system, to cooperate with the IDF in the development of the DOI system as a whole, and to provide services on behalf of their specific user community. A list of current RAs is maintained by the International DOI Foundation.

Registration agencies generally charge a fee to assign a new DOI name; parts of these fees are used to support the IDF. The DOI system overall, through the IDF, operates on a [[not-for-profit]] cost recovery basis.

==Standardization==
The DOI system is an international standard developed by the [[International Organization for Standardization]] in its technical committee on identification and description, TC46/SC9.<ref>{{cite web|url=http://www.iso.org/iso/pressrelease.htm?refid=Ref1561 |title=Digital object identifier (DOI) becomes an ISO standard |publisher=iso.org |date=10 May 2012 |accessdate=10 May 2012}}</ref> The Draft International Standard ISO/DIS 26324, Information and documentation – Digital Object Identifier System met the ISO requirements for approval. The relevant ISO Working Group later submitted an edited version to ISO for distribution as an FDIS (Final Draft International Standard) ballot,<ref>{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=about_the_doi.html DOI Standards and Specifications |publisher=Doi.org |date=28 June 2010 |accessdate=7 August 2010}}</ref> which was approved by 100% of those voting in a ballot closing on 15 November 2010.<ref>{{cite web|url=https://doi.org/about_the_doi.html#TC46 |title=Overviews & Standards – Standards and Specifications: 1. ISO TC46/SC9 Standards |publisher=Doi.org |date=18 November 2010 |accessdate=3 July 2011}}</ref> The final standard was published on 23 April 2012.<ref>{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=43506 |title=ISO 26324:2012 |publisher=iso.org |date=23 April 2012 |accessdate=10 May 2012}}</ref>

DOI is a registered URI under the [[info URI scheme]] specified by IETF RFC 4452. info:doi/ is the infoURI Namespace of Digital Object Identifiers.<ref>{{cite web|url=http://info-uri.info/registry/docs/misc/faq.html#which_namespaces |title=About "info" URIs – Frequently Asked Questions |publisher=Info-uri.info |accessdate=7 August 2010}}</ref>

The DOI syntax is a [[NISO]] standard, first standardised in 2000, ANSI/NISO Z39.84{{hyphen}}2005 Syntax for the Digital Object Identifier.<ref>{{cite web|url=http://www.techstreet.com/standards/niso-z39-84-2005-r2010?product_id=1262088 |title=ANSI/NISO Z39.84{{hyphen}}2000 Syntax for the Digital Object Identifier |publisher=Techstreet.com |accessdate=7 August 2010}}</ref>

==See also==
{{columns-list|3|
* [[Bibcode]]
* [[Digital identity]]
* [[Metadata standards]]
* [[Object identifier]]
* [[ORCID]]
* [[PubMed#PubMed identifier|PMID]]
* [[Publisher Item Identifier]] (PII)
* [[Permalink]]
* [[Scientific literature]]
* [[Universally Unique Identifier]] (UUID)
}}

==Notes==
{{notelist-ua}}

==References==
{{Reflist|colwidth=30em}}

==External links==
{{Wikidata property|P356}}
* {{official website|https://doi.org/}}
* [http://shortdoi.org Short DOI] – DOI Foundation service for converting long DOIs to shorter equivalents
* [https://doi.org/factsheets/DOIIdentifierSpecs.html Factsheet: DOI System and Internet Identifier Specifications]
* [http://search.crossref.org/ CrossRef DOI lookup]

{{Audiovisual works|state=uncollapsed}}
{{ISO standards}}
{{Authority control}}

{{DEFAULTSORT:Digital Object Identifier}}
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
<=====doc_Id=====>:396
<=====title=====>:
Novell Vibe
<=====text=====>:
{{multiple issues |{{Notability|Products|date=April 2012}}
{{refimprove|date=April 2012}}
{{advert|date=November 2012}}
}}

{{Infobox software
|name                       = Novell Vibe
|logo                       = 
|screenshot                 = 
|caption                    = 
|collapsible                = 
|author                     = 
|developer                  = [[Novell]]
|released                   = {{Start date|2008|06|25}} <!-- {{Start date|YYYY|MM|DD}} -->
|discontinued               = 
|latest release version     = 
|latest release date        = <!-- {{Start date and age|YYYY|MM|DD}} -->
|latest preview version     = 
|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->
|frequently updated         = 
|programming language       = 
|operating system           = 
|platform                   =
|size                       = 
|language                   = 
|status                     = 
|genre                      = [[Web application]]
|license                    = Proprietary
|website                    = [http://www.novell.com/products/vibe/  Novell Vibe]
}}

'''Novell Vibe''' is a web-based team collaboration platform developed by [[Novell]], and was initially released by Novell in June 2008 under the name of Novell Teaming. Novell Vibe is a collaboration platform that can serve as a knowledge repository, [[document management system]], project collaboration hub, process automation machine, corporate [[intranet]] or [[extranet]]. Users can upload, manage, comment on, and edit content in a secure manner. Supported content includes documents, calendars, discussion forums, wikis, blogs, tasks, and more.

Document management functionality allows for document versions, approvals, and document life cycle tracking. Users can download and modify pre-built custom web pages and workflows free of charge from the Vibe Resource Library.<ref>{{cite web|url=http://www.novell.com/products/vibe/resource-library/ |title=Novell Vibe Resource Library |publisher=Novell.com |date= |accessdate=2012-04-12}}</ref>

==History==
Novell Vibe is the result of a merging of two products in November 2010: Novell Teaming and Novell Pulse.<ref>http://www.novell.com/communities/node/12257/great-vibes-paths-merging-novell-teaming-and-novell-pulse</ref>

Novell Teaming began as SiteScape Forum. When Novell Acquired SiteScape in 2008, the name was changed to Novell Teaming.{{citation needed|date=August 2012}}

Created in 2009, Novell Pulse was a communication tool based on the [[Google Wave Federation Protocol]].<ref>http://www.scala-lang.org/node/6618</ref>

===Server===
Any combination of Linux and Windows servers can run the Vibe application. Furthermore, MySQL, MS SQL, and Oracle databases are supported.

===End-User operating systems===
Windows, Linux, Mac, iOS or Android

===Browsers===
Firefox, Internet Explorer, Safari, or Chrome

===Mobile===
Any mobile device that has a browser can access the Vibe site. Native iOS <ref>https://itunes.apple.com/us/app/novell-vibe/id476653054?mt=8</ref> and Android <ref>https://play.google.com/store/apps/details?id=com.novell.vibe.android</ref> apps are available for free download in the app stores.

===Microsoft Office integration===
Word, PowerPoint, and Excel (versions 2013,2010 and 2007) on the Windows operating system are supported.

==Interoperability==
Vibe can be used in conjunction with various other software products, such as [[Novell Access Manager]], [[Novell GroupWise]], [[Skype]], and [[YouTube]]. Novell Vibe integrates with an LDAP directory for authentication.

==Extendability==
Vibe administrators can extend the Vibe software by creating [[software extensions]], remote applications, or [[JAR (file format)]] files that enhance the power and usefulness of the Vibe software to create a custom experience for users.

Software extensions enable third-party developers to create abilities which extend an application. Vibe administrators or Vibe developers can create custom extensions (add-ons) to enhance Vibe. For example, you might have an extension that enables Flash video support in Vibe.

===Remote applications===
A remote application is a program that runs on a remote server and delivers data for use on the Novell Vibe site (such as data from a remote database). For example, Vibe administrators or Vibe developers could set up a remote application for Twitter that displays all of a user's Twitter entries in Vibe.

Unlike creating an extension for Vibe, creating a remote application does not modify the Vibe software.

== Open-source solutions ==
Not all of these projects implement all of the features Novell Vibe has to offer as well as Vibe is missing some features these products have:

* [[Kablink|Kablink Vibe]] - an open source version of Novell Vibe
* [[Redmine]]/[[ChiliProject]]
* [[trac]]
* [[Feng Office Community Edition]]
* [[Open Workbench]]
* [[OpenProj]]

''see also'': [[:Category:Free project management software|Wikipedia category for free project management software]]

==References==
{{Reflist}}

==External links==
* [http://www.novell.com/products/vibe/resource-library/  Novell Vibe product page]
* [http://kabtim.ru//  Kablink-Vibe]
{{Novell}}

[[Category:Novell software]]
[[Category:Proprietary wiki software]]
[[Category:Electronic documents]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Internet Protocol based network software]]
[[Category:Blog software]]
<=====doc_Id=====>:399
<=====title=====>:
Aperture card
<=====text=====>:
[[Image:Aperture card.JPG|400px|right]]
An '''aperture card''' is a type of '''[[punched card]]''' with a cut-out window into which a chip of '''[[microform|microfilm]]''' is mounted.  Such a card is used for [[archive|archiving]] or for making multiple inexpensive copies of a document for ease of distribution.  The card is typically punched with machine-readable [[metadata]] associated with the microfilm image, and printed across the top of the card for visual identification.  The microfilm chip is most commonly 35mm in height, and contains an [[optics|optically reduced]] image, usually of some type of reference document, such as an [[engineering drawing]], that is the focus of the archiving process.  Aperture cards have several advantages and disadvantages when compared to digital systems.  Machinery exists to automatically store, retrieve, sort, duplicate, create, and digitize cards with a high level of automation.  While many aperture cards still play an important role in archiving, their role is gradually being replaced by digital systems.

== Usage ==
Aperture cards are used for engineering drawings from all engineering disciplines.  The [[U.S. Department of Defense]] once made extensive use of aperture cards, and some are still in use, but most data is now digital.<ref>[https://web.archive.org/web/20060530111716id_/http://federalvoice.dscc.dla.mil/federalvoice/030924/tech.html Federal use of aperture cards (Archived Copy)]</ref>

Information about the drawing, for example the drawing number, could be both punched and printed on the remainder of the card.  With the proper machinery, this allows for automated handling.  In the absence of such machinery, the cards can still be read by a human with a lens and a light source.

=== Advantages ===
Aperture cards have, for archival purposes, some advantages over digital systems.  They have a 500-year lifetime, they are human readable, and there is no expense or risk in converting from one digital format to the next when computer systems become obsolete.<ref>{{cite journal|first=Ed |last=LoTurco |title=The Engineering Aperture Card: Still Active, Still Vital |publisher=EDM Consultants |date=January 2004 |url=http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |accessdate=October 10, 2007 |archiveurl=https://web.archive.org/web/20071128162738/http://www.aiimne.org/library/LoTurcoWhitePaper1.pdf |archivedate=November 28, 2007 |deadurl=no |df= }}</ref>

=== Disadvantages ===
{{unreferenced section|date=February 2015}}
Most of the disadvantages are related to the well established differences in analog and digital technology. In particular, searching for given strings within content is considerably slower.  Handling physical cards requires proprietary machinery and processing optical film takes significant time.

The very nature of microfilm cameras and the high contrast properties of microfilm stock itself also impose limits on the amount of detail that can be resolved particularly at the higher reduction ratios (36x or greater) needed to film larger drawings. Faded drawings or those of low or uneven contrast do not reproduce well and significant detail or annotations may be lost.

In common with other forms of microfilm mis-filing cards after use, particularly in large archives, results in the card being for all intents and purposes lost forever unless it's later found by accident.

Aperture cards created from 35mm roll film mounted on to blank cards have to be treated with great care. Bending the card can cause the film to detach and excessive pressure to a stack of cards can cause the mounting glue to ooze creating clumps of cards which will feed through duplicators and other machinery either poorly or not at all. Feeding a de-laminated card through machinery not only risks destroying the image it also risks jamming or damaging the machinery.

== Machinery ==
A set of cards could be rapidly sorted by drawing number or other punched data using a [[IBM 80 series Card Sorters|card sorter]].  Machines are now available that [[Image scanner|scan]] aperture cards and produce a digital version.<ref>For example, this aperture card scanner from  [http://www.oceusa.com/main/product_detail.jsp?FOLDER%3C%3Efolder_id=1408474395186237&PRODUCT%3C%3Eprd_id=845524441761057 Oce']</ref>  Aperture card plotters are machines that use a laser to create the image on the film.<ref>For example, this aperture card plotter from [http://www.wwl.co.uk/apertureplotters.htm Wicks & Wilson] {{webarchive |url=https://web.archive.org/web/20060627060408/http://www.wwl.co.uk/apertureplotters.htm |date=June 27, 2006 }}</ref>

== Conversion ==
Aperture cards can be converted to digital documents using scanning equipment and software. The scan software we use allows for significant image cleanup and enhancement. Often, the digital image produced is better than the visual quality available prescan. A variety of output image types can be generated, most notably, Group 4 TIFF and PDF.<ref>{{cite web|last1=Bryant|first1=Joe|title=Aperture Card Scanning|url=http://www.microcomseattle.com/solutions/document-scanning/aperture-card/|website=Micro Com Seattle|accessdate=17 March 2015}}</ref>

== References ==
{{Reflist}}

== External links ==
* [http://stinet.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=AD0232960 1959 Defense Technical Information Center report] on the technology and its use for submitting engineering plans to the military.
* [http://www.wipo.int/export/sites/www/scit/en/standards/pdf/03-07-a.pdf Detailed description of a particular format]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of Aperture cards from [[WIPO]].
* [https://web.archive.org/web/20090327011959/http://www.green-sheet.net/tutorial2.6.htm Detailed information regarding duplicating microforms and aperture cards] (select and highlight to read black on black text)

[[Category:Archival science]]
[[Category:History of computing]]
[[Category:Infographics]]
[[Category:Technical drawing]]
[[Category:Electronic documents]]
<=====doc_Id=====>:402
<=====title=====>:
Structured document
<=====text=====>:
{{Refimprove|date=April 2014}}
A '''structured document''' is an [[electronic document]] where some method of [[embedded coding]], such as [[markup language|mark-up]], is used to give the whole, and parts, of the document various structural meanings according to a [[Database schema|schema]]. A structured document whose mark-up doesn't break the schema and is designed to conform to and which obeys the [[syntax]] rules of its [[markup language|mark-up language]] is "well-formed".

{{Quote|The [[Standard Generalized Markup Language]] (SGML) has pioneered the concept of structured documents|[http://www.w3.org/People/Janne/porject/paper.html Multi-purpose publishing using HTML, XML, and CSS], [http://www.w3.org/People/howcome/ Håkon Wium Lie] & [http://www.w3.org/People/Janne/ Janne Saarela]}}

As of 2009 the most widely used [[markup language]], in all its evolving forms, is [[HTML]], which is used to structure documents according to various [[Document Type Definition|Document Type Definition (DTD)]] schema defined and described by the [[W3C]], which continually reviews, refines and evolves the specifications.

{{Quote|[[XML]] is the universal format for structured documents and data on the Web|[http://www.w3.org/MarkUp/#related XHTML2 Working Group], [[W3C]]}}

==Structural semantics==
In writing structured documents the focus is on encoding the logical structure of a document, with no explicit concern in the structural markup for its presentation to humans by printed pages, screens or other means. Structured documents, especially well formed ones, can easily be processed by computer systems to extract and present [[metadata]] about the document. In most Wikipedia articles for example, a table of contents is automatically generated from the different heading tags in the body of the document. Popular [[word processor]]s can have such a function available.

In [[HTML]] a part of the logical structure of a document may be the document body; <code><nowiki><body></nowiki></code>, containing a first level heading; <code><nowiki><h1></nowiki></code>, and a paragraph; <code><nowiki><p></nowiki></code>.

<code><nowiki><body></nowiki></code>
:<code><nowiki><h1>Structured document</h1></nowiki></code>
:<code><nowiki><p>A <strong class="selflink">structured document</strong> is an <a href="/wiki/Electronic_document" title="Electronic document">electronic document</a> where some method of <a href="/w/index.php?title=Embedded_coding&amp;action=edit&amp;redlink=1" class="new" title="Embedded coding (page does not exist)">embedded coding</a>, such as <a href="/wiki/Markup_language" title="Markup language">markup</a>, is used to give the whole, and parts, of the document various structural meanings according to a <a href="/wiki/Schema" title="Schema">schema</a>.</nowiki></code><code><nowiki></p></nowiki></code>
<code><nowiki></body></nowiki></code>

One of the most attractive features of structured documents is that they can be reused in many contexts and presented in various ways on mobile phones, TV screens, speech synthesisers, and any other device which can be programmed to process them.

=== Other semantics ===
Other meaning can be ascribed to text which isn't structural. In the [[HTML]] fragment above, there is semantic markup which has nothing to do with structure; the first of these, the <code><nowiki><strong></nowiki></code> tag, means that the enclosed text should be given a strong emphasis. In visual terms this is equivalent to the bold, <code><nowiki><b></nowiki></code> tag, but in speech synthesisers this means a voice inflection giving strong emphasis is used. The term [[semantic markup]] excludes markup like the bold tag which has no meaning other than an instruction to a visual display. The strong tag means that the presentation of the enclosed text should have a strong emphasis in all presentation forms, not just visual.<br />
The anchor <code><nowiki><a></nowiki></code> tag is a more obvious example of semantic markup unconcerned with structure, with its href attribute set it means that the text it surrounds is a [[hyperlink]].

[[HTML]] from early on has also had tags which gave presentational semantics, i.e. there were tags to give '''bold''' (<code><nowiki><b></nowiki></code>)or ''italic'' (<code><nowiki><i></nowiki></code>) text, or to alter <small>font sizes</small> or which had other effects on the presentation.<ref>{{cite web|url=http://www.w3.org/MarkUp/draft-ietf-iiir-html-01.txt|accessdate=5 March 2014}}</ref> Modern versions of markup languages discourage such markup in favour of [[Style sheet language|style sheets]]. Different style sheets can be attached to any markup, semantic or presentational, to produce different presentations. In [[HTML]], tags such as; <code><nowiki><a>, <blockquote>, <em>, <strong></nowiki></code> and others do not have a structural meaning, but do have a meaning.

== See also ==
* [[Document processor]]
* [[Machine-Readable Documents]]

 {{reflist}}

{{DEFAULTSORT:Structured Document}}
[[Category:Electronic documents]]
<=====doc_Id=====>:405
<=====title=====>:
Computable Document Format
<=====text=====>:
{{Infobox file format
| name                   = Computable Document Format
| logo                   = [[Image:WolframCDFLogoSmall.png]]
| icon                   = [[Image:WolframCDFLogoSmall.png]]
| iconcaption            = 
| screenshot             =  
| caption                =  
| extension              = .cdf
| mime                   = application/cdf
| typecode               =  
| uniform type           = com.wolfram.cdf
| magic                  =  
| owner                  = [[Wolfram Research]]
| released               = {{Start date|2011|07|21}}<!-- {{Start date|YYYY|mm|dd|df=yes}} -->
| latest release version = 
| latest release date    = <!-- {{Start date and age|YYYY|mm|dd|df=yes}} -->
| genre                  =
| container for          =
| contained by           =
| extended from          =
| extended to            =
| standard               =  
| free                   =
| url                    = [http://www.wolfram.com/cdf Computable Document Format]
}}
'''Computable Document Format''' ('''CDF''') is an electronic document format<ref>[http://www.telegraph.co.uk/technology/news/8561619/Wolfram-Alpha-creator-plans-to-delete-the-PDF.html Wolfram Alpha Creator plans to delete the PDF] The Telegraph (UK)</ref> designed to allow easy authoring<ref>[http://www.pcworld.com/businesscenter/article/236202/wolfram_makes_data_interactive.html Wolfram makes data interactive] PC World</ref> of dynamically generated interactive content. CDF is a published public format<ref>{{cite web|title=About CDFs|url=http://www.wolfram.com/cdf/faq/#aboutcdf|publisher=[[Wolfram Research]]}}</ref> created by [[Wolfram Research]].<ref name=thinq11/>

==Features==
Computable document format supports [[GUI]] elements such as sliders, menus and buttons. Content is updated using embedded computation in response to GUI interaction. Contents can include formatted text, tables, images, sounds and animations. CDF supports [[Mathematica]] typesetting and technical notation.<ref>[http://www.zdnet.com/blog/btl/wolfram-launches-new-document-format-meet-cdf/52917 Wolfram Launches new document format. Meet CDF] ZDNet</ref> Paginated layout, structured drill down layout and slide-show mode are supported. Styles can be controlled using a [[Cascading Style Sheets|cascading style sheet]].

==Reading==
CDF files can be read using a proprietary [[CDF Player]] with a restrictive license, which can be downloaded free of charge from Wolfram Research.<ref name=thinq11>[http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ Wolfram punts expanded medium for technical docs] {{webarchive |url=https://web.archive.org/web/20110725121540/http://www.thinq.co.uk/2011/7/21/wolfram-punts-expanded-medium-technical-docs/ |date=July 25, 2011 }} ThinQ</ref>

==Authoring==
CDF Files can be created using [[Mathematica]].  Online authoring tools are planned.<ref name=thinq11/><ref>[http://www.cio.com.au/article/394473/wolfram_makes_data_interactive/ Wolfram makes data interactive] CIO, 21 July 2011</ref>

==Uses==
Computable Document Format has been used in electronic books by [[Pearson Education]],<ref>[http://www.schoollibraryjournal.com/slj/home/891371-312/wolfram_launches_pdf_killer.html.csp Wolfram launches PDF Killer] School Library Journal</ref><ref>[http://www.pearsonhighered.com/briggscochran1einfo/ Briggs Cochrane Calculus]</ref> to provide the content for the [[Wolfram Demonstrations Project]], and to add client-side interactivity to [[Wolfram Alpha]].<ref>[http://thenextweb.com/apps/2011/08/12/wolfram-alpha-adds-powerful-interactive-search-results/ WolframAlpha adds powerful interactive search results] The Next Web, 12 August 2011</ref><ref>[http://www.pcpro.co.uk/news/enterprise/368815/wolfram-launches-its-own-interactive-document-format Wolfram Launches its own interactive document format] PC Pro, July 2011</ref>

== See also ==
* [[List of numerical analysis software]]
* [[Comparison of numerical analysis software]]

== References ==
{{Reflist}}

== External links ==
* [http://www.wolfram.com/cdf/ Wolfram Research CDF]
* [http://www.wolfram.com/cdf-player/ CDF Player download]

{{Graphics file formats}}
{{Ebooks}} 
{{Wolfram Research}}<!--navbox-->

[[Category:2011 introductions]]
[[Category:Wolfram Research]]
[[Category:Electronic documents]]
[[Category:Open formats]]
[[Category:Page description languages]]
[[Category:Vector graphics]]
[[Category:Computer file formats]]
[[Category:Digital press]]
<=====doc_Id=====>:408
<=====title=====>:
Kune (software)
<=====text=====>:
{{Infobox software
|name                       = Kune
|logo                       = [[File:Kune-logo.svg|frameless|upright]]
|screenshot                 = [[File:Concurrent-edit-and-chat.png|frameless|center]]
|caption                    =
|collapsible                = yes
|author                     = [[Comunes Collective]]
|developer                  = [[Comunes Collective]], IEPALA Foundation
|released                   = {{start date and age|2007}} 
|discontinued               = 
|latest release version     = 1.0.0 (Codename "free-riders")<ref name=release1.0.0>{{cite web|title=Released Kune Version 1.0.0 Codename "free-riders"|url=http://kune.ourproject.org/2015/03/released-kune-version-1-0-0-codename-free-riders/|accessdate=2015-06-23|date=2015-03-18|website=Kune Blog}}</ref>
|latest release date        = {{release date and age|2015|3|18}} 
|latest preview version     =
|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->
|frequently updated         =
|programming language       = Java-based [[Google Web Toolkit]]
|operating system           = 
|platform                   = [[Cross-platform]]
|size                       =
|language                   = Multi-language (more than 10)
|status                     = Active (as of 2015-05)
|genre                      = [[Web application]] [[Collaborative software]] [[Distributed social network]]
|license                    = [[Affero General Public License|AGPLv3]] 
|website                    = {{URL|http://kune.ourproject.org/}} {{URL|https://kune.cc/}}
}}

'''Kune''' is a [[free software|free/open source]] distributed social network focused on collaboration rather than just on communication.<ref name="kune.op.org">{{cite web|title=Kune development site|url=http://kune.ourproject.org|accessdate=3 February 2011}}</ref> That is, it focuses on online [[Collaborative real-time editor|real-time collaborative editing]], [[Distributed social network|decentralized social networking]] and web publishing, while focusing on workgroups rather than just on individuals.<ref>{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboración libre para grupos|url=http://barrapunto.com/article.pl?sid=11/08/21/2240235|language= Spanish|accessdate=28 August 2011|newspaper=Barrapunto (Spanish Slashdot)|date=22 August 2011}}</ref><ref>{{cite news|title=Presentando el proyecto Kune, redes sociales y colaboración libre para grupos|url=http://www.meneame.net/story/presentando-proyecto-kune-redes-sociales-colaboracion-libre|language= Spanish|accessdate=28 August 2011|newspaper=Menéame (Spanish Digg)|date=23 August 2011}}</ref> It aims to allow for the creation of online spaces for collaborative work where organizations and individuals can build projects online, coordinate common agendas, set up virtual meetings, publish on the web, and join organizations with similar interests. It has a special focus on [[Free culture movement|Free Culture]] and [[social movements]] needs.<ref>{{cite web|title=Kune FAQ|url=http://kune.ourproject.org/faq|accessdate=7 July 2012}}</ref><ref>{{cite news
| title       = Das neue Internet
| first       = Niels
| last        = Boeing
| authorlink  = 
| url         = http://www.zeit.de/zeit-wissen/2012/05/Das-alternative-Netz/komplettansicht
| format      = 
| agency      = 
| newspaper   = [[Die Zeit]]
| publisher   = 
| location    = Germany
| isbn        = 
| issn        = 
| oclc        = 
| pmid        = 
| pmd         = 
| bibcode     = 
| doi         = 
| id          = 
| date        = 31 August 2012
| page        = 
| pages       = 
| at          = 
| accessdate  = 5 September 2012
| language    = German
| trans_title = The new internet
| quote       = 
| archiveurl  = 
| archivedate =
| deadurl     =
| ref         = 
}}</ref> Kune is a project of the [[Comunes Collective]].

== Technical details ==
Kune is programmed using the [[Java (programming language)|Java]]-based [[Google Web Toolkit|GWT]] in the client-side, integrating [[Apache Wave]] (formerly [[Google Wave]]) and using mainly the open protocols [[XMPP]] and [[Wave Federation Protocol]]. GWT Java sources on the client side generates [[Code obfuscation|obfuscated]] and deeply optimized [[JavaScript]] conforming a [[single page application]]. Wave extensions (gadgets, bots) run on top of Kune (as in [[Facebook apps]]) and can be programmed in Java+GWT, JavaScript or Python.

The current version has been under development since 2007,<ref name="video2008">{{cite video |people= |date= 26 January 2008|title= Video: Status of Kune development (Jan 2008)|url=http://kune.ourproject.org/2008/01/status-jan08/|format= AVI |medium= |trans_title= |publisher= |location= |archiveurl= |archivedate= |accessdate=28 August 2011|time= |id= |isbn= |oclc= |quote= |ref= }}</ref> with a constant, stable growth and an established codebase.<ref>{{cite web|title=Kune project in Ohloh|url=http://www.ohloh.net/p/kune|author=[[Ohloh]]|accessdate=28 August 2011}}</ref> Nowadays the code is hosted in the GIT of [[Gitorious]],<ref>{{Cite web
|title=Kune repository in Gitorious
|url=https://gitorious.org/kune
| accessdate = 2 September 2012
| author = 
| last = 
| first = 
| authorlink = 
| work = 
| publisher = [[Gitorious]]
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref> it has a development site<ref name="kune.op.org" /> and its main node<ref>{{Cite web
|title=Kune node "Kune.cc"
|url=http://kune.cc
| accessdate = 5 September 2012
| author = 
| last = 
| first = 
| authorlink = 
| work = 
| publisher = Maintained by [[Comunes Collective]]
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref> maintained by the [[Comunes Collective]].

Kune is 100% free software and was built only using free software. Its software is licensed under the [[Affero GPL]] license while the art is under a [[Creative Commons]] BY-SA.

== Philosophy ==

Kune was born in order to face a growing concern from the community behind it. Nowadays, groups (a group of friends, activists, a NGO, a small start-up) that need to work together typically will use multiple [[Free like beer|free (like beer)]] commercial centralized for-profit services (e.g. [[Google Docs]], [[Google Groups]], [[Facebook]], [[Wordpress.com]], [[Dropbox (service)|Dropbox]], [[Flickr]], [[eBay]] ...) in order to communicate and collaborate online. However, "If you're not paying for it, you're the product".<ref>{{cite news|title=If You’re Not Paying for It; You’re the Product|url=http://lifehacker.com/5697167/if-youre-not-paying-for-it-youre-the-product|accessdate=7 July 2012|newspaper=Lifehacker|date=23 November 2010}}</ref> In order to avoid that, such groups of users may ask a technical expert to build them mailing lists, a webpage and maybe to set up an [[etherpad]]. However, technicians are needed for any new list (as they cannot configure e.g. [[GNU Mailman]]), configuration change, etc., creating a strong dependency and ultimately a bottle-neck.<ref>{{cite web|title=Kune 0.0.9 published (codename "15M")|url=http://kune.ourproject.org/2011/08/kune-0-0-9-published-codename-15m/|accessdate=12 April 2012|publisher = Kune Blog|date=4 August 2011}}</ref>

Kune aims to cover all those needs of groups to communicate and collaborate, in an usable way and thus without depending on technical experts
.<ref>{{cite news|title=Software libre, hardware libre, ¿servicios libres?|url=http://libertonia.escomposlinux.org/story/2009/5/27/12014/3120|accessdate=3 February 2011|newspaper=Libertonia News|date=27 May 2009}}</ref> It aims to be a free/libre web service (and thus in [[Internet|the cloud]]), but decentralized as email so a user can choose the server they want and still interoperate transparently with the rest.

Opposite to most distributed social networks, this software focuses on collaboration and building, not only on communication and sharing. Thus, Kune does not aim to ultimately replace Facebook, but also all the above-mentioned commercial services. Kune has a strong focus on the construction of [[Free culture movement|Free Culture]] and eventually facilitate [[Commons-based peer production]].<ref>
{{Cite book
| publisher = IOS Press
| isbn = 9781614990642
|last1= Mass Araya|first1= Elizabeth Roxana |last2= Borsetti Gregorio Vidotti|first2= Silvana Aparecida
|editor1-first=  Ana Alice|editor1-last=Baptista
|editor2-first= Peter|editor2-last= Linde
|editor3-first= Niklas|editor3-last= Lavesson
|editor4-first=Miguel |display-editors = 3 |editor4-last=  Abrunhosa de Brito
| title = Social Shaping of Digital Publishing: Exploring the Interplay Between Culture and Technology
|url= http://www.booksonline.iospress.nl/Content/View.aspx?piid=30613
|chapter= Creative Commons: a Convergence Model Between the Ideal of Commons and the Possibilities of Creation in Contemporary TimesOpposed to Copyright Impediments
| date = 15 July 2012
|accessdate= 19 August 2012
|pages= 3–11
}}</ref>

== History ==
{| class="wikitable" style="float:right; text-align:center; margin-left:1em; margin-right:0"
|-
! rowspan=1 | Version
! rowspan=1 | Code name
! rowspan=1 | Release date
|-
| 0.0.1
| --
| colspan="2" {{Version |o | 2007}}
|-
| 0.0.9
| [[15-M Movement|15M]]
| colspan="2" {{Version |o | 2011-08-04}}
|-
| 0.1.0
| [[We are the 99%|99%]]<ref>{{cite news|title=Kune new release "99%" & production site|url=https://tech.occupy.net/2012/04/24/kune-new-release-99-production-site/|accessdate=9 June 2012|date=24 April 2012|newspaper= #Occupy Tech News}}</ref>
| colspan="2" {{Version |o| 2012-04-13}}
|-
| 0.2.0
| [[Elinor Ostrom|Ostrom]]<ref name=releaseOstrom>{{cite news|title=New release of collaborative distributed social network Kune: "Ostrom"|url=https://tech.occupy.net/2012/11/26/new-release-of-collaborative-distributed-social-network-kune-ostrom/|accessdate=26 November 2012|date=26 November 2012|newspaper= #Occupy Tech News}}</ref>
| colspan="2" {{Version |o | 2012-10-22}}
|-
| 1.0.0
| "Free-riders"<ref name=release1.0.0 />
| colspan="2" {{Version |c | 2015-03-18}}

|-
| colspan="99" | <small>{{Version |l |show=011101}}</small>
|}

The origin of Kune relies on the community behind [[Ourproject.org]]. Ourproject<ref>{{cite news|title=There's Life after Microsoft - Free Software Advocates|url=http://www.ipsnews.net/interna.asp?idnews=22073|accessdate=3 February 2011|newspaper=Inter Press Service News Agency|date=24 January 2004}}</ref> aimed to provide for [[Free culture movement|Free Culture]] (social/cultural projects) what [[Sourceforge]] and other [[software forge]]s meant for [[free software]]: a collection of communication and collaboration tools that would boost the emergence of community-driven free projects.<ref>{{Cite book
| last = Camino
| first = S.
|author2=F. Javier |author3=M. Jiménez Gañán |author4=S. Frutos Cid
 | chapter = Collaborative Development within Open Source Communities
| title =Encyclopedia of Networked and Virtual Organizations
|publisher= IGI Global, Information Science Reference
|isbn = 978-1-59904-885-7
| year = 2008
}}</ref> However, although Ourproject was relatively successful, it was far from the original aims. The analysis of the situation in 2005<ref>{{cite press release
 | title = Towards a new manager of free projects (Hacia un nuevo gestor de proyectos libres)
 | publisher = [[Ourproject.org]]
 | date = 6 December 2005
 | url = http://ourproject.org/moin/Hacia_un_nuevo_gestor_de_Proyectos_Libres
 | accessdate = 22 April 2012
}}</ref> concluded that only the groups that had a [[geek|techie]] among them (who would manage [[GNU Mailman|Mailman]] or install a [[Content Management System|CMS]]) were able to move forward, while the rest would abandon the service. Thus, new free collaborative tools were needed, more usable and suitable for anyone, as the available free tools required a high degree of technical expertise. This is why Kune, whose name means "together" in [[Esperanto]], was developed.

The first prototypes of Kune were developed using [[Ruby on Rails]] and [[Pyjamas (software)|Pyjamas]]. However, with the [[Java (software platform)#Licensing|release of Java]] and the [[Google Web Toolkit]] as free software, the community embraced these technologies since 2007.<ref name="video2008" /> In 2009, with a stable codebase and about to release a major version of Kune,<ref>{{cite news|title=¡Colabora con Kune! Llamado a desarrolladores/as|url=http://www.apesol.org/news/341|publisher=Peru Free Software Association|date=5 May 2009|accessdate=3 February 2011}}</ref> Google announced the [[Google Wave]] project and promised it would be released as free software. Wave was using the same technologies of Kune (Java + GWT, Guice, XMPP protocol) so it would be easy to integrate after its release. Besides, Wave was offering an open federated protocol, easy extensibility (through gadgets), easy control versioning, and very good real-time edition of documents. Thus, the community decided to halt the development of Kune, and wait for its release... in the meanwhile developing gadgets that would be integrated in Kune later on.<ref>{{cite web
 | last =
 | first =
 | authorlink =
 | title = MassMob: Meetings and Smart Mobs 
 | work =
 | publisher = [[Comunes Collective]]
 | year = 2009
 | url = http://massmob.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}</ref><ref>{{cite web
 | last =
 | first =
 | authorlink =
 | title = Troco project: an experimental peer-to-peer currency
 | work = 
 | publisher = [[Comunes Collective]]
 | origyear =2009| year =2010
 | url = http://troco.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}</ref><ref>{{cite web
 | last =
 | first =
 | authorlink =
 | title = Karma: A Reputation Rating System
 | work =
 | publisher = [[Comunes Collective]]
 | origyear = 2009| year = 2010
 | url = http://karma.ourproject.org/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}</ref> In this same period, the community established the [[Comunes Association]] (with an acknowledged inspiration in [[Software in the Public Interest]]) as a non-profit legal umbrella for free software tools for encouraging the [[Commons]] and facilitating the work of [[social movements]].<ref>{{cite interview |last =  |first =  |subjectlink = Interview to [[Comunes Collective]] |interviewer = Serotonina EH |title = |url = http://ondaexpansiva.net/?p=1001  |work = Free Culture Forum 2011 |publisher = Radio Onda Expansiva |location = [[Burgos]], [[Spain]] |date = 9 November 2011 |accessdate =11 April 2012 }}</ref> The umbrella covered Ourproject, Kune and Move Commons,<ref>{{cite news
|title=Move Commons, crowdfunding y etiquetado de proyectos sociales
|url=http://www.misapisportuscookies.com/2011/12/move-commons/
|accessdate=11 April 2012
|newspaper=Mis APIs por tus Cookies
|date=1 December 2012
}}</ref> together with some other minor projects.

In November 2010, the free [[Apache Wave]] (previously Wave-in-a-Box) was released, under the umbrella of the [[Apache Foundation]]. Since then, the community began integrating its source code within the Kune previous codebase,<ref>{{Cite web
| url = http://ecosistemaurbano.org/castellano/move-commons-y-kune-herramientas-libres-para-el-activismo-y-la-colaboracion/
| title = Move Commons & Kune: free tools for activism and collaboration (Move Commons y Kune: herramientas libres para el activismo y la colaboración)
| accessdate = 11 April 2012
| author = 
| last = Toledo
| first = Jorge
| authorlink = 
| date = 14 February 2012
| work = 
| publisher = Ecosistema Urbano
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref> and with the support of the IEPALA Foundation.<ref>{{cite web
 | last =
 | first =
 | authorlink =
 | title = Presenting status of Kune development Jan-2011
 | work =
 | publisher =
 | date = 24 January 2011
 | url =http://kune.ourproject.org/2011/01/status-jan2011/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}</ref> Kune released its Beta and moved to production in April 2012.

Since then, Kune has been catalogued as "activism 2.0"<ref>{{Cite web
| url = https://pilargonzalo.wordpress.com/2011/11/04/activismo-2-0-y-empoderamiento-ciudadano-en-red-i/
| title = Activism 2.0 and citizen empowerment in the net (I) (Activismo 2.0 y empoderamiento ciudadano en red (I))
| accessdate = 11 April 2012
| author = 
| last = Gonzalo
| first = Pilar
| authorlink = 
| date = 4 November 2011
| work = 
| publisher = 
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref> and citizen tool,<ref>{{cite journal|title=Free Knowledge: Collective intelligence for developing free tools and community resources (Conocimiento libre: Inteligencia colectiva para desarrollar herramientas libres y recursos comunitarios)|journal=¡Rebelaos!|year=2012|volume=1|pages=10|accessdate=11 April 2012}}</ref><ref>{{cite news
|title= Cooperation, Collaboration and citizen power (Cooperación, colaboración y poder ciudadano)
|url=http://www.sindikos.com/2012/01/cooperacion-colaboracion-y-poder-ciudadano/
|accessdate=11 April 2012
|newspaper=Sindikos
|date=20 January 2012
}}</ref> a tool for NGOs,<ref>{{Cite web
| url = http://www.democraciaycooperacion.net/contenidos-sitio-web/espanol/fidc/entre-foros/iii-taller-internacional-del/informacion-398/article/las-redes-de-organizaciones
| title = Las redes de organizaciones sociales del CIS generan propuestas para la internacionalización de la acción
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| date = 5 March 2011
| work = 
| publisher = Foro Internacional Democracia y Cooperación
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref><ref>{{Cite report
 | author     = <!-- or |last= and |first= -->
 | authorlink =
 | coauthors  = 
 | date       = February 2012
 | title      = Document Summary of the Rapporteur of Second Regional Workshop Latin America and the Caribbean
 | url        = http://www.democraciaycooperacion.net/IMG/pdf/Summary_Rapporteur_and_context.pdf
 | publisher  = [[Ministry of Foreign Affairs and Cooperation (Spain)]]
 | format     =
 | others     =
 | edition    =
 | location   = [[Mexico City]]
 | chapter    =
 | section    =
 | page       =
 | pages      = 15
 | docket     =
 | accessdate = 12 April 2012
 | quote      =
}}</ref> multi-tool for general purpose<ref>{{Cite web
| url = http://www.contenidosenred.com/blog/kune/
| title = Kune
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| authors = Lucrecia Baquero, Clara Alba
| date = 17 February 2012
| work = 
| publisher = Contenidos en Red
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref> (and following that, criticized for the risk of falling on the [[second-system effect]]<ref>{{Cite web
| url = http://jotarp.org/2011/10/internet/contra-las-redes-sociales.html
| title = Against social networks (Contra las redes sociales)
| accessdate = 11 April 2012
| author = 
| last = Palacios
| first = J. Ramón
| authorlink = 
| date = 24 October 2011
| work = 
| publisher = Jotarp
| pages = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref>) and example of the new paradigm.<ref>{{Cite web
| url = https://semillasdeinnovacion.wordpress.com/2012/03/13/sobre-la-necesidad-de-acercar-la-ciudad-al-campo-y-viceversa/
| title = On the need to bring closer city and country (Sobre la necesidad de acercar la ciudad al campo y viceversa)
| accessdate = 11 April 2012
| author = 
| last = 
| first = 
| authorlink = 
| authors = Lucrecia Baquero, Clara Alba
| date = 13 March 2012
| work = 
| publisher = Semillas de Innovación
| pages = 
| format = 
| quote = 
| archiveurl = 
| archivedate = 
}}</ref> It was selected as "open website of the week" by the [[Open University of Catalonia]]<ref>{{cite news
|title= Open website of the week: Kune
|url=http://mentesabiertas.uoc.edu/webabiertas/webabiertadelasemanakune?lang=en
|accessdate=11 April 2012
|newspaper=Open Minds, [[Open University of Catalonia]] 
|date=5 March 2012
}}</ref> and as one of the [[Occupy movement|#Occupy]] Tech projects.<ref>{{cite web
 | last =
 | first =
 | authorlink =
 | title = #Occupy Tech projects
 | work =
 | publisher =
 | url =https://tech.occupy.net/projects/
 | format =
 | doi =
 | accessdate = 22 April 2012}}</ref> Nowadays there are plans of another federated social network, Lorea (based on [[Elgg (software)|Elgg]]), to connect with Kune.<ref>{{cite news
|title= Radical Community Manager
|url=https://ncomuneszgz.wordpress.com/2012/01/08/radical-community-manager/
|accessdate=11 April 2012
|newspaper=Nociones Comunes
|date=17 March 2012
}}</ref>

<!--
<ref>{{cite web
 | last =
 | first =
 | authorlink =
 | title =
 | work =
 | publisher =
 | url =
 | format =
 | doi =
 | accessdate = }}</ref>

<ref>{{cite press release
 | title =
 | publisher =
 | url =
 | accessdate = }}</ref>

<ref>{{Cite book
| last = Camino
| first = S.
|author2=F. Javier |author3=M. Jiménez Gañán |author4=S. Frutos Cid
 | chapter = Collaborative Development within Open Source Communities
| title =Encyclopedia of Networked and Virtual Organizations
|publisher= IGI Global, Information Science Reference
|isbn = 978-1-59904-885-7
| year = 2008
}}</ref>

<ref>{{Cite journal
| volume = 32
| issue = 3
| pages = 1–1
| last = Machado
| first = H.
|author2=A. Suset |author3=GJ Martín |author4=FR Funes-Monzote
 | title = From the reductionist approach to the system approach in Cuban agriculture: a necessary change of vision
| journal = Pastos y Forrajes
| year = 2009
}}</ref>

<ref>{{cite news
|title=
|url=
|accessdate=11 April 2012
|newspaper=
|date=29 January 2004
}}</ref>

-->

== Feature list ==

* All the functionalities of [[Apache Wave]], that is collaborative federated real-time editing, plus
* Communication
** Chat and chatrooms compatible with Gmail and Jabber through XMPP (with several XEP extensions), as it integrates Emite<ref>{{cite web
 | last =
 | first =
 | authorlink =
 | title = Emite: XMPP & GWT
 | work =
 | publisher =
 | url =http://emite.googlecode.com/
 | format =
 | doi =
 | accessdate = 22 April 2012 }}</ref>
** Social networking (federated)
* Real-time collaboration for groups in:
** Documents: as in [[Google Docs]]
** Wikis
** Lists: as in [[Google Groups]] but minimizing emails, through waves
** Group Tasks
** Group Calendar: as in [[Google Calendar]], with ical export
** Group Blogs
** Web-creation: aiming to publish contents directly on the web (as in [[WordPress]], with a dashboard and public view) (in development)
** Bartering: aiming to decentralize bartering as in [[eBay]]
* Advanced email
** Waves: aims to replace most uses of email
** Inbox: as in email, all your conversations and documents in all kunes are controlled from your inbox
** Email notifications (Projected: replies from email)
* Multimedia & Gadgets
** Image or Video galleries integrated in any doc
** Maps, mindmaps, Twitter streams, etc.
** Polls, voting, events, etc.
** and more via Apache Wave extensions, easy to program (as in [[Facebook apps]], they run on top of Kune)
* Federation
** Distributed Social Networking the same way as e-mail: from one inbox you control all your activity in all kunes, and you can collaborate with anyone or any group regardless of the kune where they were registered.
** Interoperable with any Kune server or Wave-based system
** Chat interoperable with any XMPP server
* Usability
** Strong focus on usability for any user
** Animated tutorials for each tool
** [[Drag and drop|Drag&Drop]] for sharing contents, add users to a doc, change roles, delete contents, etc.
** Shortcuts
* Free culture
** Developed using free software and released under [[Affero General Public License|AGPL]]
** Easy assistant for choosing content licenses for groups. Default license is [[Creative Commons]] BY-SA.
* Developer-friendly
** Debian/Ubuntu package for easy installation
** Wave Gadgets can be programmed in Java+GWT, [[JavaScript]] or [[Python (programming language)|Python]]

== Supporters and adopters ==
Kune has the active support of several organizations and institutions:
* [[Comunes Association]], whose community is behind Kune development. It hosts a Kune server for free projects: [https://kune.cc/#! https://kune.cc]
* IEPALA Foundation,<ref>{{cite web|title=IEPALA Foundation homepage|url=http://www.iepala.es|accessdate=22 April 2012}}</ref> which is supporting the development with economical and technical resources. It hosts a Kune server for [[non-governmental organizations]]: [http://social.gloobal.net "Social Gloobal"] (previously EuroSur).
* Grasia Software Agent Research Group<ref>{{cite web|title=Grasia Research Group homepage|url=http://grasia.fdi.ucm.es/main/|accessdate=22 April 2012}}</ref> of the [[Complutense University of Madrid]] has provided technical resources. It seeks to host a Kune server for academic article collaboration.
* Interns from the Master of Free Software from the [[King Juan Carlos University]] are participating in the development.
* Trainees from the [[American University of Science and Technology]] (Lebanon) participate in the system administration.
* [[Paulo Freire Institute]] in Brazil participated in the early design and prototypes.
* The Kune workgroup of the Medialab Prado<ref>{{cite web|title=Medialab-Prado (Madrid) homepage|url=http://medialab-prado.es|accessdate=22 April 2012}}</ref> are participating in the beta-testing.<ref>{{cite web|title=Comunes profile in Medialab-Prado|url=http://medialab-prado.es/person/comunes|accessdate=22 April 2012}}</ref>

== See also ==
* [[Apache Wave]]
* [[Comunes Collective]]
* [[Distributed social network]]
* [[Comparison of software and protocols for distributed social networking]]
* [[List of AGPL web applications]]
* [[Ourproject.org]]
* [[Wave Federation Protocol]]

== References ==
{{Reflist|2}}

== External links ==
* [https://kune.cc/ Kune.cc main site]
* [http://kune.cc/?locale=de#!kune.wiki.17.1678 Sites using kune]
* [http://kune.ourproject.org Kune information webpage]

{{Cleanup|reason=[[WP:OVERCAT]]|date=May 2016}}

<!--- Categories --->
[[Category:Articles created via the Article Wizard]]
[[Category:Project hosting websites]]
[[Category:Creative Commons-licensed websites]]
[[Category:Collaborative projects]]
[[Category:Virtual communities]]
[[Category:Online communities for social change]]
[[Category:Free groupware]]
[[Category:Free project management software]]
[[Category:Multilingual websites]]
[[Category:Community websites]]
[[Category:Social networking services]]
[[Category:Web applications]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Wikis]]
[[Category:Blog software]]
[[Category:Collaborative real-time editors]]
[[Category:2012 software]]
[[Category:Electronic documents]]
[[Category:Free software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Cross-platform free software]]
[[Category:Internet properties established in 2007]]
[[Category:Software using the GNU AGPL license]]
<=====doc_Id=====>:411
<=====title=====>:
IMail
<=====text=====>:
{{Multiple issues|
{{refimprove|date=February 2011}}
{{notability|date=February 2011}}
{{more footnotes|date=February 2011}}
{{COI|date=February 2011}}
}}

{{lowercase title}}

'''Invisible mail''', also referred to as '''iMail''', '''i-mail''' or '''Bote mail''', is a method of exchanging [[Digital data|digital]] messages from an author to one or more recipients in a secure and untraceable way. It is an open protocol and its java implementation (I2P-Bote) is free and open source software, licensed under the GPLv3.<ref>http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/license.txt</ref>

As with [[email]], one can send and receive iMails. However, normal [[email]]s are visible to an [[ISP]] and to the administrators of the mail servers providing the service. Https, or secure, connections still allow the server admin to view the content of an email and its related IP number. In invisible mails both the mail's content, and the identities (of the sender as well as the receiver) remain unknown to a third party observer or attacker. Furthermore, all iMails are automatically and transparently [[end-to-end principle|end-to-end]] encrypted.

At present, iMail cannot be sent to regular email accounts. iMail addresses are called iMail destinations. They are much longer than the average email addresses and do not carry the "@" sign nor a domain. They already include the encryption key, so using an iMail destination is not harder than using standard email with [[GNU Privacy Guard|gpg]] encryption. The destination is two in one: the "address" as well as the public key. In contrast to gpg- or pgp-encrypted emails, I2P-Bote also encrypts the mail headers.

I2P-Bote also works as an anonymous or pseudonymous remailer. iMails are sent via the [[I2P]] network, a secure and pseudonymous p2p overlay network on the internet and sender and receiver need not be online at the same time ([[store-and-forward]] model). The entire system is serverless and fully distributed. iMail [[Peer-to-peer|peer]]s accept, forward, store and deliver messages. Neither the users nor their computers are required to be online simultaneously; they need connect only briefly for as long as it takes to send or receive messages.

An iMail message consists of three components, the message ''envelope'', the message ''header'', and the message ''body''. The message header contains control information, including, minimally one or more recipient addresses. Usually descriptive information is also added, such as a subject header field and a message submission date/time stamp.

iMails can carry international typesets and have small  multi-media content attachments, a process standardized in [[Request for Comments|RFC]] 2045 through 2049. Collectively, these RFCs have come to be called [[Multipurpose Internet Mail Extensions]] (MIME).

==Features==
* ''secure messages'': All iMail messages are automatically end-to-end encrypted from the sender to the receiver.
* ''message authentication'': All iMail messages that are not sent without any information on the originator are automatically signed and the message's integrity and authenticity is checked by the receiver.
* ''anonymous messages'': iMails can also be sent without any information about the originator.
<ref>http://stats.i2p/cgi-bin/viewmtn/revision/file/cf46b537180b1a5b5740a1e2e85fc049ccc512ef/doc/techdoc.txt</ref>

===Attachment size limitations===
{{Main|Email attachment}}
iMail messages may have one or more attachments. Attachments serve the purpose of delivering binary or text files of unspecified size. In principle there is no technical intrinsic restriction in the I2P-Bote protocol limiting the size or number of attachments. In practice, however, the slow speeds, overheads and data volume due to redundancy limit the viable size of files or the size of an entire message.

===Email spoofing===
{{Main|Email spoofing}}
[[Email spoofing]] occurs when the header information of an email is altered to make the message appear to come from a known or trusted source. In the case of iMails, this is countered by [[cryptography|cryptographically]] signing each iMail with its originator's key.

===Tracking of sent mails===
The I2P-Bote mail service provides no mechanisms for tracking a transmitted message, but a means to verify that it has been delivered, which however does not necessarily mean it has been read.

===Drawbacks===
iMails can only be received or sent via the web interface, there is no implementation of POP3 or SMTP for iMail yet. Furthermore, there are no bridges that allow for sending from I2P-Bote to a standard internet email account or vice versa.

==See also==

===Related services===
* [[Email]]
* [[I2P]]
* [[Data security]]
* [[Email encryption]]
* [[Email client]], [[Comparison of email clients]]
* [[Email hosting service]]
* [[Internet mail standard]]s
* [[Mail transfer agent]]
* [[Mail user agent]]
* [[Unicode and email]]
* [[Webmail]]
* [[Anonymous remailer]]
* [[Disposable email address]]
* [[Email encryption]]
* [[Email tracking]]
* [[Electronic mailing list]]
* [[Mailing list archive]]

===Protocols===
* [[IMAP]]
* [[POP3]]
* [[SMTP]]
* [[UUCP]]
* [[X400]]

==References==
{{reflist}}
* http://i2pbote.i2p/src.zip  (source code)
* http://i2pbote.i2p/history.txt (history.txt)
* http://awxcnx.de/handbuch_55.htm (German Privacy Foundation)
* http://www.unitethecows.com/other-p2p-clients/48940-i2pbote-0-1-2-released.html

==External links==
{{Wiktionary|iMail|email|outbox}}
* http://i2pbote.i2p (I2P-internal)
* http://www.i2p2.de
<!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links -->

{{Computer-mediated communication}}
{{Email clients}}

[[Category:Email]]
[[Category:Internet terminology]]
[[Category:American inventions]]
[[Category:Electronic documents]]
<=====doc_Id=====>:414
<=====title=====>:
DjVu
<=====text=====>:
{{About|a computer file format|a computer-assisted translation software tool|Déjà Vu (software)}}

{{ infobox file format
| icon = Djvu icon.svg
| logo = [[File:DjVu-logo.svg|frameless]]
| screenshot =
| caption =
| extension = .djvu, .djv
| mime = image/vnd.djvu, image/x-djvu
| type code = DJVU
| uniform type =
| magic =
| owner = [[AT&T Labs|AT&T Labs – Research]]
| released = {{start date and age|1998}}
| latest release version = Version 26<ref name=djvuvers>[http://www.djvu.org/forum/phpbb/viewtopic.php?p=952&sid=33819a3f9de6fe1db7870159f47c4dd5 DjVu File Format Version], By Jim Rile, Posted: Fri Feb 23, 2007 1:08 am, PlanetDjVu</ref>
| latest release date = {{start date and age|2006|06}}
| genre = [[Image file formats]]
| container for =
| contained by =
| extended from =
| extended to =
| standard =
| free = GNU GPLv2 for DjVu Reference Library and DjVuLibre-3.5;<br>License grants under the GNU GPL for several patents that cover aspects of the library<ref>{{cite web |title=DjVu Licensing |work=DjVu Sourceforge page |publisher=Sourceforge.net |date=2011-08-17 |url=http://djvu.sourceforge.net/licensing.html |accessdate=2011-09-21}}</ref>
| url = {{url|http://www.djvu.org/}}
}}
'''DjVu''' ({{IPAc-en|ˌ|d|eɪ|ʒ|ɑː|ˈ|v|uː}} {{respell|DAY|zhah|VOO|'}},<ref name="DjVuOverview">{{Cite web|url=http://www.cuminas.jp/en/technology/?src=technology_djvu.aspx|title=DjVu Technology|publisher=Cuminas|accessdate=2014-02-12}}</ref> like {{lang-fr|[[déjà vu]]}} {{IPA-fr|deʒavy|}}) is a [[computer]] [[file format]] designed primarily to store [[image scanner|scanned documents]], especially those containing a combination of text, line drawings, indexed color images, and photographs. It uses technologies such as image layer separation of text and background/images, [[Interlacing (bitmaps)|progressive loading]], [[arithmetic coding]], and [[lossy compression]] for bitonal ([[monochrome]]) images. This allows high-quality, readable images to be stored in a minimum of space, so that they can be made available on the [[World Wide Web|web]].

DjVu has been promoted as an alternative to [[Portable Document Format|PDF]], promising smaller files than PDF for most scanned documents.<ref name="DjVu">{{Cite web|url=http://djvu.org/resources/whatisdjvu.php|title=What is DjVu – DjVu.org|publisher=DjVu.org|accessdate=2009-03-05}}</ref> The DjVu developers report that color magazine pages compress to 40–70&nbsp;kB, black-and-white technical papers compress to 15–40&nbsp;kB, and ancient manuscripts compress to around 100&nbsp;kB; a satisfactory [[JPEG]] image typically requires 500&nbsp;kB.<ref name=djvupaper>{{cite journal |author1=Léon Bottou |author2=Patrick Haffner |author3=Paul G. Howard |author4=Patrice Simard |author5=Yoshua Bengio |author6=Yann Le Cun |title=High Quality Document Image Compression with DjVu, 7(3):410–425|publisher=Journal of Electronic Imaging |year=1998 |url=http://leon.bottou.org/publications/pdf/jei-1998.pdf}}</ref> Like PDF, DjVu can contain an [[Optical character recognition|OCR]] text layer, making it easy to perform [[copy and paste]] and text search operations.

Free browser plug-ins and desktop viewers from different developers are available from the djvu.org website. DjVu is supported by a number of multi-format document viewers and e-book reader software on Linux ([[Okular]], [[Evince]]) and Windows ([[SumatraPDF]]).

== History ==
The DjVu technology was originally developed<ref name=djvupaper/> by [[Yann LeCun]], [[Léon Bottou]], Patrick Haffner, and Paul G. Howard at [[AT&T Labs]] from 1996 to 2001.

Due to its declared higher compression ratio (and thus smaller file size) and the ease of converting large volumes of text into DjVu format, and because it is an [[open file format]], some independent technologists (such as [[Brewster Kahle]]<ref name="Kahle2005">{{Cite web|url=http://itc.conversationsnetwork.org/shows/detail400.html|author=Brewster Kahle|title=Universal Access to All Knowledge|format=Audio; Speech at 1h:31m:20s|publisher=Conversations Network|date=December 16, 2004}}</ref>) have historically considered it superior to [[PDF]].

The DjVu library distributed as part of the open-source package ''DjVuLibre'' has become the reference implementation for the DjVu format. DjVuLibre has been maintained and updated by the original developers of DjVu since 2002.<ref>http://djvu.sourceforge.net/</ref>

The DjVu file format specification has gone through a number of revisions:

{| class="wikitable sortable"
|+ Revision history
! Support status
! Version
! Release date
! Notes
|-
| Unsupported
| 1–19<ref name=djvuvers />
| 1996–1999
| Developmental versions by AT&T labs preceding the sale of the format to [[LizardTech]].
|-
| Unsupported
| Version 20<ref name=djvuvers /><!--keep the word ''version'' as it is part of the name-->
| April 1999
| DjVu version 3. DjVu changed from a single-page format to a multipage format.
|-
| Older, still supported
| Version 21<ref name=djvuvers />
| September 1999
| Indirect storage format replaced. The searchable text layer was added.
|-
| Older, still supported
| Version 22<ref name=djvuvers />
| April 2001
| Page orientation, color JB2
|-
| Unsupported
| Version 23<ref name=djvuvers />
| July 2002
| CID chunk
|-
| Unsupported
| Version 24<ref name=djvuvers />
| February 2003
| LTAnno chunk
|-
| Older, still supported
| Version 25<ref name=djvuvers />
| May 2003
| NAVM chunk. Support for DjVu bookmarks (outlines) was added. Changes made by Versions 23 and 24 were made obsolete.
|-
| Current
| Version 26<ref name=djvuvers />
| April 2005
| Text/line annotations
|-
|}

== Technical overview ==

=== File structure ===
The DjVu file format is based on the [[Interchange File Format]] and is composed of hierarchically organized chunks. The IFF structure is preceded by a 4-byte <code>AT&T</code> [[magic number (programming)|magic number]]. Following is a single <code>FORM</code> chunk with a secondary identifier of either <code>DJVU</code> or <code>DJVM</code> for a single-page or a multi-page document, respectively.

==== Chunk types ====
{| class="wikitable"
|-
! Chunk identifier !! Contained by !! Description
|-
| FORM:DJVU || FORM:DJVM || Describes a single page. Can either be at the root of a document and be a single-page document or referred to from a <code>DIRM</code> chunk. 
|-
| FORM:DJVM || {{n/a}} || Describes a multi-page document. Is the document's root chunk.
|-
| FORM:DJVI || FORM:DJVM || Contains data shared by multiple pages.
|-
| FORM:THUM || FORM:DJVM || Contains thumbnails.
|-
| INFO || FORM:DJVU || Must be the first chunk. Describes the page width, height, format version, DPI, gamma, and rotation.
|-
| DIRM || FORM:DJVM || Must be the first chunk. References other <code>FORM</code> chunks. These chunks can either follow this chunk inside the <code>FORM:DJVM</code> chunk or be contained in external files. These types of documents are referred to as ''bundled'' or ''indirect'', respectively.
|-
| NAVM || FORM:DJVM || If present, must immediately follow the <code>DIRM</code> chunk. Contains a BZZ-compressed outline of the document.
|}

=== Compression ===
DjVu divides a single image into many different images, then compresses them separately. To create a DjVu file, the initial image is first separated into three images: a background image, a foreground image, and a mask image. The background and foreground images are typically lower-resolution color images (e.g., 100 dpi); the mask image is a high-resolution bilevel image (e.g., 300 dpi) and is typically where the text is stored. The background and foreground images are then compressed using a [[Wavelet compression#Wavelet compression|wavelet-based compression]] algorithm named IW44.<ref name=djvupaper/> The mask image is compressed using a method called JB2 (similar to [[JBIG2]]). The JB2 encoding method identifies nearly identical shapes on the page, such as multiple occurrences of a particular character in a given font, style, and size. It compresses the bitmap of each unique shape separately, and then encodes the locations where each shape appears on the page. Thus, instead of compressing a letter "e" in a given font multiple times, it compresses the letter "e" once (as a compressed bit image) and then records every place on the page it occurs.

Optionally, these shapes may be mapped to [[UTF-8]] codes (either by hand or potentially by a [[Text recognition|text recognition system]]), and stored in the DjVu file. If this mapping exists, it is possible to select and copy text.

Since JBIG2 was based on JB2, both compression methods have the same problems when performing lossy compression.  Numbers may be substituted with similar looking numbers (such as replacing 6 with 8) if the text was scanned at a low DPI prior to lossy compression.

== Format licensing ==
DjVu is an [[open file format]] with patents.<ref name="DjVu"/> The file format specification is published, as well as source code for the reference library.<ref name="DjVu"/> The original authors distribute an [[Open-source software|open-source]] implementation named "''DjVuLibre''" under the [[GNU General Public License]]. The rights to the commercial development of the encoding software have been transferred to different companies over the years, including [[AT&T Corporation]], [[LizardTech]], ''Celartem'' and ''Cuminas''.

==Support==
[[SumatraPDF]] (Windows) among others can manipulate DjVu files.

In 2002, the DjVu file format was chosen by the [[Internet Archive]] as a format in which its ''[[Million Book Project]]'' provides scanned [[public domain]] books online (along with [[TIFF]] and PDF).<ref>{{Cite web|url=http://wiki.laptop.org/go/DJVU |title=Image file formats – OLPC |publisher=Wiki.laptop.org |date= |accessdate=2008-09-09}}</ref>

[[Wikimedia Commons]], a media repository used by [[Wikipedia]] among others, conditionally permits PDF and DjVu media files.<ref>[[commons:Commons:Scope#PDF and DjVu formats|PDF and DjVu]]</ref>

== See also ==
* [[JBIG2]]
* [[Comparison of e-book formats]]

== References ==
{{reflist}}

== External links ==
{{Commons category|DjVu}}
* [http://djvu.org/ "The premier menu for DjVu resources"] (status of the site, which is maintained by an anonymous webmaster, is unclear)
* [http://djvu.sourceforge.net/ DjVuLibre site]
* [http://jwilk.net/software/ Jakub Wilk's pdf2djvu and other DjVu tools]
* [https://bitbucket.org/jsbien/ndt/wiki/wyniki Poliqarp for DjVu search engine and other DjVu tools]
* [http://djvu.org/forum/phpbb/viewtopic.php?t=146 Why won't Google index DjVu files after all this time?] – topic on  PlanetDjVu
* [http://any2djvu.djvuzone.org Any2Djvu Server - online document converter]
* [https://www.cuminas.jp/en/downloads Cuminas Software Downloads]
* [http://www.djvu-soft.narod.ru/soft/ Table of Djvu Programmes (Russian)]

{{Office document file formats}}
{{Graphics file formats}}

[[Category:1998 introductions]]
[[Category:Computer file formats]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Filename extensions]]
[[Category:Graphics file formats]]
[[Category:Office document file formats]]
[[Category:Open formats]]
<=====doc_Id=====>:417
<=====title=====>:
Teleadministration
<=====text=====>:
'''Teleadministration''' is based on the concept that documents in electronic format have legal value. Administrative informatics is not new, but for many years it was merely Information Technology applied to legal documents, that is, the reproduction of paper-based legal documents into electronic file systems. Instead, Teleadministration turns this approach into its head. It is based on research conducted in 1978, the year when, at a conference promoted by the [[Court of Cassation (Italy)|Court of Cassation]], [[:it:Giovanni Duni|Giovanni Duni]] launched the then-futuristic idea that an electronic document could have legal value.<ref>1. Duni, G., L'utilizzabilità delle tecniche elettroniche nell'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss.</ref> 1978 was also the year in which the first research on digital signatures ([[RSA (cryptosystem)|RSA]])<ref>2. Rivest, Shamir e [[Leonard Adleman|Adleman]], A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, febbraio 1978, 120-126. This research referred to the asymmetric encryption technology (Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss. Diffie and Hellman’s research was disseminated in Italy by  Gardner, “Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato”, in Le Scienze, December 1977, 126 ss.), who added the regulation for issuing the keys and the public certification process associated to them.</ref> was published in the United States, yet it would take more than twenty-five years for jurists and mathematicians to start working together.<ref>3. The first application of the research by Rivest, Shamir and Adleman was the 1995 Utah Code, § from 46-3-101 to 46-3-504 (Enacted by l. 1995, ch. 61). The Utah code was analysed in the brilliant dissertation written by Francesca Flora, Evoluzione della informatica nel sistema di governo degli Stati Uniti d’America (Cagliari, dept. Of Politiacl Science, November 1996). For application at the federal level one had to wait until 1998: US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. — US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998.</ref>

For many years, and even before 1978, IT helped [[Public Administration]] but kept a “safe distance”, assuming that the ‘sacred nature’ of the Law demanded the use of pen and paper. Information Technology merely managed and filed copies of legal documents: it was known as “parallel IT”,<ref>4. Duni, G., Amministrazione digitale, Voce della Enciclopedia del diritto, Annali, I, Milano 2007, p. 13-49.</ref> since it was an accessory to the activity with formal value, the one based on pen and paper.

Thus, the logical, legal and material premise of Teleadministration is the conferment of legal value to IT documents.

== Origins and terminology  ==
In Italy, the linguistic expression <ref>5. [http://www.treccani.it/enciclopedia/teleamministrazione_(Lessico-del-XXI-Secolo) Teleamministrazione, Lessico del XXI secolo], Treccani</ref> teleamministrazione was first used in 1991 at the Roman ‘La Sapienza’ university, during a conference organised by the Court of Cassation,<ref>6. Duni, G., Il progetto nazionale di teleamministrazione pubblica, in “L’informatica giuridica e il Ced della Corte di Cassazione”, proceedings of the conference held at the Univ. of Rome “La Sapienza”, 27-29 Nov. 1991, Milan 1992, p. 87 ss.</ref> in which it was said that: «the new system of administrative information technology is called “teleadministration” because all the work of the [[Public Administration]] will be carried out through devices, that could also be computers, linked to the central server through a network.» Teleadministration was indeed considered a type of teleworking.<ref>7. Applicazioni della multimedialità nella P.A.: teleamministrazione e telelavoro”, in Funzione Pubblica, special issue “I convegni di FORUM P.A. ’96”, volume I, p. 105.</ref>

With Teleadministration, amministrative procedures become electronic administrative procedures and, more specifically, those that are initiated by a party realize the electronic One Stop Shop.

== The fundamentals of teleadministration ==
In the decades from 1970 to 1990, the [[Supreme Court of Cassation (Italy)|Court of Cassation]] was at the core of research on the relationship between IT and Law, organising international conferences every five years on the topic. The 1993 international conference featured the fundamentals of teleadministration, providing the details of the administrative systems behind the One Stop Shop concept:<ref>8. Duni, G., presentation at the 5th International Congress at the Court of Cassation on “IT and Legal Activity” Rome, 3–7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p. 381 ss.</ref>
# A citizen presents his/her claim to one administration, which then manages the entire procedure.
# A single “administrative file” is created, no matter how many different administrations may be involved.
# For both internal and external stages in an administrative procedure, a "warning signal" is sent telematically to the relevant office where the next stage is due, the employee in that office then becomes responsible for that phase of the procedure.
# Any information concerning records already held by public administration is accessed telematically, without involving the citizen. 
# The (electronic) signature identifies the identity of the operator through sophisticated techniques.
# The original of an administrative act is electronic and is therefore always available telematically to any administration that may need it. 
# The presence of an increasing amount of on-line data will necessitate greater use of automatic data processing in decision-making.
# Saving data on multiple memory locations will guarantee the safekeeping of the acts. 
# Statistical data will be available in real time and under multiple profiles, with great benefits for top level decision-making.
# Private citizens can obtain paper copies of the electronic acts.
It should be noted that the 5th fundamental mentions electronic rather than digital signatures.  This is because, in the jurists’ domain, digital signatures were not yet known. The generic reference to the electronic signature is however valid, and its general nature is actually suitable for the rules contained six years later in the Directive 1999/93/EC.
As we will see, the Directive refers in particular to all procedures initiated by private citizens, but the system remains valid for all procedures initiated by the administration offices as well.

== Acknowledgement of the principles in current law ==
The legally accepted form of acts and documents evolved according to the following stages:
# Acts only exist in paper form
# Acts in electronic format are a possible option
# Electronic format is compulsory, safe for a few exceptions
In Italy, Phase 2 was launched by Art 15, para, 2,  Law N. 59 of 15 March 1997, (the so-called ‘Bassanini 1’ law: it established the legal value of electronic documents, while regulations would establish the authentication criteria). The EC intervened later with its Directive 1999/93/EC of the [[European Parliament]] and the [[Council of the European Union|Council]] of 13 December 1999 (Transposed by Law Decree N.10 of 23 January 2002), which imposes an obligation on Member States to give legal value to documents with digital signatures (not directly named as such, but all their features are described in the directive). It also establishes that electronic documents should not be rejected a priori, hence opening to a range of different solutions to establish the authorship of a document (the so-called ‘weak signatures’).

The 1993 Directive was revoked and absorbed (for reasons of legal certainty and clarity) by Regulation 910/2014 of the European Parliament and Council of 23 July 2014 (also known as eIDAS regulation) in the OJEU 28 August 2014, which did not renege on the principle of also accepting the so-called ‘weak signatures’.
In Italy, The move to Phase 3 was established by Art.40 of Legislative sl. Decree N.82 of 7 March 2007, Code of Digital Administration (CAD), entitled “Creation of electronic documents”, which states: “Public Administrations make the original copy of their documents with electronic means, according to the provisions of the present Code and the technical specifications of Article 71”. Exceptions are extremely rare: Comma 3 states: by means of appropriate regulations…. , proposed by the delegated Ministers for Public Functions, Innovation and Technology and the Minister for Cultural Heritage and Activities, the categories of administrative documents that can be created on paper in original are identified, having regard to the special historical and archive value they will have by nature” (think, for example, of the resignation of a President of the Republic).

Unfortunately, national administrations are ignoring this provision, and today it is only private companies that are no longer allowed paper-based communication with public administrations (Art. N. 5 bis of the CAD and D.P.C.M. 22 July 2011); compulsory electronic invoicing was added on 31 March 2015 by Law N. 44 of 24 December 2007, Art. N. 1, para 209-214 implemented by Ministerial  Decree N.55 of 3 April 2013, further clarified by Ministerial circular N.1 of 9 March 2015.
The modernisation of procedures was also touched by Presidential Decree N. 447 of 20 October 1998 (creation of the One Stop Shop, but only for production activities, and paper based), while interest for a telematic procedure only began with Legislative Decree N. 82 of 7 March 2005, CAD, which is not as relevant in its first version but was later modified by several interventions, particularly Legislative Decree N. 335 of 10 December 2013.

European sources are also essential. The EC, and later the European Union, have undertaken a wide range of actions on e-government: one of the most important was the launch of the IDABC programme (and financing) for Interoperable Delivery of European eGovernment Services to public Administrations, Business and Citizens, via Decision 2004/387/EC of the European Parliament and Council of 21 April 2004. However, the ultimate acknowledgement of the principles of teleadministration, with the telematic One Stop Shop, is contained in the Directive 2006/123/EC of the European Parliament and Council of 12 December 2006, on the Internal market for services, which provides for Member States to set up an electronic One Stop Shop in the wide field of administrative procedures.

== Teleadministration and the ‘star’ procedure ==
This article is not meant to argue the great effect that teleadministration has on the efficiency of administrative activity, as we assume that the reader is fully aware that, once paper based documents are abandoned, the real-time flow of documents greatly improves time management and responsibility of the single offices/ operators, while direct online access improves transparency. Rather, this paragraph wants to emphasize how teleadministration promotes maximum usage of the “star procedure”, known and researched in Germany as “Sternverfahren”. This procedure, an alternative to the sequential procedure, which has by nature longer head times, in the paper-based world would require making several copies of the administrative file (which can be extremely voluminous) for each office and each administration that needs to express an opinion or issue an authorisation. With the One Stop Shop, the administration initiating the process is charged with this task. Electronic files clearly provide evident benefits for these procedures, since all involved administrations can directly and simultaneously access the file, view the part they need to evaluate and add their opinion or authorisation directly, using a star-shaped scheme.

== Assessing the actual acceptance of teleadministration in current law and real life ==
As a scientific proposition, teleadministration sketched the system of telematic administrative procedures well ahead of the law, and particularly the electronic One Stop Shop concept. Both concepts are based on the dematerialisation of documents and on telematic administrative work.

The concept of documents’ dematerialisation,<ref>9. According to some commentators “dematerialization” is not the appropriate term for documents that are created in electronic form, but rather for those that are created in paper form and are later converted in digital format. Though conceptually correct, this observation ignores the fact that the expression now is generally understood to mean any “form that does without the material presence of paper, from the creation of the document”</ref> in existence since 1978 as a scientific notion,<ref>10. See note. 1.</ref> was first embraced in Italy (Law N. 59 of 15 March 1997, Art. 15, para 2) and later by the E.C. in Directive 1999/93/EC.

Once the principle that an electronic document can have legal value was accepted, it was possible to deal with its management within a telematic procedure.
As mentioned, configuring this procedure within the rules of teleadministration is today accepted in both European and Italian laws. European laws also provide quite a detailed description of the electronic One Stop Shop, with rules that fit nicely within the scientific rules of teleadministration; their main limitation is that they were specifically designed for the free circulation of services within Europe, and hence for the procedures these require. It is the above-mentioned 1996/123/EC Directive, whose Art.6 establishes the One Stop Shop and Art. 8 provides that it should be managed “remotely and electronically”, leaving further details to the Commission. And indeed the Commission, with its Decision of 16 October 2009, provided a number of measures to facilitate the use of electronic procedures through the “One Stop Shop” under Directive 2006/123/EC. These sources are clear and they apply to a wide-ranging sector: the problem is that any sector or procedure that is not related to the supply of services within the Union is not regulated, and Member States are therefore able to carry on with old-fashioned paper-based procedures.

In light of this limitation, a group of illustrious European Law academics, coordinated by Giovanni Duni, has drafted the most effective text for a Directive providing a universal system of telematic administrative procedure.<ref>11. The outcome of this collective research effort is the text for a draft Directive, that can be found on the CNR ITTIG journal  Informatica e diritto, Vol. XXI, 2012, N. 2, pp. 113-129: The telematic procedures in the European Union. Introducing a draft Directive, as well as  on line, with an Italian and an English language version of the draft on the site [http://www.teleamministrazione.it www.teleamministrazione.it]</ref>

Italian sources are based on the Code of Digital Administration, the above-mentioned Law Decree N.82 of 7 March 2005 in its current version, following several modifications, which (if correctly interpreted and implemented) should make it compulsory for all public administrations to use teleadministration, thus making the telematic administrative procedure the default procedural method. Art. 14, the key provision, establishes that the proceeding administration creates an electronic file, to which all involved administrations can and should have access, and feed it with the acts of their competence. Private citizens can also access it under Law 241/90
Thus the electronic file is the technical and organizational specification of the telematic administrative procedure, as it is clear that its creation is an operative stage of the procedure and not simply a new filing system for the archives.

Art. 10 of the CAD appears at first to be at odds with this interpretation, since it establishes that the One Stop Shop for productive activities provides its services electronically but leaves doubts about the possibility that the back office activities could be still paper-based. However, if Art. 10 and Art. 41 are interpreted together, the only possible conclusion is that the former is a clarification of front office activities, but all the administrative activity is based on the general rule of electronic files, and therefore on the teleadministration and the One Stop Shop. 
Compared to European and Italian law, reality is somewhat behind. Eight years after Directive 1996/123/CE, there would be grounds for an infraction procedure against Italy. But since Italy enjoys the company of several other non-compliant Member States, they are all ‘safe’ for the time being.

Though the letter of the CAD may be not be respected, it seems very unlikely that this may determine the invalidity or nullity of the acts for violation of Art. 41(electronic file) or Art. 40 (statutory requirement of digital signature), because in front of a claim of this nature, the administrative judge would apply Artt. 21 septies and 21 octies of Law 241/90. The claimant should demonstrate that the use of the electronic format and electronic file wold have led to a different outcome.

== Legal sources ==

=== US Law ===
1995 Utah Code, paras 46-3-101 to 46-3-504 (Enacted by Law 1995, Ch. 61). US Senate, S. 1594, Digital Signature and Electronic Authentication Law (SEAL) of 1998. — US House of Representatives, H.R. 3472, Digital Signature and Electronic Authentication Law (SEAL) of 1998

=== EU Law ===
*European Parliament and Council Directive 1999/93/EC of 13 December 1999, revoked and absorbed by European Parliament and Council Regulation 910/2014 of 23 July 2014, (known as the eIDAS regulation) in OJEU 28 August 2014. 
*European Parliament and Council Decision 2004/387/EC of 21 April 2004 on the interoperable delivery of European eGovernment services
*European Parliament and Council Directive 2006/123/EC of 12 December 2006, on internal market services, providing for Member States to establish the electronic One Stop Shop in this vast field of administrative procedures. 
*Decision of 16 October 2009, establishing measures to facilitate electronic procedures through the «One Stop Shop» under Directive 2006/123/CE 
The telematic procedures in the European Union. Introducing a draft Directive, research coordinated by Duni, G., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, n. 2, pp.&nbsp;113–129 and in www.teleamministrazione.it.

=== Italian Law ===
*l. 15 March 1997, N. 59, art. 15, para, 2. 
*D. lg. 23 January 2002, N. 10. 
*D.P.R. 20 October 1998, N. 447 
*D. legils. 7 March 2005, N. 82, codice dell'amministrazione digitale (CAD) 
*D.P.C.M. 22 July 2011 
*D. Legisl 10 December 2013, N. 335

== Bibliography ==
*Contaldo, A., La teleamministrazione con reti transnazionali europee come strumento per l'integrazione delle Pubbliche Amministrazioni dei paesi dell'Unione Europea, in Riv. trim. diritto amministrativo, I, 2004, p.&nbsp;95 and later 
*Diffie and Hellman, New directions in Cryptography, in IEEE Transaction on Information Theory, November 1976, 644 ss 
*Duni, G., L'utilizzabilità delle tecniche elettroniche nell'emanazione degli atti e nei procedimenti amministrativi. Spunto per una teoria dell'atto amministrativo emanato nella forma elettronica, in "Rivista amm. della Repubblica italiana", 1978, pag.407 ss. — Il progetto nazionale di teleamministrazione pubblica, in “L’informatica giuridica e il Ced della Corte di Cassazione”, proceedings of the conference held at Univ. of Rome “La Sapienza”, 27-29 Nov. 1991, Milan 1992, p.&nbsp;87 ss. — La teleamministrazione: una “scommessa” per il futuro del Paese, presentation at the 5th International Congress at the Court of Cassation on “IT and Legal Activity” Rome, 3–7 May 1993, I.P.Z.S. - Libreria dello Stato, 1994, II, p.&nbsp;381 ss. — Amministrazione digitale, Item under the Enciclopedia del diritto, Annali, I, Milan 2007, p.&nbsp;13-49 — L’amministrazione digitale. Il diritto amministrativo nell’evoluzione telematica, Giuffrè 2008. 
*Flora, F., Evoluzione della informatica nel sistema di governo degli Stati Uniti d’America. Dissertation, Cagliari, Dept. Of Political Science, November 1996. 
*Gagliotti, A., Teleamministrazione e concorsi pubblici, in Giustizia amministrativa n. 3/2003, http://www.giustamm.it/ago1/articoli/gaglioti_teleamministrazione.htm#_ednref5 
*Gardner, Un nuovo tipo di cifrario che richiederebbe milioni di anni per essere decifrato, in Le Scienze, December 1977, 126 ss. 
*Masucci, Informatica pubblica, in Dizionario di diritto pubblico directed by S. Cassese, IV, Milan, 2006, 3115 ss.; 
*Notarmuzi, Il codice dell’amministrazione digitale, in Astrid Rassegna, www.astrid-online.it, 2006, n. 12; Id., Il procedimento amministrativo informatico, ivi, n. 16; 
*Osnaghi, Firme elettroniche e documento informatico: il codice richiede ulteriori integrazioni, ivi, n. 10; 
*Rabbito. c., L'informatica al servizio della pubblica amministrazione. Dai principi della teleamministrazione ai piani di e-government, Gedit, 2007. 
*Rivest, Shamir e Adleman, A method for obtaining digital signature and public key cryptosystems, in Communications of the ACM, vol. 21, February 1978, 120-126 
*The telematic procedures in the European Union. Introducing a draft Directive, ricerca coordinata da Duni, g., in CNR ITTIG Informatica e diritto, Vol. XXI, 2012, N. 2, pp.&nbsp;113–129 and in www.teleamministrazione.it. 
*Applicazioni della multimedialità nella P.A.: teleamministrazione e telelavoro”, in Funzione Pubblica, special issue “I convegni di FORUM P.A. ’96”, volume I, p.&nbsp;105.

==See also==
*[[Digital era governance]]
*[[Electronic document]]
*[[Electronic paper]]
*[[Paperless office]]
*[[Bureaucrat]]
*[[E-Government Act of 2002]]
*[[E-government]]
*[[Public administration]]

==References==
<references/>

[[Category:E-government]]
[[Category:Administrative law]]
[[Category:Public-key cryptography]]
[[Category:Electronic documents]]
[[Category:Public administration]]
<=====doc_Id=====>:420
<=====title=====>:
TheSwizzle.com
<=====text=====>:
{{Multiple issues|
{{COI|date=August 2013}}
{{refimprove|date=August 2013}}
}}
{{Infobox dot-com company
| name   = TheSwizzle
| logo         = 
[[File:TheSwizzle.com Logo.jpg|150x50px|TheSwizzle.com Logo]]
| company_type   = [[Privately held company|Private]]
| location_city    = [[New York City]],<ref>{{cite web|url=http://www.theswizzle.com |title=TheSwizzle |accessdate=2012-11-19}}</ref> [[New York (state)|New York]]
| location_country = USA
| foundation = 2010
| founder                   = [[Scott Kurnit]]
| registration              = Optional
| current_status            = Inactive
| industry       = [[advertising]], [[online advertising]], [[email]]
| homepage       = [http://www.theswizzle.com/ www.theswizzle.com]
}}
'''TheSwizzle''' was a [[webmail]] tool that worked with existing email and enabled consumers to manage email subscriptions, primarily from commercial vendors.<ref>{{cite web |accessdate=November 19, 2012 |url=http://mashable.com/2012/10/03/swizzle-emails |title=The Swizzle Cleans Your Inbox By Combining Promo E-mails Into a Daily Digest |publisher=Mashable |date=October 25, 2012 |author=Veena Bissram }}</ref><ref>{{cite web |accessdate=November 19, 2012 |url=http://revision3.com/tzdaily/swizzle-junk-email |title=Clean Junk Mail From Your Inbox! |publisher=revision3 |date=October 25, 2012 |author=Veronica Belmont }}</ref><ref>{{cite web |accessdate=November 19, 2012 |url=http://www.pcmag.com/article2/0,2817,2411068,00.asp |title=Swizzle |publisher=pcmag |date=October 17, 2012 |author=Samara Lynn }}</ref> It was acquired by Mailstrom of 410 Labs in September 2014, and TheSwizzle.com subsequently shut down.<ref>{{Cite news|url=http://technical.ly/baltimore/2014/09/11/mailstrom-the-410-labs-email-helper-lands-ex-competitors-users-swizzle/|title=Mailstrom, the 410 Labs email helper, lands ex-competitor's users - Technical.ly Baltimore|date=2014-09-11|newspaper=Technical.ly Baltimore|language=en-US|access-date=2016-12-20}}</ref>

==Features==
The product claims several features, including cleaning up users' inboxes by helping to unsubscribe from unwanted emails while at the same time, allowing receipt as well as searching among those commercially oriented emails an individual still wants to receive. By packaging these messages into a digest format, users can consolidate their email box.

==History==
The Swizzle is a product of Keep Holdings, a consumer and brand engagement conglomerate of business units including Keep.com, [[AdKeeper]] and TheSwizzle.com.
The company was founded in March, 2010,<ref>{{cite web |accessdate=January 17, 2010 |url=http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=114604562 |title=AdKeeper, Inc. Snapshot |publisher=Bloomberg Businessweek |author=Staff }}</ref> by [[Scott Kurnit]], who serves as Chairman and CEO. Kurnit is best known as the founder of [[About.com]], which grew to a public market value of $1.7 billion, and was sold to Primedia for $724 million, in 2001. About.com is now owned by [[IAC (company)|IAC]].

==See also==
* [[Scott Kurnit]]
* [[AdKeeper]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.theswizzle.com/}}

{{DEFAULTSORT:Swizzle}}
[[Category:Electronic documents]]
[[Category:Internet properties established in 2010]]
<=====doc_Id=====>:423
<=====title=====>:
Digital signature
<=====text=====>:
A '''digital signature'''  is a mathematical scheme for demonstrating the authenticity of digital messages or documents. A valid digital signature gives a recipient reason to believe that the message was created by a known sender ([[authentication]]), that the sender cannot deny having sent the message ([[non-repudiation]]), and that the message was not altered in transit ([[Data integrity|integrity]]).

Digital signatures are a standard element of most [[cryptographic protocol]] suites, and are commonly used for software distribution, financial transactions, [[contract management software]], and in other cases where it is important to detect forgery or tampering.

== Explanation ==
Digital signatures are often used to implement [[electronic signature]]s, a broader term that refers to any electronic data that carries the intent of a signature,<ref>[http://frwebgate.access.gpo.gov/cgi-bin/getdoc.cgi?dbname=106_cong_public_laws&docid=f:publ229.106.pdf US ESIGN Act of 2000]</ref> but not all electronic signatures use digital signatures.<ref>[http://enterprise.state.wi.us/home/strategic/esig.htm State of WI]</ref><ref>[http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html National Archives of Australia] {{webarchive |url=https://web.archive.org/web/20141109/http://www.naa.gov.au/recordkeeping/er/Security/6-glossary.html |date=November 9, 2014 }}</ref> In some countries, including the United States, [[Turkey]], [[India]], Brazil, Indonesia, Saudi Arabia,<ref>{{cite book|first=Government of India|title=The Information Technology Act, 2000|url=http://www.dot.gov.in/sites/default/files/itbill2000_0.pdf}}</ref> [[Switzerland]] and the countries of the [[European Union]],<ref name=Cryptomathic_MajorStandardsDigSig>{{cite web|last1=Turner|first1=Dawn|title=Major Standards and Compliance of Digital Signatures - A World-Wide Consideration|url=http://www.cryptomathic.com/news-events/blog/major-standards-and-compliance-of-digital-signatures-a-world-wide-consideration|publisher=Cryptomathic|accessdate=7 January 2016}}</ref><ref name=CryptomathicDigSigServicesAshiqJA>{{cite web|last1=JA|first1=Ashiq|title=Recommendations for Providing Digital Signature Services|url=http://www.cryptomathic.com/news-events/blog/recommendations-for-providing-digital-signature-services|publisher=Cryptomathic|accessdate=7 January 2016}}</ref> electronic signatures have legal significance.

Digital signatures employ [[asymmetric key algorithm|asymmetric cryptography]]. In many instances they provide a layer of validation and security to messages sent through a nonsecure channel: Properly implemented, a digital signature gives the receiver reason to believe the message was sent by the claimed sender.  Digital seals and signatures are equivalent to handwritten signatures and stamped seals.<ref>[http://www.arx.com/industries/engineering/regulatory-compliance/ Regulatory Compliance: Digital signatures and seals are legally enforceable ESIGN (Electronic Signatures in Global and National Commerce) Act]</ref> Digital signatures are equivalent to traditional handwritten signatures in many respects, but properly implemented digital signatures are more difficult to forge than the handwritten type. Digital signature schemes, in the sense used here, are cryptographically based, and must be implemented properly to be effective. Digital signatures can also provide [[non-repudiation]], meaning that the signer cannot successfully claim they did not sign a message, while also claiming their [[private key]] remains secret; further, some non-repudiation schemes offer a time stamp for the digital signature, so that even if the private key is exposed, the signature is valid. Digitally signed messages may be anything representable as a [[bitstring]]: examples include [[electronic mail]], [[contract]]s, or a message sent via some other [[cryptographic protocol]].

==Definition of Digital Signature==
{{Main article|Public-key cryptography}}
A digital signature scheme typically consists of three algorithms;
* A ''[[key generation]]'' algorithm that selects a ''private key'' [[Uniform distribution (discrete)|uniformly at random]] from a set of possible private keys. The algorithm outputs the private key and a corresponding ''public key''.
* A ''signing'' algorithm that, given a message and a private key, produces a signature.
* A ''signature verifying'' algorithm that, given the message, public key and signature, either accepts or rejects the message's claim to authenticity.

Two main properties are required.  First, the authenticity of a signature generated from a fixed message and fixed private key can be verified by using the corresponding public key. Secondly, it should be computationally infeasible to generate a valid signature for a party without knowing that party's private key.
A digital signature is an authentication mechanism that enables the creator of the message to attach a code that acts as a signature. 
The [[Digital Signature Algorithm]] (DSA), developed by the [[National Institute of Standards and Technology]], is one of [[Digital signature#Some digital signature algorithms|many examples]] of a signing algorithm.

In the following discussion, 1<sup>''n''</sup> refers to a [[Unary numeral system|unary number]].

Formally, a '''digital signature scheme''' is a triple of probabilistic polynomial time algorithms, (''G'', ''S'', ''V''), satisfying:
* ''G'' (key-generator) generates a public key, ''pk'', and a corresponding private key, ''sk'', on input 1<sup>''n''</sup>, where ''n'' is the security parameter.
* ''S'' (signing) returns a tag, ''t'', on the inputs: the private key, ''sk'', and a string, ''x''.
* ''V'' (verifying) outputs ''accepted'' or ''rejected'' on the inputs: the public key, ''pk'', a string, ''x'', and a tag, ''t''. 
For correctness, ''S'' and ''V'' must satisfy

: Pr [ (''pk'', ''sk'') ← ''G''(1<sup>''n''</sup>), ''V''( ''pk'', ''x'', ''S''(''sk'', ''x'') ) = ''accepted'' ] = 1.<ref>Pass, def 135.1</ref>

A digital signature scheme is '''secure''' if for every non-uniform probabilistic polynomial time [[Adversary (cryptography)|adversary]], ''A''

: Pr [ (''pk'', ''sk'') ← ''G''(1<sup>''n''</sup>), (''x'', ''t'') ← ''A''<sup>''S''(''sk'', · )</sup>(''pk'', 1<sup>''n''</sup>), ''x'' ∉ ''Q'', ''V''(''pk'', ''x'', ''t'') = ''accepted''] < [[Negligible function|negl]](''n''),

where ''A''<sup>''S''(''sk'', · )</sup> denotes that ''A'' has access to the [[Oracle machine|oracle]], ''S''(''sk'', · ), and ''Q'' denotes the set of the queries on ''S'' made by ''A'', which knows the public key, ''pk'', and the security parameter, ''n''. Note that we require any adversary cannot directly query the string, ''x'', on ''S''.<ref>Goldreich's FoC, vol. 2, def 6.1.2. Pass, def 135.2</ref>

==History of Digital Signature==
In 1976, [[Whitfield Diffie]] and [[Martin Hellman]] first described the notion of a digital signature scheme, although they only conjectured that such schemes existed.<ref>"New Directions in Cryptography", IEEE Transactions on Information Theory, IT-22(6):644–654, Nov. 1976.</ref><ref name=lysythesis>"[http://theory.lcs.mit.edu/~cis/theses/anna-phd.pdf Signature Schemes and Applications to Cryptographic Protocol Design]", Anna Lysyanskaya, PhD thesis, [[Massachusetts Institute of Technology|MIT]], 2002.</ref>  Soon afterwards, [[Ronald Rivest]], [[Adi Shamir]], and [[Len Adleman]] invented the [[RSA (algorithm)|RSA]] algorithm, which could be used to produce primitive digital signatures<ref name="rsa">
{{cite journal
 | first = R. | last = Rivest
 | author2 = A. Shamir; L. Adleman
 | url = http://people.csail.mit.edu/rivest/Rsapaper.pdf
 | title = A Method for Obtaining Digital Signatures and Public-Key Cryptosystems
 | journal = Communications of the ACM
 | volume = 21  | issue = 2 | pages = 120–126 | year = 1978
 | doi = 10.1145/359340.359342
}}</ref> (although only as a proof-of-concept &ndash; "plain" RSA signatures are not secure<ref>For example any integer, ''r'', "signs" ''m''=''r''<sup>''e''</sup> and the product, ''s''<sub>1</sub>''s''<sub>2</sub>, of any two valid signatures, ''s''<sub>1</sub>, ''s''<sub>2</sub> of ''m''<sub>1</sub>, ''m''<sub>2</sub> is a valid signature of the product, ''m''<sub>1</sub>''m''<sub>2</sub>.</ref>). The first widely marketed software package to offer digital signature was [[Lotus Notes]] 1.0, released in 1989, which used the RSA algorithm.<ref>{{cite web|title=The History of Notes and Domino|url=http://www.ibm.com/developerworks/lotus/library/ls-NDHistory/|website=developerWorks|accessdate=17 September 2014}}</ref>

Other digital signature schemes were soon developed after RSA, the earliest being [[Lamport signature]]s,<ref>"Constructing digital signatures from a one-way function.", [[Leslie Lamport]], Technical Report CSL-98, SRI International, Oct. 1979.</ref> [[Merkle tree|Merkle signatures]] (also known as "Merkle trees" or simply "Hash trees"),<ref>"A certified digital signature", Ralph Merkle, In Gilles Brassard, ed., Advances in Cryptology – [[CRYPTO]] '89, vol. 435 of Lecture Notes in Computer Science, pp. 218&ndash;238, Spring Verlag, 1990.</ref> and [[Rabin signature]]s.<ref>"Digitalized signatures as intractable as factorization."  [[Michael O. Rabin]], Technical Report MIT/LCS/TR-212, MIT Laboratory for Computer Science, Jan. 1979</ref>

In 1988, [[Shafi Goldwasser]], [[Silvio Micali]], and [[Ronald Rivest]] became the first to rigorously define the security requirements of digital signature schemes.<ref name="SJC 17(2)">"A digital signature scheme secure against adaptive chosen-message attacks.", Shafi Goldwasser, Silvio Micali, and Ronald Rivest. SIAM Journal on Computing, 17(2):281&ndash;308, Apr. 1988.</ref> They described a hierarchy of attack models for signature schemes, and also presented the [[GMR (cryptography)|GMR signature scheme]], the first that could be proved to prevent even an existential forgery against a chosen message attack.<ref name="SJC 17(2)"/>

==How they work==
To create RSA signature keys, generate a RSA key pair containing a modulus, ''N'', that is the product of two large primes, along with integers, ''e'' and ''d'', such that ''e&nbsp;d''&nbsp;[[Modular arithmetic|≡]]&nbsp;1&nbsp;(mod&nbsp;φ(''N'')), where φ is the [[Euler's totient function|Euler phi-function]]. The signer's public key consists of ''N'' and ''e'', and the signer's secret key contains ''d''.

To sign a message, ''m'', the signer computes a signature, σ, such that σ ≡ ''m''<sup>''d''</sup> (mod ''N''). To verify, the receiver checks that σ<sup>''e''</sup> ≡ ''m'' (mod ''N'').

As noted earlier, this basic scheme is not very secure. To prevent attacks, one can first apply a [[cryptographic hash function]] to the message, ''m'', and then apply the RSA algorithm described above to the result. This approach is secure assuming the hash function is a [[random oracle model|random oracle]].

Most early signature schemes were of a similar type: they involve the use of a [[trapdoor permutation]], such as the RSA function, or in the case of the Rabin signature scheme, computing square modulo composite, ''n.'' A trapdoor permutation family is a family of [[permutation]]s, specified by a parameter, that is easy to compute in the forward direction, but is difficult to compute in the reverse direction without already knowing the private key ("trapdoor").  Trapdoor permutations can be used for digital signature schemes, where computing the reverse direction with the secret key is required for signing, and computing the forward direction is used to verify signatures.

Used directly, this type of signature scheme is vulnerable to a key-only existential forgery attack. To create a forgery, the attacker picks a random signature σ and uses the verification procedure to determine the message, ''m'', corresponding to that signature.<ref>"Modern Cryptography: Theory & Practice", Wenbo Mao, Prentice Hall Professional Technical Reference, New Jersey, 2004, pg. 308.  ISBN 0-13-066943-1</ref> In practice, however, this type of signature is not used directly, but rather, the message to be signed is first [[cryptographic hash function|hashed]] to produce a short digest that is then signed. This forgery attack, then, only produces the hash function output that corresponds to σ, but not a message that leads to that value, which does not lead to an attack. In the random oracle model, this [[Full domain hash|hash-then-sign]] form of signature is existentially unforgeable, even against a [[chosen-plaintext attack]].<ref name=lysythesis />{{Clarify|reason=Please give a page number or theorem number.|date=September 2010}}

There are several reasons to sign such a hash (or message digest) instead of the whole document.

;For efficiency: The signature will be much shorter and thus save time since hashing is generally much faster than signing in practice.
;For compatibility: Messages are typically bit strings, but some signature schemes operate on other domains (such as, in the case of RSA, numbers modulo a composite number ''N''). A hash function can be used to convert an arbitrary input into the proper format.
;For integrity: Without the hash function, the text "to be signed" may have to be split (separated) in blocks small enough for the signature scheme to act on them directly. However, the receiver of the signed blocks is not able to recognize if all the blocks are present and in the appropriate order.

==Notions of security==
In their foundational paper, Goldwasser, Micali, and Rivest lay out a hierarchy of attack models against digital signatures:<ref name="SJC 17(2)"/>

# In a ''key-only'' attack, the attacker is only given the public verification key.
# In a ''known message'' attack, the attacker is given valid signatures for a variety of messages known by the attacker but not chosen by the attacker.
# In an ''adaptive chosen message'' attack, the attacker first learns signatures on arbitrary messages of the attacker's choice.

They also describe a hierarchy of attack results:<ref name="SJC 17(2)"/>

# A ''total break'' results in the recovery of the signing key.
# A [[universal forgery]] attack results in the ability to forge signatures for any message.
# A [[selective forgery]] attack results in a signature on a message of the adversary's choice.
# An [[existential forgery]] merely results in some valid message/signature pair not already known to the adversary.

The strongest notion of security, therefore, is security against existential forgery under an adaptive chosen message attack.

==Applications of digital signatures==

As organizations move away from paper documents with ink signatures or authenticity stamps, digital signatures can provide added assurances of the evidence to provenance, identity, and status of an electronic document as well as acknowledging informed consent and approval by a signatory.  The United States Government Printing Office (GPO) publishes electronic versions of the budget, public and private laws, and congressional bills with digital signatures.  Universities including Penn State, [[University of Chicago]], and Stanford are publishing electronic student transcripts with digital signatures.

Below are some common reasons for applying a digital signature to communications:

===Authentication===
Although messages may often include information about the entity sending a message, that information may not be accurate.  Digital signatures can be used to authenticate the source of messages. When ownership of a digital signature secret key is bound to a specific user, a valid signature shows that the message was sent by that user. The importance of high confidence in sender authenticity is especially obvious in a financial context. For example, suppose a bank's branch office sends instructions to the central office requesting a change in the balance of an account. If the central office is not convinced that such a message is truly sent from an authorized source, acting on such a request could be a grave mistake.

===Integrity===
In many scenarios, the sender and receiver of a message may have a need for confidence that the message has not been altered during transmission. Although encryption hides the contents of a message, it may be possible to ''change'' an encrypted message without understanding it. (Some encryption algorithms, known as [[Malleability (cryptography)|nonmalleable]] ones, prevent this, but others do not.) However, if a message is digitally signed, any change in the message after signature invalidates the signature. Furthermore, there is no efficient way to modify a message and its signature to produce a new message with a valid signature, because this is still considered to be computationally infeasible by most cryptographic hash functions (see [[collision resistance]]).

===Non-repudiation===
Non-repudiation,<ref name="Cryptomathic_MajorStandardsDigSig" /> or more specifically ''non-repudiation of origin'', is an important aspect of digital signatures. By this property, an entity that has signed some information cannot at a later time deny having signed it. Similarly, access to the public key only does not enable a fraudulent party to fake a valid signature.

Note that these authentication, non-repudiation etc. properties rely on the secret key ''not having been revoked ''prior to its usage.  Public revocation of a key-pair is a required ability, else leaked secret keys would continue to implicate the claimed owner of the key-pair. Checking revocation status requires an "online" check; e.g., checking a [[certificate revocation list]] or via the <ref name="CryptomathicDigSigServicesAshiqJA" />[[Online Certificate Status Protocol]].   Very roughly this is analogous to a vendor who receives credit-cards first checking online with the credit-card issuer to find if a given card has been reported lost or stolen.   Of course, with stolen key pairs, the theft is often discovered only after the secret key's use, e.g., to sign a bogus certificate for espionage purpose.

==Additional security precautions==

===Putting the private key on a smart card===
All public key / private key cryptosystems depend entirely on keeping the private key secret. A private key can be stored on a user's computer, and protected by a local password, but this has two disadvantages:

* the user can only sign documents on that particular computer
* the security of the private key depends entirely on the [[computer insecurity|security]] of the computer

A more secure alternative is to store the private key on a [[smart card]]. Many smart cards are designed to be tamper-resistant (although some designs have been broken, notably by [[Ross J. Anderson (professor)|Ross Anderson]] and his students). In a typical digital signature implementation, the hash calculated from the document is sent to the smart card, whose CPU signs the hash using the stored private key of the user, and then returns the signed hash. Typically, a user must activate his smart card by entering a [[personal identification number]] or PIN code (thus providing [[two-factor authentication]]). It can be arranged that the private key never leaves the smart card, although this is not always implemented. If the smart card is stolen, the thief will still need the PIN code to generate a digital signature. This reduces the security of the scheme to that of the PIN system, although it still requires an attacker to possess the card. A mitigating factor is that private keys, if generated and stored on smart cards, are usually regarded as difficult to copy, and are assumed to exist in exactly one copy. Thus, the loss of the smart card may be detected by the owner and the corresponding certificate can be immediately revoked. Private keys that are protected by software only may be easier to copy, and such compromises are far more difficult to detect.

===Using smart card readers with a separate keyboard===
Entering a PIN code to activate the smart card commonly requires a [[numeric keypad]]. Some card readers have their own numeric keypad. This is safer than using a card reader integrated into a PC, and then entering the PIN using that computer's keyboard. Readers with a numeric keypad are meant to circumvent the eavesdropping threat where the computer might be running a [[keystroke logging|keystroke logger]], potentially compromising the PIN code. Specialized card readers are also less vulnerable to tampering with their software or hardware and are often [[Evaluation Assurance Level|EAL3]] certified.

===Other smart card designs===
Smart card design is an active field, and there are smart card schemes which are intended to avoid these particular problems, though so far with little security proofs.

===Using digital signatures only with trusted applications===
One of the main differences between a digital signature and a written signature is that the user does not "see" what he signs. The user application presents a hash code to be signed by the digital signing algorithm using the private key. An attacker who gains control of the user's PC can possibly replace the user application with a foreign substitute, in effect replacing the user's own communications with those of the attacker. This could allow a malicious application to trick a user into signing any document by displaying the user's original on-screen, but presenting the attacker's own documents to the signing application.

To protect against this scenario, an authentication system can be set up between the user's application (word processor, email client, etc.) and the signing application. The general idea is to provide some means for both the user application and signing application to verify each other's integrity. For example, the signing application may require all requests to come from digitally signed binaries.

===Using a network attached [[hardware security module]]===
One of the main differences between a [[cloud]] based digital signature service and a locally provided one is risk.  Many risk averse companies, including governments, financial and medical institutions,  and payment processors require more secure standards, like [[FIPS 140-2]] level 3 and [[FIPS 201]] certification, to ensure the signature is validated and secure.<ref>[http://www.arx.com/products/privateserver-hsm/overview/ PrivateServer HSM Overview]</ref> <!--To finish: current and future applications, actual algorithms, standards, why not as adopted as widely as expected, etc.-->

===WYSIWYS===
{{Main article|WYSIWYS}}
Technically speaking, a digital signature applies to a string of bits, whereas humans and applications "believe" that they sign the semantic interpretation of those bits. In order to be semantically interpreted, the bit string must be transformed into a form that is meaningful for humans and applications, and this is done through a combination of hardware and software based processes on a computer system. The problem is that the semantic interpretation of bits can change as a function of the processes used to transform the bits into semantic content. It is relatively easy to change the interpretation of a digital document by implementing changes on the computer system where the document is being processed. From a semantic perspective this creates uncertainty about what exactly has been signed. [[WYSIWYS]] (What You See Is What You Sign) <ref name=WYSIWYS_SeminalPaper>{{cite journal|last1=Landrock|first1=Peter|last2=Pedersen|first2=Torben|title=WYSIWYS? -- What you see is what you sign?|journal=Information Security Technical Report|date=1998|volume=3|issue=2|pages=55–61}}</ref> means that the semantic interpretation of a signed message cannot be changed. In particular this also means that a message cannot contain hidden information that the signer is unaware of, and that can be revealed after the signature has been applied. WYSIWYS is a necessary requirement for the validity of digital signatures, but this requirement is difficult to guarantee because of the increasing complexity of modern computer systems. The term WYSIWYS was coined by [[Peter Landrock]] and [[Cryptomathic|Torben Pedersen]] to describe some of the principles in delivering secure and legally binding digital signatures for Pan-European projects.<ref name=WYSIWYS_SeminalPaper />

===Digital signatures versus ink on paper signatures===

An ink signature could be replicated from one document to another by copying the image manually or digitally, but to have credible signature copies that can resist some scrutiny is a significant manual or technical skill, and to produce ink signature copies that resist professional scrutiny is very difficult.

Digital signatures cryptographically bind an electronic identity to an electronic document and the digital signature cannot be copied to another document. Paper contracts sometimes have the ink signature block on the last page, and the previous pages may be replaced after a signature is applied.  Digital signatures can be applied to an entire document, such that the digital signature on the last page will indicate tampering if any data on any of the pages have been altered, but this can also be achieved by signing with ink and numbering all pages of the contract.

==Some digital signature algorithms==
*[[RSA (algorithm)|RSA]]-based signature schemes, such as [[RSA-PSS]]
*[[Digital Signature Algorithm|DSA]] and its [[elliptic curve cryptography|elliptic curve]] variant [[Elliptic Curve Digital Signature Algorithm|ECDSA]]
*[[EdDSA|Edwards-curve Digital Signature Algorithm]] and its [[EdDSA#Ed25519|Ed25519]] variant.
*[[ElGamal signature scheme]] as the predecessor to DSA, and variants [[Schnorr signature]] and [[Pointcheval–Stern signature algorithm]]
*[[Rabin signature algorithm]]
*[[Pairing]]-based schemes such as [[Boneh–Lynn–Shacham|BLS]]
*[[Undeniable signature]]s
*[[Aggregate signature]] - a signature scheme that supports aggregation: Given n signatures on n  messages from n users, it is possible to aggregate all these signatures into a single signature whose size is constant in the number of users. This single signature will convince the verifier that the n users did indeed sign the n original messages.
*[[Signatures with efficient protocols]] - are signature schemes that facilitate efficient cryptographic protocols such as [[zero-knowledge proofs]] or [[secure computation]].

==The current state of use &ndash; legal and practical ==
{{Globalize|section|date=November 2009}}
All digital signature schemes share the following basic prerequisites regardless of cryptographic theory or legal provision:

#;Quality algorithms: Some public-key algorithms are known to be insecure, as practical attacks against them having been discovered.
#
#; Quality implementations: An implementation of a good algorithm (or [[cryptographic protocol|protocol]]) with mistake(s) will not work.
#
#; Users (and their software) must carry out the signature protocol properly.
#
#; The private key must remain private: If the private key becomes known to any other party, that party can produce ''perfect'' digital signatures of anything whatsoever.
#
#; The public key owner must be verifiable: A public key associated with Bob actually came from Bob. This is commonly done using a [[public key infrastructure]] (PKI) and the public key↔user association is attested by the operator of the PKI (called a [[certificate authority]]). For 'open' PKIs in which anyone can request such an attestation (universally embodied in a cryptographically protected  [[identity certificate]]), the possibility of mistaken attestation is non-trivial. Commercial PKI operators have suffered several publicly known problems. Such mistakes could lead to falsely signed, and thus wrongly attributed, documents. 'Closed' PKI systems are more expensive, but less easily subverted in this way.

Only if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its contents. Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.

Legislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect. The first appears to have been in [[Utah]] in the United States, followed closely by the states [[Massachusetts]] and [[California]]. Other countries have also passed statutes or issued regulations in this area as well and the UN has had an active model law project for some time. These enactments (or proposed enactments) vary from place to place, have typically embodied expectations at variance (optimistically or pessimistically) with the state of the underlying [[cryptographic engineering]], and have had the net effect of confusing potential users and specifiers, nearly all of whom are not cryptographically knowledgeable. Adoption of technical standards for digital signatures have lagged behind much of the legislation, delaying a more or less unified engineering position on [[interoperability]], [[algorithm]] choice, [[key length]]s, and so on what the engineering is attempting to provide.

{{see also|ABA digital signature guidelines}}

==Industry standards==
{{unreferenced section|date=January 2015}}
Some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators.  These include the [[Automotive Network Exchange]] for the automobile industry and the [[SAFE-BioPharma Association]] for the healthcare industry.

===Using separate key pairs for signing and encryption===
In several countries, a digital signature has a status somewhat like that of a traditional pen and paper signature, like in the [http://europa.eu/legislation_summaries/information_society/l24118_en.htm EU digital signature legislation].<ref name=Cryptomathic_MajorStandardsDigSig /> Generally, these provisions mean that anything digitally signed legally binds the signer of the document to the terms therein. For that reason, it is often thought best to use separate key pairs for encrypting and signing. Using the encryption key pair, a person can engage in an encrypted conversation (e.g., regarding a real estate transaction), but the encryption does not legally sign every message he sends. Only when both parties come to an agreement do they sign a contract with their signing keys, and only then are they legally bound by the terms of a specific document. After signing, the document can be sent over the encrypted link.  If a signing key is lost or compromised, it can be revoked to mitigate any future transactions.  If an encryption key is lost, a backup or [[key escrow]] should be utilized to continue viewing encrypted content.  Signing keys should never be backed up or escrowed unless the backup destination is securely encrypted.

==See also==
* [[21 CFR 11]]
* [[Blind signature]]
* [[Detached signature]]
* [[Digital certificate]]
* [[Digital signature in Estonia]]
* [[Electronic lab notebook]]
* [[Electronic signature]]
* [[Electronic signatures and law]]
* [[eSign (India)]]
* [[GNU Privacy Guard]]
* [[Global Trust Center]]
* [[PAdES]]
* [[Public key infrastructure]]
* [[Server-based signatures]]

==Notes==
{{Reflist}}

==References==
*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography I: Basic Tools|date=2001|publisher=Cambridge University Press|location=Cambridge|isbn=978-0-511-54689-1}}
*{{citation|last1=Goldreich|first1=Oded|title=Foundations of cryptography II: Basic Applications|date=2004|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=978-0-521-83084-3|edition=1. publ.}}
*{{citation|last1=Pass|first1=Rafael|title=A Course in Cryptography|url=https://www.cs.cornell.edu/courses/cs4830/2010fa/lecnotes.pdf|accessdate=31 December 2015}}

==Further reading==
* J. Katz and Y. Lindell, "Introduction to Modern Cryptography" (Chapman & Hall/CRC Press, 2007)
* Stephen Mason, Electronic Signatures in Law (4th edition, Institute of Advanced Legal Studies for the SAS Digital Humanities Library, School of Advanced Study, University of London, 2016). ISBN 978-1-911507-00-0.
* Lorna Brazell, Electronic Signatures and Identities Law and Regulation (2nd edn, London: Sweet & Maxwell, 2008);
* Dennis Campbell, editor, E-Commerce and the Law of Digital Signatures (Oceana Publications, 2005).
* M. H. M Schellenkens, Electronic Signatures Authentication Technology from a Legal Perspective, (TMC Asser Press, 2004).
* Jeremiah S. Buckley, John P. Kromer, Margo H. K. Tank, and R. David Whitaker, The Law of Electronic Signatures (3rd Edition, West Publishing, 2010).
* [http://journals.sas.ac.uk/deeslr/ ''Digital Evidence and Electronic Signature Law Review''] Free open source

{{Cryptography navbox | public-key}}

{{DEFAULTSORT:Digital Signature}}
[[Category:Public-key cryptography]]
[[Category:Electronic documents]]
[[Category:Key management]]
[[Category:Notary]]
[[Category:Signature]]
[[Category:Records management technology]]
<=====doc_Id=====>:426
<=====title=====>:
Social Sciences Citation Index
<=====text=====>:
{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 
| languages = 
| providers = 
| cost = 
| disciplines = Social sciences
| depth = Index & citation indexing 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/social-sciences-citation-index.html
| titles = http://ip-science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=SS
}}
The '''Social Sciences Citation Index''' ('''SSCI''') is a commercial [[citation index]] product of [[Thomson Reuters]]' Healthcare & Science division. It was developed by the [[Institute for Scientific Information]] from the [[Science Citation Index]].

==Overview==
The SSCI citation database covers some 3,000 of the world's leading [[academic journals]] in the [[social sciences]] across more than 50 [[academic discipline|disciplines]].<ref>{{cite web |title=Social Sciences Citation Index |url=http://scientific.thomson.com/products/ssci/ |accessdate=2008-06-11}}</ref> It is made available online through the [[Web of Science]] service for a fee. The database records which articles are cited by other articles.

==Criticism==
[[Philip Altbach]] has criticised the Social Sciences Citation Index of favouring English-language journals generally and American journals specifically, while greatly underrepresenting journals in non-English languages.<ref>{{cite book |last= Altbach|first=Philip |authorlink= Philip Altbach |year=2005 |article=Academic Challenges: The American Professoriate in Comparative Perspective |title=The Professoriate: Profile of a Profession |url= |location=Dortrecht |publisher=Springer |pages=147–165 |isbn= |author-link= }}</ref>

In 2004, economists [[Daniel B. Klein]] and Eric Chiang conducted a survey of the Social Sciences Citation Index and identified a bias against free market oriented research. In addition to an ideological bias, Klein and Chiang also identified several methodological deficiencies that encouraged the over-counting of citations, and they argue that the Social Sciences Citation Index does a poor job reflecting the relevance and accuracy of articles.<ref>Daniel Klein and Eric Chiang. [http://econjwatch.org/articles/the-social-science-citation-index-a-black-box-with-an-ideological-bias The Social Science Citation Index: A Black Box—with an Ideological Bias?] ''Econ Journal Watch'', Volume 1, Number 1, April 2004, pp 134–165.</ref>

==See also==
* [[Arts and Humanities Citation Index]]
* [[Science Citation Index]]

==References==
{{reflist}}

==External links==
*{{Official website|http://ip-science.thomsonreuters.com/mjl/publist_ssci.pdf}}
*[http://thomsonreuters.com/products_services/science/science_products/a-z/social_sciences_citation_index Introduction to SSCI]

{{Thomson Reuters}}

[[Category:Thomson Reuters]]
[[Category:Social sciences literature]]
[[Category:Citation indices]]
[[Category:Social science journals| ]]

{{database-stub}}
{{sci-stub}}
<=====doc_Id=====>:429
<=====title=====>:
Science Citation Index
<=====text=====>:
{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Science Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 2000-present
| languages = 
| providers = 
| cost = 
| disciplines = Science, medicine, and technology
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 

| updates = 
| p_title = 
| p_dates = 
| ISSN = 0036-827X
| web = http://thomsonreuters.com/science-citation-index-expanded/
| titles = 
}}
The '''Science Citation Index''' ('''SCI''') is a [[citation index]] originally produced by the [[Institute for Scientific Information]] (ISI) and created by [[Eugene Garfield]]. It was officially launched in 1964. It is now owned by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]]).<ref name=dimension>
{{cite journal
|doi=10.1126/science.122.3159.108
|title=Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas
|url=http://ije.oxfordjournals.org/content/35/5/1123.full
|format=Free web article download
|year=1955
|last1=Garfield
|first1=E.
|journal=Science
|volume=122
|issue=3159
|pages=108–11
|pmid=14385826|bibcode=1955Sci...122..108G
}}</ref><ref name=evolve>
{{cite journal
 |last = Garfield 
 |first = Eugene
 |doi=10.2436/20.1501.01.10
 |url=http://garfield.library.upenn.edu/papers/barcelona2007a.pdf
 |format=Free PDF download
 |title=The evolution of the Science Citation Index|doi-broken-date = 2017-01-16
 }} International microbiology '''10.'''1 (2010): 65-69.</ref><ref name=gOverview>
{{cite web
 | last = Garfield 
 | first = Eugene
 | authorlink =
 | coauthors =
 | title = Science Citation Index
 | work = Science Citation Index 1961
 | publisher = Garfield Library - UPenn
 | date = 1963
 | url = http://garfield.library.upenn.edu/papers/80.pdf
 | format = Free PDF download
 | doi =
 | accessdate = 2013-05-27}} 
* Originally published by the Institute of Scientific Information in 1964
* Other titles in this document are: What is a Citation Index? , How is the Citation Index Prepared? , How is the Citation Index Used? , Applications of the Science Citation Index , Source Coverage and Statistics , and a Glossary.</ref><ref name=history-cite-indexing>
{{cite web
 | title =History of Citation Indexing 
 | work =Needs of researchers create demand for citation indexing 
 | publisher =Thomson Ruters 
 | date =November 2010 
 | url =http://thomsonreuters.com/products_services/science/free/essays/history_of_citation_indexing/ 
 | format =Free HTML download 
 | accessdate =2010-11-04}}</ref> The larger version ('''Science Citation Index Expanded''') covers more than 8,500 notable and significant [[Scientific journal|journals]], across 150 disciplines, from 1900 to the present. These are alternatively described as the world's leading journals of [[science]] and [[technology]], because of a rigorous selection process.<ref name=Expanded>
{{cite web 
|url=https://www.thomsonreuters.com/en/products-services/scholarly-scientific-research/scholarly-search-and-discovery/science-citation-index-expanded.html 
|title=Science Citation Index Expanded 
|work= |accessdate=2017-01-17}}</ref><ref name=wetland>{{cite journal| doi= 10.1007/s12665-012-2193-y|title= The Top-cited Wetland Articles in Science Citation Index Expanded: characteristics and hotspots|url=http://dns2.asia.edu.tw/~ysho/YSHO-English/Publications/PDF/Env%20Ear%20Sci-Ma.pdf|date= December 2012| last1= Ma| first1= Jiupeng| last2= Fu| first2= Hui-Zhen| last3= Ho| first3= Yuh-Shan| journal= Environmental Earth Sciences|volume= 70|issue= 3|pages= 1039}} (Springer-Verlag)</ref><ref name=shan>
{{cite journal 
| doi= 10.1007/s11192-012-0837-z 
|title= The top-cited research works in the Science Citation Index Expanded 
|url= http://trend.asia.edu.tw/Publications/PDF/Scientometrics94,%201297.pdf 
| year= 2012 
| last1= Ho 
| first1= Yuh-Shan 
| journal= Scientometrics 
| volume= 94 
| issue= 3 
| page= 1297}}</ref>

The index is made available online through different platforms, such as the [[Web of Science]]<ref name=AtoZ>{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2010 |url=http://wokinfo.com/products_tools/products/ |format=Choose databases on method of discovery and analysis |accessdate=2010-06-24}}</ref><ref>[http://wokinfo.com/media/pdf/SSR1103443WoK5-2_web3.pdf Thomson Reuters Web of Knowledge. Thomson Reuters, 2013.]</ref> and SciSearch.<ref>{{cite web |url=http://library.dialog.com/bluesheets/html/bl0034.html |title=SCISEARCH - A CITED REFERENCE SCIENCE DATABASE |publisher=Library.dialog.com |date= |accessdate=2014-04-17}}</ref> (There are also CD and printed editions, covering a smaller number of journals). This database allows a researcher to identify which later articles have cited any particular earlier article, or have cited the articles of any particular author, or have been cited most frequently. Thomson Reuters also markets several subsets of this database, termed "Specialty Citation Indexes",<ref name=SpCI>
{{cite web 
|url=http://thomsonreuters.com/products_services/science/science_products/a-z/specialty_citation_indexes/ 
|title=Specialty Citation Indexes 
|work= |accessdate=2009-08-30}}</ref> 
such as the '''Neuroscience Citation Index'''<ref name=NCI>
{{cite web 
|url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MD 
|title=Journal Search - Science - |work= |accessdate=2009-08-30}}</ref> and the '''Chemistry Citation Index'''.<ref>{{cite web |url=http://science.thomsonreuters.com/cgi-bin/jrnlst/jloptions.cgi?PC=CD 
|title=Journal Search - Science - Thomson Reuters |accessdate=14 January 2011}}</ref>

==Chemistry Citation Index==
The Chemistry Citation Index was first introduced by Eugene Garfield, a chemist by training. His original "search examples were based on [his] experience as a chemist".<ref name=Garcci/> In 1992 an electronic and print form of the index was derived from a core of 330 chemistry journals, within which all areas were covered. Additional information was provided from articles selected from 4,000 other journals. All chemistry subdisciplines were covered: organic, inorganic, analytical, physical chemistry, polymer, computational, organometallic, materials chemistry, and electrochemistry.<ref name=Garcci>Garfield, Eugene. "[http://garfield.library.upenn.edu/essays/v15p007y1992-93.pdf New Chemistry Citation Index On CD-ROM Comes With Abstracts, Related Records, and Key-Words-Plus]." Current Contents 3 (1992): 5-9.</ref>

By 2002 the core journal coverage increased to 500 and related article coverage increased to 8,000 other journals.<ref>
[http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Institute of Process Engineering of the Chinese Academy of Sciences. 2003.</ref> 

One 1980 study reported the overall citation indexing benefits for chemistry, examining the use of citations as a tool for the study of the sociology of chemistry and illustrating the use of citation data to "observe" chemistry subfields over time.<ref>
{{cite journal
|doi=10.1007/BF02016348
|title=Science citation index and chemistry
|year=1980
|last1=Dewitt
|first1=T. W.
|last2=Nicholson
|first2=R. S.
|last3=Wilson
|first3=M. K.
|journal=Scientometrics
|volume=2
|issue=4
|page=265}}</ref>

==See also==
* [[Arts and Humanities Citation Index]], which covers 1130 journals, beginning with 1975.
* [[Impact factor]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers 1700 journals, beginning with 1956.

==References==
{{Reflist|30em}}

==Further reading==
*{{cite journal
|doi= 10.1002/aris.1440360102
|url= http://polaris.gseis.ucla.edu/cborgman/pubs/borgmanfurnerarist2002.pdf
|title=Scholarly Communication and Bibliometrics
|year= 2005
|last1= Borgman
|first1= Christine L.
|last2= Furner
|first2= Jonathan
|journal= Annual Review of Information Science and Technology
|volume= 36
|issue= 1 
|pages=3–72}}

*{{cite journal
|doi= 10.1002/asi.20677
|url= http://staff.aub.edu.lb/~lmeho/meho-yang-impact-of-data-sources.pdf
|format= Free PDF download
|title= Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar
|year= 2007
|last1= Meho
|first1= Lokman I.
|last2= Yang
|first2= Kiduk
|journal= Journal of the American Society for Information Science and Technology
|volume= 58
|issue= 13
|page= 2105}}

*{{cite journal
|doi= 10.1002/asi.5090140304
|url= http://www.garfield.library.upenn.edu/essays/v6p492y1983.pdf
|format= Free PDF download
|title= New factors in the evaluation of scientific literature through citation indexing
|year= 1963
|last1= Garfield
|first1= E.
|last2= Sher
|first2= I. H.
|journal= American Documentation
|volume= 14
|issue= 3
|page= 195}}

*{{cite journal
|doi= 10.1038/227669a0
|url= http://www.garfield.library.upenn.edu/essays/V1p133y1962-73.pdf
|format= Free PDF download
|title= Citation Indexing for Studying Science
|year= 1970
|last1= Garfield
|first1= E.
|journal= Nature
|volume= 227
|issue= 5259
|pages= 669–71
|pmid= 4914589|bibcode= 1970Natur.227..669G
}}

*{{cite book
 | last =Garfield
 | first =Eugene 
 | authorlink =
 | title =Citation Indexing: Its Theory and Application in Science, Technology, and Humanities
 | publisher =Wiley-Interscience
 | series = Information Sciences Series
 | edition = 1st
 | origyear = 1979| year = 1983
 | location = New York
 | isbn =9780894950247}}

==External links==
* [http://scientific.thomson.com/products/wos/ Introduction to SCI]
* [http://science.thomsonreuters.com/mjl/ Master journal list]
* [https://en.wikibooks.org/wiki/Chemical_Information_Sources/Author_and_Citation_Searches Chemical Information Sources/ Author and Citation Searches]. on WikiBooks. 
* [http://scientific.thomson.com/tutorials/citedreference/crs1.htm Cited Reference Searching: An Introduction]. Thomson Reuters. 
* [http://www.chinweb.com/cgi-bin/chemport/getfiler.cgi?ID=k4l7vyYF5FimYvScsOm3pxWVmEhBoH0ZuYgxjLdKBfqdmURDHLrjuVv78i16JLPX&VER=E Chemistry Citation Index]. Chinweb.

{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]
<=====doc_Id=====>:432
<=====title=====>:
Materials Science Citation Index
<=====text=====>:
{{Third-party|date=February 2013}}
'''The Materials Science Citation Index''' is a [[citation index]], established in 1992, by [[Thomson ISI]] ([[Thomson Reuters]]). Its overall focus is [[citation|cited reference]] searching of the notable and significant [[science journal|journal literature]] in [[materials science]]. The database makes accessible the various [[physical properties|properties]], behaviors, and materials in the materials science discipline. This then encompasses [[applied physics]], [[ceramic engineering|ceramics]], [[Advanced composite materials (science & engineering)|composite materials]], [[metals]] and [[metallurgy]], [[polymer engineering]], [[semiconductors]], [[thin films]], [[biomaterial]]s, [[Dentistry|dental technology]], as well as [[optics]]. The [[database]] indexes relevant materials science information from over 6,000 [[scientific journal]]s that are part of the ISI database which is [[multidisciplinary]]. Author abstracts are searchable, which links articles sharing one or more [[bibliographic]] references. The database also allows a researcher to use an appropriate (or related to research) article as a base to search forward in time to discover more recently published articles that cite it.<ref name=msci-est>Pemberton, Julia K. "''Two new databases from ISI''." CD-ROM Professional 5.4 (1992): 107+. General OneFile. Web. 20 June 2010.</ref>

''Materials Science Citation Index'' lists 625 high impact journals, and is accessible via the [[Science Citation Index Expanded]] collection of databases.<ref name=msci-jnlList>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlresults.cgi?PC=MS Materials Science Citation Index journal list]. Thomson Reuters. July 2010.</ref>

==Editions==
Coverage of Materials science is accomplished with the following editions:<ref name=MS-indexes>[http://science.thomsonreuters.com/mjl/scope/scope_scie/ Scope Notes]. Science Citation Index, Science Citation Index Expanded. Thomson Reuters. 2010.</ref><ref>[http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=D Subject categories]. Science Citation Index Expanded. Thomson Reuters. 2010</ref>
*Materials Science, Ceramics
*Materials Science, Characterization & Testing
*Materials Science, Biomaterials
*Materials Science, Coatings & Films
*Materials Science, Composites
*Materials Science, Paper & Wood
*Materials Science, Multidisciplinary
*Materials Science, Textiles

==See also==
* [[Science Citation Index]]
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Social Sciences Citation Index]], which covers over 1500 journals, beginning with 1956
* [[Arts and Humanities Citation Index]], which covers over 1000 journals, beginning with 1975
* [[Impact factor]]
* [[VINITI Database RAS]]

==References==
{{Reflist}}

{{Thomson Reuters}}

[[Category:Thomson Reuters]]
[[Category:Bibliographic databases and indexes]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Materials science journals| ]]


{{science-journal-stub}}
<=====doc_Id=====>:435
<=====title=====>:
Book Citation Index
<=====text=====>:
The '''Book Citation Index''' ('''BCI''', '''BKCI''') is an online subscription-based scientific [[citation index]]ing service maintained by [[Thomson Reuters]] and is part of the [[Web of Science|Web of Science Core Collection]].<ref>{{cite book|last1=Campbell|first1=Robert|last2=Pentz|first2=Ed|last3=Borthwick|first3=Ian|title=Academic and Professional Publishing|date=2012|publisher=Chandos Publishing|isbn=9781780633091|pages=247–248|url=https://books.google.com/books?id=IpRlAgAAQBAJ&pg=PA247&dq=%22Book+Citation+Index%22&hl=en&sa=X&ei=Ig9sVZ2QB4rLsASwyYHYBw&ved=0CGoQ6AEwCg#v=onepage&q=%22Book%20Citation%20Index%22&f=false|accessdate=1 June 2015}}</ref> It was first launched in 2011 and indexes over 60,000 editorially selected books, starting from 2005.<ref name=ATL>{{cite journal|title=Thomson reuters launches Book Citation Index|journal=Advanced Technology Libraries|date=11/01/2011|volume=40|issue=11|page=3|url=http://web.a.ebscohost.com.ezproxy2.library.drexel.edu/ehost/pdfviewer/pdfviewer?sid=3118f8aa-bb72-4992-b82e-be196198670d%40sessionmgr4002&vid=1&hid=4212|accessdate=1 June 2015}}</ref> Books in the index are electronic and print scholarly texts that contain articles based on [[original research]] and/or reviews of such literature.<ref name=ATL />

==Content==
The index covers series and non-series books as long as they include full footnotes and the index has two separate editions, a Science edition and a Social Sciences & Humanities edition. The Science edition covers physics and chemistry, engineering, computing and technology, clinical medicine, life sciences, and agriculture and biology. Currently both series only contain books that date back to 2005.<ref>{{cite book|last1=Mann|first1=Thomas|title=The Oxford Guide to Library Research|date=2015|publisher=Oxford University Press|isbn=9780199394463|url=https://books.google.com/books?id=llVLBgAAQBAJ&pg=PT193&dq=%22Book+Citation+Index%22&hl=en&sa=X&ei=Ig9sVZ2QB4rLsASwyYHYBw&ved=0CF4Q6AEwCA#v=onepage&q=%22Book%20Citation%20Index%22&f=false|accessdate=1 June 2015}}</ref>

==Reception==
In their 2014 book ''Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact'', [[Blaise Cronin]] and Cassidy R. Sugimoto noted that "for impact assessment of book-based fields, bibliometricians need a database with large numbers of books" and that while the Book Citation Index did meet this need, [[Google Books]] also fulfilled this purpose and was not only free, but was (at the time) more comprehensive for bibliometric analyses.<ref>{{cite book|last1=Cronin|first1=Blaise|last2=Sugimoto|first2=Cassidy R.|title=Beyond Bibliometrics: Harnessing Multidimensional Indicators of Scholarly Impact|date=2014|publisher=MIT Press|isbn=9780262323291|pages=33, 289, 296|url=https://books.google.com/books?id=xxSaAwAAQBAJ&pg=PA296&dq=%22Book+Citation+Index%22&hl=en&sa=X&ei=Ig9sVZ2QB4rLsASwyYHYBw&ved=0CE8Q6AEwBQ#v=onepage&q=%22Book%20Citation%20Index%22&f=false|accessdate=1 June 2015}}</ref> A 2013 article in the ''[[Journal of the American Society for Information Science and Technology]]'' remarked on the index's opportunities and limitations. It stated that the "most significant limitations to this potential application are the high share of publications without address information, the inflation of publication counts, the lack of cumulative citation counts from different hierarchical levels, and inconsistency in citation counts between the cited reference search and the book citation index."<ref name=journal>{{cite journal|last1=Gorraiz|first1=Juan|last2=Purnell|first2=Philip J.|last3=Glänze|first3=Wolfgang|title=Opportunities for and limitations of the Book Citation Index|journal=Journal of the American Society for Information Science and Technology|date=July 2013|volume=64|issue=7|pages=1388–1398|doi=10.1002/asi.22875|url=http://onlinelibrary.wiley.com/doi/10.1002/asi.22875/full|accessdate=1 June 2015}}</ref> They also stated that the Book Citation Index was "a first step toward creating a reliable and necessary citation data source for monographs — a very challenging issue, because, unlike journals and conference proceedings, books have specific requirements, and several problems emerge not only in the context of subject classification, but also in their role as cited publications and in citing publications."<ref name=journal />

==Further reading==
*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Robinson-Garcia|first2=Nicolas|last3=Miguel Campanario|first3=Juan|last4=Emilio|first4=Delgado López-Cózar|title=Coverage, field specialisation and the impact of scientific publishers indexed in the Book Citation Index|journal=Online Information Review|date=January 2014|volume=38|issue=1|pages=24–42|doi=10.1108/OIR-10-2012-0169|url=http://www.emeraldinsight.com.ezproxy2.library.drexel.edu/doi/full/10.1108/OIR-10-2012-0169|accessdate=1 June 2015}}
*{{cite journal|last1=Torres-Salinas|first1=Daniel|last2=Rodriguez-Sánchez|first2=Rosa|last3=Robinson-Garcia|first3=Nicolas|last4=Fdez-Valdivia|first4=J|last5=García|first5=J.A.|title=Mapping Citation Patterns of Book Chapters in the Book Citation Index|journal=Journal of Informetrics|date=February 2013|volume=7|issue=2|pages=412–424|doi=10.1016/j.joi.2013.01.004|arxiv=1302.5544}}

==References==
{{reflist}}

==External links==
*{{official website|http://wokinfo.com/products_tools/multidisciplinary/bookcitationindex/}}

[[Category:Bibliographic databases and indexes]]
[[Category:Full text scholarly online databases]]
[[Category:Thomson family]]
[[Category:Thomson Reuters]]
[[Category:Citation indices]]
[[Category:Books]]
<=====doc_Id=====>:438
<=====title=====>:
Chinese Science Citation Database
<=====text=====>:
{{Infobox bibliographic database 
 |title=Chinese Science Citation Database
}}
The '''Chinese Science Citation Database''' (CSCD) is a [[bibliographic database]] and [[citation index]] produced by the [[Chinese Academy of Sciences]].

It is hosted by [[Thomson Reuters]], and it was the first database in its [[Web of Science]] product in a language other than English.<ref>[http://wokinfo.com/products_tools/multidisciplinary/cscd/]</ref>

==See also==
*[[Chinese Social Sciences Citation Index]]

==References==
{{reflist}}

==External links==
*[http://thomsonreuters.com/content/dam/openweb/documents/pdf/scholarly-scientific-research/methodology/cscd-journal-list.pdf CSCD Journal List]

[[Category:Bibliographic databases and indexes]]
[[Category:Citation indices]]
[[Category:Science and technology in China]]
<=====doc_Id=====>:441
<=====title=====>:
Emerging Sources Citation Index
<=====text=====>:
{{incomplete|date=January 2014}}
{{ infobox bibliographic database
| title = Emerging Sources Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 2015-present
| languages = 
| providers = 
| cost = 
| disciplines = Multidisciplinary
| depth = 
| formats = 
| temporal = 
| geospatial = Worldwide
| number =
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://wokinfo.com/products_tools/multidisciplinary/esci/
| titles = 
}}
The '''Emerging Sources Citation Index''' is a [[citation index]] produced since 2015 by [[Thomson Reuters]], and now by [[Clarivate Analytics]]. It is accessible through the ''[[Web of Science]]''.<ref name=AtoZ>{{cite web |last=ISI Web of Knowledge platform |title =Available databases A to Z |publisher=Thomson Reuters |year=2015 |url=http://ip-science.thomsonreuters.com/mjl/}}</ref> The index includes [[academic journal]]s "of regional importance and in emerging scientific fields".<ref>{{cite web |title=ESCI Fact Sheet |url=http://wokinfo.com/media/pdf/ESCI_Fact_Sheet.pdf?utm_source=false&utm_medium=false&utm_campaign=false |publisher=Thomson Reuters |accessdate=28 April 2016 |format=PDF}}</ref> [[Jeffrey Beall]] has stated that, among the databases produced by Thomson Reuters, the Emerging Sources Citation Index is the easiest one to get into and as a result it contains many [[Predatory open access publishing|predatory journals]].<ref name=Beall>{{cite web |last1=Beall |first1=Jeffrey |authorlink1=Jeffrey Beall |title=The TR Master Journal List is not a Journal Whitelist |url=https://scholarlyoa.com/2016/04/28/the-tr-master-journal-list-is-not-a-journal-whitelist/ |website=Scholarly Open Access |publisher=WordPress.com |accessdate=28 April 2016 |date=28 April 2016}}</ref>

==References==
{{Reflist}}

==External links==
*{{Official website|http://wokinfo.com/products_tools/multidisciplinary/esci/}}
{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]
<=====doc_Id=====>:444
<=====title=====>:
CBD Media
<=====text=====>:
{{Infobox company |
 name = CBD Media LLC|
 logo = [[Image:CBD Media logo.jpg]]|
 type = Subsidiary of [[The Berry Company]]|
 slogan = |
 foundation = |
 location = |
 industry = [[Telephone directory]]|
 parent = [[Spectrum Equity]], etc. (2002-2007)<br>[[The Berry Company|Local Insight Media/Berry]] (2007-present)|
 products = Print Yellow Pages, Online Yellow Pages ads|
}}
'''CBD Media LLC''' (formerly '''Cincinnati Bell Directory''') is a division of Local Insight Media that publishes telephone directories under the [[Cincinnati Bell]] name. The company was created in 2002 following the sale of Cincinnati Bell Directory to a consortium led by [[Spectrum Equity]].

CBD Media publishes the '''Cincinnati Bell Yellow Pages''', which consists of 15 directories, published under the "Real Pages" name. CBD Media also operates [http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com], the [[electronic yellow pages]] directory for [[Cincinnati Bell]].

The company was acquired by Local Insight Media Holdings in 2007.<ref>[http://www.spectrumequity.com/investments/investment?Id=1289 Spectrum Equity | Investments]</ref> Local Insight Media owned [[Local Insight Yellow Pages]], the former directory division of [[Windstream]]. In 2009, Local Insight acquired The Berry Company from [[AT&T]], and changed its own name to '''The Berry Company LLC'''.

==See also==
*[[Engels Maps]]

==External links==
*[http://www.cincinnatibellyellowpages.com CincinnatiBellYellowPages.com]

==References==
{{reflist}}

{{Telephone directory publishers in the United States}}
{{Cincinnati Bell}}

[[Category:Advertising agencies of the United States]]
[[Category:Directories]]
[[Category:Media in Cincinnati]]
[[Category:Publishing companies established in 2002]]
[[Category:Cincinnati Bell]]
[[Category:Publishing companies of the United States]]
[[Category:Companies based in Cincinnati]]
[[Category:2002 establishments in Ohio]]
<=====doc_Id=====>:447
<=====title=====>:
Subcontractors Register
<=====text=====>:
[[Image:Subcontractors Register.jpg|thumb|1937 edition of the ''Subcontractors Register'']] 

The '''Subcontractors Register for the Allied Building Trades''' was a directory of [[subcontractors]] for the [[New York City]] area, listing companies by their trade. It was published by the "Society of the Allied Building Trades, Inc." and was published by Joseph O'Malley (1893–1985) who was later joined by his nephew, [[Walter Francis O'Malley]], as editor. The 1942 version calls itself: "A Classified List for the Allied Building Trades of Sub-Contractors, Material Dealers & Manufacturers, General Contractors & Builders, Architects - Engineers, Real Estate Management Firms".

==External links==
*[http://www.findagrave.com/cgi-bin/fg.cgi?page=gr&GSln=o'malley&GSmid=46580804&GRid=7768640& Findagrave: Joseph O'Malley]

[[Category:O'Malley family]]
[[Category:Directories]]
[[Category:History of New York City]]


{{US-stub}}
<=====doc_Id=====>:450
<=====title=====>:
High Weirdness by Mail
<=====text=====>:
{{italic title}}
'''''High Weirdness By Mail – A Directory of the Fringe: Crackpots, Kooks & True Visionaries''''', by [[Ivan Stang]] (ISBN 0-671-64260-X) is a 1988 book dedicated to an examination of "weird culture" by actually putting the reader in touch with it by mail.

The book is divided into sections&mdash;"Weird Science," "UFO Contactees," "Drug Stuff," and others, and each section contains a variety of mini-articles describing organizations. Each organization article concludes with a mailing address (and in some cases, phone numbers).

Several years after the book's publication, Stang reported on the [[newsgroup]] [[alt.slack]] that his inclusion of entries for [[white supremacist]] groups in the book caused his name to be mentioned by those groups as a possible target for retaliation.  (The book's commentaries on various [[hate group]]s were less than flattering.)  Stang reported this incident to the [[FBI]], but did not receive any actual harassment or threats from the groups in question.

The [[Association for Consciousness Exploration]] produced a follow-up lecture by Rev. Stang on cassette entitled ''High Weirdness by Mail'', recorded live at the 1993 WinterStar Symposium.

== Controversy ==

[[Bob Black]] claims that his review of ''High Weirdness By Mail'' was the cause of his being sent a small 'prank' mail bomb. [http://www.inspiracy.com/black/bomb.html]

== External links ==
* [http://www.subgenius.com Home Page of the Church of the SubGenius]
* [http://subgenius.com/hwbw.htm The Return of ''High Weirdness by Mail'']

[[Category:1988 books]]
[[Category:Church of the SubGenius]]
[[Category:Directories]]

{{Nonfiction-book-stub}}
<=====doc_Id=====>:453
<=====title=====>:
Vsya Moskva
<=====text=====>:
{{italic title}}
{{unreferenced|date=December 2008}}
'''''Vsya Moskva''''' (literally translated "''All Moscow''" or "''The Entire Moscow''") was a series of [[city directories]] of [[Moscow]], Russia, published on a yearly basis from 1872 to 1936 by [[Aleksei Sergeevich Suvorin]]. 
The directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices,  public services and medium and large businesses present in the city. Each volume was anywhere between 500 and 1500 pages long.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.

==List of residents==
Each directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an [[alphabetical]] list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed. 

The following information can be found:
*Person's surname and first name
*[[Patronymic]]
*Street address with apartment number
*[[Profession]]
*Telephone numbers (few private residents could afford a [[telephone]] before 1918)

==List of occupants of each building on every street and square==
A section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.

==Other sections==
The following information can also be found in each directory:

*Maps of the city
*Interior theater seating plan layouts
*Lists of personnel in state, public and private institutions
*Original Advertising

== Interruption in the series ==

No volumes were published in the following years:
*1918
*1919
*1920
*1921

This was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].

== Termination of series ==

Publication came to a halt after the edition of 1936, coinciding with the time of [[Joseph Stalin]]'s [[great purge]]s and [[Moscow Trials]].

==Historical and genealogical value==
Because numerous residents emigrated from Moscow after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city, and under which address.

==Availability==
Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the [[United States]], [[Europe]] (including [[The Baltics]], [[Finland]] the [[United Kingdom]] and [[Germany]]) however most only have an incomplete collection.

==Other city directories in Russia==
Suvorin also published city directories for [[Saint Petersburg]] under the title ''[[Ves Petersburg]]'' (''All Petersburg'') for the years 1894 to 1940 and for the whole country under the titles ''[[Vsya Rossiya]]'' (''All Russia'') from 1895 to 1923 and continued under than name ''[[Ves SSSR]]'' (''All USSR'') from 1924 to 1931.

== See also ==

*''[[Ves Petersburg]]''
*''[[Vsya Rossiya]]''

== External links ==
*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]

[[Category:Directories]]
[[Category:History of Moscow]]
[[Category:Russian non-fiction books]]
[[Category:Media in Moscow]]
[[Category:1872 books]]
<=====doc_Id=====>:456
<=====title=====>:
Gallia Christiana
<=====text=====>:
The '''''Gallia Christiana''''', a type of work of which there have been several editions, is a documentary catalogue or list, with brief historical notices, of all the Catholic dioceses and abbeys of France from the earliest times, also of their occupants.

== First efforts ==

In 1621 [[Jean Chenu]], an ''[[avocat]]'' at the [[Parlement of Paris]], published ''Archiepiscoporum et episcoporum Galliæ chronologica historia''. Nearly a third of the bishops are missing, and the episcopal succession as given by Chenu was very incomplete. In 1626, Claude Robert, a priest of [[Langres]], published with the approbation of [[Baronius]], a ''Gallia Christiana''. He entered a large number of churches outside of [[Gaul]], and gave a short history of the [[metropolitan see]]s, cathedrals, and abbeys.

== The Samarthani ==

Two brothers de Sainte-Marthe, Scévole (1571–1650) and Louis (1571–1656), appointed royal historiographers of France in 1620, had assisted Chenu and Robert. At the [[assembly of the French Clergy]] in 1626, a number of prelates commissioned these brothers to compile a more definitive work. They died before the completion of their work, and it was issued in 1656 by the sons of [[Scévole de Sainte-Marthe]], [[Pierre de Sainte-Marthe]] (1618–90), himself historiographer of France, [[Abel de Sainte-Marthe]] (1620–71), theologian, and later general of the [[Oratory (worship)|Oratory]], and [[Nicolas-Charles de Sainte-Marthe]] (1623–62), prior of [[Claunay]]. On 13 September 1656, the Sainte-Marthe brothers were presented to the assembly of the French Clergy, who accepted the dedication of the work on condition that a passage suspected of [[Jansenism]] be suppressed. The work formed four volumes [[in folio]], the first for the [[archdiocese]]s, the second and third for the dioceses, and the fourth for the abbeys, all in alphabetical order.<ref>The title was ''Gallia Christiana, qua series omnia archiepiscoporum, episcoporum et abbatum Franciæ vicinarumque ditionum ab origine ecclesiarum ad nostra tempora per quattor tomos deducitur, et probator ex antiquæ fidei manuscriptis Vaticani, regnum, principum tabulariis omnium Galliæ cathedralium et abbatarium''.</ref> It reproduced a large number of manuscripts. Defects and omissions, however, were obvious. The Sainte-Marthe brothers themselves announced in their preface the early appearance of a second edition corrected and enlarged. 

As early as 1660 the Jesuit [[Jean Colomb]] published at Lyons the ''Noctes Blancalandanæ'', which contains certain additions to the work of the Samarthani, as the brothers and their successors are often called. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud." The edition promised by the Sainte-Marthe brothers did not appear.

== Revision by the Maurists ==

In 1710 the Assembly of the French Clergy offered four thousand livres to [[Denys de Sainte-Marthe]] (1650–1725), a Benedictine monk of the [[Congregation of Saint-Maur]], renowned for his polemics against the Trappist [[Abbé de Rancé]] on the subject of monastic studies, on condition that he should bring the revision of the ''Gallia Christiana'' to a successful conclusion, that the first volume should appear at the end of four years, and that his Congregation should continue the undertaking after his death. Through his efforts the first volume appeared in 1715, devoted to the ecclesiastical provinces of Albi, Aix, Arles, Avignon, and Auch. In 1720 he produced the second volume dealing with the provinces of Bourges and Bordeaux; and in 1725 the third, which treated Cambrai, Cologne, and Embrun. 

After his death the Benedictines issued the fourth volume (1728) on Lyons, and the fifth volume (1731) on Mechelen and Mainz. Between 1731 and 1740, on account of the controversies over the Bull ''[[Unigenitus]]'', Dom [[Félix Hodin]] and Dom [[Etienne Brice]], who were preparing the latter volumes of the ''Gallia Christiana'', were expelled from [[Saint-Germain-des-Prés]]. They returned to Paris in 1739 and issued the sixth volume, dealing with Narbonne, also (1744) the seventh and eighth volumes on Paris and its [[suffragan see]]s. [[Père Duplessis]] united his efforts with theirs, and the ninth and tenth volumes, both on the [[province of Reims]], appeared in 1751. The eleventh volume (1759) dealing with the [[province of Rouen]] was issued by Père [[Pierre Henri]] and Dom [[Jacques Taschereau]]. In 1770 the twelfth volume on the [[province of Sens]] and [[province of Tarentaise]] appeared, and in 1785 the thirteenth, on the provinces of Toulouse and Trier. 

At the outbreak of the revolution, four volumes were lacking: Tours, Besançon, Utrecht, and Vienne. Barthélemy Hauréau published (in 1856, 1860 and 1865), for the provinces of Tours, Besançon and Vienne, respectively, and according to the Benedictine method, the fourteenth, fifteenth and sixteenth volumes of the ''Gallia Christiana''. 

The province of Utrecht alone has no place in this great collection, but this defect has been remedied in part by the ''Bullarium Trajectense'', edited by [[Gisbert Brom]], and extending from the earliest times to 1378 (The Hague, 1891–96). 

The new ''Gallia Christiana'', of which volumes I to V and XI to XIII were reprinted by Dom [[Paul Piolin]] between 1870 and 1877, and volumes VI to IX and XII by the publisher H. Welter, places after each metropolitan see its suffragan sees, and after each see the abbeys belonging to it. The original documents, instead of encumbering the body of the articles, are inserted at the end of each diocese under in a section titled ''Instrumenta''. This colossal work does great honour to the Benedictines and to the Sainte-Marthe family. "The name of Sainte-Marthe", wrote Voltaire, "is one of those of which the country has most reason to be proud."

== Later works ==

In 1774 the Abbé [[Hugues du Temps]], vicar-general of Bordeaux, undertook in seven volumes an abridgement of the ''Gallia'' under the title "Le clergé de France" of which only four volumes appeared. About 1867 [[Honoré Fisquet]] undertook the publication of an episcopal history of France ([http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF34044240.htm]''La France Pontificale''), in which, for the early period, he utilized the ''Gallia'', at the same time bringing the history of each diocese down to modern times. Twenty-two volumes appeared. 

[[Canon Albanès]] projected a complete revision of the ''Gallia Christiana'', each ecclesiastical province to form a volume. Albanès, who was one of the first scholars to search the Lateran and Vatican libraries, in his efforts to determine the initial years of some episcopal reigns, found occasionally either the acts of election or the Bulls of provision. He hoped in this way to remove certain suppositious bishops who had been introduced to fill gaps in the catalogues, but died in 1897 before the first volume appeared. Through the use of his notes and the efforts of Canon [[Ulysse Chevalier]] three addition volumes of this "Gallia Christiana (novissima)", treating Arles, Aix, and Marseilles, appeared at Montbéliard.

== See also ==
* [[Jean-Barthélemy Hauréau]]

== References ==

<references/>
* [[Dreux du Radier]], ''Bibliothèque historique et critique du Poitou'' (Paris, 1754)
* ''Gallia Christiana'', Vol. IV, Préface
* ''Gallia Christiana (novissima)'' (Montbéliard, 1899), Préface to the Aix volume
* [[de Longuemare]], ''Une famille d'auteurs aux seizième, dix-septième et dix-huitième siècles; les Sainte-Marthe'' (Paris, 1902)
* Victor Fouque, ''Du "Gallia christiana" et de ses auteurs: étude bibliographique'', Paris: E. Tross, 1857. Available on the Bibliothèque nationale's [http://gallica.bnf.fr/Catalogue/noticesInd/FRBNF30453708.htm ''Gallica''] site.

== External links ==
* {{CathEncy|url=http://www.newadvent.org/cathen/06350c.htm|title=Gallia Christiana}}

{{Catholic|wstitle=Gallia Christiana}}

[[Category:Directories]]
[[Category:Religious studies books]]
<=====doc_Id=====>:459
<=====title=====>:
Writer's Market
<=====text=====>:
{{italic title}}
{{Advert|date=March 2011}}
'''''Writer's Market'' (''WM'')''' is an annual resource book for writers who wish to sell their work. The publication is released by ''[[Writer's Digest]]'' Books (an imprint of [[F+W Media]]) and usually hits bookstores around June of each year. ''Writer's Market'' was first published in 1921, and is often called "The Bible for writers" or "the freelancer's Bible."<ref>http://search.barnesandnoble.com/Writers-Market-2008/Robert-Lee-Brewer/e/9781582974965</ref><ref>http://www.epinions.com/review/Book_Writers_Market_2007/content_298510028420</ref><ref>http://www.thegoodwebguide.co.uk/index.php?rid=000467</ref>

The most current edition is the 2016 edition; the current editor is Robert Lee Brewer.

== Listings ==
For 89 years, the book has listed thousands of markets for writers who wish to sell their work. Said markets include magazines, newspapers, theaters (for stage plays), production companies, contests of all types, greeting card companies, literary agents, and more. Each listing has detailed instructions on how to submit work, relevant contact information, as well as what work each listing seeks.

== Articles ==
The upfront section of ''WM'' has more than a dozen articles on writing topics, such as starting a freelancing business, syndication, freelancing for magazines, and a chart filled with typical payment rates concerning various writing assignments.

== "Market Books" ==
''Writer's Market'' is one of nine "[[Market (economics)|market]] books" published each year by [[Writer's Digest Books]]. Others include: ''Guide to Literary Agents'', ''Photographer's Market'', ''Children's Writer's & Illustrator's Market'', ''Novel & Short Story Writer's Market'', ''Artist and Graphic Designer's Market'', ''Poet's Market'', ''Screenwriter's & Playwright's Market'' and ''Songwriter's Market''. Each book is designed to give writers instructions on how to submit freelance work to markets.

== See also ==
* [[Publishing]]
* ''[[Writer's Digest]]''
* [[literary agent]]
* [[Literary agent#Querying|query]]
* [[screenplay]]
* [[royalties]]
* [[Authors Guild]]
* [[poetry]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://www.writersmarket.com|The book's official website}}
*[http://www.writersdigest.com ''Writer's Digest'' magazine official site]

[[Category:Directories]]
[[Category:Literary agencies|.]]
[[Category:Literary agents|.]]
[[Category:American literary agencies]]
<=====doc_Id=====>:462
<=====title=====>:
Artists' Bluebook
<=====text=====>:
{{primary sources|date=October 2013}}
The '''Artists' Bluebook''' is an international [[database]] of over 270,000 visual artists developed by AskART since 1999 (http://www.askart.com/AskART/help/AskART_about_us.aspx). Revised from its original 1993 print and CD format to digital online access, the Artists' Bluebook is considered a favorite resource for research into artists' lives, artworks and values, and where to buy or sell.

==External links==
* [http://www.askart.com/AskART/index.aspx The Artists' Bluebook website]
* [http://www.ala.org/rusa/sections/mars/marspubs/marsbestfreewebsites/marsbestref2003 AskART Bluebook 2003 - review by American Library Association(ALA)]
[[Category:Directories]]
<=====doc_Id=====>:465
<=====title=====>:
EADP
<=====text=====>:
{{refimprove|date=August 2014}}

The '''European Association of Directory and Database Publishers''', known as '''EADP''', was founded in 1966. EADP is the key representative for the [[Europe|European]] directory and database publishing sector. As such, EADP has 180 members from 36 countries and represents the interests of some 340 [[Telephone directory|directory]] publishers. The associations members and affiliate members include [[Publishing|publishers]] and stakeholders from the industry such as suppliers and vendors.<ref>{{cite web|title=About EADP|url=http://www.eadp.org/index.php?q=aboutus|accessdate=25 July 2013}}</ref>

EADP's activities include:

* Maintaining an up-to-date member directory
* Facilitating an annual congress and a separate annual conference
* Monitoring EU legal activities of relevance to the industry
* Compiling an annual statistical report and benchmarking studies

The [[North America|North American]] counterpart to the EADP is the [[Yellow Pages Association]] (YPA).

== References ==
<references />

==External links==
*[http://www.ypassociation.org/ YPA web-site]
*[http://www.eadp.org/ EADP web-site]

{{DEFAULTSORT:Eadp}}
[[Category:Companies established in 1966]]
[[Category:Directories]]


{{telephony-stub}}
<=====doc_Id=====>:468
<=====title=====>:
Boston Directory
<=====text=====>:
{{italic title}}
[[Image:1807 BostonDirectory title page.png|100px|thumb|1807 Boston Directory [[title page]]]]
'''''The Boston Directory''''' of  [[Boston]], [[Massachusetts]], was first published in 1789. It contained "a list of the merchants, mechanics, traders, and others, of the town of Boston; in order to enable strangers to find the residence of any person." Also included were listings for public officials, doctors, bank directors, and firemen.<ref>Boston Directory. 1789.</ref> The directory was issued annually after 1825; previously it had appeared irregularly.

The number of listings in each directory reflected fluctuations in the population size of Boston. In 1789, the directory included some 1,474 listings; by 1875, there were 126,769.<ref name="auto">Advertisement for Boston Directory. Boston Almanac, 1875.</ref>

Publishers included John Norman (1789); John West (1796-1803); Edward Cotton (1805-1818); Charles Stimpson (1820-1846); George Adams (1846-1857);<ref>{{citation |url=https://books.google.com/books?id=Ors-AAAAYAAJ&pg=PA87 |year=1866 |title=New England Historical & Genealogical Register }}</ref> Adams, Sampson & Co. (1858-1865); Sampson, Davenport & Co. (1865-1884); Sampson, Murdock & Co. (1885-1903); Sampson & Murdock Co. (1904-ca.1930); [[R.L. Polk & Co.]] (1944-ca.1980).<ref name="auto"/><ref>{{cite web|url=http://www.worldcat.org/oclc/228685309|title=The Boston directory ... including all localities within the city limits, as Allston, Brighton, Charlestown, Dorchester, Hyde Park, Roslindale, Roxbury, West Roxbury ...|work=worldcat.org}}</ref>
{{TOC right}}

==Boston Directories==

===18th century===
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| <!-- PUBLISHER -->
| 1789
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectory00sampgoog#page/n10/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston Directory
| <!-- PUBLISHER -->
| 1796
| [https://books.google.com/books?id=CJFIAAAAYAAJ&client=safari&pg=RA1-PA215#v=onepage&q=&f=false reprint via Google Books, p.&nbsp;215-302]
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/1 via Boston Athenæum]
|-
| Boston Directory
| <!-- PUBLISHER -->
| 1798
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/2 via Boston Athenæum]
|}

===19th century===

====1800-1829====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| <!-- PUBLISHER -->
| 1800
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/3 via Boston Athenæum]
|-
| Boston Directory
| John West
| 1803
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/4 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1805
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| [https://archive.org/stream/bostondirectory00inbost#page/n9/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston Directory
| Edward Cotton
| 1806
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/5 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1807
| <!-- GOOGLE -->
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Edward Cotton
| 1809
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/7 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1810
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/8 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1813
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/9 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1816
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/10 via Boston Athenæum]
|-
| Boston Directory
| Edward Cotton
| 1818
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/11 via Boston Athenæum]
|-
| Boston Directory
| Frost and Stimpson
| 1820
| <!-- GOOGLE -->
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Frost and Stimpson
| 1821
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/12 via Boston Athenæum]
|-
| Boston Directory
| Frost and Stimpson
| 1822
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/15 via Boston Athenæum]
|-
| Boston Directory
| Frost and Stimpson
| 1823
| [https://books.google.com/books?id=nY4vAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Frost and Stimpson
| 1825
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/010363295 via HathiTrust]
| [https://archive.org/stream/bostondirectorys1825bost via Internet Archive]
| <!-- OTHER -->
|-
| Boston Directory
| Frost and Stimpson
| 1826
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/17 via Boston Athenæum]
|-
| Boston Directory
| Hunt, Stimpson, and Frost
| 1827
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/18 via Boston Athenæum]
|-
| Boston Directory
| Hunt and Stimpson
| 1828
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/19 via Boston Athenæum]
|-
| Boston Directory
| Charles Stimpson, Jr.
| 1829
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/20 via Boston Athenæum]
|-
|}

====1830-1849====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| <!-- PUBLISHER -->
| 1830
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/41 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1831
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/stimpsonsbostond3132adam#page/n29/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1832
| [https://books.google.com/books?id=raQtAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1833
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/27 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1834
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| [https://archive.org/stream/bostondirectory01bost#page/n5/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1835
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectory03bost#page/n5/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1836
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/010363296 via HathiTrust]
| [https://archive.org/stream/stimpsonsbostond1836adam#page/n21/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1837
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/34 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1838
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/35 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1839
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/40 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1840
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/39 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1841
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/37 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1842
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/36 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1843
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/38 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1844
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/45 via Boston Athenæum]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1845
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/44 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Stimpson's Boston Directory
| Stimpson & Clapp
| 1846
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/43 via Boston Athenæum]
|-
| Adams's New Directory of the City of Boston
| George Adams
| 1846-47
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/25 via Boston Athenæum]
|-
| Adams's Boston Directory
| French and Stimpson
| 1847-48
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/24 via Boston Athenæum]
|-
| Boston Directory
| French and Stimpson
| 1848-49
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| [https://archive.org/stream/bostondirectory4849bost#page/n7/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston Directory
| George Adams
| 1849-50
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| [https://archive.org/stream/bostondirectory00bost#page/n7/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
|}

====1850-1869====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Directory of the City of Boston
| George Adams
| 1850
| [https://books.google.com/books?id=UHDPAAAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| George Adams
| 1851
| [https://books.google.com/books?id=C6UqAAAAYAAJ via Google Books]
| [http://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| George Adams
| 1852
| [https://books.google.com/books?id=2tsCAAAAYAAJ via Google Books]
| <!-- HATHI -->
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| George Adams
| 1853
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/48 via Boston Athenæum]
|-
| Boston Directory
| George Adams
| 1854
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/81 via Boston Athenæum]
|-
| Boston Directory
| George Adams
| 1855
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/82 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| George Adams
| 1856
| [https://books.google.com/books?id=zYMqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| George Adams
| 1857
| [https://books.google.com/books?id=nYIqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1858
| [https://books.google.com/books?id=En4qAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1859
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1860
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/87 via Boston Athenæum]
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1861
| [https://books.google.com/books?id=hHwqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/singleitem/collection/p16057coll32/id/93/rec/57 via Boston Athenaeum]
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1862
| [https://books.google.com/books?id=tH4qAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/91 via Boston Athenæum]
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1863
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/92 via Boston Athenæum]
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1864
| [https://books.google.com/books?id=8IEqAAAAYAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Adams, Sampson, & Co.
| 1865
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/95 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1866
| [https://books.google.com/books?id=_A5FAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/96 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1867
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100734377 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/97 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1868
| [https://books.google.com/books?id=SFwJAQAAIAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1869
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/100321756 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/69 via Boston Athenæum]
|-
|}

====1870-1889====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1870
| [https://books.google.com/books?id=GytFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/98 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1871
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/99 via Boston Athenæum]
|-
| Boston Commercial Directory
| Wentworth & Co.
| 1871
| [https://books.google.com/books?id=xfACAAAAYAAJ via Google Books]
| <!-- HATHI -->
| <!-- IA -->
| <!-- OTHER -->
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1872
| <!-- GOOGLE -->
| [http://babel.hathitrust.org/cgi/pt?id=hvd.32044092998012;view=1up;seq=19 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/65 December] supplement via Boston Athenæum<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1873
| [https://books.google.com/books?id=NqHNAAAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/67 November], [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/66 December] supplements via Boston Athenæum
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1874
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/108 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1875
| [https://books.google.com/books?id=EC5FAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/104 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1876
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/105 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1877
| [https://books.google.com/books?id=RTBFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/103 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1878
| <!-- GOOGLE -->
| [http://hdl.handle.net/2027/uc1.c047888986 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/110 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1879
| <!-- GOOGLE -->
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/111 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1880
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/107 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1881
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/109 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1882
| [https://books.google.com/books?id=NSFFAQAAMAAJ via Google Books]
| [https://catalog.hathitrust.org/Record/000499337 via HathiTrust]
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/112 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1883
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/113 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Davenport, & Co.
| 1884
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/114 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1885
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/116 via Boston Athenæum]<br>[http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1886
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/118 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1887
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/117 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1888
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/119 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1889
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/121 via Boston Athenæum]
|-
|}

====1890-1899====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1890
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/123 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1891
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/124 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1892
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/125 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1893
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/126 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1894
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/127 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1895
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/128 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1896
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/129 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1897
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/130 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1898
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/131 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1899
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/132 via Boston Athenæum]
|-
|}

===20th century===

====1900-1949====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1900
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://cdm.bostonathenaeum.org/cdm/ref/collection/p16057coll32/id/133 via Boston Athenæum]
|-
| Boston Directory
| Sampson, Murdock, & Co.
| 1905
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
| Boston Directory
| Sampson & Murdock Co.
| 1916
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostonmassachuse1916112samp#page/n9/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston Register and Business Directory
| Sampson & Murdock Co.
| 1922
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostonregisterbu1922bost#page/n13/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston Directory
| Sampson & Murdock Co.
| 1925
| <!-- GOOGLE -->
| <!-- HATHI -->
| <!-- IA -->
| [http://dca.lib.tufts.edu/features/bostonstreets/people/directories.html via Tufts University]
|-
|}

====1950-1999====
{| class="wikitable" style="font-size: 95%;"
!Title||Publisher||Year||Google Books||[[HathiTrust]]||Internet Archive||Other
|-
| Boston City Directory
| R.L. Polk & Co.
| 1955
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi00bost#page/n7/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1956
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi56bost#page/n3/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1959
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi002bost via Internet Archive]
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1961
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi11961bost#page/n3/mode/2up v.1], [https://archive.org/details/bostondirectoryi261bost v.2] via Internet Archive
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1962
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi162bost#page/n3/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1965
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi11965bost#page/n3/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1966
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi1966bost#page/n3/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1969
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi169bost#page/n3/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
| Boston City Directory
| R.L. Polk & Co.
| 1970
| <!-- GOOGLE -->
| <!-- HATHI -->
| [https://archive.org/stream/bostondirectoryi170bost#page/n3/mode/2up via Internet Archive]
| <!-- OTHER -->
|-
|}

==See also==
* ''[[Boston Almanac|Boston Almanac and Business Directory]]''
* ''[[Boston Almanac|Boston Register and Business Directory]]''
* ''[[Massachusetts Register]]''

==References==
{{reflist}}

==Further reading==
* [https://books.google.com/books?id=ALIUAAAAYAAJ New England historical and genealogical register]. Oct. 1862; p.&nbsp;387+
* [https://books.google.com/books?id=CJFIAAAAYAAJ Report of the record commissioners of the city of Boston], Volume 10. Rockwell and Churchill, 1886; p.&nbsp;163+
* {{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |title = The Development and Growth of City Directories |publication-date = 1913 |chapter=Boston, Massachusetts |chapterurl=http://hdl.handle.net/2027/nyp.33433082423645?urlappend=%3Bseq=61 }}

==External links==
* HathiTrust. [https://catalog.hathitrust.org/Record/010363295 1805 etc]; [https://catalog.hathitrust.org/Record/000499337 1849-1883]
* [http://www.damrellsfire.com/cgi-bin/directory_search.pl damrellsfire.com]
* [http://cdm.bostonathenaeum.org/cdm/landingpage/collection/p16057coll32 Boston Athenæum: The Boston Directory 1789-1900 (Ongoing Project), Digital Collection].

[[Category:History of Boston]]
[[Category:Directories]]
[[Category:Publications established in 1789]]
[[Category:18th century in Boston]]
[[Category:19th century in Boston]]
[[Category:20th century in Boston]]
[[Category:1789 establishments in Massachusetts]]
<=====doc_Id=====>:471
<=====title=====>:
Sources (website)
<=====text=====>:
{{Use dmy dates|date=May 2014}}
{{More footnotes|date=November 2011}}
'''Sources''' is a [[web portal]] for journalists, freelance writers, editors, [[authors]] and [[researchers]], focusing especially on human sources: [[expert]]s and spokespersons who are prepared to answer [[Reporter]]s' questions or make themselves available for on-air [[interview]]s.

==Structure==
The Sources website is built around a [[Controlled vocabulary|controlled-vocabulary]] subject index comprising more than 20,000 topics. This [[Subject indexing|subject index]] is underpinned by an 'Intelligent Search' system which helps reporters focus their searches by suggesting additional subjects related to their search terms. For example, a search for "cancer" will suggest terms such as "chemotherapy", "melanoma", "oncology", "radiation therapy", "tobacco diseases" and "tumours", as well as topics that actually contain the word "cancer".

Each topic reference links in turn to experts and spokespersons on that topic, with profiles describing their expertise and, where relevant, their approach to the issue, along with their phone numbers and other contact information. Sources includes listings for universities and research institutes, non-profit associations and NGOs, government and public sector bodies, businesses, and individuals including academics, public speakers, and consultants.

The subject index and the search menus are being translated into French, Spanish and German to make Sources more of an international resource.

==History==

===Print supplement===
Based in Canada, Sources was founded in 1977 as a print directory for reporters, editors, and story producers. It was first published as a supplement to ''Content'' magazine, an influential and controversial magazine of journalism criticism. ''Content'', founded by Dick MacDonald in 1970 and published by [[Barrie Wallace Zwicker|Barrie Zwicker]] after MacDonald's death in 1974, frequently took journalists to task for always relying on the same narrow range of sources representing the same conventional points of view for their stories. Zwicker and MacDonald argued in ''Content'' and in their book ''The News: Inside the Canadian Media''<ref>MacDonald, Dick; Zwicker, Barrie. ''The News: Inside the Canadian Media''. Deneau. 1982. ISBN 0-88879-053-8</ref> that there was a “terrible sameness” in the media’s coverage of many important issues, and a shutting out of other, potentially valuable, perspectives and sources of information.

Zwicker decided to do something about the problem, and in summer 1977, ''Content'' published its first directory issue, called Sources. Billed as “A Directory of Contacts for Editors and Reporters in Canada”, Sources listed “information officers, public relations officers, media relations and public affairs people, and other contacts for groups, associations, federations, unions, societies, institutions, foundations, industries and companies and federal, provincial and municipal ministries, departments, agencies and boards.”<ref>Sources: A Directory of Contacts for Editors and Reporters in Canada. ''Content''. 1977. {{ISSN|0045-835X}}</ref>

Explaining the rationale behind Sources, Zwicker said that “It’s a cliché that every story has two sides. An untrue cliché. Most have several. The reporter’s challenge is digging out all sides. Sources can help.”<ref>Sources 50. 2002. ISBN 0-920299-55-5</ref> From the beginning, Zwicker saw Sources as a public service as well as a tool for journalists. He said that Sources aimed “to help promote a system of information fairness. Communications resources are equivalent to other basic needs – shelter, food, health care, for example. Everyone should have reasonable access to all.”<ref name="ReferenceA">Sources 36. 1995. ISBN 0-920299-24-5</ref> Therefore, he said “we attempt to provide true diversity: access to people in organizations large and small, for-profit and not-for-profit, from low-tech to high-tech, long-established to just-launched.”<ref name="ReferenceA"/>

Zwicker told users that “within Sources you will find both mainstream and alternative information. Some may consider alternative as off to one side, not quite up to par, more or less second hand. Here at Sources ‘alternative’ is considered differently, considered as authentic and substantial, even if normally less accessible. The surprises, the jarring notes, the flashes of insight, the ‘odd takes’, the pearls of wisdom, the cries de coeur, the avant garde, tomorrow’s news, the prophesies, the unfiltered, the exciting, the elsewhere-squelched, the memorable, the eccentric, the thought-out-at-length, the unmentionable in polite company, the outrageous, the uncensored ... these are what ‘alternative’ media offer. So far as we can, we will include the alternative with Sources. Sources’ driving philosophy is flat-out informational democracy enabled by user-friendly technology. The assumption is that there is a significant fraction of Canadians who want to use and benefit from such an information resource. The assumption is that a significant fraction of Canadians want to expand their search for solutions, and deepen their understandings, rather than chant conventional wisdoms (however freshly minted) to each other.”<ref name="ReferenceA"/>

===Separate publications===
After a few years, Sources become so big that it could no longer fit into ''Content'' (the print directory eventually grew to more than 500 pages), and in 1981 it became an independent publication. ''Content'' itself eventually folded, but Zwicker continued to devote a substantial editorial section in Sources to coverage of topics of interest to journalists, ranging from practical topics such as grammar, style, [[fact-checking]], [[photojournalism]], [[copyright]], fees for freelancers and [[self-publishing]], to feature articles on the state of journalism and the media, to book reviews. From the early 1990s, Sources began to feature articles about online research, notably the regular feature 'Dean's Digital World'<ref>[Dean's Digital World – http://www.sources.com/SSR/DeansDigital.htm</ref> by informatics expert Dean Tudor.

===World Wide Web===

====Content====
Sources went on the Internet in 1995 and has been expanding its online portal ever since. It continues to publish a print edition of the directory, primarily for the benefit of freelancers who use it as a source of story ideas, but is now primarily a Web-based resource.

The Sources website includes not only the Sources directory itself, but a separate government directory, Parliamentary Names & Numbers; a directory of the media, Media Names & Numbers; and The Sources HotLink  [http://www.hotlink.ca (www.hotlink.ca)], which features articles about media relations and public relations. Also on the site is [http://www.sources.com/Fandf/Index.htm Fame and Fortune], a directory of awards, prizes, and scholarships available to writers and journalists, and a portal linked into the online archive of [[Connexions (Information Sharing Services)|Connexions]], a library of documents related to alternatives and social justice.

The site also houses Sources Select Resources,<ref>Sources Select Resources – http://www.sources.com/SSR.htm</ref> a large library of articles and reviews about journalism and the media, spanning a period of more than 30 years.

====Controversy====
While much of the editorial content has focused on the nitty-gritty of writing, editing and research, Sources has also regularly published articles that have sparked controversy on topics such as censorship and [[media bias]]. One campaign waged by Zwicker and others challenged the [[journalism ethics|ethics]] of journalists accepting free gifts from the people they are supposed to cover. This campaign eventually led Canadian managing editors to agree among themselves that their newspapers would not accept free tickets from travel agencies, resorts, and hotels.

A series of articles by Zwicker on "War, Peace, and the Media"<ref>Zwicker, Barrie. ''War, Peace and the Media''. Sources. 1983, 1985</ref> (later collected and published as a booklet) provoked a furor from readers upset by its criticisms of how the media cover [[United States foreign policy|U.S. foreign policy]]. As Zwicker put it in a publisher's letter in the next issue, the "reaction ranged from high praise to angry denunciation." The ''[[Toronto Sun]]'' newspaper devoted three stories to the series. The columnist Claire Hoy was left "trembling with rage" and the editor [[Peter Worthington]] felt "outraged" and a lead editorial denounced Zwicker.

Other controversial articles included one by Wendy Cukier on the public relations battle surrounding proposed [[Gun politics in Canada|gun control]] legislation, which drew the ire of the gun lobby.<ref>Cukier, Wendy. "Anatomy of the Gun Control Debate". Sources. 1996 – http://www.sources.com/SSR/Docs/PNN5-1-GunControl.htm</ref> Ulli Diemer, who succeeded Zwicker as publisher in 1999, came under attack from the [[Fraser Institute]] for his article "Ten Health Care Myths: Understanding Canada’s Medicare Debate”, in which he argued that opponents of [[public health care]] were spreading [[Misinformation|mis-information]] designed to mislead and frighten the public.<ref>Diemer, Ulli. 'Ten Health Care Myths: Understanding Canada’s Medicare Debate’. Sources. 1995. – http://www.diemer.ca/Docs/Diemer-TenHealthCareMyths.htm</ref>

====New resources====
In keeping with its mandate of encouraging a wide diversity of points of view in the media, Sources has added extra resources over time to help organizations and individuals to be heard. These include a calendar of events open to the media<ref>Sources Calendar – http://calendar.sources.com</ref> and a [[news release]] service which Sources members can use to distribute their statements and communiques via online posting and [[RSS]]. The releases are also subject indexed and integrated into the overall search structure for information on the Sources site.

==See also==
* [[Barrie Wallace Zwicker|Barrie Zwicker]]

== Notes ==
{{reflist|33em}}

==References==
{{refbegin|33em}}
* Basch, Reva. ''Secrets of the Super Net Searchers: The Reflections, Revelations, and Hard-won Wisdom of 35 of the World’s Top Internet Researchers''. Pemberton Press. 1996. ISBN 0-910965-22-6
* Berkman, Robert. ''The Skeptical Business Searcher: The Information Advisor’s Guide to Evaluating Web Data, Sites and Sources''. Information Today, 2004. ISBN 0-910965-66-8
* Bonner, Allan. ''Media Relations''. Briston House. 2003. ISBN 1-894921-00-3
* Carney, William Wray. ''In the News The Practice of Media Relations in Canada''. University of Alberta Press', 2002. ISBN 0-88864-382-9
* Comber, Mary Anne; Mayne, Robert S. ''The Newsmongers: How The Media Distort the Political News''. 1987. McClelland & Stewart
* Cormack, Paul G.; Shewchuk, Murphy (eds.) ''The Canadian Writers’ Guide''. 13th Edition. Canadian Authors Association. Fitzhenry & Whiteside, 2003. ISBN 1-55041-740-1
* Hackett, Robert A.; Gruneau, Richard. ''The Missing News: Filters and Blind Spots in Canada’s Press''. Newswatch Canada. Canadian Centre for Policy Alternatives & Garamond Press, 2000
* Hackett, Robert A. ''News and Dissent: The Press and The Politics of Peace in Canada''. 1993. Ablex.
* Hackett, Robert A.; Zhao, Yuezhi. ''Sustaining Democracy? Journalism and the Politics of Objectivity''. Garamond Press. 1998. ISBN 1-55193-013-7
* Kashmeri, Zuhair. ''The Gulf Within: Canadian Arabs, Racism, & The Gulf War''. James Lorimer. 1991
* MacDonald, Dick; Zwicker, Barrie. ''The News: Inside the Canadian Media''. Deneau. 1982. ISBN 0-88879-053-8
* Mann, Thomas. ''The Oxford Guide to Library Research''. Oxford University Press. 1998. ISBN 0-19-512313-1
* Manson, Katherine; Hackett, Robert; Winter, James; Gutstein, Donald; Gruneau, Richard (eds.) ''Blindspots in the News? Project Censored Canada Yearbook''. Project Censored Canada. 1995.
* McGuire, Mary; Stilborne, Linda; McAdams, Melinda; Hyatt, Laurel. ''The Internet Handbook for Writers, Researchers, and Journalists''. Trifolium Books. 1997, 2002. ISBN 1-895579-17-1
* Miljan, Lydia; Cooper, Barry Cooper. ''Hidden Agendas: How Journalists Influence the News''. University of British Columbia Press. 2003. ISBN 0774810203
* Miller, John. ''Yesterday’s News: Why Canada’s Daily Newspapers are Failing Us''. Fernwood Publishing, 1999
* Ouston, Rick. ''Getting the Goods: Information in B.C.: How to Find It, How to Use It''. New Star Books, 1990
* Patriquin, Larry. ''Inventing Tax Rage: Misinformation in the National Post''. Fernwood Publishing, 2004. ISBN 1-55266-146-6
* Soderlund, Walter C.; Hildebrandt, Kai (eds.) ''Canadian Newspaper Ownership in the Era of Convergence: Rediscovering Social Responsibility''. University of Alberta Press. 2005, ISBN 0-88864-439-6
* Tudor, Dean. ''Finding Answers: Approaches to Gathering Information''. McClelland & Stewart Inc., Toronto. 1993.
* Ward, Stephen J.A. ''The Invention of Journalism Ethics: The Path to Objectivity and Beyond''. McGill-Queen’s University Press. 2004. ISBN 0-7735-2810-5
* Winter, James. ''Media Think''. Black Rose Books. 2002. ISBN 1-55164-054-6
* Zwicker, Barrie. ''War, Peace and the Media''. Sources. 1983, 1985
{{refend}}

==External links==
* {{official website|http://www.sources.com/}}
** [http://www.sources.com/SSR.htm Sources Select Resources]
** [http://www.sources.com/News.htm Sources Select News]
** [http://calendar.sources.com Sources Calendar]
** [http://www.sources.com/Fandf/Index.htm Fame & Fortune]
* [http://www.hotlink.ca The Sources HotLink]
* [http://www.connexions.org Connexions Information Sharing Services]

{{DEFAULTSORT:Sources (Website)}}
[[Category:Directories]]
[[Category:Journalism organizations]]
[[Category:Knowledge markets]]
[[Category:Online databases]]
[[Category:Web directories]]
[[Category:Websites]]
<=====doc_Id=====>:474
<=====title=====>:
Mobile directory
<=====text=====>:
{{multiple issues|
{{Unreferenced|date=October 2010}}
{{orphan|date=October 2010}}
}}

A '''mobile directory''' is a collection of subscriber details of a [[mobile phone]] [[Mobile phone companies|operators]]. Generally, the mobile telephony operators do not publish a mobile [[directory (databases)|directory]]. Some third party [[websites]] offer mobile directory facility through [[Reverse telephone directory|reverse search]].

[[Category:Telephone numbers]]
[[Category:Directories]]


{{telephonenumber-stub}}
{{mobile-stub}}
<=====doc_Id=====>:477
<=====title=====>:
Blogged.com
<=====text=====>:
{{Orphan|date=February 2009}}
{{Infobox dot-com company
| name     = Blogged.com 
| logo     = 
| company_type     = [[Private company|Private]]
| foundation       = 2008
| founder          =
| location         = [[Alhambra, California]], [[United States]]
| key_people       = 
| revenue          = unknown
| operating_income = 
| net_income       = 
| owner            = 
| num_employees    = number unknown
| company_slogan   = 
| url              = [http://www.blogged.com/ www.blogged.com]
| screenshot       = [http://www.techcrunch.com/wp-content/blogged-small.png]
| caption          = Screenshot of Blogged.com home page
| alexa            = {{IncreaseNegative}} 16,125,052 ({{as of|2014|4|1|alt=April 2014}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/blogged.com |title= Blogged.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2014-04-01 }}</ref><!--Updated monthly by OKBot.-->
| website_type     = [[Wiki]] [[Blog directory]]
| language         = multilingual
| advertising      = 
| registration     = Optional
| launch_date      = {{launch date and age|2008|2|24|p=y}}
| current_status   = inactive{{Citation needed|date=May 2012}}
}}

'''Blogged.com''' is a blog directory that attempts to combine social networking with people's interests in blogging. It employs a method of niche social networking whereby people can connect to each other by their interests rather than their social connections. It attempts to use a method of [[crowdsourcing]] to help evaluate the quality of various blogs{{Citation needed|date=February 2008}}. Blogged symbolizes a trend of new sites that attempt to connect people with their interests rather than social connections{{Citation needed|date=February 2008}}. This type of niche social networking has been employed successfully by sites such as [[Flixster]], [[Yelp, Inc.|Yelp]], [[Last.FM]], and [[Stumbleupon]].  [[TechCrunch]] has recently compared Blogged.com to Yelp for blogs.<ref>
{{cite news
 | first = Erick 
 | last = Schonfeld
 | title = Blogged Hopes to Become the Yelp of Blog Directories
 | url = http://www.techcrunch.com/2008/02/24/blogged-hopes-to-become-the-yelp-of-blog-directories/
 |publisher=Tech Crunch
 |date=2008-02-24
 }}
</ref>

== Method ==

Blogged.com focuses on blog discovery and displays expert reviews and ratings on popular blogs thereby providing a basis from which to introduce new blogs to a potential reader. Traditional blog search sites such as Technorati and Google Blog Search offer users a method of searching through individual blog entries or postings, but not the blog website itself. Therefore, it is sometimes difficult to gauge the quality or importance of the search results since the credibility of the website which contains the blog entry may be in question. This method of propagating high-quality blogs via user feedback has been used by websites such as Digg. Digg allows users to vote on the importance of articles and causes those articles which are most popular to rise to the top. This method, commonly called [[crowdsourcing]], is being used by Blogged to utilize user feedback to gauge the importance of various blogs.{{Citation needed|date=February 2008}}

== Status ==

As of 2/3/2016 BLOGGED.COM does not resolve to any site and returns a "Not Found" message in the upper right corner.

The last known active date for Blogged.com is July 25, 2011.<ref>{{cite web|url=http://www.blogged.com/ |title=Blogged.com Last Crawled Date: July 25, 2011 |publisher=Internet Archive Wayback Machine |date= |accessdate=2012-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20110725121828/http://www.blogged.com/ |archivedate=July 25, 2011 }}</ref> The site currently redirects to [[Chime.in]].{{Citation needed|date=May 2012}}

== See also ==

* [[Facebook]]
* [[Yelp, Inc.]]
* [[Flixster]]
* [[Friendster]]
* [[Stumbleupon]]

==Notes==
{{reflist}}

==References==
* "[http://marketwire.com/mw/release.do?id=825026&k=blogged.com Blogged.com Connects Bloggers With Readers; Increases Traffic and Promotes Quality Content Filtered by People]," Marketwire, 2/25/2008.
* "[http://www.webware.com/8301-1_109-9877585-2.html Blogged.com launches blog directory, reviews]," ''[[CNET]] Blogs'', 2/25/2008.
* "[http://mashable.com/2008/02/24/bloggedcom/ Blogged.com. More than Just Another Blog Search Tool]," ''[[Mashable]]'', 2/25/2008.
* "[http://www.blogherald.com/2008/02/25/bloggedcom-new-blog-directory-officially-launches/ Blogged.com new blog directory officially launches]," Blogherald, 2/25/2008.
* "[http://www.bloggingtips.com/2008/02/25/bloggedcom-public-beta-goes-live/ Blogged.com Public Beta Goes Live]," BloggingTips, 2/25/2008.
* "[https://web.archive.org/web/20070214023246/http://publications.mediapost.com:80/index.cfm?fuseaction=Articles.showArticleHomePage Blogged.com Ranks Blogs For Consumers, Could Help Bloggers Monetize]," Online Media Daily, 2/25/2008

==External links==
* [https://web.archive.org/web/20110725121828/http://www.blogged.com/ Blogged.com]

{{DEFAULTSORT:Blogged.Com}}
[[Category:Blogging]]
[[Category:Directories]]
[[Category:American websites]]
[[Category:Alhambra, California]]
<=====doc_Id=====>:480
<=====title=====>:
Adelskalender (directory)
<=====text=====>:
'''Adelskalender''' ({{lang-de|Directory of Nobility}}) is a comprehensive directory of the nobility of a country or area. The best known such directory is the German [[Almanach de Gotha]] ("The Gotha") and its successor, the [[Genealogisches Handbuch des Adels]].

[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]
[[Category:Directories]]


{{royal-bio-book-stub}}
{{bio-dict-stub}}

[[cs:Adelskalender]]
[[de:Adelskalender]]
[[nl:Adelskalender]]
[[no:Adelskalender]]
[[fi:Aateliskalenteri]]
[[sv:Adelskalender]]
<=====doc_Id=====>:483
<=====title=====>:
Major Information Technology Companies of the World
<=====text=====>:
The '''Major Information Technology Companies of the World''' is a directory of [[information technology]] companies published by [[Graham & Whiteside]] annually since 1997. The directory contains over 8000 companies.<ref>{{cite web|url=http://www.gale.cengage.com/servlet/BrowseSeriesServlet?region=9&imprint=000&cf=ps&titleCode=MITCW|title=Major Information Technology Companies of the World|publisher=Cengage|accessdate=27 September 2011}}</ref> The directory is also available online as part of the Gale Directory Library.<ref>{{cite web|url=http://www.gale.cengage.com/pdf/facts/GML25909_MITCOW_Major_GDL.pdf|title=Major Information Technology Companies of the World|publisher=Gale|accessdate=27 September 2011|year=2009}}</ref>

==See also==
*[[Corporate Technology Directory]]

==References==
{{reflist}}

[[Category:Directories]]


{{technology-stub}}
<=====doc_Id=====>:486
<=====title=====>:
Clergy List
<=====text=====>:
The '''Clergy List''' was a professional directory of the [[Church of England]] which appeared between 1841-1917.  From the start it also covered Wales, together with more limited information relating to Scotland, Ireland, and other churches within the [[Anglican Communion]].

==Background and early contents==

An opportunity to compile and issue a new directory had been created by the effective disappearance of the earlier [[Clerical Guide or Ecclesiastical Directory]], edited by '''Richard Gilbert''', and also by the introduction of the much improved system of the [[Penny Post]]. 

The basic contents of the '''Clergy List''''s earlier editions was summarised on their title pages: 
*an alphabetical list of the clergy (or at least of those who held benefices)
*an alphabetical list of the benefices,with their post towns
*lists of the cathedral establishments
*benefices arranged under their ecclesiastical divisions
*lists of ecclesiastical preferments variously under the patronage of the Crown, the bishops, and the deans & chapters, etc.

The directory was always a bit less expensive than its later rival, [[Crockford's Clerical Directory]], but not surprisingly it consequently offered considerably less in the way of biographical detail.  This was especially true in the earlier editions which offered little or no information as to previous appointments, universities attended, or lists of publications by the clergy.

==Publishers and later history==

The directory was initially published by '''Charles Cox''' at the Ecclesiastical Directory Office, [[Southampton Street, London|Southampton Street]], [[Strand, London|Strand]].   Cox – who in 1839 had taken over a periodical called the '''Ecclesiastical Gazette,''' originating during the previous year – was able to produce two separate editions during the Clergy List's inaugural year of 1841.<ref name="paflin">[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article ''Shop-talk and mordant wit'', by Christopher Currie & Glyn Paflin, describing the background to ''Crockford's Clerical Directory'''s first hundred editions, 6–13 December 2007</ref>  Thereafter it managed to maintain annual publication right up until adverse trading conditions forced its closure as a separate volume in 1917.

Cox remained as the Clergy List's publisher for many years, but by 1881 the title had been taken over by John Hall of [[Whitehall|Parliament Street]], In 1888 it was further taken over by Hamilton, Adams & Company, of London's [[Paternoster Row]].  They had earlier acquired Thomas Bosworth's '''[[Clerical Guide and Ecclesiastical Directory]]''', merging the two titles in 1889.  During the following year the combined directory was still further transferred to Kelly & Company, the publishers of [[Kelly's Directories]].<ref name="paflin" /> 

The later volumes were considerably expanded to include much greater biographical detail – broadly comparable with Crockford – but this was not sufficient to sustain the publication in the longer term.  Over the years the number of pages also increased – ranging from around 300 in 1841 to around 700 by the 1890s. 

After 1917 the Clergy List finally merged with its long-time rival, '''Crockford's Clerical Directory'''.  At least as late as 1932 the latter continued to advertise on its preliminary pages that it "incorporated the '''Clergy List, the Clerical Guide and the Ecclesiastical Directory'''''".<ref name="paflin" />

In recent years certain of the earlier editions of the Clergy List (including the first edition <ref>The 1841 first edition of the ''Clergy List'' may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]</</ref>) have been reissued by various publishers – either on CD-ROM or in scanned format on the World Wide Web.

==References==
{{reflist}}

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]
<=====doc_Id=====>:489
<=====title=====>:
List of yellow pages
<=====text=====>:
{{multiple issues|
{{Advert|date=May 2010}}
{{Refimprove|date=May 2010}}
}}

[[Yellow pages]] [[telephone directories]] of businesses:

{{compact ToC|side=yes|top=yes|num=yes}}

==A==
* '''Afghanistan''': In [[Afghanistan]], the Canadian INGO [[Peace Dividend Trust]] launched a free online directory with over 2700 verified and registered Afghan enterprises in late 2006.
* '''Africa''': In [[Africa]], a business directory is YelloPagesAfrica published by ''Yellopages Development Company Limited''. It is an online business directory. It ia an interactive online business directory with a mission to integrate Africa businesses. It covers the entire Africa continent and operates on a do-it-yourself basis.
* '''Albania''': In [[Albania]], the directory is called Flete te Verdha - Albanian Yellow Pages which is a registered trademark belonging to Maxidisk SH.P.K Group and Fleteteverdha sh.p.k from Tirana.
* '''Algeria''': In [[Algeria]], the Yellow Pages business directory is published in French as '''Les Pages Jaunes'''. It is also available online in English and in French.
* '''Anguilla''': In [[Anguilla]], the directory is published by [[Global Directories Limited]] and titled ''Anguilla Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.anguillayp.com.
* '''Armenia''': In [[Armenia]], "Spyur" Information Center introduces "Armenia Yellow Pages". Directory is printed in English, [[Armenian language|Armenian]] and [[Russian language|Russian]](17500 copies a year). Other directory Armenian Business Pages was launched in 2015 by Comfy LLC and represents only digital version of yellow pages of Armenia. The directory is in the process of Electronic Armenia® trademark registration.<ref>http://www.pages.am</ref>
* '''Aruba''': In [[Aruba]], the official telephone directory of Setar is published by [[Global Directories Limited]] and titled ''Aruba Yellow Pages''. 85.000 Print copies are distributed free to households and companies and is also available online
* '''Austria''': The "Yellow Pages" and [[Yellow Pages#Internet yellow pages|IYP]] services are provided by: HEROLD Business Data GmbH, a [[European Directories]] group company.
* '''Australia''': In Australia, the most comprehensive business directory is the Yellow Pages published by [[Telstra#Directories and advertising (Sensis)|Sensis]]. The directory is also available online, on mobile and via smartphone app.

==B==
* '''Bangladesh''': In [[Bangladesh]], the business directory is published by '''''Ad Yellowpages''''' '''''Pages''''' and titled '''''Ad Yellowpages Yellow Pages'''''. AdYP, a sister conern of Ad Yellowpages.com is an new concept brought forward by the founders of the site. The site provide a variety of information about local places and businesses in Bangladesh.<ref>https://localyaar.com</ref>
* '''Bahrain''': In [[Bahrain]], the business directory is published by '''''Primedia International BSC (c)''''' and titled '''''Bahrain Yellow Pages'''''. Primedia International signifies a fundamental move away from the traditional business directories to new print & online media.
* '''Barbados''': In [[Barbados]], the directory is published by [[Global Directories Limited]] and titled ''Barbados Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.barbadosyp.com and on mobile devices at yp2go.bb.
* '''Belarus''': In [[Belarus]], the directory is titled ''Business-Belarus'' ([[Russian language|Russian]]), it is also available online. There is an alternative directory, called ''Belarus XXI vek'' (Belarus 21st century), which is analogue to Yellow Pages; it is also available online.
* '''Belgium''': In Belgium, the directory is titled ''Pages d'Or'' (golden pages) (French) or ''Gouden Gids'' (golden guide) ([[Dutch (language)|Dutch]]), and is distributed free to each telephone subscriber, it is also available online.
* '''Bonaire''': In [[Bonaire]], the directory is published by [[Global Directories Limited]] and titled ''Bonaire Yellow Pages''. Print copies are distributed free to each telephone subscriber.
* '''Bolivia''': In [[Bolivia]], yellowpages exist online under the URL ''Yellow Pages.com.bo''.
* '''Bosnia''': In [[Bosnia and Herzegovina|Bosnia]], Yellow Pages exist online under YellowPages.ba.
* '''Brazil''': In [[Brazil]], the directory is titled ''Páginas Amarelas'' and is distributed free to each telephone subscriber. Available online by DYK Internet S/A.
* '''British Virgin Islands''': In the [[British Virgin Islands]], the directory is published by [[Global Directories Limited]] and titled ''British Virgin Islands Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.britishvirginislandsyp.com.

==C==
* '''Cambodia''': In [[Cambodia]], the official Yellow Pages directory is called [[Cambodia Yellow Pages]] and published under contract to local Ministry of Posts and Telecommunications by [[CAMYP Co., Ltd]].
* '''Canada''': In Canada, the company [[Yellow Pages Group]] owns the trademarks ''Yellow Pages'' and ''Pages Jaunes''. It produces and distributes directories in both English and French. Yellow Pages Group is the market leader in print and online commercial directories and one of the largest media companies in Canada, producing the official directories of [[Bell Canada]], [[Telus]], [[Aliant]], [[Manitoba Telecom Services|MTS]], and others. [[Saskatchewan]]'s [[SaskTel]], through subsidiary [[DirectWest]], is believed to be the last major [[incumbent local exchange carrier]] to publish its own directories. Competitive local directory publishers, such as PhoneGuide or DirectWest's operations in Manitoba and Alberta, usually include commercial directories on yellow paper.
* '''Cayman Islands''': In the [[Cayman Islands]], the directory is published by [[Global Directories Limited]] and titled ''Cayman Islands Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online and on mobile devices.
* '''Chile''':
* '''China''': In China, the modern yellow pages industry was started in the late 1990s with the formation of two international joint ventures between US yellow pages publishers and China’s telecom operators, namely: a joint venture started in Shenzhen between [[RHDonnelley]] and [[China Unicom]] (later including Hong Kong’s PCCW and InfoSpace); and a joint venture between [[China Telecom Shanghai]] and what later came to be known as the yellow pages operations of [[Verizon]] {{Citation needed|date=May 2010}}. Later, another mainly state-owned telecom operator, [[China Netcom]] began to produce, either directly or on a sub-contracted basis, yellow pages in selected cities around the country. By early 2005, there were a number of independent local and international yellow pages operators in numerous cities including [[Yilong Huangbaoshu]], based in Hangzhou, Zhejiang Province with operations in Hangzhou and Ningbo {{Citation needed|date=May 2010}}. However, there is no nationwide Yellow pages in any format and only some international-trade related businesses including INBIT (USA), CHINAPAGES.COM and ALIBABA.COM (Chinese) are running some kind of national online databases based on business lists not from telephone companies. [[China Yellow Pages]] is also a common-place for finding manufacturers and exporters from China.
* '''Colombia''': In [[Colombia]], the standard yellow and [[White Pages]] are published and distributed every year free of charge by [[Publicar]], a Colombian subsidiary company of [[Carvajal Group|Carvajal]], which also publishes and distributes yellow and white pages in other Latin American countries.
* '''Croatia''': In [[Croatia]], the directory is called ''Žute stranice'' (yellow pages), published by [[MTI Telefonski imenik/Zute stranice]]. Another directory is ''CroPages Business Directory/Poslovni Adresar'', published by [[Masmedia]].
* '''Cuba''': In [[Cuba]], the equivalent online directory is titled [[Paginas Amarillas]], with information on the whole of Cuba.
* '''Cyprus''': In [[Cyprus]], the Yellow Pages is edited by ID Yellowpages Ltd [[Cyprus Yellow Pages Directory]].
* '''Cyprus (North)'''; In [[Turkish Republic of Northern Cyprus]] [[CYPYP North Cyprus Yellow Pages]]
* '''Czech Republic''': In the [[Czech Republic]] and [[Slovakia]], the directory is titled ''Zlaté stránky'' (golden pages), published by [[Mediatel]], Prague (a [[European Directories]] group company) and is distributed free to each telephone subscriber, usually in exchange for its previous version.

==D==
* '''Denmark''': In Denmark, a full online directory including most phone numbers is provided by ''De Gule Sider'' (a brand of Eniro, a Nordic search engine and directories company), with paper versions of yellow and white pages distributed to subscribers throughout the country; it was formerly a part of [[TDC Forlag]], a subsidiary of the national telecoms operator.
* '''Dominica''': In [[Dominica]], the directory is published by [[Global Directories Limited]] and titled ''Dominica Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.dominicayp.com.
* '''Dominican Republic''': In [[Dominican Republic]], published by [[Caribe Media]]. Publishing of printed and / or digital directories in the Dominican Republic.

==E==
* '''Egypt''': [[Egypt Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Egypt. Egypt Yellow Pages Ltd, founded in 1991, is the owner of the Yellow Pages trademark in Egypt.
* '''Europe''': For whole Europe, the [[European Yellow Pages]] apply. The European Yellow Pages is an effort of providing harmonized data to different language environments through keeping the character of having localized search capabilities on a regional level. Harmonizing data in this context means providing information to global users mainly in English and to local users in their native language.
* '''Europe''': For [[Europe]] the directory [[Yellobook.eu]] is providing information about many branches and companies all around 33 major European countries.

==F==
* '''Finland''': In [[Finland]], the directories are called ''Keltaiset sivut'', Eniro.fi and [[Teloos.fi]]
* '''France''': In France, Yellow Pages are referred to as ''Pages Jaunes''. They are distributed free by Pagesjaunes'''.fr''', a company affiliated with [[Orange S.A.|France Télécom]]. Pagesjaunes'''.com''', the .com version of ''Pages Jaunes'', was the issue of a major court case at [[World Intellectual Property Organization|WIPO]]; the original registrant, an individual from Los Angeles, won against France Télécom. This court decision defended by the Parisian Lawyer, Andre Bertrand, was path-setting for the whole European Yellow Pages industry, as it decided that the phrase "Yellow Pages" cannot be considered the property of a single company. Previously, many former state monopoly telecom companies outside the US had tried to ban competition by claiming the term "yellow pages", or the translation of "yellow pages" into the vernacular, as their exclusive trademark. [[Vivendi|Vivendi Universal]] moved to enter the French Yellow Pages market in 2001 with scoot.fr, but the attempt was a killed by a reorganisation of the struggling company. Another French editor of Yellow Pages is [[Bottin]]. More competition is expected in November 2005 from the liberalisation of "12", the former unique "[[4-1-1]]" number of [[Renseignements Telephoniques]], French for Directory Inquiry. In November 2006 [[Orange S.A.|France Télécom]] sold its majority share in pagesjaunes.fr to Mediannuaire. In August 2007 pagesjaunes'''.com''' finally became active, giving France two different ''Pages Jaunes''; thus creating agitation at pagesjaunes'''.fr''', which reshaped their site and started a massive advertisement campaign all over France.

==G==
* '''Georgia''': In Georgia, the directory is called ყვითელი ფურცლები and published by "Yellow Pages Tbilisi" Ltd.
* '''Germany''': In Germany, a directory titled ''Die Gelben Seiten'' is distributed free to each subscriber, by the [[Deutsche Telekom]], owner of [[T-Mobile]]. Other Yellow pages are edited by ''Go Yellow.de'', ''Klicktel.de'' and [[Gelbex.de]]. In 2006 a lawsuit with the [[Deutsches Patentamt]] denied the validity of the German Trademark "Gelbe Seiten" which in fact is the German translation of the universal expression "Yellow Pages". Klaus Harisch, an Internet Pioneer from Munich and founder of Go Yellow.de had spent over 7 Million Euros on Lawyer Fees to fight for the cancellation of the German "Gelbe Seiten" trademark. Deutsche Telekom had also registered "Yellow Pages" as a German trademark which they lost at the same time. On a European Level Deutsche Telekom had failed to register "Gelbe Seiten Deutschland" or "Yellow Pages Germany" as a Euro Trademark with [[OMPI]].
* '''Gibraltar''': A combined White and Yellow Pages directory, along with an [[Yellow Pages#Internet yellow pages|IYP]] service, are provided by: gibyellow.gi, a [[European Directories]] group company.
* '''Greece''': In Greece, Yellow Pages are called ''"Chrysos Odigos"'' that can be translated as "The Golden Guide".
* '''Grenada''': In [[Grenada]], the directory is published by [[Global Directories Limited]] and titled ''Grenada Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.grenadayp.com.
* '''Guyana''': In [[Guyana]], the directory service is provided by "[[GT&T]]" in printed format and in online services, there are quite a few, some of them are "[[YellowPagesGuyana]]", "[[YellowGuyana]]" and "[[GT&T]]'s" own online yellowpages directory--"[[yellowpages.com.gy]]".

==H==
* '''Hong Kong''': In [[Hong Kong]], the phone directory is titled ''Hong Kong Yellow Pages'', published by [[PCCW|PCCW Media Limited]].
* '''Hungary''': In Hungary, the directory is called ''Arany Oldalak'' (gold pages); are published and distributed by [[MTT Magyar Telefonkönyvkiadó]] Kft, Budaörs.

==I==
* '''India''': [[India]] is a very large country in terms of population, business activities and economy. There are multiple Yellow Pages being published by private sector companies. Some of them focus the whole nation and some are regional.
* '''Indonesia''': In [[Indonesia]], the telecommunication company [[Telkom (Indonesia)|Telkom]] with [[PT. Infomedia Nusantara]] (one of its subsidiaries), regularly publishes phone books. The company provides directory, call centre, and content services since 1984. The phone books consist of white pages and yellow pages, which are published in hard and soft copies.
* '''Iran''': In the Islamic Republic of [[Iran]], the directory is called ''The first book'' or in [[Persian language|Persian]] ''Keta:b e Avval''. This directory divides into different sections such as Directory of Businesses, jobs and maps and city guides. There is an official YellowPages in Iran owned and published by Iranian Yellow Page company. It has been developed in Persian and English languages, and contains different categories and locations of Iran. There is also an unofficial company that runs ''The Iran Yellow Pages''. This directory is published by Moballeghan Publishing and Advertising Company (1986) with the cooperation of The Trade Promotion Organization of Iran. By 2010 a new updated comprehensive directory called ''"The First Portal"'', or ''"First Eurasia E-commerce"'' or in [[Persian language|Persian]] ''"تجارت الكترونيك اول"'' comes to the [[Iran]] high potential markets.
* '''Iraq''': In Iraq, the directory is called ''Yellow Pages'' or in Arabic (Al Safahat al Safraa). This directory divides into different sections such as directory of businesses, jobs and maps and city guides and contains thousands of businesses in many categories. The directory is published by Alam Al-Rooya Publishing and Advertising Company.
* '''Ireland''': In the [[Republic of Ireland]], the directory is called ''Golden Pages'' and is published by FCR Media. Ireland's free Yellow pages is called BusinessPages.i.e.
* '''Israel''': In Israel, the yellow pages Hebrew edition is called ''Dapei Zahav'' (Golden Pages) and the English edition is ''Golden Pages''. The print directories come out in separate issues based on Israel's different telephone area codes, published by Golden Pages Publications Ltd. Five million copies of the yellow pages are distributed annually.
* '''Italy''': in Italy, the directory is titled ''Pagine Gialle'' (Yellow Pages). The printed versions come out in separate issues for [[province]] as [[White pages]]. Some years ago, an alternative directory, called ''Pagine utili'' (Useful Page) was proposed.

==J==
* '''Jamaica''': In [[Jamaica]], the directory is published by [[Global Directories Limited]] and titled ''Jamaica Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.jamaicayp.com and to mobile subscribers at yp2go.com.jm.
* '''Japan''': In Japan, the Yellow Pages directory, are known as [[:ja:タウンページ|Town Page]], and published by [[Nippon Telegraph and Telephone|NTT]].
* '''Jordan''': In [[Jordan]], the directory is titled '' Yellow Pages - Jordan'', Yellow Pages Jordan is operated since 2001 by PAGESJAUNES LIBAN, a subsidiary of the European PagesJaunes Group - France in 1997.

==K==
* '''Kazakhstan:''' In [[Kazakhstan]], the directory is ''Yellow Pages of Kazakhstan'', published by [[Yellow Pages Kazakhstan]] Management Group.
* '''South Korea:''' In [[South Korea]], the directory is published and distributed by many publishers:
** ''BiG Yellow Pages. Korean National Directory'', by [[Yellow Pages Korea]];
** ''Korea Yellow Pages'', by [[Korea Yellow Pages]];
** ''Korea English Yellow Pages'', by [[Korea Telecom Directory]].
* '''Kosovo:''' In [[Kosovo]], [[Faqe te Verdha]] is a trademark belonging to [[KOSOFT]], [[Pristina]].
* '''Kyrgyzstan:''' In [[Kyrgyzstan]], Yellowpages can be found under the URL "yellowpages.kg".

==L==
* '''Lebanon:''' In [[Lebanon]], the Yellow Pages business directory is published in [[Arabic language|Arabic]] and French by PAGESJAUNES LIBAN.

==M==
* '''Macau:''' In [[Macau]], the phone directory is titled ''Macau Yellow Pages/Páginas Amarelas'', publ. by Directel [[Macau Listas Telefonicas]] Lda.
* '''Madagascar:''' In [[Madagascar]] yellow pages can be found via the site Madagascar Yellow Pages.
* '''Malaysia:''' In [[Malaysia]], there are 4 large directories Malaysia Yellow Pages, Malaysia Super Pages, Malaysia Business Directory and BCZ.com
* '''Maldives:''' In [[Maldives]], the commercial phone directory is called Yell.
* '''Mali:''' In [[Mali]], the equivalent online directory is titled [[Malipages.com]].
* '''Malta:''' In [[Malta]], the Yellow Pages Directory is published by [[Ark Publishing Group]]. It has been publishing the Yellow Pages since 1997 and each year distributes 200,000 directories free of charge to the general public.
* '''Mauritius:''' In [[Mauritius]], the Yellow Pages Directory is published by [[Teleservices Ltd]] and is known as MT yellow pages 
* '''Mexico''': In Mexico, there are several commercial phone directories. The incumbent is called Seccion Amarilla.com.mx (Yellow Section) is published by Anuncios en Directorios, S.A. de C.V., a subsidiary of Telmex, the local Telco. Others are Paginas Amarillas.com.mx (Yellow Pages) published by Phonebook of the World, Paginas Amarillas.com published by Publicar, Mexico Data Online.com published by the Mexico Business Directory and Paginas Utiles.com.mx published by Ideas Intercativas, S.A.
* '''Moldova:''' In [[Moldova]] yellow pages can be found via the site [[www.yp.md]].
* '''Mongolia:''' In [[Mongolia]], the directory is called ''Mongolia Yellow Pages'' (yellow pages) and can be found via [[www.yp.mn]].
* '''Montserrat''': In [[Montserrat]], the directory is published by [[Global Directories Limited]] and titled ''Montserrat Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online.
* '''Morocco:''' In [[Morocco]], the directory is called ''Pages Jaunes'' (yellow pages).
* '''Myanmar:''' In [[Myanmar]], the directory is called ''Yangon Directory'' (ရန်ကုန်လမ်းညွန်).

==N==
* '''Netherlands:''' In [[Netherlands]], the directory is called ''Gouden Gids'' (literally "Golden Guide"), and within the district concerned, it is distributed free to each telephone subscriber, by De Telefoongids BV (a [[European Directories]] group company).
* '''New Zealand:''' In New Zealand, the yellow.co.nz directory is printed in 18 regional editions by Yellow Pages Group (YPG). YPG also publishes 18 regional editions of '[[White Pages]]' (combined government, residential and business listings), and a 'Local Directory' for some urban areas and sub-regions.
* '''Nigeria:''' In [[Nigeria]], Yellow Pages companies are privately owned. [[NigerianYellowPages.com]] is the official trademark owner of the walking finger logo with six (6) edition of its yellow pages in different formats. YellowPages.net.ng claimed to be the first Yellow Pages Directory in the world to emulate social media network concept.  
** '''Nigerian Yellow Pages''': Content of [[Nigerian Yellow Pages]] or commonly known as NigerianYellowPages.com is available in six formats (editions): ''CD-ROM directory''; ''MS Windows Desktop directory''; ''Internet directory''; ''Mobile Phone Internet directory''  ''Mobile Phone SMS text directory'' and ''Print directory''. There is also a mobile edition, which is accessible on mobile phones and other mobile devices such as PDA. There is a dedicated classifieds section in their yellow pages for jobs, properties, homes, rentals and announcements. It has its own toolbar, the [[Nigerian Yellow Pages Toolbar]].
** '''Africaonline business directory''': The other Yellowpage business directory in Nigeria is the Africaonline business directory, [[yellopages.com]]. This is an interactive online business directory that enables businesses to upload their profiles and place their adverts. Yellopages.com includes Nigerian content and serves to integrate Nigerian businesses.
* '''Norway:''' In Norway, the directory is called ''Gule Sider'' (Yellow Pages). The two second largest directories are [[Opplysningen 1881]] and [[Nettkatalogen]]. [[Gul.no]], [[180.no]], [[Avanti Media AS]], [[Bedriftssøket AS]], [[Gul Index]] and [[Finnfirma.no]] are som of the other directories in growth. The searchengine Sesam.no provides a business directory branded [[Sesam Katalog]].

==P==
* '''Pakistan:'''
** '''Jamal's Yellow Pages of Pakistan:''' is a B2B [[Trade Directory]] published by US Publishers (Pvt) Ltd. since 1983. The directory is published in printed form (3 volumes per set), CDROM version and online.
** '''Time's Trade Directory of Pakistan:''' Time Publishers (Pvt) Ltd. published "Time's Trade Directory of Pakistan - National Yellow Pages" since 2002. B2B Version also launch similarly as Time's e-Directory. The online version also provide comprehensive information about Pakistan Businesses to the web user worldwide.
** '''PhoneBook.com.pk:''' The [[Pakistan Telecommunication Company]] maintains a yellow pages and white pages directory. JS Enterprises Private Limited is publishing this directory, which is also associated with [[Jang Group of Newspapers]] and the GEO Television Network.
** '''Ebizpk.com: '''Online Green & Yellow Pages of Pakistan. Launched in January 2010. Initially Listing 10 companies of each sector.
** '''Dmoz Pakistan:''' Database of [[Pakistani]] companies, [[Government]] departments and business organizations in categorized format.
** '''[http://ypages.pk Ypages.pk]:'''  Launched in June 2012, Online Yellow Pages of [[Pakistan]] provides all local business contact details. Ypages associated with ALM Advertising Agency.
** '''[yellowpagespk.com]: '''Marshall Online Yellow Pages  in Islamabad Pakistan & Online Business Directory in Islamabad Rawalpindi Lahore Karachi Pakistan.
* '''Palestine:''' [[Palestine Yellow Pages Ltd]] is the official publisher of Yellow Pages branded products in Palestine. [[Palestine Yellow Pages]] is the exclusive owner of the Yellow Pages, Walking Fingers & Design, and YellowPages.com.ps trademarks in Palestine. [[Palestine Yellow Pages]] is part of the [[Al Wahda-Express Group of Companies]]. Founded in 1986, [[Al Wahda-Express Group of Companies]] employs nearly 1000 employees publishing print, online and mobile Yellow Pages directories throughout 5 countries including Palestine.
* '''Panama:''' In [[Panama]], [[Yellow Pages Panama]]
* '''Peru:''' In United States, [[Peruvian Yellow Pages]], since 1993, the printed edition, is the first and oldest publication for Peruvians living in the USA. Now with the online version covering coast to coast the American territory. The online version of the Peruvian yellow pages is available at [[Peruvian Yellow Pages]].
* '''Philippines:''' In [[Philippines]], Directories Philippines Corporation (DPC), regularly publishes phone books of more than a dozen telecom companies in the country.
* '''Poland:''' In [[Poland]], it is called ''żółte strony'' and is distributed by [[Polskie Książki Telefoniczne]] (a [[European Directories]] group company) as a part of their phone books. The second largest directory, published by [[Eniro]], is called "Panorama Firm" (panorama of companies). [[YellowPages.pl]]. It is the biggest online directory in Poland. Polish Yellow Pages has existed on the market since 1998. Yellow Pages enables them to search companies and products and services, it is a business platform, which helps to promote a company and to establish trade relations. In April 2007 started [[Zumi.pl]] - first local search web which connects maps and information about companies in Poland. Several historical directories from Poland are available online as scans, and can be searched via [[kalter.org]].
* '''Portugal:''' In [[Portugal]], the ''Páginas Amarelas'' are controlled by [[Portugal Telecom]] and the website is [[pai.pt]]. The printed version is distributed for free to all land line users. There is also available a residential listing called Páginas Brancas.

==Q==
* '''Qatar''': In [[Qatar]], the official Yellow Pages directory is called Qatar Yellow Pages and published by ''''Primedia Qatar W.L.L'.''' The Qatar yellowpages features comprehensive business listings for industrial and commercial establishments across the region markets. This Directory is one of the most economical media for business to showcase their products and services. The user has a choice to reference print or source online or mobile wap.

==R==
* '''Romania:''' In [[Romania]], the directory is called 'Pagini Aurii' (Golden Pages) [[paginiaurii.ro]].
* '''Russia:''' In Russia, KONTAKT EAST HOLDINGS (KEH.ST) established in 2006, is a Swedish holding company that owns Russian Company OOO ''Желтые страницы'' ("JOLTI STRANITSI") (Russian translation of Yellow Pages). OOO "JOLTI STRANITSI" is the result of the successful merger in 2007 of YPI YELLOW PAGES, established in 1993, a leading publisher of Yellow Pages directories in the St. Petersburg and Perm markets and Eniro RUS-M, a publisher of leading Yellow Pages directories in Moscow, Samara and 7 other Russian cities in the Urals and Volga region.

Other directories in Russia include:

:*''Адрес Mосква'' (Moscow address), by ZAO [[Verlag Euro Address]];
:*''Большая Телефонная Книга Москвы'' (Big Phone Book of Moscow), by [[Extra M Media]];
:*''Вся Деловая Москва'' (all business Moscow), by [[Biznes-Karta Business Information Agency]];
:*''Московский Бизнес - Moscow Business Telephone Guide'' by [[Московский Бизнес - Moscow Business Telephone Guide]].

==S==
* '''Saint Kitts and Nevis''': In [[St Kitts & Nevis]], the directory is published by [[Global Directories Limited]] and titled ''St Kitts and Nevis Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.stkittsandnevisyp.com.
* '''Saint Lucia''': In [[Saint Lucia]], the directory is published by [[Global Directories Limited]] and titled ''St Lucia Yellow Pages''. Print copies are distributed free to each telephone subscriber.
* '''Saint Vincent''': In [[Saint Vincent and the Grenadines]], the directory is published by [[Global Directories Limited]] and titled ''St Vincent Yellow Pages''. Print copies are distributed free to each telephone subscriber and is also available online at www.stvincentyp.com.
* '''Saudi Arabia''': In [[Saudi Arabia]], the directory is Saudianyellowpages.com' 'Saudiarabyellowpages.com'',. Established in 2001, is the LARGEST yellow pages of Saudi Arabia. Yellow Pages Saudi Arabia.
* '''Saudi Arabia''': Daleeli.com is an online business directory in [[Saudi Arabia]] to locate addresses, Phone numbers, maps, websites & locations of Business Places and offices in Saudi Arabia.
* '''Serbia:''' In [[Serbia]], the directory is called '''Zute Strane - Yellow Pages''' (Serbian Business Directory) which is a registered trademark belonging to Yellow Pages Co. from Belgrade.
* '''Sierra Leone:''' In [[Sierra Leone]], the online yellow pages directory, [[LeoneDirect]] powered by [[Denza, LLC.]] provides contact information for local companies.
* '''Singapore:''' In [[Singapore]], it is known as "Yellow Pages" and is registered as a [[Public company]] under the name [[Yellow Pages Singapore|Yellow Pages (Singapore) Limited (Reg. no.:200304719G)]]. It is [[Public company|listed]] on the Singapore [[Singapore Exchange|SGX]] mainboard on 9 Dec 2004. It includes the Singapore Phone Book, the Chinese Yellow Pages and the Yellow Pages Buying and Commercial/Industrial Guides and advertisement sales. Yellow Pages Singapore also publishes and distributes niche directories and guides.
* '''Slovakia:''' In [[Slovakia]], it is called "Zlaté stránky" (which means Golden Pages), published by [[Mediatel]] (a [[European Directories]] group company), Bratislava and is distributed free to each telephone subscriber, usually in exchange for its previous version. The online version is available at [[zlatestranky.sk]].
* '''Slovenia:''' In [[Slovenia]], the directory is called ''[[Rumene strani]]'' (Yellow Pages) which is a registered trademark belonging to Inter Marketing.
* '''South Africa:''' In [[South Africa]], the directory is called 'the Yellow Pages' which is distributed by Trudon [[yellowpages.co.za]], a subsidiary of World Directories which also publishes books in Belgium, Ireland, the Netherlands, Portugal and Romania. There are 19 regional editions covering the nine provinces. Each of the four metropolitan areas has a separate white and yellow pages book. The remaining 15 areas have both sections in one book. They also have a mobile version [[pocketbook.co.za]]
* '''Spain:''' In Spain, it is called ''Páginas Amarillas'', it was distributed by [[Telefónica|Telefónica Publicidad e Información]] S.A. Yellowpages - now known as Yell Publicidad - can also be found via the Internet Address [[www.paginasamarillas.es]]. Since July 2006 the company is owned by Yell Group from the UK. A competitor is [[www.qdq.com]], a directory edited by Pages Jaunes Group from France. Another competitor is [[citiservi]], a different yellow pages service where Professionals search for Customers requesting services. Also there is an English-speaking expat directory of businesses along the south east of Spain called [[www.littleyellowpages.com]]. This site is aimed mainly at English speaking expatriates living in Spain.
* '''Sri Lanka:''' In [[Sri Lanka]], the official 'Yellow Pages' publisher is produced by [[Sri Lanka Telecom]]. However, competing publishers also use the term 'Rainbow Pages' though not the walking fingers logo.
* '''Sweden:''' In Sweden, it is called ''Gula Sidorna'', distributed by [[Eniro]]. yellowpages.se is a portal to different Yellowpages from Sweden. Gulex.se is an alternative Swedish directory, distributed by the Norwegian company Advista. Lokaldelen i Sverige AB (a [[European Directories]] group company) provide over 250 local directories in Sweden. Also hitta.se, an Online business directory by Norwegian company [[Schibsted]].
* '''Switzerland:''' In Switzerland the brand local.ch produces and distributes directories in several forms (printed, online and on mobile) including yellow and white pages - online available in [[German language|German]], French, [[Italian language|Italian]] and English.
* '''Syria:''' In [[Syria]], the directory is called الصفحات الصفراء (Yellow Pages).

==T==
* '''Thailand:''' In [[Thailand]] it is called ''Samood Nar Leung'' and also called ''Thailand YellowPages''. The company [[Teleinfo Media Public Company Limited]] produce and distribute Yellow pages nationwide. Thailand YellowPages is generated in several forms e.g. paper, Call Center no. 1188. Thailand YellowPages is produced both in Thai and English.
* '''Tunisia:''' In [[Tunisia]], it is called "الصفحات الصفراء" (Pages Jaunes) and it is owned by "Les Editions Techniques Spécialisées", a Tunisian private company. The online version, available at [[pagesjaunes.com.tn]] for free was one of the first online directories in Arabic.
* '''Turkey:''' Yellow Medya Istanbul based Yellow Medya.
* '''Turkish Republic of Northern Cyprus''' In [Turkish Republic of Northern Cyprus]. Known as CYPYP it is found at [[cypyp.com]]
* '''Turks and Caicos Islands''': In the [[Turks and Caicos Islands]] there are two telephone directories. One is published by Olympia Publishing Company, a Turks & Caicos Islands company, and carries listings from the two major telecommunications companies on the Island and the other is published by a subsidiary of Global Directories Limited, a Caymanian-based company, which carries the listings from one of the two major telecommunications companies on the Islands. Both publications are titled the ''Turks and Caicos Islands Yellow Pages'' and refer to themselves as "Local" but the Olympia Publishing Company directory is the larger and more definitive and it is the only local directory publisher.

==U==
* '''Ukraine''': In [[Ukraine]], the free business directory is titled ''PromUA'' ([[Russian language|Russian]]), it is available online at [[prom.ua]]. Other directories are: ukrindustrial.com, ukrbiznes.com, [[ukrpartner.com]].
* '''United Arab Emirates:''' Dubai-based Local Search UAE is an Online Business Directory UAE where all businesses across Abu Dhabi, Al Ain, Dubai, Fujairah, Sharjah, Ras Al Khaimah & Umm Al Quwain are listed and can be searched.
* '''United Arab Emirates:''' Dubai-based [[Express Print (Publishers) L.L.C.]] is the official publisher of '''Etisalat Yellow Pages''' branded products in the UAE. Express Print (Publishers) L.L.C. publishes the Yellow Pages in both print and electronic formats. Etisalat Yellow Pages print edition consists of 3 regional directories for the areas of Abu Dhabi, Dubai and the Northern Emirates. Directories are published annually and distributed towards the end of the first quarter of each year. Express Print (Publishers) L.L.C. also publishes the Etisalat Yellow Pages on 2 electronic platforms -Online & Mobile.
* '''United Arab Emirates:''' As of late 2016, Dubai-based [[ZOSER MEA]] is the official publisher of [[du Yellow Pages]] branded products in the UAE. ZOSER MEA publishes the Yellow Pages in both print and electronic formats. Directories are published annually and distributed in the month of January each year.
* '''United Arab Emirates:''' In [[United Arab Emirates]], the directory is titled ''Yellow Page Gulf UAE'',. Established in January 2011, is the LARGEST yellow pages of GULF. Yellow Pages Gulf.
* '''United Kingdom:''' The first Yellow Pages directory in the UK was produced by the [[Kingston upon Hull|Hull]] Corporation's telephone department (now [[Kingston Communications]]) in 1954. This was distributed with the alphabetical phone directory rather than as a stand-alone publication. The company now produces [[Hull Colour Pages|The Hull Colour Pages]].

:With the encouragement of [[The Thomson Corporation]], at the time an advertising sales agent for the nationalised [[General Post Office (United Kingdom)|General Post Office]]'s [[telephone directory]], a business telephone number directory named the Yellow Pages was first produced in 1966 by the GPO for the [[Brighton]] area, and was rolled out nationwide in 1973. The Thomson Corporation formed Thomson Yellow Pages in 1966 to publish and to distribute the directory to telephone subscribers for the GPO, and later for [[Royal Mail|The Post Office]].

:Thomson Yellow Pages was sold by The Thomson Corporation in 1980, at the same time as Post Office Telecommunications became the (then) state-owned [[British Telecom]] (BT). The Yellow Pages directory continued to be distributed to all telephone subscribers by BT. At the same time, The Thomson Corporation formed Thomson Directories Ltd, and began to publish the [[Thomson Local]] directory, which would remain the Yellow Pages' main, and often sole, competitor in the UK for more than the next two decades, and would be the competitive driving force behind such changes to Yellow Pages as the adoption (in 1999) of colour printing and "white knock out" listings.

:In 1984 the year that BT was privatised, the department producing the directory became a stand alone subsidiary of BT, named Yellow Pages. In the mid-1990s the Yellow Pages business was re-branded as [[Yell Group|Yell]], although the directory itself continued to be known as the Yellow Pages.

:Yell was bought by venture capitalists in 2001, and in 2003 was floated on the Stock Exchange. After the one year "no competition" clause expired BT too went into competition with the Yellow Pages, re-entering the market by adding similar content to their existing directory, "The Phone Book", adding a classified section to the traditional alphabetical domestic and business listings.

:Yellow Pages, [[Thomson Local]] and BT's [[The Phone Book]] display advertising and can be booked directly with advertising sales representatives.

:Nowadays the KC Yellow Pages is referred to as [[Hull Colour Pages]], and is separate from the White Pages. Yell now also publishes an East Yorkshire edition of Yellow Pages in competition.

[[Image:Bsyps.png|right|Bell System Yellow Pages Logo]]
* '''United States:''' In the past, AT&T, Verizon, and Qwest, the three largest phone companies in the U.S., dominated the U.S. yellow pages industry; however, the term "yellow pages" and the ''Walking Fingers'' logo was heavily marketed by AT&T pre [[Bell System Divestiture|divestiture]]. However, AT&T never filed a trademark registration application for the current and most recognized version of the ''Walking Fingers'' logo, so it is in the public domain. AT&T allowed the "independent yellow pages" industry to use the logo freely.<ref>[http://www.ll.georgetown.edu/federal/judicial/fed/opinions/9_opinions/91-1461.html Bellsouth v. Datanational]</ref> The "independents" are unrelated to the incumbent phone company and are either pure advertising operations with no phone infrastructure or telephone companies who provide local telephone service elsewhere. Such independents include operators who typically focus on industry or business segments, or local market directories.

:Yellow pages publishers or their agents sell the right to place advertisements within the same category, next to the basic listings.

:For example, [[AT&T]] is the dominant local telephone service provider in [[California]], but since Bell Atlantic and [[GTE]] merged to become [[Verizon]], it now provides service in many pockets such as [[West Los Angeles (region)|West Los Angeles]]. [[Los Angeles]] telephone users can select from telephone directories published by AT&T, Verizon (published by [[Idearc Media|Idearc]]), Yellow Book USA, PDC Pages (Phone Directories Company) [[PDC Pages.com]] and other independent publishing companies. [[R. H. Donnelley]] is also in local markets across country with Dex Printed Directories and [[DexKnows.com]]. In Northern California, Valley Yellow Pages [[MyYP.com]] is a large regional independent publisher. Additionally, in the smaller markets, many yellow pages publishers are beginning to offer directories catering to specific niche business or industry segments, such as automotive, manufacturing, environmental/green products, imports, exports, and the like. One such example is the [[Export Yellow Pages]] (a yellow page directory published in partnership with the US Department of Commerce that focuses on U.S. exporters) and vertical directories offered by Yellow Pages Nationwide, Inc. Media an Online Digital Yellow Pages company, Consolidation and M&A activity in the directory publishing market continues to remain very high in the U.S. and there is an increasing move toward internet based directories as internet usage for search increases and concerns over the possible negative environmental effects of the books becomes more evident.
: [[Yellowpages.com]] LLC is a subsidiary of AT&T.

* '''Uzbekistan:''' In [[Uzbekistan]], the directory is called ''Yellow Pages of Uzbekistan'', published by Yellow Pages Ltd.

==V==
* '''Vietnam:''' In [[Vietnam]], the official title "Telephone Directory & Yellow Pages'' in English and ''Nien Giam Dien Thoai Nhung Trang Vang va Nhung Trang Trang "in Vietnamese are produced and distributed nationwide by [http://yp.vn/ VietNam Yellow Pages Media JSC].

Vietnam Business Yellow Pages in Vietnamese and English is directory of Vietnam Business.

==Notes and references==
{{reflist}}

==See also==
* [[Telecommunications service]]
* [[Yellow pages]]

[[Category:Directories|*]]
[[Category:Yellow pages|*]]
<=====doc_Id=====>:492
<=====title=====>:
Category:Yellow pages
<=====text=====>:
{{Commons category|Yellow pages}}
{{Cat main|Yellow pages}}

[[Category:Directories]]
[[Category:Advertising by medium]]
<=====doc_Id=====>:495
<=====title=====>:
R.L. Polk & Company
<=====text=====>:
{{Advert|date=July 2009}}
{{Infobox company
|name = R.L. Polk & Company
|logo =
|type = Acquired by [[IHS Inc.]]
|foundation = [[Detroit, MI]] {1870}
|location_city =  [[Southfield, Michigan|Southfield]], MI
|area_served = Worldwide
|founder = [[Ralph Lane Polk]]
|key_people = Stephen R. Polk
* Chairman, President and CEO
Tim Rogers
* President, Polk
Richard Raines
* President, CARFAX
Michelle Goff
* Senior Vice President/Chief Financial Officer
|homepage = [http://www.polk.com www.polk.com]
|industry = Automotive
}}

'''R. L. Polk & Company''' is a provider of [[automotive]] information and marketing [[solution]]s to the automotive industry, insurance companies, and related businesses.<ref name="usa.polk.com">[http://usa.polk.com/Company/WhoWeAre/ R. L. Polk & Company :: Our Company :: Who We Are :: Index] Retrieved on 10/31/08  {{webarchive |url=https://web.archive.org/web/20080702225115/http://usa.polk.com/Company/WhoWeAre/ |date=July 2, 2008 }}</ref>

Polk was acquired by [[IHS Inc.]] on July 15, 2013 <ref>http://press.ihs.com/press-release/corporate-financial/ihs-completes-acquisition-rl-polk-co</ref> and is based in Southfield, Michigan with operations in several countries, including the United States, Canada, Germany, United Kingdom, France, Japan, China and Australia.<ref name="usa.polk.com"/>

==Company history==
[[Image:1880 spine Illinois State Gazetteer by Polk & Co.png|thumb|100px|left|Polk's ''Illinois State Gazetteer'', 1880]]
R. L. Polk & Company was founded by [[Ralph Lane Polk]] in 1870 in Detroit, MI as a publisher of business directories. In 1872, the company first published a City Directory, for Evansville, Indiana, plus a listing of post offices in nine states. Additional directories followed in the ensuing years as the business grew.<ref name=heritage>[http://usa.polk.com/Company/Heritage/ R.L. Polk : Heritage]  {{webarchive |url=https://web.archive.org/web/20091229120305/http://usa.polk.com/Company/Heritage/ |date=December 29, 2009 }}</ref> claiming 1000 directories by 1960.<ref>{{cite book|title=Polk's Abilene (Taylor County, Texas) City Directory, 1960|date=1960|publisher=R. L. Polk & Co|page=7|url=http://texashistory.unt.edu/ark:/67531/metapth160223/m1/7/|accessdate=27 September 2014}}</ref>  Affiliates included the Polk-Husted Directory Co. of Oakland, California.<ref>{{cite book |url=https://books.google.com/books?id=TNlKAQAAIAAJ&pg=PA547 |title=Polk's San Jose City and Santa Clara County Directory |year=1907 }}</ref> In addition to city directories, the company published bank directories.

In 1907, R.L. Polk & Co. was publishing a "[[Gazetteer]]" Business directory for the State of Michigan and Windsor and Walkerville Ontario, as well as gazetteers for Alaska, Arkansas, California, Idaho, Illinois, Oklahoma, Indiana, Iowa, Kansas, Kentucky, Maryland, Minnesota, North Dakota, South Dakota, Montana, Missouri, Nevada, Oregon, Washington State, Pennsylvania, Tennessee, Texas, Utah, West Virginia, and Wisconsin.<ref>{{cite book|title=Michigan State Gazetteer and Business Directory|date=1907|publisher=R.L. Polk & Co.|location=Detroit|page=2|edition=1907-1908|url=https://books.google.com/books?id=absfAQAAMAAJ&lpg=PA250&dq=%22manitou%22%20steamship%20charlevoix&pg=PA81#v=onepage&q=%22manitou%22%20steamship%20charlevoix&f=false|accessdate=7 June 2016}}</ref>

In 1921, a conversation between Ralph Lane Polk II and [[Alfred P. Sloan]] (who later became president of General Motors) helped fuel R. L. Polk & Company's entry into the automotive industry. During the conversation, Sloan asked Polk to impartially tabulate and publish statistical information on cars and trucks in operation. R.L. Polk & Company launched its motor vehicle statistical operations in 1922, when the first car registration reports were published.<ref>http://web.archive.org/web/20071116145915/http://www.salesforce.com/customers/business-services/case-studies/rlpolk.jsp Retrieved on 11/4/08</ref> In 1922, R.L. Polk & Co. published its first Passenger Car Registration Report, covering 58 makes and accounting for 9.2 million passenger automobiles on America's highways.

From 1951 to 1958, the company pioneered the use of electronic punch card tabulating equipment. In 1956, Polk's reporting services included monthly statistics on boats, business aircraft, motorcycles, commercial trailers, and recreational vehicles. In 1976, the National Vehicle Population Profile (NVPP) was introduced.

===1990s===
In 1993, Polk combined their Canadian activities with Blackburn Marketing Services to form Blackburn / Polk Marketing Services Inc. (BPMSI).  Polk also acquired a 35% interest in CARFAX from Blackburn Marketing Services.  In 1995, Polk entered an alliance with Marketing Systems GmBH and acquired a substantial minority interest in The Ultimate Perspective (T.U.P).

In 1996, Polk completed acquisition of the Blackburn / Polk operations and renamed it Polk Canada Marketing Services Inc. (PCMSI).  This acquisition unified and strengthened their North American operations in Polk's strategy to be a global information services provider.  They also announced their first Automotive Loyalty Award winners.

In 1997, Polk acquired the MSS division of Automatic Data Processing's European Operations.

In 1999, Polk completed acquisition of CARFAX and sold Advertising Unlimited, Inc. to Norwood Promotional Products.

===2000 and Beyond===
In 2000, Polk sells its Consumer Information Solutions (CIS) business units Direct Marketing, Data Information Services / Polk Verity, City Directory, and the Compusearch and Prospects Unlimited units of Polk Canada to Equifax.

Polk launches Garage Predictors and Polk Canada, Inc. announces Polk Canada Net. Polk also completes its acquisition of Marketing Systems Group.

Ralph Lane Polk II is inducted into the prestigious Automotive Hall of Fame (AHF) located in Dearborn, Michigan in 2001. Stephen R. Polk is also a part of the AHF as a director<ref>http://ias.net/ahof/v1n3/ Retrieved on 11/5/08</ref> and R. L. Polk & Co. is also considered a Sapphire Level Supporter.<ref>http://ias.net/ahof/v1n3/ Retrieved 11/5/08</ref>

In 2002, Polk launches the Polk Vehicle Lifecycle System and the Polk Cross Sell is introduced.

Also in 2002, Ralph Lane Polk II is inducted into The Direct Marketing Association (DMA) Hall of Fame, the highest professional honor in direct and interactive marketing. DMA inducts into "The Hall of Fame" as many as four individuals each year for the significant impact these leaders have had on the growth of the direct and interactive process.<ref>http://www.the-dma.org/awards/halloffame.shtml Retrieved on 11/4/08</ref>

In 2003, PolkInsight is launched. Polk Total Market Predictor (Polk TMP) is also introduced.

In 2004, R. L. Polk & Company launches Polk Cross Sell Report and RLPTechnologies, a new wholly owned subsidiary, is established. Also, The [[Software Engineering Institute|Software Engineering Institute (SEI)]] awards R. L. Polk & Company with a Level II Capability Maturity Model Integrated (CMMI) rating.

In 2005, R. L. Polk & Company introduces the Polk Inventory Efficiency Award. The Polk Inventory Efficiency Award recognizes and rewards outstanding aftermarket companies for process improvements relative to inventory efficiency.<ref>http://www.reuters.com/article/pressRelease/idUS147801+21-May-2008+PRN20080521 Retrieved 11/14/08</ref>

In 2007, R. L. Polk & Co. acquire a majority interest in ROADTODATA, a rapidly growing supplier of automotive price and specifications data.<ref>http://japan.polk.com/News/LatestNews/R.+L.+Polk+and+ROADTODATA+Merge.htm Retrieved 12/26/08</ref>

In 2010, R. L. Polk & Company partners with Citytwist.<ref>https://www.ihs.com/Customer/citytwist-auto-excellence-award.html</ref>

In 2013, IHS, Inc announced a $1.4B purchase of R.L. Polk.<ref>http://www.mlive.com/auto/index.ssf/2013/06/information_company_ihs_to_pur.html</ref>

The company's business-to-business marketing services include PolkInsight, the National Vehicle Population Profile (NVPP), Blackburn / Polk Marketing Services Inc. (BPMSI), Polk Dealer Marketing Manager,<ref>http://google.com/search?q=cache:2uEMeAtCgckJ:findarticles.com/p/articles/mi_hb6674/is_/ai_n26650183+polk+and+Marketing+Systems+GmBH&hl=en&ct=clnk&cd=7&gl=us Retrieved on 11/4/08</ref> The Ultimate Perspective (T.U.P), Polk Canada Net, Polk Vehicle Lifecycle System, Polk CrossSell Reports,<ref>http://www.prnewswire.com/cgi-bin/stories.pl?ACCT=104&STORY=/www/story/01-27-2004/0002097303&EDATE= Retrieved on 11/4/08</ref> and Polk Total Market Predictor (Polk TMP).{{Citation needed|date=July 2009}}

==CARFAX==

The Polk Company announced on August 2, 1999 that it had completed acquisition of [[Carfax (company)|Carfax]]. Polk had previously owned 35 percent of Carfax, in partnership with the Blackburn Group, Inc., of London, Ontario, [[Canada]], and has now acquired the remaining 65 percent.<ref name="theautochannel.com">http://www.theautochannel.com/articles/press/date/19990802/press027618.html Retrieved 11/7/08</ref> Carfax compiles vehicle histories from various sources, with about 75 percent of the information coming from Polk data.  Using the [[vehicle identification number]] (VIN), each history provides potential buyers with all available facts about a used car being considered for purchase.  This may include original use of the vehicle odometer records, number of owners, and other items that might affect a purchase decision.<ref name="theautochannel.com"/>

==See also==
* [[St. Louis City Directories]]

==References==
{{reflist}}

==Further reading==
* {{cite book |url=https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 |title=Directory of Directories |publisher=R.L. Polk & Co. |location=NY |year=1916 }}

==External links==
{{commons category|R.L. Polk & Co.}}
* Internet Archive. [https://archive.org/search.php?query=creator%3A%22R.L.+Polk+%26+Co%22 Works published by R.L. Polk & Co.], various dates
* Hathi Trust. [http://catalog.hathitrust.org/Search/Home?checkspelling=true&lookfor=%22polk+%26+co%22&type=publisher&sethtftonly=true&submit=Find Works related to R.L. Polk & Co.], various dates
* OCLC WorldCat. [http://www.worldcat.org/search?q=au%3A%22polk+%26+co Works related to R.L. Polk & Co.], various dates

{{DEFAULTSORT:Polk and Co.}}
[[Category:Companies based in Detroit]]
[[Category:Directories|polk]]
[[Category:Publishing companies established in 1870]]
[[Category:American companies established in 1870]]
<=====doc_Id=====>:498
<=====title=====>:
Slater's Directory
<=====text=====>:
#REDIRECT[[Isaac Slater]]

[[Category:Directories]]
<=====doc_Id=====>:501
<=====title=====>:
Women Environmental Artists Directory
<=====text=====>:
The '''Women Environmental Artists Directory''' (WEAD) focuses on promoting environmental and [[Social justice]] art. <ref>{{cite web | url=http://weadartists.org/about-us | title=About Us | publisher=Women Environmental Artists Directory | accessdate=2013-08-12}}</ref> WEAD was founded in 1996 by Jo Hanson, Susan Leibovitz Steinman and Estelle Akamine.<ref>{{cite web | url=http://greenmuseum.org/generic_content.php?ct_id=285 | title=JO HANSON: Pioneering Environmental Artist Dies in San Francisco | publisher=Green Museum | accessdate=2013-08-12 | last=Leibovitz Steinman | first=Susan}}</ref> 

WEAD has been listed among the best projects relating to [[Environmental art]],<ref>{{cite web | url=http://www.andrew.cmu.edu/user/md2z/greenarts/artprojects.html | title=Green Arts Web: Artists & Projects | publisher=Carnegie Mellon University | accessdate=2013-08-12}}</ref> and has sponsored a number of exhibits about activist eco art.<ref>{{cite web|title=Earthly Concerns, Activist EcoArt curated by WEAD|url=http://www.usfca.edu/uploadedFiles/Destinations/Library/thacher/archive/Earthly%20Concerns.pdf|publisher=University of San Francisco|accessdate=2013-08-12}}</ref> <ref>{{cite web|title=CONVERGENCE/DIVERGENCE SYMPOSIUM|url=http://www.losmedanos.edu/art/archive.aspx|publisher=Los Medanos College|accessdate=2013-08-12}}</ref> <ref>{{cite web|title=WEAD East I Women and the Environment|url=http://www.kbcc.cuny.edu/artgallery/Pages/ewead.aspx|publisher=Kingsborough Community College|accessdate=2013-08-12}}</ref> 

One of the co-founders, Ms. Steinman, is considered a leader in the eco art field and has participated in roundtables and artists in residences programs,<ref>{{cite web|title=Artist Talk with Susan Steinman|url=http://goddard.edu/news-events/events/artist-talk-susan-steinman|publisher=Goddard College|accessdate=2013-08-12}}</ref> <ref>{{cite web|title=Eco Art Video Salon|url=http://www.berkeleyartcenter.org/programs_Q4-2010.html|publisher=Berkeley Arts Center|accessdate=2013-08-12}}</ref> and is listed in the sculptor directory of the International Sculpture Center.<ref>{{cite web|title=Sculptor Susan Leibovitz Steinman|url=http://www.sculpture.org/portfolio/sculptorPage.php?sculptor_id=1000451|publisher=International Sculpture Center|accessdate=2013-08-12}}</ref>  Another co-founder, Jo Hanson, was instrumental in founding an EPA Artist in Residence Program, which was aimed at educating the public about recycling. Another of the WEAD co-founders, Estelle Akamine, was also one of the artists in residence.<ref>{{cite web|title=Recology’s Artist in Residence|url=http://www.epa.gov/wastes/conserve/smm/web-academy/2011/feb11.htm|publisher=US Environmental Protection Agency|accessdate=2013-08-12}}</ref> Ms. Akamine's work has also been featured at the Museum of Craft and Folk Art museum store<ref>{{cite web|title=Museum Store|url=http://www.mocfa.org/store/artists.htm|publisher=Museum of Craft and Folk Art|accessdate=2013-08-12}}</ref> and has lectured at a textile lecture series.<ref>{{cite web|last=Valoma|first=Deborah|title=Textiles Lecture Series Archive|url=https://www.cca.edu/news/2012/08/27/textiles-lecture-archive|publisher=California College for the Arts|accessdate=2013-08-12}}</ref> All three co-founders were featured in a discussion about women artists of the American West whose art was about current social concerns.<ref>{{cite web|last=Cohn|first=Terri|title=Nature, Culture and Public Space|url=http://www.cla.purdue.edu/WAAW/Cohn/index.html|publisher=Purdue University|accessdate=2013-08-12}}</ref> 

The directory lists a wide variety of [[Woman artists]], such as [[Marina DeBris]], a [[trashion]] artist, [[Betty Beaumont]], often called a pioneer of environmental art, and Shai Zakai.

WEAD also published a magazine, which focuses on such topics as dirty water, and the legacy of atomic energy. A recent guest editor was Dr. Elizabeth Dougherty, founder of Wholly H2O, and speaker at events such as Pacific Gas and Electric Company conference on [[Water conservation]]<ref>{{cite web|title=2010 Water Conservation Showcase Speakers Save Water by Going Paperless!|url=http://www.pge.com/pec/water/presentations.shtml|publisher=Pacific Gas and Electric Company|accessdate=2013-08-12}}</ref> and Toulumne County's conference on [[greywater]].<ref>{{cite web|title=Greywater in California:  Designing, Managing, Monitoring|url=http://portal.co.tuolumne.ca.us/psp/ps/TUP_HS_ENVIR_HEALTH/ENTP/c/TU_DEPT_MENU.TUOCM_HTML_COMP.GBL?action=U&CONTENT_PNM=EMPLOYEE&CATGID=2651|publisher=TUOLUMNE COUNTY ENVIRONMENTAL HEALTH|accessdate=2013-08-12}}</ref> Linda Weintraub was a contributor to a recent issue of the WEAD magazine. Ms. Weintraub is the author of well known books on art and activism<ref>{{cite web|title=Drop Dead Gorgeous: Beauty and the Aesthetics of Activism|url=http://artsci.ucla.edu/?q=events/art-activism-linda-weintraub|publisher=UCLA Art Sci Center|accessdate=2013-08-12}}</ref> such as "To Life!"<ref>{{cite web|title=To Life! Eco Art in Pursuit of a Sustainable Planet|url=http://www.ucpress.edu/book.php?isbn=9780520273627|publisher=University of California Press|accessdate=2013-08-12}}</ref>  and is an eco art activist.<ref>{{cite web|last=Lambe|first=Claire|title=An Interview with Linda Weintraub – Curator of “Dear Mother Nature: Hudson Valley Artists 2012” at The Dorsky|url=http://www.rollmagazine.com/an-interview-with-linda-weintraub-%E2%80%93-curator-of-%E2%80%9Cdear-mother-nature-hudson-valley-artists-2012%E2%80%9D-at-the-dorsky/|publisher=Roll Magazine, Mark Gruber Gallery|accessdate=2013-08-12}}</ref> 

==References==
{{reflist}}

[[Category:1996 introductions]]
[[Category:Directories]]
[[Category:Environmental art]]
[[Category:Women artists]]
<=====doc_Id=====>:504
<=====title=====>:
Writers' & Artists' Yearbook
<=====text=====>:
[[File:Writers' & Artists' Yearbook cover 2003.jpg|thumb|150px|2003 edition of ''Writers' & Artists' Yearbook'']]
{{Italic title}}'''''Writers' & Artists' Yearbook''''' is an annual directory for writers, designers, illustrators and photographers. It is published in the UK each July, with a separate version for children's writers and artists published in August. The yearbook contains some 4,500 named industry contacts updated for each edition and includes articles about getting work published.<ref Name=BBC>[http://news.bbc.co.uk/dna/place-lancashire/plain/A16932017 "The Writers' and Artists' Yearbook", BBC]</ref><ref name="Irish Times">{{cite news|title=Essential Reading for Writers|newspaper=Irish Times|date=13 September 2003}}</ref> In 2007, an associated website, known as Writers&Artists, was launched.<ref name="website launch">{{cite web|title=New website with free resources for writers and artists|url=http://www.publishers.org.uk/index.php?option=com_content&view=article&id=554:new-website-with-free-resources-for-writers-and-artists&catid=80:general-news&Itemid=1617|publisher=Publishers Association|accessdate=2 March 2014}}</ref>

== History ==

First published in 1906, by [[A & C Black|Adam & Charles Black]], the original ''Writers’ & Artists’ Yearbook'' was an 80-page booklet, costing one [[shilling]]. It gave details of seven literary agents and 89 publishers.<ref Name=BBC/> It has been published on an annual basis since, expanding over time to include information for illustrators and photographers.<ref Name=BBC/> A & C Black became part of [[Bloomsbury Publishing]] in 2000, and other titles in its reference division include ''[[Who's Who (UK)|Who's Who]]'', ''[[Wisden Cricketers' Almanack|Wisden]]'' and ''[[Black's Medical Dictionary]]''.<ref name="A & C Black">{{cite news|last=Neill|first=Graeme|title=Coleman to leave A & C Black for Magi|url=http://www.thebookseller.com/news/coleman-leave-c-black-magi.html|accessdate=2 March 2014|newspaper=The Bookseller|date=2 February 2011}}</ref>
Articles offering advice first appeared in the 1914 yearbook.<ref Name=BBC/> Forewords have been written by, among others, [[William Boyd (writer)|William Boyd]] and [[Kate Mosse]].<ref name=A&U>{{cite web|title=Writers' and Artists' Yearbook 2013|url=https://www.allenandunwin.com/default.aspx?page=305&book=9781408157497|publisher=Allen & Unwin|accessdate=2 March 2014}}</ref><ref name=Bibliography>{{cite web|last=Mosse|first=Kate|title=Complete Bibliography|url=http://www.katemosse.co.uk/index.php/kates-books/|publisher=Kate Mosse|accessdate=2 March 2014}}</ref> Following the success of ''[[Fifty Shades of Grey]]'', a new section on writing erotic fiction – by an anonymous author – appeared in the 2014 edition.<ref name="Fifty Shades">{{cite news|last=Wyatt|first=Daisy|title=Fifty Shades of Grey inspires new chapter on erotic fiction in Bloomsbury Writers' and Artists' Yearbook|url=http://www.independent.co.uk/arts-entertainment/books/news/fifty-shades-of-grey-inspires-new-chapter-on-erotic-fiction-in-bloomsbury-writers-and-artists-yearbook-8685703.html|accessdate=2 March 2014|newspaper=The Independent|date=3 July 2013}}</ref>

=== Website and competitions ===

In 2007, ''Writers' & Artists' Yearbook'' launched an associated website. Initially this was only accessible to anyone purchasing the print edition.<ref name="website launch"/> In 2009, the website was relaunched and now includes blogs from guest authors and a social networking feature that enables authors and artists to add a public profile.<ref name=Bookseller>{{cite news|last=Gallagher|first=Victoria|title=Writers and Artists Yearbook launches social networking|url=http://www.thebookseller.com/news/writers-and-artists-yearbook-launches-social-networking.html|accessdate=2 March 2014|date=7 August 2009}}</ref> From 2013, the website featured a section focusing on [[self-publishing]], also hosting a conference on the subject in November of that year in association with [[National Novel Writing Month]].<ref name=self-publish>{{cite news|title=Self-published writers get online resource|url=http://www.thebookseller.com/news/self-published-writers-get-online-resource.html|accessdate=2 March 2014|newspaper=The Bookseller|date=27 September 2013}}</ref>
''Writers' & Artists' Yearbook'' runs an annual short story competition and has also collaborated with Bloomsbury to run a competition for aspiring crime writers.<ref>{{cite web|title=Writers’ & Artists’ Yearbook 2014 Short Story Competition|url=http://www.commonwealthwriters.org/writers-and-artists-short-story-competition-2014/|publisher=Commonwealth Writers|accessdate=2 March 2014}}</ref><ref name="Book Trust">{{cite web|title=Prizes|url=http://www.booktrust.org.uk/books/adults/short-stories/prizes/|publisher=Book Trust|accessdate=2 March 2014}}</ref><ref name="crime competition">{{cite news|last=Williams|first=Charlotte|title=Bloomsbury launches crime story competition|url=http://www.thebookseller.com/news/bloomsbury-launches-crime-story-competition.html|accessdate=2 March 2014|newspaper=The Bookseller|date=1 March 2012}}</ref>

== Sections and listings ==

The yearbook is divided into the following sections:<ref Name=BBC/>
* Newspapers and magazines – regional, national and overseas, [[Print syndication|syndicates]] and [[News agency|news agencies]]
* Books – regional, national and overseas, audio publishers, [[Book packaging|book packagers]] and [[Book sales club|book clubs]]
* Poetry organisations
* Television, film and radio broadcasters
* Theatre – producers
* [[Literary agent]]s 
* Art and illustration – agents, commercial studios and card and stationery publishers 
* Societies, prizes and festivals – associations and clubs, prizes and awards and [[literary festival]]s
* Digital and self-publishing
* Resources for writers – courses, libraries and writers' retreats
* Copyright and libel information
* Finance for writers and artists.

== See also ==

* ''[[Writer's Digest]]''
* ''[[Novel & Short Story Writer's Market]]''

== References ==

{{reflist|2}}

== External links ==
*[http://www.writersandartists.co.uk/ Writers&Artists website]

{{DEFAULTSORT:Writers' and Artists' Yearbook}}
[[Category:Directories]]
[[Category:1906 establishments in the United Kingdom]]
[[Category:Handbooks and manuals]]
[[Category:Yearbooks]]
[[Category:Publishing]]
[[Category:A & C Black books]]
<=====doc_Id=====>:507
<=====title=====>:
Category:Index (publishing)
<=====text=====>:
[[Category:Library science]]
[[Category:Publishing]]
[[Category:Directories]]
<=====doc_Id=====>:510
<=====title=====>:
Category:Directory services
<=====text=====>:
{{cat main|Directory service}}

[[Category:Directories]]
[[Category:Computer access control protocols]]
[[Category:Access control software]]
[[Category:Network service]]
[[Category:Database management systems]]
<=====doc_Id=====>:513
<=====title=====>:
Category:Public records
<=====text=====>:
{{Cat main|Public records}}

[[Category:Directories]]
[[Category:Documents]]
[[Category:Government information]]
[[Category:Privacy]]
<=====doc_Id=====>:516
<=====title=====>:
Business directory
<=====text=====>:
{{unreferenced|date=June 2009}}

[[File:PigotDirectory1839Kent.jpg|thumb|An example page from [[Pigot's Directory|Pigot's 1839 directory]] of businesses in the counties of Kent, Surrey and Sussex in England.]]
A '''business directory''' is a website or [[print media|printed]] listing of [[information]] which lists all businesses within some category. Businesses can be categorized by business, location, activity, or size. Business may be compiled either manually or through an automated online search software.  Online [[yellow pages]] are a type of business directory, as is the traditional [[phone book]].

The details provided in a business directory varies from business to business. They may include the business name, addresses, telephone numbers, location, type of service or products the business provides, number of employees, the service region and any [[professional association]]s. Some directories include a section for user reviews, comments, and feedback. Business directories in the past would take a printed format but have recently been upgraded to websites due to the advent of the internet.

Many business directories offer complimentary listings in addition to the premium options. There are many business directories and some of these have moved over to the [[internet]] and away from printed format. Whilst not being [[search engine]]s, business directories often have a search facility.

== Formats ==
Business directories can be in either [[hard copy]] or in [[Digital formats|digital format]]. Ease of use and distribution means that many trade directories have digital version.

Online Business Directories vary in quality and content. There is a balance between professional advertising, value for money and quality of service. Business owners are looking for ROI, web traffic, exposure for their business, plus [[Search engine optimization|SEO]] benefits of [[Backlink|backlinks]].

==See also==
*[[Web directory]]
*[[Kelly's Directory]]
*[[Surplus Record Machinery & Equipment Directory]]

{{DEFAULTSORT:Business Directory}}
[[Category:Business]]
[[Category:Directories]]
<=====doc_Id=====>:519
<=====title=====>:
Index Herbariorum
<=====text=====>:
{{Italic title}}
The '''Index Herbariorum''' provides a global directory of [[herbaria]] and their associated staff. This searchable online index allows scientists rapid access to data related to 3,400 locations where a total of 350&nbsp;million botanical [[Biological specimen|specimens]] are permanently housed (singular, [[herbarium]]; plural, herbaria). The Index Herbariorum has its own staff and website. Overtime, six editions of the Index were published from 1952 to 1974. The Index became available on-line in 1997.<ref name=IH>{{cite web|url=http://sciweb.nybg.org/science2/IndexHerbariorum.asp|title=Index Herbariorum|publisher=sciweb.nybg.org|accessdate=2014-11-23}}</ref>

The index was originally published by the [[International Association for Plant Taxonomy]], which sponsored the first six editions (1952–1974); subsequently the [[New York Botanical Garden]] took over the responsibility for the index. The Index provides the supporting institution's name (often a university, botanical garden, or not-for-profit organization) its city and state, each herbarium's acronym, along with contact information for staff members along with their research specialties and the important holdings of each herbarium's collection.

==Editors==
*6th edition (1974)  was co-edited by [[Patricia Holmgren]], Director of the  New York Botanical Garden, and
*7th printed edition ed. by  Patricia Holmgren. 
*8th printed editions, ed. by  Patricia Holmgren.
*Online edition, prepared by Noel Holmgren of the New York Botanical Garden
*2006+, ed. by Barbara M. Thiers, Director of the New York Botanical Garden  Herbarium <ref name=IH />
<ref name=IH />

==References==
{{Reflist}}

[[Category:Directories]]
[[Category:Herbaria]]


{{botany-stub}}
<=====doc_Id=====>:522
<=====title=====>:
Sands Directory
<=====text=====>:
{{Use Australian English|date=March 2015}}
{{Use dmy dates|date=March 2015}}

[[File:Sands Directory 1899 (book).JPG|thumb|1899 edition of Sands Directory ([[National Library of Australia]])]]
The '''Sands Directories''', also published as the '''Sands and Kenny Directory''' and the '''Sands and McDougall Directory''' were annual publications in Australia.

They listed household, business, society, and Government contacts<ref name=":0" /> in [[Melbourne]], [[Adelaide]] and [[Sydney]] including some rural areas of Victoria and New South Wales from the 1850s.<ref name=Bibliog>{{cite book|last1=Eslick, Christine; Joy Hughes, and R. Ian Jack|title=Bibliography of New South Wales local history: an annotated bibliography of secondary works published before 1982 and New South Wales directories 1828 -1950|date=1987|publisher=New South Wales University Press|location=Kensington, NSW|url=http://library.sl.nsw.gov.au/record=b1187352~S2|isbn=0-86840-154-4|pages=372; 398}}</ref> [[City directory|City directories]] are an important resource for historical research, allowing individual addresses and occupations to be linked to specific streets and suburbs.<ref>{{cite book|last1=Williams|first1=A.V.|title=The development and growth of city directories|date=1913|publisher=Williams directory co.|location=Cincinnati, Ohio|url=http://babel.hathitrust.org/cgi/pt?id=nyp.33433082423645;view=1up;seq=5|accessdate=5 March 2015}}</ref>

==Publisher==
[[File:Sands Directory 1899 (cover).JPG|thumb|1899 edition of Sands Directory (cover)]]
[[John Sands (printer)|John Sands]] (1818-1873) was an engraver, printer and stationer.  Born in England he moved to [[Sydney, Australia|Sydney]] in 1837.<ref name="ADB Sands">{{cite book|last1=Walsh|first1=G.P.|title='Sands, John (1818–1873)', Australian Dictionary of Biography|date=1976|publisher=National Centre of Biography, Australian National University|url=http://adb.anu.edu.au/biography/sands-john-4536|accessdate=5 March 2015}}</ref>  Sands formed several business partnerships, in 1851 with his brother-in-law Thomas Kenny, and in 1860 with Dugald McDougall with the business being known as [[John Sands (company)|Sands, Kenny & Co.]]<ref name="ADB Sands"/> Directory titles changed as the publisher changed partners, and at different points the Sands Directories were also published as the 'Sands and Kenny' or 'Sands and McDougall Directories'.<ref name=Kingston />

==Sands, Kenny & Co's commercial and general Melbourne directory==
The first Melbourne Directory was published by Sands and Kenny in 1857.<ref name=Kingston>{{cite web|title=Sands and McDougall Melbourne Directories|url=http://www.kingston.vic.gov.au/library/Library-Services/Family-History-Resources/Sands-and-McDougall-Melbourne-Directories|website=Kingston Libraries|publisher=Kingston Libraries|accessdate=10 February 2015|ref=Kingston}}</ref> By 1858 the second edition of the directory was distributed to public libraries in the major seaports of Great Britain, Ireland, the United States of America, and Canada.<ref>{{cite news |url=http://nla.gov.au/nla.news-article154855152 |title=Publications Received |newspaper=[[The Age]] |location=Melbourne |date=1 February 1858 |accessdate=5 March 2015 |page=6 }}</ref>  From 1862 to 1974 the Melbourne directories were published as the Sands and McDougall Melbourne Directory.<ref name=Kingston /><ref>{{cite news|last1=Stephens|first1=Andrew|title=Sands & McDougall directory exhibition brings old Melbourne back to life.|url=http://www.smh.com.au/entertainment/sands--mcdougall-directory-exhibition-brings-old-melbourne-back-to-life-20140811-102quc.html|accessdate=5 March 2015|work=Sydney Morning Herald|date=15 August 2014}}</ref>

The 1860 Melbourne directory was 400 pages long and contained over 10,000 entries.<ref name=":0">{{cite news |url=http://nla.gov.au/nla.news-article154880275 |title=Sands and Kenny's Melbourne Directory|newspaper=[[The Age]]|location=Melbourne |date=24 January 1860 |accessdate=5 March 2015 |page=5 }}</ref>

==Sands Sydney, Suburban and Country Commercial Directory==
[[File:Sands Directory 1899 (spine).JPG|thumb|1899 edition of Sands Directory (spine)]]
The ''Sands Sydney, Suburban and Country Commercial Directory'', first published in 1858,<ref name="FMP">{{cite web|title=New South Wales, Sydney Directory 1847-1913|url=http://www.findmypast.com.au/articles/world-records/full-list-of-australia-and-new-zealand-records/newspapers-directories-and-social-history/new-south-wales-sydney-directory-1847-1913|website=Find My Past|accessdate=5 March 2015}}</ref> included a variety of information including street addresses and businesses, farms and country towns, stock numbers (e.g. horses, cattle and sheep on each station) as well as information about public watering places including dams, tanks and wells.<ref>{{cite news |url=http://nla.gov.au/nla.news-article118819151 |title=Sands' Directory |newspaper=[[The Evening News (Sydney)|The Evening News]] |location=Sydney |date=15 January 1923 |accessdate=5 March 2015 |page=9 }}</ref>  With the primary function of post office directory it provides lists of householders, businesses, public institutions and officials.<ref>{{cite web|title=Family history and genealogy|url=http://www.sl.nsw.gov.au/about/collections/documenting/social/famhist.html|website=State Library of NSW|accessdate=5 March 2015}}</ref>

The Sydney editions of the directory, covering the state of New South Wales, were published each year from 1858–59 to 1932–33.<ref name=Sydney>{{cite web|title=Sands Sydney, Suburban and Country Commercial Directory|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=The City of Sydney|publisher=The City of Sydney|accessdate=10 February 2015}}</ref>  There were four years when the directory did not appear during this time, they were 1872, 1874, 1878 and 1881.<ref name= CGHG>{{Citation | author1=Cridland, Marilyn | author2=Central Coast Family History Group (N.S.W.) | title=A guide to the Sands Directory | publication-date=1997 | publisher=Central Coast Family History Group Inc | page =1|url=http://trove.nla.gov.au/work/35275389 | accessdate=5 March 2015 }}</ref>  The directory is arranged by municipalities in which properties were located, listing the primary householder street by street.<ref>{{cite web|title=Sands Sydney Directory Guide|url=http://www.waverley.nsw.gov.au/__data/assets/pdf_file/0009/28557/Sands_Sydney_Directory_guide_for_LS_website_revised.pdf|website=Waverley Council|accessdate=5 March 2015}}</ref> As a consequence, the household and business information in the directories is used for research into Sydney history,<ref>{{cite web|title=Sands Directory – Researching your house's history|url=http://insidehistorymagazine.blogspot.com.au/2011/11/sands-directory-researching-your-houses.html|website=Inside History magazine|accessdate=5 March 2015}}</ref> with particular application for genealogical research.<ref name= CGHG/><ref name="Sands digital edition">{{cite web|title=Sands' Directory [digital edition]|url=http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory|website=City of Sydney|accessdate=5 March 2015}}</ref><ref>{{cite web|last1=Royal Australian Historical Society|title=Sands Directories are now online!|url=http://www.rahs.org.au/sands-directories-are-now-online/|accessdate=5 March 2015}}</ref>

By 1909 the Sydney directory contained over 1700 pages.<ref name=SMH_Trove>{{cite news |url=http://nla.gov.au/nla.news-article15027528 |title=Sands' Directory 1909. |newspaper=[[The Sydney Morning Herald]] |location=NSW |date=9 January 1909 |accessdate=11 February 2015 |page=11}}</ref> The full title of the 1913 edition of the directory of Sydney is ''Sands Sydney, Suburban and Country Directory for 1913 comprising, amongst other information, street, alphabetical, trade and professional, country towns, country alphabetical, pastoral, educational, governmental, parliamentary, law and miscellaneous lists''.<ref name="FMP"/>

==Sands & McDougall's South Australian directory==
Sands and McDougall arrived in [[Adelaide, South Australia|Adelaide]] in 1883.<ref name="SLSA">{{cite web|title=South Australian directories|url=http://guides.slsa.sa.gov.au/directories|website=State Library of South Australia|accessdate=5 March 2015}}</ref>  They took over the directory previously published by Josiah Boothby, publishing their first South Australian directory in January 1884.<ref name="SLSA"/><ref>{{Citation | author1=Sands & McDougall Limited | title=Sands & McDougall's South Australian directory : with which is incorporated Boothby's South Australian directory | publication-date=1884 | publisher=Printed and published by Sands & McDougall | url=http://trove.nla.gov.au/work/21481893 | accessdate=5 March 2015 }}</ref><ref>{{cite news |url=http://nla.gov.au/nla.news-article166349067 |title=South Australian Directory |newspaper=[[The Southern Cross (South Australia)|The Southern Cross]] |location=Adelaide |date=27 March 1896 |accessdate=5 March 2015 |page=4}}</ref> The Sands & McDougall's Directory of South Australia was published from 1884 to 1974.<ref name=Trove_SA>{{cite web | author1=Sands & McDougall Limited | title=Sands & McDougall's directory of South Australia | publication-date=1884 | publisher=Sands & McDougall | url=http://trove.nla.gov.au/work/21481863 | accessdate=11 February 2015 }}</ref>

==See also==
[[Western Australia Post Office Directory|Wise Directories]]

==External links==
* [http://www.cityofsydney.nsw.gov.au/learn/search-our-collections/sands-directory digitised Sydney sands directory], at the City of Sydney archives

==References==
{{reflist|2}}

[[Category:Gazetteers]]
[[Category:Directories]]
[[Category:Australian directories]]
<=====doc_Id=====>:525
<=====title=====>:
Yachting Pages
<=====text=====>:
{{Use dmy dates|date=September 2013}}
{{Use British English|date= September 2013}}
{{Infobox company
| logo     = [[File:Yachting Pages Logo Black KNOCKOUT.svg]]
| foundation       = [[Antibes]], [[France]] (2003)
| location         = [[Bristol]], [[Somerset]]
[[United Kingdom]]
| key_people       = 
| num_employees    = 34 (2013)
| industry         = [[Superyacht]]
| homepage         = {{url|www.yachting-pages.com/}}
}}
'''''Yachting Pages''''' is a [[superyacht]] business with a range of products aimed at [[Captain (nautical)|captains]] and [[crew]], [[shipyards]], refit yards and all within the superyacht industry. ''Yachting Pages'' is available either in its original printed form, or online. Since the first edition of ''Yachting Pages'' was released in 2004, the book has grown rapidly into an established superyacht [[Trade directory|directory]].

The annual print directory is available in three separate editions:

*''Mediterranean, Europe, Africa & Middle East''
*''USA, the Americas & Caribbean''
*''Australasia, Asia Pacific & Far East''

The [[Port]] Maps section at the front of every edition totals over 350 detailed maps of the world's superyacht [[marinas]]. Copies of the printed directory are hand delivered free of charge directly to superyachts by uniformed crew, and also to superyacht marinas and land-based superyacht businesses in over 92 countries.<ref> Yachtingpages.com </ref>

'''The Company'''

''Yachting Pages'' was founded in May 2003 from current CEO Steve Crowe's spare bedroom in [[Antibes]], [[France]], with only one other member of staff. The company is now based in [[Bristol]], [[United Kingdom]] with 34 staff members, many of whom are [[multi-lingual]]. 

The first copy of ''Yachting Pages'' was launched at the Genoa Charter Show, in May 2004. 

Since then, growth of the business has created more products: ''Yachtingpages.com, Yachting Pages Refit, Yachting Pages Delivers and Superyacht Owners' Guide (SYOG).'' 

'''Awards'''

Queens award for Enterprise: International Trade 2009. 
EADP European B2B Award 2009 

==References==
{{Reflist}}
{{refimprove|date=August 2013}}

==External links==
* [http://www.yachting-pages.com/ Yachting Pages]

[[Category:Directories]]
[[Category:Yachting]]
<=====doc_Id=====>:528
<=====title=====>:
Whitepages (company)
<=====text=====>:
{{pp-protect|small=yes}}
{{good article}}
{{Infobox dot-com company
| name             = Whitepages
| logo     = [[File:White-Pages-Logo.png|175px]]
| caption          =
| type             = Private 
| industry         = 
| foundation       = 1997
| founder          = [[Alex Algard]]
| defunct          = <!-- {{End date|YYYY|MM|DD}} -->
| location_city    = Seattle, Washington, US
| location_country = US
| locations        = <!-- Number of locations, stores, offices, etc. -->
| area_served      = Worldwide
| key_people       =  Rob Eleveld (CEO)<ref name="newCEO"/>
| products         = People search, contact data, mobile apps
| production       = 
| services         = 
| revenue          = $70 million (2015)<ref name="recentsource"/>
| operating_income = 
| net_income       = 
| aum              = <!-- Only used with financial services companies -->
| assets           = 
| equity           = 
| owner            = 
| num_employees    = 120 (2016)<ref>{{citation|publisher=Whitepages|title=Careers|url=http://whitepagesinc.com/about/careers.html|accessdate=August 19, 2013}}</ref>
| parent           = 
| divisions        = 
| subsid           = 
| website = {{URL|http://www.whitepages.com}}
| footnotes        = 
| intl             =
| bodystyle        =
| website_type          = Directory
| current_status        = Active
}}'''Whitepages''' is a provider of online directory services, fraud screening and identity verification for businesses, public record background checks, and other products, based on its database of contact information for people and businesses. It has the largest database available of contact information on US residents.<ref name="VB"/>

Whitepages was founded in 1997 as a hobby for then-[[Stanford]] student [[Alex Algard]]. It was incorporated in 2000 and received $45 million in funding in 2005. Investors were later bought-out by Algard in 2013. From 2008 to 2013, Whitepages released several mobile apps, a re-design in 2009, the ability for consumers to control their contact information, and other features. From 2010 to 2016, the company shifted away from advertising revenue and began focusing more on selling business services and subscription products.

==History==
The idea for Whitepages was conceived by Alex Algard, while studying at [[Stanford]] in 1996. Algard was searching for a friend's contact information and the phone company gave him the wrong number.<ref name="ppp"/> He thought of an online email directory as an easier to way to find people.<ref name="seven"/><ref name="two">{{cite news|title=WhitePages.com has number for fast growth|url=http://community.seattletimes.nwsource.com/archive/?date=20031013&slug=btinterface13|newspaper=The Seattle Times|accessdate=August 7, 2013|date=October 13, 2003}}</ref> Algard bought the Whitepages.com domain for nine hundred dollars,<ref name="four">{{cite news|first=Nicholas|last=Carlson|date=January 24, 2007|url=http://www.internetnews.com/xSP/article.php/3655611|publisher=InternetNews|title=WhitePages.com: Reach out and search someone|accessdate=December 2, 2013}}</ref><ref name="recentsource"/> which he says was all of his savings at the time.<ref name="seven"/> He continued operating the website as a hobby while working as an investment banker for [[Goldman Sachs]].<ref name="dakfhukajehf"/> He expanded the database of contact information using data licensed from American Business Information (now a part of Infogroup).<ref name="recentsource"/> Eventually WhitePages was producing more ad-revenue than Algard was earning at Goldman Sachs.<ref name="recentsource"/> In 1998, Algard left his job to focus on the website; he incorporated Whitepages in 2000.<ref name="dakfhukajehf">{{citation|publisher=Private Equity Growth Capital Council|url=http://www.pegcc.org/wordpress/wp-content/uploads/pec_cs_whitepages_020309a.pdf|title=WhitePages.com: From hobby to number one people search destination|accessdate=August 6, 2013}}</ref>

The site grew and attracted more advertisers. The company brokered deals with Yellowpages and Superpages, whereby Whitepages earned revenue for sending them referral traffic. By 2005, $15 million in annual revenues was coming from these contracts.<ref name="recentsource"/> In 2003, Algard stepped down as CEO to focus on CarDomain.com, which he had also founded<ref name="ppp">{{cite news|first=Brad|last=Broberg|title=Founder returns to WhitePages.com|publisher=Puget Sound Business Journal|date=September 30, 2007|url=http://www.bizjournals.com/seattle/stories/2007/10/01/focus10.html|accessdate=August 7, 2013}}</ref> and Max Bardon took his place as CEO temporarily.<ref name="recentsource"/> In 2005, Technology Crossover Ventures and Providence Equity Partners invested $45 million in the company.<ref name="recentsource"/><ref name="one"/> That same year, MSN adopted Whitepages' directory data for its "Look it up" feature.<ref>{{cite news|title=MSN Replaces InfoSpace with WhitePages.com|url=http://www.mediapost.com/publications/article/28828/#axzz2bIuB3tM1|first=Shankar|last=Gupta|date=April 5, 2005|accessdate=August 7, 2013|publisher=MediaPost}}</ref> Algard returned to the company in 2007.<ref name="ppp"/> By the end of that year, the Whitepages database had grown to 180 million records<ref>{{cite news|title=WhitePages.com coverage expands from 40 to 80 percent|url=http://seattletimes.com/html/businesstechnology/2004062675_btbriefs10.html|newspaper=The Seattle Times|date=December 10, 2007|accessdate=August 7, 2013}}</ref> and the company was listed as one of [[Deloitte]]'s 500 fastest growing technology companies in North America three times.<ref name="seven"/><ref>{{cite news|title=WhitePages hires new CTO|first=Rebecca|last=Collins|url=http://www.bizjournals.com/seattle/blog/techflash/2010/11/whitepages-taps-new-cto.html|publisher=Puget Sound Business Journal|date=November 17, 2010|accessdate=August 8, 2013}}</ref> By 2008 the company had $66 million in annual revenues.<ref name="recentsource"/>

In 2008, Whitepages said it would start working on options for users to control their information on the site.<ref>{{cite news|first=Steven|last=Vaughan-Nichols|newspaper=Computerworld|url=http://www.computerworld.com.au/article/216557/whitepages_com_grapples_privacy_web_2_0_world/?|title=WhitePages.com grapples with privacy in Web 2.0 world|date=May 19, 2008|accessdate=August 7, 2013}}</ref> That same year, it acquired [[Voice over Internet Protocol|VoIP]] developer [[Snapvine]]<ref name="one">{{cite news|first=Angel|last=Gonzalez|url=http://seattletimes.com/html/businesstechnology/2004458452_whitepages05.html|newspaper=The Seattle Times|title=WhitePages.com to buy Snapvine|accessdate=August 7, 2013|date=June 5, 2008}}</ref> in order to add features where users could be called through the website without giving out their phone number.<ref>{{cite news|title=WhitePages.com to buy Snapvine for around $20 million|first=Michael|last=Arrington|date=June 4, 2008|accessdate=August 7, 2013|url=http://techcrunch.com/2008/06/04/whitepagescom-to-buy-snapvine-for-around-20-million/|publisher=TechCrunch}}</ref> It also introduced an [[api]],  which gave third-party developers access to Whitepages' data.<ref>{{cite news|first=Mike|last=Gunderloy|date=March 31, 2008|url=http://gigaom.com/2008/03/31/open-phone-data-whitepages/|accessdate=August 7, 2013|title=Open Phone Data from WhitePages.com|publisher=Giga Om}}</ref> Whitepages released an iOS app that August, followed by the Whitepages Caller ID app for Android devices  in February 2009<ref>{{cite news|publisher=VentureBeat|first=MG|last=Siegler|date=February 27, 2009|accessdate=August 7, 2013|url=http://venturebeat.com/2009/02/27/caller-id-a-paid-android-app-to-better-screen-my-phone-calls/|title=Caller ID: A paid Android app to better screen my phone calls}}</ref> and for Blackberry that May.<ref name="plp">{{cite news|publisher=VentureBeat|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|first=Matt|last=Marshall|date=May 7, 2009|accessdate=August 7, 2013}}</ref> 

The app displays information on callers, such as their latest social media posts, local weather at the caller's location and the identity of the caller.<ref name="eightlyy">{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}</ref><ref>{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}</ref><ref name="twenty">{{cite news|title=WhitePages' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}</ref> It originally had the ability to display information on callers, such as their latest social media posts, local weather at the caller's location and the identity of the caller.<ref name="eightlyy">{{cite news|first=Austin|last=Carr|newspaper=Fast Company|url=http://www.fastcompany.com/3000252/whitepages-launches-caller-id-social-mobile-age|title=WhitePages Launches Caller ID for the Social, Mobile Age|date=August 7, 2012|accessdate=August 7, 2013}}</ref><ref>{{cite news|newspaper=Time Magazine|first=Doug|last=Aamoth|url=http://techland.time.com/2012/12/04/top-10-tech-lists/slide/current-caller-id-android/|date=December 4, 2012|accessdate=August 7, 2013|title=Current Caller ID (Android)}}</ref><ref name="twenty">{{cite news|title=WhitePages' new Current Caller ID App is the future of smartphone calling|url=http://venturebeat.com/2012/08/08/whitepages-current-caller-id-android/|date=August 8, 2012|first=Devindra|last=Hardawar|accessdate=August 7, 2013|publisher=VentureBeat}}</ref> The ability for consumers to add themselves to the directory was added in the summer of 2009 and being able to edit existing entries was added that October.<ref>{{cite news|title=WhitePages Now Lets you control your own listings|first=Erick|last=Schonfeld|date=October 14, 2009|accessdate=August 8, 2013|url=http://techcrunch.com/2009/10/14/whitepages-now-lets-you-control-your-own-listings/|publisher=TechCrunch}}</ref>

Whitepages.com underwent a re-design in 2009.<ref name="three">{{cite news|title=WhitePages launches $2.5 million overhaul|first=Brier|last=Dudley|url=http://seattletimes.com/html/technologybrierdudleysblog/2009467080_whitepagescom_launches_25_mill.html|date=July 14, 2009|accessdate=August 7, 2013|newspaper=The Seattle Times}}</ref> According to VentureBeat reporter Matt Marshall, the redesign made the advertising "cleaner" and made it more obvious when someone was going to a third-party website like US Search.<ref name="VB">{{cite news|date=July 14, 2009|first=Matt|last=Marshall|url=http://venturebeat.com/2009/07/14/whitepages-now-the-largest-database-of-american-people-cleans-up-act/|publisher=VentureBeat|title=WhitePages, now the largest database of American people, cleans up act|accessdate=August 7, 2013}}</ref> Marshall had previously criticized Whitepages, because website users that clicked on US Search ads and purchased data from US Search were sent through perpetual advertisements for other services that made it difficult to access the information they paid for.<ref name="VB"/><ref>{{cite news|title=The background-check scams: Is WhitePages really better than Intelius?|url=http://venturebeat.com/2009/05/07/the-background-check-scams-is-whitepages-really-better-than-intelius/|date=May 7, 2009|first=Matt|last=Marshall|accessdate=August 7, 2013}}</ref> A local business lookup feature called "Store Finder" was added in June 2010.<ref>{{cite news|title=WhitePages upgrades business search, adds "store finder"|url=http://seattletimes.com/html/technologybrierdudleysblog/2012197459_whitepages_upgrades_business_s.html|first=Brier|last=Dudley|newspaper=The Seattle Times|date=June 24, 2010}}</ref> The following month, Whitepages.com launched a deal site, Dealpop.com,<ref>{{cite news|title=Local shops join forces with coupon websites to sweeten sales|first=Melissa|last=Allison|author2=Amy Martinez |url=http://seattletimes.com/html/retailreport/2012259556_retailreport02.html|newspaper=The Seattle Times|date=July 1, 2010|accessdate=August 6, 2013}}</ref> which differed from [[Groupon]] by offering short-term deals on nationally available products.<ref>{{cite news|first=Amy|last=Martinez|date=October 20, 2010|accessdate=August 7, 2013|newspaper=The Seattle Times|url=http://seattletimes.com/html/businesstechnology/2013209878_dealpopweb21.html|title=WhitePages' DealPop to try national approach as it takes on Groupon, other coupon websites}}</ref> Dealpop was sold to [[Martin Tobias#Tippr.com|Tippr]] the following year.<ref>{{cite news|title=Tippr Grabs Sales & Tech Talent in DealPop Acquisition, Continuing Daily Deals Dogfight for Third Place|url=http://www.xconomy.com/seattle/2011/06/01/tippr-grabs-sales-tech-talent-in-dealpop-acquisition-continuing-daily-deals-dogfight-for-third-place/|newspaper=Xconomy|date=July 1, 2011|accessdate=August 7, 2013|first=Curt|last=Wooodward}}</ref>

In 2010, Superpages and Yellowpages cut back spending with Whitepages from $33 million to $7 million, causing a substantial decline in revenues and a tense relationship with investors. Algard spent $50 million in cash the company had on-hand and $30 million from a bank loan, to buyout the investors in 2013. He also used his personal house, savings account and personal belongings as collateral for the loan.<ref name="recentsource"/> Algard began shifting the company's business model to reduce its reliance on advertising and instead focus on business users and paid subscriptions.<ref name="recentsource"/><ref name="Carlson 2013">{{cite web | last=Carlson | first=Nicholas | title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million | website=Business Insider | date=October 21, 2013 | url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10 | accessdate=August 18, 2016}}</ref> 

Whitepages released the Localicious app in July 2011. The app was released on Android first, because Whitepages was frustrated with Apple's approval process for iPhone apps.<ref name="agiu">{{cite news|title=WhitePages goes Android first with latest app|url=http://news.cnet.com/8301-1023_3-20079150-93/whitepages-goes-android-first-with-latest-app/|date=July 13, 2011|first=Ina|last=Fried|accessdate=August 7, 2013|publisher=All Things Digital}}</ref> Whitepages PRO was also introduced that same year.<ref name="cardnotpresent">{{cite news|url=http://pro.whitepages.com/sites/pro.whitepages.com/files/Marketing_Documents/CardNotPresent%20Article%2010.24.12.pdf|publisher=CNP Report|first=D.J.|last=Murphy|date=October 24, 2012|accessdate=September 24, 2013|title=WhitePages PRO Taps Phone Data and More to Identify CNP Fraud}}</ref> An updated Android app called  Current Caller ID was released in August 2012.<ref name="eightlyy"/> Within a year of its release, 5 billion calls and texts had been transmitted using the app. It was updated in July 2013 with new features, such as the ability to customize the layout of caller information for each caller and the ability to "Like" Facebook posts from within the app.<ref name="fgy">{{cite news|title=WhitePages' Current Caller ID app powers more than 5B calls & texts, adds new customization features|url=http://venturebeat.com/2013/07/25/whitepages-current-caller-id-app-powers-more-than-5b-calls-texts-adds-new-customization-features/|publisher=VentureBeat|first=Devindra|last=Hardawar|date=July 25, 2013|accessdate=August 7, 2013}}</ref> In June 2013, Whitepages acquired Mr. Number, an Android app for blocking unwanted callers.<ref>{{cite news|title=WhitePages Scoops up Mr. Number, an Android App for Blocking Unwanted Calls|date=June 1, 2013|first=Ina|last=Fried|url=http://allthingsd.com/20130601/whitepages-scoops-up-mr-number-an-android-app-for-blocking-unwanted-calls/|newspaper=The Wall Street Journal|accessdate=August 7, 2013}}</ref>

In August 2013 Whitepages purchased all the interests in the company owned by investors for $80 million.<ref name="dafhybniub">{{cite news|title=With Buyback, 16-Year-Old Startup WhitePages Is Doing Something Very Rare With $80 Million|first=Nicholas|last=Carlson|date=October 21, 2013|url=http://www.businessinsider.com/whitepages-stock-buyback-2013-10#ixzz2qRETXgXX|publisher=Business Insider|accessdate=October 30, 2014}}</ref><ref>{{cite news|title=Nextcast: WhitePages CEO Alex Algard on the distraction of outside investors and keeping your startup zeal|first=Jeff|last=Dickey|date=April 5, 2014|accessdate=May 2, 2014|url=http://www.geekwire.com/2014/nextcast-whitepages-ceo-alex-algard-distraction-outside-investors-keep-startup-zeal/|publisher=Geekwire}}</ref> In 2015, WhitePages acquired San Francisco-based NumberCorp to improve the database of phone numbers used for scams in the Caller ID app.<ref name="Perez 2015">{{cite web | last=Perez | first=Sarah | title=Whitepages Acquires NumberCop To Improve Its Scam-Detecting Caller ID App | website=TechCrunch | date=June 10, 2015 | url=http://social.techcrunch.com/2015/06/10/whitepages-acquires-numbercop-to-improve-its-scam-detecting-caller-id-app/ | accessdate=August 12, 2016}}</ref> In April 2016, Whitepages spun-off its caller ID business into a separate company called Hiya<ref name="Lunden 2016">{{cite web | last=Lunden | first=Ingrid | title=Whitepages spins out its caller-ID business as Hiya to take on TrueCaller | website=TechCrunch | date=April 27, 2016 | url=http://social.techcrunch.com/2016/04/27/whitepages-hiya/ | accessdate=July 8, 2016}}</ref> with a staff of 40 in Seattle.<ref name="Flynn 2016">{{cite web | last=Flynn | first=Kerry | title=Meet Hiya: Whitepages Spins Off Caller ID Business With Mission To Fight Robocalls, Spam Texts Worldwide | website=International Business Times | date=April 27, 2016 | url=http://www.ibtimes.com/meet-hiya-whitepages-spins-caller-id-business-mission-fight-robocalls-spam-texts-2360298 | accessdate=July 8, 2016}}</ref> In September 2016, Alex Algard stepped down as CEO of WhitePages, in order to focus on the mobile spam-blocking spin-off Hiya. He appointed Rob Eleveld as the new WhitePages CEO.<ref name="newCEO">{{cite web | title=Whitepages Founder Alex Algard Gives Up CEO Slot To Focus On Caller ID Startup Hiya | newspaper=Forbes | date=September 16, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/09/16/whitepages-founder-alex-algard-gives-up-ceo-slot-there-to-focus-on-caller-id-spinoff-hiya/#2db4ba761803 | accessdate=September 20, 2016}}</ref>

==Services==
Whitepages has the largest database of contact information on Americans.<ref name="VB"/> As of 2008, it had data on about 90 percent of the US adult population,<ref>{{cite news|publisher=IntoMobile|first=Dusan|last=Belic|date=May 8, 2012|accessdate=September 24, 2013|url=http://www.intomobile.com/2012/05/08/whitepages-ios-app-gets-nearby-search-capability/|title=WhitePages' iOS app gets nearby search capability}}</ref> including 200 million records on people and 15 million business listings.<ref name="seven">{{cite news|title=A Directory of Success: WhitePages CEO Alex Algard|date=February 2, 2011|newspaper=Examiner|first=Paul|last=Kim}}</ref> Whitepages' data is collected from property deeds,<ref name="five"/> telecom companies, and public records.<ref name="ll">{{cite news|title=WhitePages IDs Growth in the Explosion of Personal Data|date=August 20, 2012|first=Curt|last=Woodward|accessdate=August 7, 2013|url=http://www.xconomy.com/seattle/2012/08/20/whitepages/}}</ref> Privacy is a common concern regarding Whitepages' publishing of personal contact information.<ref name="StairReynolds2008">{{cite book|author1=Ralph M. Stair|author2=George Reynolds|author3=George Walter Reynolds|title=Fundamentals of Information Systems|url=https://books.google.com/books?id=J85RP4YmBTYC&pg=PA253|accessdate=7 August 2013|date=December 2008|publisher=Cengage Learning|isbn=978-1-4239-2581-1|pages=253–}}</ref> The Whitepages.com website has features that allow users to remove themselves from the directory or correct and update information.<ref name="five">{{cite news|title=Connecticut may let residents remove directory information|url=http://www.scmagazine.com/connecticut-may-let-residents-remove-directory-data/article/100267/#|date=December 28, 2007|first=Dan|last=Kaplan|newspaper=SC Magazine}}</ref><ref name="StairReynolds2008"/> WhitePages.com has about 50 million unique visitors per month<ref>{{cite news|publisher=VentureBeat|title=WhitePages acquires Mr. Number, the phone-spam Android app with 7M downloads, to reduce phone spam|url=http://www.reuters.com/article/2013/05/31/idUS27174982720130531|first=John|last=Koetsier|date=May 31, 2013|accessdate=December 2, 2013}}</ref> and performs two billion searches per month.<ref name="cardnotpresent"/>

WhitePages started developing features for business users around 2010.<ref name="recentsource"/> WhitePages Pro is used for things like verifying the identity of a sales lead, find fake form data in online forms and to check form data from consumers making a purchase against common indicators of fraud, like shipping to a mailbox at an unoccupied building.<ref name="recentsource">{{cite news| first=Amy|last=Feldman |title=Alex Algard Risked Everything To Turn His Struggling Firm, Whitepages, Into A Growing Tech Company | newspaper=Forbes | date=August 23, 2016 | url=http://www.forbes.com/sites/amyfeldman/2016/08/03/alex-algard-risked-everything-to-turn-his-struggling-firm-whitepages-into-a-growing-tech-company/#165b97ae73d0 | accessdate=August 10, 2016}}</ref><ref name="cardnotpresent"/><ref name="Whitepages Pro">{{cite web | title=Whitepages Pro – Mobile Identity Data for Businesses | website=Whitepages Pro | url=http://pro.whitepages.com/ | accessdate=August 15, 2016}}</ref> In 2016, advertising on WhitePages.com was turned off in favor of selling monthly subscriptions that give users unlimited background checks and other records.<ref name="recentsource"/>

As of 2013 Whitepages provides its data and related services through seven web properties, ten mobile apps<ref>{{citation|url=http://whitepagesinc.com/about/|publisher=WhitePages|title=About Us|accessdate=December 2, 2013}}</ref> and  through multiple web properties, including 411.com and Switchboard.com.<ref name="SuiElwood2012">{{cite book|author1=Daniel Zhi Sui|author2=Sarah Elwood|author3=Michael F. Goodchild|title=Crowdsourcing Geographic Knowledge: Volunteered Geographic Information (VGI) in Theory and Practice|url=https://books.google.com/books?id=SSbHUpSk2MsC&pg=PA267|accessdate=7 August 2013|date=10 August 2012|publisher=Springer|isbn=978-94-007-4587-2|pages=267–}}</ref> The Hiya app (previously known as WhitePages Caller ID) checks incoming calls against a database of phone numbers known for spam or scam calls and helps users report scams to the Federal Trade Commission.<ref name="Stern 2016">{{cite web | last=Stern | first=Joanna | title=How to Stop Robocalls … or at Least Fight Back | website=WSJ | date=June 28, 2016 | url=http://www.wsj.com/articles/how-to-stop-robocalls-or-at-least-fight-back-1467138771 | accessdate=July 8, 2016}}</ref><ref name="Lerman 2016">{{cite web | last=Lerman | first=Rachel | title=Whitepages spins out mobile caller-ID startup Hiya | website=The Seattle Times | date=April 27, 2016 | url=http://www.seattletimes.com/business/technology/whitepages-spins-out-mobile-caller-id-startup-ceo-takes-on-dual-roles/ | accessdate=July 8, 2016}}</ref> Hiya mobile app replaces the Android user interface for making and receiving phone calls.<ref name="fgy"/>

==References==
{{reflist|2}}

==External links==
*[http://www.whitepages.com/ Official website]

{{DEFAULTSORT:Whitepages.Com}}
[[Category:Directories]]
[[Category:Internet properties established in 1997]]
[[Category:Privately held companies based in Washington (state)]]
[[Category:Companies based in Seattle, Washington]]
[[Category:Online person databases]]
<=====doc_Id=====>:531
<=====title=====>:
VisualRank
<=====text=====>:
'''VisualRank''' is a system for [[image retrieval|finding]] and ranking images by analysing and comparing their content, rather than searching image names, Web links or other text.  [[Google]] scientists made their VisualRank work public in a paper describing applying [[PageRank]] to Google image search at the International World Wide Web Conference in [[Beijing]] in 2008.
<ref name=Jing08>
{{cite journal
 | author = Yushi Jing and Baluja, S.
 | title = VisualRank: Applying PageRank to Large-Scale Image Search
 | journal = Pattern Analysis and Machine Intelligence, IEEE Transactions on
 | year = 2008
 | volume = 30
 | number = 11
 | pages = 1877–1890
 | ISSN = 0162-8828
 | doi = 10.1109/TPAMI.2008.121}}.
</ref>

<blockquote>
We cast the image-ranking problem into the task of identifying "authority" nodes on an inferred visual similarity graph and propose VisualRank to analyze the visual link structures among images. The images found to be "authorities" are chosen as those that answer the image-queries well. 
</blockquote>

==Methods==
Both [[computer vision]] techniques and [[locality-sensitive hashing]] (LSH) are used in the VisualRank [[algorithm]].  Consider an image search initiated by a text query.  An existing search technique based on image metadata and surrounding text is used to retrieve the initial result candidates ([[PageRank]]), which along with other images in the index are clustered in a [[Graph (data structure)|graph]] according to their similarity (which is precomputed).  [[Centrality]] is then measured on the clustering, which will return the most canonical image(s) with respect to the query.  The idea here is that agreement between users of the web about the image and its related concepts will result in those images being deemed more similar.  VisualRank is defined iteratively by <math>VR = S^* \times VR</math>, where <math>S^*</math> is the image similarity matrix.  As matrices are used, [[eigenvector centrality]] will be the measure applied, with repeated multiplication of <math>VR</math> and <math>S^*</math> producing the [[eigenvector]] we're looking for.  Clearly, the image similarity measure is crucial to the performance of VisualRank since it determines the underlying graph structure.

The main VisualRank system begins with local feature vectors being extracted from images using [[scale-invariant feature transform]] (SIFT).  Local feature descriptors are used instead of color histograms as they allow similarity to be considered between images with potential rotation, scale, and perspective transformations. Locality-sensitive hashing is then applied to these feature vectors using the [[locality-sensitive hashing#methods|p-stable distribution scheme]].  In addition to this, LSH amplification using AND/OR constructions are applied.  As part of the applied scheme, a [[Gaussian distribution]] is used under the [[L2 norm#Euclidean norm|<math>l_2</math> norm]].

==References==
{{Reflist}}

==External links==
*[http://www.nytimes.com/2008/04/28/technology/28google.html?adxnnl=1&ref=business&adxnnlx=1210140241-DOwaJr/5AjMPCYJDerw++Q New York Times article]
*[http://tech.slashdot.org/article.pl?sid=08/04/28/1852254&from=rss Slashdot article]
[[Category:Internet search]]
[[Category:Image processing]]
<=====doc_Id=====>:534
<=====title=====>:
Category:Real-time web
<=====text=====>:
{{Cat main|Real-time web}}

[[Category:Internet search]]
[[Category:Real-time computing|web]]
<=====doc_Id=====>:537
<=====title=====>:
Mystery Seeker
<=====text=====>:
{{Infobox website
| name               = Mystery Seeker
| logo               = MYSTERYSEEKER logo.jpeg
| logo_alt           = The mysteryseeker.com logo
| logocaption       = The logo found at [http://www.mysteryseeker.com mysteryseeker.com], decorated with fog and a moon in the background.
| screenshot         = mysteryseeker.com screenshot.jpeg
| collapsible        = y
| screenshot_alt     = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com]
| caption            = Screenshot of [http://www.mysteryseeker.com mysteryseeker.com].
| url                =  {{URL|www.mysteryseeker.com}}
| slogan             = “What will you search for?”
| commercial         = <!-- "Yes", "No" or leave blank -->
| type               = Search engine
| registration       = None
| language           = [[English language|English]]
| content_license    = &copy; Mystery Seeker, 2009
| owner              = Mystery Seeker
| launch_date        = {{start date and age|2009|10|02|df=yes}}
| alexa              = {{decrease}} [http://www.alexa.com/siteinfo/mysteryseeker.com 3110015] (Global, November 2015)
| current_status     = Online but defunct 
| footnotes          = 
}}
'''Mystery Seeker''' is a website based on the [[Google]] search engine.<ref name="Chivers 2009" /> Until November 30, 2009, the website was known as '''Mystery Google''', but on December 1, 2009, the name changed to '''Mystery Seeker'''. It has been featured in a number of technology blogs.<ref>{{cite news| url=http://www.huffingtonpost.com/2009/10/12/mystery-google-surprise-y_n_318089.html | work=Huffington Post | first=Bianca | last=Bosker | title=Mystery Google: Surprise Yourself With Someone Else's Search Results | date=October 12, 2009}}</ref><ref>[http://mashable.com/2009/10/12/mystery-google/ Mystery Google: The “I’m Feeling Lucky” Button Re-imagined]</ref><ref>[http://www.geekologie.com/2009/11/i_wasnt_looking_for_that_myste.php I Wasn't Looking For That: Mystery Google Gives You Previous Person's Search Query | Geekologie]</ref> Upon a search query, Mystery Seeker returns the results from the previous search, so “you get what the person before you searched for.”<ref name="Chivers 2009">{{Cite news|url=http://www.telegraph.co.uk/technology/google/6316140/Mystery-google-returns-other-peoples-search-results.html|title=Mystery Google returns other people's search results|accessdate=2009-11-23|publisher=The Telegraph|date=13 Oct 2009|author=Tom Chivers|location=London}}</ref>

There is a trend among the people on Mystery Seeker to add so-called "missions", where the next user is asked to do something. For example, "Your mission is to copy and paste this until you see it again. Then and only then will you be a true ninja".<ref>[http://www.softsailor.com/news/12457-how-to-receivegive-google-mystery-missions-and-why-they-are-fun.html Tech Source]</ref> Other examples of possible missions include telling someone you love them, sending someone a get well card, mailing a banana to someone, etc. There are also references to [[MyLifeIsAverage|MLIA]]. Due to the high number of posted missions involving phone numbers, Mystery Seeker received enough complaints to remove phone numbers from the site. However, the developers are testing Mystery Missions Beta in order to allow the continuance of missions.

A number of phrases yield intentional responses ([[easter egg (media)|easter egg]]s).

In November 2009 Mystery Seeker had 440,000 unique visitors,<ref>[http://siteanalytics.compete.com/mysterygoogle.com/ mysterygoogle.com UVs for November 2012 | Compete]</ref> making it one of the most highly trafficked social entertainment sites online.

Google has not commented on any possible connection to the site.<ref name="Chivers 2009" /> The [[domain name]] ''www.mysterygoogle.com'' is registered to a private registrant {{As of|2009|10|2|lc=on}}.<ref>http://whois.domaintools.com/mysterygoogle.com</ref>

== References ==
{{Reflist|30em}}

== External links ==
* [http://www.mysteryseeker.com/ Mystery Seeker site]

[[Category:Websites]]
[[Category:Internet search]]
<=====doc_Id=====>:540
<=====title=====>:
Real-time web
<=====text=====>:
{{multiple issues|
{{more footnotes|date=November 2010}}
{{refimprove|date=November 2010}}
{{essay|date=April 2015}}
{{Buzzword|date=July 2011}}xxdxxx
}}

The '''real-time web''' is a network web using technologies and practices that enable users to receive information as soon as it is published by its authors, rather than requiring that they or their software check a source periodically for updates.

==Difference from real-time computing==
The real-time web is fundamentally different from [[real-time computing]] since there is no knowing when, or if, a response will be received. The information types transmitted this way are often short messages, status updates, news alerts, or links to longer documents. The content is often "soft" in that it is based on the [[social web]]—people's opinions, attitudes, thoughts, and interests—as opposed to hard news or facts.

==(Old) True-realtime web (an "alternate" model)==
From another point of view, the real-time web consists in making the client interface (or the web side; or the web layer) of a web application, to communicate continuously with the corresponding real-time server, during every user connection. As a fast pic of the client/server model, imagine each client object (each web module of the web [[GUI]] of an application) having its object class alive as a sub process (of its user session) in the server environment. In this scenario, the web is considered as the human entrance (interface) to the real-time environment: at each connected web URL, or Internet real-time zone, corresponds a different "front-end" web application. The real-time server acts as a [[logic network operating system]] for the programmable array of applications; handles the array of connected users for each application; attends for connections from real-world appliances and second level real-time servers. Applications behaviours and the intercommunication procedures between online services or applications, online users, and connected devices or appliances, are settled in the corresponding source code of each real-time service written in the real-time-interpreted programming language of the centric server.

As opposite to previous scenario, real-time web is exactly soft [[real-time computing]]: the round trip of a data ping-pong signal from the real-time server to the client must take about 1s (max) to be considered real-time and not to be annoying for humans (or users) during their connections.{{Citation needed|date=April 2016}} About the dispute between social web and real-time web, we can say real-time web is social by default and it is not true the contrary (WEB-r comes before Web 2.0). The WEB-r model is called [[true-realtime web]] to highlight the differences with the defective (de facto) model of real-time web generally perceived. From the industry point of view, this model of (general) real-time Internet can also be defined as [[electronic web]], that comes with the intrinsic meaning of not being limited to the web side of the Net, and with the direct reference to its server/rest-of-the-world perspective as a mechanism of a single clock.

==History==
Examples of real-time web are Facebook's newsfeed, and Twitter, implemented in social networking, search, and news sites. Benefits are said to include increased user engagement ("flow") and decreased server loads. In December 2009 real-time search facilities were added to [[Google Search]].<ref>{{cite web|url=http://googleblog.blogspot.com/2009/12/relevance-meets-real-time-web.html|title=Relevance meets the real-time web}}</ref>

The absolutely first realtime web implementation worldwide have been the WIMS true-realtime server and its web apps in 2001-2011 (WIMS = Web Interactive Management System); based on the WEB-r model of above; built in Java (serverside) and Adobe Flash (clientside). The true-realtime web model was born in 2000 at mc2labs.net by an Italian independent researcher.

==Real-time search==
A problem created by the rapid pace and huge volume of information created by real-time web technologies and practices is finding relevant information. One approach, known as '''real-time search''', is the concept of searching for and finding information online as it is produced. Advancements in web search technology coupled with growing use of [[social media]] enable online activities to be queried as they occur. A traditional [[web search]] [[Web crawler|crawls]] and [[Index (search engine)|indexes]] web pages periodically, returning results based on relevance to the search query. [[Google Real-Time Search]] was available in [[Google Search]] until July 2011.

==See also==
*[[Comet (programming)|Comet]]
*[[Collaborative real-time editor]]
*[[Firebase]]
*[[Internet of Things|Internet of Things (IoT)]]
*[[Meteor (web framework)|Meteor]]
*[[Microblogging]]
*[[Node.js]]
*[[Prospective search]]
*[[PubNub]]
*[[Push technology|Push Technology]]
*[[Scoopler]]
*[[Vert.x]]
*[https://www.syncano.io Syncano]

==References==
<references />

==External links==
*{{Cite news|url=https://www.theguardian.com/business/2009/may/19/google-twitter-partnership|title=Google 'falling behind Twitter'|last=Wray|first=Richard|date=19 May 2009|work=The Guardian|accessdate=17 June 2009}}
*{{Cite news|url=http://www.nytimes.com/2009/06/14/business/14digi.html|title=Hey, Just a Minute (or Why Google Isn't Twitter)|last=Stross|first=Randall|date=13 June 2009 |work=New York Times|accessdate=17 June 2009}}
*{{Cite news|url=http://online.wsj.com/article/BT-CO-20090615-712397.html |title=Internet Giants Look For Edge In Real-Time Search |last=Morrison |first=Scott |date=15 June 2009 |work=Wall Street Journal |accessdate=17 June 2009 |deadurl=yes |archiveurl=https://web.archive.org/web/20090616204058/http://online.wsj.com/article/BT-CO-20090615-712397.html |archivedate=16 June 2009 }} 
*{{Cite news|url=http://www.readwriteweb.com/archives/explaining_the_real-time_web_in_100_words_or_less.php|title=Explaining the Real-Time Web in 100 Words or Less|last=Kirkpatrick|first=Marshall|date=22 September 2009|work=ReadWriteWeb}}

{{Use dmy dates|date=October 2010}}

{{DEFAULTSORT:Real-Time Web}}
[[Category:Internet search]]
[[Category:Real-time web| ]]
<=====doc_Id=====>:543
<=====title=====>:
Hybrid search engine
<=====text=====>:
{{Notability|date=December 2009}}
A '''hybrid search engine''' ('''HSE''') is a type of [[computer]] [[search engine]] that uses different types of data with or without ontologies to produce the [[algorithm]]ically generated results based on [[web crawling]]. Previous types of search engines only use text to generate their results.
Hybrid search engines use a combination of both crawler-based results and directory results. More and more search engines these days are moving to a hybrid-based model.
==References==
{{No footnotes|date=April 2010}}
*http://eprints.ecs.soton.ac.uk/17457/
*http://eprints.whiterose.ac.uk/3771/
*http://www.picollator.com
*http://elocalfinder.com/HSearch.aspx

[[Category:Internet search]]


{{web-stub}}
<=====doc_Id=====>:546
<=====title=====>:
URL redirection
<=====text=====>:
{{Selfref|For redirection on Wikipedia, see [[Wikipedia:Redirect]].}}
{{Merge from|Rewrite engine|discuss=Talk:URL redirection#Merge Rewrite engine|date=November 2015}}
{{refimprove|date=December 2015}}
'''URL redirection''', also called '''URL forwarding''', is a [[World Wide Web]] technique for making a [[web page]] available under more than one [[Uniform Resource Locator|URL]] address. When a [[web browser]] attempts to open a URL that has been redirected, a page with a different URL is opened. Similarly, domain redirection or domain forwarding is when all pages in a URL [[Domain name|domain]] are redirected to a different domain, as when [http://www.wikipedia.com wikipedia.com] and [http://www.wikipedia.net wikipedia.net] are automatically redirected to [http://www.wikipedia.org wikipedia.org]. URL redirection is done for various reasons: for [[URL shortening]]; to prevent [[link rot|broken links]] when web pages are moved; to allow multiple domain names belonging to the same owner to refer to a single [[website|web site]]; to guide navigation into and out of a website; for privacy protection; and for less innocuous purposes such as [[phishing]] attacks.

== Purposes ==
There are several reasons to use URL redirection:

=== Similar domain names ===
A user might mistype a URL, for example, "example.com" and "exmaple.com". Organizations often register these "misspelled" domains and redirect them to the "correct" location: example.com. The addresses example.com and example.net could both redirect to a single domain, or web page, such as example.org. This technique is often used to "reserve" other [[top-level domain]]s (TLD) with the same name, or make it easier for a true ".edu" or ".net" to redirect to a more recognizable ".com" domain.

=== Moving pages to a new domain ===
Web pages may be redirected to a new domain for three reasons:
* a site might desire, or need, to change its domain name;
* an author might move his or her individual pages to a new domain;
* two web sites might merge.

With URL redirects, incoming links to an outdated URL can be sent to the correct location. These links might be from other sites that have not realized that there is a change or from bookmarks/favorites that users have saved in their browsers. The same applies to [[search engine]]s. They often have the older/outdated domain names and links in their database and will send search users to these old URLs. By using a "moved permanently" redirect to the new URL, visitors will still end up at the correct page. Also, in the next search engine pass, the search engine should detect and use the newer URL.

=== Logging outgoing links ===
The access logs of most web servers keep detailed information about where visitors came from and how they browsed the hosted site.  They do not, however, log which links visitors left by.  This is because the visitor's browser has no need to communicate with the original server when the visitor clicks on an outgoing link. This information can be captured in several ways.  One way involves URL redirection.  Instead of sending the visitor straight to the other site, links on the site can direct to a URL on the original website's domain that automatically redirects to the real target. This technique bears the downside of the delay caused by the additional request to the original website's server. As this added request will leave a trace in the server log, revealing exactly which link was followed, it can also be a privacy issue.<ref>
{{cite journal
  | title = Google revives redirect snoopery
  | journal = blog.anta.net
  | date = 2009-01-29
  | url = http://blog.anta.net/2009/01/29/509/
  | issn = 1797-1993
  | archiveurl=https://web.archive.org/web/20110817024348/http://blog.anta.net/2009/01/29/509/
  | archivedate=2011-08-17
}}</ref> The same technique is also used by some corporate websites to implement a statement that the subsequent content is at another site, and therefore not necessarily affiliated with the corporation. In such scenarios, displaying the warning causes an additional delay.

=== Short aliases for long URLs ===
{{Main article|URL shortening}}

Web applications often include lengthy descriptive attributes in their URLs which represent data hierarchies, command structures, transaction paths and session information. This practice results in a URL that is aesthetically unpleasant and difficult to remember, and which may not fit within the size limitations of [[microblogging]] sites. [[URL shortening]] services provide a solution to this problem by redirecting a user to a longer URL from a shorter one.

=== Meaningful, persistent aliases for long or changing URLs ===
{{See also|Permalink|PURL|Link rot}}

Sometimes the URL of a page changes even though the content stays the same. Therefore, URL redirection can help users who have bookmarks. This is routinely done on Wikipedia whenever a page is renamed.

=== Post/Redirect/Get ===
{{Main article|Post/Redirect/Get}}

Post/Redirect/Get (PRG) is a [[web development]] [[design pattern]] that prevents some duplicate [[form (web)|form]] submissions, creating a more intuitive interface for [[user agent]]s (users).

=== Device targeting and geotargeting ===

Redirects can be effectively used for targeting purposes like [[device targeting]] or [[geotargeting]]. Device targeting has become increasingly important with the rise of mobile clients. There are two approaches to serve mobile users: Make the website [[responsive web design|responsive]] or redirect to a mobile website version. If a mobile website version is offered, users with mobile clients will be automatically forwarded to the corresponding mobile content. For device targeting, client side redirects or non-cacheable server side redirects are used. Geotargeting is the approach to offer localized content and automatically forward the user to a localized version of the requested URL. This is helpful for websites that target audience in more than one location and/or language. Usually server side redirects are used for Geotargeting but client side redirects might be an option as well, depending on requirements.<ref>{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects & SEO - The Total Guide |accessdate=2015-11-29 |publisher=Audisto}}</ref>

=== Manipulating search engines ===
Redirects have been used to manipulate search engines with unethical intentions, e.g. [[sneaky redirects]] or [[URL hijacking]]. The goal of misleading redirects is to drive search traffic to landing pages, which do not have enough ranking power on their own or which are only remotely or not at all related to the search target. The approach requires a rank for a range of search terms with a number of URLs that would utilize sneaky redirects to forward the searcher to the target page. This method had a revival with the uprise of mobile devices and device targeting. URL hijacking is an off-domain redirect technique<ref>{{cite web|url=https://www.mattcutts.com/blog/seo-advice-discussing-302-redirects/ |title=SEO advice: discussing 302 redirects |date=4 January 2006 |publisher=Matt Cutts, former Head of Google Webspam Team}}</ref> that exploited the nature of the search engine's handling for temporary redirects. If a temporary redirect is encountered, search engines have to decide whether they assign the ranking value to the URL that initializes the redirect or to the redirect target URL. The URL that initiates the redirect may be kept to show up in search results, as the redirect indicates a temporary nature. Under certain circumstances it was possible to exploit this behaviour by applying temporary redirects to well ranking URLs, leading to a replacement of the original URL in search results by the URL that initialized the redirect, therefore "stealing" the ranking. This method was usually combined with sneaky redirects to re-target the user stream from the search results to a target page. Search engines have developed efficient technologies to detect these kind of manipulative approaches. Major search engines usually apply harsh ranking penalties on sites that get caught applying techniques like these.<ref>{{cite web|url=https://support.google.com/webmasters/answer/2721217?hl=en |title=Sneaky Redirects |date=3 December 2015 |publisher=Google Webmaster Guidelines}}</ref>

=== Manipulating visitors ===
URL redirection is sometimes used as a part of [[phishing]] attacks that confuse visitors about which web site they are visiting.<ref>{{cite web|url=https://www.owasp.org/index.php/Unvalidated_Redirects_and_Forwards_Cheat_Sheet |title=Unvalidated Redirects and Forwards Cheat Sheet |date=21 August 2014 |publisher=Open Web Application Security Project (OWASP)}}</ref> Because modern browsers always show the real URL in the address bar, the threat is lessened. However, redirects can also take you to sites that will otherwise attempt to attack in other ways. For example, a redirect might take a user to a site that would attempt to trick them into downloading antivirus software and, ironically, installing a [[trojan horse (computing)|trojan]] of some sort instead.

=== Removing <code>referer</code> information ===
When a link is clicked, the browser sends along in the [[HTTP request]] a field called [[HTTP referer|referer]] which indicates the source of the link. This field is populated with the URL of the current web page, and will end up in the [[server log|logs]] of the server serving the external link. Since sensitive pages may have sensitive URLs (for example, <code><nowiki>http://company.com/plans-for-the-next-release-of-our-product</nowiki></code>), it is not desirable for the <code>referer</code> URL to leave the organization. A redirection page that performs [[Referer#Referrer hiding|referrer hiding]] could be embedded in all external URLs, transforming for example <code><nowiki>http://externalsite.com/page</nowiki></code> into <code><nowiki>http://redirect.company.com/http://externalsite.com/page</nowiki></code>. This technique also eliminates other potentially sensitive information from the referer URL, such as the [[session ID]], and can reduce the chance of [[phishing]] by indicating to the end user that they passed a clear gateway to another site.

== Implementation ==
Several different kinds of response to the browser will result in a redirection.  These vary in whether they affect [[HTTP headers]] or HTML content.  The techniques used typically depend on the role of the person implementing it and their access to different parts of the system.  For example, a web author with no control over the headers might use a [[meta refresh|Refresh meta tag]] whereas a web server administrator redirecting all pages on a site is more likely to use server configuration.

=== Manual redirect ===
The simplest technique is to ask the visitor to follow a link to the new page, usually using an HTML anchor like:

<source lang="html4strict">
Please follow <a href="http://www.example.com/">this link</a>.
</source>

This method is often used as a fall-back&nbsp;— if the browser does not support the automatic redirect, the visitor can still reach the target document by following the link.

=== HTTP status codes 3xx ===
In the [[HTTP]] [[Protocol (computing)|protocol]] used by the [[World Wide Web]], a '''redirect''' is a response with a [[List of HTTP status codes|status code]] beginning with ''3'' that causes a browser to display a different page. If a client encounters a redirect, it needs to make a number of decisions how to handle the redirect. Different status codes are used by clients to understand the purpose of the redirect, how to handle caching and which request method to use for the subsequent request.

HTTP/1.1 defines several status codes for redirection (RFC 7231):
* [[HTTP 300|300 multiple choices]] (e.g. offer different languages)
* [[HTTP 301|301 moved permanently]]
* [[HTTP 302|302 found]] (originally "temporary redirect" in HTTP/1.0 and popularly used for CGI scripts; superseded by 303 and 307 in HTTP/1.1 but preserved for backward compatibility)
* [[HTTP 303|303 see other]] (forces a GET request to the new URL even if original request was POST)
* [[HTTP 307|307 temporary redirect]] (provides a new URL for the browser to resubmit a GET or POST request)
* [[HTTP 308|308 permanent redirect]] (provides a new URL for the browser to resubmit a GET or POST request)

==== Redirect status codes and characteristics ====

{| class="wikitable"
|-
! HTTP Status Code !! HTTP Version !! Temporary / Permanent !! Cacheable !! Request Method Subsequent Request
|-
| 301 || HTTP/1.0 || Permanent || yes || GET / POST may change
|-
| 302 || HTTP/1.0 || Temporary || not by default || GET / POST may change
|-
| 303 || HTTP/1.1 || Temporary || never || always GET
|-
| 307 || HTTP/1.1 || Temporary || not by default || may not change
|-
| 308 || HTTP/1.1 || Permanent || by default || may not change
|-
|}<ref>{{Cite web |url=https://audisto.com/insights/guides/31/ |title=Redirects & SEO - The Complete Guide |accessdate=2015-11-29 |publisher=Audisto}}</ref>

All of these status codes require the URL of the redirect target to be given in the Location: header of the HTTP response. The 300 multiple choices will usually list all choices in the body of the message and show the default choice in the Location: header.

(Status codes [[HTTP 304|304 not modified]] and [[HTTP 305|305 use proxy]] are not redirects).

==== Example HTTP response for a 301 redirect ====

A [[HTTP]] response with the 301 "moved permanently" redirect looks like this:

<syntaxhighlight lang="http">
HTTP/1.1 301 Moved Permanently
Location: http://www.example.org/
Content-Type: text/html
Content-Length: 174

<html>
<head>
<title>Moved</title>
</head>
<body>
<h1>Moved</h1>
<p>This page has moved to <a href="http://www.example.org/">http://www.example.org/</a>.</p>
</body>
</html>
</syntaxhighlight>

==== Using server-side scripting for redirection ====
Web authors producing HTML content can't usually create redirects using HTTP headers as these are generated automatically by the web server program when serving an HTML file.  The same is usually true even for programmers writing CGI scripts, though some servers allow scripts to add custom headers (e.g. by enabling "non-parsed-headers").  Many web servers will generate a 3xx status code if a script outputs a "Location:" header line.  For example, in [[PHP]], one can use the "header" function:

<source lang="php">
header('HTTP/1.1 301 Moved Permanently');
header('Location: http://www.example.com/');
exit();
</source>

More headers may be required to prevent caching.<ref name="php-301-robust-solution">{{cite web|url=http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution/ |title=PHP Redirects: 302 to 301 Rock Solid Robust Solution |publisher=WebSiteFactors.co.uk |archiveurl=https://web.archive.org/web/20121012042703/http://www.websitefactors.co.uk/php/2011/05/php-redirects-302-to-301-rock-solid-solution |archivedate=2012-10-12}}</ref> The programmer must ensure that the headers are output before the body.  This may not fit easily with the natural flow of control through the code.  To help with this, some frameworks for server-side content generation can buffer the body data.  In the [[Active Server Pages|ASP scripting]] language, this can also be accomplished using <code>response.buffer=true</code> and <code>response.redirect <nowiki>"http://www.example.com/"</nowiki></code> HTTP/1.1 allows for either a relative URI reference or an absolute URI reference.<ref>{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 7.1.2 | sectionname = Location | page = 68 | editor1 = Roy T. Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}</ref> If the URI reference is relative the client computes the required absolute URI reference according to the rules defined in RFC 3986.<ref>{{cite IETF | title = Uniform Resource Identifier (URI): Generic Syntax | rfc = 3986 | section = 5 | sectionname = Reference Resolution | page = 28 | first1 = Tim | last1 = Berners-Lee | author1-link = Tim Berners-Lee | first2 = Roy T. | last2 = Fielding | author2-link = Roy Fielding | first3 = Larry | last3 = Masinter | year = 2005 | month = January | publisher = [[Internet Engineering Task Force|IETF]]}}</ref>

==== Apache mod_rewrite ====
The [[Apache HTTP Server]] mod_alias extension can be used to redirect certain requests. Typical configuration directives look like:
<source lang="apache">
Redirect permanent /oldpage.html http://www.example.com/newpage.html
Redirect 301 /oldpage.html http://www.example.com/newpage.html
</source>

For more flexible [[URL rewriting]] and redirection, Apache mod_rewrite can be used. E.g., to redirect a requests to a canonical domain name:
<source lang="apache">
RewriteEngine on
RewriteCond %{HTTP_HOST} ^([^.:]+\.)*oldsite\.example\.com\.?(:[0-9]*)?$ [NC]
RewriteRule ^(.*)$ http://newsite.example.net/$1 [R=301,L]
</source>

Such configuration can be applied to one or all sites on the server through the server configuration files or to a single content directory through a <code>[[.htaccess]]</code> file.

==== nginx rewrite ====
[[Nginx]] has an integrated http rewrite module,<ref>{{cite web|url=http://nginx.org/r/rewrite |title=Module ngx_http_rewrite_module - rewrite |publisher=nginx.org |date= |accessdate=24 December 2014}}</ref> which can be used to perform advanced URL processing and even web-page generation (with the <tt>return</tt> directive).  A showing example of such advanced use of the rewrite module is [http://mdoc.su/ mdoc.su], which implements a deterministic [[URL shortening]] service entirely with the help of nginx configuration language alone.<ref>{{cite mailing list |date=18 February 2013 |url=http://mailman.nginx.org/pipermail/nginx/2013-February/037592.html |mailinglist=nginx@nginx.org |title=A dynamic web-site written wholly in nginx.conf? Introducing mdoc.su! |first=Constantine A. |last=Murenin |accessdate=24 December 2014}}</ref><ref>{{cite web |url=http://mdoc.su/ |title=mdoc.su — Short manual page URLs for FreeBSD, OpenBSD, NetBSD and DragonFly BSD |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>

For example, if a request for [http://mdoc.su/DragonFlyBSD/HAMMER.5 <tt>/DragonFlyBSD/HAMMER.5</tt>] were to come along, it would first be redirected internally to <tt>/d/HAMMER.5</tt> with the first rewrite directive below (only affecting the internal state, without any HTTP replies issued to the client just yet), and then with the second rewrite directive, an [[HTTP response]] with a [[HTTP 302|302 Found status code]] would be issued to the client to actually redirect to the external [[Common Gateway Interface|cgi script]] of web-[[man page|man]]:<ref>{{cite web |url=http://nginx.conf.mdoc.su/mdoc.su.nginx.conf |title=mdoc.su.nginx.conf |first=Constantine A. |last=Murenin |date=23 February 2013 |accessdate=25 December 2014}}</ref>
<source lang="nginx">
	location /DragonFly {
		rewrite	^/DragonFly(BSD)?([,/].*)?$	/d$2	last;
	}
	location /d {
		set	$db	"http://leaf.dragonflybsd.org/cgi/web-man?command=";
		set	$ds	"&section=";
		rewrite	^/./([^/]+)\.([1-9])$		$db$1$ds$2	redirect;
	}
</source>

=== Refresh Meta tag and HTTP refresh header ===
[[Netscape]] introduced the [[meta refresh]] feature which refreshes a page after a certain amount of time.  This can specify a new URL to replace one page with another.  This is supported by most web browsers.<ref>[http://www.w3schools.com/tags/tag_meta.asp HTML <meta> tag]</ref><ref>[http://web.archive.org/web/20020802170847/http://wp.netscape.com/assist/net_sites/pushpull.html An exploration of dynamic documents]</ref> A timeout of zero seconds effects an immediate redirect. This is treated like a 301 permanent redirect by Google, allowing transfer of PageRank to the target page.<ref>[http://sebastians-pamphlets.com/google-and-yahoo-treat-undelayed-meta-refresh-as-301-redirect/ "Google and Yahoo accept undelayed meta refreshs as 301 redirects"]. Sebastian's Pamphlets. 3 September 2007.</ref>

This is an example of a simple HTML document that uses this technique:
<source lang="html4strict">
<html>
<head>
<meta http-equiv="Refresh" content="0; url=http://www.example.com/" />
</head>
<body>
<p>Please follow <a href="http://www.example.com/">this link</a>.</p>
</body>
</html>
</source>

This technique can be used by [[Web designer|web authors]] because the meta tag is contained inside the document itself.  The meta tag must be placed in the "head" section of the HTML file.  The number "0" in this example may be replaced by another number to achieve a delay of that many seconds.  The anchor in the "body" section is for users whose browsers do not support this feature.

The same effect can be achieved with an HTTP <code>refresh</code> header:
<source lang="http">
HTTP/1.1 200 ok
Refresh: 0; url=http://www.example.com/
Content-type: text/html
Content-length: 78

Please follow <a href="http://www.example.com/">this link</a>.
</source>

This response is easier to generate by CGI programs because one does not need to change the default status code.

Here is a simple CGI program that effects this redirect:
<source lang="perl">
#!/usr/bin/perl
print "Refresh: 0; url=http://www.example.com/\r\n";
print "Content-type: text/html\r\n";
print "\r\n";
print "Please follow <a href=\"http://www.example.com/\">this link</a>!"
</source>

Note: Usually, the HTTP server adds the status line and the Content-length header automatically.

The [[World Wide Web Consortium|W3C]] discourage the use of meta refresh, since it does not communicate any information about either the original or new resource, to the browser (or [[search engine]]). The W3C's [http://www.w3.org/TR/WAI-WEBCONTENT/#tech-no-periodic-refresh Web Content Accessibility Guidelines (7.4)] discourage the creation of auto-refreshing pages, since most web browsers do not allow the user to disable or control the refresh rate.  Some articles that they have written on the issue include [http://www.w3.org/TR/WAI-WEBCONTENT/#gl-movement W3C Web Content Accessibility Guidelines (1.0): Ensure user control of time-sensitive content changes], [http://www.w3.org/QA/Tips/reback Use standard redirects: don't break the back button!] and [http://www.w3.org/TR/WCAG10-CORE-TECHS/#auto-page-refresh Core Techniques for Web Content Accessibility Guidelines 1.0 section 7].

=== JavaScript redirects ===
[[JavaScript]] can cause a redirect by setting the <code>window.location</code> attribute, e.g.:
<syntaxhighlight lang="ecmascript">
window.location='http://www.example.com/'
</syntaxhighlight>
Normally JavaScript pushes the redirector site's [[URL]] to the browser's history. It can cause redirect loops when users hit the back button. With the following command you can prevent this type of behaviour.<ref>{{cite web|url=http://insider.zone/tools/client-side-url-redirect-generator/|title=Cross-browser client side URL redirect generator|publisher=Insider Zone}}</ref>
<syntaxhighlight lang="ecmascript">
window.location.replace('http://www.example.com/')
</syntaxhighlight>
However, HTTP headers or the refresh meta tag may be preferred for security reasons and because JavaScript will not be executed by some browsers and many [[web crawler]]s.

=== Frame redirects ===

A slightly different effect can be achieved by creating an inline frame:

<source lang="html4strict">
<iframe height="100%" width="100%" src="http://www.example.com/">
Please follow <a href="http://www.example.com/">link</a>.
</iframe>
</source>

One main difference to the above redirect methods is that for a frame redirect, the browser displays the URL of the frame document and not the URL of the target page in the URL bar. This ''cloaking'' technique may be used so that the reader sees a more memorable URL or to fraudulently conceal a [[phishing]] site as part of [[website spoofing]].<ref>Aaron Emigh (19 January 2005). [http://www.sfbay-infragard.org/Documents/phishing-sfectf-report.pdf "Anti-Phishing Technology"] (PDF). Radix Labs.</ref>

Before HTML5,<ref>
https://www.w3.org/TR/html5/obsolete.html</ref> the same effect could be done with an [[Framing (World Wide Web)|HTML frame]] that contains the target page:
<source lang="html4strict">
<frameset rows="100%">
  <frame src="http://www.example.com/">
  <noframes>
    <body>Please follow <a href="http://www.example.com/">link</a>.</body>
  </noframes>
</frameset>
</source>

=== Redirect chains ===
One redirect may lead to another. For example, the URL [http://www.wikipedia.com/wiki/URL_redirection http://www.wikipedia'''.com'''/wiki/URL_redirection] (with [[domain name]] in [[.com]]) is first redirected to [[:www:URL redirection|http://www.wikipedia'''.org'''/wiki/URL redirection]] (with domain name in [[.org]]), then to the [[HTTPS]] URL [[:www:URL redirection|'''https:'''//www.wikipedia.org/wiki/URL redirection]] and finally to the language-specific site https://'''en'''.wikipedia.org/wiki/URL redirection. This is unavoidable if the different links in the chain are served by different servers though it should be minimised by ''[[rewriting]]'' the URL as much as possible on the server before returning it to the browser as a redirect.

=== Redirect loops ===
Sometimes a mistake can cause a page to end up redirecting back to itself, possibly via other pages, leading to an infinite sequence of redirects. Browsers should stop redirecting after a certain number of hops and display an error message.

The HTTP/1.1 Standard states:<ref name="rfc7231sec6.4">{{cite IETF | title = Hypertext Transfer Protocol (HTTP/1.1): Semantics and Content | rfc = 7231 | section = 6.4 | sectionname = Redirection 3xx | page = 54 | editor1 = Roy T. Fielding | editor1-link = Roy Fielding | editor2 = Julian F. Reschke | year = 2014 | month = June | publisher = [[Internet Engineering Task Force|IETF]]}}</ref>
<blockquote>
A client ''SHOULD'' detect and intervene in cyclical redirections (i.e., "infinite" redirection loops).

Note: An earlier version of this specification recommended a maximum of five redirections ([RFC 2068], Section 10.3).  Content developers need to be aware that some clients might implement such a fixed limitation.
</blockquote>
Note that the URLs in the sequence might not repeat, e.g.: http://www.example.com/1 -> http://www.example.com/2 -> http://www.example.com/3 ...

== Services ==
There exist services that can perform URL redirection on demand, with no need for technical work or access to the web server your site is hosted on.

=== URL redirection services ===
A '''redirect service''' is an information management system, which provides an internet link that redirects users to the desired content. The typical benefit to the user is the use of a memorable domain name, and a reduction in the length of the URL or web address. A redirecting link can also be used as a permanent address for content that frequently changes hosts, similarly to the [[Domain Name System]]. Hyperlinks involving URL redirection services are frequently used in spam messages directed at blogs and wikis.  Thus, one way to reduce spam is to reject all edits and comments containing hyperlinks to known URL redirection services; however, this will also remove legitimate edits and comments and may not be an effective method to reduce spam.
Recently, URL redirection services have taken to using [[AJAX]] as an efficient, user friendly method for creating shortened URLs. A major drawback of some URL redirection services is the use of delay pages, or frame based advertising, to generate revenue.

==== History ====
The first redirect services took advantage of [[top-level domains]] (TLD) such as "[[.to]]" (Tonga), "[[.at]]" (Austria) and "[[.is]]" (Iceland). Their goal was to make memorable URLs. The first mainstream redirect service was V3.com that boasted 4 million users at its peak in 2000.  V3.com success was attributed to having a wide variety of short memorable domains including "r.im", "go.to", "i.am", "come.to" and "start.at".  V3.com was acquired by FortuneCity.com, a large free web hosting company, in early 1999.<ref>{{cite news| url=http://news.bbc.co.uk/2/hi/technology/6991719.stm | work=BBC News | title=Net gains for tiny Pacific nation | date=2007-09-14 | accessdate=2010-05-27}}</ref> As the sales price of top level domains started falling from $70.00 per year to less than $10.00, use of redirection services declined. With the launch of [[TinyURL]] in 2002 a new kind of redirecting service was born, namely [[URL shortening]]. Their goal was to make long URLs short, to be able to post them on internet forums. Since 2006, with the 140 character limit on the extremely popular [[Twitter]] service, these short URL services have been heavily used.

=== Referrer masking ===
Redirection services can hide the [[referrer]] by placing an intermediate page between the page the link is on and its destination. Although these are conceptually similar to other URL redirection services, they serve a different purpose, and they rarely attempt to shorten or obfuscate the destination URL (as their only intended side-effect is to hide referrer information and provide a clear gateway between other websites.) This type of redirection is often used to prevent potentially-malicious links from gaining information using the referrer, for example a [[session ID]] in the query string. Many large community websites use link redirection on external links to lessen the chance of an exploit that could be used to steal account information, as well as make it clear when a user is leaving a service, to lessen the chance of effective [[phishing]]  .

Here is a simplistic example of such a service, written in [[PHP]].
<source lang="html+php">
<?php
$url = htmlspecialchars($_GET['url']);
header( 'Refresh: 0; url=http://'.$url );
?>
<!-- Fallback using meta refresh. -->
<html>
 <head>
  <title>Redirecting...</title>
  <meta http-equiv="refresh" content="0;url=http://<?php echo $url; ?>">
 </head>
 <body>
 Attempting to redirect to <a href="http://<?php echo $url; ?>">http://<?php echo $url; ?></a>.
 </body>
</html>
</source>

The above example does not check who called it (e.g. by referrer, although that could be spoofed).  Also, it does not check the url provided.  This means that a malicious person could link to the redirection page using a url parameter of his/her own selection, from any page, which uses the web server's resources.

==Security issues==
URL redirection can be abused by attackers for [[phishing]] attacks, such as [[Open Redirect|open redirect]] and [[Covert Redirect|covert redirect]]. "An open redirect is an application that takes a parameter and redirects a user to the parameter value without any validation."<ref name="Open_Redirect">{{cite web | url=https://www.owasp.org/index.php/Open_redirect | title=Open Redirect |publisher= OWASP |date=16 March 2014 | accessdate=21 December 2014}}</ref> "Covert redirect is an application that takes a parameter and redirects a user to the parameter value WITHOUT SUFFICIENT validation."<ref name="Covert_Redirect">{{cite web | url=http://tetraph.com/covert_redirect/ | title=Covert Redirect |publisher= Tetraph |date=1 May 2014 | accessdate=21 December 2014}}</ref> It was disclosed in May 2014 by a mathematical doctoral student Wang Jing from Nanyang Technological University, Singapore.<ref name="CNET">{{cite web | url=http://www.cnet.com/news/serious-security-flaw-in-oauth-and-openid-discovered/ | title=Serious security flaw in OAuth, OpenID discovered |publisher= CNET |date=2 May 2014 | accessdate=21 December 2014}}</ref>

==See also==
* [[Link rot]]
* [[Canonical link element]]
* [[Canonical meta tag]]
* [[Domain masking]]
* [[URL normalization]]
* [[Semantic URL]]

==References==
{{Reflist}}

==External links==
* [http://httpd.apache.org/docs/1.3/urlmapping.html Mapping URLs to Filesystem Locations]
* [http://www.cs.ucdavis.edu/~hchen/paper/www07.pdf Paper on redirection spam (UC Davis)] (403 Forbidden link)
* [http://projects.webappsec.org/URL-Redirector-Abuse Security vulnerabilities in URL Redirectors] The Web Application Security Consortium Threat Classification

{{Spamming}}

{{Use dmy dates|date=November 2010}}

{{DEFAULTSORT:Url Redirection}}
[[Category:Uniform Resource Locator]]
[[Category:Black hat search engine optimization]]
[[Category:Internet search]]
[[Category:Internet terminology]]
<=====doc_Id=====>:549
<=====title=====>:
Figaro Systems
<=====text=====>:
{{Infobox company
|name = Figaro Systems, Inc.
|logo = [[Image:Figaro-logo.png|Figaro logo]]
|type = [[Privately held company|Private]]
|foundation = 1993
|location_city = [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]]
|location_country =[[United States]]
|key_people = Patrick Markle, [[president]] and [[CEO]], [[Geoff Webb]], [[vice president|VP]]
|homepage = [http://www.figarosystems.com figarosystems.com]
}}

'''Figaro Systems, Inc.''' is an American company that provides  seatback and [[wireless]] titling [[software]] and system installations to [[opera houses]] and other music performance venues worldwide. The company is based in [[Santa Fe, New Mexico|Santa Fe]], New Mexico. It was established in 1993 <ref>Andrew Webb, “Opera Subtitle Firm Eyes New Game,” ''New Mexico Business Weekly'', Nov. 21, 2003 [http://www.bizjournals.com/albuquerque/stories/2003/11/24/story2.html]</ref>
by Patrick Markle, [[Geoff Webb]], and Ron Erkman  <ref name="figaro-systems.com"/> and was the first company to provide [[assistive technology]] that enables individualized, simultaneous, multi-lingual [[dialogue]] and [[libretto]]-reading for audiences.
<ref>[http://www.highbeam.com/DocPrint.aspx?DocID=1P2:115622912 David Belcher, “Nothing Lost in Translation: [[Video]] system allows patrons to read words on chair backs,”] ''Albuquerque Journal'', June 4, 2006</ref>

==History==
Figaro Systems grew out of a conversation in 1992 among three opera colleagues: Patrick Markle, at that time Production Director of The [[Santa Fe Opera]], Geoffrey Webb, Design Engineer for the [[Metropolitan Opera House (Lincoln Center)|Metropolitan Opera House]] in New York, and Ronald Erkman, then a technician for the Met. At that time, opera houses had two options for the display of libretto and dialogue subtitles: projection onto a large screen above the stage or onto smaller screens throughout the theatre. Typically, the translation was in a single language.<ref>[http://www.bizjournals.com/albuquerque/stories/2005/04/11/story5.html?q=Figaro%20Systems Dennis Domrzalski, "Figaro: Eyes translate when ears don't get it",] ''New Mexico Business Weekly'', April 8, 2005</ref>

The [[Americans with Disabilities Act of 1990]] had recently been enacted; Markle was trying to solve the problem of venues which lacked accessibility to patrons with disabilities, including the profoundly [[deaf]].  Markle, Webb, and Erkman devised the first [[prototype]] of a personal seatback titling device and [[John Crosby (conductor)|John Crosby]], then General Director of The [[Santa Fe Opera]], saw its potential for opera patrons.<ref name="figaro-systems.com">[http://www.figaro-systems.com/about.php  Figaro Systems Official Website]</ref> Markle, Webb, and Erkman were further reinforced by their understanding of technology’s role in remediating the physical barriers people encounter, worldwide, which frustrate or prevent their access to the visual performing arts.<ref>[http://figarosystems.com/linkdownloads/052007_figaro_auditoria_article.pdf “[[User-friendly]] art: In-seat text displays that subtitle and translate”, ''Auditoria'', May 2007]</ref> Markle, Webb, and Erkman applied for and were granted [[patent]]s for their invention.
<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=11&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 5,739,869, "Electronic libretto display apparatus and method," issued April 14, 1998. [[United States Patent and Trademark Office]] ]</ref><ref>[http://www.lanl.gov/orgs/pa/News/050701.html  Los Alamos Laboratory, ''Daily News Bulletin'', May 7, 2001]</ref>

Philanthropist and investor [[Alberto Vilar]] counted Figaro Systems among the companies in which he was a majority shareholder.<ref>[http://nymag.com/nymetro/arts/music/features/5616/ [[Robert Hilferty]], "A Knight at the Opera," ''[[New York Magazine]]'', January 14, 2002]</ref><ref>[http://biography.jrank.org/pages/3490/Vilar-Alberto-1940-Investor-Philanthropist-Privileges-Wealth.html  "Alberto Vilar: The Privileges of Wealth," ''The Free Encyclopedia'']</ref>  He donated the company's [[electronic libretto]] system to European venues including the [[Royal Opera House]] in [[London]], La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]]. As a consequence of his failures to pay promised donations, most of these companies lost money.

In 2005 the Met charged the New Mexico company with unlawfully using its name in advertising promoting its "Simultext, system which defendant claims can display a simultaneous translation of an opera as it occurs on a stage and that defendant represented that its system is installed at the Met." <ref>[http://classactionlitigation.com/library/consumerlaw2006update.html#_edn173#_edn173 Timothy E. Eble, ''Class Action Litigation Information''] on classactionlitigation.com</ref>

==Products and technology==
The company’s products are known variously as seat back titles, [[surtitles]],
<ref>[http://app1.kuhf.org/houston_public_radio-news-display.php?articles_id=20614 Eric Skelly, "Surtitles at the Opera," ''Public Radio News and Information in Houston, Texas'', KUHF 88.7 FM Houston Public Radio] on app1.kuhf.org/</ref> [[electronic libretto]] systems, opera supertitles, projected titles, and libretto translations.

Opera venues have utilized the system to display librettos in [[English language|English]], [[French language|French]], [[German language|German]], [[Italian language|Italian]], [[Japanese language|Japanese]], [[Mandarin Chinese|Mandarin]], [[Russian language|Russian]], and [[Spanish language|Spanish]]
<ref>[http://www.sandia.gov/news-center/news-releases/2005/tech-trans/smbusiness.html "Sandia helps 278 state businesses in 2004 through New Mexico Small Business Assistance Program," Sandia National Laboratories, Sandia Corporation, March 22, 2005] on sandia.gov</ref> although the software enables the reading of the libretto in any [[written language]].
<ref name="entertanmentengineering.com">[http://www.entertanmentengineering.com/v4.issue04/page.06.html  “Giving the Opera a New Voice,”] ''Entertainment Engineering," Volume 4, Issue 2, p. 6</ref> Translation is provided by one screen and delivery system per person.<ref>[http://www.figarosystems.com  Figaro Systems Official Website]</ref>

Typically, but not in all cases, the system is permanently installed along the backs of rows of seats. Each screen is positioned so that the text is clearly visible to each user. The displays were initially available in [[vacuum fluorescent display]], ([[Vacuum fluorescent display|VFD]]) and, in 2000, [[liquid crystal display]], ([[LCD]]) was used. In 2004 the displays became available with [[organic light-emitting diode]], ([[OLED]]) screens.  Each type of display provides the same text information and program annotation on eight channels simultaneously, may be turned off by the user, and is user-operated with a single button. The software is capable of supporting venues’ existing systems as well as Figaro Systems' "Simultext" system. The software enables cueing of each line as it is sung, and it appears instantly on the screen.<ref name="entertanmentengineering.com"/>

The company builds fully [[modular]] systems including its [[wireless]] [[handheld]] screens 
<ref>[http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=figaro.ASNM.&OS=AN/figaro&RS=AN/Figaro  United States Patent 6,760,010. "Wireless electronic libretto display apparatus and method," issued July 6, 2004:] United States Patent and Trademark Office Patent Full-Text and Image Database</ref> for users who cannot use seatback systems, for example people in [[wheelchair]]s, who may be viewing the opera in areas lacking seatback viewing, or people with compromised eyesight.

==Venues==
In the US, the company’s systems are in use in the [[Ellie Caulkins Opera House]] 
<ref>[http://www.highbeam.com/doc/1G1-135788390.html Marc Shulgold, "Opera dialogue shows on seat in front of you,"] ''Rocky Mountain News'' (Denver, Colorado), September 3, 2005 on highbeam.com,</ref> in [[Denver, Colorado|Denver]], Colorado, The Santa Fe Opera in Santa Fe,<ref>[https://web.archive.org/web/20080512022822/http://www.santafeopera.org/yournite/operatitles.php  Santa Fe Opera, Santa Fe, NM. Cached webpage],</ref> the [[Brooklyn Academy of Music]]<ref>[http://www.appliancemagazine.com/editorial.php?article=1768&zone=210&first=1  “An Operatic Performance,” ''Appliance Magazine'', June 2007],</ref> the [[Metropolitan Opera]], New York, where it is called "MetTitles"),<ref>[http://www.figaro-systems.com/installations.php  Figaro Systems Official Website. Installations],</ref> the [[Roy E. Disney]] Theatre in [[Albuquerque]]'s [[National Hispanic Cultural Center]], [[McCaw Hall]] in [[Seattle Washington]], the [[Opera Theatre of St. Louis]] in St. Louis, Missouri, the [[Des Moines Metro Opera]] in [[Des Moines, Iowa|Des Moines]], Iowa and the Lyric Opera of Kansas City,  Missouri.<ref name="figaro-systems.com"/>

In the UK and Europe, the systems have been installed in venues including the [[Royal Opera House]] in London, the [[Teatro alla Scala]] and La Scala's [[Teatro degli Arcimboldi]] opera houses in [[Milan, Italy|Milan]], Italy, the [[Gran Teatre del Liceu]] in [[Barcelona, Spain|Barcelona]], Spain, and the [[Wiener Staatsoper]] in [[Wien]], [[Austria]].
<ref>[http://www.entertainmentengineering.com/v4.issue04/page.06.html “Giving the Opera a New Voice,” ''Entertainment Engineering.'', Volume 4, Issue 2, p. 6], on entertainmentengineering.com</ref>

==Awards==
In 2001, the company won the [[Los Alamos, New Mexico|Los Alamos]] Laboratories’ Technology Commercialization Award for its Simultext system.<ref>[http://www.lanl.gov/news/index.php/fuseaction/home.story/story_id/1170 Todd Hanson, "Los Alamos announces technology commercialization awards," ''Los Alamos National Laboratory News''], Los Alamos National Security, LLC, US Department of Energy's NNSA, May 7, 2001 on lanl.gov/news.</ref>
In 2008, the company’s software was one of four finalists for the Excellence Award for Commercial Software awarded by the New Mexico Information Technology and Software Association.

==References==
{{Reflist}}

[[Category:Assistive technology]]
[[Category:Companies based in Santa Fe, New Mexico]]
[[Category:Companies established in 1993]]
[[Category:Educational technology companies]]
[[Category:Information retrieval organizations]]
[[Category:Privately held companies based in New Mexico]]
[[Category:Software companies based in New Mexico]]
<=====doc_Id=====>:552
<=====title=====>:
Text Retrieval Conference
<=====text=====>:
{{Other uses of|TREC|TREC (disambiguation)}}
The '''Text REtrieval Conference''' ('''TREC''') is an ongoing series of [[workshop]]s focusing on a list of different [[information retrieval]] (IR) research areas, or ''tracks.'' It is co-sponsored by the [[National Institute of Standards and Technology]] (NIST) and the [[Intelligence Advanced Research Projects Activity]] (part of the office of the [[Director of National Intelligence]]), and began in 1992 as part of the [[DARPA TIPSTER Program|TIPSTER Text program]]. Its purpose is to support and encourage research within the information retrieval community by providing the infrastructure necessary for large-scale ''evaluation'' of [[text retrieval]] methodologies and to increase the speed of lab-to-product [[technology transfer|transfer of technology]].

Each track has a challenge wherein NIST provides participating groups with data sets and test problems. Depending on track, test problems might be questions, topics, or target extractable [[Features (pattern recognition)|features]]. Uniform scoring is performed so the systems can be fairly evaluated. After evaluation of the results, a workshop provides a place for participants to collect together thoughts and ideas and present current and future research work.

== Tracks ==

===Current tracks===
''New tracks are added as new research needs are identified, this list is current for TREC 2016.''<ref>http://trec.nist.gov/pubs/call2016.html</ref>
* [http://www.trec-cds.org/ Clinical Decision Support Track] - '''Goal:''' to investigate techniques for linking medical cases to information relevant for patient care
* [http://sites.google.com/site/treccontext/ Contextual Suggestion Track] - '''Goal:''' to investigate search techniques for complex information needs that are highly dependent on context and user interests.
* [http://trec-dd.org/ Dynamic Domain Track] - '''Goal:'''  to investigate  domain-specific search algorithms that adapt to the dynamic information needs of professional users as they explore in complex domains. 
* [http://trec-liveqa.org/ LiveQA Track] -  '''Goal:'''  to  generate answers to real questions originating from real users via a live question stream, in real time. 
* [http://trec-open-search.org/ OpenSearch Track] - '''Goal:''' to  explore an evaluation paradigm for IR that involves real users of operational search engines. For this first year of the track the task will be ad hoc Academic Search.
* Real-Time Summarization Track -  '''Goal:''' to explore techniques for constructing real-time update summaries from social media streams in response to users' information needs. 
* [http://www.cs.ucl.ac.uk/tasks-track-2016/ Tasks Track] - '''Goal:'''  to test whether systems can induce the possible tasks users might be trying to accomplish given a query. 
* [http://trec-total-recall.org/ Total Recall Track] -  '''Goal:''': to evaluate methods to achieve very high recall, including methods that include a human assessor in the loop.

===Past tracks===
* Chemical Track - '''Goal:''' to develop and evaluate technology for large scale search in [[chemistry]]-related documents, including academic papers and patents, to better meet the needs of professional searchers, and specifically [[patent search]]ers and chemists.
* [[Crowdsourcing]] Track - '''Goal:''' to provide a collaborative venue for exploring [[crowdsourcing]] methods both for evaluating search and for performing search tasks. 
* [[TREC Genomics|Genomics Track]] - '''Goal:''' to study the retrieval of [[Genomics|genomic]] data, not just gene sequences but also supporting documentation such as research papers, lab reports, etc. Last ran on TREC 2007.
* [[Enterprise search|Enterprise Track]] - '''Goal:''' to study search over the data of an organization to complete some task. Last ran on TREC 2008.
* Entity Track - '''Goal:''' to perform entity-related search on Web data. These search tasks (such as finding entities and properties of entities) address common information needs that are not that well modeled as ad hoc document search.
* [[Cross-language information retrieval|Cross-Language]] Track - '''Goal:''' to investigate the ability of retrieval systems to find documents topically regardless of source language.
* [[Federated search|FedWeb]] Track - '''Goal:''' to select best resources to forward a query to, and merge the results so that most relevant are on the top.
* Federated Web Search Track - '''Goal:''' to investigate techniques for the selection and combination of search results from a large number of real on-line web search services.
* Filtering Track - '''Goal:''' to binarily decide retrieval of new incoming documents given a stable [[information need]].
* HARD Track - '''Goal:''' to achieve High Accuracy Retrieval from Documents by leveraging additional information about the searcher and/or the search context.
* Interactive Track - '''Goal:''' to study user [[Human-computer interaction|interaction]] with text retrieval systems.
* Knowledge Base Acceleration Track - '''Goal:''' to develop techniques to dramatically improve the efficiency of (human) knowledge base curators by having the system suggest modifications/extensions to the KB based on its monitoring of the data streams.
* Legal Track - '''Goal:''' to develop search technology that meets the needs of lawyers to engage in effective [[discovery (law)|discovery]] in digital document collections.
* Medical Records Track - '''Goal:''' to explore methods for searching unstructured information found in patient medical records. 
* [[Microblog]] Track - '''Goal:''' to examine the nature of real-time information needs and their satisfaction in the context of microblogging environments such as Twitter. 
* Novelty Track - '''Goal:''' to investigate systems' abilities to locate new (i.e., non-redundant) information.
* [[Question answering|Question Answering]] Track - '''Goal:''' to achieve more [[information retrieval]] than just [[document retrieval]] by answering factoid, list and definition-style questions.
* Robust Retrieval Track - '''Goal:''' to focus on individual topic effectiveness.
* [[Relevance feedback|Relevance Feedback]] Track - '''Goal:''' to further deep evaluation of relevance feedback processes.
* Session Track - '''Goal:''' to develop methods for measuring multiple-query sessions where information needs drift or get more or less specific over the session.
* [[Spam (electronic)|Spam]] Track - '''Goal:''' to provide a standard evaluation of current and proposed [[spam filter]]ing approaches.
* Temporal Summarization Track - '''Goal:''' to develop systems that allow users to efficiently monitor the information associated with an event over time.
* [[Terabyte]] Track - '''Goal:''' to investigate whether/how the [[information retrieval|IR]] community can scale traditional IR test-collection-based evaluation to significantly large collections.
* [[Video search engine|Video]] Track - '''Goal:''' to research in automatic segmentation, [[index (search engine)|index]]ing, and content-based retrieval of [[digital video]].
:In 2003, this track became its own independent evaluation named [[TRECVID]].
* Web Track - '''Goal:''' to explore information seeking behaviors common in general web search.

===Related events===
In 1997, a Japanese counterpart of TREC was launched (first workshop in 1999), called [http://research.nii.ac.jp/ntcir/ NTCIR] ([[National Institute of Informatics|NII]] Test Collection for IR Systems), and in 2000, a European counterpart was launched, called [http://www.clef-campaign.org/ CLEF] (Cross Language Evaluation Forum).

== Conference contributions to search effectiveness==

NIST claims that within the first six years of the workshops, the effectiveness of retrieval systems approximately doubled.<ref>[http://trec.nist.gov/overview.html From TREC homepage: "... effectiveness approximately doubled in the first six years of TREC"]</ref> The conference was also the first to hold large-scale evaluations of non-English documents, speech, video and retrieval across languages. Additionally, the challenges have inspired a large body of [http://trec.nist.gov/pubs.html publications]. Technology first developed in TREC is now included in many of the world's commercial [[search engine]]s.  An independent report by RTII found that "about one-third of the improvement in web search engines from 1999 to 2009 is attributable to TREC. Those enhancements likely saved up to 3 billion hours of time using web search engines. ... Additionally, the report showed that for every $1 that NIST and its partners invested in TREC, at least $3.35 to $5.07 in benefits were accrued to U.S. information retrieval researchers in both the private sector and academia."
<ref>{{cite web|url=http://rti.org/page.cfm?objectid=75E125DC-5056-B100-31A5A6BDE897DE6D |title=NIST Investment Significantly Improved Search Engines |publisher=Rti.org |date= |accessdate=2012-01-19}}</ref>
<ref>http://www.nist.gov/director/planning/upload/report10-1.pdf</ref>

While one study suggests that the state of the art for ad hoc search has not advanced substantially in the past decade,<ref>Timothy G. Armstrong, Alistair Moffat, William Webber, Justin Zobel.  Improvements that don't add up: ad hoc retrieval results since 1998.  CIKM 2009.  ACM.</ref> it is referring just to search for topically relevant documents in small news and web collections of a few gigabytes.  There have been advances in other types of ad hoc search in the past decade.  For example, test collections were created for known-item web search which found improvements from the use of anchor text, title weighting and url length, which were not useful techniques on the older ad hoc test collections.  In 2009, a new billion-page web collection was introduced, and spam filtering was found to be a useful technique for ad hoc web search, unlike in past test collections.

The test collections developed at TREC are useful not just for (potentially) helping researchers advance the state of the art, but also for allowing developers of new (commercial) retrieval products to evaluate their effectiveness on standard tests.  In the past decade, TREC has created new tests for enterprise e-mail search, genomics search, spam filtering, e-Discovery, and several other retrieval domains.

TREC systems often provide a baseline for further research.  Examples include:
* [[Hal Varian]], Chief Economist at [[Google]], says ''Better data makes for better science. The history of information retrieval illustrates this principle well," and describes TREC's contribution.<ref>[http://googleblog.blogspot.com/2008/03/why-data-matters.html Why Data Matters]</ref>
* TREC's Legal track has influenced the e-Discovery community both in research and in evaluation of commercial vendors.<ref>[http://blogs.the451group.com/information_management/2009/01/29/standards-in-e-discovery-%E2%80%93-walking-the-walk/ The 451 Group: Standards in e-Discovery -- walking the walk]</ref>
* The [[IBM]] researcher team building [[IBM Watson]] (aka [[DeepQA]]), which beat the world's best [[Jeopardy!]] players,<ref>[http://www-03.ibm.com/press/us/en/presskit/27297.wss IBM and Jeopardy! Relive History with Encore Presentation of Jeopardy!: The IBM Challenge]</ref> used data and systems from TREC's QA Track as baseline performance measurements.<ref>[http://www.aaai.org/AITopics/articles&columns/Ferrucci-Watson2010.pdf David Ferrucci, Eric Brown, Jennifer Chu-Carroll, James Fan, David Gondek, Aditya A. Kalyanpur, Adam Lally, J. William Murdock, Eric Nyberg, John Prager, Nico Schlaefer, and Chris Welt. '''Building Watson:  An Overview of the DeepQA Project''']</ref>

== Participation ==
The conference is made up of a varied, international group of researchers and developers.<ref>{{cite web|url=https://wiki.ir-facility.org/index.php/Participants |title=Participants - IRF Wiki |publisher=Wiki.ir-facility.org |date=2009-12-01 |accessdate=2012-01-19}}</ref><ref>http://trec.nist.gov/pubs/trec17/papers/LEGAL.OVERVIEW08.pdf</ref><ref>{{cite web|url=http://trec.nist.gov/pubs/trec17/appendices/million.query.results.html |title=Text REtrieval Conference (TREC) TREC 2008 Million Query Track Results |publisher=Trec.nist.gov |date= |accessdate=2012-01-19}}</ref> In 2003, there were 93 groups from both academia and industry from 22 countries participating.

==References==
{{reflist}}

== External links ==
*[http://trec.nist.gov/ TREC website at NIST]
*[http://www.nist.gov/itl/div894/894.02/related_projects/tipster/ TIPSTER]
*[http://www.amazon.com/TREC-Experiment-Evaluation-Information-Electronic/dp/0262220733/ The TREC book (at Amazon)]

[[Category:Information retrieval organizations]]
[[Category:Computational linguistics]]
[[Category:Natural language processing]]
[[Category:Computer science competitions]]
<=====doc_Id=====>:555
<=====title=====>:
European Summer School in Information Retrieval
<=====text=====>:
The '''European Summer School in Information Retrieval''' ('''ESSIR''') is a scientific event founded in 1990, which starts off a series of Summer Schools to provide high quality teaching of information retrieval on advanced topics. ESSIR is typically a week-long event consisting of guest lectures and seminars from invited lecturers who are recognized experts in the field.
The aim of ESSIR is to give to its participants a common ground in different aspects of '''[[Information Retrieval]] (IR)'''. Maristella Agosti in 2008 stated that: "''The term IR identifies the activities that a person – the user – has to conduct to choose, from a collection of documents, those that can be of interest to him to satisfy a specific and contingent information need.''"<ref>Agosti, M.: "Information Access using the Guide of User Requirements". In: ''Information Access through Search Engines and Digital Libraries''. Agosti, M. ed., Springer-Verlag Berlin Heidelberg, pp. 1-12, (2008).</ref>

IR is a discipline with many facets and at the same time influences and is influenced by many other scientific disciplines. Indeed, IR ranges from [[computer science]] to [[information science]] and beyond; moreover, a large number of IR methods and techniques are adopted and absorbed by several technologies. The IR core methods and techniques are those for designing and developing IR systems, Web search engines, and tools for information storing and querying in Digital Libraries. IR core subjects are: system architectures, algorithms, formal theoretical models, and evaluation of the diverse systems and services that implement functionalities of storing and retrieving documents from multimedia document collections, and over wide area networks such as the [[Internet]].

ESSIR aims to give a deep and authoritative insight of the core IR methods and subjects along these three dimensions and also for this reason it is intended for researchers starting out in IR, for industrialists who wish to know more about this increasingly important topic and for people working on topics related to management of information on the Internet.

Two books have been prepared as readings in IR from editions of ESSIR, the first one is ''Lectures on Information Retrieval'',<ref>Agosti, M., Crestani, F. and Pasi, G. (Eds): "Lectures on Information Retrieval". Revised Lectures of Third European Summer-School, ESSIR 2000 Varenna, Italy, September 11–15, 2000. LNCS Vol. 1980, Springer-Verlag, Berlin Heidelberg, 2001.</ref> the second one is ''Advanced Topics in Information Retrieval''.<ref>Melucci, M., and Baeza-Yates, R. (Eds): "Advanced Topics in Information Retrieval". The Information Retrieval Series, Vol. 33, Springer-Verlag, Berlin Heidelberg, 2011.</ref>

== ESSIR Editions ==
ESSIR series started in 1990 coming out from the successful experience of the Summer School in Information Retrieval (SSIR) conceived and designed by [http://www.dei.unipd.it/~agosti/ Maristella Agosti], [[University of Padua]], Italy and [[Nick Belkin]], [[Rutgers University]], U.S.A., for an Italian audience in 1989.

{| class="wikitable" border="1"
|-
! Edition
! Web Site
! Location
! Organiser(s)
|-
|  10th
|  [http://mklab.iti.gr/essir2015/ ESSIR 2015]
|  Thessaloniki, Greece
|  Ioannis (Yiannis) Kompatsiaris, Symeon Papadopoulos, Theodora Tsikrika, and Stefanos Vrochidis
|-
|  9th
|  [http://www.ugr.es/~essir2013/ ESSIR 2013]
|  Granada, Spain
|  Juan M. Fernadez-Luna and Juan F. Huete
|-
|  8th
|  [http://essir.uni-koblenz.de/ ESSIR 2011]
|  Koblenz, Germany
|  Sergej Sizov and Steffen Staab
|-
|  7th
|  [http://essir2009.dei.unipd.it/ ESSIR 2009]
|  Padua, Italy
|  Massimo Melucci and Ricardo Baeza-Yates
|-
|  6th
|  [http://www.dcs.gla.ac.uk/essir2007/ ESSIR 2007]
|  Glasgow, Scotland, United Kingdom
|  Iadh Ounis and Keith van Rijsbergen
|-
|  5th
|  [http://www.cdvp.dcu.ie/ESSIR2005/ ESSIR 2005]
|  Dublin, Ireland
|  Alan Smeaton
|-
|  4th
|  [http://www-clips.imag.fr/mrim/essir03/main_essir.html ESSIR 2003]
|  Aussois (Savoie), France
|  Catherine Berrut and Yves Chiaramella
|-
|  3rd
|  [http://www.itim.mi.cnr.it/Eventi/essir2000/index.htm ESSIR 2000]
|  Varenna, Italy
|  Maristella Agosti, Fabio Crestani, and Gabriella Pasi
|-
|  2nd
|  [http://www.dcs.gla.ac.uk/essir/ ESSIR 1995]
|  Glasgow, United Kingdom
|  Keith van Rijsbergen
|-
|  1st
|  [http://ims.dei.unipd.it/websites/essir/essir1990.html ESSIR 1990]
|  Brixen, Italy
|  Maristella Agosti
|}

==Notes==
{{reflist}}

==External links==
* [http://ims.dei.unipd.it/websites/essir/home.html ESSIR presentation page of the IMS Research Group]
* [http://ims.dei.unipd.it IMS Research Group, Department of Information Engineering – University of Padua, Italy]
* [http://www.dei.unipd.it/ Department of Information Engineering – University of Padua, Italy]
* [http://www.unipd.it/en/index.htm University of Padua, Italy]

[[Category:Information retrieval organizations]]
[[Category:Summer schools]]
<=====doc_Id=====>:558
<=====title=====>:
Coveo
<=====text=====>:
{{Infobox company
| name = Coveo Solutions Inc.
| logo = [[Image:Coveo logo.png|120px]]
| type = Private
| slogan = 
| foundation =  2005
| location_city = [[Quebec City]], [[Canada]]
| key_people = Louis Têtu, Chairman and CEO <br />Laurent Simoneau, President and CTO
| num_employees =200+
| industry = [[Enterprise search]]
| products = Coveo Search & Relevance Platform,<br />Coveo for Sitecore,<br />Coveo for Salesforce
| homepage = http://www.coveo.com
}}

'''Coveo''' is a provider of [[enterprise search]] and website search technologies, with integrated plug-ins for [[Salesforce.com]], Sitecore CEP, and [[Microsoft Outlook]] and [[SharePoint]].  APIs also allow for custom integration with other applications.

==History==
Coveo Solutions Inc. was founded in 2005 as a spin-off of [[Copernic|Copernic Technologies Inc.]] Laurent Simoneau, Coveo's president and chief executive officer was formerly Copernic's chief operating officer. About 30 employees moved into the new company, with offices at that time in [[Quebec City]] and [[Montreal]] in Canada and in [[Palo Alto]], Calif.<ref>http://www.eweek.com/c/a/Enterprise-Applications/Copernic-Ready-to-Take-On-Google-In-Enterprise-Search-Product/</ref>

==Products==
'''Coveo Search & Relevance Platform'''

Coveo Search & Relevance Platform is a modular enterprise search technology that can index information stored in diverse repositories throughout the company, perform text analytics and metadata enrichment on the indexed content, and make the content findable through search-driven interfaces.

'''Coveo for Sitecore'''

Coveo for Sitecore is an integrated website search product to be used in conjunction with Sitecore’s Customer Experience Platform.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Sitecore console.

'''Coveo for Salesforce'''

Coveo for Salesforce is an integrated CRM search product to be used in conjunction with Salesforce.com Service Cloud and Communities Editions.  The product enables the unified indexing of multiple repositories, contextual search, and search management via the Salesforce console.

==Customers==
Coveo claims its clients include more than 700 implementations including AmerisourceBergen, CA, California Water Service Co., Deloitte, ESPN, Haley & Aldrich, GEICO, Lockheed Martin, P&G, PRTM, PricewaterhouseCoopers, Rabobank, SNC-Lavalin, Spencer Stuart, Theodoor Gilissen, and the U.S. Navy.<ref>{{cite web|url=http://www.coveo.com/en/~/media/Files/about-us/Coveo-Corporate-Fact-Sheet-Q109.ashx |title=Coveo corporate fact sheet |date= |accessdate=2011-02-27}}</ref> These companies were also mentioned while not confirmed by a citation: HP, PwC, Netezza Corporation, NATO, NASA, AC Nielsen, among many others.{{Citation needed|date=February 2010}}

==References==
{{reflist}}

==External links==
* [http://www.coveo.com/ Coveo.com]

[[Category:Companies based in Quebec City]]
[[Category:Information retrieval organizations]]
[[Category:BlackBerry development software]]
<=====doc_Id=====>:561
<=====title=====>:
Conference and Labs of the Evaluation Forum
<=====text=====>:
The '''Conference and Labs of the Evaluation Forum''' (formerly '''Cross-Language Evaluation Forum'''), or '''CLEF''', is an organization promoting research in multilingual [[information access]] (currently focusing on [[European Commissioner for Multilingualism|European languages]]). Its specific functions are to maintain an underlying framework for testing [[information retrieval]] systems and to create [[digital library|repositories]] of data for researchers to use in developing  comparable [[Technical standard|standards]].<ref name="Peters">{{cite conference | first1 = Carol | last1 = Peters| first2 = Martin | last2 = Braschler | first3 = Khalid | last3 = Choukri | first4 = Julio | last4 = Gonzalo | first5 = Michael | last5 = Kluck | title = The Future of Evaluation for Cross-Language Information Retrieval Systems | conference = Second Workshop of the Cross-Language Evaluation Forum, CLEF 2001 | citeseerx = 10.1.1.109.7647 }}</ref>
The organization holds a forum meeting   every September in Europe. Prior to each forum, participants receive a set of challenge tasks. The tasks  are designed to test various aspects of information retrieval systems and encourage their development. Groups of researchers propose and organize campaigns to satisfy those tasks. The results are used as [[benchmark (computing)|benchmarks]] for the state of the art  in the specific areas.,<ref>{{cite journal | url = http://www.springerlink.com/content/l7v0354471u53385/ | title = Special Issue on CLEF | journal = Information Retrieval | volume = 7 | issue = 1–2 | year = 2004 }}</ref><ref>Fredric C. Gey, Noriko Kando, and Carol Peters "Cross-Language Information Retrieval: the way ahead" in ''Information Processing & Management''
vol. 41, no. 3,  p.415-431 May 2005, {{doi|10.1016/j.ipm.2004.06.006}}</ref>

For example, the 2010 medical retrieval task focuses on retrieval of computed tomography,  MRI, and radiographic images.<ref name="ImageCLEFmed">{{cite web | last = Mueller| first = Henning| authorlink = | coauthors = | title = Medical Retrieval Task| work = | publisher =ImageCLEF - Cross-language image retrieval evaluations | date = 20 May 2010| url =http://www.imageclef.org/2010/medical | format = | doi = | accessdate = 27 May 2010 }}</ref>

==References==
{{reflist}}

== External links ==
* [http://www.clef-initiative.eu/ CLEF homepage]

[[Category:Information retrieval organizations]]


{{Compu-conference-stub}}
<=====doc_Id=====>:564
<=====title=====>:
Category:Google
<=====text=====>:
{{Commons category|Google}}
{{Cat main|Google}}

[[Category:Internet search engines]]
[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Internet companies of the United States]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Web portals]]
[[Category:Information retrieval organizations]]
[[Category:Alphabet Inc.]]
[[Category:Wikipedia categories named after information technology companies of the United States]]
[[Category:Wikipedia categories named after websites]]
<=====doc_Id=====>:567
<=====title=====>:
Smartlogic
<=====text=====>:
{{Multiple issues|
{{advert|date=June 2015}}
{{COI|date=June 2015}}
{{notability|Companies|date=November 2015}}
}}
{{Infobox company |
name = Smartlogic  |
logo =  |
slogan = "The Content Intelligence Company" |
type = [[Privately held company|Private]] |
foundation = 2006 |
location = United States, UK | 
area_served = Global |
industry = [[Information retrieval]] |
products = Semaphore Cloud, Semaphore Ontology Editor, Semaphore Classification Server, Semaphore Semantic Enhancement Server, Advanced Language Packs, Search Appliance Framework, Text Miner, Classification Review Tool, Classification Analysis Tool  | 
num_employees  = 55|
homepage = http://www.smartlogic.com
}}

'''Smartlogic ''' is a [[software company]] which specializes in developing [[information retrieval]], [[text analytics]] and [[knowledge management]] solutions.

==History==
Smartlogic was founded in the United Kingdom in 2006. It is a privately held company and has offices in San Jose, CA; Alexandria, VA; Cambridge, MA and London, UK. The company develops and sells a suite of products; Semaphore Ontology Editor, Classification Server, Advanced Language Packs, Semantic Enhancement Server, Text Miner, Classification Review tool, and Classification Analysis tool.

==Products==

===Semaphore Ontology Editor===
Semaphore Ontology Editor is a web-based tool used to build taxonomies, ontologies, controlled vocabularies as well as other knowledge organization systems. Models are used by organizations to enhance the capabilities of enterprise search engines,<ref>[http://www.cmswire.com/events/item/webinar-leverage-metadata-to-drive-critical-business-processes-022370.php] CMSWire Leverage Metadata to Drive Critical Business Processes</ref> content management and workflow systems deployed by clients to augment and enhance their investment.

===Semaphore Classification Server===
Semaphore Classification Server uses the model structure from Semaphore Ontology Editor and auto classifies unstructured information assets by applying metadata tags to the unstructured information.

===Semaphore Advanced Language Packs===

===Semantic Enhancement Server===

==Integrations==
Semaphore integrates with [[Microsoft Sharepoint]],<ref>[http://www.cmswire.com/cms/information-management/sharepoint-2013-office-365-get-semantic-search-with-smartlogic-semaphore-018353.php] CMSWire SharePoint 2013 Office 365 Get Semantic Search with Smartlogic Semaphore</ref> [[Google Search Appliance]],<ref>[https://www.google.com/enterprise/marketplace/viewVendorListings?vendorId=33&pli=1] Google Enterprise Catalogue</ref> [[Apache Solr]],<ref>[http://www.flatironssolutions.com/blog/alfresco-semaphore-integration/] Alfresco-Semaphore Integration</ref> FAST ESP<ref>[http://arnoldit.com/wordpress/2009/10/23/smartlogic-and-fast-esp-integration/] Stephen E. Arnold - Beyond Search</ref> and others.

==References==
{{Reflist}}

==External links==
* [http://www.smartlogic.com/ Smartlogic]

[[Category:Software companies of the United Kingdom]]
[[Category:Information retrieval organizations]]
[[Category:Analytics companies]]
[[Category:Knowledge management]]
<=====doc_Id=====>:570
<=====title=====>:
Gerard Salton Award
<=====text=====>:
The '''Gerard Salton Award''' is presented by the [[Association for Computing Machinery]] (ACM) [[Special Interest Group on Information Retrieval]] (SIGIR) every three years to an individual who has made "significant, sustained and continuing contributions to research in [[information retrieval]]". SIGIR also co-sponsors (with [[SIGWEB]]) the [[Vannevar Bush Award]], for the best paper at the [[Joint Conference on Digital Libraries]].

==Chronological honorees and lectures==
* 1983 - [[Gerard Salton]], [[Cornell University]] : "About the future of automatic information retrieval."
* 1988 - [[Karen Spärck Jones]], [[University of Cambridge]] : "A look back and a look forward."
* 1991 - [[Cyril Cleverdon]], [[Cranfield Institute of Technology]] : "The significance of the Cranfield tests on index languages."
* 1994 - William S. Cooper, [[University of California, Berkeley]] : "The formalism of probability theory in IR: a foundation or an encumbrance?"
* 1997 - [[Tefko Saracevic]], [[Rutgers University]] : "Users lost (summary): reflections on the past, future, and limits of information science." 
* 2000 - [[Stephen Robertson (computer scientist)|Stephen E. Robertson]], [[City University, London|City University London]] : "On theoretical argument in information retrieval."<BR>'''For ...''' ''"Thirty years of significant, sustained and continuing contributions to research in information retrieval. Of special importance are the theoretical and empirical contributions to the development, refinement, and evaluation of probabilistic models of information retrieval."''
* 2003 - [[W. Bruce Croft]], [[University of Massachusetts Amherst]] : "Information retrieval and computer science: an evolving relationship."<BR>'''For ...''' ''"More than twenty years of significant, sustained and continuing contributions to research in information retrieval. His contributions to the theoretical development and practical use of [[Bayesian inference]] networks and [[language modelling]] for retrieval, and to their evaluation through extensive experiment and application, are particularly important. The Center for Intelligent Information Retrieval which he founded illustrates the strong synergies between fundamental research and its application to a wide range of practical information management problems."''
* 2006 - [[C. J. van Rijsbergen]], [[University of Glasgow]] : 	"Quantum haystacks."
* 2009 - [[Susan Dumais]], [[Microsoft Research]] : "An Interdisciplinary Perspective on Information Retrieval."
* 2012 - [[Norbert Fuhr]], [[University of Duisburg-Essen]]: "Information Retrieval as Engineering Science."
* 2015 - [[Nicholas J. Belkin]], [[Rutgers University]]: “People, Interacting with Information”

==External links==
* [http://www.acm.org/sigir/ ACM SIGIR homepage]
* [http://www.sigir.org/awards/awards.html ACM SIGIR awards]

[[Category:Association for Computing Machinery]]
[[Category:Computer science awards]]
[[Category:Information retrieval organizations]]
<=====doc_Id=====>:573
<=====title=====>:
Datanet
<=====text=====>:
{{Use mdy dates|date=September 2011}}
'''DataNet''', or '''Sustainable Digital Data Preservation and Access Network Partner''' was a research program of the U.S. [[National Science Foundation]] Office of Cyberinfrastructure.  The office announced a request for proposals with this title on September 28, 2007.<ref name="datanetprogram">{{cite web
|url=http://www.nsf.gov/funding/pgm_summ.jsp?pims_id=503141
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners (DataNet) Program Summary
|date=September 28, 2007
|accessdate=October 3, 2007
}}</ref>  The lead paragraph of its synopsis describes the program as:

<blockquote>Science and engineering research and education are increasingly digital and increasingly data-intensive.  Digital data are not only the output of research but provide input to new hypotheses, enabling new scientific insights and driving innovation. Therein lies one of the major challenges of this scientific generation: how to develop the new methods, management structures and technologies to manage the diversity, size, and complexity of current and future data sets and data streams.  This solicitation addresses that challenge by creating a set of exemplar national and global data research infrastructure organizations (dubbed DataNet Partners) that provide unique opportunities to communities of researchers to advance science and/or engineering research and learning.</blockquote>

The introduction in the solicitation<ref name="datanetsolicitation">{{cite web
|url=http://www.nsf.gov/publications/pub_summ.jsp?ods_key=nsf07601
|publisher=National Science Foundation
|title=Sustainable Digital Data Preservation and Access Network Partners Program Announcements & Information
|date=September 28, 2007
|accessdate=October 3, 2007
}}</ref> goes on to say:

<blockquote>Chapter 3 (Data, Data Analysis, and Visualization) of [http://www.nsf.gov/pubs/2007/nsf0728/index.jsp NSF’s Cyberinfrastructure Vision for 21st century Discovery] presents a vision in which “science and engineering digital data are routinely deposited in well-documented form, are regularly and easily consulted and analyzed by specialists and non-specialists alike, are openly accessible while suitably protected, and are reliably preserved.” The goal of this solicitation is to catalyze the development of a system of science and engineering data collections that is open, extensible and evolvable.</blockquote>

The initial plan called for a $100 million initiative: five awards of $20&nbsp;million each over five years with the possibility of continuing funding.  Awards were given in two rounds. In the first round, for which  full proposals were due on March 21, 2008, two DataNet proposals were awarded. [[DataONE]],<ref>{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by William Michener at the [[University of New Mexico]] covers ecology, evolutionary, and earth science. The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]], focuses on astronomy, earth science, life sciences, and social science. 

For the second round, preliminary proposals were due on October 6, 2008 and full proposals on February 16, 2009. Awards from the second round were greatly delayed, and funding was reduced substantially from $20 million per project to $8 million.<ref>{{cite web|author=National Science Foundation |url=http://www.nsf.gov/awardsearch/simpleSearchResult?queryText=%22datanet+full+proposal%3A%22 |title=NSF DataNet Awards |publisher=www.nsf.gov | accessdate=2013-01-19}}</ref> Funding for three second round projects began in Fall 2011. SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]], seeks to provide data curation software and services for the "long tail" of small- and medium-scale data producers in the domain of sustainability science. The DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Reagan Moore of the [[University of North Carolina]], uses the integrated Rule-Oriented Data System (iRODS) to provide data grid infrastructure for science and engineering. ''Terra Populus'',<ref>{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]] focuses on tools for data integration across the domains of social science and environmental data, allowing interoperability of the three major data formats used in these domains: microdata, areal data, and raster data.

==References==
{{reflist|30em}}

==External links==
* [http://www.dataone.org DataONE]
* [http://dataconservancy.org/ Data Conservancy]
* [http://sead-data.net/ SEAD Sustainable Environment - Actionable Data]
* [http://datafed.org/ DataNet Federation Consortium]
* [http://www.terrapop.org/ Terra Populus: Integrated Data on Population and the Environment] 
 

[[Category:National Science Foundation]]
[[Category:Science and technology in the United States]]
[[Category:Information retrieval organizations]]
[[Category:Digital library projects]]
<=====doc_Id=====>:576
<=====title=====>:
TeLQAS
<=====text=====>:
'''TeLQAS''' (Telecommunication Literature Question Answering System) is an experimental [[question answering]] system developed for answering English questions in the [[telecommunications]] domain.<ref>Mahmoud R. Hejazi, Maryam S. Mirian , Kourosh Neshatian, Azam Jalali, and Bahadorreza Ofoghi, ''A Telecommunication Literature Question/Answering System Benefits from a Text Categorization Mechanism'', International Conference on Information and Knowledge Engineering (IKE2003), July 2003, USA.</ref>

==Architecture==
TeLQAS includes three main subsystems: an online subsystem, an offline subsystem, and an [[ontology]]. The online subsystem answers questions submitted by users in real time. During the online process, TeLQAS processes the question using a [[natural language processing]] component that implements [[part-of-speech tagging]] and simple [[syntactic parsing]]. The online subsystem also utilizes an inference engine in order to carry out necessary inference on small elements of knowledge. The offline subsystem automatically indexes documents collected by a ''focused [[web crawler]]'' from the web. An ontology server along with its [[API]] is used for knowledge representation.<ref>Kourosh Neshatian and Mahmoud R. Hejazi, ''An Object Oriented Ontology Interface for Information Retrieval Purposes in Telecommunication Domain'', International Symposium on Telecommunication (IST2003).</ref> The main concepts and classes of the ontology are created by domain experts. Some of these classes, however, can be instantiated automatically by the offline components.

==References==
<references/>

[[Category:Computational linguistics]]
[[Category:Information retrieval systems]]
[[Category:Natural language processing software]]
<=====doc_Id=====>:579
<=====title=====>:
Category:Search engine software
<=====text=====>:
[[Category:Information retrieval systems]]
[[Category:Utility software by type]]
[[Category:Marketing software]]
[[Category:Web software]]
<=====doc_Id=====>:582
<=====title=====>:
Agrep
<=====text=====>:
{{lowercase|title=agrep}}
{{Infobox software
| name                   = agrep
| logo                   = <!-- Image name is enough -->
| logo caption           = 
| logo_size              = 
| logo_alt               = 
| screenshot             = <!-- Image name is enough -->
| caption                = 
| screenshot_size        = 
| screenshot_alt         = 
| collapsible            = 
| developer              = {{Plainlist|
* [[Udi Manber]]
* Sun Wu
}}
| released               = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| discontinued           = 
| latest release version = 
| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| latest preview version = 
| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->
| status                 = 
| programming language   = C
| operating system       = {{Plainlist|
* [[Unix-like]]
* [[OS/2]]
* [[DOS]]
* [[Microsoft Windows|Windows]]
}}
| platform               = 
| size                   = 
| language               = 
| language footnote      = 
| genre                  = [[Pattern matching]]
| license                = [https://raw.githubusercontent.com/Wikinaut/agrep/master/COPYRIGHT ISC open source license]
| standard               = 
| website                = {{URL|http://www.tgries.de/agrep}}
}}

'''agrep''' (approximate [[grep]]) is an [[open-source]] [[approximate string matching]] program, developed by [[Udi Manber]] and Sun Wu between 1988 and 1991, for use with the [[Unix]] operating system. It was later ported to [[OS/2]], [[DOS]], and [[Microsoft Windows|Windows]].

It selects the best-suited algorithm for the current query from a variety of the known fastest (built-in) [[string searching algorithm]]s, including Manber and Wu's [[bitap algorithm]] based on [[Levenshtein distance]]s.

agrep is also the [[Search engine (computing)|search engine]] in the indexer program [[GLIMPSE]]. agrep is under a free [[ISC License]].<ref>[http://webglimpse.net/sublicensing/licensing.html WebGlimpse, Glimpse and also AGREP license] since 18.09.2014 ([http://opensource.org/licenses/ISC ISC License]).</ref>

== Alternative implementations ==
A more recent agrep is the command-line tool provided with the [[TRE (computing)|TRE]] regular expression library. TRE agrep is more powerful than Wu-Manber agrep since it allows weights and total costs to be assigned separately to individual groups in the pattern. It can also handle Unicode.<ref>{{cite web | title=TRE - TRE regexp matching package - Features | url=http://laurikari.net/tre/about }}</ref> Unlike Wu-Manber agrep, TRE agrep is licensed under a [[BSD licenses#BSD-style licenses|2-clause BSD-like license]].

FREJ (Fuzzy Regular Expressions for Java) open-source library provides command-line interface which could be used in the way similar to agrep. Unlike agrep or TRE it could be used for constructing complex substitutions for matched text.<ref>{{cite web | title=FREJ - Fuzzy Regular Expressions for Java - Guide and Examples | url=http://frej.sf.net/rules.html }}</ref> However its syntax and matching abilities differs significantly from ones of ordinary [[regular expression]]s.

==References==
{{Reflist}}

==External links==
* Wu-Manber agrep
**[http://www.tgries.de/agrep AGREP home page]
**[ftp://ftp.cs.arizona.edu/agrep/ For Unix]  (To compile under OSX 10.8, add <code>-Wno-return-type</code> to the <code>CFLAGs  = -O</code> line in the Makefile)
*[http://wiki.christophchamp.com/index.php/Agrep_(command) Entry for "agrep" in Christoph's Personal Wiki]
*See also
**[http://laurikari.net/tre TRE regexp matching package]
**[https://web.archive.org/web/20080513225010/http://www1.bell-labs.com/project/wwexptools/cgrep/ cgrep a defunct command line approximate string matching tool]
**[http://www.dcc.uchile.cl/~gnavarro/software/ nrgrep] a command line approximate string matching tool
**[http://finzi.psych.upenn.edu/R/library/base/html/agrep.html agrep as implemented in R]

[[Category:Information retrieval systems]]
[[Category:Unix text processing utilities]]
[[Category:Software using the ISC license]]
<=====doc_Id=====>:585
<=====title=====>:
Locate (Unix)
<=====text=====>:
{{lowercase}}
'''<code>locate</code>''' is a [[Unix]] utility which serves to find [[computer file|file]]s on [[filesystem]]s. It searches through a prebuilt [[database]] of files generated by the <code>updatedb</code> command or by a [[Daemon (computing)|daemon]] and compressed using [[incremental encoding]]. It operates significantly faster than <code>[[find]]</code>, but requires regular updating of the database. This sacrifices overall efficiency (because of the regular interrogation of filesystems even when no user needs information) and absolute accuracy (since the database does not update in [[Real-time computing|real time]]) for significant speed improvements, particularly on very large filesystems.

<code>locate</code> was first created in 1982.<ref>{{cite magazine|last=Woods|first=James A.|date=1983-01-15|title=Finding Files Fast|url=https://archive.org/stream/login-feb83/login_feb83_issue#page/n9/mode/2up|magazine=[[;login:]]|volume=8|issue=1|pages=8–10|publisher=[[Usenix]]|access-date=2016-03-27}}</ref>  The BSD and [[GNU Findutils]] versions derive from the original implementation.<ref>{{cite web|url=https://www.gnu.org/software/findutils/manual/html_node/find_html/Introduction.html#Introduction|title=Finding Files|date=2012-11-17|website=[[GNU]]|publisher=[[Free Software Foundation]]|access-date=2016-03-27|quote=GNU locate and its associated utilities were originally written by James Woods, with enhancements by David MacKenzie.}}</ref>  Their primary database is world-readable, so the index is built as an unprivileged user.

<code>mlocate</code> (Merging Locate) and the earlier <code>slocate</code> (Secure Locate) use a restricted-access database, only showing filenames accessible to the user.<ref>{{cite web|url=http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-url=https://web.archive.org/web/20060411074142/http://carolina.mff.cuni.cz/~trmac/blog/mlocate/|archive-date=2006-04-11|title=mlocate|date=2005|author=Miloslav Trmač|access-date=2016-03-27|quote=...faster and does not trash the system caches as much...attempts to be compatible to GNU locate, when it does not conflict with slocate compatibility.|dead-url=yes}}</ref><ref>{{cite web|url=http://www.geekreview.org/slocate/|archive-url=https://web.archive.org/web/20050507092723/http://www.geekreview.org/slocate/|archive-date=2005-05-07|title=Secure Locate|date=1999|author=Kevin Lindsay|access-date=2016-03-27|quote=...will also check file permissions and ownership so that users will not see files they do not have access to.|dead-url=yes}}</ref>

== References ==
{{reflist}}

== External links ==
* [https://www.gnu.org/software/findutils/findutils.html GNU Findutils]
* [https://fedorahosted.org/mlocate/ mlocate]
* {{man|1|locate|FreeBSD}}
* {{man|1|locate|OpenBSD}}

Variants:
* [http://rlocate.sourceforge.net/ rlocate] - Variant using kernel module and daemon for continuous updates.
* [http://www.kde-apps.org/content/show.php/KwickFind+(Locate+GUI+Frontend)?content=54817 KwickFind] - KDE GUI frontend for locate
* [http://www.locate32.net/ Locate32 for Windows] - GPL'ed graphical Windows variant

{{unix commands}}

[[Category:GNU Project software]]
[[Category:Unix file system-related software]]
[[Category:Information retrieval systems]]


{{Unix-stub}}
<=====doc_Id=====>:588
<=====title=====>:
Statistically improbable phrase
<=====text=====>:
A '''statistically improbable phrase''' ('''SIP''') is a phrase or set of words that occurs more frequently in a document (or collection of documents) than in some larger [[Text corpus|corpus]].<ref>{{cite web|url=http://courses.cms.caltech.edu/cs145/2011/wikipedia.pdf |title=SIPping Wikipedia |website=Courses.cms.caltech.edu |accessdate=2017-01-01}}</ref><ref>{{cite web|url=https://www.plagiarismtoday.com/2012/07/03/how-long-should-a-statistically-improbably-phrase-be/|title=How Long Should a Statistically Improbably Phrase Be?|author=Jonathan Bailey|date=3 July 2012|work=Plagiarism Today}}</ref><ref>{{cite journal|url=http://bioinformatics.oxfordjournals.org/content/26/11/1453|title=Identifying duplicate content using statistically improbable phrases|first1=Mounir|last1=Errami|first2=Zhaohui|last2=Sun|first3=Angela C.|last3=George|first4=Tara C.|last4=Long|first5=Michael A.|last5=Skinner|first6=Jonathan D.|last6=Wren|first7=Harold R.|last7=Garner|date=1 June 2010|publisher=|journal=Bioinformatics|volume=26|issue=11|pages=1453–1457|accessdate=1 January 2017|via=bioinformatics.oxfordjournals.org|doi=10.1093/bioinformatics/btq146|pmid=20472545|pmc=2872002}}</ref> [[Amazon.com]] uses this concept in determining keywords for a given book or chapter, since keywords of a book or chapter are likely to appear disproportionately within that section.<ref>{{cite web|url=http://www.amazon.com/gp/search-inside/sipshelp.html|title=What are Statistically Improbable Phrases?|accessdate=2007-12-18|publisher=[[Amazon.com]]}}</ref><ref>{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2005/08/29/AR2005082901873.html|title=Amazon's Vital Statistics Show How Books Stack Up|last=Weeks|first=Linton|work=[[The Washington Post]]|date=August 30, 2005|accessdate=September 8, 2015}}</ref> [[Christian Rudder]] has also used this concept with data from [[Online dating service|online dating profiles]] and [[Twitter]] posts to determine the phrases most characteristic of a given race or gender in his book ''Dataclysm''.<ref>{{cite book |last=Rudder |first=Christian |date=2014 |title=Dataclysm: Who We Are When We Think No One's Looking |location=New York |publisher=Crown Publishers |page= |isbn=978-0-385-34737-2}}</ref>

==Example== 
In a document about [[computer]]s, the most common word is likely to be the word "the", but since "the" is the most commonly used word in the English language, it is likely that any given document will have the word "the" used very frequently.  However, a word like "program" might occur in the document at a much higher rate than its average rate in the English language.  Hence, it is a word unlikely to occur in any given document, but ''did'' occur in the document given.  "Program" would be a statistically improbable phrase.

The statistically improbable phrases of Darwin's ''[[On the Origin of Species]]'' are: ''temperate productions, genera descended, transitional gradations, unknown progenitor, fossiliferous formations, our domestic breeds, modified offspring, doubtful forms, closely allied forms, profitable variations, enormously remote, transitional grades, very distinct species'' and ''mongrel offspring''.<ref>[http://crookedtimber.org/2005/04/02/sociologically-improbable-phrases/ Sociologically Improbable Phrases] Crooked Timber April 2005</ref>

==See also==
* [[Googlewhack]] – A pair of words occurring on a single webpage, as indexed by Google
* [[tf-idf]] – A statistic used in information retrieval and text mining

==References==
{{Reflist}}

{{Amazon}}

[[Category:Amazon.com]]
[[Category:Bookselling]]
[[Category:Information retrieval systems]]
<=====doc_Id=====>:591
<=====title=====>:
Category:Image search
<=====text=====>:


[[Category:Applications of computer vision]]
[[Category:Information retrieval systems]]
<=====doc_Id=====>:594
<=====title=====>:
Outline of search engines
<=====text=====>:
<!--... Attention:  THIS IS AN OUTLINE

        part of the set of 700+ outlines listed at
             [[Portal:Contents/Outlines]].

                 Wikipedia outlines are
              a special type of list article.
              They make up one of Wikipedia's
                content navigation systems

                See [[Wikipedia:Outlines]]
                      for more details.
                   Further improvements
              to this outline are on the way
...-->
The following [[Outline (list)|outline]] is provided as an overview of and topical guide to search engines. 

'''[[Search engine (computing)|Search engine]]''' &ndash; [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented as a list, and are commonly called ''hits''.

{{TOC limit|limit=2}}

== What ''type'' of thing is a search engine? ==

A search engine can be described as all of the following:

* [[Software]] &ndash;
** [[Computer program]] &ndash;
*** [[Application software]] &ndash; computer software designed to help the user to perform specific tasks. Also known as an application or an "app".

== Types of search engines ==

* [[Database search engine]] &ndash;
* [[Desktop search engine]] &ndash;
* [[Distributed search engine]] &ndash; search engine where there is no central server. Unlike traditional centralized search engines, work such as crawling, data mining, indexing, and query processing is distributed among several peers in decentralized manner where there is no single point of control.
* [[Enterprise search engine]] &ndash; search engine employed on and for access to the information on an organization's computer network.
* [[Human search engine]] &ndash; uses human participation to filter the search results and assist users in clarifying their search request. The goal is to provide users with a limited number of relevant results, as opposed to traditional search engines that often return a large number of results that may or may not be relevant.
* [[Hybrid search engine]] &ndash; uses different types of data with or without ontologies to produce the algorithmically generated results based on web crawling. Previous types of search engines only use text to generate their results.
* [[Intelligent medical search engine]]
* [[Metasearch engine]] &ndash; search tool[1] that sends user requests to several other search engines and/or databases and aggregates the results into a single list or displays them according to their source. Metasearch engines enable users to enter search criteria once and access several search engines simultaneously.
** [[Search aggregator]]
* [[Organic search engine]] &ndash; manually operated search service which uses a combination of computer algorithms and human researchers to look up a search query. A search query submitted to an organic search engine is analysed by a human operator who researches the query then formats the response to the user.
* [[Web search engine]] &ndash; designed to search for information on the World Wide Web and FTP servers. The search results are generally presented in a list of results often referred to as SERPS, or "search engine results pages".
** [[Audio search engine]] &ndash; web-based search engine which crawls the web for audio content.
** [[Collaborative search engine]] &ndash; emerging trend for Web search and Enterprise search within company intranets. CSEs let users concert their efforts in information retrieval (IR) activities, share information resources collaboratively using knowledge tags, and allow experts to guide less experienced people through their searches.
** [[Social search engine]] &ndash; type of web search that takes into account the Social Graph of the person initiating the search query.
** [[Video search engine]] &ndash; web-based search engine which crawls the web for video content. Some video search engines parse externally hosted content while others allow content to be uploaded and hosted on their own servers.
* [[Visual search engine]] &ndash; designed to search for information on the World Wide Web through the input of an image or a search engine with a visual display of the search results. Information may consist of web pages, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query).

== Specific search engines ==
{{Main|List of search engines}}

== Search engine software ==

* [[List of search engine software]]

== Search-based applications ==

[[Search-based application]] &ndash;
* [[Bibliographic database]]
* [[Online database]]
** [[List of online databases]]
*** [[List of academic databases and search engines]]
* [[Digital library]]
** [[List of digital library projects]]
*** [[List of online magazines]]
*** [[Wikipedia:List of online newspaper archives]]
* [[Electronic journal]]
** [[Lists of academic journals]]
*** [[List of open-access journals]]
* Digital encyclopedia
** [[Internet encyclopedia]]*
*** [[List of online encyclopedias]]
* [[Wiki]]
** [[List of wikis]]
* Digital dictionary
** [[Online dictionary]]
*** [[List of online dictionaries]]

== Search engine technology ==

[[Search engine technology]]
* [[Search algorithm]]
* [[Search engine image protection]]
* [[Search engine indexing]]
* [[Search engine optimization]]
* [[Search engine results page]]
* [[List of search engine software|Search engine software]]
* [[Search engine submission]]
** [[Search engine optimization copywriting]]
* [[Web crawler]]

== Search engine marketing ==
[[Search engine marketing]]
* [[Pay per click]]
* [[Cost per impression]]
* [[Search analytics]]
* [[Web analytics]]

== Persons influential in search engines ==
* [[Sergey Brin]]
* [[Larry Page]]
* [[Eric Schmidt]]

== See also ==
* [[Outline of the Internet]]
** [[Outline of Google]]
* [[Human flesh search engine]]
{{Clear}}

== References ==
{{Reflist}}

== External links ==
{{Sisterlinks|Search engine}}

* [http://wikimindmap.com/viewmap.php?wiki=en.wikipedia.org&topic=Outline+of+search+engines&Submit=Search This outline displayed as a mindmap], at ''wikimindmap.com''
* {{Dmoz|Computers/Internet/Searching/Search_Engines/|Search Engines}}

{{Outline footer}}

[[Category:Information retrieval systems]]
[[Category:Wikipedia outlines|Search engines]]
[[Category:Articles created via the Article Wizard|Search engines]]
<=====doc_Id=====>:597
<=====title=====>:
Quandl
<=====text=====>:
{{Infobox dot-com company
|name               = Quandl, Inc.
|logo               = [[File:Quandl-logo.png|100px]]
|company_type       = [[Private company|Private]]
|founder            = {{Plainlist|
* Tammer Kamel
* Abraham Thomas
}}
|location           = [[Toronto]], [[Canada]]
|area_served        = Worldwide
|key_people         = {{Plainlist|
* Tammer Kamel <small>(CEO)</small>
* Abraham Thomas <small>(CDO)</small>
}}
|industry           = [[Internet]]
|products           = Quandl Data Marketplace
|services           = Data [[subscriptions]]
|num_employees      = 16
|url                = {{URL|quandl.com}}
|programming_language = [[Ruby (programming language)|Ruby]] and [[Java (programming language)|Java]]
|website_type       = [[E-commerce]]
|language           = English
|launch_date        = {{start date and age|2013|01|01|df=yes}}
}}

'''Quandl''' ({{IPAc-en|ˈ|k|w|ɑː|n|d|əl}}) is a Toronto-based platform for financial, economic, and alternative data, serving investment professionals. Quandl sources data from over 500 publishers.<ref>{{Cite web|url=https://www.producthunt.com/tech/quandl|title=Quandl - Product Hunt|website=Product Hunt|language=en-US|access-date=2016-09-01}}</ref>  All Quandl's data are accessible via an [[API]].<ref>{{cite web |url= http://www.econometricsbysimulation.com/2013/05/quandl-package-5000000-free-datasets-at.html |title= Quandl Package - 5,000,000 free datasets at the tip of your fingers! |date= 5 May 2013 |publisher=EconBS}}</ref> API access is possible through packages for multiple programming languages including [[R (programming language)|R]], [[Python (programming language)|Python]], [[Matlab]], [[Maple (software)]] and [[Stata]].<ref>{{cite web |url= http://blogs.computerworld.com/business-intelligenceanalytics/21881/quandl-wikipedia-data |title= Quandl: Wikipedia for data  |date= 8 March 2013 |publisher=Computer World |last = Machlis |first = Sharon }}</ref><ref>{{cite web |url=https://www.quandl.com/tools/full-list |title=Full list of tools supported on Quandl}}</ref>

An Excel add-in allows access to data, including stock price information.
 
Quandl's sources include the [[United Nations|UN]], [[Worldbank]], [[CLS Group]], Zacks, and several hundred more.<ref>{{cite web |url= http://gigaom.com/2013/05/31/its-a-beautiful-thing-when-free-data-meets-free-analytics/ |title= It's a beautiful thing when free data meets free analytics |date= 31 May 2013 |publisher=Gigaom |last = Harris |first = Derrick }}</ref><ref>{{cite web |url= http://www.quandl.com/resources/data-sources |title= Quandl Data Sources}}</ref>

== History ==
Quandl was founded in 2012 by Tammer Kamel and Abraham Thomas.<ref>{{Cite web|url=https://www.quandl.com/about|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}</ref> In March 2013, Quandl raised $1.5m,<ref>{{Cite web|url=https://www.crunchbase.com/organization/quandl|title=Quandl {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-09-01}}</ref> in seed funding followed by a $5.4m Series A from [[August Capital]] in 2014.<ref>{{Cite web|url=http://blogs.wsj.com/venturecapital/2014/11/13/quandl-raises-5-4-million-for-its-financial-data-marketplace/|title=Quandl Raises $5.4 Million for Its Financial-Data Marketplace|last=Gage|first=Deborah|access-date=2016-09-01}}</ref>

Since its launch, Quandl has been discussed as a disruptive force in the anachronistic financial data sector.<ref>{{Cite web|url=http://mattturck.com/2014/03/19/can-the-bloomberg-terminal-be-toppled/|title=Can the Bloomberg Terminal be "Toppled"?|date=2014-03-19|website=Matt Turck|access-date=2016-09-01}}</ref> With over 100,000 users<ref>{{Cite web|url=https://www.integrity-research.com/new-data-provider-quandl/|title=New Data Provider Quandl Moves Toward Alternative Data • Integrity Research|language=en-US|access-date=2016-09-01}}</ref> Quandl is positioning itself as a possible replacement for both Bloomberg and Reuters terminals.<ref>{{Cite web|url=http://www.huffingtonpost.com/irene-aldridge/blindsided-by-innovation-_b_9025960.html|title=Blindsided by innovation like Bloomberg? Don't become a statistic.|last=AbleMarkets.com|first=Irene Aldridge Quantitative portfolio manager; MD at|last2=speaker|date=2016-01-22|website=The Huffington Post|access-date=2016-09-01|last3=author|last4=Trading'|first4='High-Frequency}}</ref> Quandl is an alternative for people who are unable to afford the expensive licensing fees of Bloomberg and Reuters.<ref>{{Cite web|url=https://openforum.hbs.org/challenge/understand-digital-transformation-of-business/data/quandl-a-marketplace-for-financial-data|title=Quandl: A Marketplace for Financial Data|access-date=2016-09-01}}</ref>

== Products ==
Quandl's main focus, and area of expertise, is in the realm of alternative data.<ref name=":0">{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2462823/quandl-embarks-on-quest-for-alternative-data|title=Quandl Embarks on Quest for Alternative Data|access-date=2016-09-01}}</ref> Quandl sells alternative datasets, defined as "any data that is not typically made available to Wall Street firms by traditional sources".<ref name=":0" /> Quandl "sources, evaluates and productizes undiscovered data" and then sells it to financial institutions, who use it to enhance their trading strategies.<ref>{{Cite web|url=https://www.quandl.com/institutions|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}</ref>

Quandl also offers market data through its marketplace. Some data sets are free while others require a subscription. Different datasets have different prices. They have hundreds of databases and providers ranging from stock price history to global fundamentals to commodities data to Asian market data.<ref>{{Cite web|url=http://www.quandl.com/vendors|title=Quandl Financial and Economic Data|website=www.quandl.com|access-date=2016-09-01}}</ref>

Quandl has an exclusive relationship with CLS Group in London, and is the only source of commercial [[Foreign exchange market|FX]] volume data<ref>{{Cite web|url=http://www.waterstechnology.com/inside-market-data/news/2465222/quandl-adds-cls-fx-trade-volume-data-to-online-platform|title=Quandl Adds CLS FX Trade, Volume Data to Online Platform|access-date=2016-09-01}}</ref>

== References ==
{{Reflist|30em}}

[[Category:Information retrieval systems]]
<=====doc_Id=====>:600
<=====title=====>:
Trip (search engine)
<=====text=====>:
{{Infobox software
|name = Trip
|logo =
|screenshot =
|caption =
|developer = Trip Database Ltd
|latest_release_version =
|latest_release_date =
|latest_preview_version =
|latest_preview_date =
|operating_system =
|genre = [[Search engine]]
|language =
|license = [[Freeware]]
|website = [http://www.tripdatabase.com Trip]
}}

'''Trip''' is a free [[Illness|clinical]] [[search engine]].  Its primary function is to help [[clinician]]s identify the best available evidence with which to answer clinical questions.  Its roots are firmly in the world of [[evidence-based medicine]].

==History==
The site was created in 1997 as a search tool to help the staff of ATTRACT<ref>[http://www.attract.wales.nhs.uk ATTRACT]</ref> answer clinical questions for [[General practitioner|GP]]s in [[Gwent (county)|Gwent]], [[South Wales]].  Shortly afterwards ''[[Bandolier (journal)|Bandolier]]'' highlighted the Trip Database and this helped establish the site.  In 2003, after a period of steady growth, Trip became a subscription-only service.  This was abandoned In September 2006 and since then the growth in usage has been significant. Originally "Trip" stood for Turning Research Into Practice, but the system is now simply called Trip.<ref>{{cite web |url= http://www.tripdatabase.com/about |title=About |work=Trip |publisher=Trip Database Ltd |accessdate=3 April 2013}}</ref>

==Process==
The core to Trip’s system is the identification and incorporation of new evidence.  The people behind Trip are heavily involved in clinical question answering systems (e.g., [[NLH Q&A Service]]).  Therefore, if resources are identified that are useful in the Q&A process they tend to be added to Trip.

==Users==
A site survey (September 2007) showed that the site was searched over 500,000 times per month, with 69% from [[health professional]]s and 31% from members of the public. Of the health professionals around 43% are doctors.  Most users come from either the United Kingdom or the United States.  In September 2008 the site was searched 1.4 million times. To date the site has been searched over 100 millions times.

==Recent updates==
At the end of 2012 Trip had a major upgrade which saw significant new enhancements:

* New content - widening the coverage
* New design
* Advanced search
* PICO search - to help users formulate focused searches
* Improved filtering
* Search history/timeline - recording all a user activity on the site
* Related articles

==Education tracker==
Trip has an education tracker which allows users to record their activity on Trip which can then be used, subject to local regulations, for revalidation/re-licensing.

==Future areas of work==
Trip is exploring numerous innovative technologies to improve the site, these include:
* Link out to full-text articles via Trip.
* RCT database.
* Rapid (within a week) systematic review quality reviews.
* Learning from users prior use of the site and that of similar users to improve search results.

==Trip Answers==
In November 2008, Trip released a new website, Trip Answers.  This is a repository of clinical Q&As from a variety of Q&A services. At launch it had over 5,000 Q&As and currently has over 6,300.  This content has been integrated into Trip.

==References==
{{reflist}}

==External links==
* [http://www.tripdatabase.com Trip]
* [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1852632/ Using the Turning Research Into Practice (TRIP) database: how do clinicians really search?] an evaluation of the website.
* [http://libguides.lhl.uab.edu/content.php?pid=108596&sid=1099056 Reviews: From Systematic to Narrative] review of the site
* [http://guides.library.manoa.hawaii.edu/content.php?pid=250484&sid=2157277 Evidence Based Pyramid] a pictorial representation of TRIP's approach to the evidence

[[Category:Medical websites]]
[[Category:Information retrieval systems]]
<=====doc_Id=====>:603
<=====title=====>:
Poliqarp
<=====text=====>:
'''Poliqarp''' is an [[open source]] [[search engine]] designed to process [[text corpus|text corpora]], among others the [[National Corpus of Polish]] created at the Institute of Computer Science, [[Polish Academy of Sciences]].

==Features==
* Custom [[query language]]
* Two-level [[regular expressions]]:
** operating at the level of characters in words
** operating at the level of words in statements/paragraphs
* Good performance
* Compact corpus representation (compared to similar projects)
* Portability across operating systems: [[Linux]]/[[BSD]]/[[Win32]]
* Lack of portability across [[endianness]] (current release works only on little endian devices)

==External links==
* [http://www.korpus.pl/index.php?lang=en&page=welcome Polish corpus website (in English)]
* [http://poliqarp.sourceforge.net/ Project website on SourceForge]
* [http://poliqarp.suxx.pl/ Search plugin for Firefox]

[[Category:Information retrieval systems]]
<=====doc_Id=====>:606
<=====title=====>:
Database search engine
<=====text=====>:
A '''database search engine''' is a [[search engine]] that operates on material stored in a digital [[database]].

== Search engines ==

Categories of search engine software include: 
* Web search or full-text search (e.g. [[Lucene]]).
* [[Database]] or [[structured data]] search (e.g. [[Dieselpoint]]).
* Mixed or [[enterprise search]] (e.g. [[Google Search Appliance]]). 

The largest online directories, such as [[Google]] and [[Yahoo]], utilize thousands of computers to process billions of website documents using [[web crawlers]] or [[spiders (software)]], returning results for thousands of searches per second. Processing high query volumes requires software to run in a distributed environment with redundancy.

== Components ==

Searching for textual content in [[databases]] or [[structured data]] formats (such as [[XML]] and [[Comma-separated values|CSV]]) presents special challenges and opportunities which specialized search engines resolve. [[Databases]] allow logical queries such as the use of multi-field [[Boolean logic]], while full-text searches do not. "Crawling" (a human by-eye search) is not necessary to find information stored in a database because the data is already structured. [[index (datatbase)|Indexing]] the data allows for faster searches.

Database search engines are usually included with major database software products. 

== Applications ==

Database search technology is used by large public and private entities including government database services, e-commerce companies, online advertising platforms, telecommunications service providers and other consumers with a need to access information in large repositories.

==See also==

*[[Outline of search engines]]
*[[List of search engines]]

==External links==
* [http://www.searchtools.com/info/database-search.html Searching for Text Information in Databases]

{{DEFAULTSORT:Search Engine Technology}}
[[Category:Information retrieval systems]]
<=====doc_Id=====>:609
<=====title=====>:
Sørensen–Dice coefficient
<=====text=====>:
The '''Sørensen–Dice index''', also known by other names (see [[Sørensen–Dice_coefficient#Name|Name]], below), is a [[statistic]] used for comparing the similarity of two [[Sample (statistics)|samples]]. It was independently developed by the [[botanist]]s [[Thorvald Sørensen]]<ref>{{cite journal |last=Sørensen |first=T. |year=1948 |title=A method of establishing groups of equal amplitude in [[plant sociology]] based on similarity of species and its application to analyses of the vegetation on Danish commons |journal=[[Kongelige Danske Videnskabernes Selskab]] |volume=5 |issue=4 |pages=1–34 |doi= }}</ref> and [[Lee Raymond Dice]],<ref>{{cite journal |last=Dice |first=Lee R. |title=Measures of the Amount of Ecologic Association Between Species |jstor=1932409 |journal=Ecology |volume=26 |issue=3 |year=1945 |pages=297–302 |doi=10.2307/1932409 }}</ref> who published in 1948 and 1945 respectively.
The Sørensen–Dice is also known as [[F1 score]] or Dice similarity coefficient (DSC).

==Name==
The index is known by several other names, usually '''Sørensen index''' or '''Dice's coefficient'''. Both names also see "similarity coefficient", "index", and other such variations. Common alternate spellings for Sørensen are Sorenson, Soerenson index and Sörenson index, and all three can also be seen with the –sen ending.

Other names include:
*[[Jan Czekanowski|Czekanowski]]'s binary (non-quantitative) index<ref name ="gallagher"/>

==Formula==
Sørensen's original formula was intended to be applied to presence/absence data, and is

:<math> QS =  \frac{2 |X \cap Y|}{|X|+ |Y|}</math>

where |''X''| and |''Y''| are the numbers of elements in the two samples. Based on what is written here,

:<math> DSC = \frac{2 TP}{2 TP + FP + FN}</math>,

as compared with the Jaccard index, which omits true negatives from both the numerator and the denominator. QS is the quotient of similarity and ranges between 0 and&nbsp;1.<ref>http://www.sekj.org/PDF/anbf40/anbf40-415.pdf</ref> It can be viewed as a similarity measure over sets.

Similarly to the [[Jaccard index]], the set operations can be expressed in terms of vector operations over binary vectors ''A'' and ''B'':

:<math>s_v = \frac{2 | A \cdot B |}{| A |^2 + | B |^2} </math>

which gives the same outcome over binary vectors and also gives a more general similarity metric over vectors in general terms.

For sets ''X'' and ''Y'' of keywords used in [[information retrieval]], the coefficient may be defined as twice the shared information (intersection) over the sum of cardinalities :<ref>{{cite book |last=van Rijsbergen |first=Cornelis Joost |year=1979
|title=Information Retrieval
|url=http://www.dcs.gla.ac.uk/Keith/Preface.html |publisher=Butterworths |location=London |isbn=3-642-12274-4 }}</ref>

When taken as a string similarity measure, the coefficient may be calculated for two strings, ''x'' and ''y'' using [[bigram]]s as follows:<ref>{{cite conference |last=Kondrak |first=Grzegorz |author2=Marcu, Daniel |author3= Knight, Kevin  |year=2003
|title=Cognates Can Improve Statistical Translation Models
|booktitle=Proceedings of HLT-NAACL 2003: Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics
|pages=46–48 |url=http://aclweb.org/anthology/N/N03/N03-2016.pdf}}</ref>

:<math>s = \frac{2 n_t}{n_x + n_y}</math>

where ''n''<sub>''t''</sub> is the number of character bigrams found in both strings, ''n''<sub>''x''</sub> is the number of bigrams in string ''x'' and ''n''<sub>''y''</sub> is the number of bigrams in string ''y''. For example, to calculate the similarity between:

:<code>night</code>
:<code>nacht</code>

We would find the set of bigrams in each word:
:{<code>ni</code>,<code>ig</code>,<code>gh</code>,<code>ht</code>}
:{<code>na</code>,<code>ac</code>,<code>ch</code>,<code>ht</code>}

Each set has four elements, and the intersection of these two sets has only one element: <code>ht</code>.

Inserting these numbers into the formula, we calculate, ''s''&nbsp;=&nbsp;(2&nbsp;·&nbsp;1)&nbsp;/&nbsp;(4&nbsp;+&nbsp;4)&nbsp;=&nbsp;0.25.

==Difference from Jaccard ==
This coefficient is not very different in form from the [[Jaccard index]].  However, since it doesn't satisfy the triangle inequality, it can be considered a [[Metric (mathematics)#Generalized metrics|semimetric]] version of the Jaccard index.<ref name ="gallagher"/>

The function ranges between zero and one, like Jaccard. Unlike Jaccard, the corresponding difference function

:<math>d = 1 -  \frac{2 | X \cap Y |}{| X | + | Y |} </math>

is not a proper distance metric as it does not possess the property of [[triangle inequality]].<ref name ="gallagher">Gallagher, E.D., 1999. [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.9.1334&rep=rep1&type=pdf COMPAH Documentation], University of Massachusetts, Boston</ref> The simplest counterexample of this is given by the three sets {a}, {b}, and {a,b}, the distance between the first two being 1, and the difference between the third and each of the others being one-third. To satisfy the triangle inequality, the sum of ''any'' two of these three sides must be greater than or equal to the remaining side. However, the distance between {a} and {a,b} plus the distance between {b} and {a,b} equals 2/3 and is therefore less than the distance between {a} and {b} which is 1.

==Applications==
The Sørensen–Dice coefficient is useful for ecological community data (e.g. Looman & Campbell, 1960<ref>[http://links.jstor.org/sici?sici=0012-9658%28196007%2941%3A3%3C409%3AAOSK%28F%3E2.0.CO%3B2-1 Looman, J. and Campbell, J.B. (1960) Adaptation of Sorensen's K (1948) for estimating unit affinities in prairie vegetation. Ecology 41 (3): 409–416.]</ref>). Justification for its use is primarily  empirical rather than theoretical (although it can be justified  theoretically as the intersection of two [[fuzzy set]]s<ref>[http://dx.doi.org/10.1007/BF00039905 Roberts, D.W. (1986) Ordination on the basis of fuzzy set theory. Vegetatio 66 (3): 123–131.]</ref>). As compared to [[Euclidean distance]], Sørensen distance retains sensitivity in more heterogeneous data sets and gives less weight to outliers.<ref>McCune, Bruce & Grace, James (2002) Analysis of Ecological Communities. Mjm Software Design; ISBN 0-9721290-0-6.</ref> Recently the Dice score (and its variations, e.g. logDice taking a logarithm of it) has become popular in computer [[lexicography]] for measuring the lexical association score of two given words.<ref>[http://nlp.fi.muni.cz/raslan/2008/raslan08.pdf#page=14 Rychlý, P. (2008) A lexicographer-friendly association score. Proceedings of the Second Workshop on Recent Advances in Slavonic Natural Language Processing RASLAN 2008: 6–9]</ref> It is also commonly used in [[Image segmentation]], in particular for comparing algorithm output against reference masks in medical applications{{Citation needed|reason=Some seminal works need to be cited to show how Dice coefficient is used|date=December 2016}}.

==Abundance version==
The expression is easily extended to [[Abundance (ecology)|abundance]] instead of presence/absence of species. This quantitative version is known by several names:
* Quantitative Sørensen–Dice index<ref name ="gallagher"/>
* Quantitative Sørensen index<ref name ="gallagher"/>
* Quantitative Dice index<ref name ="gallagher"/>
* [[Bray–Curtis dissimilarity|Bray–Curtis similarity]] (1 minus the ''Bray-Curtis dissimilarity'')<ref name ="gallagher"/>
* [[Jan Czekanowski|Czekanowski]]'s quantitative index<ref name ="gallagher"/>
* Steinhaus index<ref name ="gallagher"/>
* [[E. C. Pielou|Pielou]]'s percentage similarity<ref name ="gallagher"/>
* 1 minus the [[Hellinger distance]]<ref>{{cite journal |first=J. Roger |last=Bray |first2=J. T. |last2=Curtis |year=1957 |title=An Ordination of the Upland Forest Communities of Southern Wisconsin |journal=Ecological Monographs |volume=27 |issue=4 |pages=326–349 |doi=10.2307/1942268 }}</ref>

==See also==
* [[Correlation]]
* [[Jaccard index]]
* [[Hamming distance]]
* [[Mantel test]]
* [[Morisita's overlap index]]
* [[Most frequent k characters]]
* [[Overlap coefficient]]
* [[Renkonen similarity index]] (due to [[Olavi Renkonen]])
* [[Tversky index]]
* [[Universal adaptive strategy theory (UAST)]]

==References==
{{reflist}}

==External links==
{{Wikibooks|Algorithm implementation|Strings/Dice's coefficient|Dice's coefficient}}

{{DEFAULTSORT:Sorensen-Dice coefficient}}
[[Category:Information retrieval evaluation]]
[[Category:String similarity measures]]
[[Category:Measure theory]]
<=====doc_Id=====>:612
<=====title=====>:
Champion list
<=====text=====>:
{{orphan|date=January 2011}}

A '''champion list''', also called '''top doc''' or '''fancy list''' is a precomputed list sometimes used with the [[vector space model]] to avoid computing relevancy rankings for all documents each time a document collection is queried. The champion list contains a set of n documents with the highest weights for the given term. The number n can be chosen to be different for each term and is often higher for rarer terms. The weights can be calculated by for example [[tf-idf]].

[[Category:Information retrieval evaluation]]


{{computing-stub}}
<=====doc_Id=====>:615
<=====title=====>:
Discounted cumulative gain
<=====text=====>:
'''Discounted cumulative gain''' ('''DCG''') is a measure of ranking quality. In [[information retrieval]], it is often used to measure effectiveness of [[World Wide Web|web]] [[search engine]] [[algorithm]]s or related applications. Using a [[Relevance (information retrieval)|graded relevance]] scale of documents in a search engine result set, DCG measures the usefulness, or ''gain'', of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom with the gain of each result discounted at lower ranks.<ref>Kalervo Jarvelin, Jaana Kekalainen: Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422–446 (2002)</ref>

== Overview ==

Two assumptions are made in using DCG and its related measures.

# Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)
# Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents.

DCG originates from an earlier, more primitive, measure called Cumulative Gain.

=== Cumulative Gain ===

Cumulative Gain (CG) is the predecessor of DCG and does not include the position of a result in the consideration of the usefulness of a result set. In this way, it is the sum of the graded relevance values of all results in a search result list. The CG at a particular rank position <math>p</math> is defined as:

:<math> \mathrm{CG_{p}} = \sum_{i=1}^{p} rel_{i} </math>

Where <math>rel_{i}</math> is the graded relevance of the result at position <math>i</math>.

The value computed with the CG function is unaffected by changes in the ordering of search results. That is, moving a highly relevant document <math>d_{i}</math> above a higher ranked, less relevant, document <math>d_{j}</math> does not change the computed value for CG. Based on the two assumptions made above about the usefulness of search results, DCG is used in place of CG for a more accurate measure.

=== Discounted Cumulative Gain ===

The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result. The discounted CG accumulated at a particular rank position <math>p</math> is defined as:<ref name="stanfordireval">{{cite web|title=Introduction to Information Retrieval - Evaluation|url=http://www.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf|publisher=Stanford University|accessdate=23 March 2014|date=21 April 2013}}</ref>

:<math> \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{rel_{i}}{\log_{2}(i+1)} = rel_1 + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}(i+1)} </math>

Previously there has not been  any theoretically sound justification for using a [[logarithm]]ic reduction factor<ref name=CMS2009>{{cite book | title=Search Engines: Information Retrieval in Practice |author1=B. Croft |author2=D. Metzler |author3=T. Strohman |year=2010 | publisher=''Addison Wesley"}}</ref> other than the fact that it produces a smooth reduction. But Wang et al. (2013)<ref>Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Wei Chen, Tie-Yan Liu. 2013. A Theoretical Analysis of Normalized Discounted Cumulative Gain (NDCG) Ranking Measures. In Proceedings of the 26th Annual Conference on Learning Theory (COLT 2013).</ref> give theoretical guarantee for using the logarithmic reduction factor in NDCG. The authors show that for every pair of substantially different ranking functions, the NDCG can decide which one is better in a consistent manner.

An alternative formulation of DCG<ref>Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. 2005. Learning to rank using gradient descent. In Proceedings of the 22nd international conference on Machine learning (ICML '05). ACM, New York, NY, USA, 89-96. DOI=10.1145/1102351.1102363 http://doi.acm.org/10.1145/1102351.1102363</ref> places stronger emphasis on retrieving relevant documents:

:<math> \mathrm{DCG_{p}} = \sum_{i=1}^{p} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} </math>

The latter formula is commonly used in industry including major web search companies<ref name="stanfordireval"/> and data science competition platform such as Kaggle.<ref>{{cite web|title=Normalized Discounted Cumulative Gain|url=https://www.kaggle.com/wiki/NormalizedDiscountedCumulativeGain|accessdate=23 March 2014}}</ref>

These two formulations of DCG are the same when the relevance values of documents are [[binary function|binary]];<ref name=CMS2009/>{{rp|320}} <math>rel_{i} \in \{0,1\}</math>.

Note that Croft et al. (2010) and Burges et al. (2005) present the second DCG with a log of base e, while both versions of DCG above use a log of base 2.  When computing NDCG with the second formulation of DCG, the base of the log does not matter, but the base of the log does affect the value of NDCG for the first formulation.  Clearly, the base of the log affects the value of DCG in both formulations.<!-- Not very clear, does it affect or no the value of DCG? -->

=== Normalized DCG ===

Search result lists vary in length depending on the [[Web search query|query]]. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of <math>p</math> should be normalized across queries. This is done by sorting all '''relevant''' documents in the corpus by their relative relevance, producing the maximum possible DCG through position <math>p</math>, also called Ideal DCG (IDCG) through that position. For a query, the ''normalized discounted cumulative gain'', or nDCG, is computed as:

:<math> \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG_{p}} </math>,

where:

:<math> \mathrm{IDCG_{p}} = \sum_{i=1}^{|REL|} \frac{ 2^{rel_{i}} - 1 }{ \log_{2}(i+1)} </math>

and |REL| represents the list of relevant documents (ordered by their relevance) in the corpus up to position p.

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a search engine's ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

The main difficulty encountered in using nDCG is the unavailability of an ideal ordering of results when only partial [[relevance feedback]] is available.

== Example ==

Presented with a list of documents in response to a search query, an experiment participant is asked to judge the relevance of each document to the query. Each document is to be judged on a scale of 0-3 with 0 meaning not relevant, 3 meaning highly relevant, and 1 and 2 meaning "somewhere in between". For the documents ordered by the ranking algorithm as

:<math> D_{1}, D_{2}, D_{3}, D_{4}, D_{5}, D_{6} </math>

the user provides the following relevance scores:

:<math> 3, 2, 3, 0, 1, 2 </math>

That is: document 1 has a relevance of 3, document 2 has a relevance of 2, etc. The Cumulative Gain of this search result listing is:

:<math> \mathrm{CG_{6}} = \sum_{i=1}^{6} rel_{i} = 3 + 2 + 3 + 0 + 1 + 2 = 11</math>

Changing the order of any two documents does not affect the CG measure. If <math>D_3</math> and <math>D_4</math> are switched, the CG remains the same, 11. DCG is used to emphasize highly relevant documents appearing early in the result list. Using the logarithmic scale for reduction, the DCG for each result in order is:


{| class="wikitable" border="1"
|-
! <math>i</math>
! <math>rel_{i}</math>
! <math>\log_{2}(i+1)</math>
! <math> \frac{rel_{i}}{\log_{2}(i+1)} </math>
|-
| 1
| 3
| 1
| 3
|-
| 2
| 2
| 1.585
| 1.262
|-
| 3
| 3
| 2
| 1.5
|-
| 4
| 0
| 2.322
| 0
|-
| 5
| 1
| 2.585
| 0.387
|-
| 6
| 2
| 2.807
| 0.712
|}

So the <math>DCG_{6}</math> of this ranking is:

:<math> \mathrm{DCG_{6}} = \sum_{i=1}^{6} \frac{rel_{i}}{\log_{2}(i+1)} = 3 + 1.262 + 1.5 + 0 + 0.387 + 0.712 = 6.861</math>

Now a switch of <math>D_3</math> and <math>D_4</math> results in a reduced DCG because a less relevant document is placed higher in the ranking; that is, a more relevant document is discounted more by being placed in a lower rank.

The performance of this query to another is incomparable in this form since the other query may have more results, resulting in a larger overall DCG which may not necessarily be better. In order to compare, the DCG values must be normalized.

To normalize DCG values, an ideal ordering for the given query is needed. For this example, that ordering would be the [[Monotonic|monotonically decreasing]] sort of the relevance judgments provided by the experiment participant, which is:

:<math> 3, 3, 2, 2, 1, 0 </math>

The DCG of this ideal ordering, or ''IDCG (Ideal DCG)'' , is then:

:<math> \mathrm{IDCG_{6}} = 7.141 </math>

And so the nDCG for this query is given as:

:<math> \mathrm{nDCG_{6}} = \frac{DCG_{6}}{IDCG_{6}} = \frac{6.861}{7.141} = 0.961 </math>

== Limitations ==
# Normalized DCG metric does not penalize for bad documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,0 </math> respectively, both would be considered equally good even if the latter contains a bad result. One way to take into account this limitation is to use <math>1 - 2^{rel_{i}}</math> in the numerator for scores for which we want to penalize and <math>2^{rel_{i}} - 1</math> for all others. For example, for the ranking judgments <math>Excellent, Fair, Bad</math> one might use numerical scores <math>1,0,-1</math> instead of <math>2,1,0</math>.
# Normalized DCG does not penalize for missing documents in the result. For example, if a query returns two results with scores <math> 1,1,1 </math> and <math> 1,1,1,1,1 </math> respectively, both would be considered equally good. One way to take into account this limitation is to enforce fixed set size for the result set and use minimum scores for the missing documents. In previous example, we would use the scores <math> 1,1,1,0,0 </math> and <math> 1,1,1,1,1 </math> and quote nDCG as nDCG@5.
# Normalized DCG may not be suitable to measure performance of queries that may typically often have several equally good results. This is especially true when this metric is limited to only first few results as it is done in practice. For example, for queries such as "restaurants" nDCG@1 would account for only first result and hence if one result set contains only 1 restaurant from the nearby area while the other contains 5, both would end up having same score even though latter is more comprehensive.

== References ==
{{Reflist|1}}

[[Category:Information retrieval evaluation]]
<=====doc_Id=====>:618
<=====title=====>:
Matthews correlation coefficient
<=====text=====>:
The '''Matthews correlation coefficient''' is used in [[machine learning]] as a measure of the quality of binary (two-class) [[Binary classification|classifications]], introduced by biochemist [[Brian Matthews (biochemist)|Brian W. Matthews]] in 1975.<ref name="Matthews1975">{{cite journal|last=Matthews|first=B. W.|title=Comparison of the predicted and observed secondary structure of T4 phage lysozyme|journal=Biochimica et Biophysica Acta (BBA) - Protein Structure|date=1975|volume=405|issue=2|pages=442–451|doi=10.1016/0005-2795(75)90109-9}}</ref> It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient between the observed and predicted binary classifications; it returns a value between &minus;1 and +1. A coefficient of +1 represents a perfect prediction, 0 no better than random prediction and &minus;1 indicates total disagreement between prediction and observation. The statistic is also known as the [[phi coefficient]]. MCC is related to the [[Pearson's chi-square test|chi-square statistic]] for a 2×2 [[contingency table]]

: <math>|\text{MCC}| = \sqrt{\frac{\chi^2}{n}}</math>

where ''n'' is the total number of observations.

While there is no perfect way of describing the [[confusion matrix]] of true and false positives and negatives by a single number, the Matthews correlation coefficient is generally regarded as being one of the best such measures.<ref name="Powers2011"/> Other measures, such as the proportion of correct predictions (also termed [[accuracy]]), are not useful when the two classes are of very different sizes. For example, assigning every object to the larger set achieves a high proportion of correct predictions, but is not generally a useful classification.

The MCC can be calculated directly from the [[confusion matrix]] using the formula:

: <math>
\text{MCC} = \frac{ TP \times TN - FP \times FN } {\sqrt{ (TP + FP) ( TP + FN ) ( TN + FP ) ( TN + FN ) } }
</math>

In this equation, ''TP'' is the number of [[true positive]]s, ''TN'' the number of [[true negative]]s, ''FP'' the number of [[false positive]]s and ''FN'' the number of [[false negative]]s. If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.

The original formula as given by Matthews was:<ref name=Matthews1975 />
: <math>
\text{N} = TN + TP + FN + FP
</math>
: <math>
\text{S} = \frac{ TP + FN } { N }
</math>
: <math>
\text{P} = \frac{ TP + FP } { N }
</math>
: <math>
\text{MCC} = \frac{ TP / N - S \times P } {\sqrt{ P S  ( 1 - S)  ( 1 - P ) } }
</math>

This is equal to the formula given above. As a [[Correlation and dependence|correlation coefficient]], the Matthews correlation coefficient is the [[geometric mean]] of the [[regression coefficient]]s of the problem and its [[Dual (mathematics)|dual]]. The component regression coefficients of the Matthews correlation coefficient are [[Markedness]] (Δp) and [[Youden's J statistic]] ([[Informedness]] or Δp').<ref name="Powers2011">{{cite journal |first=David M W |last=Powers |date=2011 |title=Evaluation: From Precision, Recall and F-Measure  to ROC, Informedness, Markedness & Correlation |journal=Journal of Machine Learning Technologies |volume=2 |issue=1 |pages=37–63 |url=http://www.flinders.edu.au/science_engineering/fms/School-CSEM/publications/tech_reps-research_artfcts/TRRA_2007.pdf}}</ref><ref name="Perruchet2004">{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Markedness]] and [[Informedness]] correspond to different directions of information flow and generalize [[Youden's J statistic]], the deltap statistics and (as their geometric mean) the Matthews Correlation Coefficient to more than two classes.<ref name="Powers2011"/>

== Confusion Matrix ==
{{main article|Confusion matrix}}

{{Confusion matrix terms|recall=}}

Let us define an experiment from '''P''' positive instances and '''N''' negative instances for some condition. The four outcomes can be formulated in a 2×2 ''[[contingency table]]'' or ''[[confusion matrix]]'', as follows:

{{DiagnosticTesting_Diagram}}

== Multiclass Case ==
The Matthews correlation coefficient has been generalized to the multiclass case. This generalization was called the  <math>R_K</math> statistic (for K different classes) by the author, and defined in terms of a <math>K\times K</math> confusion matrix <math>C</math>
<ref name="gorodkin2004comparing">{{cite journal|last=Gorodkin|first=Jan|title=Comparing two K-category assignments by a K-category correlation coefficient|journal=Computational biology and chemistry|date=2004|volume=28|number=5|pages=367–374|publisher=Elsevier}}</ref>
.<ref name="GorodkinRk2006">{{cite web|last1=Gorodkin|first1=Jan|title=The Rk Page|url=http://rk.kvl.dk/introduction/index.html|website=The Rk Page|accessdate=28 December 2016}}</ref>

:<math>
\text{MCC} = \frac{\sum_{k}\sum_{l}\sum_{m} C_{kk}C_{lm} - C_{kl}C_{mk}}{
\sqrt{
\sum_{k}(\sum_l C_{kl} )(\sum_{k' | k' \neq k}\sum_{l'} C_{k'l'})
}
\sqrt{
\sum_{k}(\sum_l C_{lk} )(\sum_{k' | k' \neq k}\sum_{l'} C_{l'k'})
}
}
</math>

When there are more than two labels the MCC will no longer range between -1 and +1. Instead the minimum value will be between -1 and 0 depending on the true distribution. The maximum value is always +1. 

<!-- 
TODO: potentially un-comment later, for now just stick with referenced version

This formula can be more easily understood by defining intermediate variables: 
* <math>t_k=\sum_i C_{ik}</math> the number of times class k truly occurred, 
* <math>p_k=\sum_i C_{ki}</math> the number of times class k was predicted, 
* <math>c=\sum_{k} C_{kk}</math> the total number of samples correctly predicted, 
* <math>s=\sum_i \sum_j C_{ij}</math> the total number of samples. This allows the formula to be expressed as:

:<math>
\text{MCC} = \frac{cs - \vec{t} \cdot \vec{p}}{
\sqrt{s^2 - \vec{p} \cdot \vec{p}}
\sqrt{s^2 - \vec{t} \cdot \vec{t}}
}
</math>
-->

== See also ==
* [[Phi coefficient]]
* [[F1 score]]
* [[Cramér's V (statistics)|Cramér's V]], a similar measure of association between nominal variables.
* [[Cohen's kappa]]

== References ==

{{Reflist}}

<!--should reference in the main text  === General References ===
* [[Pierre Baldi|Baldi, P.]]; Brunak, S.; Chauvin, Y.; Andersen, C. A. F.; Nielsen, H. Assessing the accuracy of prediction algorithms for classification: an overview" ''Bioinformatics'' 2000, 16, 412&ndash;424. [http://bioinformatics.oxfordjournals.org/cgi/content/abstract/16/5/412]
* Carugo, O., Detailed estimation of bioinformatics prediction reliability through the Fragmented Prediction Performance Plots. BMC Bioinformatics 2007. [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2148069/]
-->

{{DEFAULTSORT:Matthews Correlation Coefficient}}
[[Category:Machine learning]]
[[Category:Information retrieval evaluation]]
[[Category:Statistical classification]]
[[Category:Computational chemistry]]
[[Category:Cheminformatics]]
[[Category:Bioinformatics]]
[[Category:Statistical ratios]]
[[Category:Summary statistics for contingency tables]]
<=====doc_Id=====>:621
<=====title=====>:
Universal IR Evaluation
<=====text=====>:
{{Multiple issues|
{{refimprove|date=April 2011}}
{{orphan|date=April 2010}}
}}

In [[computer science]], ''universal [[Information retrieval evaluation|IR (information retrieval) evaluation]]'' aims to develop measures of database retrieval performance that shall be comparable across all information retrieval tasks.

==Measures of "relevance"==
[[Information retrieval evaluation|IR (information retrieval) evaluation]] begins whenever a user submits a query (search term) to a [[database]]. If the user is able to determine the [[Relevance (information retrieval)|relevance]] of each document in the database (relevant or not relevant), then for each query, the complete set of documents is naturally divided into four distinct (mutually exclusive) subsets: relevant documents that are retrieved, not relevant documents that are retrieved, relevant documents that are not retrieved, and not relevant documents that are not retrieved. These four subsets (of documents) are denoted by the letters a,b,c,d respectively and are called Swets variables, named after their inventor.<ref>Swets, J.A. (1969). Effectiveness of information retrieval methods. ''American Documentation, 20''(1), 72-89.</ref>

In addition to the Swets definitions, four relevance metrics have also been defined: [[Precision (information retrieval)|Precision]] refers to the fraction of relevant documents that are retrieved (a/(a+b)), and [[Precision (information retrieval)|Recall]] refers to the fraction of retrieved documents that are relevant (a/(a+c)). These are the most commonly used and well-known relevance metrics found in the IR evaluation literature. Two less commonly used metrics include the Fallout, i.e., the fraction of not relevant documents that are retrieved (b/(b+d)), and the Miss, which refers to the fraction of relevant documents that are not retrieved (c/(c+d)) during any given search.

==Universal IR evaluation techniques==
Universal IR evaluation addresses the mathematical possibilities and relationships among the four relevance metrics Precision, Recall, Fallout and Miss, denoted by P, R, F and M, respectively. One aspect of the problem involves finding a mathematical derivation of a complete set of universal IR evaluation points.<ref>Schatkun, M. (2010). A Second look at Egghe's universal IR surface and a simple derivation of a complete set of universal IR evaluation points. ''Information Processing & Management, 46''(1), 110-114.</ref> The complete set of 16 points, each one a quadruple of the form (P,R,F,M), describes all the possible universal IR outcomes. For example, many of us have had the experience of querying a database and not retrieving any documents at all. In this case, the Precision would take on the undetermined form 0/0, the Recall and Fallout would both be zero, and the Miss would be any value greater than zero and less than one (assuming a mix of relevant and not relevant documents were in the database, none of which were retrieved). This universal IR evaluation point would thus be denoted by (0/0, 0, 0, M), which represents only one of the 16 possible universal IR outcomes.

The mathematics of universal IR evaluation is a fairly new subject since the relevance metrics P,R,F,M were not analyzed collectively until recently (within the past decade). A lot of the theoretical groundwork has already been formulated, but new insights in this area await discovery. For a detailed mathematical analysis, a query in the [[ScienceDirect]] database for "universal IR evaluation" retrieves several relevant peer-reviewed papers.

==See also==
* [[Information retrieval]]
* [[Web search query]]

==References==
{{Reflist}}

==External links==
* [http://www.sciencedirect.com Science Direct]

{{DEFAULTSORT:Universal Ir Evaluation}}
[[Category:Databases]]
[[Category:Information retrieval evaluation]]
<=====doc_Id=====>:624
<=====title=====>:
Category:Ranking functions
<=====text=====>:
[[Category:Information retrieval techniques]]
[[Category:Rankings]]
<=====doc_Id=====>:627
<=====title=====>:
Ordered weighted averaging aggregation operator
<=====text=====>:
In applied mathematics – specifically in [[fuzzy logic]] – the '''ordered weighted averaging (OWA) operators''' provide a [[parameter]]ized class of mean type aggregation operators. They were introduced by [[Ronald R. Yager]]. Many notable mean operators such as the max, [[arithmetic average]], median and min, are members of this class. They have been widely used in [[computational intelligence]] because of their ability to model linguistically expressed aggregation instructions.

== Definition ==

Formally an '''OWA''' operator of dimension <math> \ n </math> is a mapping <math> F: R_n \rightarrow R </math> that has an associated collection of weights <math> \  W = [w_1, \ldots, w_n] </math> lying in the unit interval and summing to one and with 		

:<math> F(a_1, \ldots , a_n) =  \sum_{j=1}^n  w_j b_j</math>

where <math> b_j </math> is the ''j''<sup>th</sup> largest of the <math> a_i </math>.

By choosing different ''W'' one can implement different aggregation operators. The OWA operator is a non-linear operator as a result of the process of determining the ''b''<sub>''j''</sub>.

== Properties ==

The OWA operator is a mean operator. It is [[Bounded operator|bounded]], [[monotonic]], [[symmetric operator|symmetric]], and [[idempotent]], as defined below.

{|class="wikitable"
|[[Bounded operator|Bounded]]
|<math>   \min(a_1, \ldots, a_n) \le F(a_1, \ldots, a_n) \le \max(a_1, \ldots, a_n) </math>
|-
|[[Monotonic]]
|<math>   F(a_1, \ldots, a_n) \ge F(g_1, \ldots, g_n) </math> if <math> a_i \ge g_i </math> for <math>\ i = 1,2,\ldots,n </math>
|-
|[[symmetric operator|Symmetric]]
|<math>   F(a_1, \ldots, a_n)  = F(a_\boldsymbol{\pi(1)}, \ldots, a_\boldsymbol{\pi(n)})</math> if <math>\boldsymbol{\pi} </math> is a permutation map
|-
|[[Idempotent]]
|<math>  \ F(a_1, \ldots, a_n)  =  a </math> if all <math> \ a_i = a </math>
|}

== Notable OWA operators ==
:<math> \ F(a_1, \ldots, a_n) = \max(a_1, \ldots, a_n) </math> if <math> \ w_1 = 1 </math> and <math> \ w_j = 0 </math> for <math> j \ne 1 </math>

:<math> \ F(a_1, \ldots, a_n) = \min(a_1, \ldots, a_n) </math> if <math> \ w_n = 1 </math> and <math> \ w_j = 0 </math> for <math> j \ne n </math>

== Characterizing features ==

Two features have been used to characterize the OWA operators. The first is the attitudinal character(orness).

This is defined as
:<math>A-C(W)= \frac{1}{n-1} \sum_{j=1}^n (n - j) w_j. </math>

It is known that <math> A-C(W) \in [0, 1] </math>.

In addition ''A''&nbsp;&minus;&nbsp;''C''(max) = 1, A&nbsp;&minus;&nbsp;C(ave) = A&nbsp;&minus;&nbsp;C(med) = 0.5 and A&nbsp;&minus;&nbsp;C(min) = 0. Thus the A&nbsp;&minus;&nbsp;C goes from 1 to 0 as we go from Max to Min aggregation. The attitudinal character characterizes the similarity of aggregation to OR operation(OR is defined as the Max).

The second feature is the dispersion. This defined as

:<math>H(W) = -\sum_{j=1}^n w_j \ln (w_j).</math>

An alternative definition is <math>E(W) = \sum_{j=1}^n w_j^2 .</math> The dispersion characterizes how uniformly the arguments are being used
ÀĚ

== A literature survey: OWA (1988-2014)==
The historical reconstruction of scientific development of the OWA field, the identification of the dominant direction of knowledge accumulation that emerged since the publication of the first OWA paper, and to discover the most active lines of research has recently been published, (see: http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full). The results suggest, as expected, that Yager's paper[1] (IEEE Trans. Systems Man Cybernet, 18(1), 183–190, 1988) is the most influential paper and the starting point of all other research using OWA. Starting from his contribution, other lines of research developed and we describe them. Full list of papers published in OWA is also available at http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full)

== Type-1 OWA aggregation operators ==

The above Yager's OWA operators are used to aggregate the crisp values. Can we aggregate fuzzy sets in the OWA mechanism ? The
'''[[Type-1 OWA operators]]''' have been proposed for this purpose. So the '''[[type-1 OWA operators]]''' provides us with a new technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and data mining, where these uncertain objects are modelled by fuzzy sets.

The '''[[Type-1 OWA operators|type-1 OWA operator]]''' is defined according to the alpha-cuts of fuzzy sets as follows:

Given the ''n'' linguistic weights <math>\left\{ {W^i} \right\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\;\;1]</math>, then for each <math>\alpha \in [0,\;1]</math>, an <math>\alpha </math>-level type-1 OWA operator with <math>\alpha </math>-level sets <math>\left\{ {W_\alpha ^i } \right\}_{i = 1}^n </math> to aggregate the <math>\alpha </math>-cuts of fuzzy sets <math>\left\{ {A^i} \right\}_{i =1}^n </math> is given as

: <math>
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}</math>

where <math>W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}</math>, and <math>\sigma :\{\;1, \ldots ,n\;\} \to \{\;1, \ldots ,n\;\}</math> is a permutation function such that <math>a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \ldots ,n - 1</math>, i.e., <math>a_{\sigma (i)} </math> is the <math>i</math>th largest
element in the set <math>\left\{ {a_1 , \ldots ,a_n } \right\}</math>.

The computation of the '''[[Type-1 OWA operators|type-1 OWA]]''' output is implemented by computing the left end-points and right end-points of the intervals <math>\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)</math>:
<math>\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_{-} </math> and <math>
\Phi _\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right)_ {+},</math>
where <math>A_\alpha ^i=[A_{\alpha-}^i, A_{\alpha+}^i], W_\alpha ^i=[W_{\alpha-}^i, W_{\alpha+}^i]</math>. Then membership function of resulting aggregation fuzzy set is:

:<math>\mu _{G} (x) = \mathop \vee _{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha </math>

For the left end-points, we need to solve the following programming problem:

:<math> \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \min\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } </math>

while for the right end-points, we need to solve the following programming problem:

:<math>\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \max\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } </math>

[http://dx.doi.org/10.1109/TKDE.2010.191 This paper] has presented a fast method to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently.

== References ==

* Yager, R. R., "On ordered weighted averaging aggregation operators in multi-criteria decision making," IEEE Transactions on Systems, Man and Cybernetics 18, 183–190, 1988.
* Yager, R. R. and Kacprzyk, J., [http://www.amazon.com/dp/079239934X The Ordered Weighted Averaging Operators: Theory and Applications], Kluwer: Norwell, MA, 1997.
* Liu, X., "The solution equivalence of minimax disparity and minimum variance problems for OWA operators," International Journal of Approximate Reasoning 45, 68–81, 2007.
* Emrouznejad (2009) SAS/OWA: ordered weighted averaging in SAS optimization, Soft Computing [http://www.springerlink.com/content/7277l73334r108x5/]
* Emrouznejad, A. and M. Marra (2014), Ordered Weighted Averaging Operators 1988–2014: A citation-based literature survey, International Journal of Intelligent Systems, 29:994-1014 [http://onlinelibrary.wiley.com/doi/10.1002/int.21673/full  & http://onlinelibrary.wiley.com/store/10.1002/int.21673/asset/supinfo/int21673-sup-0001-SupMat.docx?v=1&s=c0d8bdd220a31c876eb5885521cfa16d191f334d].
* Torra, V. and Narukawa, Y., Modeling Decisions: Information Fusion and Aggregation Operators, Springer: Berlin, 2007.
* Majlender, P., "OWA operators with maximal Rényi entropy," Fuzzy Sets and Systems 155, 340–360, 2005.
* Szekely, G. J. and Buczolich, Z., " When is a weighted average of ordered sample elements a maximum likelihood estimator of the location parameter?" Advances in Applied Mathematics 10, 1989, 439–456.
* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers," Fuzzy Sets and Systems, Vol.159, No.24, pp.&nbsp;3281–3296, 2008 [http://dx.doi.org/10.1016/j.fss.2008.06.018]
* S.-M. Zhou, F. Chiclana, R. I. John and J. M. Garibaldi, "Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments," IEEE Transactions on Knowledge and Data Engineering, vol. 23, no.10, 2011, pp.&nbsp;1455–1468.[http://dx.doi.org/10.1109/TKDE.2010.191]
* S.-M. Zhou, R. I. John, F. Chiclana and J. M. Garibaldi, "On aggregating uncertain information by type-2 OWA operators for soft decision making," International Journal of Intelligent Systems, vol. 25, no.6, pp.&nbsp;540–558, 2010.[http://dx.doi.org/10.1002/int.20420]

[[Category:Artificial intelligence]]
[[Category:Logic in computer science]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:630
<=====title=====>:
Vocabulary mismatch
<=====text=====>:
{{refimprove|date=June 2015}}
'''Vocabulary mismatch''' is a common phenomenon in the usage of natural languages, occurring when different people name the same thing or concept differently.

Furnas et al. (1987) were perhaps the first to quantitatively study the vocabulary mismatch problem.<ref>Furnas, G., et al, The Vocabulary Problem in Human-System Communication, Communications of the ACM, 1987, 30(11), pp. 964-971.</ref>  Their results show that on average 80% of the times different people (experts in the same field) will name the same thing differently.  There are usually tens of possible names that can be attributed to the same thing.  This research motivated the work on [[latent semantic indexing]].

The vocabulary mismatch between user created queries and relevant documents in a corpus causes the term mismatch problem in [[information retrieval]].  Zhao and Callan (2010)<ref>Zhao, L. and Callan, J., Term Necessity Prediction, Proceedings of the 19th ACM Conference on Information and Knowledge Management (CIKM 2010). Toronto, Canada, 2010.</ref> were perhaps the first to quantitatively study the vocabulary mismatch problem in a retrieval setting.  Their results show that an average query term fails to appear in 30-40% of the documents that are relevant to the user query.  They also showed that this probability of mismatch is a central probability in one of the fundamental probabilistic retrieval models, the [[Binary Independence Model]].  They developed novel term weight prediction methods that can lead to potentially 50-80% accuracy gains in retrieval over strong keyword retrieval models.  Further research along the line shows that expert users can use Boolean Conjunctive Normal Form expansion to improve retrieval performance by 50-300% over unexpanded keyword queries.<ref name="cnf">Zhao, L. and Callan, J., Automatic term mismatch diagnosis for selective query expansion, SIGIR 2012.</ref>

== Techniques that solve mismatch ==

* [[Stemming]]
* [[Full-text indexing]] instead of only indexing keywords or abstracts
* Indexing text on inbound links from other documents (or other social tagging
* [[Query expansion]].  A 2012 study by Zhao and Callan<ref name="cnf"/> using expert created manual [[Conjunctive normal form]] queries has shown that searchonym expansion in the Boolean conjunctive normal form is much more effective than the traditional bag of word expansion e.g. [[Rocchio algorithm|Rocchio expansion]].
* Translation-based models

== References ==

{{Reflist}}

[[Category:Linguistic research]]
[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]
<=====doc_Id=====>:633
<=====title=====>:
Thesaurus (information retrieval)
<=====text=====>:
{{about|thesauri used to support indexing, tagging or searching for information|thesauri used in general/literary applications|Thesaurus|the Clare Fischer album|Thesaurus (album)}}

In the context of [[information retrieval]], a '''thesaurus''' (plural: "thesauri") is a form of controlled vocabulary that seeks to dictate semantic manifestations of [[metadata]] in the indexing of content objects. A thesaurus serves to minimise semantic ambiguity by ensuring uniformity and consistency in the storage and retrieval of the manifestations of content objects. ANSI/NISO Z39.19-2005 defines a content object as "any item that is to be described for inclusion in an information retrieval system, website, or other source of information".<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.11</ref> The thesaurus aids the assignment of preferred terms to convey semantic metadata associated with the content object.<ref>ANSI & NISO 2005, Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies, NISO, Maryland, U.S.A, p.12</ref>

A thesaurus serves to guide both an indexer and a searcher in selecting the same preferred term or combination of preferred terms to represent a given subject. [[ISO 25964]], the international standard for information retrieval thesauri, defines a thesaurus as a “controlled and structured vocabulary in which concepts are represented by terms, organized so that relationships between concepts are made explicit, and preferred terms are accompanied by lead-in entries for synonyms or quasi-synonyms.”

A thesaurus is composed by at least three elements: 1-a list of words (or terms), 2-the relationship amongst the words (or terms), indicated by their hierarchical relative position (e.g. parent/broader term; child/narrower term, synonym, etc.), 3-a set of rules on how to use the thesaurus.

== History ==
Wherever there have been large collections of information, whether on paper or in computers, scholars have faced a challenge in pinpointing the items they seek. The use of classification schemes to arrange the documents in order was only a partial solution. Another approach was to index the contents of the documents using words or terms, rather than classification codes. In the 1940s and 1950s some pioneers, such as [[Calvin Mooers]], Charles L. Bernier, [http://pubs.acs.org/cen/priestley/recipients/1951crane.html Evan J. Crane] and [[Hans Peter Luhn]], collected up their index terms in various kinds of list that they called a “thesaurus” (by analogy with the well known thesaurus developed by [[Peter Roget]]).<ref>Roberts, N. The pre-history of the information retrieval thesaurus. ''Journal of Documentation'', 40(4), 1984, p.271-285.</ref> The first such list put seriously to use in information retrieval was the thesaurus developed in 1959 at the E I Dupont de Nemours Company.<ref>Aitchison, J. and Dextre Clarke, S. The thesaurus: a historical viewpoint, with a look to the future. ''Cataloging & Classification Quarterly'', 37 (3/4), 2004, p.5-21.</ref><ref>Krooks, D.A. and Lancaster, F.W. The evolution of guidelines for thesaurus construction. ''Libri'', 43(4), 1993, p.326-342.</ref>

The first two of these lists to be published were the ''Thesaurus of ASTIA Descriptors'' (1960) and the ''Chemical Engineering Thesaurus'' of the American Institute of Chemical Engineers (1961), a descendant of the Dupont thesaurus. More followed, culminating in the influential ''Thesaurus of Engineering and Scientific Terms'' (TEST) published jointly by the Engineers Joint Council and the US Department of Defense in 1967. TEST did more than just serve as an example; its Appendix 1 presented ''Thesaurus rules and conventions'' that have guided thesaurus construction ever since.
Hundreds of thesauri have been produced since then, perhaps thousands. The most notable innovations since TEST have been:
(a)	Extension from monolingual to multilingual capability; and 
(b)	Addition of a conceptually organized display to the basic alphabetical presentation.

Here we mention only some of the national and international standards that have built steadily on the basic rules set out in TEST:

* [[UNESCO]] ''Guidelines for the establishment and development of monolingual thesauri''. 1970 (followed by later editions in 1971 and 1981)
* DIN 1463 ''Guidelines for the establishment and development of monolingual thesauri''. 1972 (followed by later editions)
* ISO 2788 ''Guidelines for the establishment and development of monolingual thesauri''. 1974 (revised 1986)
* ANSI ''American National Standard for Thesaurus Structure, Construction, and Use''. 1974 (revised 1980 and superseded by ANSI/NISO Z39.19-1993)
* ISO 5964 ''Guidelines for the establishment and development of multilingual thesauri''. 1985
* ANSI/NISO Z39.19 ''Guidelines for the construction, format, and management of monolingual thesauri''. 1993 (revised 2005 and renamed ''Guidelines for the construction, format, and management of monolingual controlled vocabularies''.)
* ISO 25964 ''Thesauri and interoperability with other vocabularies''. Part 1 (''Thesauri for information retrieval'' published 2011; Part 2 (''Interoperability with other vocabularies'') published 2013.

The most clearly visible trend across this history of thesaurus development has been from the context of small-scale isolation to a networked world.<ref>Dextre Clarke, Stella G. and Zeng, Marcia Lei. [http://www.niso.org/publications/isq/2012/v24no1/clarke/ From ISO 2788 to ISO 25964: the evolution of thesaurus standards towards interoperability and data modeling] ''Information standards quarterly'', 24(1), 2012, p.20-26.</ref> Access to information was notably enhanced when thesauri crossed the divide between monolingual and multilingual applications. More recently, as can be seen from the titles of the latest ISO and NISO standards, there is a recognition that thesauri need to work in harness with other forms of vocabulary or knowledge organization system, such as subject heading schemes, classification schemes, taxonomies and ontologies. The official website for ISO 25964 gives more information, including a reading list.<ref>''[http://www.niso.org/schemas/iso25964/ ISO 25964 – the international standard for thesauri and interoperability with other vocabularies.]'' National Information Standards Organization, 2013.</ref>

== Purpose ==
{{refimprove section|small=z|date=March 2016}}
In information retrieval, a thesaurus can be used as a form of controlled vocabulary to aid in the indexing of appropriate metadata for information bearing entities. A thesaurus helps with expressing the manifestations of a concept in a prescribed way, to aid in improving [[precision and recall]]. This means that the semantic conceptual expressions of information bearing entities are easier to locate due to uniformity of language. Additionally, a thesaurus is used for maintaining a hierarchical listing of terms; usually single words or bound phrases that aid the indexer in narrowing the terms and limiting semantic ambiguity.

The [[Art and Architecture Thesaurus|Art & Architecture Thesaurus]], for example, is used by countless museums around the world, to catalogue their collections. [[AGROVOC]], the thesaurus of the UN’s [[Food and Agriculture Organization]], is used to index and/or search its AGRIS database of worldwide literature on agricultural research.

== Structure ==
{{refimprove section|small=z|date=March 2016}}
Information retrieval thesauri are formally organized so that existing relationships between concepts are made clear. For example, “citrus fruits” might be linked to the broader concept of “fruits”, and the narrower ones of “oranges”, “lemons”, etc. When the terms are displayed online, the links between them make it very easy to surf around the thesaurus, selecting useful terms for a search. When a single term could have more than one meaning, like tables (furniture) or tables (data), these are listed separately so that the user can choose which concept to search for and avoid retrieving irrelevant results. For any one concept, all the known synonyms are listed, such as “mad cow disease”, “bovine spongiform encephalopathy”, “BSE”, etc. The idea is to guide all the indexers and all the searchers to use the same term for the same concept, so that search results will be as complete as possible. If the thesaurus is multilingual, equivalent terms in other languages are shown too. Following international standards, concepts are generally arranged hierarchically within facets or grouped by themes or topics. Unlike a general thesaurus used for literary purposes, information retrieval thesauri typically focus on one discipline, subject or field of study.

== See also ==
* [[Controlled vocabulary]]
* [[ISO 25964]]
* [[Thesaurus]]

== References ==
{{Reflist}}

== External links ==
* [http://www.niso.org/schemas/iso25964/ Official site for ISO 25964] 
* [http://www.taxonomywarehouse.com/ Taxonomy Warehouse]

[[Category:Information retrieval techniques]]
[[Category:Thesauri]]
<=====doc_Id=====>:636
<=====title=====>:
Search suggest drop-down list
<=====text=====>:
A '''search suggest drop-down list''' is a [[Query language|query]] feature used in [[computing]] to show the searcher [[Computer shortcut|shortcut]]s, while the query is typed into a [[text box]]. Before the query is complete, a[[drop-down list]] with the suggested completions appears to provide options to select. The suggested queries then enable the searcher to complete the required search quickly. As a form of [[Autocomplete|autocompletion]], the suggestion list is distinct from [[web browsing history|search history]] in that it attempts to be predictive even when the user is searching for the first time. Data may come from popular searches, sponsors, geographic location or other sources.<ref>{{cite web|url=http://www.thingsontop.com/googles-new-search-suggestions-may-kill-your-website-158.html|title=Google's new search suggestions may kill your website|first=Vegard|last=Sandvoid|publisher=Things On Top|date=2008-12-14|accessdate=2016-08-03}}</ref><ref>{{cite web|url=https://diegobasch.com/search-for-obama-on-facebook-and-you-get-romney|title=Search for Obama on Facebook and you get Romney|first=Diego|last=Basch|date=2012-09-19|accessdate=2016-08-03}}</ref><ref name="se-land">{{cite web|url=http://searchengineland.com/how-google-instant-autocomplete-suggestions-work-62592|title=How Google Instant's autocomplete suggestions work|first=Danny|last=Sullivan|publisher=Search Engine Land|date=2011-04-06|accessdate=2016-08-03}}</ref> These lists are used by [[operating system]]s, [[web browsers]] and various [[website]]s, particularly [[search engine]]s. Search suggestions are common with a 2014 survey finding that over 80% of [[e-commerce]] websites included them.<ref>{{cite web|url=https://www.smashingmagazine.com/2014/08/the-current-state-of-e-commerce-search/|title=The current state of e-commerce search|first=Christian|last=Holt|publisher=Smash Magazine|date=2014-08-18|accessdate=2016-08-03}}</ref>

The [[Computer science|computing science]] of [[syntax]] and [[algorithm]]s are used to form search results from a [[database]]. [[Content management system]]s and frequent searches can assist [[Software engineering|software engineers]] in [[Optimization (computer science)|optimizing]] more refined queries with methods of parameters and subroutines. Suggestions can be results for the current query or related queries by words, time and dates, categories and [[Tag (metadata)|tags]]. The suggestion list may be reordered by other options, as [[Enumeration|enumerative]], [[Hierarchical organization|hierarchical]] or [[Faceted classification|faceted]].

Although not the first deployment of search suggestions, [[Google Suggest]] is one of the most prominent. Four years before it was considered stable, the feature was developed in 2004 by Google engineer Kevin Gibbs and the name was chosen by [[Marissa Mayer]].<ref>{{cite web|url=http://allthingsd.com/20130823/nearly-a-decade-later-the-autocomplete-origin-story-kevin-gibbs-and-google-suggest/|title=Nearly a Decade Later, the Autocomplete Origin Story: Kevin Gibbs and Google Suggest|first=Liz|last=Gannes|publisher=All Things D|date=2013-08-23|accessdate=2016-08-03}}</ref> Google, and other large search companies, maintain a blacklist that prevents the display of queries that could be interpreted as violating their [[social responsibility]]. Despite this, the company regularly receives complaints that several popular suggestions, or suggestions whose positions have been inflated by [[Internet bot|bots]], should be added to this list.<ref name="se-land" /><ref>{{cite web|url=https://utopiaordystopia.com/2015/02/22/truth-and-prediction-in-the-dataclysm/|title=Truth and Prediction in the Dataclysm|first=Rick|last=Searle|publisher=Utopia or Dystopia|date=2015-02-22|accessdate=2016-08-03}}</ref> The [[Electronic Frontier Foundation]]'s [[Jillian York]] has criticized [[Apple Computers|Apple]]'s blacklist for including words that are merely provocative.<ref>{{cite web|url=http://www.thedailybeast.com/articles/2013/07/16/the-apple-kill-list-what-your-iphone-doesn-t-want-you-to-type.html|title=The Apple 'Kill List': What your iPhone doesn't want you to type|first=Michael|last=Keller|publisher=The Daily Beast|date=2013-07-16|accessdate=2016-08-03}}</ref>

One example of a project using suggested queries to expose societal attitudes was a 2013 ad series called ''The Autocomplete Truth'' by [[UN Women]]. The campaign showed several gender stereotypes being displayed as popular searches by Google Suggest.<ref>{{cite web|url=http://www.adweek.com/adfreak/after-viral-success-inequality-ads-creators-say-they-will-expand-campaign-153363|title=After viral succes of inequality ads, creators say they will expand campaign|first=David|last=Griner|publisher=Ad Week|date=2013-10-24|accessdate=2016-08-03}}</ref> Another was a story by [[Bad Astronomy]] that revealed a distrustful perspective on scientists in the suggestion box.<ref>{{cite web|url=http://www.slate.com/blogs/bad_astronomy/2013/12/04/search_engine_bias_scientists_are.html|title="Scientists are..."|first=Phil|last=Plait|publisher=Slate|date=2013-12-04|accessdate=2016-08-03}}</ref> Additionally, cases related to [[libel]] laws have posited that suggestions may inspire people to associate specific names with specific alleged crimes when they would not have otherwise.<ref name="japan">{{cite web|url=http://www.tamingthebeast.net/blog/online-world/google-autocomplete-angst.htm|title=Some Folks *Really* Hate Autocomplete|first=Michael|last=Bloch|publisher=Taming The Beast|date=2012-03-27|accessdate=2016-08-03}}</ref><ref>{{cite journal|url=http://ijlit.oxfordjournals.org/content/23/3/261.full|title=Search engine liability for autocomplete suggestions: personality, privacy and the power of the algorithm|first1=Stavroula|last1=Karapapa|first2=Maurizio|last2=Borghi|journal=International Journal of Law and Information Technology|volume=23|pages=261-289|year=2015|accessdate=2016-08-03}}</ref>

Some users have criticized the fact that suggestion-enabled text boxes, unlike the [[web forms]] of static HTML, send data about each keystroke to a central server.<ref>{{cite web|url=http://thekeesh.com/2011/08/who-does-facebook-think-you-are-searching-for/|title=Who does Facebook think you are searching for?|first=Jeremy|last=Keeshin|publisher=The Keesh|date=2011-08-18|accessdate=2016-08-03}}</ref> Such data has the potential to [[keystroke dynamics|identify specific people]]. This has caused at least one [[Mozilla Firefox]] developer to opine that "users mostly dislike search suggestions".<ref>{{cite web|url=https://bugzilla.mozilla.org/show_bug.cgi?id=1189719|title=Recall and display search history within main browser UI|first=Richard|last=Newman|publisher=Mozilla|date=2015-08-25|accessdate=2016-08-03}}</ref> Apart from the privacy debate, some users have expressed negative reception over the usefulness of search autocompletion.<ref name="japan" /><ref>{{cite web|url=http://arnoldit.com/wordpress/2012/09/10/google-autocomplete-is-smart-help-a-hindrance/|title=Google Autocomplete: Is Smart Help A Hindrance?|first=Stephen|last=Arnold|publisher=Beyond Search|date=2012-09-09|accessdate=2016-08-03}}</ref><ref>{{cite web|url=https://www.quora.com/How-do-very-strange-stupid-auto-complete-statements-appear-while-searching-on-Google-Do-people-actually-do-these-kind-of-searches-or-are-they-pun-intended|title=How do very strange/stupid auto-complete statements appear while searching on Google? Do people actually do these kind of searches or are they pun intended?|first=Seshal|last=Jain|publisher=Quora|date=2015-05-02|accessdate=2016-08-03}}</ref> Specifically, the sudden appearance of a suggestion box in some programs has been compared to the behaviour of a [[pop-up ad]].<ref>{{cite web|url=http://martesmartes.blogspot.com/2008/07/disabling-openoffices-stupid.html|title=Disabling Open Office's Stupid Autocomplete|first=Jeff|last=Martens|publisher=Martes-Martes|date=2008-07-09|accessdate=2016-08-03}}</ref><ref>{{cite web|url=https://mikesmithers.wordpress.com/2012/01/28/turning-off-code-completion-in-sqldeveloper-a-grumpy-old-man-fights-back/|title=Turning off code completion in SQLDeveloper &mdash; A grumpy old man fights back|first=Mike|last=Smithers|publisher=The Anti-Kyte|date=2012-01-28|accessdate=2016-08-03}}</ref>

==See also==
*[[Autocomplete]]
*[[Search engine (computing)]]
*[[Search box]]
*[[Search algorithm]]
*[[Censorship by Google#Search suggestions|Censorship by Google § Search suggestions]]

==References==
{{Reflist}}

{{DEFAULTSORT:Search Suggest Drop-Down List}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:639
<=====title=====>:
Category:Concordances (publishing)
<=====text=====>:
{{Cat main|Concordance (publishing)}}
[[Category:Biblical studies]]
[[Category:Index (publishing)]]
[[Category:Textual criticism]]
[[Category:Reference works]]
[[Category:Information retrieval techniques]]
[[Category:Hypertext]]
[[Category:Corpus linguistics]]
<=====doc_Id=====>:642
<=====title=====>:
Extended Boolean model
<=====text=====>:
The '''Extended Boolean model''' was described in a Communications of the ACM article appearing in 1983, by Gerard Salton, Edward A. Fox, and Harry Wu. The goal of the Extended Boolean model is to overcome the drawbacks of the Boolean model that has been used in [[information retrieval]]. The Boolean model doesn't consider term weights in queries, and the result set of a Boolean query is often either too small or too big. The idea of the extended model is to make use of partial matching and term weights as in the vector space model. It combines the characteristics of the [[Vector Space Model]] with the properties of [[Boolean algebra (logic)|Boolean algebra]] and ranks the similarity between queries and documents. This way a document may be somewhat relevant if it matches some of the queried terms and will be returned as a result, whereas in the [[Standard Boolean model]] it wasn't.<ref>	
{{citation | url=http://portal.acm.org/citation.cfm?id=358466 | last1=Salton | first1=Gerard | first2=Edward A. | last2=Fox | first3= Harry | last3=Wu | title=Extended Boolean information retrieval | publisher=Communications of the ACM, Volume 26,  Issue 11 | year=1983 }}</ref>

Thus, the extended Boolean model can be considered as a generalization of both the Boolean and vector space models; those two are special cases if suitable settings and definitions are employed. Further, research has shown effectiveness improves relative to that for Boolean query processing.  Other research has shown that [[relevance feedback]] and [[query expansion]] can be integrated with extended Boolean query processing.

==Definitions==
In the '''Extended Boolean model''', a document is represented as a vector (similarly to in the vector model). Each ''i'' [[Dimension (vector space)|dimension]] corresponds to a separate term associated with the document.

The weight of term {{math|''K<sub>x</sub>''}} associated with document {{math|''d<sub>j</sub>''}} is measured by its normalized [[Term frequency]] and can be defined as:

<math>
w_{x,j}=f_{x,j}*\frac{Idf_{x}}{max_{i}Idf_{i}}
</math>

where {{math|''Idf<sub>x</sub>''}} is [[inverse document frequency]].

The weight vector associated with document {{math|''d<sub>j</sub>''}} can be represented as:

<math>\mathbf{v}_{d_j} = [w_{1,j}, w_{2,j}, \ldots, w_{i,j}]</math>

==The 2 Dimensions Example==
{{multiple image
 | width     = 150
 | image1    = 2D_Extended_Boolean_model_OR_example.png
 | alt1      = Figure 1
 | caption1  = '''Figure 1:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.
 | image2    = 2D_Extended_Boolean_model_AND_example.png
 | alt2      = Figure 2
 | caption2  = '''Figure 2:''' The similarities of {{math|''q'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}} with documents {{math|''d<sub>j</sub>''}} and {{math|''d''<sub>''j''+1</sub>}}.
}}

Considering the space composed of two terms {{math|''K<sub>x</sub>''}} and {{math|''K<sub>y</sub>''}} only, the corresponding term weights are {{math|''w''<sub>1</sub>}} and {{math|''w''<sub>2</sub>}}.<ref>[http://www.cs.cityu.edu.hk/~cs5286/Lectures/Lwang.ppt Lusheng Wang]</ref>  Thus, for query {{math|''q<sub>or</sub>'' {{=}} (''K<sub>x</sub>'' &or; ''K<sub>y</sub>'')}}, we can calculate the similarity with the following formula:
 
<math>sim(q_{or},d)=\sqrt{\frac{w_1^2+w_2^2}{2}}</math>

For query {{math|''q<sub>and</sub>'' {{=}} (''K<sub>x</sub>'' &and; ''K<sub>y</sub>'')}}, we can use:

<math>sim(q_{and},d)=1-\sqrt{\frac{(1-w_1)^2+(1-w_2)^2}{2}}</math>

==Generalizing the idea and P-norms==
We can generalize the previous 2D extended Boolean model example to higher t-dimensional space using Euclidean distances.

This can be done using [[P-norm]]s which extends the notion of distance to include p-distances, where {{math|1 &le; ''p'' &le; &infin;}} is a new parameter.<ref>{{ citation | last=Garcia | first= Dr. E. | url=http://www.miislita.com/term-vector/term-vector-6-boolean-model.html | title=The Extended Boolean Model - Weighted Queries: Term Weights, p-Norm Queries and Multiconcept Types. Boolean OR Extended? AND that is the Query }}</ref>

*A generalized conjunctive query is given by:
:<math>q_{or}=k_1 \lor^p k_2 \lor^p .... \lor^p k_t  </math>

*The similarity of <math>q_{or}</math> and <math>d_j</math> can be defined as:
''':<math>sim(q_{or},d_j)=\sqrt[p]{\frac{w_1^p+w_2^p+....+w_t^p}{t}}</math>'''

*A generalized disjunctive query is given by:
:<math>q_{and}=k_1 \land^p k_2 \land^p .... \land^p k_t  </math>

*The similarity of <math>q_{and}</math> and <math>d_j</math> can be defined as:
:<math>sim(q_{and},d_j)=1-\sqrt[p]{\frac{(1-w_1)^p+(1-w_2)^p+....+(1-w_t)^p}{t}}</math>

==Examples==
Consider the query {{math|''q'' {{=}} (''K''<sub>1</sub> &and; ''K''<sub>2</sub>) &or; ''K''<sub>3</sub>}}. The similarity between query {{math|''q''}} and document {{math|''d''}} can be computed using the formula:

<math>sim(q,d)=\sqrt[p]{\frac{(1-\sqrt[p]{(\frac{(1-w_1)^p+(1-w_2)^p}{2}}))^p+w_3^p}{2}}</math>

==Improvements over the Standard Boolean Model==

Lee and Fox<ref>{{citation | last1=Lee | first1=W. C. | first2=E. A. | last2=Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries | url = http://eprints.cs.vt.edu/archive/00000112/01/TR-88-27.pdf}}</ref> compared the Standard and Extended Boolean models with three test collections, CISI, CACM and INSPEC.
Using P-norms they obtained an average precision improvement of 79%, 106% and 210% over the Standard model, for the CISI, CACM and INSPEC collections, respectively.<br>
The P-norm model is computationally expensive because of the number of exponentiation operations that it requires but it achieves much better results than the Standard model and even [[Fuzzy retrieval]] techniques. The [[Standard Boolean model]] is still the most efficient.

==Further reading==
* [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.1997 Adaptive Feedback Methods in an Extended Boolean Model  by Dr.Jongpill Choi]
* [http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6VC8-454T5MS-2&_user=513551&_rdoc=1&_fmt=&_orig=search&_sort=d&_docanchor=&view=c&_searchStrId=1117914301&_rerunOrigin=google&_acct=C000025338&_version=1&_urlVersion=0&_userid=513551&md5=4eab0da46bfe361afa883e48f2060feb Interpolation of the extended Boolean retrieval model ]
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | first2=S. | last2=Betrabet | first3=M. | last3=Koushik | first4=W. | last4=Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}
* {{citation | title=Experiments with Automatic Query Formulation in the Extended Boolean Model | url=http://www.springerlink.com/content/tk1t141253257613/ | first1= Lucie | last1= Skorkovská | first2=Pavel | last2=Ircing | year=2009 | publisher= Springer Berlin / Heidelberg}}

==See also==
*[[Information retrieval]]

==References==
{{reflist}}

{{DEFAULTSORT:Extended Boolean Model}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:645
<=====title=====>:
Subsetting
<=====text=====>:
In research communities (for example, [[earth science]]s, [[astronomy]], [[business]], and [[government]]), '''subsetting''' is the process of retrieving just the parts of large files which are of interest for a specific purpose. This occurs usually in a client—server setting, where the extraction of the parts of interest occurs on the server before the data is sent to the client over a network. The main purpose of subsetting is to save bandwidth on the network and storage space on the client computer.

Subsetting may be favorable for the following reasons:<ref name="Institute2012">{{cite book|author=SAS Institute|title=SAS/ETS 12.1 User's Guide|url=https://books.google.com/books?id=OE0UfAhit4kC&pg=PA70|date=1 August 2012|publisher=SAS Institute|isbn=978-1-61290-379-8|pages=70}}</ref>
* restrict or divide the time range
* select [[Cross-sectional data|cross section]]s of data
* select particular kinds of [[time series]]
* exclude particular observations

==References==
{{reflist}}


==External links==
*[http://www.subset.org/index.jsp Subset.org]

[[Category:Information retrieval techniques]]

{{Statistics-stub}}
<=====doc_Id=====>:648
<=====title=====>:
Semantic compression
<=====text=====>:
In [[natural language processing]], '''semantic compression''' is a process of compacting a lexicon used to build 
a textual document (or a set of documents) by reducing language heterogeneity, while maintaining text [[semantics]]. 
As a result, the same ideas can be represented using a smaller set of words.

Semantic compression is a [[lossy compression]], that is, some data is being discarded, and an original document 
cannot be reconstructed in a reverse process.

==Semantic compression by generalization==
Semantic compression is basically achieved in two steps, using [[frequency list|frequency dictionaries]] and [[semantic network]]:
#	determining cumulated term frequencies to identify target lexicon,
#	replacing less frequent terms with their hypernyms ([[generalization]]) from target lexicon.<ref>[http://dx.doi.org/10.1007/978-3-642-12090-9_10 D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems], Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010</ref>

Step 1 requires assembling word frequencies and 
information on semantic relationships, specifically [[hyponymy]]. Moving upwards in word hierarchy, 
a cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:
<math>cum f(k_{i}) = f(k_{i}) + \sum_{j} cum f(k_{j})</math> where <math>k_{i}</math> is a hypernym of <math>k_{j}</math>.
Then, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.

In the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence 
of a less frequent hyponym as its hypernym in output text.

;Example

The below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.

<blockquote>They are both '''nest''' building '''social insects''', but '''paper wasps''' and honey '''bees''' '''organize''' their '''colonies''' 
in very different '''ways'''. In a new study, researchers report that despite their '''differences''', these insects 
'''rely on''' the same network of genes to guide their '''social behavior'''.The study appears in the Proceedings of the 
'''Royal Society B''': Biological Sciences. Honey '''bees''' and '''paper wasps''' are separated by more than 100 million years of 
'''evolution''', and there are '''striking differences''' in how they divvy up the work of '''maintaining''' a '''colony'''.</blockquote>

The procedure outputs the following text:

<blockquote>They are both '''facility''' building '''insect''', but '''insect'''s and honey '''insects''' '''arrange''' their '''biological groups''' 
in very different '''structure'''. In a new study, researchers report that despite their '''difference of opinions''', these insects 
'''act''' the same network of genes to '''steer''' their '''party demeanor'''. The study appears in the proceeding of the 
'''institution bacteria''' Biological Sciences. Honey '''insects''' and '''insect''' are separated by more than hundred million years of 
'''organic processes''', and there are '''impinging differences of opinions''' in how they divvy up the work of '''affirming''' a '''biological group'''.</blockquote>

==Implicit semantic compression==
A natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid [[pleonasm]]s)
.<ref>[http://dx.doi.org/10.3115/990100.990155 N. N. Percova, On the types of semantic compression of text],
COLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982</ref>

==Applications and advantages==
In the [[vector space model]], compacting a lexicon leads to a reduction of [[curse of dimensionality|dimensionality]], which results in less 
[[Computational complexity theory|computational complexity]] and a positive influence on efficiency. 

Semantic compression is advantageous in information retrieval tasks, improving their effectiveness (in terms of both precision and recall).<ref>[http://dl.acm.org/citation.cfm?id=1947662.1947683 D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification] Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010</ref> This is due to more precise descriptors (reduced effect of language diversity – limited language redundancy, a step towards a controlled dictionary).

As in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).

==See also==
* [[Text simplification]]
* [[Lexical substitution]]
* [[Information theory]]
* [[Quantities of information]]

==References==
<references/>

==External links==
* [http://semantic.net.pl/semantic_compression.php Semantic compression on Project SENECA (Semantic Networks and Categorization) website]

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]
[[Category:Quantitative linguistics]]
[[Category:Computational linguistics]]
<=====doc_Id=====>:651
<=====title=====>:
Document clustering
<=====text=====>:
{{Multiple issues|
{{disputed|date=March 2014}}
{{more footnotes|date=March 2014}}
}}

'''Document clustering''' (or '''text clustering''') is the application of [[cluster analysis]] to textual documents. It has applications in automatic document organization, [[topic (linguistics)|topic]] extraction and fast [[information retrieval]] or filtering.

==Overview==
Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Examples of document clustering include web document clustering for search users.

The application of document clustering can be categorized to two types, online and offline. Online applications are usually constrained by efficiency problems when compared to offline applications.

In general, there are two common algorithms. The first one is the hierarchical based algorithm, which includes single link, complete linkage, group average and Ward's method.  By aggregating or dividing, documents can be clustered into hierarchical structure, which is suitable for browsing. However, such an algorithm usually suffers from efficiency problems. The other algorithm is developed using the [[K-means algorithm]] and its variants. Generally hierarchical algorithms produce more in-depth information for detailed analyses, while algorithms based around variants of the [[K-means algorithm]] are more efficient and provide sufficient information for most purposes.<ref name="manning">Manning, Chris, and Hinrich Schütze, ''Foundations of Statistical Natural Language Processing'', MIT Press. Cambridge, MA: May 1999.</ref>{{rp|Ch.14}}

These algorithms can further be classified as hard or soft clustering algorithms. Hard clustering computes a hard assignment – each document is a member of exactly one cluster. The assignment of soft clustering algorithms is soft – a document’s assignment is a distribution over all clusters. In a soft assignment, a document has fractional membership in several clusters.<ref name="manning"/>{{rp|499}} [[Dimensionality reduction]] methods can be considered a subtype of soft clustering; for documents, these include [[latent semantic indexing]] ([[truncated singular value decomposition]] on term histograms)<ref>http://nlp.stanford.edu/IR-book/pdf/16flat.pdf</ref> and [[topic model]]s.

Other algorithms involve graph based clustering, ontology supported clustering and order sensitive clustering.

Given a clustering, it can be beneficial to automatically derive human-readable labels for the clusters. [[Cluster labeling|Various methods]] exist for this purpose.

==Clustering in search engines==
A [[web search engine]] often  returns thousands of pages in response to a broad query, making it difficult for users to browse or to identify relevant information.  Clustering methods can be used to automatically group the retrieved documents into a list of meaningful categories, as is achieved by e.g. open source software such as [[Carrot2]].

==Procedures==
In practice, document clustering often takes the following steps:
 
1. [[Tokenization (lexical analysis)|Tokenization]]

Tokenization is the process of parsing text data into smaller units (tokens) such as words and phrases. Commonly used tokenization methods include [[Bag-of-words model]] and [[N-gram model]].

2. [[Stemming]] and [[lemmatization]]

Different tokens might carry out similar information (e.g. tokenization and tokenizing). And we can avoid calculating similar information repeatedly by reducing all tokens to its base form using various stemming and lemmatization dictionaries.

3. Removing [[stop words]] and [[punctuation]]

Some tokens are less important than others. For instance, common words such as "the" might not be very helpful for revealing the essential characteristics of a text. So usually it is a good idea to eliminate stop words and punctuation marks before doing further analysis.

4. Computing term frequencies or [[tf-idf]]

After pre-processing the text data, we can then proceed to generate features. For document clustering, one of the most common ways to generate features for a document is to calculate the term frequencies of all its tokens. Although not perfect, these frequencies can usually provide some clues about the topic of the document. And sometimes it is also useful to weight the term frequencies by the inverse document frequencies. See [[tf-idf]] for detailed discussions.

5. Clustering

We can then cluster different documents based on the features we have generated. See the algorithm section in [[cluster analysis]] for different types of clustering methods.

6. Evaluation and visualization

Finally, the clustering models can be assessed by various metrics. And it is sometimes helpful to visualize the results by plotting the clusters into low (two) dimensional space. See [[multidimensional scaling]] as a possible approach.

== Clustering v. Classifying ==
Clustering algorithms in computational text analysis groups documents into what are called subsets or ''clusters'' where the algorithm's goal is to create internally coherent clusters that are distinct from one another.<ref>{{Cite web|url=http://nlp.stanford.edu/IR-book/|title=Introduction to Information Retrieval|website=nlp.stanford.edu|pages=349|access-date=2016-05-03}}</ref> Classification on the other hand, is a form of [[supervised learning]] where the features of the documents are used to predict the "type" of documents.

== References ==
{{reflist}}
Publications:
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. ''Flat Clustering'' in <u>Introduction to Information Retrieval.</u> Cambridge University Press. 2008
* Nicholas O. Andrews and Edward A. Fox, Recent Developments in Document Clustering, October 16, 2007 [http://eprints.cs.vt.edu/archive/00001000/01/docclust.pdf]
* Claudio Carpineto, Stanislaw Osiński, Giovanni Romano, Dawid Weiss. A survey of Web clustering engines. ACM Computing Surveys, Volume 41, Issue 3 (July 2009), Article No. 17, {{ISSN|0360-0300}}

==See also==
*[[Cluster Analysis]]
*[[Fuzzy clustering]]

[[Category:Information retrieval techniques]]
<=====doc_Id=====>:654
<=====title=====>:
Contextual searching
<=====text=====>:
{{Orphan|date=May 2015}}

'''Contextual search''' is a form of optimizing web-based search results based on context provided by the user and the computer being used to enter the query.<ref>{{cite journal | first = Susan E. | last = Feldman | title = The Answer Machine | journal = Synthesis Lectures on Information Concepts, Retrieval, and Services | doi = 10.2200/S00442ED1V01Y201208ICR023 }}</ref> Contextual search services differ from current search engines based on traditional information retrieval that return lists of documents based on their [[Relevance (information retrieval)|relevance]] to the query. Rather, contextual search attempts to increase the [[Precision and recall|precision]] of results based on how valuable they are to individual users.<ref>{{cite journal | last1 = Pitokow | first1 = James | first2 = Hinrich | last2 = Schütze | first3 = Todd | last3 = Cass | first4 = Rob | last4 = Cooley | first5 = Don | last5 = Turnbull | first6 = Andy | last6 = Edmonds | first7 = Eytan | last7 = Adar | first8 = Thomas | last8 = Breuel | date = 2002 | title = Personalized search | url = http://www.cond.org/p50-pitkow.pdf | journal = Communications of the ACM (CACM) | volume = 45 | issue = 9 | pages = 50–55 }}</ref>

== Basic Contextual Search ==
The basic form of contextual search is the process of scanning the full-text of a query in order to understand what the user needs. Web search engines scan HTML pages for content and return an index rating based on how relevant the content is to the entered query. HTML pages that have a higher occurrence of query keywords within their content are not rated higher. Users have limited control over the context of their query based on the words they use to search with.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 25, 2000.</ref>  For example, users looking for the menu portion of a website can add “menu” to the end of their query to provide the search engine with context of what they need. The next step in contextualizing search is for the search service itself to request information that narrows down the results, such as Google asking for a time range to search within.

== Explicitly Supplied Context ==
Certain search services, including many Meta search engines, request individual contextual information from users to increase the precision of returned documents. Inquirus 2 is a Meta search engine that acts as a mediator between the user query and other search engines. When searching on Inquirus 2, users enter a query and specify constraints such as the information need category, maximum number of hits, and display formats.<ref>Steve Lawrence. ''Context in Web Search'', IEEE Data Engineering Bulletin, Volume 23, Number 3, pp. 27, 2000.</ref> For example, a user looking for research papers can specify documents with “references” or “abstracts” to be rated higher. If another user is searching for general information on the topic rather than research papers, they can specify the GenScore attribute to have a heavier weight.<ref>Steve Lawrence, C. Lee Giles. ''Inquirus, the NECI meta search engine''[http://www7.scu.edu.au/1906/com1906.htm]</ref>

Explicitly supplied context effectively increases the precision of results, however, these search services tend to suffer from poor user-experience. Learning the interface of programs like Inquirus can prove challenging for general users without knowledge of search metrics. Aspects of supplied context do appear on major search engines with better user-interaction such as Google and Bing. Google allows users to filter by type: Images, Maps, Shopping, News, Videos, Books, Flights, and Apps.<ref>[https://support.google.com/websearch/answer/142143?hl=en https://support.google.com/websearch/answer/142143?hl=en], Filter your search results</ref> Google has an extensive [https://support.google.com/websearch/answer/2466433?rd=1 list of search operators] that allow users to explicitly limit results to fit their needs such as restricting certain file types or removing certain words.<ref>[https://support.google.com/websearch/answer/2466433?rd=1 https://support.google.com/websearch/answer/2466433?rd=1], Search Operators</ref> Bing also uses a similar set of search operators to assist users in explicitly narrowing down the context of their queries. Bing allows users to search within a time range, by file type, by location, language, and more.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/], Bing Tricks</ref>

== Automatically Inferred Context ==
There are other systems being developed that are working on automatically inferring the context of user queries based on the content of other documents they view or edit. [[Watson (computer)|IBM's Watson Project]] aims to create a cognitive technology that dynamically learns as it processes user queries. When presented with a query Watson creates a hypothesis that is evaluated against its present bank of knowledge based on previous questions. As related terms and relevant documents are matched against the query, Watson's hypothesis is modified to reflect the new information provided through unstructured data based on information it has obtained in previous situations.<ref>[http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html http://www.ibm.com/smarterplanet/us/en/ibmwatson/what-is-watson.html], How Watson Works - IBM</ref> Watson's ability to build off previous knowledge allows queries to be automatically filtered for similar contexts in order to supply precise results.

Major search services such as Google, Bing, and Yahoo also have a system of automatically inferring the context of particular user queries. Google tracks user's previous queries and selected results to further personalize results for those individuals. For example, if a user consistently searches for articles related to animals, wild animals, or animal care a search for "jaguar" would rank an article on jaguar cats higher than links to Jaguar Cars.<ref>{{cite journal | first1 = Eric J. | last1 = Glover | first2 = Steve | last2 = Lawrence | first3 = Michael D. | last3 = Gordon | first4 = William P. | last4 = Birmingham | first5 = C. Lee | last5 = Giles | title = Web Search - Your Way | publisher = NEC Research Institution | citeseerx = 10.1.1.41.7499 }}</ref> Similar to Watson, search services strive to learn from users based on previous experiences to automatically provide context on current queries. Bing also provides automatic context for particular queries based on content of the query itself. A [http://www.bing.com/search?q=pizza&go=Submit&qs=n&form=GEOMA1&pq=pizza&sc=8-1&sp=-1&sk=&cvid=883269b61529466e810bc096e371ec19 search of "pizza"] returns an interactive list of restaurants and their ratings based on the approximate location of the user's computer. The Bing server automatically infers that when a user searches for a food item they are interested in documents within the context of purchasing that food item or finding restaurants that sell that particular item.

=== Contextual Mobile Search ===
The drive to develop better contextualized search coincides with the increasing popularity of using mobile phones to complete searches. BIA/Kelsey research marketing firm projects that by 2015 mobile local search will "exceed local search by more than 27 billion queries".<ref>[http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp http://www.biakelsey.com/Company/Press-Releases/120418-Mobile-Local-Search-Volume-Will-Surpass-Desktop-Local-Search-in-2015.asp], Mobile Search to Surpass Desktop</ref> Mobile phones provide the opportunity to provide search services with a broader supply of contextual information, particularly for location services but also personalized searches based on the wealth of information stored locally on the phone including contacts information, geometric analysis such as speed and elevation, and installed apps.<ref>[http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/ http://blog.broadcom.com/ces/beyond-gps-smartphones-get-smarter-with-context-awareness-at-ces-2014/], Contextually Aware Mobile Devices</ref>

== References ==
{{reflist}}

{{Internet search}}

{{DEFAULTSORT:Contextual Searching}}
[[Category:Internet search engines]]
[[Category:Semantic Web]]
[[Category:Information retrieval techniques]]
[[Category:Internet terminology]]
<=====doc_Id=====>:657
<=====title=====>:
Faceted search
<=====text=====>:
'''Faceted search''', also called '''faceted navigation''' or '''faceted browsing''', is a technique for accessing information organized according to a [[faceted classification]] system, allowing users to explore a collection of information by applying multiple filters. A faceted classification system classifies each information element along multiple explicit dimensions, called facets, enabling the classifications to be accessed and ordered in multiple ways rather than in a single, pre-determined, [[taxonomy (general)|taxonomic]] order.<ref name="Faceted Search">[http://www.morganclaypool.com/doi/abs/10.2200/S00190ED1V01Y200904ICR005 Faceted Search], Morgan & Claypool, 2009</ref>

Facets correspond to properties of the information elements. They are often derived by analysis of the text of an item using [[entity extraction]] techniques or from pre-existing fields in a database such as author, descriptor, language, and format. Thus, existing web-pages, product descriptions or online collections of articles can be augmented with navigational facets.

Within the academic community, faceted search has attracted interest primarily among [[library and information science]] researchers, and to some extent among [[computer science]] researchers specializing in [[information retrieval]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]</ref>

==Development==

The [[Association for Computing Machinery]]'s [[Special Interest Group on Information Retrieval]] provided the following description of the role of faceted search for a 2006 workshop:
<blockquote>
The web search world, since its very beginning, has offered two paradigms:
*Navigational search uses a hierarchy structure (taxonomy) to enable users to browse the information space by iteratively narrowing the scope of their quest in a predetermined order, as exemplified by [[Yahoo! Directory]], [[Open Directory Project|DMOZ]], etc.
*Direct search allows users to simply write their queries as a bag of words in a text box. This approach has been made enormously popular by [[Web search engine]]s. 
Over the last few years, the direct search paradigm has gained dominance and the navigational approach became less and less popular. Recently, a new approach has emerged, combining both paradigms, namely the faceted search approach. Faceted search enables users to navigate a multi-dimensional information space by combining text search with a progressive narrowing of choices in each dimension. It has become the prevailing user interaction mechanism in e-commerce sites and is being extended to deal with [[semi-structured data]], continuous dimensions, and [[Folksonomy | folksonomies]].<ref name="sigir06">[http://facetedsearch.googlepages.com SIGIR'2006 Workshop on Faceted Search - Call for Participation]</ref>
</blockquote>

==Mass market use==

Faceted search has become a popular technique in commercial search applications, particularly for online retailers and libraries. An increasing number of [[List of Enterprise Search Vendors|enterprise search vendors]] provide software for implementing faceted search applications.

Online retail catalogs pioneered the earliest applications of faceted search, reflecting both the faceted nature of product data (most products have a type, brand, price, etc.) and the ready availability of the data in retailers' existing information-systems. In the early 2000s retailers started using faceted search. A 2014 benchmark of 50 of the largest US based online retailers reveals that despite the benefits of faceted search, only 40% of the sites have implemented it. <ref name="Smashing Magazine: The Current State of E-Commerce Search (2014)">[http://www.smashingmagazine.com/2014/08/18/the-current-state-of-e-commerce-search/ Smashing Magazine: The Current State of E-Commerce Search] Retrieved on 2014-08-27.</ref> Examples include the filtering options that appear in the left column on [[amazon.com]] or [[Google Shopping]] after a keyword search has been performed.

==Libraries and information science==

In 1933, the noted librarian [[S. R. Ranganathan|Ranganathan]] proposed a [[faceted classification]] system for library materials, known as [[colon classification]]. In the pre-computer era, he did not succeed in replacing the pre-coordinated [[Dewey Decimal Classification]] system.<ref name="Major classification systems : the Dewey Centennial">[https://archive.org/details/majorclassificat00alle Major classification systems : the Dewey Centennial]</ref>

Modern online library catalogs, also known as [[OPAC]]s, have increasingly adopted faceted search interfaces. Noted examples include the [[North Carolina State University]] library catalog (part of the Triangle Research Libraries Network) and the [[Online Computer Library Center|OCLC]] Open [[WorldCat]] system. The [[CiteSeerX]] project<ref>[http://citeseerx.ist.psu.edu/ CiteSeerX]. Citeseerx.ist.psu.edu. Retrieved on 2013-07-21.</ref> at the [[Pennsylvania State University]] allows faceted search for academic documents and continues to expand into other facets such as table search.

==See also==
* [[Enterprise search]]
* [[Exploratory search]]
* [[Faceted classification]]
* [[Human–computer information retrieval]]
* [[Information extraction]]
* [[NoSQL]]

==References==
<References/>

{{DEFAULTSORT:Faceted Search}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:660
<=====title=====>:
Personalization
<=====text=====>:
{{Multiple issues|
{{cleanup-reorganize|date=June 2008}}
{{refimprove|date=September 2015}}
{{tone|date=September 2015}}
}}

'''Personalization''', sometimes known as '''customization''', consists of tailoring a service or a product to accommodate specific individuals, sometimes tied to groups or segments of individuals.  A wide variety of organizations use personalization to improve [[customer satisfaction]], digital sales conversion, marketing results, branding, and improved website metrics as well as for [[advertising]]. Personalization is a key element in [[social media]] and [[recommender system]]s.

==Web pages==
[[Web page]]s can be personalized based on the characteristics (interests, social category, context, etc.), actions (click on button, open a link, etc.), intent (make a purchase, check status of an entity), or any other parameter that can be identified and associated with an individual, therefore providing them with a tailored user experience.  Note that the experience is rarely simply accommodation of the user but a relationship between the user and the desires of the site designers in driving specific actions to achieve objectives (e.g. Increase sales conversion on a page).  The term ''customization'' is often used when the site only uses explicit data such as product ratings or user preferences.

Technically, web personalization can be achieved by associating a visitor segment with a predefined action. Customizing the user experience based on behavioural, contextual and technical data is proven to have a positive impact on conversion rate optimization efforts. Associated actions can range from changing the content of a webpage, presenting a modal display, presenting interstitials, triggering a personalized email or even automating a phone call to the user.

According to a 2014 study from research firm Econsultancy, less than 30% of [[e-commerce]] websites have invested in the field of web personalization. However, many companies now offer services for web personalization as well as web and email recommendation systems that are based on personalization or anonymously-collected user behaviours.<ref name=behaviours>[http://online.wsj.com/article/SB10001424052748703294904575385532109190198.html ''Wall Street Journal'', “On the Web's Cutting Edge, Anonymity in Name Only”], August 4, 2010</ref> According to a study done by Compass, e-commerce websites that use personalization can see an increase in revenue of as much as 29%. <ref name=29%>[http://blog.compass.co/improving-ecommerce-retention-revenue-personalization/ ''Compass Blog'', “Improving Ecommerce Retention and Revenue with Personalization”], August 11, 2016</ref>

There are many categories of web personalization including
# Behavioral
# Contextual
# Technical
# Historic data
# Collaboratively filtered

There are several camps in defining and executing web personalization.  A few broad methods for web personalization may include:
# Implicit
# Explicit
# Hybrid

With implicit personalization, the web personalization is performed based on the different categories mentioned above. It can also be learned from direct interactions with the user based on implicit data, such as items purchased or pages viewed.<ref>{{cite web|last1=Flynn|first1=Lawrence|title=5 Things To Know About Siri And Google Now's Growing Intelligence|url=http://www.forbes.com/sites/parmyolson/2014/07/08/5-things-to-know-about-siri-and-google-nows-growing-intelligence/|website=Forbes}}</ref> With explicit personalization, the web page (or information system) is changed by the user using the features provided by the system. Hybrid personalization combines the above two approaches to leverage the ''best of both worlds''.

Web personalization is can be linked to the notion of [[Adaptive hypermedia]] (AH). The main difference is that the former would usually work on what is considered an "open corpus hypermedia," whilst the latter would traditionally work on "closed corpus hypermedia." However, recent research directions in the AH domain take both closed and open corpus into account. Thus, the two fields are closely inter-related.

Personalization is also being considered for use in less overtly commercial applications to improve the user experience online.<ref>[[Jonathan Bowen|Bowen, J.P.]] and Filippini-Fantoni, S., [http://www.archimuse.com/mw2004/papers/bowen/bowen.html Personalization and the Web from a Museum Perspective]. In [[David Bearman]] and Jennifer Trant (eds.), ''[[Museums and the Web]] 2004: Selected Papers from an International Conference'', Arlington, Virginia, USA, 31 March – 3 April 2004. Archives & Museum Informatics, pages 63–78, 2004.</ref> Internet activist [[Eli Pariser]] has documented that search engines like [[Google]] and [[Yahoo! News]] give different results to different people (even when logged out). He also points out social media site [[Facebook]] changes user's friend feeds based on what it thinks they want to see. Pariser warns that these algorithms can create a "[[filter bubble]]" that prevents people from encountering a diversity of viewpoints beyond their own, or which only presents facts which confirm their existing views.

On an [[intranet]] or [[B2E]] [[Web portal#Enterprise Web portals|Enterprise Web portals]], personalization is often based on user attributes such as department, functional area, or role. The term "customization" in this context refers to the ability of users to modify the page layout or specify what content should be displayed.

==Digital media==
Another aspect of personalization is the increasing prevalence of [[open data]] on the Web. Many companies make their data available on the Web via [[API]]s, web services, and [[open data]] standards. One such example is Ordnance Survey Open Data.<ref>{{cite news| url=https://www.theguardian.com/news/datablog/2010/apr/02/ordnance-survey-open-data | location=London | work=The Guardian | first1=Chris | last1=Thorpe | first2=Simon | last2=Rogers | title=Ordnance Survey opendata maps: what does it actually include? | date=2 April 2010}}</ref> Data made available in this way is structured to allow it to be inter-connected and re-used by third parties.<ref>{{cite web|url=http://www.cio.com/article/372363/Google_Opens_Up_Data_Center_For_Third_Party_Web_Applications |title=Google Opens Up Data Centre for Third Party Web Applications |publisher=Cio.com |date=2008-05-28 |accessdate=2013-01-16}}</ref>

Data available from a user's personal [[social graph]] can be accessed by third-party [[application software]] to be suited to fit the personalized [[web page]] or [[information appliance]].

Current [[open data]] standards on the Web include:
# [[Attention Profiling Mark-up Language]] (APML)
# DataPortability
# [[OpenID]]
# [[OpenSocial]]

== Mobile phones ==

Over time mobile phones have seen an increased emphasis placed on user personalization. Far from the black and white screens and monophonic ringtones of the past, phones now offer interactive wallpapers and MP3 TruTones. In the UK and Asia, WeeMees have become popular. WeeMees are three-dimensional characters that are used as wallpaper and respond to the tendencies of the user. Video Graphics Array (VGA) picture quality allows people to change their background with ease without sacrificing quality. All of these services are downloaded through the provider with the goal to make the user feel connected to the phone.<ref>May, Harvey, and Greg Hearn. "The Mobile Phone as Media." International Journal of Cultural Studies 8.2 (2005): 195-211. Print.</ref>

==Print media==
{{main|Mail merge}}

In print media, ranging from [[magazine]]s to [[admail|promotional publication]]s, personalization uses databases of individual recipients' information. Not only does the written document address itself by name to the reader, but the advertising is targeted to the recipient's demographics or interests using fields within the database, such as "first name", "last name", "company", etc.

The term "personalization" should not be confused with variable data, which is a much more granular method of marketing that leverages both images and text with the medium, not just fields within a database. Although personalized children's books are created by companies who are using and leveraging all the strengths of [[variable data printing|variable data printing (VDP)]]. This allows for full image and text variability within a printed book.
With the advent of online 3D printing services such as Shapeways and Ponoko we are seeing personalization enter into the realms of product design.

== Promotional merchandise ==
Promotional items ([[mug]]s, [[T-shirt]]s, [[keychain]]s, [[ball]]s etc.) are regularly personalized. Personalized children's storybooks—wherein the child becomes the [[protagonist]], with the name and image of the child personalized—are also popular. Personalized CDs for children also exist. With the advent of [[digital printing]], personalized calendars that start in any month, birthday cards, cards, e-cards, posters and photo books can also be obtained.

== 3D printing ==
3D printing is a production method that allows to create unique and personalized items on a global scale. Personalized apparel and accessories, such as jewellery, are increasing in popularity.<ref>{{cite web|url=http://www.jewellermagazine.com/Article.aspx?id=2167&h=New-jewellery-website-targets-|title=New jewellery website targets 'customisers'|last=Weinman|first=Aaron|date=21 February 2012|publisher=Jeweller Magazine|language=|accessdate=6 January 2015}}</ref> This kind of customization is also relevant in other areas like Consumer Electronics<ref>{{Cite web|url=http://www.3ders.org/articles/20160121-philips-launches-worlds-first-personalized-3d-printed-face-shaver-for-limited-edition-run.html|title=Philips launches the world's first personalized, 3D printed face shaver for limited edition run|website=3ders.org|language=en-US|access-date=2016-03-02}}</ref> and Retail.<ref>{{Cite web|url=http://twikblog.twikit.com/belgian-3d-company-twikit-brings-3d-customization-french-retail/|title=Twikit brings 3D customization to French retail.|website=Twikit Blog {{!}} 3D Customization, 3D Printing|language=en-US|access-date=2016-03-02}}</ref> By combining 3D printing with complex software a product can easily be customized by an end-user.

== Mass personalization ==

{{tone|section|date=January 2011}}

Mass personalization is defined as custom tailoring by a company in accordance with its end users tastes and preferences.<ref>{{cite web|url=http://www.answers.com/personalization&r=67 |title=personalize: Definition, Synonyms from |publisher=Answers.com |date= |accessdate=2013-01-16}}</ref> From collaborative engineering perspective, mass customization can be viewed as collaborative efforts between customers and manufacturers, who have different sets of priorities and need to jointly search for solutions that best match customers' individual specific needs with manufacturers' customization capabilities.<ref>	Chen, S., Y. Wang and M. M. Tseng. 2009. Mass Customization as a Collaborative Engineering Effort. International Journal of Collaborative Engineering, 1(2): 152-167</ref> The main difference between mass customization and mass personalization is that customization is the ability for a company to give its customers an opportunity to create and choose product to certain specifications, but does have limits.<ref>Haag et al., ''Management Information Systems for the Information Age'', 3rd edition, 2006, page 331.</ref>

A website knowing a user's location, and buying habits, will present offers and suggestions tailored to the user's demographics; this is an example of mass personalization. The personalization is not individual but rather the user is first classified and then the personalization is based on the group they belong to.<ref>{{cite news| url=http://www.telegraph.co.uk/foodanddrink/9808015/How-supermarkets-prop-up-our-class-system.html | location=London | work=The Daily Telegraph | first=Harry | last=Wallop | title=How supermarkets prop up our class system | date=2013-01-18}}</ref>

[[Behavioral targeting]] represents a concept that is similar to mass personalization.

== Predictive personalization ==

Predictive personalization is defined as the ability to predict customer behavior, needs or wants - and tailor offers and communications very precisely.<ref>{{cite web|url=http://www.slideshare.net/jwtintelligence/jwt-10-trends-for-2013-executive-summary|title=10 Trends for 2013 Executive Summary: Definition, Projected Trends |publisher=JWTIntelligence.com |date= |accessdate=2012-12-04}}</ref>  Social data is one source of providing this predictive analysis, particularly social data that is structured.  Predictive personalization is a much more recent means of personalization and can be used well to augment current personalization offerings.

== Map personalization ==
{{Expand section|date=September 2015}}Digital [[Web mapping|web maps]] are also being personalized. [[Google Maps]] change the content of the map based on previous searches and other profile information.<ref>{{Cite web|title = The Next Frontier For Google Maps Is Personalization|url = http://social.techcrunch.com/2013/02/01/the-next-frontier-for-google-maps-is-personalization/|website = TechCrunch|accessdate = 2015-09-13|first = Frederic|last = Lardinois}}</ref> Technology writer [[Evgeny Morozov]] has criticized map personalization as a threat to [[public space]].<ref>{{Cite news|title = My Map or Yours?|url = http://www.slate.com/articles/technology/future_tense/2013/05/google_maps_personalization_will_hurt_public_space_and_engagement.html|newspaper = Slate|date = 2013-05-28|access-date = 2015-09-13|issn = 1091-2339|language = en|first = Evgeny|last = Morozov}}</ref>

==See also==
* [[Adaptation (computer science)]]
* [[Mass customization]]
* [[Adaptive hypermedia]]
* [[Behavioral targeting]]
* [[Bespoke]]
* [[Collaborative filtering]]
* [[Configurator]]
* [[Personalized learning]]
* [[Preorder economy]]
* [[Real-time marketing]]
* [[Recommendation system]]
* [[User modeling]]

==References==
{{reflist|2}}

==External links==
* [http://www.iimcp.org International Institute on Mass Customization & Personalization which organizes MCP, a biannual conference on customization and personalization]
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] ''The Journal of Personalization Research''

[[Category:Human–computer interaction]]
[[Category:World Wide Web]]
[[Category:User interface techniques]]
[[Category:Usability|Personas]]
[[Category:Types of marketing]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:663
<=====title=====>:
Cosine similarity
<=====text=====>:
'''Cosine similarity''' is a measure of similarity between two non zero vectors of an [[inner product space]] that measures the [[cosine]] of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. The name derives from the term "direction cosine": in this case, note that unit vectors are maximally "similar" if they're parallel and maximally "dissimilar" if they're orthogonal (= perpendicular).  It should not escape the alert reader's attention that this is analogous to cosine, which is unity (maximum value) when the segments subtend a zero angle and zero (uncorrelated) when the segments are perpendicular.

Note that these bounds apply for any number of dimensions, and cosine similarity is most commonly used in high-dimensional positive spaces. For example, in [[information retrieval]] and [[text mining]], each term is notionally assigned a different dimension and a document is characterised by a vector where the value of each dimension corresponds to the number of times that term appears in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be in terms of their subject matter.<ref>Singhal, Amit (2001). "Modern Information Retrieval: A Brief Overview". Bulletin of the IEEE Computer Society Technical Committee on Data Engineering 24 (4): 35–43.</ref>

The technique is also used to measure cohesion within clusters in the field of [[data mining]].<ref>P.-N. Tan, M. Steinbach & V. Kumar, "Introduction to Data Mining", , Addison-Wesley (2005), ISBN 0-321-32136-7, chapter 8; page 500.</ref>

''Cosine distance'' is a term often used for the complement in positive space, that is: <math>D_C(A,B) = 1 - S_C(A,B)</math>. It is important to note, however, that this is not a proper [[distance metric]] as it does not have the [[triangle inequality]] property—or, more formally, the [[Schwarz inequality]]—and it violates the coincidence axiom; to repair the triangle inequality property while maintaining the same ordering, it is necessary to convert to angular distance (see below.)

One of the reasons for the popularity of cosine similarity is that it is very efficient to evaluate, especially for sparse vectors, as only the non-zero dimensions need to be considered.

==Definition==

The cosine of two non zero vectors can be derived by using the [[Euclidean vector#Dot product|Euclidean dot product]] formula:

:<math>\mathbf{a}\cdot\mathbf{b}
=\left\|\mathbf{a}\right\|\left\|\mathbf{b}\right\|\cos\theta</math>

Given two [[Vector (geometric)|vectors]] of attributes, ''A'' and ''B'', the cosine similarity, ''cos(θ)'', is represented using a [[dot product]] and [[Magnitude (mathematics)#Euclidean vector space|magnitude]] as

:<math>\text{similarity} = \cos(\theta) = {\mathbf{A} \cdot \mathbf{B} \over \|\mathbf{A}\| \|\mathbf{B}\|} = \frac{ \sum\limits_{i=1}^{n}{A_i  B_i} }{ \sqrt{\sum\limits_{i=1}^{n}{A_i^2}}  \sqrt{\sum\limits_{i=1}^{n}{B_i^2}} }</math> , where <math>A_i</math> and <math>B_i</math> are [[Euclidean vector#Decomposition|components]] of vector <math>A</math> and <math>B</math> respectively.

The resulting similarity ranges from &minus;1 meaning exactly opposite, to 1 meaning exactly the same, with 0 indicating orthogonality (decorrelation), and in-between values indicating intermediate similarity or dissimilarity.

For text matching, the attribute vectors ''A'' and ''B'' are usually the [[tf-idf|term frequency]] vectors of the documents.  The cosine similarity can be seen as a method of normalizing document length during comparison.

In the case of [[information retrieval]], the cosine similarity of two documents will range from 0 to 1, since the term frequencies ([[tf-idf]] weights) cannot be negative. The angle between two term frequency vectors cannot be greater than&nbsp;90°.

If the attribute vectors are normalized by subtracting the vector means (e.g., <math>A - \bar{A}</math>), the measure is called centered cosine similarity and is equivalent to the [[Pearson product-moment correlation coefficient#For a sample|Pearson Correlation Coefficient]].

=== Angular distance and similarity ===

The term "cosine similarity" is sometimes used to refer to different definition of similarity provided below. However the most common use of "cosine similarity" is as defined above and the similarity and distance metrics defined below are referred to as "angular similarity" and "angular distance" respectively. The normalized angle between the vectors is a formal [[distance metric]] and can be calculated from the similarity score defined above. This angular distance metric can then be used to compute a similarity function bounded between 0 and 1, inclusive.

When the vector elements may be positive or negative:

:<math>\text{distance} = \frac{ \cos^{-1}( \text{similarity} ) }{ \pi } </math>

:<math>\text{similarity} = 1 - \text{distance} </math>

Or, if the vector elements are always positive:

:<math>\text{distance} = \frac{ 2 \cdot \cos^{-1}( \text{similarity} ) }{ \pi }</math>

:<math>\text{similarity} = 1 - \text{distance}</math>

Although the term "cosine similarity" has been used for this angular distance, the term is oddly used as the cosine of the angle is used only as a convenient mechanism for calculating the angle itself and is no part of the meaning. The advantage of the angular similarity coefficient is that, when used as a difference coefficient (by subtracting it from 1) the resulting function is a proper [[distance metric]], which is not the case for the first meaning. However, for most uses this is not an important property. For any use where only the relative ordering of similarity or distance within a set of vectors is important, then which function is used is immaterial as the resulting order will be unaffected by the choice.

=== Confusion with "Tanimoto" coefficient ===

The cosine similarity may be easily confused with the Tanimoto metric - a specialised form of a similarity coefficient with a similar algebraic form:

:<math>T(A,B) = {A \cdot B \over \|A\|^2 +\|B\|^2 - A \cdot B}</math>

In fact, this algebraic form [[Jaccard index#Tanimoto Similarity and Distance|was first defined by Tanimoto]] as a mechanism for calculating the [[Jaccard coefficient]] in the case where the sets being compared are represented as [[bit vector]]s. While the formula extends to vectors in general, it has quite different properties from cosine similarity and bears little relation other than its superficial appearance.

=== Ochiai coefficient ===
This coefficient is also known in biology as Ochiai coefficient, or Ochiai-Barkman coefficient, or Otsuka-Ochiai coefficient:<ref>''Ochiai A.'' Zoogeographical studies on the soleoid fishes found Japan and its neighboring regions. II // Bull. Jap. Soc. sci. Fish. 1957. V. 22. № 9. P. 526-530.</ref><ref>''Barkman J.J.'' Phytosociology and ecology of cryptogamic epiphytes, including a taxonomic survey and description of their vegetation units in Europe. – Assen. Van Gorcum. 1958. 628 p.</ref>
:<math>K =\frac{n(A \cap B)}{\sqrt{n(A) \times n(B)}}</math>
Here, <math>A</math> and <math>B</math> are sets, and <math>n(A)</math> is the number of elements in <math>A</math>. If sets are represented as [[bit vector]]s, the Ochiai coefficient can be seen to be the same as the cosine similarity.

== Properties ==
Cosine similarity is related to [[Euclidean distance]] as follows. Denote Euclidean distance by the usual <math>\|A - B\|</math>, and observe that

:<math>\|A - B\|^2 = (A - B)^\top (A - B) = \|A\|^2 + \|B\|^2 - 2 A^\top B</math>

by [[Polynomial expansion|expansion]]. When {{mvar|A}} and {{mvar|B}} are normalized to unit length, <math>\|A\|^2 = \|B\|^2 = 1</math> so the previous is equal to

:<math>2 (1 - \cos(A, B))</math>

'''Null distribution:''' For data which can be negative as well as positive, the [[null distribution]] for cosine similarity is the distribution of the dot product of two independent random unit vectors. This distribution has a [[mean]] of zero and a [[variance]] of <math>1/n</math> (where <math>n</math> is the number of dimensions), and although the distribution is bounded between -1 and +1, as <math>n</math> grows large the distribution is increasingly well-approximated by the [[normal distribution]].<ref>{{cite journal
 | author = Spruill, Marcus C
 | year = 2007
 | title = Asymptotic distribution of coordinates on high dimensional spheres
 | journal = Electronic communications in probability
 | volume = 12 | pages = 234–247
 | doi = 10.1214/ECP.v12-1294
}}</ref><ref>[http://stats.stackexchange.com/questions/85916/distribution-of-dot-products-between-two-random-unit-vectors-in-mathbbrd CrossValidated: Distribution of dot products between two random unit vectors in RD]</ref>
For other types of data, such as bitstreams (taking values of 0 or 1 only), the null distribution will take a different form, and may have a nonzero mean.<ref>{{cite journal
 | author = Graham L. Giller 
 | year = 2012
 | title = The Statistical Properties of Random Bitstreams and the Sampling Distribution of Cosine Similarity
 | journal = Giller Investments Research Notes
 | number = 20121024/1
 | doi = 10.2139/ssrn.2167044
}}</ref>

== Soft cosine measure ==
Soft cosine measure
is a measure of “soft” similarity between two vectors, i.e., the measure that considers similarity of pairs of features.<ref>{{cite journal|last1=Sidorov|first1=Grigori|last2=Gelbukh|first2=Alexander|last3=Gómez-Adorno|first3=Helena|last4=Pinto|first4=David|title=Soft Similarity and Soft Cosine Measure: Similarity of Features in Vector Space Model|journal=Computación y Sistemas|volume=18|issue=3|pages=491–504|doi=10.13053/CyS-18-3-2043|url=http://cys.cic.ipn.mx/ojs/index.php/CyS/article/view/2043|accessdate=7 October 2014}}</ref> The traditional cosine similarity considers the [[vector space model]] (VSM) features as independent or completely different, while the soft cosine measure proposes considering the similarity of features in VSM, which allows generalization of the concepts of cosine measure and also the idea of similarity (soft similarity).

For example, in the field of [[natural language processing]] (NLP) the similarity among features is quite intuitive. Features such as words, n-grams or syntactic n-grams<ref>{{cite book|last1=Sidorov|first1=Grigori|last2=Velasquez|first2=Francisco|last3=Stamatatos|first3=Efstathios|last4=Gelbukh|first4=Alexander|last5=Chanona-Hernández|first5=Liliana|title=Syntactic Dependency-based N-grams as Classification Features|publisher=LNAI 7630|isbn=978-3-642-37798-3|pages=1–11|url=http://link.springer.com/chapter/10.1007%2F978-3-642-37798-3_1|accessdate=7 October 2014}}</ref> can be quite similar, though formally they are considered as different features in the VSM. For example, words “play” and “game” are different words and thus are mapped to different dimensions in VSM; yet it is obvious that they are related semantically. In case of [[n-grams]] or syntactic n-grams, [[Levenshtein distance]] can be applied (in fact, Levenshtein distance can be applied to words as well).

For calculation of the soft cosine measure, the matrix {{math|'''s'''}} of similarity between features is introduced. It can be calculated using Levenshtein distance or other similarity measures, e.g., various [[WordNet]] similarity measures. Then we just multiply by this matrix.

Given two {{math|''N''}}-dimension vectors a and b, the soft cosine similarity is calculated as follows:

:<math>\begin{align}
    \operatorname{soft\_cosine}_1(a,b)=
    \frac{\sum\nolimits_{i,j}^N s_{ij}a_ib_j}{\sqrt{\sum\nolimits_{i,j}^N s_{ij}a_ia_j}\sqrt{\sum\nolimits_{i,j}^N s_{ij}b_ib_j}},
\end{align}
</math>

where {{math|''s<sub>ij</sub>'' {{=}} similarity(feature<sub>''i''</sub>, feature<sub>''j''</sub>)}}.

If there is no similarity between features ({{math|''s<sub>ii</sub>'' {{=}} 1}}, {{math|''s<sub>ij</sub>'' {{=}} 0}} for {{math|''i'' ≠ ''j''}}), the given equation is equivalent to the conventional cosine similarity formula.

The complexity of this measure is quadratic, which makes it perfectly applicable to real world tasks. The complexity can be transformed to subquadratic.{{citation needed|date=December 2015}}

== See also ==
* [[Sørensen similarity index|Sørensen's quotient of similarity]]
* [[Hamming distance]]
* [[Correlation]]
* [[Dice's coefficient]]
* [[Jaccard index]]
* [[SimRank]]
* [[Information retrieval]]

==References==
{{reflist}}

== External links ==
* [http://mathforum.org/kb/message.jspa?messageID=5658016&tstart=0 Weighted cosine measure]
* [http://blog.christianperone.com/?p=2497 A tutorial on cosine similarity using Python]
* [http://www.rxnlp.com/api-reference/text-similarity-api-reference/ Web API to Compute Cosine, Jaccard and Dice for Text in Any Language]

{{DEFAULTSORT:Cosine Similarity}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:666
<=====title=====>:
Collaborative filtering
<=====text=====>:
{{external links|date=November 2013}}
{{Use dmy dates|date=June 2013}}
{{Recommender systems}}
[[File:Collaborative filtering.gif|300px|thumb|

This image shows an example of predicting of the user's rating using [[Collaborative software|collaborative]] filtering. At first, people rate different items (like videos, images, games). After that, the system is making [[prediction]]s about user's rating for an item, which the user hasn't rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in our case the system has made a prediction, that the active user won't like the video.]]

'''Collaborative filtering''' ('''CF''') is a technique used by [[recommender system]]s.<ref name="handbook">Francesco Ricci and Lior Rokach and Bracha Shapira, [http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf Introduction to Recommender Systems Handbook], Recommender Systems Handbook, Springer, 2011, pp. 1-35</ref> Collaborative filtering has two senses, a narrow one and a more general one.<ref name=recommender>{{cite web|title=Beyond Recommender Systems: Helping People Help Each Other|url=http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf|publisher=Addison-Wesley|accessdate=16 January 2012|page=6|year=2001|last1=Terveen|first1=Loren|last2=Hill|first2=Will|authorlink1=Loren Terveen}}</ref>  

In the newer, narrower sense, collaborative filtering is a method of making automatic [[prediction]]s (filtering) about the interests of a user by collecting preferences or [[taste (sociology)|taste]] information from [[crowdsourcing|many users]] (collaborating). The underlying assumption of the collaborative filtering approach is that if a person ''A'' has the same opinion as a person ''B'' on an issue, A is more likely to have B's opinion on a different issue ''x'' than to have the opinion on x of a person chosen randomly. For example, a collaborative filtering recommendation system for [[television]] tastes could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes).<ref>[http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations An integrated approach to TV & VOD Recommendations] {{webarchive |url=https://web.archive.org/web/20120606225352/http://www.redbeemedia.com/insights/integrated-approach-tv-vod-recommendations |date=6 June 2012 }}</ref> Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an [[average]] (non-specific) score for each item of interest, for example based on its number of [[vote]]s.

In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc.<ref name="recommender" />  Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications  where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.

==Introduction==
The [[internet growth|growth]] of the [[Internet]] has made it much more difficult to effectively [[information extraction|extract useful information]] from all the available [[online information]]. The overwhelming amount of data necessitates  mechanisms for efficient [[information filtering]]. Collaborative filtering is one of the techniques used for dealing with this problem.

The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with tastes similar to themselves. Collaborative filtering encompasses techniques for matching people with similar interests and making [[recommender system|recommendations]] on this basis.

Collaborative filtering algorithms often require (1) users' active participation, (2) an easy way  to represent users' interests, and (3) algorithms that are able to match people with similar interests.

Typically, the workflow of a collaborative filtering system is:
# A user expresses his or her preferences by rating items (e.g. books, movies or CDs) of the system. These ratings can be viewed as an approximate representation of the user's interest in the corresponding domain.
# The system matches this user's ratings against other users'  and finds the people with most "similar" tastes.
# With similar users, the system recommends items that the similar users have rated highly but not yet being rated by this user (presumably the absence of rating is often considered as the unfamiliarity of an item)
A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.

==Methodology==

[[File:Collaborative Filtering in Recommender Systems.jpg|thumb|Collaborative Filtering in Recommender Systems]]

Collaborative filtering systems have many forms, but many common systems can be reduced to two steps:
# Look for users who share the same rating patterns with the active user (the user whom the prediction is for).
# Use the ratings from those like-minded users found in step 1 to calculate a prediction for the active user
This falls under the category of user-based collaborative filtering. A specific application of this is the user-based [[K-nearest neighbor algorithm|Nearest Neighbor algorithm]].

Alternatively, [[item-item collaborative filtering|item-based collaborative filtering]] (users who bought x also bought y), proceeds in an item-centric manner:
# Build an item-item matrix determining relationships between pairs of items
# Infer the tastes of the current user by examining the matrix and matching that user's data
See, for example, the [[Slope One]] item-based collaborative filtering family.

Another form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance.  These predictions then have to be filtered through [[business logic]] to determine how they might affect the actions of a business system.  For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.

Relying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as [[WWW|web]] search and [[data clustering]].

==Types==

===Memory-based===
This approach uses user rating data to compute the similarity between users or items. This is used for making recommendations. This was an early approach used in many commercial systems. It's effective and easy to implement. Typical examples of this approach are neighbourhood-based CF and item-based/user-based top-N recommendations. For example, in user based approaches, the value of ratings user 'u' gives to item 'i' is calculated as an aggregation of some similar users' rating of the item:
:<math>r_{u,i} = \operatorname{aggr}_{u^\prime \in U} r_{u^\prime, i}</math>

where 'U' denotes the set of top 'N' users that are most similar to user 'u' who rated item 'i'. Some examples of the aggregation function includes:
:<math>r_{u,i} = \frac{1}{N}\sum\limits_{u^\prime \in U}r_{u^\prime, i}</math>
:<math>r_{u,i} = k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)r_{u^\prime, i}</math>
:<math>r_{u,i} = \bar{r_u} +  k\sum\limits_{u^\prime \in U}\operatorname{simil}(u,u^\prime)(r_{u^\prime, i}-\bar{r_{u^\prime}} )</math>

where k is a normalizing factor defined as <math>k =1/\sum_{u^\prime \in U}|\operatorname{simil}(u,u^\prime)| </math>. and <math>\bar{r_u}</math> is the average rating of user u for all the items rated by u.

The neighborhood-based algorithm calculates the similarity between two users or items produces a prediction for the user by taking the [[weighted average]] of all the ratings. Similarity computation between items or users is an important part of this approach. Multiple measures, such as [[Pearson product-moment correlation coefficient|Pearson correlation]] and [[Cosine similarity|vector cosine]] based similarity are used for this.

The Pearson correlation similarity of two users x, y is defined as 
:<math> \operatorname{simil}(x,y) = \frac{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})(r_{y,i}-\bar{r_y})}{\sqrt{\sum\limits_{i \in I_{xy}}(r_{x,i}-\bar{r_x})^2\sum\limits_{i \in I_{xy}}(r_{y,i}-\bar{r_y})^2}} </math>

where I<sub>xy</sub> is the set of items rated by both user x and user y.

The cosine-based approach defines the cosine-similarity between two users x and y as:<ref name="Breese1999">John S. Breese, David Heckerman, and Carl Kadie, [http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=231&proceeding_id=14 Empirical Analysis of Predictive Algorithms for Collaborative Filtering], 1998 {{webarchive |url=https://web.archive.org/web/20131019134152/http://uai.sis.pitt.edu/displayArticleDetails.jsp?mmnu=1&smnu=2&article_id=231&proceeding_id=14 |date=19 October 2013 }}</ref>
:<math>\operatorname{simil}(x,y) = \cos(\vec x,\vec y) = \frac{\vec x \cdot \vec y}{||\vec x|| \times ||\vec y||} = \frac{\sum\limits_{i \in I_{xy}}r_{x,i}r_{y,i}}{\sqrt{\sum\limits_{i \in I_{x}}r_{x,i}^2}\sqrt{\sum\limits_{i \in I_{y}}r_{y,i}^2}}</math>

The user based top-N recommendation algorithm uses a similarity-based vector model to identify the k most similar users to an active user. After the k most similar users are found, their corresponding user-item matrices are aggregated to identify the set of items to be recommended. A popular method to find the similar users is the [[Locality-sensitive hashing]], which implements the [[Nearest neighbor search|nearest neighbor mechanism]] in linear time.

The advantages with this approach include: the explainability of the results, which is an important aspect of recommendation systems; easy creation and use; easy facilitation of new data; content-independence of the items being recommended; good scaling with co-rated items.

There are also several disadvantages with this approach. Its performance decreases when [[sparsity|data gets sparse]], which occurs frequently with web-related items. This hinders the [[scalability]] of this approach and creates problems with large datasets. Although it can efficiently handle new users because it relies on a [[data structure]], adding new items becomes more complicated since that representation usually relies on a specific [[vector space]]. Adding new items requires inclusion of the new item and the re-insertion of all the elements in the structure.

===Model-based===
Models are developed using [[data mining]], [[machine learning]] algorithms to find patterns based on training data. These are used to make predictions for real data. There are many model-based CF algorithms. These include [[Bayesian networks]], [[Cluster Analysis|clustering models]], [[Latent Semantic Indexing|latent semantic models]] such as [[singular value decomposition]], [[probabilistic latent semantic analysis]], multiple multiplicative factor, [[latent Dirichlet allocation]] and [[Markov decision process]] based models.<ref name="Suetal2009">Xiaoyuan Su, Taghi M. Khoshgoftaar, [http://www.hindawi.com/journals/aai/2009/421425/ A survey of collaborative filtering techniques], Advances in Artificial Intelligence archive, 2009.</ref>

This approach has a more holistic goal to uncover latent factors that explain observed ratings.<ref>[http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] {{webarchive |url=https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 |date=23 October 2010 }}</ref> Most of the models are based on creating a classification or clustering technique to identify the user based on the training set. The number of the parameters can be reduced based on types of [[Principal Component Analysis|principal component analysis]].

There are several advantages with this paradigm. It handles the sparsity better than memory based ones. This helps with scalability with large data sets. It improves the prediction performance. It gives an intuitive rationale for the recommendations.

The disadvantages with this approach are in the expensive model building. One needs to have a tradeoff between prediction performance and scalability. One can lose useful information due to reduction models. A number of models have difficulty explaining the predictions.

===Hybrid===
A number of applications combine the memory-based and the model-based CF algorithms. These overcome the limitations of native CF approaches and improve prediction performance. Importantly, they overcome the CF problems such as sparsity and loss of information. However, they have increased complexity and are expensive to implement.<ref>{{cite journal | url = http://www.sciencedirect.com/science/article/pii/S0020025512002587 | doi=10.1016/j.ins.2012.04.012 | volume=208 | title=Kernel-Mapping Recommender system algorithms | journal=Information Sciences | pages=81–104}}
</ref> Usually most commercial recommender systems are hybrid, for example, the Google news recommender system.<ref>{{cite web|url=http://dl.acm.org/citation.cfm?id=1242610|title=Google news personalization|publisher=}}</ref>

==Application on social web==
Unlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like [[Reddit]], [[YouTube]], and [[Last.fm]] are typical example of collaborative filtering based media.<ref>[http://www.readwriteweb.com/archives/collaborative_filtering_social_web.php Collaborative Filtering: Lifeblood of The Social Web]</ref>

One scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of [[Reddit]] as they are "voted up" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.

Another aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.

===Problems===
A collaborative filtering system does not necessarily succeed in automatically matching content to one's preferences. Unless the platform achieves unusually good diversity and independence of opinions, one point of view will always dominate another in a particular community. As in the personalized recommendation scenario, the introduction of new users or new items can cause the [[cold start]] problem, as there will be insufficient data on these new entries for the collaborative filtering to work accurately. In order to make appropriate recommendations for a new user, the system must first learn the user's preferences by analysing past voting or rating activities. The collaborative filtering system requires a substantial number of users to rate a new item before that item can be recommended.

==Challenges==

===Data sparsity===
In practice, many commercial recommender systems are based on large datasets. As a result, the user-item matrix used for collaborative filtering could be extremely large and sparse, which brings about the challenges in the performances of the recommendation.

One typical problem caused by the data sparsity is the [[cold start]] problem. As collaborative filtering methods recommend items based on users' past preferences,  new users will need to rate sufficient number of items to enable the system to capture their preferences accurately and thus provides reliable recommendations.

Similarly,  new items also have the same problem. When new items are added to system, they need to be rated by substantial number of users before they could be recommended to users who have similar tastes with the ones rated them. The new item problem does not limit the [[Content-based filtering|content-based recommendation]], because the recommendation of an item is based on its discrete set of descriptive qualities rather than its ratings.

===Scalability===
As the numbers of users and items grow, traditional CF algorithms will suffer serious scalability problems{{Citation needed|date=April 2013}}. For example, with tens of millions of customers <math>O(M)</math> and millions of items <math>O(N)</math>, a CF algorithm with the complexity of <math>n</math> is already too large. As well, many systems need to react immediately to online requirements and make recommendations for all users regardless of their purchases and ratings history, which demands a higher scalability of a CF system. Large web companies such as Twitter use clusters of machines to scale recommendations for their millions of users, with most computations happening in very large memory machines.<ref name="twitterwtf">Pankaj Gupta, Ashish Goel, Jimmy Lin, Aneesh Sharma, Dong Wang, and Reza Bosagh Zadeh [http://dl.acm.org/citation.cfm?id=2488433 WTF: The who-to-follow system at Twitter], Proceedings of the 22nd international conference on World Wide Web</ref>

===Synonyms===
[[Synonyms]] refers to the tendency of a number of the same or very similar items to have different names or entries. Most recommender systems are unable to discover this latent association and thus treat these products differently.

For example, the seemingly different items "children movie" and "children film" are actually referring to the same item. Indeed, the degree of variability in descriptive term usage is greater than commonly suspected.{{citation needed|date=September 2013}} The prevalence of synonyms decreases the recommendation performance of CF systems. Topic Modeling (like the [[Latent Dirichlet Allocation]] technique) could solve this by grouping different words belonging to the same topic.{{citation needed|date=September 2013}}

===Gray sheep===
Gray sheep refers to the users whose opinions do not consistently agree or disagree with any group of people and thus do not benefit from collaborative filtering. [[Black sheep]] are the opposite group whose idiosyncratic tastes make recommendations nearly impossible. Although this is a failure of the recommender system, non-electronic recommenders also have great problems in these cases, so black sheep is an acceptable failure.

===Shilling attacks===
In a recommendation system where everyone can give the ratings, people may give lots of positive ratings  for their own items and negative ratings for their competitors. It is often necessary for the collaborative filtering systems to introduce precautions to discourage such kind of manipulations.

===Diversity and the long tail===
Collaborative filters are expected to increase diversity because they help us discover new products. Some algorithms, however, may unintentionally do the opposite. Because collaborative filters recommend products based on past sales or ratings, they cannot usually recommend products with limited historical data. This can create a rich-get-richer effect for popular products, akin to [[positive feedback]]. This bias toward popularity can prevent what are otherwise better consumer-product matches. A [[Wharton School of the University of Pennsylvania|Wharton]] study details this phenomenon along with several ideas that may promote diversity and the "[[long tail]]."<ref>{{cite journal| last1= Fleder | first1= Daniel | first2= Kartik |last2= Hosanagar | title=Blockbuster Culture's Next Rise or Fall: The Impact of Recommender Systems on Sales Diversity|journal=Management Science |date=May 2009|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=955984 | doi = 10.1287/mnsc.1080.0974 }}</ref> Several collaborative filtering algorithms have been developed to promote diversity and the "[[long tail]]" by recommending novel, unexpected,<ref>{{cite journal| last1= Adamopoulos | first1= Panagiotis | first2= Alexander |last2= Tuzhilin | title=On Unexpectedness in Recommender Systems: Or How to Better Expect the Unexpected|journal=ACM Transactions on Intelligent Systems and Technology |date=January 2015|url=http://dl.acm.org/citation.cfm?id=2559952 | doi = 10.1145/2559952}}</ref> and serendipitous items.<ref>{{cite journal| last1= Adamopoulos | first1= Panagiotis | title=Beyond rating prediction accuracy: on new perspectives in recommender systems|journal=Proceedings of the 7th ACM conference on Recommender systems |date=October 2013|url=http://dl.acm.org/citation.cfm?id=2508073| doi = 10.1145/2507157.2508073}}</ref>

==Innovations==
{{Prose|date=May 2012}}
* New algorithms have been developed for CF as a result of the [[Netflix prize]].
* Cross-System Collaborative Filtering where user profiles across multiple [[recommender systems]] are combined in a privacy preserving manner.
* [[Robust collaborative filtering]], where recommendation is stable towards efforts of manipulation. This research area is still active and not completely solved.<ref>{{cite web|url=http://dl.acm.org/citation.cfm?id=1297240 |title=Robust collaborative filtering |doi=10.1145/1297231.1297240 |publisher=Portal.acm.org |date=19 October 2007 |accessdate=2012-05-15}}</ref>

==See also==
{{div col|3}}
* [[Attention Profiling Mark-up Language|Attention Profiling Mark-up Language (APML)]]
* [[Cold start]]
* [[Collaborative model]]
* [[Collaborative search engine]]
* [[Collective intelligence]]
* [[Customer engagement]]
* [[Delegative Democracy]], the same principle applied to voting rather than filtering
* [[Enterprise bookmarking]]
* [[Firefly (website)]], a defunct website which was based on collaborative filtering
* [[Filter bubble]]
* [[Preference elicitation]]
* [[Recommendation system]]
* [[Relevance (information retrieval)]]
* [[Reputation system]]
* [[Robust collaborative filtering]]
* [[Similarity search]]
* [[Slope One]]
* [[Social translucence]]
{{div col end}}

==References==
{{Reflist|30em}}

==External links==
*[http://www.grouplens.org/papers/pdf/rec-sys-overview.pdf ''Beyond Recommender Systems: Helping People Help Each Other''], page 12, 2001
*[http://www.prem-melville.com/publications/recommender-systems-eml2010.pdf Recommender Systems.] Prem Melville and Vikas Sindhwani. In Encyclopedia of Machine Learning, Claude Sammut and Geoffrey Webb (Eds), Springer, 2010.
*[http://arxiv.org/abs/1203.4487 Recommender Systems in industrial contexts - PHD thesis (2012) including a comprehensive overview of many collaborative recommender systems]
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1423975  Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions]{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}. Adomavicius, G. and Tuzhilin, A. IEEE Transactions on Knowledge and Data Engineering 06.2005
*[https://web.archive.org/web/20060527214435/http://ectrl.itc.it/home/laboratory/meeting/download/p5-l_herlocker.pdf Evaluating collaborative filtering recommender systems] ([http://www.doi.org/ DOI]: [http://dx.doi.org/10.1145/963770.963772 10.1145/963770.963772])
*[http://www.grouplens.org/publications.html GroupLens research papers].
*[http://www.cs.utexas.edu/users/ml/papers/cbcf-aaai-02.pdf Content-Boosted Collaborative Filtering for Improved Recommendations.] Prem Melville, Raymond J. Mooney, and Ramadass Nagarajan. Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI-2002), pp.&nbsp;187–192, Edmonton, Canada, July 2002.
*[http://agents.media.mit.edu/projects.html A collection of past and present "information filtering" projects (including collaborative filtering) at MIT Media Lab]
*[http://www.ieor.berkeley.edu/~goldberg/pubs/eigentaste.pdf Eigentaste: A Constant Time Collaborative Filtering Algorithm. Ken Goldberg, Theresa Roeder, Dhruv Gupta, and Chris Perkins. Information Retrieval, 4(2), 133-151. July 2001.]
*[http://downloads.hindawi.com/journals/aai/2009/421425.pdf A Survey of Collaborative Filtering Techniques] Su, Xiaoyuan and Khoshgortaar, Taghi. M
*[http://dl.acm.org/citation.cfm?id=1242610 Google News Personalization: Scalable Online Collaborative Filtering] Abhinandan Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. International World Wide Web Conference, Proceedings of the 16th international conference on World Wide Web
*[https://web.archive.org/web/20101023032716/http://research.yahoo.com/pub/2435 Factor in the Neighbors: Scalable and Accurate Collaborative Filtering] Yehuda Koren, Transactions on Knowledge Discovery from Data (TKDD) (2009)
*[http://webpages.uncc.edu/~asaric/ISMIS09.pdf Rating Prediction Using Collaborative Filtering]
*[http://www.cis.upenn.edu/~ungar/CF/ Recommender Systems]
*[http://www2.sims.berkeley.edu/resources/collab/ Berkeley Collaborative Filtering]

{{Authority control}}

{{DEFAULTSORT:Collaborative Filtering}}
[[Category:Collaboration]]
[[Category:Collaborative software]]
[[Category:Collective intelligence]]
[[Category:Information retrieval techniques]]
[[Category:Recommender systems]]
[[Category:Social information processing]]
<=====doc_Id=====>:669
<=====title=====>:
Webometrics
<=====text=====>:
{{For|Webometrics Ranking of World Universities|Webometrics Ranking of World Universities}}
{{refimprove|date=May 2014}}
The science of '''webometrics''' (also '''cybermetrics''') tries to measure the [[World Wide Web]] to get knowledge about the number and types of [[hyperlink]]s, structure of the World Wide Web and usage patterns. According to Björneborn and Ingwersen (2004), the definition of '''webometrics''' is "the study of the quantitative aspects of the construction and use of information resources, structures and technologies on the Web drawing on [[Bibliometrics|bibliometric]] and [[informetrics|informetric]] approaches." The term ''webometrics'' was first coined by Almind and Ingwersen (1997). A second definition of webometrics has also been introduced, "the study of web-based content with primarily quantitative methods for social science research goals using techniques that are not specific to one field of study" (Thelwall, 2009), which emphasizes the development of applied methods for use in the wider social sciences. The purpose of this alternative definition was to help publicize appropriate methods outside of the information science discipline rather than to replace the original definition within information science.

Similar scientific fields are [[Bibliometrics]], [[Informetrics]], [[Scientometrics]], [[Virtual ethnography]], and [[Web mining]].
[[File:Site based graph relationship.jpg|thumb|Site based graph relationship. The idea was taken from paper "Web-communicator creation costs sharing problem as a cooperative game"{{sfn|Mazalov|Pechnikov|Chirkov|Chuyko|2010|p=189}}]]

One relatively straightforward measure is the "Web Impact Factor" (WIF) introduced by Ingwersen (1998). The WIF measure may be defined as the number of web pages in a web site receiving links from other web sites, divided by the number of web pages published in the site that are accessible to the crawler. However the use of WIF has been disregarded due to the mathematical artifacts derived from power law distributions of these variables. Other similar indicators using size of the institution instead of number of webpages have been proved more useful.

== See also ==
* [[Altmetrics]]
* [[Impact factor]]
* [[PageRank]]
* [[Network mapping]]
* [[Search engine]]
* [[Webometrics Ranking of World Universities]]

== References ==
<references />

== Bibliography ==

* {{cite journal |author1=Tomas C. Almind  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 1997 | title = Informetric analyses on the World Wide Web: Methodological approaches to 'webometrics' | journal = Journal of Documentation | volume = 53 | issue = 4 | pages = 404–426 | doi = 10.1108/EUM0000000007205}}
* {{cite journal |author1=Lennart Björneborn  |author2=Peter Ingwersen  |lastauthoramp=yes | year = 2004 | title = Toward a basic framework for webometrics | journal = Journal of the American Society for Information Science and Technology | volume = 55 | issue = 14 | pages = 1216–1227 | url = http://www3.interscience.wiley.com/cgi-bin/abstract/109594194/ABSTRACT | doi = 10.1002/asi.20077}}
*{{cite journal |author = Peter Ingwersen | year = 1998 |title = The calculation of web impact factors | journal = Journal of Documentation |volume = 54 |issue = 2 | pages = 236–243 |doi = 10.1108/EUM0000000007167}}
*{{cite journal |author1=Mike Thelwall |author2=Liwen Vaughan |author3=Lennart Björneborn | year = 2005 |title = Webometrics | journal = Annual Review of Information Science and Technology |volume = 39 | pages = 81–135 |doi = 10.1002/aris.1440390110}}
* {{cite book |author=Mike Thelwall |title= Introduction to Webometrics: Quantitative Web Research for the Social Sciences |publisher= Morgan & Claypool |year= 2009 |isbn= 978-1-59829-993-9 |url=http://www.morganclaypool.com/doi/abs/10.2200/S00176ED1V01Y200903ICR004}}
* {{cite conference 
|url            = http://www.mtas.ru/upload/library/UBS30112.pdf
|title          = Web-communicator creation costs sharing problem as a cooperative game (in Russian)
|last1=Mazalov |first1= Vladimir
|last2=Pechnikov |first2=Andrey
|last3=Chirkov |first3=Alexandr
|last4=Chuyko |first4=Julia
|year           = 2010
|booktitle      = Управление большими системами: сборник трудов
|pages          = 
|location       = 
|ref = harv
}}

* {{cite web
 |url        = http://eprints.rclis.org/7554/
 |title      = Webometrics: ten years of expansion
 |last       = Ingwersen
 |first      = Peter
 |year       = 2006
 |accessdate = 2013
 |ref = harv
}}

[[Category:World Wide Web]]
[[Category:Information science]]
[[Category:Information retrieval techniques]]


{{web-stub}}

[[pt:Webometria]]
<=====doc_Id=====>:672
<=====title=====>:
Music alignment
<=====text=====>:
[[Image:MusicAlignment_BeethovenFifth.png|thumb|300px|right|First theme of Symphony No. 5 by Ludwig van Beethoven in a sheet music, audio,
and piano-roll representation. The red bidirectional arrows indicate the aligned time positions of corresponding note events in the different representations.]]

[[Music]] can be described and represented in many different ways including [[sheet music]], symbolic representations, and audio recordings. For each of these representations, there may exist different versions that correspond to the same musical work. The general goal of '''music alignment''' (sometimes also referred to as '''music synchronization''') is to automatically link the various data streams, thus interrelating the multiple information sets related to a given musical work. More precisely, music alignment is taken to [[mean]] a procedure which, for a given position in one representation of a piece of music, determines the corresponding position within another representation.<ref name=Mueller15_Chapter3FMP_SPRINGER>
{{cite book
| last = Müller
| first = Meinard
| title = Music Synchronization. In Fundamentals of Music Processing, chapter 3, pages 115-166
| url = http://www.music-processing.de
| publisher = Springer
| year = 2015
| doi = 10.1007/978-3-319-21945-5
| isbn = 978-3-319-21944-8 }}
</ref> In the figure on the right, such an alignment is visualized by the red bidirectional arrows. Such [[synchronization]] results form the basis for novel interfaces that allow users to access, search, and browse musical content in a convenient way.<ref name=DammFTCKM12_DML_IJDL>
{{cite journal
|last1=Damm
|first1=David 
|last2=Fremerey
|first2=Christian
|last3=Thomas
|first3=Verena
|last4=Clausen
|first4=Michael
|last5=Kurth
|first5=Frank
|last6=Müller
|first6=Meinard
|title=A digital library framework for heterogeneous music collections: from document acquisition to cross-modal interaction
|url = http://link.springer.com/article/10.1007%2Fs00799-012-0087-y 
|journal=International Journal on Digital Libraries: Special Issue on Music Digital Libraries
|volume=12
|issue=2-3
|year=2012
|pages=53–71
|doi=10.1007/s00799-012-0087-y}}
</ref><ref name=MuellerCKEF10_Sync_ISR>
{{cite journal
|last1=Müller
|first1=Meinard 
|last2=Clausen
|first2=Michael
|last3=Konz
|first3=Verena
|last4=Ewert
|first4=Sebastian
|last5=Fremerey
|first5=Christian 
|title=A Multimodal Way of Experiencing and Exploring Music
|url = https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/03-publications/2010_MuellerClausenKonzEwertFremerey_MusicSynchronization_ISR.pdf 
|journal=Interdisciplinary Science Reviews (ISR)
|volume=35
|issue=2
|year=2010
|pages=138–153
|doi=10.1179/030801810X12723585301110}}</ref>

==Basic procedure==
[[File:MusicAlignment Procedure.png|thumb|300px|right|Overview of the processing pipeline of a typical music alignment procedure.]]

Given two different music representations, typical music alignment approaches proceed in two steps.<ref name=Mueller15_Chapter3FMP_SPRINGER/> In the first step, the two representations are transformed into sequences of suitable features. In general, such feature representations need to find a compromise between two conflicting goals. On the one hand, features should show a large degree of [[robustness]] to variations that are to be left unconsidered for the task at hand. On the other hand, features should capture enough characteristic information to accomplish the given task. For music alignment, one often uses '''[[chroma feature|chroma-based features]]''' (also called [[chromagram]]s or [[harmonic pitch class profiles|pitch class profiles]]), which capture harmonic and melodic characteristics of music, while being robust to changes in timbre and instrumentation, are being used.

In the second step, the derived feature sequences have to be brought into (temporal) correspondence. To this end, techniques related to '''[[dynamic time warping|dynamic time warping (DTW)]]''' or '''[[hidden Markov model|hidden Markov models (HMMs)]]''' are used to compute an optimal alignment between two given feature sequences.

==Related tasks==
Music alignment and related synchronization tasks have been studied extensively within the field of [[music information retrieval]]. In the following, we give some pointers to related tasks. Depending upon the respective types of music representations, one can distinguish between various synchronization scenarios. For example, audio alignment refers to the task of temporally aligning two different audio recordings of a piece of music. Similarly, the goal of score–audio alignment is to coordinate note events given in the score representation with audio data. In the  [[offline]] scenario, the two data streams to be aligned are known prior to the actual alignment. In this case, one can use global optimization procedures such as [[dynamic time warping|dynamic time warping (DTW)]] to find an optimal alignment. In general, it is harder to deal with scenarios where the data streams are to be processed online. One prominent online scenario is known as '''[[score following]]''', where a musician is performing a piece according to a given musical score. The goal is then to identify the currently played musical events depicted in the score with high accuracy and low latency.<ref>
{{cite journal
|last1=Cont
|first1=Arshia
|title=A Coupled Duration-Focused Architecture for Real-Time Music-to-Score Alignment
|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence
|volume=32
|issue=6
|year=2010
|pages=974–987
|issn=0162-8828
|doi=10.1109/TPAMI.2009.106}}</ref><ref>{{cite journal
|last1=Orio
|first1=Nicola
|last2=Lemouton
|first2=Serge
|last3=Schwarz
|first3=Diemo
|title=Score following: State of the art and new developments
|url = http://recherche.ircam.fr/equipes/temps-reel/suivi/resources/orio.2002.nime.pdf
|journal=Proceedings of the International Conference on New Interfaces for Musical Expression (NIME)
|date=2003
|pages=36–41}}</ref> In this scenario, the score is known as a whole in advance, but the performance is known only up to the current point in time. In this context, alignment techniques such as hidden Markov models or particle filters have been employed, where the current score position and tempo are modeled in a statistical sense.<ref>
{{cite journal
|last1=Duan
|first1=Zhiyao
|last2=Pardo
|first2=Bryan
|journal = Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)
|title=A state space model for online polyphonic audio-score alignment
|url = http://www.ece.rochester.edu/~zduan/resource/DuanPardo_ScoreFollowing_ICASSP11.pdf
|year=2011
|pages=197–200
|doi=10.1109/ICASSP.2011.5946374}}</ref><ref>{{cite journal
|last1=Montecchio
|first1=Nicola
|last2=Cont
|first2=Arshia
|title=A unified approach to real time audio-to-score and audio-to-audio alignment using sequential Montecarlo inference techniques
|url = http://articles.ircam.fr/textes/Montecchio11a/index.pdf
|year=2011
|pages=193–196
|doi=10.1109/ICASSP.2011.5946373}}</ref> As opposed to classical DTW, such an online synchronization procedure inherently has a running time that is linear in the duration of the performed version. However, as a main disadvantage, an online strategy is very sensitive to local tempo variations and deviations from the score - once the procedure is out of sync, it is very hard to recover and return to the right track. A further online synchronization problem is known as '''[[Pop music automation#Automatic accompaniment|automatic accompaniment]]'''. Having a solo part played by a musician, the task of the computer is to accompany the musician according to a given score by adjusting the tempo and other parameters in real time. Such systems were already proposed some decades ago.<ref>{{cite journal
|last1=Dannenberg
|first1=Roger B.
|title=An on-line algorithm for real-time accompaniment
|journal=Proceedings of the International Computer Music Conference (ICMC)
|url = http://www.cs.cmu.edu/~rbd/papers/icmc84accomp.pdf
|date=1984
|pages=193–198}}</ref><ref>
{{cite journal
|last1=Raphael
|first1=Christopher
|title=A probabilistic expert system for automatic musical accompaniment
|journal = Journal of Computational and Graphical Statistics
|url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.20.6559&rep=rep1&type=pdf
|year=2001
|pages=487–512
}}</ref><ref>
{{cite journal
|last2=Raphael
|first2=Christopher
|year=2006
|title=Music score alignment and computer accompaniment
|url=http://www.cs.cmu.edu/~rbd/papers/accompaniment-cacm-06.pdf
|journal=Communications of the ACM
|volume=49
|issue=8
|pages=38–43
|doi=10.1145/1145287.1145311
|issn=0001-0782
|last1=Dannenberg
|first1=Roger B.}}</ref>

==References==
{{Reflist}}

[[Category:Music information retrieval]]
[[Category:Music technology]]
[[Category:Musicology]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:675
<=====title=====>:
Personalized search
<=====text=====>:
{{Multiple issues|
{{essay-like|date=January 2015}}
{{original research|date=January 2015}}
}}

'''Personalized search''' refers to [[web search]] experiences that are tailored specifically to an individual's interests by incorporating information about the individual beyond specific query provided. Pitkow et al. describe two general approaches to [[personalizing]] search results, one involving modifying the user's query and the other re-ranking search results.<ref>{{cite journal|last=Pitokow|first=James|author2=Hinrich Schütze |author3=Todd Cass |author4=Rob Cooley |author5=Don Turnbull |author6=Andy Edmonds |author7=Eytan Adar |author8=Thomas Breuel |title=Personalized search|journal=Communications of the ACM |year=2002|volume=45|issue=9|pages=50–55|url=http://portal.acm.org/citation.cfm?doid=567498.567526}}</ref>

==History==

[[Google]] introduced personalized search in 2004 and it was implemented in 2005 to Google search. Google has personalized search implemented for all users, not only those with a Google account. There is not very much information on how exactly Google personalizes their searches; however, it is believed that they use user language, location, and [[web history]].<ref>{{cite conference | url=http://personalization.ccs.neu.edu/paper.pdf | title=Measuring Personalization of Web Search | year=2013 | archiveurl=https://web.archive.org/web/20130425195202/http://personalization.ccs.neu.edu/paper.pdf | archivedate=April 25, 2013 | deadurl=y|author1=Aniko Hannak|author2=Piotr Sapiezynski|author3=Arash Molavi Kakhki|author4=Balachander Krishnamurthy|author5=David Lazer|author6=Alan Mislove|author7=Christo Wilson}}</ref>

Early [[search engine]]s, like [[Google]] and [[AltaVista]], found results based only on key words. Personalized search, as pioneered by Google, has become far more complex with the goal to "understand exactly what you mean and give you exactly what you want."<ref name=Remerowski>{{cite AV media| last=Remerowski|first=Ted|title=National Geographic: Inside Google|year=2013}}</ref> Using mathematical algorithms, search engines are now able to return results based on the number of links to and from sites; the more links a site has, the higher it is placed on the page.<ref name=Remerowski/> Search engines have two degrees of expertise: the shallow expert and the deep expert. An expert from the shallowest degree serves as a witness who knows some specific information on a given event. A deep expert, on the other hand, has comprehensible knowledge that gives it the capacity to deliver unique information that is relevant to each individual inquirer.<ref name=Simpson>{{cite journal|last=Simpson|first=Thomas|title=Evaluating Google as an epistemic tool|journal=Metaphilosophy|year=2012|volume=43|issue=4|pages=969–982}}</ref> If a person knows what he or she wants than the search engine will act as a shallow expert and simply locate that information. But search engines are also capable of deep expertise in that they rank results indicating that those near the top are more relevant to a user's wants than those below.<ref name=Simpson/>

While many search engines take advantage of information about people in general, or about specific groups of people, personalized search depends on a user profile that is unique to the individual. Research systems that personalize search results model their users in different ways. Some rely on users explicitly specifying their interests or on demographic/cognitive characteristics.<ref>{{cite journal|last=Ma|first=Z.|author2=Pant, G. |author3=Sheng, O. |title=Interest-based personalized search.|journal=ACM TOIS|year=2007|volume=25|issue=5}}</ref><ref>{{cite journal|last=Frias-Martinez|first=E.|author2=Chen, S.Y. |author3=Liu, X. |title=Automatic cognitive style identification of digital library users for personalization.|journal=JASIST|year=2007|volume=58|issue=2|pages=237–251|doi=10.1002/asi.20477}}</ref> However, user-supplied information can be difficult to collect and keep up to date. Others have built implicit user models based on content the user has read or their history of interaction with Web pages.<ref>{{cite journal|last=Chirita|first=P.|author2=Firan, C. |author3=Nejdl, W. |title=Summarizing local context to personalize global Web search|journal=SIGIR|year=2006|pages=287–296}}</ref><ref>{{cite journal|last=Dou|first=Z.|author2=Song, R. |author3=Wen, J.R. |title=A large-scale evaluation and analysis of personalized search strategies|journal=WWW|year=2007|pages=581–590}}</ref><ref>{{cite journal|last=Shen|first=X. |author2=Tan, B. |author3=Zhai, C.X.|title=Implicit user modeling for personalized search|journal=CIKM|year=2005|pages=824–831}}</ref><ref>{{cite journal|last=Sugiyama|first=K.|author2=Hatano, K. |author3=Yoshikawa, M. |title=Adaptive web search based on user profile constructed without any effort from the user|journal=WWW|year=2004|pages=675–684}}</ref><ref>{{cite journal|last=Teevan|first=J.|author2=Dumais, S.T. |author3=Horvitz, E. |title=Personalizing search via automated analysis of interests and activities|journal=SIGIR|year=2005|pages=415–422|url=http://people.csail.mit.edu/teevan/work/publications/papers/tochi10.pdf}}</ref>

There are several publicly available systems for personalizing Web search results (e.g., [[Google Personalized Search]] and [[Bing (search engine)|Bing]]'s search result personalization<ref>{{cite web|last=Crook|first=Aidan, and Sanaz Ahari|title=Making search yours|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx|publisher=Bing|accessdate=14 March 2011}}</ref>). However, the technical details and evaluations of these commercial systems are proprietary. One technique Google uses to personalize searches for its users is to track log in time and if the user has enabled web history in his browser. If a user accesses the same site through a search result from Google many times, it believes that they like that page. So when users carry out certain searches, Google's personalized search algorithm gives the page a boost, moving it up through the ranks. Even if a user is signed out, Google may personalize their results because it keeps a 180-day record of what a particular web browser has searched for, linked to a cookie in that browser.<ref>{{cite web|last=Sullivan|first=Danny|title=Of "Magic Keywords" and Flavors Of Personalized Search At Google|url=http://searchengineland.com/flavors-of-google-personalized-search-139286|accessdate=21 April 2014}}</ref>

In order to better understand how personalized search results are being presented to the users, a group of researchers at Northeastern University compared an aggregate set of searches from logged in users against a [[control group]]. The research team found that 11.7% of results show differences due to personalization; however, this varies widely by [[Web search query|search query]] and result ranking position.<ref name=Briggs>{{cite web|last=Briggs|first=Justin|title=A Better Understanding of Personalized Search|url=http://justinbriggs.org/better-understanding-personalized-search|date=24 June 2013|accessdate=21 April 2014}}</ref> Of various factors tested, the two that had measurable impact were being logged in with a Google account and the [[IP address]] of the searching users. It should also be noted that results with high degrees of personalization include companies and politics. One of the factors driving personalization is localization of results, with company queries showing store locations relevant to the location of the user. So, for example, if a user searched for "used car sales", Google may produce results of local car dealerships in their area. On the other hand, queries with the least amount of personalization include factual queries ("what is") and health.<ref name=Briggs/>

When measuring personalization, it is important to eliminate background noise. In this context, one type of background noise is the carry-over effect. The carry-over effect can be defined as follows: when a user performs a search and follow it with a subsequent search, the results of the second search is influenced by the first search. A noteworthy point is that the top-ranked [[URL]]s are less likely to change based off personalization, with most personalization occurring at the lower ranks. This is a style of personalization based on recent search history, but it is not a consistent element of personalization because the phenomenon times out after 10 minutes, according to the researchers.<ref name=Briggs/>

==The filter bubble==
{{Main article|Filter bubble}}

Several concerns have been brought up regarding personalized search. It decreases the likelihood of finding new information by [[bias]]ing search results towards what the user has already found. It introduces potential privacy problems in which a user may not be aware that their search results are personalized for them, and wonder why the things that they are interested in have become so relevant. Such a problem has been coined as the "filter bubble" by author [[Eli Pariser]]. He argues that people are letting major websites drive their destiny and make decisions based on the vast amount of data they've collected on individuals. This can isolate users in their own worlds or "filter bubbles" where they only see information that they want to, such a consequence of "The Friendly World Syndrome". As a result, people are much less informed of problems in the developing world which can further widen the gap between the North (developed countries) and the South (developing countries).<ref name=Pariser>{{cite book| url=http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf | title=The Filter Bubble | archiveurl=https://web.archive.org/web/20131228150122/http://www.sp.uconn.edu/~jbl00001/pariser_the%20filter%20bubble_introduction.pdf|archivedate=December 28, 2013|author=E. Pariser|year=2011}}</ref>

The methods of personalization, and how useful it is to "promote" certain results which have been showing up regularly in searches by like-minded individuals in the same community. The personalization method makes it very easy to understand how the filter bubble is created. As certain results are bumped up and viewed more by individuals, other results not favored by them are relegated to obscurity. As this happens on a community-wide level, it results in the community, consciously or not, sharing a skewed perspective of events.<ref>{{cite journal|last=Smyth|first=B.|title=Adaptive Information Access:: Personalization And Privacy |journal=International Journal of Pattern Recognition & Artificial Intelligence |year=2007|pages=183–205}}</ref>

An area of particular concern to some parts of the world is the use of personalized search as a form of control over the people utilizing the search by only giving them particular information ([[selective exposure]]). This can be used to give particular influence over highly talked about topics such as gun control or even gear people to side with a particular political regime in different countries.<ref name=Pariser/> While total control by a particular government just from personalized search is a stretch, control of the information readily available from searches can easily be controlled by the richest corporations. The biggest example of a corporation controlling the information is Google. Google is not only feeding you the information they want but they are at times using your personalized search to gear you towards their own companies or affiliates. This has led to a complete control of various parts of the web and a pushing out of their competitors such as how Google Maps took a major control over the online map and direction industry with MapQuest and others forced to take a backseat.<ref name="Consumer Watchdog">{{cite web| title=Traffic Report: How Google is squeezing out competitors and muscling into new markets|url= http://www.consumerwatchdog.org/resources/TrafficStudy-Google.pdf|date=2 June 2010|accessdate= 27 April 2014|work=Consumer Watchdog}}</ref>

Many search engines use concept-based user profiling strategies that derive only topics that users are highly interested in but for best results, according to researchers Wai-Tin and Dik Lun, both positive and negative preferences should be considered. Such profiles, applying negative and positive preferences, result in highest quality and most relevant results by separating alike queries from unalike queries. For example, typing in 'apple' could refer to either the fruit or the [[Macintosh]] computer and providing both preferences aids search engines' ability to learn which apple the user is really looking for based on the links clicked. One concept-strategy the researchers came up with to improve personalized search and yield both positive and negative preferences is the click-based method. This method captures a user's interests based on which links they click on in a results list, while downgrading unclicked links.<ref>{{cite journal|last=Wai-Tin|first=Kenneth|author2=Dik Lun, L|title=Deriving concept-based user profiles from search engine logs|journal=IEEE Transactions on Knowledge and Data Engineering|year=2010|volume=22|issue=7|pages=969–982|doi=10.1109/tkde.2009.144}}</ref>

The feature also has profound effects on the [[search engine optimization]] industry, due to the fact that search results will no longer be ranked the same way for every user.<ref>[http://www.networkworld.com/news/2009/120709-google-personalized-results-could-be.html "Google Personalized Results Could Be Bad for Search"]. ''Network World''. Retrieved July 12, 2010.</ref> An example of this is found in Eli Pariser's, The Filter Bubble, where he had two friends type in "BP" into Google's search bar. One friend found information on the BP oil spill in the Gulf of Mexico while the other retrieved investment information.<ref name=Pariser/>

Some have noted that personalized search results not only serve to customize a user's search results, but also [[Advertising|advertisements]].  This has been criticized as an [[Expectation of privacy|invasion on privacy]].<ref>{{cite web|url=http://www.seooptimizers.com/search-engines-and-customized-results-based-on-your-internet-history.html|title=Search Engines and Customized Results Based on Your Internet History|publisher=SEO Optimizers|accessdate=27 February 2013}}</ref>

==The case of Google==
{{Main article|Google Personalized Search}}

An important example of search personalization is [[Google]]. There are a host of Google applications, all of which can be personalized and integrated with the help of a Google account. Personalizing search does not require an account. However, one is almost deprived of a choice, since so many useful Google products are only accessible if one has a Google account. The Google Dashboard, introduced in 2009, covers more than 20 products and services, including Gmail, Calendar, Docs, YouTube, etc.<ref>{{cite journal|last=Mattison|first=D.|title=Time, Space, And Google: Toward A Real-Time, Synchronous, Personalized, Collaborative Web. |journal=Searcher|year=2010|pages=20–31}}</ref> that keeps track of all the information directly under one's name. The free Google Custom Search is available for individuals and big companies alike, providing the Search facility for individual websites and powering corporate sites such as that of the ''[[New York Times]]''. The high level of personalization that was available with Google played a significant part in helping remain the world's most favorite search engine.

One example of Google's ability to personalize searches is in its use of Google News. Google has geared its news to show everyone a few similar articles that can be deemed interesting, but as soon as the user scrolls down, it can be seen that the news articles begin to differ. Google takes into account past searches as well as the location of the user to make sure that local news gets to them first. This can lead to a much easier search and less time going through all of the news to find the information one want. The concern, however, is that the very important information can be held back because it does not match the criteria that the program sets for the particular user. This can create the "[[filter bubble]]" as described earlier.<ref name=Pariser/>

An interesting point about personalization that often gets overlooked is the privacy vs personalization battle. While the two do not have to be mutually exclusive, it is often the case that as one becomes more prominent, it compromises the other. Google provides a host of services to people, and many of these services do not require information to be collected about a person to be customizable. Since there is no threat of privacy invasion with these services, the balance has been tipped to favor personalization over privacy, even when it comes to search. As people reap the rewards of convenience from customizing their other Google services, they desire better search results, even if it comes at the expense of private information. Where to draw the line between the information versus search results tradeoff is new territory and Google gets to make that decision. Until people get the power to control the information that is being collected about them, Google is not truly protecting privacy.
Google's popularity as a search engine and Internet browser has allowed it to gain a lot of power. Their popularity has created millions of usernames, which have been used to collect vast amounts of information about individuals. Google can use multiple methods of personalization such as traditional, social, geographic, IP address, browser, cookies, time of day, year, behavioral, query history, bookmarks, and more. Although having Google personalize search results based on what users searched previously may have its benefits, there are negatives that come with it.<ref>{{cite web|last=Jackson|first=Mark|title=The Future of Google's Search Personalization|url=http://searchenginewatch.com/article/2067001/The-Future-of-Googles-Search-Personalization|accessdate=29 April 2014}}</ref><ref>{{cite web|last=Harry|first=David|title=Search Personalization and the User Experience|url=http://searchenginewatch.com/article/2118126/Search-Personalization-the-User-Experience|accessdate=29 April 2014}}</ref>
With the power from this information, Google has chosen to enter other sectors it owned, such as videos, document sharing, shopping, maps, and many more. Google has done this by steering searchers to their own services offered as opposed to others such as MapQuest.

Using search personalization, Google has doubled its video market share to about eighty percent. The legal definition of a [[monopoly]] is when a firm gains control of seventy to eighty percent of the market. Google has reinforced this monopoly by creating significant barriers of entry such as manipulating search results to show their own services. This can be clearly seen with Google Maps being the first thing displayed in most searches.

The analytical firm Experian Hitwise stated that since 2007, MapQuest has had its traffic cut in half because of this. Other statistics from around the same time include Photobucket going from twenty percent of market share to only three percent, Myspace going from twelve percent market share to less than one percent, and ESPN from eight percent to four percent market share. In terms of images, Photobucket went from 31% in 2007 to 10% in 2010 and Yahoo Images has gone from 12% to 7%. It becomes apparent that the decline of these companies has come because of Google's increase in market share from 43% in 2007 to about 55% in 2009.

It can be said that Google is more dominant because they provide better services. However, Experian Hitwise has also created graphs to show the market share of about fifteen different companies at once. This has been done for every category for the market share of pictures, videos, product search, and more. The graph for product search is evidence enough for Google's influence because their numbers went from 1.3 million unique visitors to 11.9 unique visitors in one month. That kind of growth can only come with the change of a process.

In the end, there are two common themes with all of these graphs. The first is that Google's market share has a directly inverse relationship to the market share of the leading competitors. The second is that this directly inverse relationship began around 2007, which is around the time that Google began to use its "Universal Search" method.<ref>{{cite web|title=TRAFFIC REPORT:How Google is Squeezing out Competitors and Muscling into New Markets |url=https://courses.lis.illinois.edu/pluginfile.php/226148/mod_resource/content/1/TrafficStudy-Google.pdf|publisher=ConsumerWatchDog.org|accessdate=29 April 2014}}</ref>

==Benefits==

One of the most critical benefits personalized search has is to improve the quality of decisions consumers make. The internet has made the transaction cost of obtaining information significantly lower than ever. However, human ability to process information has not expanded much.<ref name=Diehl>{{cite journal|author=Diehl, K.|year=2003|title=Personalization and Decision Support Tools: Effects on Search and Consumer Decision Making|journal=Advances In Consumer Research|volume=30|issue=1|pages=166–169}}</ref> When facing overwhelming amount of information, consumers need a sophisticated tool to help them make high quality decisions. Two studies examined the effects of personalized screening and ordering tools, and the results show a [[positive correlation]] between personalized search and the quality of consumers' decisions.

The first study was conducted by Kristin Diehl from the [[University of South Carolina]]. Her research discovered that reducing search cost led to lower quality choices. The reason behind this discovery was that 'consumers make worse choices because lower search costs cause them to consider inferior options.' It also showed that if consumers have a specific goal in mind, they would further their search, resulting in an even worse decision.<ref name=Diehl/> The study by Gerald Haubl from the [[University of Alberta]] and Benedict G.C. Dellaert from [[Maastricht University]] mainly focused on recommendation systems. Both studies concluded that a personalized search and recommendation system significantly improved consumers' decision quality and reduced the number of products inspected.<ref name=Diehl/>

==Models==

Personalized search gains popularity because of the demand for more relevant information. Research has indicated low success rates among major search engines in providing relevant results; in 52% of 20,000 queries, searchers did not find any relevant results within the documents that Google returned.<ref>{{cite book|author1=Coyle, M.|author2=Smyth, B.|lastauthoramp=y|year=2007|chapter=Information recovery and discovery in collaborative web search|title=Advances in Information Retrieval|pp=356–367|doi=10.1007/978-3-540-71496-5_33|isbn=978-3-540-71494-1|series=Lecture Notes in Computer Science}}</ref> Personalized search can improve search quality significantly and there are mainly two ways to achieve this goal.

The first model available is based on the users' historical searches and search locations. People are probably familiar with this model since they often find the results reflecting their current location and previous searches.

There is another way to personalize search results. In Bracha Shapira and Boaz Zabar's "Personalized Search: Integrating Collaboration and Social Networks", Shapira and Zabar focused on a model that utilizes a [[recommendation system]].<ref>{{cite journal|author1=Shapira, B.|author2=Zabar, B.|lastauthoramp=y|year=2011|title=Personalized search: Integrating collaboration and social networks|journal=Journal of the American Society for Information Science & Technology|volume=62|issue=1|pages=146–160|doi=10.1002/asi.21446}}</ref> This model shows results of other users who have searched for similar keywords. The authors examined keyword search, the recommendation system, and the recommendation system with social network working separately and compares the results in terms of search quality. The results show that a personalized search engine with the recommendation system produces better quality results than the standard search engine, and that the recommendation system with social network even improves more.

Recent paper “[https://arxiv.org/abs/1612.03597 Search personalization with embeddings]” shows that a new embedding model for search personalization, where users are embedded on a topical interest space, produces better search results than strong learning-to-rank models.

==Disadvantages==

While there are documented benefits of the implementation of search personalization, there are also arguments against its use. The foundation of this argument against its use is because it confines internet users' search engine results to material that aligns with the users' interests and history. It limits the users' ability to become exposed to material that would be relevant to the user's search query but due to the fact that some of this material differs from the user's interests and history, the material is not displayed to the user. Search personalization takes the objectivity out of the search engine and undermines the engine. "Objectivity matters little when you know what you are looking for, but its lack is problematic when you do not".<ref>{{cite journal|last=Simpson|first=Thomas W.|title=Evaluating Google As An Epistemic Tool|journal=Metaphilosophy|date=2012|volume=43.4|pages=426–445|doi=10.1111/j.1467-9973.2012.01759.x}}</ref> Another criticism of search personalization is that it limits a core function of the web: the collection and sharing of information. Search personalization prevents users from easily accessing all the possible information that is available for a specific search query.  Search personalization adds a bias to user's search queries. If a user has a particular set of interests or internet history and uses the web to research a controversial issue, the user's search results will reflect that. The user may not be shown both sides of the issue and miss potentially important information if the user's interests lean to one side or another. A study done on search personalization and its effects on search results in Google News resulted in different orders of news stories being generated by different users, even though each user entered the same search query. According to Bates, "only 12% of the searchers had the same three stories in the same order. This to me is prima facie evidence that there is filtering going on".<ref>{{cite journal|last=Bates|first=Mary Ellen|title=Is Google Hiding My News?|year=2011|journal=Online|volume=35|issue=6|pages=64}}</ref> If search personalization was not active, all the results in theory should have been the same stories in an identical order.

Another disadvantage of search personalization is that internet companies such as Google are gathering and potentially selling their users' internet interests and histories to other companies. This raises a privacy issue concerning whether people are comfortable with companies gathering and selling their internet information without their consent or knowledge.  Many web users are unaware of the use of search personalization and even fewer have knowledge that user data is a valuable commodity for internet companies.

==Sites that use it==

E. Pariser, author of ''The Filter Bubble'', explains how there are differences that search personalization has on both [[Facebook]] and Google. Facebook implements personalization when it comes to the amount of things people share and what pages they "like". An individual's [[social interaction]]s, whose profile they visit the most, who they message or chat with are all indicators that are used when Facebook uses personalization. Rather than what people share being an indicator of what is filtered out, Google takes into consideration what we "click" to filter out what comes up in our searches. In addition, Facebook searches are not necessarily as private as the Google ones. Facebook draws on the more public self and users share what other people want to see. Even while [[tag (metadata)|tag]]ging photographs, Facebook uses personalization and [[face recognition]] that will automatically assign a name to face. In terms of Google, users are provided similar websites and resources based on what they initially click on. There are even other websites that use the filter tactic to better adhere to user preferences. For example, [[Netflix]] also judges from the users search history to suggest movies that they may be interested in for the future. There are sites like [[Amazon.com|Amazon]] and personal [[shopping site]]s also use other peoples history in order to serve their interests better. [[Twitter]] also uses personalization by "suggesting" other people to follow. In addition, based on who one "follows", "tweets" and "retweets" at, Twitter filters out suggestions most relevant to the user.  [[Mark Zuckerberg]], founder of Facebook, believed that people only have one identity. E. Pariser argues that is completely false and search personalization is just another way to prove that isn't true. Although personalized search may seem helpful, it is not a very accurate representation of any person. There are instances where people also search things and share things in order to make themselves look better. For example, someone may look up and share political articles and other intellectual articles. There are many sites being used for different purposes and that do not make up one person's [[Personal identity|identity]] at all, but provide false representations instead.<ref name=Pariser/>

==Online shopping==
{{main article|Online shopping}}
Search engines such as Google and Yahoo! utilize personalized search to attract possible customers to products that fit their presumed desires. Based on a large amount of collected data aggregated from an individual's web clicks, search engines can use personalized search to put advertisements that may pique the interest of an individual. Utilizing personalized search can help consumers find what they want faster, as well as help match up products and services to individuals within more specialized and/or niche markets. Many of these products or services that are sold via personalized online results would struggle to sell in [[brick-and-mortar]] stores. These types of products and services are called long tail items.<ref>{{cite journal|author=Badke, William|title=Personalization and Information Literacy|journal=Online|volume=36|issue=1|page=47|date=February 2012}}</ref> Using personalized search allows faster product and service discoveries for consumers, and reduces the amount of necessary advertisement money spent to reach those consumers. In addition, utilizing personalized search can help companies determine which individuals should be offered online coupon codes to their products and/or services. By tracking if an individual has perused their website, considered purchasing an item, or has previously made a purchase a company can post advertisements on other websites to reach that particular consumer in an attempt to have them make a purchase.

Aside from aiding consumers and businesses in finding one another, the search engines that provide personalized search benefit greatly. The more data collected on an individual, the more personalized results will be. In turn, this allows search engines to sell more advertisements because companies understand that they will have a better opportunity to sell to high percentage matched individuals then medium and low percentage matched individuals. This aspect of personalized search angers many scholars, such as William Badke and Eli Pariser, because they believe personalized search is driven by the desire to increase advertisement revenues. In addition, they believe that personalized search results are frequently utilized to sway individuals into using products and services that are offered by the particular search engine company or any other company in partnered with them. For example, Google searching any company with at least one brick-and-mortar location will offer a map portraying the closest company location using the Google Maps service as the first result to the query.<ref"Consumer Watchdog"/> In order to use other mapping services, such as MapQuest, a user would have to dig deeper into the results. Another example pertains to more vague queries. Searching the word "shoes" using the Google search engine will offer several advertisements to shoe companies that pay Google to link their website as a first result to consumer's queries.

==References==
{{reflist|30em}}

{{DEFAULTSORT:Personalized search}}
[[Category:Information retrieval techniques]]
[[Category:Internet search engines|*Personalized search]]
[[Category:Internet terminology]]
[[Category:Personalized search| ]]
<=====doc_Id=====>:678
<=====title=====>:
Adversarial information retrieval
<=====text=====>:
'''Adversarial information retrieval''' ('''adversarial IR''') is a topic in [[information retrieval]] related to strategies for working with a data source where some portion of it has been manipulated maliciously.  Tasks can include gathering, indexing, filtering, retrieving and ranking information from such a data source. Adversarial IR includes the study of methods to detect, isolate, and defeat such manipulation.

On the Web, the predominant form of such manipulation is [[spamdexing|search engine spamming]] (also known as spamdexing), which involves employing various techniques to disrupt the activity of [[web search engines]], usually for financial gain. Examples of spamdexing are [[Google bomb|link-bombing]], [[comment spam (disambiguation)|comment]] or [[referrer spam]], [[spam blog]]s (splogs), malicious tagging.  [[Reverse engineering]] of [[ranking function|ranking algorithms]], [[Ad filtering|advertisement blocking]], [[click fraud]],<ref>Jansen, B. J. (2007) [https://faculty.ist.psu.edu/jjansen/academic/jansen_click_fraud.pdf Click fraud]. IEEE Computer. 40(7), 85-86.</ref> and [[web content filtering]] may also be considered forms of adversarial [[data manipulation]].<ref>B. Davison, M. Najork, and T. Converse (2006), [http://wayback.archive.org/web/20090320173324/http://www.acm.org/sigs/sigir/forum/2006D/2006d_sigirforum_davison.pdf SIGIR Worksheet Report: Adversarial Information Retrieval on the Web (AIRWeb 2006)]</ref>

Activities intended to poison the supply of useful data make search engines less useful for users. If search engines are more exclusionary they risk becoming more like directories and less dynamic.

== Topics ==
Topics related to Web spam (spamdexing):

* [[Link spam]]
* [[Keyword spamming]]
* [[Cloaking]]
* Malicious tagging
* Spam related to blogs, including [[spam in blogs|comment spam]], [[spam blog|splogs]], and [[sping|ping spam]]

Other topics:
* [[Click fraud]] detection
* Reverse engineering of  [[search engine]]'s [[ranking]] algorithm
* Web [[content filtering]]
* [[Ad filtering|Advertisement blocking]]
* Stealth [[web crawling|crawling]]
*[[Troll (Internet)]]
* Malicious tagging or voting in [[social networks]]
* [[Astroturfing]]
* [[Sockpuppetry]]

== History ==
The term "adversarial information retrieval" was first coined in 2000 by [[Andrei Broder]] (then Chief Scientist at [[Alta Vista]]) during the Web plenary session at the [[Text Retrieval Conference|TREC]]-9 conference.<ref>D. Hawking and N. Craswell (2004), [http://es.csiro.au/pubs/trecbook_for_website.pdf Very Large Scale Retrieval and Web Search (Preprint version)]</ref>

== See also ==
*[[Spamdexing]]
*[[Information retrieval]]

== References ==
{{reflist}}

== External links ==
*[http://airweb.cse.lehigh.edu/ AIRWeb]: series of workshops on Adversarial Information Retrieval on the Web
*[http://webspam.lip6.fr/ Web Spam Challenge]: competition for researchers on Web Spam Detection
*[http://wayback.archive.org/web/20100217125910/http://barcelona.research.yahoo.net/webspam/ Web Spam Datasets]: datasets for research on Web Spam Detection

{{DEFAULTSORT:Adversarial Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Internet fraud]]
<=====doc_Id=====>:681
<=====title=====>:
Temporal information retrieval
<=====text=====>:
'''Temporal Information Retrieval (T-IR)''' is an emerging area of research related to the field of [[information retrieval]] (IR) and a considerable number of sub-areas, positioning itself, as an important dimension in the context of the user information needs.

According to [[information theory]] science (Metzger, 2007),<ref name="Metzger2007">{{cite journal |last=Metzger |first=Miriam |title=Making Sense of Credibility on the Web: Models for Evaluating Online Information and Recommendations for Future Research |journal=Journal of the American Society for Information Science and Technology |volume=58 |issue=13 |pages=2078–2091 |year =2007 |url=http://dl.acm.org/citation.cfm?id=1315940 |doi=10.1002/asi.20672 }}</ref> timeliness or currency is one of the key five aspects that determine a document’s credibility besides relevance, accuracy, objectivity and coverage. One can provide many examples when the returned search results are of little value due to temporal problems such as obsolete data on weather, outdated information about a given company’s earnings or information on already-happened or invalid predictions.

T-IR, in general, aims at satisfying these temporal needs and at combining traditional notions of document relevance with the so-called temporal relevance. This will enable the return of temporally relevant documents, thus providing a temporal overview of the results in the form of timeliness or similar structures. It also shows to be very useful for query understanding, query disambiguation, query classification, result diversification and so on.

This page contains a list of the most important research in temporal information retrieval (T-IR) and its related sub-areas. As several of the referred works are related with different research areas a single article can be found in more than one different table. For ease of reading the articles are categorized in a number of different sub-areas referring to its main scope, in detail.

== Temporal dynamics (T-dynamics) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza, Y.''' (2002). [http://www.dcs.bbk.ac.uk/webDyn2/proceedings/baeza_yates_web_strucutre.pdf/ Web Structure, Dynamics and Page Quality]. In A. Laendar & A. Oliveira (Eds.), ''In Lecture Notes in Computer Science - SPIRE2002: 9th International Symposium on String Processing and Information Retrieval'' (Vol. 2476/2002, pp.&nbsp;117 – 130). Lisbon, Portugal. September 11–13: Springer Berlin / Heidelberg. || 2002 || SPIRE || T-Dynamics ||
|-
|'''Cho, J., & Garcia-Molina, H.''' (2003). [http://dl.acm.org/citation.cfm?id=857170 Estimating Frequency of Change]. ''In [http://toit.acm.org TOIT: ACM Transactions on Internet Technology]'', 3(3), 256 - 290.|| 2003 || TOIT || T-Dynamics ||
|-
| '''Fetterly, D., Manasse, M., Najork, M., & Wiener, J.''' (2003). [http://dl.acm.org/citation.cfm?id=775246|A Large-Scale Study of the Evolution of Web Pages]]. ''In [http://www2003.org/ WWW2003]: Proceedings of the 12th International World Wide Web Conference'' (pp.&nbsp;669 – 678). Budapest, Hungary. May 20–24: ACM Press. || 2003 || WWW || T-Dynamics ||
|-
| '''Ntoulas, A., Cho, J., & Olston, C.''' (2004). [http://dl.acm.org/citation.cfm?id=988674 What's New on the Web?: the Evolution of the Web from a Search Engine Perspective]. In [http://www2004.org WWW2004]: Proceedings of the 13th International World Wide Web Conference (pp.&nbsp;1 – 12). New York, NY, United States. May 17–22: ACM Press. || 2004 || WWW || T-Dynamics ||
|-
| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Bordino, I., Boldi, P., Donato, D., Santini, M., & Vigna, S.''' (2008). [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4734022&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F4733896%2F4733897%2F04734022.pdf%3Farnumber%3D4734022 Temporal Evolution of the UK Web]. In [http://compbio.cs.uic.edu/adn-icdm08/ ADN2008]: Proceedings of the 1st International Workshop on Analysis of Dynamic Networks associated to [http://icdm08.isti.cnr.it/ ICDM2008]: IEEE International Conference on Data Mining (pp.&nbsp;909 – 918). Pisa, Italy. December 19: IEEE Computer Society Press. || 2008 || ICDM - ADN || T-Dynamics ||
|-
| '''Adar, E., Teevan, J., Dumais, S. T., & Elsas, J. L.''' (2009). [http://portal.acm.org/citation.cfm?id=1498837 The Web Changes Everything: Understanding the Dynamics of Web Content]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;282 – 291). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || T-Dynamics ||
|-
| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Aji, A., Agichtein, E.''' (2010). [http://dl.acm.org/citation.cfm?id=2175298.2175332 Deconstructing Interaction Dynamics in Knowledge Sharing Communities]. ''In [http://sbp.asu.edu/sbp2010/sbp10.html]: Third International Conference on Social Computing, Behavioral-Cultural Modeling, & Prediction'' (pp.&nbsp;273 – 281). Washington DC, United States. March 30–31: Springer-Verlag. || 2010 || SBP || T-Dynamics ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://ceur-ws.org/Vol-707/TWAW2011.pdf What is the Temporal Value of Web Snippets?] ''In [http://temporalweb.net/page3/page3.html TWAW 2011]: Proceedings of the 1st International Temporal Web Analytics Workshop associated to [http://www.www2011india.com/ WWW2011]: 20th International World Wide Web Conference''. Hyderabad, India. March 28.: CEUR Workshop Proceedings. || 2011 || WWW - TWAW || T-Dynamics ||
|-
| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2011 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. In [http://www.sigir2011.org/ SIGIR2011]: ''In Proceedings of the 34th international ACM SIGIR conference on Research and development in Information'' (pp.&nbsp;1171 – 1172). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || T-Dynamics ||
|-
| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October 24 - 28: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||
|-
| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal markup languages (T-MLanguages) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Setzer, A., & Gaizauskas, R.''' (2000). [ftp://ftp.dcs.shef.ac.uk/home/robertg/papers/lrec00-tempann.pdf Annotating Events and Temporal Information in Newswire Texts]. ''In [http://www.xanthi.ilsp.gr/lrec/ LREC2000]: Proceedings of the 2nd International Conference on Language Resources and Evaluation''. Athens, Greece. May 31 - June 2: ELDA. || 2000 || LREC || T-MLanguages ||
|-
| '''Setzer, A.''' (2001). [http://www.andrea-setzer.org.uk/PAPERS/thesis.pdf Temporal Information in Newswire Articles: An Annotation Scheme and Corpus Study]. Sheffield, UK: University of Sheffield. || 2001 || Phd Thesis || T-MLanguages ||
|-
| '''Ferro, L., Mani, I., Sundheim, B., & Wilson, G.''' (2001). [http://www.timeml.org/site/terqas/readings/MTRAnnotationGuide_v1_02.pdf TIDES Temporal Annotation Guidelines]. Version 1.0.2. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2001 || Technical Report || T-MLanguages ||
|-
| '''Pustejovsky, J., Castaño, J., Ingria, R., Sauri, R., Gaizauskas, R., Setzer, A., et al.''' (2003). TimeML: Robust Specification of Event and Temporal Expression in Text. ''In [http://iwcs.uvt.nl/iwcs5/index.htm IWCS2003]: Proceedings of the 5th International Workshop on Computational Semantics'', (pp.&nbsp;28 – 34). Tilburg, Netherlands. January 15–17. || 2003 || IWCS || T-MLanguages ||
|-
| '''Ferro, L., Gerber, L., Mani, I., Sundheim, B., & Wilson, G.''' (2005). [http://projects.ldc.upenn.edu/ace/docs/English-TIMEX2-Guidelines_v0.1.pdf TIDES 2005 Standard for the Annotation of Temporal Expressions]. Technical Report, MITRE Corporation, McLean, Virginia, United States. || 2005 || Technical Report || T-MLanguages ||
|}

== Temporal taggers (T-taggers) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [http://www.timeml.org/site/tarsqi/toolkit/manual/ TempEx Module] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] - '''Mani, I., & Wilson, G.''' (2000). [[dl.acm.org/citation.cfm?id=1075228|Robust Temporal Processing of News]]. ''In [http://www.cse.ust.hk/acl2000/ ACL2000]: Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;69 – 76). Hong Kong, China. October 1–8: Association for Computational Linguistics. || 2000 || ACL || T-Taggers ||
|-
| [http://www.aktors.org/technologies/annie/ Annie] - [http://gate.ac.uk/download/index.html GATE distribution] - '''Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V.''' (2002). [http://eprints.aktors.org/90/01/acl-main.pdf GATE: A Framework And Graphical Development Environment For Robust NLP Tools And Applications]. ''In [http://www.aclweb.org/mirror/acl2002/ ACL2002]: Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics'' (pp.&nbsp;168 – 175). Philadelphia, PA, United States. July 6–12: Association for Computational Linguistics. || 2002 || ACL || T-Taggers ||
|-
| [http://www.timeml.org/site/tarsqi/modules/gutime/download.html GUTime] - [http://www.timeml.org/site/tarsqi/toolkit/index.html Tarsqi Toolkit] || 2002 ||  || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., & Gertz, M.''' (2010). [http://delivery.acm.org/10.1145/1860000/1859735/p321-strotgen.pdf?ip=188.80.124.88&acc=OPEN&CFID=82473711&CFTOKEN=13661527&__acm__=1337002719_1b05141ffc83e798f400c972756d43ad HeidelTime: High Quality Rule-based Extraction and Normalization of Temporal Expressions]. ''In [http://semeval2.fbk.eu/semeval2.php SemEval2010]: Proceedings of the 5th International Workshop on Semantic Evaluation associated to [http://acl2010.org/ ACL2010]: 41st Annual Meeting of the Association for Computational Linguistics'', (pp.&nbsp;321 – 324). Uppsala, Sweden. July 11–16.|| 2010 || ACL - SemEval || T-Taggers ||
|-
| [http://www.timen.org/ TIMEN] '''Llorens, H., Derczynski, L., Gaizauskas, R. & Saquete, E.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/128_Paper.pdf TIMEN: An Open Temporal Expression Normalisation Resource]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| '''Chang, A., & Manning, C.''' (2012). [http://www.lrec-conf.org/proceedings/lrec2012/pdf/284_Paper.pdf SUTIME: A Library for Recognizing and Normalizing Time Expressions]. ''In [http://www.lrec-conf.org/lrec2012/ LREC2012]: Proceedings of the 8th International Conference on Language Resources and Evaluation''. Istanbul, Turkey. May 23-25. || 2012 || LREC || T-Taggers ||
|-
| [http://dbs.ifi.uni-heidelberg.de/index.php?id=form-downloads HeidelTime] - '''Strötgen, J., & Gertz, M.''' (2012). [http://www.springerlink.com/content/64767752451075k8/ Multilingual and cross-domain temporal tagging]. ''In [http://www.springerlink.com/content/1574-020x/ LRE]: Language Resources and Evaluation'', 1 - 30.|| 2012 || LRE || T-Taggers ||
|-
| [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ ManTIME] - '''Filannino, M., Brown, G. & Nenadic G.''' (2013). [http://www.aclweb.org/anthology/S/S13/S13-2.pdf#page=89 ManTIME: Temporal expression identification and normalization in the TempEval-3 challenge]. ''In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic Evaluation (SemEval 2013)'', 53 - 57, Atlanta, Georgia, June 14-15, 2013.|| 2013 || ACL - SemEval || T-Taggers || [http://www.cs.man.ac.uk/~filannim/projects/tempeval-3/ online demo]
|}

== Temporal indexing (T-indexing) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Berberich, K., Bedathur, S., Neumann, T., & Weikum, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1277831 A Time Machine for Text Search]. ''In [http://www.sigir.org/sigir2007 SIGIR 2007]: Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;519 – 526). Amsterdam, Netherlands. July 23–27: ACM Press. || 2007 || SIGIR || W-Archives ||
|-
| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2010). [http://dl.acm.org/citation.cfm?id=1871437.1871528 Efficient temporal keyword search over versioned text]. ''In [http://www.yorku.ca/cikm10/ CIKM2010]: Proceedings of the 19th ACM international conference on Information and knowledge management'', (pp.&nbsp;699-708). Toronto, Canada. October 26-30. ACM Press. || 2010 || CIKM || W-Archives||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2009991 Temporal index sharding for space-time efficiency in archive search]. ''In [http://www.sigir.org/sigir2011/ SIGIR2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;545-554). Beijing, China. July 24-28. ACM Press. || 2011 || SIGIR || T-Indexing||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235 – 243). Portland, United States. August 12-16. ACM Press. || 2012 || SIGIR || W-Archives ||
|}

== Temporal query understanding (TQ-understanding) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Vlachos, M., Meek, C., Vagena, Z., & Gunopulos, D.''' (2004). [http://portal.acm.org/citation.cfm?id=1007586 Identifying Similarities, Periodicities and Bursts for Online Search Queries]]. In [http://www09.sigmod.org/sigmod04/eproceedings/ SIGMOD2004]: Proceedings of the International Conference on Management of Data (pp.&nbsp;131 – 142). Paris, France. June 13–18: ACM Press. || 2004 || SIGMOD || T-Dynamics ||
|-
| '''Beitzel, S. M., Jensen, E. C., Chowdhury, A., Frieder, O., & Grossman, D.''' (2007). [http://dl.acm.org/citation.cfm?id=1190282 Temporal analysis of a very large topically categorized Web query log]]. ''In [http://www.asis.org/jasist.html JASIST]: Journal of the American Society for Information Science and Technology'', 58(2), 166 - 178. || 2007 || JASIST || T-Dynamics ||
|-
| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Diaz, F.''' (2009). [http://dl.acm.org/citation.cfm?id=1498825 Integration of News Content into Web Results]. ''In [http://wsdm2009.org/ WSDM2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;182 – 191). Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM || TQ-Understanding ||
|-
| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''König, A.''' (2009). [http://dl.acm.org/citation.cfm?id=1572002 Click-Through Prediction for News Queries]. ''In [http://www.sigir2009.org/ SIGIR2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;347 – 354). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Kanhabua, N., & Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Zhang, R., Konda, Y., Dong, A., Kolari, P., Chang, Y., & Zheng, Z.''' (2010). [http://dl.acm.org/citation.cfm?id=1870768 Learning Recurrent Event Queries for Web Search]. ''In [http://www.lsi.upc.edu/events/emnlp2010/ EMNLP2010]: Proceedings of the Conference on Empiral Methods in Natural Language Processing'' (pp.&nbsp;1129 – 1139). Massachusetts, United States. October 9–11: Association for Computational Linguistics. || 2010 || EMNLP || TQ-Understanding ||
|-
| '''Kulkarni, A., Teevan, J., Svore, K. M., & Dumais, S. T.''' (2011). [http://portal.acm.org/citation.cfm?id=1935862 Understanding Temporal Query Dynamics]]. ''In [http://www.wsdm2011.org/ WSDM2011]: In Proceedings of the 4th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;167 – 176). Hong Kong, China. February 9–12: ACM Press. || 2011 || WSDM || T-Dynamics ||
|-
| '''Campos, R.''' (2011). [http://dl.acm.org/citation.cfm?id=2010182 Using k-top Retrieved Web Snippets to Date Temporal Implicit Queries based on Web Content Analysis]. ''In [http://www.sigir2011.org/%20 SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (p.&nbsp;1325). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., & Dias, G.''' (2011). [http://ciir.cs.umass.edu/sigir2011/qru/campos+al.pdf Using Web Snippets and Query-logs to Measure Implicit Temporal Intents in Queries]. ''In [http://ciir.cs.umass.edu/sigir2011/qru/ QRU 2011]: Proceedings of the Query Representation and Understanding Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR 2005 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;13 – 16). Beijing, China. July 28. || 2011 || SIGIR - QRU || T-Dynamics ||
|-
| '''Shokouhi, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010104 Detecting Seasonal Queries by Time-Series Analysis]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;1171 – 1172). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2169103&CFID=102654836&CFTOKEN=48651941 Enriching Temporal Query Understanding through Date Identification: How to Tag Implicit Temporal Queries?] ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;41 – 48). Lyon, France. April 17.: ACM - DL. || 2012 || WWW - TWAW || TQ-Understanding ||
|-
| '''Shokouhi, M., & Radinsky, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2348364 Time-Sensitive Query Auto-Completion]. ''In [http://www.sigir.org/sigir2012/ SIGIR 2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;601 – 610). Portland, United States. August 12–16.: ACM Press. || 2012 || SIGIR || TQ-Understanding ||
|-
| '''Campos, R., Dias, G., Jorge, A., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2398567&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 GTE: A Distributional Second-Order Co-Occurrence Approach to Improve the Identification of Top Relevant Dates] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2035 – 2039). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || TQ-Understanding ||
|-
| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Time-aware retrieval/ranking models (T-RModels) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Li, X., & Croft, B. W.''' (2003). [http://dl.acm.org/citation.cfm?doid=956863.956951 Time-Based Language Models]. ''In CIKM 2003: Proceedings of the 12th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;469 – 475). New Orleans, Louisiana, United States. November 2–8: ACM Press. || 2003 || CIKM || T-RModels ||
|-
| '''Sato, N., Uehara, M., & Sakai, Y.''' (2003). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?reload=true&arnumber=1232026&contentType=Conference+Publications Temporal Information Retrieval in Cooperative Search Engine]. ''In [http://www.dexa.org/previous/dexa2003/cfp/dexa.html DEXA2003]: Proceedings of the 14th International Workshop on Database and Expert Systems Applications'' (pp.&nbsp;215 – 220). Prague, Czech Republic. September 1–5: IEEE. || 2003 || DEXA || T-RModels ||
|-
| '''Berberich, K., Vazirgiannis, M., & Weikum, G.''' (2005). [http://projecteuclid.org/DPubS?verb=Display&version=1.0&service=UI&handle=euclid.im/1150474885&page=record Time-Aware Authority Ranking]. ''In [http://www.tandf.co.uk/journals/journal.asp?issn=1542-7951&linktype=44 IM: Internet Mathematics]'', 2(3), 301 - 332. || 2005 || IM || T-RModels ||
|-
| '''Cho, J., Roy, S., & Adams, R.''' (2005). [http://dl.acm.org/citation.cfm?id=1066220 Page Quality: In Search of an Unbiased Web Ranking]. In [http://cimic.rutgers.edu/~sigmod05/ SIGMOD2005]: Proceedings of the International Conference on Management of Data (pp.&nbsp;551 – 562). Baltimore, United States. June 13–16: ACM Press. || 2005 || SIGMOD || T-RModels ||
|-
| '''Perkiö, J., Buntine, W., & Tirri, H.''' (2005). [http://dl.acm.org/citation.cfm?id=1076171 A Temporally Adaptative Content-Based Relevance Ranking Algorithm]. ''In [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;647 – 648). Salvador, Brazil. August 15–16: ACM Press. || 2005 || SIGIR || T-RModels ||
|-
| '''Jones, R., & Diaz, F.''' (2007). [http://dl.acm.org/citation.cfm?id=1247720 Temporal Profiles of Queries]. ''In [http://tois.acm.org/ TOIS: ACM Transactions on Information Systems]'', 25(3). Article No.: 14. || 2007 || TOIS || TQ-Understanding ||
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|-
| '''Dakka, W., Gravano, L., & Ipeirotis, P. G.''' (2008). [http://dl.acm.org/citation.cfm?id=1458320 Answering General Time Sensitive Queries]. ''In [http://www.cikm2008.org/ CIKM 2008]: Proceedings of the 17th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1437 – 1438). Napa Valley, California, United States. October 26–30: ACM Press. || 2008 || CIKM || TQ-Understanding ||
|-
| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Arikan, I., Bedathur, S., & Berberich, K.''' (2009). [http://www.wsdm2009.org/arikan_2009_temporal_expressions.pdf Time Will Tell: Leveraging Temporal Expressions in IR]. ''In [http://wsdm2009.org/ WSDM 2009]: Proceedings of the 2nd ACM International Conference on Web Search and Data Mining''. Barcelona, Spain. February 9–12: ACM Press. || 2009 || WSDM ||| T-RModels ||
|-
| '''Zhang, R., Chang, Y., Zheng, Z., Metzler, D., & Nie, J.-y.''' (2009). [http://dl.acm.org/citation.cfm?id=1620899 Search Result Re-ranking by Feedback Control Adjustment for Time-sensitive Query]. ''In [http://www.naaclhlt2009.org/ NAACL2009]: Proceedings of the North American Chapter of the Association for Computational Linguistics - Human Language Technologies'', (pp.&nbsp;165 – 168). Boulder, Colorado, United States. May 31 - June 5. || 2009 || NAACL || T-RModels ||
|-
| '''Metzler, D., Jones, R., Peng, F., & Zhang, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1572085 Improving Search Relevance for Implicitly Temporal Queries]. ''In [http://www.sigir2009.org/ SIGIR 2009]: Proceedings of the 32nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;700 – 701). Boston, MA, United States. July 19–23: ACM Press. || 2009 || SIGIR || TQ-Understanding ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Elsas, J. L., & Dumais, S. T.''' (2010). [http://dl.acm.org/citation.cfm?id=1718489 Leveraging Temporal Dynamics of Document Content in Relevance Ranking]. ''In [http://www.wsdm-conference.org/2010/ WSDM10]: Third ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;1 – 10). New York, United States. February 3–06: ACM Press. || 2010 || WSDM || T-Dynamics ||
|-
| '''Aji, A., Wang, Y., Agichtein, E., Gabrilovich, E.''' (2010). [http://dl.acm.org/citation.cfm?id=1871519 Using the Past to Score the Present: Extending Term Weighting Models Through Revision History Analysis] ''In [http://www.cikm2010.org/ CIKM 2010]: Proceedings of the 19th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;629 – 638). Toronto, ON, Canada. October 26 - October 30: ACM Press. || 2010 || CIKM || T-RModels ||
|-
| '''Dong, A., Chang, Y., Zheng, Z., Mishne, G., Bai, J., Zhang, R., et al.''' (2010). [http://dl.acm.org/citation.cfm?id=1718490 Towards Recency Ranking in Web Search]. In [http://www.wsdm-conference.org/2010/ WSDM2010]: ''In Proceedings of the 3rd ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;11 – 20). New York, United States. February 3–6: ACM Press. || 2010 || WSDM || T-RModels ||
|-
| '''Berberich, K., Bedathur, S., Alonso, O., & Weikum, G.''' (2010). [http://www.springerlink.com/content/b193008160713350/ A Language Modeling Approach for Temporal Information Needs]. In C. Gurrin, Y. He, G. Kazai, U. Kruschwitz, S. Little, T. Roelleke, et al. (Eds.), ''In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://kmi.open.ac.uk/events/ecir2010/ ECIR 2010]: 32nd European Conference on Information Retrieval'' (Vol. 5993/2010, pp.&nbsp;13 – 25). Milton Keynes, UK. March 28–31: Springer Berlin / Heidelberg. || 2010 || ECIR || T-RModels ||
|-
| '''Dong, A., Zhang, R., Kolari, P., Jing, B., Diaz, F., Chang, Y., Zheng, Z., & Zha, H.''' (2010). [http://dl.acm.org/citation.cfm?id=1772725&dl=ACM&coll=DL&CFID=204979644&CFTOKEN=99312511 Time is of the Essence: Improving Recency Ranking Using Twitter Data]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;331 – 340). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || T-RModels ||
|-
| '''Inagaki, Y., Sadagopan, N., Dupret, G., Dong, A., Liao, C., Chang, Y., & Zheng, Z.''' (2010). [http://labs.yahoo.com/files/aaai10_recencyfeature_2.pdf Session Based Click Features for Recency Ranking]. ''In [http://www.aaai.org/Conferences/AAAI/aaai10.php AAAI2010]: Proceedings of the 24th AAAI Conference on Artificial Intelligence'' (pp.&nbsp;331 – 340). Atlanta, United States. June 11–15: AAAI Press. || 2010 || AAAI || T-RModels ||
|-
| '''Dai, N., & Davison, B.''' (2010). [http://dl.acm.org/citation.cfm?id=1835471 Freshness Matters: In Flowers, Food, and Web Authority]. ''In [http://www.sigir2010.org/doku.php SIGIR 2010]: Proceedings of the 33rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;114 – 121). Geneve, Switzerland. July 19–23: ACM Press. || 2010 || SIGIR || T-RModels ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Kanhabua, N., & Nørvåg, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1887796 Determining Time of Queries for Re-Ranking Search Results]. ''In [http://www.ecdl2010.org/ ECDL2010]: Proceedings of The European Conference on Research and Advanced Technology for Digital Libraries''. Glasgow, Scotland. September 6–10: Springer Berlin / Heidelberg. || 2010 || ECDL || TQ-Understanding ||
|-
| '''Efron, M., & Golovchinsky, G.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009984 Estimation Methods for Ranking Recent Information]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;495 – 504). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Dai, N., Shokouhi, M., & Davison, B. D.''' (2011). [http://dl.acm.org/citation.cfm?id=2009916.2009933 Learning to Rank for Freshness and Relevance]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;95 – 104). Beijing, China. July 24–28.: ACM Press. || 2011 || SIGIR || T-RModels ||
|-
| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=102654836&CFTOKEN=48651941 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Chang, P-T., Huang, Y-C., Yang, C-L., Lin, S-D., & Cheng, P-J.''' (2012). [http://dl.acm.org/citation.cfm?id=2348489 Learning-Based Time-Sensitive Re-Ranking for Web Search]. ''In Proceedings of the [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1101 – 1102). Portland, United States. August 12 - 16. || 2012 || SIGIR || T-RModels ||
|-
| '''Efron, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/efrontemporalwsv02.pdf Query-Specific Recency Ranking: Survival Analysis for Improved Microblog Retrieval]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || T-RModels ||
|-
| '''Kanhabua, N., & Nørvåg, K.''' (2012). [http://dl.acm.org/citation.cfm?id=2398667 Learning to Rank Search Results for Time-Sensitive Queries] ''In [http://www.cikm2012.org/ CIKM 2012]: Proceedings of the 21st ACM Conference on Information and Knowledge Management'' (pp.&nbsp;2463 – 2466). Maui, Hawaii, United States. October 29 - November 02.: ACM Press. || 2012 || CIKM || T-RModels ||
|-
| '''Kim G., and Xing E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Costa, M., & Silva, M. J., & Couto, F. M.''' (2014). [http://dl.acm.org/citation.cfm?id=2609619 Learning Temporal-Dependent Ranking Models]. ''In Proceedings of the [http://sigir.org/sigir2014/ SIGIR2014]: 37th Annual ACM SIGIR Conference'' (pp.&nbsp;757--766). Gold Coast, Australia. July 6–11: ACM Press. || 2014 || SIGIR || T-RModels ||
|}

== Temporal clustering (T-clustering) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., & Jorge, A.''' (2009). [http://www.ccc.ipt.pt/~ricardo/ficheiros/KDIR2009.pdf Disambiguating Web Search Results By Topic and Temporal Clustering: A Proposal]. In [http://www.kdir.ic3k.org/ KDIR2009]: Proceedings of the International Conference on Knowledge Discovery and Information Retrieval, (pp.&nbsp;292 – 296). Funchal - Madeira, Portugal. October 6–8. || 2009 || KDIR || T-Clustering ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Campos, R., Jorge, A., Dias, G., & Nunes, C.''' (2012). [http://dl.acm.org/citation.cfm?id=2457524.2457656 Disambiguating Implicit Temporal Queries by Clustering Top Relevant Dates in Web Snippets] ''In [http://www.fst.umac.mo/wic2012/WI/ WIC 2012]: Proceedings of the 2012 IEEE/WIC/ACM International Joint Conferences on Web Intelligence and Intelligent Agent Technology,'' Vol. 1, (pp.&nbsp;1 – 8). Macau, China. December 04-07. || 2012 || WIC || T-Clustering ||
|}

== Temporal text classification (T-classification) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Jong, F., Rode, H., & Hiemstra, D.''' (2006). [http://doc.utwente.nl/66448/ Temporal Language Models for the Disclosure of Historical Text]. ''In [http://www.dans.knaw.nl/en AHC2005]: Proceedings of the XVIth International Conference of the Association for History and Computing'' (pp.&nbsp;161 – 168). Amsterdam, Netherlands. September 14–17 || 2005 || AHC || T-Classification ||
|-
| '''Toyoda, M., & Kitsuregawa, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1135777.1135815 What's Really New on the Web? Identifying New Pages from a Series of Unstable Web Snapshots]. ''In [http://www2006.org WWW2006]: Proceedings of the 15th International World Wide Web Conference'' (pp.&nbsp;233 – 241). Edinburgh, Scotland. May 23–26: ACM Press. || 2006 || WWW || T-Classification ||
|-
| '''Nunes, S., Ribeiro, C., & David, G.''' (2007). [http://dl.acm.org/citation.cfm?id=1316924 Using Neighbors to Date Web Documents]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 9th ACM International Workshop on Web Information and Data Management associated to [[www2.fc.ul.pt/cikm2007|CIKM2007]]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;129 – 136). Lisboa, Portugal. November 9: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2007). [http://dl.acm.org/citation.cfm?id=1316925 Detecting Age of Page Content]. ''In [http://workshops.inf.ed.ac.uk/WIDM2007/ WIDM2007]: Proceedings of the 8th International Workshop on Web Information and Data Management associated to [http://www2.fc.ul.pt/cikm2007 CIKM2007]: 16th International Conference on Knowledge and Information Management'' (pp.&nbsp;137 – 144). Lisbon. Portugal. November 9.: ACM Press. || 2007 || CIKM - WIDM || T-Classification ||
|-
| '''Kanhabua, N., & Nørvåg, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1429902 Improving Temporal Language Models for Determining Time of Non-timestamped Documents]. ''In Christensen-Dalsgaard, B., Castelli, D., Jurik, B. A., Lippincott, J. (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2008.org/ ECDL 2008]: 12th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 5173/2008, pp.&nbsp;358 – 370). Aarhus, Denmark. September 14–19: Springer Berlin / Heidelberg. || 2008 || ECDL || T-Classification ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Strötgen, J., Alonso, O., & Gertz, M.''' (2012). [http://dl.acm.org/citation.cfm?id=2169095.2169102&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Identification of Top Relevant Temporal Expressions in Documents]. ''In [http://www.temporalweb.net/ TWAW 2012]: Proceedings of the 2nd International Temporal Web Analytics Workshop associated to [http://www2012.wwwconference.org/ WWW2012]: 20th International World Wide Web Conference'' (pp.&nbsp;33 – 40). Lyon, France. April 17: ACM - DL. || 2012 || WWW - TWAW || T-Classification ||
|-
| '''Filannino, M., and Nenadic, G.''' (2014). [http://www.aclweb.org/anthology/W/W14/W14-4502.pdf Mining temporal footprints from Wikipedia]. ''In Proceedings of the First AHA!-Workshop on Information Discovery in Text'' (Dublin, Ireland, August 2014), Association for Computational Linguistics and Dublin City University, pp. 7–13. || 2014 || COLING || T-Classification || [http://www.cs.man.ac.uk/~filannim/projects/temporal_footprints/ online demo]
|}

== Temporal visualization (T-interfaces) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| [https://books.google.com/ngrams Google Ngram Viewer] ||  ||  || T-Interfaces ||
|-
| '''Cousins, S., & Kahn, M.''' (1991). [http://www.sciencedirect.com/science/article/pii/093336579190005V The Visual Display of Temporal Information]. (E. Keravnou, Ed.) ''In AIM: Artificial Intelligence in Medicine'', 3(6), 341 - 357. || 1991 || AIM || T-Interfaces ||
|-
| '''Karam, G. M.''' (1994). [http://dl.acm.org/citation.cfm?id=187157 Visualization Using Timelines]. In T. J. Ostrand (Ed.), ''ISSTA1994: Proceedings of the International Symposium on Software Testing and Analysis associated to SIGSOFT: ACM Special Interest Group on Software Engineering'' (pp.&nbsp;125 – 137). Seattle, Washington, United States. August 17–19: ACM Press. || 1994 || ISSTA || T-Interfaces ||
|-
| '''Plaisant, C., Miiash, B., Rose, A., Widoff, S., & Shneiderman, B.''' (1996). [http://dl.acm.org/citation.cfm?id=238493 LifeLines: Visualizing Personal Histories]. ''In [http://www.sigchi.org/chi96/proceedings/index.htm CHI1996]: Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'' (pp.&nbsp;221 – 227). Vancouver, British Columbia, Canada. April 13–18: ACM Press. || 1996 || CHI || T-Interfaces ||
|-
| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Catizone, R., Dalli, A., & Wilks, Y.''' (2006). [http://www.lrec-conf.org/proceedings/lrec2006/pdf/702_pdf.pdf Evaluating Automatically Generated Timelines from the Web]. ''In [http://www.lrec-conf.org/lrec2006/ LREC2006]: Proceedings of the 5th International Conference on Language Resources and Evaluation''. Genoa, Italy. May 24–26: ELDA. || 2006 || LREC || T-Interfaces ||
|-
| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jatowt, A., Kawai, Y., & Tanaka, K.''' (2008). [http://dl.acm.org/citation.cfm?id=1367497.1367736 Visualizing Historical Content of Web pages]]. ''In [http://www2008.org/ WWW2008]: Proceedings of the 17th International World Wide Web Conference'' (pp.&nbsp;1221 – 1222). Beijing, China. April 21–25: ACM Press. || 2008 || WWW || W-Archives ||
|-
| '''Nunes, S., Ribeiro, C., & David, G.''' (2008). [http://dl.acm.org/citation.cfm?id=1822292 WikiChanges - Exposing Wikipedia Revision Activity]. ''In [http://www.wikisym.org/ws2008/ WikiSym2008]: Proceedings of the 4th International Symposium on Wikis''. Porto, Portugal. September 8–10: ACM Press. || 2008 || WikiSym || T-Interfaces ||
|-
| '''Nunes, S., Ribeiro, C., & David, G.''' (2009). [http://epia2009.web.ua.pt/onlineEdition/601.pdf Improving Web User Experience with Document Activity Sparklines]. ''In L. S. Lopes, N. Lau, P. Mariano, & L. Rocha (Ed.), [http://epia2009.web.ua.pt EPIA2009]: Proceedings of the 14th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'', (pp.&nbsp;601 – 604). Aveiro, Portugal. October 12–15. || 2009 || EPIA || T-Interfaces ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Khurana, U., Nguyen V., Cheng H., Ahn, J., Chen X., & Shneiderman, B.''' (2011). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&arnumber=6113166 Visual Analysis of Temporal Trends in Social Networks Using Edge Color Coding and Metric Timelines]. ''In [http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6112285]: Proceedings of the IEEE Social Computing'', (pp.&nbsp;549 – 554). Boston, United States. || 2011 || SocialCom || T-Interfaces ||
|}

== Temporal search engines (T-SEngine) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., & Gertz, M.''' (2006). [http://dl.acm.org/citation.cfm?id=1148170.1148273&coll=DL&dl=GUIDE&CFID=102654836&CFTOKEN=48651941 Clustering of Search Results using Temporal Attributes]. ''In [http://www.sigir.org/sigir2006/ SIGIR 2006]: Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;597 – 598). Seattle, Washington, United States. August 6–11: ACM Press. || 2006 || SIGIR || T-Clustering ||
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2007). Exploratory Search Using Timelines. ''In ESCHI: Proceedings of the Workshop on Exploratory Search and Computer Human Interaction associated to [http://www.chi2007.org/ CHI2007]: [http://research.microsoft.com/en-us/um/people/ryenw/esi/acceptedposters.html SIGCHI] Conference on Human Factors in Computing Systems''. San Jose, CA, United States. April 29: ACM Press. || 2007 || CHI - ESCHI || T-SEngine ||
|-
| '''Jin, P., Lian, J., Zhao, X., & Wan, S.''' (2008). [http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4739991 TISE: A Temporal Search Engine for Web Contents]. ''In IITA2008: Proceedings of the 2nd International Symposium on Intelligent Information Technology Application'' (pp.&nbsp;220 – 224). Shanghai, China. December 21–22: IEEE Computer Society Press. || 2008 || IITA || T-SEngine ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2009). [http://dl.acm.org/citation.cfm?id=1645953.1645968 Clustering and Exploring Search Results using Timeline Constructions]. ''In [http://www.comp.polyu.edu.hk/conference/cikm2009/ CIKM 2009]: Proceedings of the 18th International ACM Conference on Information and Knowledge Management''. Hong Kong, China. November 2–6: ACM Press. || 2009 || CIKM || T-Clustering ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|}

== Temporal question answering (T-QAnswering) ==
{| class="wikitable sortable"
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Pasca, M.''' (2008). [http://dl.acm.org/citation.cfm?id=1363946 Towards Temporal Web Search]. ''In [http://www.acm.org/conferences/sac/sac2008/ SAC2008]: Proceedings of the 23rd ACM Symposium on Applied Computing'' (pp.&nbsp;1117 – 1121). Fortaleza, Ceara, Brazil. March 16–20: ACM Press. || 2008 || SAC || T-QAnswering ||
|}

== Temporal snippets (T-snippets) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Alonso, O., Baeza-Yates, R., & Gertz, M.''' (2009). [http://www.wssp.info/2009/WSSP2009AlonsoBaezaYatesGertz.pdf Effectiveness of Temporal Snippets]. ''In [http://www.wssp.info/2009.html WSSP2009]: Proceedings of the Workshop on Web Search Result Summarization and Presentation associated to [[www2009.org/|WWW2009]]: 18th International World Wide Web Conference''. Madrid, Spain. April 20–24: ACM Press. || 2009 || WWW - WSSP || T-Snippets ||
|-
| '''Alonso, O., Gertz, M., & Baeza-Yates, R.''' (2011). [http://www.springerlink.com/content/u78qu8x10h613471/ Enhancing Document Snippets Using Temporal Information]. ''In R. Grossi, F. Sebastiani, & F. Silvestri (Eds.), Lecture Notes in Computer Science, [http://spire2011.isti.cnr.it/ SPIRE2011]: 18th International Symposium on String Processing and Information Retrieval'' (Vol. 7024, pp.&nbsp;26 – 31). Pisa, Italy. October 17–21.: Springer Berlin / Heidelberg. || 2011 || SPIRE || T-Snippets ||
|-
| '''Svore, K. M., Teevan, J., Dumais, S. T., & Kulkarni, A.''' (2012). [http://dl.acm.org/citation.cfm?id=2348461 Creating Temporally Dynamic Web Search Snippets]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;1045 – 1046). Portland, United States. August 12-16. ACM Press  || 2012 || SIGIR || T-Snippets ||
|}

== Future information retrieval (F-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Baeza-Yates, R.''' (2005). [http://www.dcs.vein.hu/CIR/cikkek/searching_the_future.pdf Searching the Future]. ''In S. Dominich, I. Ounis, & J.-Y. Nie (Ed.), MFIR2005: Proceedings of the Mathematical/Formal Methods in Information Retrieval Workshop associated to [http://www.dcc.ufmg.br/eventos/sigir2005/ SIGIR 2005]: 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Salvador, Brazil. August 15–19: ACM Press. || 2005 || SIGIR - MFIR || F-IRetrieval ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2009). [http://dl.acm.org/citation.cfm?id=1555420 Supporting Analysis of Future-Related Information in News Archives and the Web]. ''In [http://www.jcdl2009.org JCDL2009]: Proceedings of the Joint Conference on Digital Libraries'' (pp.&nbsp;115 – 124). Austin, United States. June 15–19.: ACM Press. || 2009 || JCDL || F-IRetrieval ||
|-
| '''Kawai, H., Jatowt, A., Tanaka, K., Kunieda, K., & Yamada, K.''' (2010). [http://dl.acm.org/citation.cfm?id=2108647 ChronoSeeker: Search Engine for Future and Past Events]. ''In ICUIMC 2010: Proceedings of the 4th International Conference on Uniquitous Information Management and Communication'' (pp.&nbsp;166 – 175). Suwon, Republic of Korea. January 14–15: ACM Press. || 2010 || ICIUMC || T-SEngine ||
|-
| '''Jatowt, A., Kawai, H., Kanazawa, K., Tanaka, K., & Kunieda, K.''' (2010). [http://dl.acm.org/citation.cfm?id=1772835 Analyzing Collective View of Future, Time-referenced Events on the Web]. ''In [http://www2010.org/www/index.html WWW2010]: Proceedings of the 19th International World Wide Web Conference'' (pp.&nbsp;1123 – 1124). Raleigh, United States. April 26–30: ACM Press. || 2010 || WWW || F-IRetrieval ||
|-
| '''Matthews, M., Tolchinsky, P., Blanco, R., Atserias, J., Mika, P., & Zaragoza, H.''' (2010). [http://research.yahoo.com/pub/3341 Searching through time in the New York Times]. ''In [http://www.iiix2010.org/hcir-workshop/ HCIR2010]: Proceedings of the 4th Workshop on Human-Computer Interaction and Information Retrieval'', (pp.&nbsp;41 – 44). New Brunswick, United States. August 22. || 2010 || HCIR || T-SEngine ||
|-
| '''Dias, G., Campos, R., & Jorge, A.''' (2011). [http://select.cs.cmu.edu/meetings/enir2011/papers/dias-campos-jorge.pdf Future Retrieval: What Does the Future Talk About?] ''In [http://select.cs.cmu.edu/meetings/enir2011/ ENIR 2011]: Proceedings of the Enriching Information Retrieval Workshop associated to [http://www.sigir2011.org/ SIGIR2011]: 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval''. Beijing, China. July 28. || 2011 || SIGIR - ENIR || F-IRetrieval ||
|-
| '''Kanhabua, N., Blanco, R., & Matthews, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2010018&dl=ACM&coll=DL&CFID=82290723&CFTOKEN=53881602 Ranking Related News Predictions]. ''In [http://www.sigir2011.org/ SIGIR 2011]: Proceedings of the 34th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;755 – 764). Beijing, China. July 24–28: ACM Press. || 2011 || SIGIR || F-IRetrieval ||
|-
| '''Kanazawa, K., Jatowt, A., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=2052362 Improving Retrieval of Future-Related Information in Text Collections]. ''In [http://liris.cnrs.fr/~wi-iat11/WI 2011/ WIC2011]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;278 – 283). Lyon, France. August 22–27: IEEE Computer Society Press. || 2011 || WIC || F-IRetrieval ||
|-
| '''Campos, R., Dias, G., & Jorge, A. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2051169 An Exploratory Study on the impact of Temporal Features on the Classification and Clustering of Future-Related Web Documents]. ''In L. Antunes, & H. S. Pinto (Eds.), Lecture Notes in Artificial Intelligence - Progress in Artificial Intelligence - [http://epia2011.appia.pt/ EPIA2011]: 15th Portuguese Conference on Artificial Intelligence associated to APPIA: Portuguese Association for Artificial Intelligence'' (Vol. 7026/2011, pp.&nbsp;581 – 596). Lisboa, Portugal. October 10–13: Springer Berlin / Heidelberg. || 2011 || EPIA || F-IRetrieval ||
|-
| '''Jatowt, A., & Yeung, C. M.''' (2011). [http://dl.acm.org/citation.cfm?id=2063759 Extracting Collective Expectations about the Future from Large Text Collections]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1259 – 1264). Glasgow, Scotland, UK. October: ACM Press. || 2011 || CIKM || F-IRetrieval ||
|-
| '''Weerkamp, W., & Rijke, M.''' (2012). [http://research.microsoft.com/en-us/people/milads/taia2012-activities.pdf Activity Prediction: A Twitter-based Exploration]. ''In [http://research.microsoft.com/en-us/people/milads/taia2012.aspx TAIA 2012]: Proceedings of the Time-Aware Information Access Workshop associated to [http://www.sigir.org/sigir2012/ SIGIR2012]: 35th Annual International ACM SIGIR 2012 Conference on Research and Development in Information Retrieval''. Portland, United States. August 16. || 2012 || SIGIR - TAIA || F-IRetrieval ||
|-
| '''Radinski, K., & Horvitz, E.''' (2013). [http://dl.acm.org/citation.cfm?id=2433431 Mining the Web to Predict Future Events]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;255 – 264). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || F-IRetrieval ||
|}

== Temporal image retrieval (T-IRetrieval) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Dias, G., Moreno, J. G., Jatowt, A., & Campos, R.''' (2012). [http://link.springer.com/content/pdf/10.1007%2F978-3-642-34109-0_21 Temporal Web Image Retrieval]. In Calderón-Benavides, L., González-Caro, C., Chávez, E., Ziviani, N. (Eds.), ''In Lecture Notes in Computer Science - [http://catic.unab.edu.co/spire/ SPIRE2012]: 19th International Symposium on String Processing and Information Retrieval'' (Vol. 7608/2012, pp.&nbsp;199 – 204). Cartagena de Indias, Colombia. October 21–25: Springer Berlin / Heidelberg. || 2012 || SPIRE || T-IRetrieval ||
|-
| '''Palermo, F., Hays, J., & Efros, A.''' (2012). [http://link.springer.com/content/pdf/10.1007/978-3-642-33783-3_36 Dating Historical Color Images]. In Fitzgibbon, A., Lazebnik, S., Sato, Y., Schmid, C. (Eds.), ''In Lecture Notes in Computer Science - [http://eccv2012.unifi.it/ ECCV2012]: 12th European Conference on Computer Vision'' (Vol. 7577/2012, pp.&nbsp;499 – 512). Firenze, Italy. October 07–13: Springer Berlin / Heidelberg. || 2012 || ECCV || T-IRetrieval ||
|-
| '''Kim, G., & Xing, E. P.''' (2013). [http://dl.acm.org/citation.cfm?id=2433417 Time-Sensitive Web Image Ranking and Retrieval via Dynamic Multi-Task Regression]. ''In [http://wsdm2013.org/ WSDM2013]: Proceedings of the 6th ACM International Conference on Web Search and Data Mining'' (pp.&nbsp;163 – 172). Rome, Italy. February 4–8: ACM Press. || 2013 || WSDM || T-IRetrieval ||
|-
| '''Martin, P., Doucet, A., & Jurie, F.''' (2014). [http://dl.acm.org/citation.cfm?id=2578790 Dating Color Images with Ordinal Classification]. ''In [http://www.icmr2014.org/ ICMR2014]: Proceedings of International Conference on Multimedia Retrieval'' (pp. 447). Glasgow, United Kingdom. April 01-04: ACM Press. || 2014 || ICMR || T-IRetrieval ||
|}

== Collective memory (C-memory) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Surowiecki, J.''' (2004). [http://www.amazon.com/The-Wisdom-Crowds-Collective-Economies/dp/0385503865 The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations]. USA: DoubleDay. || 2004 ||  || C-Memory ||
|-
| '''Hall, D., Jurafsky, D., & Manning, C. D.''' (2008). [http://dl.acm.org/citation.cfm?id=1613715.1613763 Studying the History of Ideas using Topic Models]. ''In [http://conferences.inf.ed.ac.uk/emnlp08 EMNLP 2008]: Proceedings of the Conference on Empirical Methods in Natural Language Processing'' (pp.&nbsp;363 – 371). Waikiki, Honolulu, Hawaii. October 25–27: Association for Computational Linguistics. || 2008 || EMNLP || C-Memory ||
|-
| '''Shahaf, D., & Guestrin, C.''' (2010). [http://dl.acm.org/citation.cfm?id=1835884 Connecting the dots between News Articles]]. In [http://www.sigkdd.org/kdd2010/ KDD2010]: Proceedings of the 16th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp.&nbsp;623 – 632). Washington, United States. July 25–28: ACM Press. || 2010 || KDD || C-Memory ||
|-
| '''Takahashi, Y., Ohshima, H., Yamamoto, M., Iwasaki, H., Oyama, S., & Tanaka, K.''' (2011). [http://dl.acm.org/citation.cfm?id=1995980 Evaluating Significance of Historical Entities based on Tempo-spatial Impacts Analysis using Wikipedia Link Structure]]. ''In [http://www.ht2011.org/ HT2011]: Proceedings of the 22nd ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;83 – 92). Eindhoven, Netherlands. June 6–9: ACM Press. || 2011 || HT || C-Memory ||
|-
| '''Michel, J.-B., Shen, Y. K., Aiden, A. P., Veres, A., Gray, M. K., Team, T. G., et al.''' (2011). [http://www.sciencemag.org/content/331/6014/176 Quantitative Analysis of Culture Using Millions of Digitized Books]. In [http://www.sciencemag.org/ Science], 331(6014), 176 - 182. || 2011 || Science || C-Memory ||
|-
| '''Yeung, C.-m. A., & Jatowt, A.''' (2011). [http://dl.acm.org/citation.cfm?id=2063755 Studying How the Past is Remembered: Towards Computational History through Large Scale Text Mining]]. ''In Proceedings of the [http://www.cikm2011.org/ CIKM2011]: 20th ACM Conference on Information and Knowledge Management'' (pp.&nbsp;1231 – 1240). Glasgow, Scotland, UK. October 24–28: ACM Press. || 2011 || CIKM || C-Memory ||
|}

== Web archives (W-archives) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| [[List of Web archiving initiatives|List of Web Archive Initiatives]] || 2011 ||  || W-Archives ||
|-
| '''Kahle, B.''' (1997, 03). [http://www.sciamdigital.com/index.cfm?fa=Products.ViewIssuePreview&ISSUEID_CHAR=00B8E369-1805-4A27-A331-9D727FEAC21&ARTICLEID_CHAR=00B10B9E-5F13-40B2-AA51-0A4D5C41549 Preserving the Internet]. ''In [https://www.scientificamerican.com/sciammag/ Scientific American Magazine]'', 276(3), pp.&nbsp;72 – 73. || 1997 || SAM || W-Archives ||
|-
| '''Toyoda, M., & Kitsuregawa, M.''' (2005). [http://dl.acm.org/citation.cfm?id=1083387 A System for Visualizing and Analyzing the Evolution of the Web with a Time Series of Graphs]. ''In [http://www.ht05.org HT2005]: Proceedings of the 16th ACM Conference on Hypertext and Hypermedia'' (pp.&nbsp;151 – 160). Salzburg, Austria. September 6–9: ACM Press. || 2005 || HT || W-Archives ||
|-
| '''Efendioglu, D., Faschetti, C., & Parr, T.''' (2006). [http://dl.acm.org/authorize?815487 Chronica: a temporal web search engine]]. In ''D. Wolber, N. Calder, & ,. C. Brooks (Ed.), [http://www.icwe2006.org/ ICWE2006]: Proceedings of the 6th International Conference on Web Engineering'' (pp.&nbsp;119 – 120). Palo Alto, California, United States. July 11–14: ACM Press. || 2006 || ICWE || W-Archives ||
|-
| '''Jatowt, A., Kawai, Y., Nakamura, S., Kidawara, Y., & Tanaka, K.''' (2006). [http://dl.acm.org/citation.cfm?id=1149969 Journey to the Past: Proposal of a Framework for Past Web Browser]. ''In HT2006: Proceedings of the 17th Conference on Hypertext and Hypermedia'' (pp.&nbsp;135 – 144). Odense, Denmark. August 22–25: ACM Press. || 2006 || HT || W-Archives ||
|-
| '''Adar, E., Dontcheva, M., Fogarty, J., & Weld, D. S.''' (2008). [http://dl.acm.org/citation.cfm?id=1449756 Zoetrope: Interacting with the Ephemeral Web]]. ''In S. B. Cousins, & M. Beaudouin-Lafon (Ed.), [http://www.acm.org/uist/uist2008/ UIST 2008]: Proceedings of the 21st Annual ACM Symposium on User Interface Software and Technology'' (pp.&nbsp;239 – 248). Monterey, CA, United States. October 19–22: ACM Press. || 2008 || UIST || W-Archives ||
|-
| '''Song, S., & JaJa, J.''' (2008). [http://www.umiacs.umd.edu/~joseph/temporal-web-archiving-final-umiacs-tr-2008-08.pdf Archiving Temporal Web Information: Organization of Web Contents for Fast Access and Compact Storage]. Technical Report UMIACS-TR-2008-08, University of Maryland Institute for Advanced Computer Studies, Maryland, MD, United States. || 2008 || Technical Report || W-Archives ||
|-
| '''Gomes, D., Miranda, J., & Costa, M.''' (2011). [http://dl.acm.org/citation.cfm?id=2042590 A Survey on Web Archiving Initiatives]]. ''In [http://www.tpdl2011.org/ TPDL2011]: Proceedings of the 15th international conference on Theory and practice of digital libraries: research and advanced technology for digital libraries'' (pp.&nbsp;408 – 420). Berlin, Germany. September 25–29: Springer-Verlag || 2011 || TPDL || W-Archives ||
|-
| '''Anand, A., Bedathur, S., Berberich, K., & Schenkel, R.''' (2012). [http://dl.acm.org/citation.cfm?id=2348318 Index Maintenance for Time-Travel Text Search]. ''In [http://www.sigir.org/sigir2012/ SIGIR2012]: Proceedings of the 35th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'', (pp.&nbsp;235 – 243). Portland, United States. August 12-16. ACM Press || 2012 || SIGIR || W-Archives ||
||
|-
| '''Costa, M., & Silva, M.J.''' (2012). [http://link.springer.com/chapter/10.1007%2F978-3-642-35063-4_32 Evaluating Web Archive Search Systems]. ''In [http://www.wise2012.cs.ucy.ac.cy/ WISE2012]: Proceedings of the 13th International Conference on Web Information System Engineering'', (pp.&nbsp;440 - 454). Paphos, Cyprus. November 28-30. Springer-Verlag || 2012 || WISE || W-Archives ||
|}

== Topic detection and tracking (TDT) ==
{| class="wikitable sortable"
|-
! Reference !! Year !! Conference/Journal !! Main Scope !! Comments
|-
| '''Allan, J., Carbonell, J., Doddington, G., & Yamron, J.''' (1998). [http://www.cs.pitt.edu/~chang/265/proj10/sisref/1.pdf Topic Detection and Tracking Pilot Study Final Report]. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop, (pp.&nbsp;194 – 218). Lansdowne, Virginia, United States. February. || 1998 || Technical Report || TDT ||
|-
| '''Swan, R., & Allan, J.''' (1999). [http://dl.acm.org/citation.cfm?id=319956 Extracting Significant Time-Varying Features from Text]]. ''In [http://cikmconference.org/1999/ CIKM 1999]]: Proceedings of the 8th International ACM Conference on Information and Knowledge Management'' (pp.&nbsp;38 – 45). Kansas City, Missouri, United States. November 2–6: ACM Press. || 1999 || CIKM || TDT ||
|-
| '''Swan, R., & Jensen, D.''' (2000). [http://www.cs.cmu.edu/~dunja/.../Swan_TM.pdf TimeMines: Constructing Timelines with Statistical Models of Word Usage]. ''In M. Grobelnik, D. Mladenic, & N. Milic-Frayling (Ed.), [http://www.cs.cmu.edu/~dunja/WshKDD2000.html TM2000]: Proceedings of the Workshop on Text Mining associated to [http://www.sigkdd.org/kdd2000/ KDD2000]: 6th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining'' (pp.&nbsp;73 – 80). Boston, Massachusetts, United States. August 20–23: ACM Press. || 2000 || KDD - TM || TDT ||
|-
| '''Swan, R., & Allan, J.''' (2000). [http://dl.acm.org/citation.cfm?id=345546 Automatic Generation of Overview Timelines]. ''In [http://www.aueb.gr/conferences/sigir2000/ SIGIR 2000]: Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval'' (pp.&nbsp;49 – 56). Athens, Greece. July 24–28: ACM Press. || 2000 || SIGIR || TDT ||
|-
| '''Makkonen, J., & Ahonen-Myka, H.''' (2003). [http://www.springerlink.com/content/a5ev5br7wwh5lvyl/ Utilizing Temporal Information in Topic Detection and Tracking]. ''In T. Koch, & I. T. Solvberg (Eds.), In Lecture Notes in Computer Science - Research and Advanced Technology for Digital Libraries, [http://www.ecdl2003.org/ ECDL 2003]: 7th European Conference on Research and Advances Technology for Digital Libraries'' (Vol. 2769/2004, pp.&nbsp;393 – 404). Trondheim, Norway. August 17–22: Springer Berlin / Heidelberg. || 2003 || ECDL || TDT ||
|-
| '''Shaparenko, B., Caruana, R., Gehrke, J., & Joachims, T.''' (2005). [http://www.cs.cornell.edu/people/tj/publications/shaparenko_etal_05a.pdf Identifying Temporal Paterns and Key Players in Document Collections]. ''In [http://users.cis.fiu.edu/~taoli/workshop/TDM2005/index.html TDM2005]: Proceedings of the Workshop on Temporal Data Mining associated to [http://www.cacs.louisiana.edu/~icdm05/ ICDM2005]'' (pp.&nbsp;165 – 174). Houston, United States. November 27–30: IEEE Press. || 2005 || ICDM - TDM || TDT ||
|-
| '''Mori, M., Miura, T., & Shioya, I.''' (2006). [http://dl.acm.org/citation.cfm?id=1249137 Topic Detection and Tracking for News Web Pages]. ''In [http://www.comp.hkbu.edu.hk/~wii06/wi/ WIC2006]: IEEE Main Conference Proceedings of the IEEE/WIC/ACM International Conference on Web Intelligence'' (pp.&nbsp;338 – 342). Hong Kong, China. December 18–22: IEEE Computer Society Press. || 2006 || WIC || TDT ||
|-
| '''Kim, P., Myaeng, S.H.''' (2004). [http://dl.acm.org/citation.cfm?id=1039624 Usefulness of Temporal Information Automatically Extracted from News Articles for Topic Tracking]. ''In [http://talip.acm.org/index.htm TALIP]:Journal of ACM Transactions on Asian Language Information Processing'' (pp.&nbsp;227 – 242). New York, United States. || 2004 || TALIP || TDT ||
|}

==References==
{{reflist}}

[[Category:Information retrieval genres]]
<=====doc_Id=====>:684
<=====title=====>:
Visual search engine
<=====text=====>:
{{multiple issues|
{{original research|date=February 2012}}
{{refimprove|date=February 2012}}
}}

A '''Visual Search Engine''' is a [[search engine (computing)|search engine]] designed to search for information on the [[World Wide Web]] through the input of an image or a search engine with a visual display of the search results. Information may consist of [[web page]]s, locations, other images and other types of documents. This type of search engines is mostly used to search on the mobile Internet through an image of an unknown object (unknown search query). Examples are buildings in a foreign city. These search engines often use techniques for [[CBIR|Content Based Image Retrieval]].

A visual search engine searches images, patterns based on an algorithm which it could recognize and gives relative information based on the selective or apply pattern match technique.

== Classification ==
Depending on the nature of the search engine there are two main groups, those which aim to find visual information  and those with a visual display of results.

== Visual information searchers ==
[[File:Imatge cercadors 1.jpg|thumb|Screenshot of results shown by the image searcher through example GOS]]

=== Image search ===
An image search is a search engine that is designed to find an image. The search can be based on keywords, a picture, or a web [[Hyperlink|link]] to a picture. The results depend on the search criterion, such as [[metadata]], distribution of color, shape, etc., and the search technique which the browser uses.
[[File:Imatge wiki 2.png|thumb|Diagram of a search realized through example based on detectable regions from an image]]

==== Image search techniques ====
Two techniques currently used in image search:

'''Search by metadata:''' Image search is based on comparison of metadata associated with the image as keywords, text, etc.. and it is obtained a set of images sorted by relevance. The metadata associated with each image can reference the title of the image, format, color, etc.. and can be generated manually or automatically. This metadata generation process is called audiovisual indexing.

'''Search by example:''' In this technique, also called [[content-based image retrieval]], the search results are obtained through the comparison between images using computer vision techniques. During the search it is examined the content of the image such as color, shape, texture or any visual information that can be extracted from the image. This system requires a higher [[Computational complexity theory|computational complexity]], but is more efficient and reliable than search by metadata.

There are image searchers that combine both search techniques, as the first search is done by entering a text, and then, from the images obtained can refine the search using as search parameters the images which appear as a result.

=== Video search ===
A video search is a [[search engine (computing)|search engine]] designed to search video on the net. Some video searchers process the search directly in the Internet, while others shelter the videos from which the search is done. Some searchers also enable to use as search parameters the [[File format|format]] or the length of the video. Usually the results come with a miniature capture of the video.

==== Video search techniques ====
Currently, almost all video searchers are based on keywords (search by metadata) to perform searches. These keywords can be found in the title of the video, text accompanying the video or can be defined by the author. An example of this type of search is [[YouTube]].

Some searchers generate keywords manually, while others use [[algorithms]] to analyze the audiovisual content of the video and to generate labels. The combination of these two processes improves the reliability of the search.

=== 3D Models searcher ===
A searcher of 3D models aims to find the file of a 3D modeling object from a [[database]] or network. At first glance the implementation of this type of searchers may seem unnecessary, but due to the continuous documentary inflation of the Internet, every day it becomes more necessary indexing information.

==== 3D Models search techniques ====
These have been used with traditional text-based searchers (keywords / tags), where the authors of the indexed material, or Internet users, have contributed these tags or keywords.  Because it is not always effective, it has recently been investigated in the implementation of search engines that combine the search using text with the search compared to 2D drawings, 3D drawings and 3D models.

[[Princeton University]] has developed a search engine that combines all these parameters to perform the search, thus increasing the efficiency of search.<ref name= funk>{{cite journal | last= Funkhouser | first= Thomas  | first2 = Patrick | last2 = Min | first3 = Michael | last3 = Kazhdan | first4 = Joyce | last4 = Chen | first5 = Alex | last5 = Halderman | first6 = David | last6 = Dobkin | first7 = David | last7 = Jacobs  | year= 2002 | title=  A Search Engine for 3D Models | journal= ACM Transactions on Graphics |url=https://www.cs.princeton.edu/~funk/tog03.pdf | volume= 22 | issue =1  | pages= 83-105  |doi = 10.1145/588272.588279 }}</ref>

Imaginestics LLC created the world's first online shape search engine in the fall of 2005.<ref>{{cite web|url=http://www.purdue.edu/uns/html3month/2006/060824.Imaginestics.grant.html|title=Purdue Research Park's Imaginestics wins grant for research on search engines|work=purdue.edu}}</ref> They currently use VizSeek search engine technology in the industrial and manufacturing settings to help discover parts using shape as the matching criteria.

=== Mobile visual search ===
A mobile image searcher is a type of [[search engine]] designed exclusively for mobile phones, through which you can find any information on [[Internet]], through an image made with the own [[mobile phone]] or using certain words ([[Keyword (computer programming)|keywords]]).

==== Introduction ====
Mobile phones have evolved into powerful image and video processing devices equipped with high-resolution cameras, color displays, and hardware-accelerated graphics. They are also increasingly equipped with a global positioning system and connected to broadband wireless networks. All this enables a new class of applications that use the camera phone to initiate search queries about objects in visual proximity to the user (Figure 1). Such applications can be used, e.g., for identifying products, comparison shopping, finding information about movies, compact disks (CDs), real estate, print media, or artworks.

==== Process ====
Typically, this type of search engine uses techniques of [[query by example]] or [[Content-based image retrieval|Image query by example]], which use the content, shape, texture and color of the image to compare them in a [[database]] and then deliver the approximate results from the query.

The process used in these searches in the [[mobile phone]]s is as follows:

First, the image is sent to the server application. Already on the server, the image will be analyzed by different analytical teams, as each one is specialized in different fields that make up an image. Then, each team will decide if the submitted image contains the fields of their speciality or not.

Once this whole procedure is done, a central computer will analyze the data and create a page of the results sorted with the efficiency of each team, to eventually be sent to the [[mobile phone]].

==== Applications ====
[[Google Goggles]] is the most popular application of image search engines{{citation required|date=November 2016}}, developed by [[Google labs|Google Labs]]. Available for [[Android (operating system)|Android]] only today. [[CamFind]] is a similar application available for both [[Android (operating system)|Android]] and [[iOS]].

JustVisual.com (formerly known as 'Superfish') and its LikeThat showcase apps are API for developers to create their own visual-search mobile app.

Other companies in the image recognition space are the [[reverse image search]]-engines [[TinEye]] and [[Google]]'s [[Google Images#Search by image|"search by image" feature of Google Images]].

== Visual display searchers ==
Another type of visual search is a search engine that shows results with a visual display image. This is an alternative to the traditional results of a sequence of links. Through some kind of image display, such as graphs, diagrams, previews of the websites, etc., it presents the results visually so that it is easier to find the desired material.
Such search engines like [http://www.kiddle.co/ Kiddle] and Manzia<ref>{{cite web|title=Manzia Search|website=http://www.manzia.com|accessdate=8 August 2014}}</ref> present a new concept in the presentation of results, but the search techniques used are the same as in other search engines.

==References==
{{reflist}}

[[Category:Information retrieval genres]]
[[Category:Internet search engines]]
[[Category:Multimedia]]
<=====doc_Id=====>:687
<=====title=====>:
Cross-language information retrieval
<=====text=====>:
{{refimprove|date=September 2014}}

'''Cross-language information retrieval (CLIR)''' is a subfield of [[information retrieval]] dealing with retrieving information written in a language different from the language of the user's query. For example, a user may pose their query in English but retrieve relevant documents written in French. To do so, most of CLIR systems use translation techniques.<ref>"[https://www.academia.edu/2475776/Versatile_question_answering_systems_seeing_in_synthesis Versatile question answering systems: seeing in synthesis]", Mittal et al., IJIIDS, 5(2), 119-142, 2011.</ref>  CLIR techniques can be classified into different categories based on different translation resources: 
* Dictionary-based CLIR techniques
* Parallel corpora based CLIR techniques
* Comparable corpora based CLIR techniques
* Machine translator based CLIR techniques

The first workshop on CLIR was held in Zürich during the SIGIR-96 conference.<ref>The proceedings of this workshop can be found in the book ''Cross-Language Information Retrieval'' (Grefenstette, ed; Kluwer, 1998) ISBN 0-7923-8122-X.</ref> Workshops have been held yearly since 2000 at the meetings of the [[Cross Language Evaluation Forum]] (CLEF).

The term "cross-language information retrieval" has many synonyms, of which the following are perhaps the most frequent: cross-lingual information retrieval, translingual information retrieval, multilingual information retrieval. The term "multilingual information retrieval" refers to CLIR in general, but it also has a specific meaning of cross-language information retrieval where a document collection is multilingual.

[[Google Search]] had a cross-language search feature that was removed in 2013.<ref>{{cite web|url=http://searchengineland.com/google-drops-translated-foreign-pages-search-option-due-to-lack-of-use-160157|title=Google Drops "Translated Foreign Pages" Search Option Due To Lack Of Use|date=20 May 2013|publisher=}}</ref>

==See also==
*[[EXCLAIM]] (EXtensible Cross-Linguistic Automatic Information Machine)

==References==
<references />

==External links==
*[http://www.glue.umd.edu/~oard/research.html A resource page for CLIR]
*[http://www.2lingual.com/ A search engine for CLIR]
{{DEFAULTSORT:Cross-Language Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Natural language processing]]


{{linguistics-stub}}
<=====doc_Id=====>:690
<=====title=====>:
Human–computer information retrieval
<=====text=====>:
'''Human-computer information retrieval''' ('''HCIR''') is the study and engineering of [[information retrieval]] techniques that bring human intelligence into the [[search engine|search]] process. It combines the fields of [[human-computer interaction]] (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback.

== History ==

This term ''human–computer information retrieval'' was coined by Gary Marchionini in a series of lectures delivered between 2004 and 2006.<ref name=march2006>[http://www.asis.org/Bulletin/Jun-06/marchionini.html Marchionini, G. (2006). Toward Human-Computer Information Retrieval Bulletin, in June/July 2006 Bulletin of the American Society for Information Science]</ref> Marchionini’s main thesis is that "HCIR aims to empower people to explore large-scale information bases but demands that people also take responsibility for this control by expending cognitive and physical energy."

In 1996 and 1998, a pair of workshops at the [[University of Glasgow]] on [[information retrieval]] and [[human–computer interaction]] sought to address the overlap between these two fields. Marchionini notes the impact of the [[World Wide Web]] and the sudden increase in [[information literacy]] – changes that were only embryonic in the late 1990s.

A few workshops have focused on the intersection of IR and HCI. The Workshop on Exploratory Search, initiated by the [[University of Maryland Human-Computer Interaction Lab]] in 2005, alternates between the [[Association for Computing Machinery]] [[Special Interest Group on Information Retrieval]] (SIGIR) and [[CHI (conference)|Special Interest Group on Computer-Human Interaction]] (CHI) conferences. Also in 2005, the [[European Science Foundation]] held an Exploratory Workshop on Information Retrieval in Context. Then, the first Workshop on Human Computer Information Retrieval was held in 2007 at the [[Massachusetts Institute of Technology]].

== Description ==

HCIR includes various aspects of IR and HCI. These include [[exploratory search]], in which users generally combine querying and browsing strategies to foster learning and investigation; information retrieval in context (i.e., taking into account aspects of the user or environment that are typically not reflected in a query); and interactive information retrieval, which Peter Ingwersen defines as "the interactive communication processes that occur during the retrieval of information by involving all the major participants in information retrieval (IR), i.e. the user, the intermediary, and the IR system."<ref name=ingwer1992>[http://vip.db.dk/pi/iri/index.htm Ingwersen, P. (1992). Information Retrieval Interaction. London: Taylor Graham.]</ref>

A key concern of HCIR is that IR systems intended for human users be implemented and evaluated in a way that reflects the needs of those users.<ref>{{cite web|title=Mira working group (1996). Evaluation Frameworks for Interactive Multimedia Information Retrieval Applications|url=http://www.dcs.gla.ac.uk/mira/}}</ref>

Most modern IR systems employ a [[ranking|ranked]] retrieval model, in which the documents are scored based on the [[probability]] of the document's [[relevance]] to the query.<ref>Grossman, D. and Frieder, O. (2004). Information Retrieval Algorithms and Heuristics. </ref> In this model, the system only presents the top-ranked documents to the user. This systems are typically evaluated based on their [[Information retrieval#Average precision of precision and recall|mean average precision]] over a set of benchmark queries from organizations like the [[Text Retrieval Conference]] (TREC).

Because of its emphasis in using human intelligence in the information retrieval process, HCIR requires different evaluation models – one that combines evaluation of the IR and HCI components of the system. A key area of research in HCIR involves evaluation of these systems. Early work on interactive information retrieval, such as Juergen Koenemann and [[Nicholas J. Belkin]]'s 1996 study of different levels of interaction for automatic query reformulation, leverage the standard IR measures of [[Information retrieval#Precision|precision]] and [[Information retrieval#Recall|recall]] but apply them to the results of multiple iterations of user interaction, rather than to a single query response.<ref name=koene1996>[http://sigchi.org/chi96/proceedings/papers/Koenemann/jk1_txt.htm Koenemann, J. and Belkin, N. J. (1996). A case for interaction: a study of interactive information retrieval behavior and effectiveness. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems: Common Ground (Vancouver, British Columbia, Canada, April 13–18, 1996). M. J. Tauber, Ed. CHI '96. ACM Press, New York, NY, 205-212]</ref> Other HCIR research, such as Pia Borlund's IIR evaluation model, applies a methodology more reminiscent of HCI, focusing on the characteristics of users, the details of experimental design, etc.<ref name=borlund2003>[http://informationr.net/ir/8-3/paper152.html Borlund, P. (2003). The IIR evaluation model: a framework for evaluation of interactive information retrieval systems. Information Research, 8(3), Paper 152]</ref>

== Goals ==
HCIR researchers have put forth the following goals towards a system where the user has more control in determining relevant results.<ref name=march2006/><ref name=ipm2013>[https://dl.acm.org/citation.cfm?id=2504017 White, R., Capra, R., Golovchinsky, G., Kules, B., Smith, C., and Tunkelang, D. (2013). Introduction to Special Issue on Human-computer Information Retrieval. Journal of Information Processing and Management 49(5), 1053-1057]</ref>

Systems should
*no longer only deliver the relevant documents, but must also provide semantic information along with those documents
*increase user responsibility as well as control; that is, information systems require human intellectual effort
*have flexible architectures so they may evolve and adapt to increasingly more demanding and knowledgeable user bases
*aim to be part of information ecology of personal and [[Collective memory|shared memories]] and tools rather than discrete standalone services
*support the entire information life cycle (from creation to preservation) rather than only the dissemination or use phase
*support tuning by end users and especially by information professionals who add value to information resources
*be engaging and fun to use

In short, information retrieval systems are expected to operate in the way that good libraries do. Systems should help users to bridge the gap between data or information (in the very narrow, granular sense of these terms) and knowledge (processed data or information that provides the context necessary to inform the next iteration of an information seeking process). That is, good libraries provide both the information a patron needs as well as a partner in the learning process — the [[information professional]] — to navigate that information, make sense of it, preserve it, and turn it into knowledge (which in turn creates new, more informed information needs).

== Techniques ==

The techniques associated with HCIR emphasize representations of information that use human intelligence to lead the user to relevant results. These techniques also strive to allow users to explore and digest the dataset without penalty, i.e., without expending unnecessary costs of time, mouse clicks, or context shift.

Many [[search engines]] have features that incorporate HCIR techniques. [[Spelling suggestion]]s and [[query expansion|automatic query reformulation]] provide mechanisms for suggesting potential search paths that can lead the user to relevant results. These suggestions are presented to the user, putting control of selection and interpretation in the user’s hands.

[[Faceted search]] enables users to navigate information [[hierarchy|hierarchically]], going from a category to its sub-categories, but choosing the order in which the categories are presented. This contrasts with traditional [[Taxonomy (general)|taxonomies]] in which the hierarchy of categories is fixed and unchanging. [[Faceted classification|Faceted navigation]], like taxonomic navigation, guides users by showing them available categories (or facets), but does not require them to browse through a hierarchy that may not precisely suit their needs or way of thinking.<ref>Hearst, M. (1999). User Interfaces and Visualization, Chapter 10 of Baeza-Yates, R. and Ribeiro-Neto, B., Modern Information Retrieval.</ref>

[[Combinatorial search#Lookahead|Lookahead]] provides a general approach to penalty-free exploration. For example, various [[web applications]] employ [[Ajax (programming)|AJAX]] to automatically complete query terms and suggest popular searches. Another common example of lookahead is the way in which search engines annotate results with summary information about those results, including both static information (e.g., [[metadata]] about the objects) and "snippets" of document text that are most pertinent to the words in the search query.

[[Relevance feedback]] allows users to guide an IR system by indicating whether particular results are more or less relevant.<ref>Rocchio, J. (1971). Relevance feedback in information retrieval. In: Salton, G (ed), The SMART Retrieval System.</ref>

Summarization and [[analytics]] help users digest the results that come back from the query. Summarization here is intended to encompass any means of [[aggregate data|aggregating]] or [[data compression|compressing]] the query results into a more human-consumable form. Faceted search, described above, is one such form of summarization. Another is [[cluster analysis|clustering]], which analyzes a set of documents by grouping similar or co-occurring documents or terms. Clustering allows the results to be partitioned into groups of related documents. For example, a search for "java" might return clusters for [[Java (programming language)]], [[Java|Java (island)]], or [[Java (coffee)]].

[[information visualization|Visual representation of data]] is also considered a key aspect of HCIR. The representation of summarization or analytics may be displayed as tables, charts, or summaries of aggregated data. Other kinds of [[information visualization]] that allow users access to summary views of search results include [[tag clouds]] and [[treemapping]].

== Related Areas ==
* [[Exploratory Video Search]]

== References ==

<References/>

==External links==
*{{cite web|url=https://sites.google.com/site/hcirworkshop/ |title=Workshops on Human Computer Information Retrieval}}
*{{cite web|url=http://www.chiir.org/ |title=ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR)}}

{{DEFAULTSORT:Human-computer information retrieval}}
[[Category:Information retrieval genres]]
[[Category:Human–computer interaction]]
<=====doc_Id=====>:693
<=====title=====>:
Expertise finding
<=====text=====>:
{{multiple issues|
{{original research|date=June 2015}}
{{refimprove|date=June 2015}}
{{cleanup|date=November 2010}}{{External links|date=January 2012}}
}}
'''Expertise finding''' is the use of tools for finding and assessing individual [[expertise]], with particular focus on scientific expertise.

== Importance of expertise ==

It can be argued that human expertise is more valuable than capital, means of production or intellectual property. Contrary to expertise, all other aspects of capitalism are now relatively generic: access to capital is global, as is access to means of production for many areas of manufacturing.  [[Intellectual property]] can be similarly licensed.  Furthermore, expertise finding is also a key aspect of [[institutional memory]], as without its experts an institution is effectively decapitated.  However, finding and “licensing” expertise, the key to the effective use of these resources, remain much harder, starting with the very first step: finding expertise that you can trust.

Until very recently, finding expertise required a mix of individual, social and collaborative practices, a haphazard process at best.  Mostly, it involved contacting individuals one trusts and asking them for referrals, while hoping that one’s judgment about those individuals is justified and that their answers are thoughtful.

In the last fifteen years, a class of [[knowledge management]] software has emerged to facilitate and improve the quality of expertise finding, termed “expertise locating systems”.  These software range from [[Social network service|social networking systems]] to [[knowledge base]]s.  Some software, like those in the social networking realm, rely on users to connect each other, thus using social filtering to act as [[Recommender system|“recommender systems”]].

At the other end of the spectrum are specialized [[knowledge base]]s that rely on experts to populate a specialized type of [[database]] with their self-determined areas of expertise and contributions, and do not rely on user recommendations.  Hybrids that feature expert-populated content in conjunction with user recommendations also exist, and are arguably more valuable for doing so.

Still other expertise knowledge bases rely strictly on external manifestations of expertise, herein termed “gated objects”, e.g., [[citation impact]]s for scientific papers or [[data mining]] approaches wherein many of the work products of an expert are collated.  Such systems are more likely to be free of user-introduced biases (e.g., [http://researchscorecard.com/ ResearchScorecard] ), though the use of computational methods can introduce other biases.

More recently, LinkedIn Expertise Search introduces a hybrid approach based on user-generated data (e.g., member profiles), community-based signals (e.g., recommendations and skill endorsements) and personalized signals (e.g., social connection between searcher and results).<ref name=":0">{{Cite journal|last=Ha-Thuc|first=Viet|last2=Venkataraman|first2=Ganesh|last3=Rodriguez|first3=Mario|last4=Sinha|first4=Shakti|last5=Sundaram|first5=Senthil|last6=Guo|first6=Lin|date=2016-02-15|title=Personalized Expertise Search at LinkedIn|url=http://arxiv.org/abs/1602.04572|journal=arXiv:1602.04572 [cs]}}</ref> Given required [[LinkedIn#Skills|skills]] and other types of information need like location and industries, the system allows recruiters to search for hiring candidates amongst more than 450 million LinkedIn members.

Examples of the systems outlined above are listed in Table 1.

'''Table 1: A classification of expertise location systems'''

{| class="wikitable" border="1"
|-
! Type
! Application domain
! Data source
! Examples
|-
| Social networking
| Professional networking
| User-generated and community-generated
|
* [[LinkedIn]] <ref name=":0" />
|-
| [[Scientific literature]]
| Identifying publications with strongest research impact
| Third-party generated
|
* [[Science Citation Index]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/science/science_products/a-z/science_citation_index]
|-
| [[Scientific literature]]
| Expertise search
| Software
|
* [[Arnetminer]][http://arnetminer.org]
|-
| Knowledge base
| Private expertise database
| User-Generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* Decisiv Search Matters & Expertise ([[Recommind (software company)|Recommind]], Inc.)
* [http://www.profinda.com ProFinda] (ProFinda Ltd)
* [https://skillhive.com Skillhive] (Intunex)
* [[Tacit Software]] (Oracle Corporation)
* [http://www.guruscan.nl GuruScan] (GuruScan Social Expert Guide)
|-
| Knowledge base
| Publicly accessible expertise database
| User-generated
|
* [http://expertisefinder.com/ Expertise Finder]<ref>http://expertisefinder.com/</ref>
* [[Community of Science]] Expertise [http://expertise.cos.com]
* [[ResearcherID]] (Thomson Reuters)[http://www.thomsonreuters.com/products_services/scientific/ResearcherID]
|-
| Knowledge base
| Private expertise database
| Third party-generated
|
* [http://www.mitre.org/news/the_edge/june_98/third.html MITRE Expert Finder] (MITRE Corporation)
* MIT ExpertFinder (ref. 3)
* MindServer Expertise ([[Recommind]], Inc.)
* Tacit Software
|-
| Knowledge base
| Publicly accessible expertise database
| Third party-generated
|
* [http://researchscorecard.com ResearchScorecard] (ResearchScorecard Inc.)
* [http://authoratory.com/ authoratory.com]
* [http://biomedexperts.com BiomedExperts] (Collexis Holdings Inc.)
* [http://www.hcarknowledgemesh.com/ KnowledgeMesh] (Hershey Center for Applied Research)
* [http://med.stanford.edu/profiles/ Community Academic Profiles] (Stanford School of Medicine)
* [https://web.archive.org/web/20081120175851/http://www.researchcrossroads.org/ ResearchCrossroads.org]  (Innolyst, Inc.)
|-
| Blog [[search engine]]s
|
| Third party-generated
|
* [[Technorati]] [http://technorati.com/]
|}

== Technical problems ==
A number of interesting problems follow from the use of expertise finding systems:

* The matching of questions from non-expert to the database of existing expertise is inherently difficult, especially when the database does not store the requisite expertise.  This problem grows even more acute with increasing ignorance on the part of the non-expert due to typical search problems involving use of keywords to search unstructured data that are not semantically normalized, as well as variability in how well an expert has set up their descriptive content pages.  Improved question matching is one reason why third-party semantically normalized systems such as [http://researchscorecard.com ResearchScorecard] and [[BiomedExperts]] should be able to provide better answers to queries from non-expert users.
* Avoiding expert-fatigue due to too many questions/requests from users of the system (ref. 1).
* Finding ways to avoid “gaming” of the system to reap unjustified expertise [[credibility]].
* Infer expertise on implicit skills. Since users typically do not declare all of the skills they have, it is important to infer their implicit skills that are highly related their explicit ones. The inference step can significantly improve [[Precision and recall|recall]] in expertise finding.

== Expertise ranking ==

Means of classifying and ranking expertise (and therefore experts) become essential if the number of experts returned by a query is greater than a handful.  This raises the following social problems associated with such systems:

* How can expertise be assessed objectively? Is that even possible?
* What are the consequences of relying on unstructured social assessments of expertise, such as user recommendations?
* How does one distinguish [[Authority|''authoritativeness'']] as a proxy metric of expertise from simple ''popularity'', which is often a function of one's ability to express oneself coupled with a good social sense?
* What are the potential consequences of the social or professional stigma associated with the use of an authority ranking, such as used in [http://technorati.com Technorati] and [http://researchscorecard.com ResearchScorecard])?
* How to make expertise ranking personalized to each individual searcher? This is particularly important for recruiting purpose since given the same skills, recruiters from different companies, industries, locations might have different preferences on candidates <ref name=":0" />

== Sources of data for assessing expertise ==
Many types of data sources have been used to infer expertise.  They can be broadly categorized based on whether they measure "raw" contributions provided by the expert, or whether some sort of filter is applied to these contributions.

Unfiltered data sources that have been used to assess expertise, in no particular ranking order:

* user recommendations
* help desk tickets: what the problem was and who fixed it
* e-mail traffic between users
* documents, whether private or on the web, particularly publications
* user-maintained web pages
* reports (technical, marketing, etc.)

Filtered data sources, that is, contributions that require approval by third parties (grant committees, referees, patent office, etc.) are particularly valuable for measuring expertise in a way that minimizes biases that follow from popularity or other social factors:

* [[patent]]s, particularly if issued
* scientific publications
* issued grants (failed grant proposals are rarely know beyond the authors)
* [[clinical trial]]s
* product launches
* pharmaceutical drugs

== Approaches for creating expertise content ==
* Manual, either by experts themselves (e.g., [https://skillhive.com Skillhive]) or by a curator ([http://expertisefinder.com/ Expertise Finder])
* Automated, e.g., using [[software agent]]s (e.g., MIT's [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] and the [http://wiki.foaf-project.org/ExpertFinder ExpertFinder] initiative) or a combination of agents and human curation (e.g., [http://researchscorecard.com/ ResearchScorecard] )
* In industrial expertise search engines (e.g., LinkedIn), there are many signals coming into the ranking functions, such as, user-generated content (e.g., profiles), community-generated content (e.g., recommendations and skills endorsements) and personalized signals (e.g., social connections). Moreover, user queries might contain many other aspects rather required expertise, such as, locations, industries or companies. Thus, traditional [[information retrieval]] features like text matching are also important. [[Learning to rank]] is typically used to combine all of these signals together into a ranking function <ref name=":0" />

== Interesting expertise systems over the years ==
In no particular order...

* [http://www.guruscan.nl/ GuruScan]
* Autonomy's IDOL
* AskMe
* [http://expertisefinder.com/ Expertise Finder]
* Tacit Knowledge Systems' ActiveNet
* Triviumsoft's SEE-K
* MIT’s [http://web.media.mit.edu/~lieber/Lieberary/Expert-Finder/Expert-Finder-Intro.html ExpertFinder] (ref 3)
* MITRE’s (ref 1) [http://www.mitre.org/news/the_edge/june_98/third.html Expert Finder]
* MITRE’s XpertNet
* Arnetminer (ref 2)
* Dataware II Knowledge Directory
* Thomson’s tool
* Hewlett-Packard’s CONNEX
* Microsoft’s SPUD project
* [http://www.profinda.com ProFinda]
* [http://www.xperscore.com Xperscore]
* [http://intunex.fi/skillhive/ Skillhive]
* LinkedIn<ref name=":0" />

== Conferences ==
# [http://expertfinder.info/pickme2008 The ExpertFinder Initiative]

== References ==
{{Reflist}}

# Ackerman, Mark and McDonald, David (1998) "Just Talk to Me: A Field Study of Expertise Location" ''Proceedings of the 1998 ACM Conference on Computer Supported Cooperative Work''.
# Hughes, Gareth and Crowder, Richard (2003) "Experiences in designing highly adaptable expertise finder systems"  ''Proceedings of the DETC Conference 2003''.
# Maybury, M., D’Amore, R., House, D. (2002). "Awareness of organizational expertise." ''International Journal of Human-Computer Interaction'' '''14'''(2): 199-217.
# Maybury, M., D’Amore, R., House, D. (2000). Automating Expert Finding. ''International Journal of Technology Research Management.'' 43(6): 12-15.
# Maybury, M., D’Amore, R, and House, D. December (2001). Expert Finding for Collaborative Virtual Environments.  ''Communications of the ACM 14''(12): 55-56. In Ragusa, J. and Bochenek, G. (eds). Special Section on Collaboration Virtual Design Environments.
# Maybury, M., D’Amore, R. and House, D. (2002). Automated Discovery and Mapping of Expertise.  In Ackerman, M., Cohen, A., Pipek, V. and Wulf, V. (eds.). ''Beyond Knowledge Management: Sharing Expertise.'' Cambridge: MIT Press.
# Mattox, D., M. Maybury, ''et al.'' (1999). "Enterprise expert and knowledge discovery". ''Proceedings of the 8th International Conference on Human-Computer Interactions (HCI International 99)'', Munich, Germany.
# Tang, J., Zhang J., Yao L., Li J., Zhang L. and Su Z.(2008) "ArnetMiner: extraction and mining of academic social networks" ''Proceeding of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining''.
# Viavacqua, A. (1999). "Agents for expertise location". ''Proceedings of the 1999 AAAI Spring Symposium on Intelligent Agents in Cyberspace'', Stanford, CA.

[[Category:Evaluation methods]]
[[Category:Metrics]]
[[Category:Analysis]]
[[Category:Impact assessment]]
[[Category:Knowledge sharing]]
[[Category:Library science]]
[[Category:Information retrieval genres]]
[[Category:Science studies]]
<=====doc_Id=====>:696
<=====title=====>:
Audio mining
<=====text=====>:
{{unreferenced|date=January 2012}}
'''Audio mining''' is a technique by which the content of an audio signal can be automatically analysed and searched. It is most commonly used in the field of [[speech recognition|automatic speech recognition]], where the analysis tries to identify any speech within the audio. The audio will typically be processed by a speech recognition system in order to identify word or [[phoneme]] units that are likely to occur in the spoken content. This information may either be used immediately in pre-defined searches for keywords or phrases (a real-time "word spotting" system), or the output of the speech recogniser may be stored in an index file. One or more audio mining index files can then be loaded at a later date in order to run searches for keywords or phrases.

The results of a search will normally be in terms of hits, which are regions within files that are good matches for the chosen keywords. The user may then be able to listen to the audio corresponding to these hits in order to verify if a correct match was found.

Audio mining systems used in the field of speech recognition are often divided into two groups: those that use [[Large Vocabulary Continuous Speech Recogniser]]s (LVCSR) and those that use phonetic recognition. 

Musical audio mining (also known as [[music information retrieval]]) relates to the identification of perceptually important characteristics of a piece of music such as melodic, harmonic or rhythmic structure. Searches can then be carried out to find pieces of music that are similar in terms of their melodic, harmonic and/or rhythmic characteristics.

==See also==
* [[Speech Analytics]]


[[Category:Speech recognition]]
[[Category:Music information retrieval]]
[[Category:Information retrieval genres]]
[[Category:Computational linguistics]]
<=====doc_Id=====>:699
<=====title=====>:
Full-text search
<=====text=====>:
{{Multiple issues|
{{refimprove|date=August 2012}}
{{cleanup|date=September 2009}}
}}

In [[text retrieval]], '''full-text search''' refers to techniques for searching a single [[computer]]-stored [[document]] or a collection in a [[full text database]]. Full-text search is distinguished from searches based on [[metadata]] or on parts of the original texts represented in databases (such as titles, abstracts, selected sections, or bibliographical references).

In a full-text search, a [[search engine]] examines all of the words in every stored document as it tries to match search criteria (for example, text specified by a user). Full-text-searching techniques became common in online [[bibliographic databases]] in the 1990s.{{Verify source|date=October 2008}} Many websites and application programs (such as [[word processing]] software) provide full-text-search capabilities. Some web search engines, such as [[AltaVista]], employ full-text-search techniques, while others index only a portion of the web pages examined by their indexing systems.<ref>In practice, it may be difficult to determine how a given search engine works. The [[search algorithms]] actually employed by web-search services are seldom fully disclosed out of fear that web entrepreneurs will use [[search engine optimization]] techniques to improve their prominence in retrieval lists.</ref>

==Indexing==
When dealing with a small number of documents, it is possible for the full-text-search engine to directly scan the contents of the documents with each [[Information retrieval|query]], a strategy called "[[Serial memory processing|serial scanning]]". This is what some tools, such as [[grep]], do when searching.

However, when the number of documents to search is potentially large, or the quantity of search queries to perform is substantial, the problem of full-text search is often divided into two tasks: indexing and searching. The indexing stage will scan the text of all the documents and build a list of search terms (often called an [[Search index|index]], but more correctly named a [[concordance (publishing)|concordance]]). In the search stage, when performing a specific query, only the index is referenced, rather than the text of the original documents.<ref name="Capabilities of Full Text Search System ">[http://www.lucidimagination.com/full-text-search Capabilities of Full Text Search System] {{webarchive |url=https://web.archive.org/web/20101223192214/http://www.lucidimagination.com/full-text-search |date=December 23, 2010 }}</ref>

The indexer will make an entry in the index for each term or word found in a document, and possibly note its relative position within the document. Usually the indexer will ignore [[stop words]] (such as "the" and "and") that are both common and insufficiently meaningful to be useful in searching. Some indexers also employ language-specific [[stemming]] on the words being indexed. For example, the words "drives", "drove", and "driven" will be recorded in the index under the single concept word "drive".

==The precision vs. recall tradeoff==
[[Image:Full-text-search-results.png|150px|thumb|right|Diagram of a low-precision, low-recall search]]
Recall measures the quantity of relevant results returned by a search, while precision is the measure of the quality of the results returned. Recall is the ratio of relevant results returned to all relevant results. Precision is the number of relevant results returned to the total number of results returned.

The diagram at right represents a low-precision, low-recall search. In the diagram the red and green dots represent the total population of potential search results for a given search. Red dots represent irrelevant results, and green dots represent relevant results. Relevancy is indicated by the proximity of search results to the center of the inner circle. Of all possible results shown, those that were actually returned by the search are shown on a light-blue background. In the example only 1 relevant result of 3 possible relevant results was returned, so the recall is a very low ratio of 1/3, or 33%. The precision for the example is a very low 1/4, or 25%, since only 1 of the 4 results returned was relevant.<ref name="isbn1430215941">{{cite book|last=Coles|first=Michael|year=2008|title=Pro Full-Text Search in SQL Server 2008|edition=Version 1|publisher=[[Apress|Apress Publishing Company]]|isbn=1-4302-1594-1}}</ref>

Due to the ambiguities of [[natural language]], full-text-search systems typically includes options like [[stop words]] to increase precision and [[stemming]] to increase recall. [[Controlled vocabulary|Controlled-vocabulary]] searching also helps alleviate low-precision issues by [[tag (metadata)|tagging]] documents in such a way that ambiguities are eliminated. The trade-off between precision and recall is simple: an increase in precision can lower overall recall, while an increase in recall lowers precision.<ref name="YuwonoLee">{{Cite conference | first = Yuwono | last = B. |author2=Lee, D. L. | title = Search and ranking algorithms for locating resources on the World Wide Web | pages = 164 | publisher = 12th International Conference on Data Engineering (ICDE'96) | year = 1996}}</ref>

{{See also|Precision and recall}}

==False-positive problem==

Free text searching is likely to retrieve many documents that are not [[relevance|relevant]] to the ''intended'' search question. Such documents are called ''false positives'' (see [[Type I and type II errors#Type I error|Type I error]]). The retrieval of irrelevant documents is often caused by the inherent ambiguity of [[natural language]]. In the sample diagram at right, false positives are represented by the irrelevant results (red dots) that were returned by the search (on a light-blue background).

Clustering techniques based on [[Bayesian inference|Bayesian]] algorithms can help reduce false positives. For a search term of "bank", clustering can be used to categorize the document/data universe into "financial institution", "place to sit", "place to store" etc. Depending on the occurrences of words relevant to the categories, search terms or a search result can be placed in one or more of the categories. This technique is being extensively deployed in the [[Electronic discovery|e-discovery]] domain.{{clarify|date=January 2012}}

==Performance improvements==

The deficiencies of free text searching have been addressed in two ways: By providing users with tools that enable them to express their search questions more precisely, and by developing new search algorithms that improve retrieval precision.

===Improved querying tools===

*[[Index term|Keyword]]s. Document creators (or trained indexers) are asked to supply a list of words that describe the subject of the text, including synonyms of words that describe this subject. Keywords improve recall, particularly if the keyword list includes a search word that is not in the document text.
* [[Field-restricted search]]. Some search engines enable users to limit free text searches to a particular [[field (computer science)|field]] within a stored [[Record (computer science)|data record]], such as "Title" or "Author."
* [[Boolean query|Boolean queries]]. Searches that use [[Boolean logic|Boolean]] operators (for example, <tt>"encyclopedia" [[Logical conjunction|AND]] "online" [[Negation|NOT]] "Encarta"<tt>) can dramatically increase the precision of a free text search. The <tt>AND</tt> operator says, in effect, "Do not retrieve any document unless it contains both of these terms." The <tt>NOT</tt> operator says, in effect, "Do not retrieve any document that contains this word." If the retrieval list retrieves too few documents, the <tt>OR</tt> operator can be used to increase [[recall (information retrieval)|recall]]; consider, for example, <tt>"encyclopedia" AND "online" [[Logical disjunction|OR]] "Internet" NOT "Encarta"</tt>. This search will retrieve documents about online encyclopedias that use the term "Internet" instead of "online." This increase in precision is very commonly counter-productive since it usually comes with a dramatic loss of recall.<ref>Studies have repeatedly shown that most users do not understand the negative impacts of boolean queries.[http://eprints.cs.vt.edu/archive/00000112/]</ref>
* [[Phrase search]]. A phrase search matches only those documents that contain a specified phrase, such as <tt>"Wikipedia, the free encyclopedia."</tt>
* [[Concept search]]. A search that is based on multi-word concepts, for example [[Compound term processing]]. This type of search is becoming popular in many e-Discovery solutions.
* [[Concordance search]]. A concordance search produces an alphabetical list of all principal words that occur in a [[Plain text|text]] with their immediate context.
* [[Proximity search (text)|Proximity search]]. A phrase search matches only those documents that contain two or more words that are separated by a specified number of words; a search for <tt>"Wikipedia" WITHIN2 "free"<tt> would retrieve only those documents in which the words <tt>"Wikipedia" and "free"</tt> occur within two words of each other.
* [[Regular expression]]. A regular expression employs a complex but powerful querying [[syntax]] that can be used to specify retrieval conditions with precision.
* [[Fuzzy search]] will search for document that match the given terms and some variation around them (using for instance [[edit distance]] to threshold the multiple variation)
* [[Wildcard character|Wildcard search]]. A search that substitutes one or more characters in a search query for a wildcard character such as an [[asterisk]]. For example, using the asterisk in a search query <tt>"s*n"</tt> will find "sin", "son", "sun", etc. in a text.

===Improved search algorithms===
The [[PageRank]] algorithm developed by [[Google]] gives more prominence to documents to which other [[Web page]]s have linked.<ref>{{Cite patent | inventor-last = Page | inventor-first = Lawrence | publication-date = 1/9/1998 | issue-date = 9/4/2001 | title = Method for node ranking in a linked database | country-code = US | description = A method assigns importance ranks to nodes in a linked database, such as any database of documents containing citations, the world wide web or any other hypermedia database. The rank assigned to a document is calculated from the ranks of documents citing it. In addition, the rank of a document is... | patent-number = 6285999 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref> See [[Search engine]] for additional examples.

==Software==

The following is a partial list of available software products whose predominant purpose is to perform full text indexing and searching. Some of these are accompanied with detailed descriptions of their theory of operation or internal algorithms, which can provide additional insight into how full text search may be accomplished.

{{col-float}}

=== Free and open source software ===
<!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

-->
* [[BaseX]]
* [[Clusterpoint|Clusterpoint Database]]
* [[Elasticsearch]]
* [[Ht-//Dig|ht://Dig]]
* [[KinoSearch]]
* [[Lemur Project|Lemur/Indri]]
* [[Lucene]]
* [[mnoGoSearch]]
* [[Searchdaimon]]
* [[Sphinx (search engine)|Sphinx]]
* [[Swish-e]]
* [[Xapian]]
* [[Apache Solr]]

{{col-float-break}}

=== Proprietary software ===
<!--

Please do not add web links or products which do not have Wikipedia articles. They will be summarily deleted.

-->
* [[Algolia]]
* [[Autonomy Corporation]]
* [[Azure Search]]
* [[Bar Ilan Responsa Project]]
* [[Brainware]]
* [[BRS/Search]] 
* [[Concept Searching Limited]]
* [[Dieselpoint]]
* [[dtSearch]]
* [[Endeca]]
* [[Exalead]]
* [[Funnelback]]
* [[Fast Search & Transfer]]
* [[Inktomi (company)|Inktomi]]
* [[Dan Wagner#Locayta|Locayta]](rebranded to [[ATTRAQT]] in 2014)
* [[Lucid Imagination]]
* [[MarkLogic]]
* [[SAP HANA]]<ref>http://www.martechadvisor.com/news/databases-big-data/sap-adds-hanabased-software-packages-to-iot-portfolio/</ref>
* [[Swiftype]]
* [[Thunderstone Software LLC.]]
* [[Vivísimo]]
{{col-float-end}}

==Notes==
{{Reflist}}

==See also==
*[[Pattern matching]] and [[string matching]]
*[[Compound term processing]]
*[[Enterprise search]]
*[[Information extraction]]
*[[Information retrieval]]
*[[Faceted search]]
*[[List of enterprise search vendors]]
*[[WebCrawler]], first FTS engine
*[[Search engine indexing]] - how search engines generate indices to support full text searching

{{DEFAULTSORT:Full Text Search}}
[[Category:Text editor features]]
[[Category:Information retrieval genres]]
<=====doc_Id=====>:702
<=====title=====>:
Maarten de Rijke
<=====text=====>:
[[File:Maarten de Rijke - CLEF 2011 (cropped).jpg|thumb|Maarten de Rijke, 2011]]'''Maarten de Rijke''' (born 1 August 1961) is a [[Netherlands|Dutch]] computer scientist. His work initially focused on [[modal logic]] and [[knowledge representation]], but since the early years of the 21st century he has worked mainly in [[information retrieval]]. His work is supported by grants from the [[Nederlandse Organisatie voor Wetenschappelijk Onderzoek]] (NWO), public-private partnerships, and the European Commission (under the Sixth and Seventh Framework programmes).


==Biography==
Maarten de Rijke was born in [[Vlissingen]].  He studied philosophy (MSc 1989) and mathematics (MSc 1990) and wrote a PhD thesis, defended in 1993, on extended modal logics, under the supervision of [[Johan van Benthem (logician)|Johan van Benthem]].

De Rijke worked as a postdoc at the [[Centrum Wiskunde & Informatica]], before becoming a Warwick Research Fellow at the [[University of Warwick]]. He joined the [[University of Amsterdam]] in 1998, and was appointed professor of Information Processing and Internet at the [[Informatics Institute]] of the University of Amsterdam in 2004.<ref name="MdR11Bio">[http://staff.science.uva.nl/~mdr/Bio/ Bio of Maarten de Rijke] at the University of Amsterdam. Retrieved 16 March 2011.</ref>

He leads the Information and Language Processing group<ref>[http://ilps.science.uva.nl Information and Language Processing group]</ref> at the University of Amsterdam, the Intelligent Systems Lab Amsterdam<ref>[http://isla.science.uva.nl Intelligent Systems Lab Amsterdam] within the Informatics Institute of the University of Amsterdam.</ref> and the Center for Creation, Content and Technology.<ref>[http://www.ccct.uva.nl Center for Creation, Content and Technology] at the University of Amsterdam.</ref>

==Work==
During the first ten years of his scientific career Maarten de Rijke worked on formal and applied aspects of modal logic. At the start of the 21st century, De Rijke switched to information retrieval. He has since worked on [[XML retrieval]], [[question answering]], [[expert finding]] and [[social media analysis]].

==Publications==
Maarten de Rijke has published more than 600 papers and books.<ref name="MdR11Pubs">[http://staff.science.uva.nl/~mdr/Publications/ List of publications of Maarten de Rijke] at the University of Amsterdam". Retrieved 16 March 2011.</ref>

==References==
{{reflist}}
*[http://albumacademicum.uva.nl/cgi/b/bib/bib-idx?type=simple;lang=en;c=ap;rgn1=entirerecord;q1=rijke;x=0;y=0;cc=ap;view=reslist;sort=achternaam;fmt=long;page=reslist;size=1;start=14 Prof. dr. M. de Rijke, 1961 -] at the [[University of Amsterdam]] ''Album Academicum'' website

==External links==
* [http://staff.science.uva.nl/~mdr Home page]

{{DEFAULTSORT:Rijke, Maarten De}}
[[Category:1961 births]]
[[Category:Living people]]
[[Category:Dutch computer scientists]]
[[Category:University of Amsterdam alumni]]
[[Category:University of Amsterdam faculty]]
[[Category:People from Vlissingen]]
[[Category:Information retrieval researchers]]
<=====doc_Id=====>:705
<=====title=====>:
Stephen Robertson (computer scientist)
<=====text=====>:
{{More footnotes|date=September 2012}}
{{Infobox scientist
| name = Stephen Robertson
| image =
| image_size = 100px
| residence = United Kingdom
| nationality = British
| field = Computer science
| alma_mater = Cambridge, City University, University College London
| doctoral_advisor = B.C (Bertie) Brookes
| doctoral_students = Ayse Göker, Andrew MacFarlane, Xiangji (Jimmy) Huang, Olga Vechtomova, Murat Karamuftuoglu, Micheline Beaulieu, Efthimis Efthimiadis, Anna Ritchie, Jagadeesh Gorla
| known_for  = Work on information retrieval and inverse document frequency
| prizes = [[Gerard Salton Award]] (2000), [[Tony Kent Strix award]] (1998), [[ACM Fellow]] (2013)
| website = {{URL|http://staff.city.ac.uk/~sb317}}
}}

'''Stephen Robertson''' is a [[United Kingdom|British]] computer scientist. He is known for his work on [[information retrieval]]<ref>{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}</ref> and the [[Okapi BM25]] weighting model.<ref>{{Cite journal | doi = 10.1016/S0306-4573(00)00015-7| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 1| journal = Information Processing & Management| volume = 36| issue = 6| pages = 779–808| year = 2000| last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| last2 = Walker | first2 = S.| last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}</ref><ref>{{Cite journal | doi = 10.1016/S0306-4573(00)00016-9| title = A probabilistic model of information retrieval: Development and comparative experiments: Part 2| journal = Information Processing & Management| volume = 36| issue = 6| pages = 809–840| year = 2000| last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| last2 = Walker | first2 = S. | last3 = Robertson | first3 = S. E. | authorlink3 = Stephen Robertson (computer scientist)}}</ref>

After completing his undergraduate degree in mathematics at [[Cambridge university|Cambridge University]], he took an MS at [[City University, London|City University]], and then worked for [[ASLIB]]. He then studied for his PhD at [[University College London]] under the renowned statistician and scholar B. C. Brookes. He then returned to City University working there from 1978 until 1998 in the Department of [[Information Science]], continuing as a part-time professor and subsequently as professor emeritus. He is also a fellow of [[Girton College, Cambridge|Girton College]], Cambridge University.

From 1998 to 2013 he worked in the Cambridge laboratory of [[Microsoft Research]], where he led a group investigating core search processes such as term weighting, document scoring and ranking algorithms, combining evidence from different sources, and metrics and methods for the evaluation and optimisation of search. Much of his work has contributed to the [[Microsoft]] [[Web search engine|search engine]] [[Bing (search engine)|Bing]]. He participated a number of times in the [[Text Retrieval Conference|TREC conference]].

==References==
{{Reflist}}

== External links ==
* {{cite book|last1=Robertson|first1=Stephen|last2=Zaragoza|first2=Hugo|title=The Probabilistic Relevance Framework: BM25 and Beyond|date=2009|publisher=NOW Publishers, Inc.|isbn=978-1-60198-308-4|url=http://staff.city.ac.uk/~sb317/papers/foundations_bm25_review.pdf}}

{{DEFAULTSORT:Robertson, Stephen}}
[[Category:British computer scientists]]
[[Category:Alumni of University College London]]
[[Category:Fellows of Girton College, Cambridge]]
[[Category:Living people]]
[[Category:Alumni of City, University of London]]
[[Category:Academics of City, University of London]]
[[Category:Information retrieval researchers]]
<=====doc_Id=====>:708
<=====title=====>:
Susan Dumais
<=====text=====>:
{{ Infobox scientist
| name              = Susan T. Dumais
| image             = Susan Dumais.jpg
| image_size        = 200px
| caption           = Susan Dumais in 2009 in her office at Microsoft Research.
| birth_date        = 
| birth_place       = [[Maine]], [[United States|US]]  
| death_date        = 
| death_place       = 
| nationality       = American
| fields            = [[Computer Science]]
| workplaces        = [[Microsoft Research]]
| alma_mater        = [[Indiana University]] <br />[[Bates College]]
| doctoral_advisor  = 
| doctoral_students = 
| known_for         = [[Human Computer Interaction]]<br /> [[Information Retrieval]]
| website           = {{URL|http://research.microsoft.com/~sdumais/}} 
| awards            = ACM-W Athena Lecturer Award (2014)
}}

'''Susan Dumais''' is an American computer scientist who is a leader in the field of information retrieval, and has been a significant contributor to Microsoft's search technologies.<ref>{{cite news|title=100 Top Women in Seattle Tech|url=http://www.bizjournals.com/seattle/blog/techflash/2009/05/Top_100_Women_in_Seattle_Tech_44225472.html|accessdate=23 February 2016|newspaper=Puget Sound Business Journal|date=8 May 2009}}</ref>
According to Mary Jane Irwin, who heads the Athena Lecture awards committee, “Her sustained contributions have shaped the thinking and direction of human-computer interaction and information retrieval."<ref>{{cite news|last=Burns|first=Jay|title=Microsoft’s Susan Dumais ’75 Is a Big Reason Why, Computer-Wise, You Find What You Seek|url=https://www.bates.edu/news/2014/05/01/microsoft-susan-dumais-75/|accessdate=23 February 2016|newspaper=Bates News|date=28 October 2015}}</ref>

==Biography==

Susan Dumais is a Distinguished Scientist at Microsoft and deputy managing director of the [[Microsoft Research]] lab in Redmond. She is also an Affiliate Professor at the [[University of Washington Information School]].

Before joining Microsoft in 1997, Dumais was a researcher at Bellcore (now [[Telcordia Technologies]]), where she and her colleagues conducted research into what is now called the [[vocabulary problem]] in [[information retrieval]].<ref>{{cite journal
| title=The Vocabulary Problem in Human-System Communication
| journal=Communications of the ACM
| author=[[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], L. M. Gomez, S. T. Dumais
| volume = 30
| pages = 964–971
| year = 1987
| url = http://citeseer.ist.psu.edu/furnas87vocabulary.html
| doi=10.1145/32206.32212
| issue = 11
}}</ref> Their study demonstrated, through a variety of experiments, that different people use different vocabulary to describe the same thing, and that even choosing the "best" term to describe something is not enough for others to find it.  One implication of this work is that because the author of a document may use different vocabulary than someone searching for the document, traditional [[information retrieval]] methods will have limited success.

Dumais and the other Bellcore researchers then began investigating ways to build search systems that avoided the vocabulary problem.  The result was their invention of [[Latent Semantic Indexing]].<ref>
 {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 | title=Indexing by Latent Semantic Analysis
 | author=[[Scott Deerwester|S. Deerwester]], Susan Dumais, [[George Furnas|G. W. Furnas]], [[Thomas Landauer|T. K. Landauer]], [[Richard Harshman|R. Harshman]]
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391–407
 |year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
}}</ref>

==Awards==

In 2006, Dumais was inducted as a [[Fellow]] of the [[Association for Computing Machinery]]. In 2009, she received the [[Gerard Salton Award]], an information retrieval lifetime achievement award. In 2011, she was inducted to the [[National Academy of Engineering]] for innovation and leadership in organizing, accessing, and interacting with information. In 2014, Dumais received the Athena Lecturer Award for "fundamental contributions to computer science.".<ref>{{cite news|last=Knies|first=Rob|title=Dumais Receives Athena Lecturer Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/08/dumais-receives-athena-lecturer-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}</ref> and the [[Tony Strix]] Award for "sustained contributions that are both innovative and practical" with "significant impact". <ref>{{cite web|title=The winner of the 2014 Tony Kent Strix Award is Dr Susan Dumais|url=http://www.ukeig.org.uk/awards/tony-kent-strix|accessdate=17 September 2014}}</ref>
In 2015, she was inducted into the [[American Academy of Arts and Sciences]].<ref>{{cite news|last=Tice|first=Lindsay|title=Lewiston native inducted into American Academy of Arts and Sciences|url=http://www.sunjournal.com/news/lewiston-auburn/0001/11/30/lewiston-native-inducted-american-academy-arts-and-sciences/1808943|accessdate=23 February 2016|newspaper=Lewinston-Auburn Sun-Journal|date=28 October 2015}}</ref>

==References==
{{reflist}}

==External links==
* [http://research.microsoft.com/~sdumais/ Home page at Microsoft Research]

{{DEFAULTSORT:Dumais, Susan}}
[[Category:People in information technology]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Microsoft employees]]
[[Category:Living people]]
[[Category:Women computer scientists]]
[[Category:University of Washington faculty]]
[[Category:Information retrieval researchers]]
<=====doc_Id=====>:711
<=====title=====>:
KM programming language
<=====text=====>:
{{Infobox programming language
| name = KM
| paradigm = [[knowledge representation]]
| generation =
| year = 
| designer =
| developer = 
| latest_release_version = 
| latest_release_date = 
| turing-complete = 
| typing = 
| implementations = 
| dialects = 
| influenced_by = [[KRL (programming language)|KRL]]
| influenced = 
}}

'''KM''', the '''Knowledge Machine''', is a [[Knowledge frame|frame]]-based language used for [[knowledge representation]] work.

It has first-order logic semantics, and includes machinery for reasoning, including selection by description, unification, classification, and reasoning about actions. Its origins were the Theo language and [[KRL (programming language)|KRL]], and is implemented in [[Lisp (programming language)|Lisp]].

==External links==
* [http://www.cs.utexas.edu/users/mfkb/RKF/km.html KM: The Knowledge Machine]. 
* An Ontology editor for the KM language: [http://www.algo.be/ref-projects.htm#KMgen/ KMgen].

[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]
[[Category:Common Lisp software]]


{{compu-lang-stub}}
<=====doc_Id=====>:714
<=====title=====>:
Colon classification
<=====text=====>:
'''Colon classification''' ('''CC''') is a system of [[library classification]] developed by [[S. R. Ranganathan]]. It was the first ever [[Faceted classification|faceted]] (or analytico-synthetic) [[Taxonomic classification|classification]]. The first edition was published in 1933. Since then six more editions have been published. It is especially used in [[library|libraries]] in [[India]].

Its name "colon classification" comes from the use of [[Colon (punctuation)|colons]] to separate facets in class numbers. However, many other classification schemes, some of which are completely unrelated, also use colons and other [[punctuation]] in various functions. They should not be confused with colon classification.

In CC, facets describe "personality" (the most specific subject), matter, energy, space, and time (PMEST).  These facets are generally associated with every item in a library, and so form a reasonably universal sorting system.<ref>GOPINATH (M A). Colon classification: Its theory and practice.
Library Herald
.
26, 1
-
2; 1987; 1
-
3.</ref>

As an example, the subject "research in the cure of tuberculosis of lungs by x-ray conducted in India in 1950" would be categorized as:

:Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India'1950

This is summarized in a specific call number:

:L,45;421:6;253:f.44'N5

== Organization ==

The colon classification uses 42 main classes that are combined with other letters, numbers and marks in a manner resembling the [[Library of Congress Classification]] to sort a publication.

=== Facets ===

CC uses five primary categories, or facets, to further specify the sorting of a publication. Collectively, they are called ''PMEST'':

:<nowiki>,</nowiki> Personality, the most specific or focal subject.
:<nowiki>;</nowiki> Matter or property, the substance, properties or materials of the subject.
:<nowiki>:</nowiki> Energy, including the processes, operations and activities.
:<nowiki>.</nowiki> Space, which relates to the geographic location of the subject.
:<nowiki>'</nowiki> Time, which refers to the dates or seasons of the subject.

=== Classes ===

The following are the main classes of CC, with some subclasses, the main method used to sort the subclass using the PMEST scheme and examples showing application of PMEST.

:z Generalia
:1 Universe of Knowledge
:2 [[Library Science]]
:3 Book science
:4 [[Journalism]]
:B [[Mathematics]]
::B2 [[Algebra]]
:C [[Physics]]
:D [[Engineering]]
:E [[Chemistry]]
:F [[Technology]]
:G [[Biology]]
:H [[Geology]]
::HX [[Mining]]
:I [[Botany]]
:J [[Agriculture]]
::J1 [[Horticulture]]
::J2 Feed
::J3 Food
::J4 Stimulant
::J5 Oil
::J6 Drug
::J7 Fabric
::J8 Dye
:K [[Zoology]] 
::KZ Animal Husbandry 
:L Medicine
::LZ3 [[Pharmacology]]
::LZ5 [[Pharmacopoeia]]
:M [[Useful arts]]
::M7 Textiles ''[material]:[work]''
:Δ Spiritual experience and [[mysticism]] ''[religion],[entity]:[problem]''
:N [[Fine arts]]
::ND Sculpture
::NN Engraving
::NQ Painting
::NR Music
:O Literature
:P Linguistics
:Q [[Religion]]
:R [[Philosophy]]
:S [[Psychology]]
:T [[Education]]
:U [[Geography]]
:V [[History]]
:W [[Political science]]
:X [[Economics]]
:Y [[Sociology]]
:YZ [[Social Work]]
:Z [[Law]]

== Example ==

A common example of the colon classification is:

* "Research in the cure of the tuberculosis of lungs by x-ray conducted in India in 1950s":
* Main classification is Medicine
** (Medicine)
* Within Medicine, the Lungs are the main concern
** (Medicine,Lungs)
* The property of the Lungs is that they are afflicted with Tuberculosis
** (Medicine,Lungs;Tuberculosis)
* The Tuberculosis is being performed (:) on, that is the intent is to cure (Treatment)
** (Medicine,Lungs;Tuberculosis:Treatment)
* The matter that we are treating the Tuberculosis with are X-Rays
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray)
* And this discussion of treatment is regarding the Research phase
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research)
* This Research is performed within a geographical space (.) namely India
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India)
* During the time (') of 1950
** (Medicine,Lungs;Tuberculosis:Treatment;X-ray:Research.India'1950)
* And translating into the codes listed for each subject and facet the classification becomes
** L,45;421:6;253:f.44'N5

==See also==
*[[Bliss bibliographic classification]]
*[[Subject (documents)]]
*[[Universal Decimal Classification]]

== References ==
{{Reflist|2}}
* [http://www.essessreference.com/servlet/esGetBiblio?bno=000374 ''Colon Classification'' (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Ess Publications, Delhi, India
* Chan, Lois Mai. ''Cataloging and Classification: An Introduction''. 2nd ed. New York: McGraw-Hill, c1994. ISBN 0-07-010506-5.

==External links==
* [http://www.iskoi.org/doc/colon.htm More Detail about the Colon Classification at ISKO Italia]

{{Library classification systems}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
<=====doc_Id=====>:717
<=====title=====>:
Faceted classification
<=====text=====>:
{{mergefrom|Faceted search|date=January 2015}}
'''Faceted classification''' is a [[classification scheme]] used in organizing knowledge into a systematic order. A faceted classification uses semantic categories, either general or subject-specific, that are combined to create the full classification entry. Many library classification systems use a combination of a fixed, enumerative taxonomy of concepts with subordinate facets that further refine the topic.

== Definition ==

There are two primary types of classification used for information organization: enumerative and faceted. An enumerative classification contains a full set of entries for all concepts.<ref name=lcsh>{{Citation
 |publisher = Libraries Unlimited
 |isbn = 1591581540
 |publication-place = Westport, Conn
 |title = Library of Congress subject headings
 |url = http://openlibrary.org/books/OL3311856M/Library_of_Congress_subject_headings
 |author = Lois Mai Chan
 |publication-date = 2005
 |id = 1591581540
 }}</ref> A faceted classification system uses a set of semantically cohesive categories that are combined as needed to create an expression of a concept. In this way, the faceted classification is not limited to already defined concepts. While this makes the classification quite flexible, it also makes the resulting expression of topics complex.<ref name=sven>{{Citation
        |publisher = MIT Press
        |isbn = 0262194333
        |publication-place = Cambridge, Mass
        |title = The intellectual foundation of information organization
        |url = http://openlibrary.org/books/OL44967M/The_intellectual_foundation_of_information_organization
        |author = Elaine Svenonius
        |publication-date = 2000
        |id = 0262194333
        }}</ref> To the extent possible, facets represent "clearly defined, mutually exclusive, and collectively exhaustive aspects, properties or characteristics of a class or specific subject".<ref name=taylor>Taylor, A. G. (1992). Introduction to Cataloging and Classification. 8th ed. Englewood, Colorado: Libraries Unlimited.</ref> Some commonly used general-purpose facets are time, place, and form.<ref name=chan />

There are few purely faceted classifications; the best known of these is the [[Colon Classification]] of [[S. R. Ranganathan]], a general knowledge classification for libraries. Some other faceted classifications are specific to special topics, such as the Art and Architecture Thesaurus and the faceted classification of occupational safety and health topics created by D. J. Foskett for the International Labour Organization.<ref name=coyle />

Many library classifications combine the enumerative and faceted classification techniques. The [[Dewey Decimal Classification]], the [[Library of Congress Classification]], and the [[Universal Decimal Classification]] all make use of facets at various points in their enumerated classification schedules. The allowed facets vary based on the subject area of the classification. These facets are recorded as tables that represent recurring types of subdivisions within subject areas. There are general facets that can be used wherever appropriate, such as geographic subdivisions of the topic. Other tables are applied only to specific areas of the schedules. Facets can be combined to create a complex subject statement.<ref name=chan />

Arlene Taylor describes faceted classification using an analogy: “If one thinks of each of the faces of a cut and polished diamond as a facet for the whole diamond, one can picture a classification notation that has small notations  standing for subparts of the whole topic strung together to create a complete classification notation”.<ref>{{cite book|last1=Taylor|first1=Arlene G.|year=2004|title=The organization of information|location=Westport, CT|publisher=Libraries Unlimited}}</ref>

Faceted classifications exhibit many of the same problems as classifications based on a hierarchy. In particular, some concepts could belong in more than one facet, so their placement in the classification may appear to be arbitrary to the classifier. It also tends to result in a complex notation because each facet must be distinguishable as recorded.<ref name=sven />

== Retrieval ==

Search in systems with faceted classification can enable a user to navigate information along multiple paths corresponding to different orderings of the facets. This contrasts with traditional taxonomies in which the hierarchy of categories is fixed and unchanging.<ref name="Star, S.L. 1998">Star, S.L. (1998, Fall). Grounded classification: grounded theory and faceted classification. [Electronic version]. Library Trends. 47.2, 218.</ref>  It is also possible to use facets to filter search results to more quickly find desired results.

==Examples of Faceted Classifications==

=== Colon classification for library materials ===
The [[colon classification]] developed by [[S. R. Ranganathan]] is an example of general faceted classification designed to be applied to all library materials. In the Colon Classification system, a book is assigned a set of values from each independent facet.<ref>Garfield, E. (1984, February). A tribute to S.R. Ranganathan, the father of Indian library science. Essays of an Information Scientist, 7, 37-44.</ref>  This facet formula uses punctuation marks and symbols placed between the facets to connect them. Colon classification was named after its use of the colon as the primary symbol in its notation.<ref>Chan, L.M. (1994). Cataloging and classification.  New York: McGraw-Hill, Inc.</ref><ref>[http://www.essessreference.com/servlet/esGetBiblio?bno=000374 Colon Classification (6th Edition)] by Dr. S.R. Ranganathan, published by Ess Publications, Delhi, India.</ref>

Ranganathan stated that hierarchical classification schemes like the Dewey Decimal Classification (DDC) or the Library of Congress Subject Headings are too limiting and finite to use for modern classification and that many items can pertain information to more than one subject.  He organized his classification scheme into 42 classes.  Each class can be categorized according to particular characteristics, that he called facets.  Ranganathan said that there are five fundamental categories that can be used to demonstrate the facets of a subject: personality, material, energy, space and time.  He called this the PMEST formula:<ref>Ranganathan, S. R (1987). Colon classification, 7th ed. revised and edited by M.A. Gopinath. Bangalore: Sarada Ranganathan Endowment for Library Science, 1987</ref>  
*Personality is the most specific or focal subject.
*Matter is the substance, properties or materials of the subject.
*Energy includes the processes, operations and activities.
*Space relates to the geographic location of the subject.
*Time refers to the dates or seasons of the subject.

=== Universal Decimal Classification ===
Another example of a faceted classification scheme is the [[Universal Decimal Classification]] (UDC), the UDC is considered to be a complex multilingual classification that can be used in all fields of knowledge.<ref>About universal decimal classification and the udc consortium. (2006). Retrieved November 30, 2013, from http://www.udcc.org/about.htm</ref>
The Universal Decimal Classification scheme was created at the end of the nineteenth century by Belgian bibliographers [[Paul Otlet]] and [[Henri la Fontaine]]. The goal of their system was to create an index that would be able to record knowledge even if it is stored in non-conventional ways including materials in notebooks and ephemera. They also wanted their index to organize material systematically instead of alphabetically.<ref>Batty, D. (2003). Universal decimal classification.  Encyclopedia of Library and Information Science.</ref>

The UDC has an overall taxonomy of knowledge that is extended with a number of facets, such as language, form, place and time. Each facet has its own symbol in the notation, such as: "=" for language; "-02" for materials, "[...]" for subordinate concepts.<ref name=chan>{{Cite book     |publisher = The Scarecrow Press, Inc.     |isbn = 978-0-8108-5944-9     |title = Cataloging and classification     |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification     |last = Chan|first=Lois Mai|edition = Third     |publication-date = 2007 |page=321|id = 0810859440}}</ref>

===Faceted Classification for Occupational Safety and Health===

[[Douglas John Foskett|D. J. Foskett]], a member of the [[Classification Research Group]] in London, developed classification of occupational safety and health materials for the library of the [[International Labour Organization]].<ref name=coyle>{{cite journal|last1=Coyle|first1=Karen|title=A Faceted Classification for Occupational Safety and Health|journal=Special Libraries|date=1975|volume=66|issue=5-6|pages=256–9}}</ref><ref name=foskett>{{cite book|last1=Foskett|first1=D. J.|title=Proceedings of the International Conference on Scientific Information|chapter=Construction of a Faceted Classification for a Special Subject|date=1959|publisher=National Science Foundation|isbn=0-309-57421-8|pages=867–888}}</ref> After a study of the literature in the field, he created the classification with the following facets:

*Facet A: Occupational Safety and Health: General
*Facet B: Special Classes of Workers, Industries
*Facet C: Sources of Hazards: Fire, Machinery, etc.
*Facet D: Industrial Accidents and Diseases
*Facet E: Preventive Measures, Protection
*Facet F: Organisation, Administration

Notation was solely alphabetic, with the sub-facets organized hierarchically using extended codes, such as "g Industrial equipment and processes", "ge Machines".<ref name=foskett />

===Art and Architecture Thesaurus (AAT)===

While not strictly a classification system, the [[Art and Architecture Thesaurus|AAT]] uses facets similar to those of Ranganathan's Colon Classification:

*Associated Concepts (e.g., philosophy)
*Physical Attributes
*Styles and Periods
*Agents (People/Organizations)
*Activities (similar to Ranganathan's Energy)
*Materials (similar to Ranganathan's Matter)
*Objects (similar to Ranganathan's Personality)<ref name=denton>{{cite web |url=https://www.miskatonic.org/library/facet-web-howto.html|author=William Denton|title=How to Make a Faceted Classification and Put it on the Web}}</ref>

==Comparison between faceted and single hierarchical classification==
Hierarchical classification refers to the classification of objects using one ''single'' hierarchical taxonomy. Faceted classification may actually employ hierarchy in one or more of its facets, but allows for the use of more than one taxonomy to classify objects.

*Faceted classification systems allow the assignment of multiple classifications to an object, and enable those classifications to be applied by searchers in multiple ways, rather than in a single, predetermined order. Multiple facets may be used as a first step in a search process.<ref name="Categories, Facets—and Browsable Facets?">Sirovich, Jaimie (2011). Categories, Facets—and Browsable Facets?, from http://www.uxmatters.com/mt/archives/2011/08/categories-facetsand-browsable-facets.php</ref> For example, one may ''start'' from language or subject.
*Hierarchical classification systems are developed classes that are subdivided from the most general subjects to the most specific.<ref>Reitz, Joan M. (2004). Dictionary for library and information science. Westport, CT: Libraries Unlimited</ref>
*Faceted classification systems allow for the combination of facets to [[Filter (software)|filter]] the set of objects rapidly. In addition, the facets can be used to address multiple classification criteria.<ref>Godert, Winfried. F. (1991). Facet classification in online retrieval. International Classification, 18, 98-109</ref>
*A faceted system focuses on the important, essential or persistent characteristics of content objects, helping it to be useful for categorization of fine-grained rapidly changing repositories.
*In faceted classification systems one does not have to know the name of the category into which an object is placed a priori. A controlled vocabulary is presented with the number of documents matching each vocabulary term.
*New facets may be created at any time without disruption of a single hierarchy or reorganizing other facets.
*Faceted classification systems make few assumptions about the scope and organization of the domain. It is difficult to ''break'' a faceted classification schema.<ref>Adkisson, Hiedi P. (2005).  Use of faceted classification.  Retrieved December 1, 2013, from http://www.webdesignpractices.com/navigation/facets.html</ref>

==See also==
* [[Classification Research Group]]
* [[Controlled vocabulary]]
* [[Findability]]
* [[Folksonomy]]
* [[Information architecture]]
* [[Tag (metadata)]]
* [[Universal Decimal Classification]]

==References==
{{Reflist|colwidth=35em}}

==External links==
* [http://eprints.soton.ac.uk/271488/ How to ''Reuse'' a Faceted Classification and Put It On the ''Semantic'' Web]

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
<=====doc_Id=====>:720
<=====title=====>:
Ontic
<=====text=====>:
{{The Works of Aristotle}}
In [[philosophy]], '''ontic''' (from the [[Greek language|Greek]] {{lang|grc|ὄν}}, genitive {{lang|grc|ὄντος}}: "of that which is") is physical, real, or factual existence.

"Ontic" describes what is there, as opposed to the nature or properties of that being. To illustrate:

*[[Roger Bacon]], observing that all languages are built upon a common grammar, stated that they share a foundation of ontically anchored linguistic structures.
*[[Martin Heidegger]] posited the concept of ''Sorge'', or caring, as the fundamental concept of the [[intentionality|intentional being]], and presupposed an ontological significance that distinguishes [[ontology|ontological]] being from mere "thinghood" of an ontic being. He uses the [[German language|German]] word "[[Dasein]]" for a being that is capable of ontology, that is, [[recursivity|recursively]] comprehending [[property (philosophy)|properties]] of the very fact of its own Being. For Heidegger, "ontical" signifies concrete, specific realities, whereas "ontological" signifies deeper underlying structures of reality. Ontological objects or subjects have an ontical dimension, but they also include aspects of being like self-awareness, evolutionary vestiges, future potentialities, and networks of relationship.<ref>{{cite web|title=Ontico-Ontological Distinction|url=http://www.blackwellreference.com/public/tocnode?id=g9781405106795_chunk_g978140510679516_ss1-33|publisher=Blackwell Reference|accessdate=26 February 2015}}</ref><ref>{{cite web|last1=Duffy|first1=Michael|title=The Ontological and the Ontic|url=http://mikejohnduff.blogspot.com/2007/08/ontological.html|accessdate=26 February 2015}}</ref>
* [[Nicolai Hartmann]] distinguishes among ontology, ontics, and metaphysics: (i) ontology concerns the categorical analysis of entities by means of the knowledge categories able to classify them, (ii) ontics refers to a pre-categorical and pre-objectual connection which is best expressed in the relation to transcendent acts, and (iii) metaphysics is that part of ontics or that part of ontology which concerns the residue of being that cannot be rationalized further according to categories.

== Usage in philosophy of science ==
[[Harald Atmanspacher]] writes extensively about the philosophy of science, especially as it relates to [[Chaos theory]], [[determinism]], [[Causality|causation]], and [[stochastic process|stochasticity]]. He explains that "''ontic'' states describe all properties of a physical system exhaustively. ('Exhaustive' in this context means that an ''ontic'' state is 'precisely the way it is,' without any reference to [[epistemic]] knowledge or ignorance.)"{{ref|autonumber}}

In an earlier paper, Atmanspacher portrays the difference between an [[epistemic]] perspective of a [[system]], and an ontic perspective:

:Philosophical [[discourse]] traditionally distinguishes between [[ontology]] and [[epistemology]] and generally enforces this distinction by keeping the two subject areas separated. However, the relationship between the two areas is of central importance to [[physics]] and [[philosophy of physics]]. For instance, many [[measurement]]-related problems force us to consider both our [[knowledge]] of the [[Classical mechanics|states]] and [[observables]] of a [[system]] ([[epistemic perspective]]) and its states and observables, independent of such knowledge (ontic perspective). This applies to [[quantum|quantum systems]] in particular.{{ref|autonumber}}

== Usage in philosophy of critical realism ==
The [[United Kingdom|British]] [[philosopher]] [[Roy Bhaskar]], who is closely associated with the philosophical [[Cultural movement|movement]] of [[Critical realism (philosophy of the social sciences)|Critical Realism]] writes:
:"I differentiate the 'ontic' ('ontical' etc.) from the 'ontological'. I employ the former to refer to

:# whatever pertains to being generally, rather than some distinctively philosophical (or scientific) theory of it (ontology), so that in this sense, that of the '''ontic<sub>1</sub>''', we can speak of the ontic presuppositions of a work of art, a [[joke]] or a strike as much as a [[epistemology|theory of knowledge]]; and, within this [[rubric]], to
:# the [[intransitive]] [[object (philosophy)|object]]s of some specific, [[historically determinate]], [[scientific investigation]] (or set of such investigations), the '''ontic<sub>2</sub>'''.

:"The ontic<sub>2</sub> is always specified, and only identified, by its relation, as the intransitive object(s) of some or other (denumerable set of) particular transitive process(es) of enquiry. It is cognitive process-, and level-specific; whereas the ontological (like the ontic<sub>1</sub>) is not."{{ref|autonumber}}

[[Ruth Groff]] offers this expansion of Bhaskar's note above:
:"'ontic<sub>2</sub>' is an abstract way of denoting the [[object-domain]] of a particular [[scientific]] area, field, or inquiry. E.g.: [[molecules]] feature in the ontic<sub>2</sub> of chemistry. He's just saying that the scientific undertaking ITSELF is not one of the objects of said, most narrowly construed, immediate object-domain. So [[chemistry]] itself is not part of the ontic<sub>2</sub> of chemistry."

==See also==
*[[Ding an sich#Noumenon and the thing-in-itself|Ding an sich]]
*[[Ontologism]]
*[[Physical ontology]]
*[[Substance theory]]

==References==
{{Reflist}}
# {{Note|autonumber}} Atmanspacher, Dr. H., and Primas, H., 2003 [2005], "Epistemic and Ontic [[Quantum]] [[Reality|Realities]]", in Khrennikov, A (Ed.), ''Foundations of Probability and Physics'' ([[American Institute of Physics]] 2005, pp 49&ndash;61, Originally published in ''Time, Quantum and Information'', edited by Lutz Castell and Otfried Ischebeck, Springer, Berlin, 2003, pp 301&ndash;321
# {{Note|autonumber}} Atmanspacher, Harald (2001) ''[[Determinism]] Is Ontic, Determinability is [[Epistemic]]'' ([http://philsci-archive.pitt.edu/archive/00000939/00/determ.pdf [[University of Pittsburgh]] Archives])
# {{Note|autonumber}} Bhaskar, R.A., 1986, ''Scientific Realism and Human Emancipation'' (London: Verso), pp 36 and 37, as quoted by [[Howard Engelskirchen]] in the [http://archives.econ.utah.edu/archives/bhaskar/2001m11/msg00015.htm Bhaskar mailing list archive]
{{Continental philosophy}}
{{wiktionary}}

[[Category:Concepts in metaphysics]]
[[Category:Knowledge representation]]
[[Category:Martin Heidegger]]
[[Category:Modal logic]]
[[Category:Ontology]]
[[Category:Philosophy of science]]
[[Category:Reality]]
<=====doc_Id=====>:723
<=====title=====>:
CDS ISIS
<=====text=====>:
{{Use dmy dates|date=May 2014}}
'''CDS/ISIS''' is a [[software]] package for generalised ''Information Storage and Retrieval systems'' developed, maintained and disseminated by [[UNESCO]]. It was first released in 1985 and since then over 20,000 [[license|licences]] have been issued by UNESCO and a worldwide network of distributors. It is particularly suited to bibliographical applications and is used for the [[library catalog|catalogues]] of many small and medium-sized [[library|libraries]]. Versions have been produced in Arabic, Chinese, English, French, German, Portuguese, Russian and Spanish amongst other languages. UNESCO makes the software available free for non-commercial purposes, though distributors are allowed to charge for their expenses.

CDS/ISIS is an acronym which stands for Computerised Documentation Service / Integrated Set of Information Systems. In 2003 it was stated that "This package is accepted by libraries in the developing countries as a standard software for information system development".<ref>National Science Foundation of Sri Lanka. "CDS ISIS Library Software" [Last Update 10 January 2003.] http://www.nsf.ac.lk/slstic/isis.htm  Accessed 20 June 2007.</ref>

The original CDS/ISIS ran on an [[IBM]] [[mainframe computer|mainframe]] and was designed in the mid-1970s under Mr Giampaolo Del Bigio for UNESCO's Computerized Documentation System (CDS). It was based on the internal ISIS (Integrated Set of Information Systems) at the [[International Labour Organization]] in Geneva.

In 1985 a version was produced for mini- and microcomputers programmed in Pascal. It ran on an [[IBM PC]] under [[MS-DOS]]<ref>Buxton, Andrew and Hopkinson, Alan. ''The CDS/ISIS handbook''. London: Library Association, 1994</ref>
. ''Winisis'', the [[Microsoft Windows|Windows]] version,  first demonstrated in 1995, may run on a single [[computer]] or in a [[local area network]]. A ''JavaISIS'' client/server component was designed in 2000, allowing remote [[database management system|database management]] over the [[Internet]] from [[Microsoft Windows|Windows]], [[Linux]] and [[Apple Macintosh|Macintosh]] computers. Furthermore, ''GenISIS'' allows the user to produce [[HTML]] Web forms for CDS/ISIS database searching. The ''ISIS_DLL'' provides an [[API]] for developing CDS/ISIS based applications. The [[OpenIsis]] library, developed independently from 2002 to 2004, provided another [[API]] for developing CDS/ISIS-like applications.

The most recent effort towards a completely renewed [[Free and open-source software|FOSS]], [[Unicode]] implementation of CDS/ISIS is the J-Isis project, developed by UNESCO since 2005 and currently maintained by Mr Jean Claude Dauphin.

== See also ==
* [[IDIS (software)|IDIS]] is a tool for direct data exchange between CDS/ISIS and IDAMS.

== External links ==
* [http://kenai.com/projects/j-isis J-ISIS New UNESCO Java CDS/ISIS Software]
* [http://portal.unesco.org/ci/en/ev.php-URL_ID=2071&URL_DO=DO_TOPIC&URL_SECTION=201.html CDS/ISIS database software (UNESCO)]
* [http://lists.iccisis.org International list hosted from 2010 by the ICCIsis (International Coordination Committee on ISIS)]
* [https://listserv.surfnet.nl/archives/cds-isis.html Archives of CDS-ISIS@NIC.SURFNET.NL (discontinued in 2010)]
* http://openisis.org/ (discontinued)
* http://sourceforge.net/projects/isis (discontinued)
* [http://pecl.php.net/package/isis PHP extension for reading CDS/ISIS databases]

== References ==
{{Reflist}}

[[Category:Knowledge representation]]
[[Category:Proprietary database management systems]]
<=====doc_Id=====>:726
<=====title=====>:
Philosophy of information
<=====text=====>:
{{Information science}}
The '''philosophy of information''' ('''PI''') is the area of research that studies conceptual issues arising at the intersection of [[computer science]], [[information science]], [[information technology]], and [[philosophy]].

It includes:

# the critical investigation of the conceptual nature and basic principles of [[information]], including its dynamics, utilisation and sciences
# the elaboration and application of information-theoretic and computational methodologies to philosophical problems.<ref>Luciano Floridi, [http://www.blackwellpublishing.com/pci/downloads/introduction.pdf "What is the Philosophy of Information?"], ''Metaphilosophy'', 2002, (33), 1/2.</ref>

==History==
The philosophy of information (PI) has evolved from the [[philosophy of artificial intelligence]], [[logic of information]], [[cybernetics]], [[social theory]], [[ethics]] and the study of language and information.

===Logic of information===
The [[logic of information]], also known as the ''logical theory of information'', considers the information content of logical [[sign (semiotics)|sign]]s and expressions along the lines initially developed by [[Charles Sanders Peirce]].

===Cybernetics===
One source for the philosophy of information can be found in the technical work of [[Norbert Wiener]], [[Alan Turing]] (though his work has a wholly different origin and theoretical framework), [[William Ross Ashby]], [[Claude Shannon]], [[Warren Weaver]], and many other scientists working on computing and information theory back in the early 1950s. See the main article on [[Cybernetics]].

Some important work on information and communication was done by [[Gregory Bateson]] and his colleagues.

===Study of language and information===
Later contributions to the field were made by [[Fred Dretske]], [[Jon Barwise]], [[Brian Cantwell Smith]], and others.

The [[Center for the Study of Language and Information|Center for the Study of Language and Information (CSLI)]] was founded at Stanford University in 1983 by philosophers, computer scientists, linguists, and psychologists, under the direction of [[John Perry (philosopher)|John Perry]] and [[Jon Barwise]].

===P.I.===
More recently this field has become known as the philosophy of information. The expression was coined in the 1990s by [[Luciano Floridi]], who has published prolifically in this area with the intention of elaborating a unified and coherent, conceptual frame for the whole subject.{{citation needed|date=April 2015}}

==Definitions of "information"==

The concept ''information'' has been defined by several theorists.

===Peirce===
[[Charles S. Peirce]]'s theory of information was embedded in his wider theory of symbolic communication he called the ''semeiotic'', now a major part of [[semiotics]]. For Peirce, information integrates the aspects of [[sign]]s and [[Expression (mathematics)|expressions]] separately covered by the concepts of [[denotation]] and [[extension (semantics)|extension]], on the one hand, and by [[connotation]] and [[comprehension (logic)|comprehension]] on the other.

=== Shannon and Weaver ===
Claude E. Shannon, for his part, was very cautious: "The word 'information' has been given different meanings by various writers in the general field of information theory. It is likely that at least a number of these will prove sufficiently useful in certain applications to deserve further study and permanent recognition. It is hardly to be expected that a single concept of information would satisfactorily account for the numerous possible applications of this general field." (Shannon 1993, p.&nbsp;180){{full citation needed|date=April 2015}}. Thus, following Shannon, Weaver supported a tripartite analysis of information in terms of (1) technical problems concerning the quantification of information and dealt with by Shannon's theory; (2) semantic problems relating to meaning and truth; and (3) what he called "influential" problems concerning the impact and effectiveness of information on human behaviour, which he thought had to play an equally important role. And these are only two early examples of the problems raised by any analysis of information.

A map of the main senses in which one may speak of information is provided by  [http://plato.stanford.edu/entries/information-semantic/ the Stanford Encyclopedia of Philosophy article]. The previous paragraphs are based on it.

===Bateson===
[[Gregory Bateson]] defined information as "a difference that makes a difference".<ref>[http://plato.acadiau.ca/courses/educ/reid/papers/PME25-WS4/SEM.html Extract from "Steps to an Ecology of Mind"]</ref> which is based on [[Donald M. MacKay]]: information is a distinction that makes a difference.<ref>The Philosophy of Information.
Luciano Floridi. Chapter 4. Oxford University Press, USA (March 8, 2011) ASIN: 0199232385 [http://www.amazon.com/Philosophy-Information-Luciano-Floridi/dp/0199232385]</ref>

===Floridi===
According to Luciano Floridi{{citation needed|date=April 2015}}, four kinds of mutually compatible phenomena are commonly referred to as "information": 
*  Information about something (e.g. a train timetable)
*  Information as something (e.g. DNA, or fingerprints)
*  Information for something (e.g. algorithms or instructions)
*  Information in something (e.g. a pattern or a constraint).

The word "information" is commonly used so metaphorically or so abstractly that the meaning is unclear.

==Philosophical directions==

===Computing and philosophy===
Recent creative advances and efforts in [[computing]], such as [[semantic web]], [[ontology engineering]], [[knowledge engineering]], and modern [[artificial intelligence]] provide [[philosophy]] with fertile notions, new and evolving subject matters, methodologies, and models for philosophical inquiry.  While [[computer science]] brings new opportunities and challenges to traditional philosophical studies, and changes the ways philosophers understand foundational concepts in philosophy, further major progress in [[computer science]] would only be feasible when philosophy provides sound foundations for areas such as bioinformatics, software engineering, knowledge engineering, and ontologies.

Classical topics in philosophy, namely, [[mind]], [[consciousness]], [[experience]], [[reasoning]], [[knowledge]], [[truth]], [[morality]] and [[creativity]] are rapidly becoming common concerns and foci of investigation in [[computer science]], e.g., in areas such as agent computing, [[software agents]], and intelligent mobile agent technologies.{{citation needed|date=December 2012}}

According to Luciano Floridi "<ref>Luciano Floridi, [http://www.philosophyofinformation.net/publications/pdf/oppi.pdf ''Open Problems in the Philosophy of Information''] ''Metaphilosophy'' 35.4, 554-582. Revised version of ''The Herbert A. Simon Lecture on Computing and Philosophy'' given at Carnegie Mellon University in 2001, with [http://ethics.sandiego.edu/video/CAP/CMU2001/Floridi/index.html RealVideo]</ref> one can think of several ways for applying computational methods towards philosophical matters:
# Conceptual experiments in silico: As an innovative extension of an ancient tradition of [[thought experiment]], a trend has begun in philosophy to apply computational [[Computer model|modeling]] schemes to questions in [[logic]], [[epistemology]], [[philosophy of science]], [[philosophy of biology]], [[philosophy of mind]], and so on.
# [[Digital physics#Pancomputationalism or the computational universe theory|Pancomputationalism]]: By this view, computational and informational concepts are considered to be so powerful that given the right level of [[abstraction]], anything in the world could be modeled and represented as a computational system, and any process could be simulated computationally. Then, however, pancomputationalists have the hard task of providing credible answers to the following two questions:
## how can one avoid blurring all differences among systems?
## what would it mean for the system under investigation not to be an [[Information system|informational system]] (or a computational system, if computation is the same as information processing)?

===Information and society===
Numerous philosophers and other thinkers have carried out philosophical studies of the social and cultural aspects of electronically mediated information.

* [[Albert Borgmann]], ''Holding onto Reality: The Nature of Information at the Turn of the Millennium'' (Chicago University Press, 1999)
* [[Mark Poster]], ''The Mode of Information'' (Chicago Press, 1990)
* [[Luciano Floridi]], "The Informational Nature of Reality", ''Fourth International European Conference on Computing and Philosophy'' 2006 (Dragvoll Campus, NTNU Norwegian University for Science and Technology, Trondheim, Norway, 22–24 June 2006).

==See also==
{{col-begin}}
{{col-break}}
* [[Barwise prize]]
* [[Complex system]]
* [[Digital divide]]
* [[Digital philosophy]]
* [[Digital physics]]
* [[Game theory]]
* [[Freedom of information]]
* [[Informatics (academic field)|Informatics]]
{{col-break}}
* [[Information]]
* [[Information art]]
* [[Information ethics]]
* [[Information theory]]
* [[International Association for Computing and Philosophy]]
* [[Logic of information]]
{{col-break}}
* [[Philosophy of artificial intelligence]]
* [[Philosophy of computer science]]
* [[Philosophy of technology]]
* [[Philosophy of thermal and statistical physics]]
* [[Physical information]]
* [[Relational quantum mechanics]]
* [[Social informatics]]
* [[Statistical mechanics]]
{{col-end}}

==Notes==
{{reflist}}

==Further reading==
*[[Luciano Floridi]], "[http://www.blackwellpublishing.com/pci/downloads/introduction.pdf What is the Philosophy of Information?]" ''Metaphilosophy'', 33.1/2: 123-145. Reprinted in T.W. Bynum and J.H. Moor (eds.), 2003. ''CyberPhilosophy: The Intersection of Philosophy and Computing''. Oxford – New York: Blackwell.
*-------- (ed.), 2004. ''[http://www.blackwellpublishing.com/pci/default.htm The Blackwell Guide to the Philosophy of Computing and Information.]'' Oxford - New York: Blackwell.
*Greco, G.M., Paronitti G., Turilli M., and Floridi L., 2005. ''[http://www.wolfson.ox.ac.uk/~floridi/pdf/htdpi.pdf How to Do Philosophy Informationally.]'' ''Lecture Notes on Artificial Intelligence'' 3782, pp.&nbsp;623–634.

== External links ==
{{Library resources box}}
*{{cite SEP |url-id=information |title=Information |last=Adriaans |first=Peter |editor-last=Zalta |editor-first=Edward N. ||date=Autumn  2013}}
*{{cite SEP |url-id=information-semantic |title=Semantic Conceptions of Information |last=Floridi |first=Luciano |editor-last=Zalta |editor-first=Edward N. |date=Spring 2015}}
*[http://web.comlab.ox.ac.uk/oucl/research/areas/ieg/ IEG site], the Oxford University research group on the philosophy of information.
*[[Luciano Floridi]], "[https://web.archive.org/web/20060820223325/http://academicfeeds.friwebteknologi.org/index.php?id=28 Where are we in the philosophy of information?]" [[University of Bergen]], [[Norway]]. Podcast dated 21.06.06.

{{Navboxes
|list=
{{Philosophy topics}}
{{philosophy of language}}
{{philosophy of mind}}
{{philosophy of science}}
}}

[[Category:Philosophy by topic|Inf]]
[[Category:Philosophy of artificial intelligence]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:729
<=====title=====>:
Frame language
<=====text=====>:
{{Duplication|dupe=Frame (artificial intelligence)}}

A '''frame language''' is a technology used for [[knowledge representation]] in [[artificial intelligence]]. Frames are stored as [[Ontology (information science)|ontologies]] of [[Set theory|sets]] and subsets of the [[Frame_(artificial_intelligence)|frame concepts]]. They are similar to class hierarchies in [[object-oriented languages]] although their fundamental design goals are different. Frames are focused on explicit and intuitive representation of knowledge whereas objects focus on [[Encapsulation (object-oriented programming)|encapsulation]] and [[information hiding]]. Frames originated in AI research and objects primarily in [[software engineering]]. However, in practice the techniques and capabilities of frame and object-oriented languages overlap significantly.

==Description==
Early work on Frames was inspired by psychological research going back to the 1930s that indicated people use stored stereotypical knowledge to interpret and act in new cognitive situations.<ref>{{cite book|last=Bartlett|first=F.C.|title=Remembering: A Study in Experimental and Social Psychology|year=1932|publisher=Cambridge University Press|location=Cambridge, England}}</ref>  The term Frame was first used by [[Marvin Minsky]] as a paradigm to understand visual reasoning and natural language processing.<ref>{{cite book|last=Minsky|first=Marvin|title=The Psychology of Computer Vision|year=1975|publisher=McGraw Hill|location=New York|pages=211–277|editor=Pat Winston|chapter=A Framework for Representing Knowledge}}</ref> In these and many other types of problems the potential solution space for even the smallest problem is huge. For example, extracting the phonemes from a raw audio stream or detecting the edges of an object. Things which seem trivial to humans are actually quite complex. In fact, how difficult they really were was probably not fully understood until AI researchers began to investigate the complexity of getting computers to solve them.

The initial notion of Frames or Scripts as they were also called is that they would establish the context for a problem and in so doing automatically reduce the possible search space significantly. The idea was also adopted by Schank and Abelson who used it to illustrate how an AI system could process common human interactions such as ordering a meal at a restaurant.<ref>{{cite book|last=Schank|first=Roger|title=Scripts, Plans, Goals, and Understanding|year=1977|publisher=Lawrence Erlbaum|location=Hillsdale, New Jersey|author2=R. P. Abelson}}</ref>  These interactions were standardized as Frames with slots that stored relevant information about each Frame. Slots are analogous to object properties in object-oriented modeling and to relations in entity-relation models. Slots often had default values but also required further refinement as part of the execution of each instance of the scenario. I.e., the execution of a task such as ordering at a restaurant was controlled by starting with a basic instance of the Frame and then instantiating and refining various values as appropriate. Essentially the abstract Frame represented an object class and the frame instances an object instance. In this early work the emphasis was primarily on the static data descriptions of the Frame. Various mechanisms were developed to define the range of a slot, default values, etc. However, even in these early systems there were procedural capabilities. One common technique was to use "triggers" (similar to the database concept of triggers) attached to slots. A trigger was simply procedural code that was attached to a slot. The trigger could fire either before and/or after a slot value was accessed or modified.

As with object classes, Frames were organized in [[Subsumption relation|subsumption]] hierarchies. For example, a basic frame might be ordering at a restaurant. An instance of that would be Joe goes to McDonalds. A specialization (essentially a [[Subclass (computer science)|subclass]]) of the restaurant frame would be a frame for ordering at a fancy restaurant. The fancy restaurant frame would inherit all the default values from the restaurant frame but also would either add more slots or change one or more of the default values (e.g., expected price range) for the specialized frame.<ref>{{cite book|last=Feigenbaum|first=Edward|title=The Handbook of Artificial Intelligence, Volume III|publisher=Addison-Wesley|isbn=0201118114|pages=216–222|url=https://archive.org/stream/handbookofartific01barr#page/156/mode/2up|author2=Avron Barr|date=September 1, 1986}}</ref><ref>{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3–46|doi=10.1207/s15516709cog0101_2}}</ref>

Much of the early Frame language research (e.g. Schank and Abelson) had been driven by findings from experimental psychology and attempts to design knowledge representation tools that corresponded to the patterns humans were thought to use to function in daily tasks. These researchers were less interested in mathematical formality since they believed such formalisms were not necessarily good models for the way the average human conceptualizes the world. The way humans use language for example is often far from truly logical.

Similarly, in linguistics, [[Charles J. Fillmore]] in the mid-1970s started working on his theory of [[Frame semantics (linguistics)|frame semantics]], which later would lead to computational resources like [[FrameNet]].<ref>{{cite news|last=Lakoff|first=George|title=Charles Fillmore, Discoverer of Frame Semantics, Dies in SF at 84: He Figured Out How Framing Works|url=http://www.huffingtonpost.com/george-lakoff/charles-fillmore-discover_b_4807590.html|accessdate=7 March 2014|newspaper=The Huffington Post|date=18 February 2014}}</ref> Frame semantics was motivated by reflections on human language and human cognition.

Researchers such as [[Ron Brachman]] on the other hand wanted to give AI researchers the mathematical formalism and computational power that were associated with Logic. Their aim was to map the Frame classes, slots, constraints, and rules in a Frame language to set theory and logic.  One of the benefits of this approach is that the validation and even creation of the models could be automated using theorem provers and other automated reasoning capabilities. The drawback was that it could be more difficult to initially specify the model in a language with a formal semantics.

This evolution also illustrates a classic divide in AI research known as the "[[neats vs. scruffies]]". The "neats" were researchers who placed the most value on mathematical precision and formalism which could be achieved via [[First Order Logic]] and [[Set Theory]]. The "scruffies" were more interested in modeling knowledge in representations that were intuitive and psychologically meaningful to humans.<ref>{{cite book|last=Crevier|first=Daniel|title=AI: The Tumultuous Search for Artificial Intelligence|year=1993|publisher=Basic Books|location=New York|isbn=0-465-02997-3|page=168}}</ref>

The most notable of the more formal approaches was the [[KL-ONE]] language.<ref>{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}</ref> KL-ONE later went on to spawn several subsequent Frame languages. The formal semantics of languages such as KL-ONE gave these frame languages a new type of automated reasoning capability known as the [[Deductive classifier|classifier]]. The classifier is an engine that analyzes the various declarations in the frame language: the definition of sets, subsets, relations, etc. The classifier can then automatically deduce various additional relations and can detect when some parts of a model are inconsistent with each other. In this way many of the tasks that would normally be executed by forward or backward chaining in an inference engine can instead be performed by the classifier.<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref>

This technology is especially valuable in dealing with the Internet. It is an interesting result that the formalism of languages such as KL-ONE can be most useful dealing with the highly informal and unstructured data found on the Internet. On the Internet it is simply not feasible to require all systems to standardize on one data model. It is inevitable that terminology will be used in multiple inconsistent forms. The automatic classification capability of the classifier engine provides AI developers with a powerful toolbox to help bring order and consistency to a very inconsistent collection of data (i.e., the Internet). The vision for an enhanced Internet, where pages are ordered not just by text keywords but by classification of concepts is known as the [[Semantic Web]]. Classification technology originally developed for Frame languages is a key enabler of the Semantic Web.<ref>{{cite journal|last=Berners-Lee |first=Tim |author2=James Hendler |author3=Ora Lassila |title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities |journal=Scientific American |date=May 17, 2001 |url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html |doi=10.1038/scientificamerican0501-34 |volume=284 |pages=34–43 |deadurl=yes |archiveurl=https://web.archive.org/web/20130424071228/http://www.cs.umd.edu/%7Egolbeck/LBSC690/SemanticWeb.html |archivedate=2013-04-24 |df= }}</ref><ref>{{cite web|last=Horridge|first=Mathew|title=Protégé OWL Tutorial A step-by-step guide to modelling in OWL using the popular Protégé OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}</ref> The "neats vs. scruffies" divide also emerged in Semantic Web research, culminating in the creation of the [[Linking Open Data]] community—their focus was on exposing data on the Web rather than modeling.

==Example==
A simple example of concepts modeled in a frame language is the [[FOAF (ontology)|Friend of A Friend (FOAF) ontology]] defined as part of the Semantic Web as a foundation for social networking and calendar systems. The primary frame in this simple example is a ''Person''. Example slots are the person's ''email'', ''home page, phone,'' etc. The interests of each person can be represented by additional frames describing the space of business and entertainment domains. The slot ''knows'' links each person with other persons. Default values for a person's interests can be inferred by the web of people they are friends of.<ref>{{cite web|title=FOAF|url=http://semanticweb.org/wiki/FOAF|website=http://semanticweb.org|accessdate=7 June 2014}}</ref>

==Implementations==
The earliest Frame based languages were custom developed for specific research projects and were not packaged as tools to be re-used by other researchers. Just as with [[expert system]] [[inference engine]]s, researchers soon realized the benefits of extracting part of the core infrastructure and developing general purpose frame languages that were not coupled to specific applications. One of the first general purpose frame languages was KRL.<ref>{{cite journal|last=Bobrow|first=D.G.|author2=Terry Winograd|title=An Overview of KRL: A Knowledge Representation Language|journal=Cognitive Science|year=1977|volume=1|pages=3–46|doi=10.1207/s15516709cog0101_2}}</ref> One of the most influential early Frame languages was [[KL-ONE]]<ref>{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}</ref> KL-ONE spawned several subsequent Frame languages. One of the most widely used successors to KL-ONE was the [[LOOM (ontology)|Loom language]] developed by Robert MacGregor at the [[Information Sciences Institute]].<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref>

In the 1980s Artificial Intelligence generated a great deal of interest in the business world fueled by expert systems. This led to the development of many commercial products for the development of knowledge-based systems. These early products were usually developed in Lisp and integrated constructs such as IF-THEN rules for logical reasoning with Frame hierarchies for representing data. One of the most well known of these early Lisp knowledge-base tools was the [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE provided a full Frame language with multiple inheritance, slots, triggers, default values, and a rule engine that supported backward and forward chaining. As with most early commercial versions of AI software KEE was originally deployed in [[Lisp (programming language)|Lisp]] on [[Lisp machine]] platforms but was eventually ported to PCs and Unix workstations.<ref>{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}</ref>

The research agenda of the [[Semantic Web]] spawned a renewed interest in automatic classification and frame languages.  An example is the [[Web Ontology Language]] (OWL) standard for describing information on the Internet. OWL is a standard to provide a semantic layer on top of the Internet. The goal is that rather than organizing the web using keywords as most applications (e.g. Google) do today the web can be organized by concepts organized in an ontology.

The name of the OWL language itself provides a good example of the value of a Semantic Web. If one were to search for "OWL" using the Internet today most of the pages retrieved would be on the bird [[Owl]] rather than the standard [[Web Ontology Language|OWL]]. With a Semantic Web it would be possible to specify the concept "Web Ontology Language" and the user would not need to worry about the various possible acronyms or synonyms as part of the search. Likewise the user would not need to worry about homonyms crowding the search results with irrelevant data such as information about birds of prey as in this simple example.

In addition to OWL various standards and technologies that are relevant to the Semantic Web and were influenced by Frame languages include [[Ontology Inference Layer|OIL]] and [[DARPA Agent Markup Language|DAML]].  The [[Protégé (software)|Protege]] Open Source software tool from Stanford University provides an ontology editing capability that is built on OWL and has the full capabilities of a classifier.<ref>{{cite web|last=Horridge|first=Mathew|title=Protégé OWL Tutorial A step-by-step guide to modelling in OWL using the popular Protégé OWL tools.|url=http://130.88.198.11/tutorials/protegeowltutorial/|work=Manchester University|publisher=Manchester University|accessdate=9 December 2013}}</ref>

==Comparison of frames and objects==
Frame languages have a significant overlap with [[object-oriented]] languages. The terminologies and goals of the two communities were different but as they moved from the academic world and labs to the commercial world developers tended to not care about philosophical issues and focused primarily on specific capabilities, taking the best from either camp regardless of where the idea began. What both paradigms have in common is a desire to reduce the distance between concepts in the real world and their implementation in software. As such both paradigms arrived at the idea of representing the primary software objects in taxonomies starting with very general types and progressing to more specific types.

The following table illustrates the correlation between standard terminology from the object-oriented and frame language communities:

{| class="wikitable"
|-
! Frame Terminology !! OO Terminology
|-
| Frame || Object Class
|-
| Slot || Object property or attribute
|-
| Trigger || Accessor and Mutator methods
|-
| Method (e.g. Loom, KEE) || Method
|}

The primary difference between the two paradigms was in the degree that encapsulation was considered a major requirement. For the object-oriented paradigm encapsulation was one of the if not the most critical requirement. The desire to reduce the potential interactions between software components and hence manage large complex systems was a key driver of object-oriented technology. For the frame language camp this requirement was less critical than the desire to provide a vast array of possible tools to represent rules, constraints, and programming logic. In the object-oriented world everything is controlled by methods and the visibility of methods. So for example, accessing the data value of an object property must be done via an accessor method. This method controls things such as validating the data type and constraints on the value being retrieved or set on the property. In Frame languages these same types of constraints could be handled in multiple ways. Triggers could be defined to fire before or after a value was set or retrieved. Rules could be defined that managed the same types of constraints. The slots themselves could be augmented with additional information (called "facets" in some languages) again with the same type of constraint information.

The other main differeniator between frame and OO languages was multiple inheritance (allowing a frame or class to have two or more superclasses). For frame languages multiple inheritance was a requirement.  This follows from the desire to model the world the way humans do, human conceptualizations of the world seldom fall into rigidly defined non-overlapping taxonomies. For many OO languages, especially in the later years of OO, single inheritance was either strongly desired or required. Multiple inheritance was seen as a possible step in the analysis phase to model a domain but something that should be eliminated in the design and implementation phases in the name of maintaining encapsulation and modularity.<ref>{{cite web|title=The Unified Modeling Language|url=http://www.essentialstrategies.com/publications/modeling/uml.htm|work=essentialstrategies.com|publisher=Essential Strategies Inc.|accessdate=10 December 2013|year=1999|quote=In your author’s experience, nearly all examples that appear to require multiple inheritance or multiple type hierarchies can be solved by attacking the model from a different direction.}}</ref>

Although the early frame languages such as KRL did not include message passing, driven by the demands of developers, most of the later frame languages (e.g. Loom, KEE) included the ability to define messages on Frames.<ref>{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}</ref>

On the object-oriented side, standards have also emerged that provide essentially the equivalent functionality that frame languages provided, albeit in a different format and all standardized on object libraries. For example, the [[Object Management Group]] has standardized specifications for capabilities such as associating test data and constraints with objects (analogous to common uses for facets in Frames and to constraints in Frame languages such as Loom) and for integrating rule engines.<ref>{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}</ref><ref>{{cite web|title=OMG Formal Specifications|url=http://www.omg.org/spec/|work=omg.org|publisher=Object Management Group|accessdate=10 December 2013}}</ref>

==See also==
*[[Description logic]]
* [[Deductive classifier]]
*[[First-order logic]]
*[[Knowledge base]]
*[[Knowledge-based system]]
*[[Ontology language]]
*[[Semantic Networks]]

==References==
{{reflist}}

==Additional References==
* Marvin Minsky, [http://web.media.mit.edu/~minsky/papers/Frames/frames.html A Framework for Representing Knowledge], MIT-AI Laboratory Memo 306, June, 1974.
* Daniel G. Bobrow, Terry Winograd, [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/76/581/CS-TR-76-581.pdf An Overview of KRL, A Knowledge Representation Language],  Stanford Artificial Intelligence Laboratory Memo AIM 293, 1976.
* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-408.pdf The FRL Primer], 1977
* R. Bruce Roberts and Ira P. Goldstein, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-409.pdf The FRL Manual], 1977
* {{cite journal | last1 = Brachman | first1 = R. | last2 = Schmolze | first2 = J. | year = 1985 | title = An overview of the KL-ONE Knowledge Representation System | url = | journal = Cognitive science | volume = 9 | issue = | pages = 171–216 | doi=10.1016/s0364-0213(85)80014-8}}
* {{cite journal | last1 = Fikes | first1 = R. E. | last2 = Kehler | first2 = T. | year = 1985 | title = The role of frame-based representation in knowledge representation and reasoning | url = | journal = Communications of the ACM | volume = 28 | issue = 9| pages = 904–920 | doi=10.1145/4284.4285}}
* Peter Clark & Bruce Porter:  KM - The Knowledge Machine 2.0: Users Manual,  http://www.cs.utexas.edu/users/mfkb/RKF/km.html.
* Peter D. Karp, [http://www.ai.sri.com/pub_list/236 The Design Space of Frame Knowledge Representation Systems], Technical Note 520. [[Artificial Intelligence Center]], [[SRI International]], 1992

==External links==
*[http://www.cs.umbc.edu/771/papers/nebel.html Frame-Based Systems]
*[http://www.ai.sri.com/~gfp/spec/paper/paper.html The Generic Frame Protocol]
*[http://protege.stanford.edu/ The Protégé Ontology Editor]
*[http://www.csee.umbc.edu/courses/771/current/presentations/frames.pdf Intro Presentation to Frame Languages]

[[Category:Artificial intelligence]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:732
<=====title=====>:
Attempto Controlled English
<=====text=====>:
{{Refimprove|date=April 2016}}
'''Attempto Controlled English''' ('''ACE''') is a [[controlled natural language]], i.e. a subset of standard [[English grammar|English]] with a restricted syntax and restricted semantics described by a small set of construction and interpretation rules.<ref>{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Gerold Schneider | title = Attempto Controlled English Meets the Challenges of Knowledge Representation, Reasoning, Interoperability and User Interfaces | booktitle = FLAIRS 2006 | date = 2006 | url = http://attempto.ifi.uzh.ch/site/publications/papers/FLAIRS0601FuchsN.pdf | format = [[PDF]]}}</ref> It has been under development at the [[University of Zurich]] since 1995. In 2013, ACE version 6.7 was announced.<ref>http://attempto.ifi.uzh.ch/site/news/</ref>

ACE can serve as [[knowledge representation]], [[specification language|specification]], and [[query language]], and is intended for professionals who want to use formal notations and formal methods, but may not be familiar with them. Though ACE appears perfectly natural – it can be read and understood by any speaker of English – it is in fact a [[formal language]].

ACE and its related tools have been used in the fields of [[requirements analysis|software specifications]], [[theorem proving]], [[automatic summarization|text summaries]], [[ontologies]], rules, querying, [[health informatics|medical documentation]] and [[planning]].

Here are some simple examples:

# Every woman is a human.
# A woman is a human.
# A man tries-on a new tie. If the tie pleases his wife then the man buys it.

ACE construction rules require that each noun be introduced by a determiner (''a'', ''every'', ''no'', ''some'', ''at least 5'', ...). ACE interpretation rules decide that (1) is interpreted as [[Universal quantification|universally quantified]], while (2) is interpreted as [[Existential quantification|existentially quantified]]. Sentences like "Women are human" do not follow ACE syntax and are consequently not valid.

Interpretation rules resolve the [[Deixis#Anaphoric reference|anaphoric references]] in (3): ''the tie'' and ''it'' of the second sentence refer to ''a new tie'' of the first sentence, while ''his'' and ''the man'' of the second sentence refer to ''a man'' of the first sentence. Thus an ACE text is a coherent entity of anaphorically linked sentences.

The Attempto Parsing Engine (APE) translates ACE texts unambiguously into [[Discourse Representation Theory|discourse representation structures]] (DRS) that use a variant of the language of [[first-order logic]].<ref>{{cite conference |author1=Norbert E. Fuchs |author2=Kaarel Kaljurand |author3=Tobias Kuhn | title = Discourse Representation Structures for ACE 6.6 | booktitle = Technical Report ifi-2010.0010, Department of Informatics, University of Zurich | date = 2010 | url = http://attempto.ifi.uzh.ch/site/pubs/papers/drs_report_66.pdf| format = [[PDF]] }}</ref> A DRS can be further translated into other [[formal languages]], for instance AceRules with various semantics,<ref>{{cite conference | author = Tobias Kuhn | title = AceRules: Executing Rules in Controlled Natural Language | booktitle = First International Conference on Web Reasoning and Rule Systems (RR 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/kuhn07acerules.pdf| format = [[PDF]]}}</ref>   [[Web Ontology Language|OWL]],<ref>{{cite conference |author1=Kaarel Kaljurand |author2=Norbert E. Fuchs | title = Verbalizing OWL in Attempto Controlled English | booktitle = OWL: Experiences and Directions (OWLED 2007) | year = 2007  | url = http://attempto.ifi.uzh.ch/site/pubs/papers/owled2007_kaljurand.pdf | format = [[PDF]]}}</ref> and [[Semantic Web Rule Language|SWRL]]. Translating an ACE text into (a fragment of) first-order logic allows users to  [[inference|reason]] about the text, for instance to [[formal verification|verify]], to [[formal verification|validate]], and to [[Information retrieval|query]] it.

== ACE in a nutshell ==
{{unreferenced section|date=May 2013}}
As an overview of the current version 6.6 of ACE this section:

* Briefly describes the vocabulary
* Gives an account of the syntax
* Summarises the handling of ambiguity
* Explains the processing of anaphoric references.

=== Vocabulary ===

The vocabulary of ACE comprises:

* Predefined function words (e.g. determiners, conjunctions)
* Predefined phrases (e.g. "it is false that ...", "it is possible that ...")
* Content words (e.g. nouns, verbs, adjectives, adverbs).

=== Grammar ===

The grammar of ACE defines and constrains the form and the meaning of ACE sentences and texts. ACE's grammar is expressed as a set of  [http://attempto.ifi.uzh.ch/site/docs/ace_constructionrules.html  construction rules]. The meaning of sentences is described as a small set of [http://attempto.ifi.uzh.ch/site/docs/ace_interpretationrules.html  interpretation rules]. A [http://attempto.ifi.uzh.ch/site/docs/ace_troubleshooting.html Troubleshooting Guide] describes how to use ACE and how to avoid pitfalls.

==== ACE texts ====

An ACE text is a sequence of declarative sentences that can be anaphorically interrelated. Furthermore, ACE supports questions and commands.

==== Simple sentences ====

A simple sentence asserts that something is the case — a fact, an event, a state.

:The temperature is -2 °C.
:A customer inserts 2 cards. 
:A card and a code are valid.

Simple ACE sentences have the following general structure:

:subject + verb + complements + adjuncts

Every sentence has a subject and a verb. Complements (direct and indirect objects) are necessary for transitive verbs (''insert something'') and ditransitive verbs (''give something to somebody''), whereas adjuncts (adverbs, prepositional phrases) are optional.

All elements of a simple sentence can be elaborated upon to describe the situation in more detail. To further specify the nouns ''customer'' and ''card'', we could add adjectives:

:A trusted customer inserts two valid cards.

possessive nouns and ''of''-prepositional phrases:

:John's customer inserts a card of Mary.

or variables as appositions:

:John inserts a card A.

Other modifications of nouns are possible through relative sentences:

:A customer who is trusted inserts a card that he owns.

which are described below since they make a sentence composite. We can also detail the insertion event, e.g. by adding an adverb:

:A customer inserts some cards manually.

or, equivalently:

:A customer manually inserts some cards.

or, by adding prepositional phrases:

:A customer inserts some cards into a slot.

We can combine all of these elaborations to arrive at:

:John's customer who is trusted inserts a valid card of Mary manually into a slot A.

==== Composite sentences ====

Composite sentences are recursively built from simpler sentences through [[coordination (linguistics)|coordination]], [[subordination (linguistics)|subordination]], [[Quantification (linguistics)|quantification]], and [[negation]]. Note that ACE composite sentences overlap with what linguists call compound sentences and complex sentences.

===== Coordination =====

Coordination by ''and'' is possible between sentences and between phrases of the same syntactic type.

:A customer inserts a card and the machine checks the code.
:There is a customer who inserts a card and who enters a code.
:A customer inserts a card and enters a code.
:An old and trusted customer enters a card and a code.

Note that the coordination of the noun phrases ''a card'' and ''a code'' represents a plural object.

Coordination by ''or'' is possible between sentences, verb phrases, and relative clauses.

:A customer inserts a card or the machine checks the code.
:A customer inserts a card or enters a code.
:A customer owns a card that is invalid or that is damaged.

Coordination by ''and'' and ''or'' is governed by the standard binding order of logic, i.e. ''and'' binds stronger than ''or''. Commas can be used to override the standard binding order. Thus the sentence:

:A customer inserts a VisaCard or inserts a MasterCard, and inserts a code.

means that the customer inserts a VisaCard and a code, or alternatively a MasterCard and a code.

===== Subordination =====

There are four constructs of subordination: relative sentences, ''if-then'' sentences, modality, and sentence subordination.

Relative sentences starting with ''who'', ''which'', and ''that'' allow to add detail to nouns:

:A customer who is trusted inserts a card that he owns.

With the help of ''if-then'' sentences we can specify conditional or hypothetical situations:

:If a card is valid then a customer inserts it.

Note the anaphoric reference via the pronoun ''it'' in the ''then''-part to the noun phrase ''a card'' in the ''if''-part.

Modality allows us to express possibility and necessity:

:A trusted customer can/must insert a card.
:It is possible/necessary that a trusted customer inserts a card.

Sentence subordination comes in various forms:

:It is true/false that a customer inserts a card.
:It is not provable that a customer inserts a card.
:A clerk believes that a customer inserts a card.

===== Quantification =====

Quantification allows us to speak about all objects of a certain class ([[universal quantification]]), or to denote explicitly the existence of at least one object of this class ([[existential quantification]]). The textual occurrence of a universal or existential quantifier opens its scope that extends to the end of the sentence, or in coordinations to the end of the respective coordinated sentence.

To express that all involved customers insert cards we can write

:Every customer inserts a card.

This sentence means that each customer inserts a card that may, or may not, be the same as the one inserted by another customer. To specify that all customers insert the same card — however unrealistic that situation seems — we can write:

:A card is inserted by every customer.

or, equivalently:

:There is a card that every customer inserts.

To state that every card is inserted by a customer we write:

:Every card is inserted by a customer.

or, somewhat indirectly:

:For every card there is a customer who inserts it.

===== Negation =====

Negation allows us to express that something is not the case:

:A customer does not insert a card.
:A card is not valid.

To negate something for all objects of a certain class one uses ''no'':

:No customer inserts more than 2 cards.

or, ''there is no'':

:There is no customer who inserts a card.

To negate a complete statement one uses sentence negation:

:It is false that a customer inserts a card.

These forms of negation are logical negations, i.e. they state that something is provably not the case. Negation as failure states that a state of affairs cannot be proved, i.e. there is no information whether the state of affairs is the case or not.

:It is not provable that a customer inserts a card.

==== Queries ====

ACE supports two forms of queries: ''yes/no''-queries and ''wh''-queries.

''Yes/no''-queries ask for the existence or non-existence of a specified situation. If we specified:

:A customer inserts a card.

then we can ask:

:Does a customer insert a card?

to get a positive answer. Note that interrogative sentences always end with a question mark.

With the help of ''wh''-queries, i.e. queries with query words, we can interrogate a text for details of the specified situation. If we specified:

:A trusted customer inserts a valid card manually in the morning in a bank.

we can ask for each element of the sentence with the exception of the verb.

:Who inserts a card?
:Which customer inserts a card?
:What does a customer insert?
:How does a customer insert a card?
:When does a customer enter a card?
:Where does a customer enter a card?

Queries can also be constructed by a sequence of declarative sentences followed by one interrogative sentence, for example:

:There is a customer and there is a card that the customer enters. Does a customer enter a card?

==== Commands ====

ACE also supports commands. Some examples:

:John, go to the bank!
:John and Mary, wait!
:Every dog, bark!
:A brother of John, give a book to Mary!

A command always consists of a noun phrase (the addressee), followed by a comma, followed by an uncoordinated verb phrase. Furthermore, a command has to end with an exclamation mark.

=== Constraining ambiguity ===

To constrain the ambiguity of full natural language ACE employs three simple means:

* Some ambiguous constructs are not part of the language; unambiguous alternatives are available in their place
* All remaining ambiguous constructs are interpreted deterministically on the basis of a small number of interpretation rules
* Users can either accept the assigned interpretation, or they must rephrase the input to obtain another one.

==== Avoidance of ambiguity ====

In natural language, relative sentences combined with coordinations can introduce ambiguity:

:A customer inserts a card that is valid and opens an account.

In ACE the sentence has the unequivocal meaning that the customer opens an account, as reflected by the paraphrase:

:A card is valid. A customer inserts the card. The customer opens an account.

To express the alternative — though not very realistic — meaning that the card opens an account, the relative pronoun ''that'' must be repeated, thus yielding a coordination of relative sentences:

:A customer inserts a card that is valid and that opens an account.

This sentence is unambiguously equivalent in meaning to the paraphrase:

:A card is valid. The card opens an account. A customer inserts the card.

==== Interpretation rules ====

Not all ambiguities can be safely removed from ACE without rendering it artificial. To deterministically interpret otherwise syntactically correct ACE sentences we use a small set of interpretation rules. For example, if we write:

:A customer inserts a card with a code.

then ''with a code'' attaches to the verb ''inserts'', but not to ''a card''. However, this is probably not what we meant to say. To express that ''the code'' is associated with ''the card'' we can employ the interpretation rule that a relative sentence always modifies the immediately preceding noun phrase, and rephrase the input as:

:A customer inserts a card that carries a code.

yielding the paraphrase:

:A card carries a code. A customer inserts the card.

or — to specify that the customer inserts a card and a code — as:

:A customer inserts a card and a code.

=== Anaphoric references ===

Usually ACE texts consist of more than one sentence:

:A customer enters a card and a code. If a code is valid then SimpleMat accepts a card.

To express that all occurrences of card and code should mean the same card and the same code, ACE provides anaphoric references via the definite article:

:A customer enters a card and a code. If the code is valid then SimpleMat accepts the card.

During the processing of the ACE text, all anaphoric references are replaced by the most recent and most specific accessible noun phrase that agrees in gender and number. As an example of "most recent and most specific", suppose an ACE parser is given the sentence:

:A customer enters a red card and a blue card.

Then:

:The card is correct.

refers to the second card, while:

:The red card is correct.

refers to the first card.

Noun phrases within ''if-then'' sentences, universally quantified sentences, negations, modality, and subordinated sentences cannot be referred to anaphorically from subsequent sentences, i.e. such noun phrases are not "accessible" from the following text. Thus for each of the sentences:

:If a customer owns a card, then they enter it.
:Every customer enters a card.
:A customer does not enter a card.
:A customer can enter a card.
:A clerk believes that a customer enters a card.

we cannot refer to ''a card'' with:

:The card is correct.

Anaphoric references are also possible via personal pronouns:

:A customer enters a card and a code. If it is valid then SimpleMat accepts the card.

or via variables:

:A customer enters a card X and a code Y. If Y is valid then SimpleMat accepts X.

Anaphoric references via definite articles and variables can be combined:

:A customer enters a card X and a code Y. If the code Y is valid then SimpleMat accepts the card X.

Note that proper names like ''SimpleMat'' always refer to the same object.

==See also==
*[[Gellish]]
*[[Natural Language Processing]]
*[[Knowledge Representation]]
*[[Natural language programming]]
*[[Structured English]]
**[[ClearTalk]], another machine-readable knowledge representation language
**[[Inform 7]], a programming language with English syntax

==References==
{{Reflist}}

==External links==
*[http://attempto.ifi.uzh.ch Project Attempto]

[[Category:Controlled English]]
[[Category:Knowledge representation]]
[[Category:Controlled natural languages]]
[[Category:Natural language processing]]
[[Category:Natural language parsing]]
<=====doc_Id=====>:735
<=====title=====>:
Semantic interoperability
<=====text=====>:
{{multiple issues|
{{underlinked|date=January 2013}}
{{more footnotes|date=February 2011}}
}}

'''Semantic interoperability''' is the ability of [[computer]] systems to exchange [[data]] with unambiguous, shared meaning. [[Semantic]] interoperability is a requirement to enable machine computable logic, inferencing, knowledge discovery, and data federation between information systems.<ref>NCOIC, [https://www.ncoic.org/technology/deliverables/scope/ "SCOPE"], [https://www.ncoic.org/home ''Network Centric Operations Industry Consortium''], 2008</ref>

Semantic interoperability is therefore concerned not just with the packaging of data ([[syntax]]), but the simultaneous transmission of the meaning with the data (semantics).  This is accomplished by adding data about the data (metadata), linking each data element to a controlled, shared vocabulary.  The meaning of the data is transmitted with the data itself, in one self-describing "information package" that is independent of any information system.  It is this shared vocabulary, and its associated links to an ontology, which provides the foundation and capability of machine interpretation, inferencing, and logic.

Syntactic interoperability is a prerequisite for semantic interoperability.  Syntactic interoperability refers to the packaging and transmission mechanisms for data.  In healthcare, HL7 has been in use for over thirty years (which predates the internet and web technology), and uses the pipe character (|) as a data delimiter. The current internet standard for document markup is XML, which uses "< >" as a data delimiter.  The data delimiters convey no meaning to the data other than to structure the data.  Without a data dictionary to translate the contents of the delimiters, the data remains meaningless.  While there are many attempts at creating data dictionaries and information models to associate with these data packaging mechanisms, none have been practical to implement.  This has only perpetuated the ongoing "babelization" of data and inability to exchange of data with meaning.

Since the introduction of the Semantic Web concept by [[Tim Berners-Lee]] in 1999,<ref>{{cite book |last=Berners-Lee |first=Tim |authorlink=Tim Berners-Lee |author2=Fischetti, Mark |title=[[Tim Berners Lee#Weaving the Web|Weaving the Web]] |publisher=[[HarperSanFrancisco]] |year=1999 |pages=chapter 12 |isbn=978-0-06-251587-2 |nopp=true }}</ref> there has been growing interest and application of the W3C (World Wide Web Consortium, [[WWWC]]) standards to provide web-scale semantic data exchange, federation, and inferencing capabilities.

== Semantic as a function of syntactic interoperability ==

Syntactic interoperability, provided by for instance [[XML]] or the [[SQL]] standards, is a pre-requisite to semantic.  It involves a common data format and common protocol to structure any data so that the manner of processing the information will be interpretable from the structure.  It also allows detection of syntactic errors, thus allowing receiving systems to request resending of any message that appears to be garbled or incomplete.  No semantic communication is possible if the syntax is garbled or unable to represent the data.  However, information represented in one syntax may in some cases be accurately translated into a different syntax.  Where accurate translation of syntaxes is possible, systems using different syntaxes may also interoperate accurately.  In some cases the ability to accurately translate information among systems using different syntaxes may be limited to one direction, when the formalisms used have different levels of ''expressivity'' (ability to express information).

A single ontology containing representations of every term used in every application is generally considered impossible, because of the rapid creation of new terms or assignments of new meanings to old terms.  However, though it is impossible to anticipate ''every'' concept that a user may wish to represent in a computer, there is the possibility of finding some finite set of "primitive" concept representations that can be combined to create any of the more specific concepts that users may need for any given set of applications or ontologies.  Having a foundation ontology (also called ''[[upper ontology]]'') that contains all those primitive elements would provide a sound basis for general semantic interoperability, and allow users to define any new terms they need by using the basic inventory of ontology elements, and still have those newly defined terms properly interpreted by any other computer system that can interpret the basic foundation ontology.  Whether the number of such primitive concept representations is in fact finite, or will expand indefinitely, is a question under active investigation.  If it is finite, then a stable foundation ontology suitable to support accurate and general semantic interoperability can evolve after some initial foundation ontology has been tested and used by a wide variety of users.  At the present time, no foundation ontology has been adopted by a wide community, so such a stable foundation ontology is still in the future.

== Words and Meanings ==

One persistent misunderstanding recurs in discussion of semantics - the confusion of words and meanings.  The meanings of words change, sometimes rapidly. But a formal language such as used in an ontology can encode the meanings (semantics) of concepts in a form that does not change.  In order to determine what is the meaning of a particular word (or term in a database, for example) it is necessary to label each fixed concept representation in an ontology with the word(s) or term(s) that may refer to that concept.  When multiple words refer to the same (fixed) concept, in language this is called synonymy; when one word is used to refer to more than one concept, that is called ambiguity.  Ambiguity and synonymy are among the factors that make computer understanding of language very difficult.  The use of words to refer to concepts (the meanings of the words used)is very sensitive to the context and the purpose of any use for many human-readable terms.  The use of ontologies in supporting semantic interoperability is to provide a fixed set of concepts whose meanings and relations are stable and can be agreed to by users.  The task of determining which terms in which contexts (each database is a different context) then is separated from the task of creating the ontology, and must be taken up by the designer of a database, or the designer of a form for data entry, or the developer of a program for language understanding.  When a word used in some interoperability context changes its meaning, then to preserve interoperability it is necessary to change the pointer to the ontology element(s) that specifies the meaning of that word.

== Knowledge representation requirements and languages ==

A knowledge representation language may be sufficiently expressive to describe nuances of meaning in well understood fields.  There are at least five levels of complexity of these{{specify|date=June 2014}}.

For general [[semi-structured data]] one may use a general purpose language such as XML.<ref>[http://www.cs.umd.edu/projects/plus/SHOE/pubs/extreme2000.pdf XML as a tool for Semantic Interoperability] Semantic Interoperability on the Web, Jeff Heflin and James Hendler</ref>

Languages with the full power of first-order predicate logic may be required for many tasks.

Human languages are highly expressive, but are considered too ambiguous to allow the accurate interpretation desired, given the current level of human language technology.   In human languages the same word may be used to refer to different concepts (ambiguity), and the same concept may be referred to by different words (synonymy).

== Prior agreement not required ==
{{confusing|section|date=February 2016}}

Semantic interoperability may be distinguished from other forms of interoperability by considering whether the information transferred has, in its communicated form, all of the meaning required for the receiving system to interpret it correctly, even when the algorithms used by the receiving system are unknown to the sending system.  Consider sending one number:

If that number is intended to be the sum of money owed by one company to another, it implies some action or lack of action on the part of both those who send it and those who receive it.

It may be correctly interpreted if sent in response to a specific request, and received at the time and in the form expected.  This correct interpretation does not depend only on the number itself, which could represent almost any of millions of types of quantitative measure, rather it depends strictly on the circumstances of transmission.  That is, the interpretation depends on both systems expecting that the algorithms in the other system use the number in exactly the same sense, and it depends further on the entire envelope of transmissions that preceded the actual transmission of the bare number.  By contrast, if the transmitting system does not know how the information will be used by other systems, it is necessary to have a shared agreement on how information with some specific meaning (out of many possible meanings) will appear in a communication.  For a particular task, one solution is to standardize a form, such as a request for payment; that request would have to encode, in standardized fashion, all of the information needed to evaluate it, such as: the agent owing the money, the agent owed the money, the nature of the action giving rise to the debt, the agents, goods, services, and other participants in that action; the time of the action; the amount owed and currency in which the debt is reckoned; the time allowed for payment; the form of payment demanded; and other information.  When two or more systems have agreed on how to interpret the information in such a request, they can achieve semantic interoperability ''for that specific type of transaction''.  For semantic interoperability generally, it is necessary to provide standardized ways to describe the meanings of many more things than just commercial transactions, and the number of concepts whose representation needs to be agreed upon are at a minimum several thousand.

== Ontology research ==

How to achieve semantic interoperability for more than a few restricted scenarios is currently a matter of research and discussion.  For the problem of General Semantic Interoperability, some form of foundation ontology ('[[upper ontology]]') is required that is sufficiently comprehensive to provide the defining concepts for more specialized ontologies in multiple domains.  Over the past decade more than ten foundation ontologies have been developed, but none have as yet been adopted by a wide user base.

The need for a single comprehensive all-inclusive ontology to support Semantic Interoperability can be avoided by designing the common foundation ontology as a set of basic ("primitive") concepts that can be combined to create the logical descriptions of the meanings of terms used in local domain ontologies or local databases.  This tactic is based on the principle that:

'''If:'''
<pre style="white-space:pre-wrap;">
(1) the meanings and usage of the primitive ontology elements in the foundation ontology are agreed on, and 
(2) the ontology elements in the  domain ontologies are constructed as logical
combinations of the elements in the foundation ontology,
</pre>
'''Then:'''
<pre style="white-space:pre-wrap;">
The intended meanings of the domain ontology elements can be computed automatically using an FOL reasoner, by any system that accepts the meanings of the elements in the foundation ontology, and has both the foundation ontology and the logical specifications of the elements in the domain ontology.
</pre>
'''Therefore:'''
<pre style="white-space:pre-wrap;">
Any system wishing to interoperate accurately with another system need transmit only the data to be communicated, plus any logical descriptions of terms used in that data that were created locally and are not already in the common foundation ontology.
</pre>

This tactic then limits the need for prior agreement on meanings to only those ontology elements in the common Foundation Ontology (FO).  Based on several considerations, this is likely to be fewer than 10,000 elements (types and relations).

In practice, together with the FO focused on representations of the primitive concepts, a set of domain extension ontologies to the FO with elements specified using the FO elements will likely also be used.  Such pre-existing extensions will ease the cost of creating domain ontologies by providing existing elements with the intended meaning, and will reduce the chance of error by using elements that have already been tested.  Domain extension ontologies may be logically inconsistent with each other, and that needs to be determined if different domain extensions are used in any communication.

Whether use of such a single foundation ontology can itself be avoided by sophisticated mapping techniques among independently developed ontologies is also under investigation.

== Importance==

The practical significance of semantic interoperability has been measured by several studies that estimate the cost (in lost efficiency) due to lack of semantic interoperability.  One study,<ref>[http://content.healthaffairs.org/cgi/content/full/hlthaff.w5.10/DC1 Jan Walker, Eric Pan, Douglas Johnston, Julia Adler-Milstein, David W. Bates and Blackford Middleton, ''The Value of Healthcare Information Exchange and Interoperability'' Health Affairs, 19 January 2005]</ref> focusing on the lost efficiency in the communication of healthcare information, estimated that US$77.8 billion per year could be saved by implementing an effective interoperability standard in that area.  Other studies, of the construction industry<ref>[http://www.bfrl.nist.gov/oae/publications/gcrs/04867.pdf Microsoft Word - 08657 Final Rpt_8-2-04.doc<!-- Bot generated title -->]</ref> and of the automobile manufacturing supply chain,<ref>http://www.nist.gov/director/prog-ofc/report99-1.pdf</ref> estimate costs of over US$10 billion per year due to lack of semantic interoperability in those industries.  In total these numbers can be extrapolated to indicate that well over US$100 billion per year is lost because of the lack of a widely used semantic interoperability standard in the US alone.

There has not yet been a study about each policy field that might offer big cost savings applying semantic interoperability standards. But to see which policy fields are capable of profiting from semantic interoperability see '[[Interoperability]]' in general. Such policy fields are [[eGovernment]], health, security and many more. The EU also set up the [[Semantic Interoperability Centre Europe]] in June 2007.

==See also==
*[[Interoperability]], Interoperability generally
*[[Semantic Computing]]
*[[Upper ontology (computer science)]], Discussion of using an ''upper ontology''.
*[[Conceptual interoperability|Levels of Conceptual Interoperability]], A discussion describing an interoperability spectrum in the context of exchange of Modeling and Simulation information, in which ''semantic interoperability '' is not defined as fully independent of context, as described here.
*[[UDEF]], Universal Data Element Framework

==External links==
*[http://colab.cim3.net/cgi-bin/wiki.pl?OntologyTaxonomyCoordinatingWG/OntacGlossary the ONTACWG Glossary Other definitions of Semantic Interoperability]
*[http://marinemetadata.org/guides/vocabs/cvchooseimplement/cvsemint MMI Guide: Achieving Semantic Interoperability]

==References==
{{reflist|2}}

[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Computing terminology]]
[[Category:Telecommunication theory]]
[[Category:Interoperability]]
<=====doc_Id=====>:738
<=====title=====>:
Category:Multi-agent systems
<=====text=====>:
This [[Wikipedia:category|category]] is about [[multi-agent system]]s, systems composed of several [[software agent]]s. 


[[Category:Computing platforms]]
[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Distributed computing architecture]]

== See also ==
*[[Cognitive architecture]]
*[[Intelligent agent]]
*[[Autonomous agent]]
*[[Internet bot]]
*[[Daemon (computer software)|Daemon]]
<=====doc_Id=====>:741
<=====title=====>:
Minimum Information Standards
<=====text=====>:
{{Multiple issues|
{{confusing|date=January 2010}}
{{essay-like|date=January 2010}}
{{lead rewrite|date=January 2010}}
{{external links|date=September 2012}}
{{more footnotes|date=January 2010}}
{{expert-subject|Computational Biology|date=January 2010}}
}}

The '''minimum information standard''' is a set of guidelines for [[data reporting|reporting]]  [[data]] derived by relevant methods in biosciences. If followed, it ensures that the data can be easily verified, analysed and clearly interpreted by the wider scientific community. Keeping with these recommendations also facilitates the foundation of structuralized databases, public repositories and development of data analysis tools.<ref name="MIFlowCyt: The minimum information about a flow cytometry experiment">{{cite journal|last=Lee|first=Jamie A. |author2=Spidlen, Josef |author3=Boyce, Keith |author4=Cai, Jennifer |author5=Crosbie, Nicholas |author6=Dalphin, Mark |author7=Furlong, Jeff |author8=Gasparetto, Maura |author9=Goldberg, Michael |author10=Goralczyk, Elizabeth M. |author11=Hyun, Bill |author12=Jansen, Kirstin |author13=Kollmann, Tobias |author14=Kong, Megan |author15=Leif, Robert |author16=McWeeney, Shannon |author17=Moloshok, Thomas D. |author18=Moore, Wayne |author19=Nolan, Garry |author20=Nolan, John |author21=Nikolich-Zugich, Janko |author22=Parrish, David |author23=Purcell, Barclay |author24=Qian, Yu |author25=Selvaraj, Biruntha |author26=Smith, Clayton |author27=Tchuvatkina, Olga |author28=Wertheimer, Anne |author29=Wilkinson, Peter |author30=Wilson, Christopher |author31=Wood, James |author32=Zigon, Robert |author33=Scheuermann, Richard H. |author34=Brinkman, Ryan R. |title=MIFlowCyt: The minimum information about a flow cytometry experiment|journal=Cytometry Part A|date=1 October 2008|volume=73A|issue=10|pages=926–930|doi=10.1002/cyto.a.20623|pmid=18752282|pmc=2773297}}</ref><ref name="Minimum information about a microarray experiment (MIAME)—toward standards for microarray data">{{cite journal|last=Brazma|first=Alvis |author2=Hingamp, Pascal |author3=Quackenbush, John |author4=Sherlock, Gavin |author5=Spellman, Paul |author6=Stoeckert, Chris |author7=Aach, John |author8=Ansorge, Wilhelm |author9=Ball, Catherine A. |author10=Causton, Helen C. |author11=Gaasterland, Terry |author12=Glenisson, Patrick |author13=Holstege, Frank C.P. |author14=Kim, Irene F. |author15=Markowitz, Victor |author16=Matese, John C. |author17=Parkinson, Helen |author18=Robinson, Alan |author19=Sarkans, Ugis |author20=Schulze-Kremer, Steffen |author21=Stewart, Jason |author22=Taylor, Ronald |author23=Vilo, Jaak |author24=Vingron, Martin |title=Minimum information about a microarray experiment (MIAME)—toward standards for microarray data|journal=Nature Genetics|date=30 November 2001|volume=29|issue=4|pages=365–371|doi=10.1038/ng1201-365|pmid=11726920}}</ref>

The individual '''minimum information standards''' are brought by the communities of cross-disciplinary specialists focused on the problematic of the specific method used in experimental biology.  The standards then provide specifications what information about the experiments ([[metadata]]) is crucial and important to be reported together with the resultant data to make it comprehensive.<ref name="MIFlowCyt: The minimum information about a flow cytometry experiment"/><ref name="Minimum information about a microarray experiment (MIAME)—toward standards for microarray data"/> The need for this standardization is largely driven by the development of high-throughput experimental methods that provide tremendous amounts of data.  The development of minimum information standards of different methods is since 2008 being harmonized by "Minimum Information about a Biomedical or Biological Investigation" (MIBBI) project.<ref name="Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project">{{cite journal|last=Taylor|first=Chris F|title=Promoting coherent minimum reporting guidelines for biological and biomedical investigations: the MIBBI project|journal=Nature Biotechnology|year=2008|volume=26|pages=889–896|doi=10.1038/nbt.1411}}</ref>

==MI Standards==

===MIAME, gene expression microarray===
Minimum Information About a Microarray Experiment (MIAME) describes the Minimum Information About a Microarray Experiment that is needed to enable the interpretation of the results of the experiment unambiguously and potentially to reproduce the experiment and is aimed at facilitating the dissemination of data from microarray experiments.

MIAME contains a number of extensions to cover specific biological domains, including MIAME-env, MIAME-nut and MIAME-tox, covering environmental genomics, nutritional genomics and toxogenomics, respectively

===MINI: Minimum Information about a Neuroscience Investigation===

====MINI: Electrophysiology====
[[Electrophysiology]] is a technology used to study the electrical properties of biological cells and tissues. Electrophysiology typically involves the measurements of voltage change or electric current flow on a wide variety of scales from single ion channel
proteins to whole tissues. This document is a single module, as part of the Minimum Information about a Neuroscience investigation (MINI) family of reporting guideline
documents, produced by community consultation and continually available for public comment. A MINI module represents the minimum information that should be reported about a dataset to facilitate computational access and analysis to allow a reader to interpret and critically evaluate the processes performed and the conclusions reached, and to support their experimental corroboration. In practice a MINI module comprises a checklist of information that should be provided (for example about the protocols employed) when
a data set is described for publication. The full specification of the MINI module can be found here.<ref>Gibson, Frank, Overton, Paul, Smulders, Tom, Schultz, Simon, Eglen, Stephen, Ingram, Colin, Panzeri, Stefano, Bream, Phil, Sernagor, Evelyne, Cunningham, Mark, Adams, Christopher, Echtermeyer, Christoph, Simonotto, Jennifer, Kaiser, Marcus, Swan, Daniel, Fletcher, Marty, and Lord, Phillip. Minimum Information about a Neuroscience Investigation (MINI) Electrophysiology. Available from Nature Precedings <http://hdl.handle.net/10101/npre.2008.1720.1> (2008)</ref>

===MIARE, RNAi experiment===
Minimum Information About an RNAi Experiment (MIARE) is a [[data reporting]] guideline which describes the minimum information that should be reported about an RNAi experiment to enable the unambiguous interpretation and reproduction of the results.

===MIACA, cell based assay===
Advances in genomics and functional genomics have enabled large-scale analyses of gene and protein function by means of high-throughput cell biological analyses. Thereby, cells in culture can be perturbed in vitro and the induced effects recorded and analyzed. Perturbations can be triggered in several ways, for instance with molecules (siRNAs, expression constructs, small chemical compounds, ligands for receptors, etc.), through environmental stresses (such as temperature shift, serum starvation, oxygen deprivation, etc.), or combinations thereof. The cellular responses to such perturbations are analyzed in order to identify molecular events in the biological processes addressed and understand biological principles.
We propose the Minimum Information About a Cellular Assay (MIACA) for reporting a cellular assay, and CA-OM, the modular cellular assay object model, to facilitate exchange of data and accompanying information, and to compare and integrate data that originate from different, albeit complementary approaches, and to elucidate higher order principles. [http://sourceforge.net/project/showfiles.php?group_id=158121 Documents describing MIACA] are available and provide further information as well as the checklist of terms that should be reported.

===MIAPE, proteomic experiments===
The Minimum Information About a Proteomic Experiment documents describe information which should be given along with a proteomic experiment. The parent document describes the processes and principles underpinning the development of a series of domain specific documents which now cover all aspects of a MS-based proteomics workflow.
{{Details|Minimum Information About a Proteomics Experiment }}

===MIMIx, molecular interactions===
This document has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info) and describes the Minimum Information about a Molecular Interaction experiment.

===MIAPAR, protein affinity reagents===
The Minimum Information About a Protein Affinity Reagent has been developed and maintained by the Molecular Interaction worktrack of the HUPO-PSI (www.psidev.info)in conjunction with the HUPO Antibody Initiative and a European consortium of binder producers and seeks to encourage users to improve their description of binding reagents, such as antibodies, used in the process of protein identification.

===MIABE, bioactive entities===
The Minimum Information About a Bioactive Entity was produced by representatives from both large pharma and academia who are looking to improve the description of usually small molecules which bind to, and potentially modulate the activity of, specific targets in a living organism. This document encompasses drug-like molecules as well as hebicides, pesticides and food additives. It is primarily maintained through the EMBL-EBI Industry program (www.ebi.ac.uk/industry).

===MIGS/MIMS, genome/metagenome sequences===
This specification is being developed by the [[Genomic Standards Consortium]]

===MIFlowCyt, flow cytometry===

====Minimum Information about a Flow Cytometry Experiment====
The fundamental tenet of any scientific research is that the published results of any study have to be open to independent validation or refutation. The Minimum Information about a Flow Cytometry Experiment (MIFlowCyt) establishes the criteria to record information about the experimental overview, samples, instrumentation and data analysis. It promotes consistent annotation of clinical, biological and technical issues surrounding a flow cytometry experiment by specifying the requirements for data content and by providing a structured framework for capturing information.

More information can be found at:
* The Flow Informatics and Computational Cytometry Socienty (FICCS) [http://wiki.ficcs.org/ficcs/MIFlowCyt MIFlowCyt wiki] page.
* The Bioinformatics Standards for Flow Cytometry [http://flowcyt.sourceforge.net/miflowcyt/ MIFlowCyt web] page.

===MISFISHIE, In Situ Hybridization and Immunohistochemistry Experiments===
{{Emptysection|date=February 2013}}

===MIAPA, Phylogenetic Analysis===
Criteria for Minimum Information About a Phylogenetic Analysis were described in 2006. <ref> {{Cite journal | doi = 10.1089/omi.2006.10.231| title = Taking the First Steps towards a Standard for Reporting on Phylogenies: Minimum Information about a Phylogenetic Analysis (MIAPA)| journal = OMICS: A Journal of Integrative Biology| volume = 10| issue = 2| pages = 231| year = 2006| last1 = Leebens-Mack | first1 = J. | last2 = Vision | first2 = T. | last3 = Brenner | first3 = E. | last4 = Bowers | first4 = J. E. | last5 = Cannon | first5 = S. | last6 = Clement | first6 = M. J. | last7 = Cunningham | first7 = C. W. | last8 = Depamphilis | first8 = C. | last9 = Desalle | first9 = R. | last10 = Doyle | first10 = J. J. | last11 = Eisen | first11 = J. A. | last12 = Gu | first12 = X. | last13 = Harshman | first13 = J. | last14 = Jansen | first14 = R. K. | last15 = Kellogg | first15 = E. A. | last16 = Koonin | first16 = E. V. | last17 = Mishler | first17 = B. D. | last18 = Philippe | first18 = H. | last19 = Pires | first19 = J. C. | last20 = Qiu | first20 = Y. L. | last21 = Rhee | first21 = S. Y. | last22 = Sjölander | first22 = K. | last23 = Soltis | first23 = D. E. | last24 = Soltis | first24 = P. S. | authorlink24 = Pamela S. Soltis| last25 = Stevenson | first25 = D. W. | last26 = Wall | first26 = K. | last27 = Warnow | first27 = T. | last28 = Zmasek | first28 = C. }} </ref>

===MIAO, ORF===
{{Emptysection|date=February 2013}}

===MIAMET, METabolomics experiment===
{{Emptysection|date=February 2013}}

===MIAFGE, Functional Genomics Experiment===
{{Emptysection|date=February 2013}}

===MIRIAM, Minimum Information Required in the Annotation of Models===
The Minimal Information Required In the Annotation of Models ([[MIRIAM]]), is a set of rules for the curation and annotation of quantitative models of biological systems.

===MIASE, Minimum Information About a Simulation Experiment===
The Minimum Information About a Simulation Experiment ([[MIASE]]) is an effort to standardize the description of simulation experiments in the field of systems biology.

===CIMR, Core Information for Metabolomics Reporting===

==External links==
* [http://mibbi.sourceforge.net/ MIBBI (Minimum Information for Biological and Biomedical Investigations)] A ‘one-stop shop’ for exploring the range of extant projects, foster collaborative development and ultimately promote gradual integration.
* [http://www.biosharing.org BioSharing catalogue]

==References==
{{reflist}}

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:744
<=====title=====>:
Futures wheel
<=====text=====>:
[[Image:Futures wheel 01.svg|thumb|right|250px|A futures wheel as described by Jerome C. Glenn.]]
'''The Futures wheel''' is a method for graphical [[visualization (graphic)|visualisation]] of direct and indirect [[future]] '''consequences''' of a particular change or development. It was invented by [[Jerome C. Glenn]] in 1971, when he was a [[student]] at the Antioch Graduate School of Education (now [[Antioch University New England]]).
<blockquote>The Futures Wheel is a way of organizing thinking and questioning about the future – a kind of structured brainstorming. (Jerome C. Glenn (1994) The Futures Wheel)</blockquote>

==Description==

To start a Futures wheel the central [[terminology|term]] describing the change to evaluate is positioned in the center of the page (or drawing area). Then, events or consequences following directly from that development are positioned around it. Next, the (indirect) consequences of the direct consequences are positioned around the first level consequences. The terms may be connected as nodes in a tree (or even a web). The levels will often be marked by concentric circles.

==Usage==

The Futures wheel is usually used to organize [[thought]]s about a future development or trend. With it, possible impacts can be collected and put down in a structured way. The use of interconnecting lines makes it possible to visualize interrelationships of the causes and resulting changes. Thus, Futures wheels can assist in developing multi-concepts about possible future development by offering a futures-conscious perspective and aiding in group [[brainstorming]].

==See also==

* [[Mind Mapping]]

==Bibliography==

* Glenn, Jerome C. ''Futurizing Teaching vs Futures Course'', Social Science Record, Syracuse University, Volume IX, No. 3 Spring 1972.
* Snyder, David Pearce. Monograph: ''The Futures Wheel: A Strategic Thinking Exercise'', The Snyder Family Enterprise, Bethesda, Maryland 1993.
* Glenn, Jerome C. ''Futures Wheel'', Futures Research Methodology Version 3.0, The Millennium Project, Washington, D.C. 2009.

==External links==
* [https://web.archive.org/web/20080612175450/http://www.ltag.education.tas.gov.au/glossary.htm Learning, Teaching and Assessment Guide Glossary] at Tasmania's [[Department of Education (Tasmania)|Department of Education]]'s homepage.
* Downloadable template of a [https://web.archive.org/web/20070927143447/http://www.globaleducation.edna.edu.au/globaled/go/cache/offonce/pid/1835;jsessionid=050A14CB101EAF863AE979C80461FCB3 Futures wheel] at the [[Australia]]n [http://www.globaleducation.edna.edu.au/ Global Education] website.
* Futures Wheel, Futures Research Methodology Version 3.0, The Millennium Project, Washington, DC 2009 [http://millennium-project.org/millennium/FRM-V3.html] 

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Futurology]]
<=====doc_Id=====>:747
<=====title=====>:
Living graph
<=====text=====>:
In terms of knowledge representation, a '''living graph''' (also referred to as a "lifeline", "living timeline"<ref name="history"/> or "fortune line".<ref name="NatStrat"/>) is a graph similar to a [[chronology]] timeline which places events along a vertical axis to reflect changes over time. The vertical axis can be used to represent many factors, such as relative importance, degrees of success/failure, danger/safety or happiness/sadness. In this sense they have been described as being "timelines with attitude".<ref name="history"/>

==References==
{{Reflist|refs=
<ref name="history"> {{Cite web
  | last = Dawson
  | first = Ian
  | authorlink = 
  |author2=Dawson, Patricia Ann
   | title = Introducing Living Graphs
  | work = 
  | publisher = 
  | url = http://thinkinghistory.co.uk/ActivityModel/ActModTimeline.html#graph
  | format = 
  | doi = 
  | accessdate = 25 March 2010}} from Thinking History website
</ref>
<ref name="NatStrat"> {{Cite web
  | last = 
  | first = 
  | authorlink = 
  | title = Living Graphs and Fortune Lines
  | work = 
  | publisher = The National Strategies
  | url = http://downloads.nationalstrategies.co.uk/pdf/67dbff4bdcf5e5534122e1d6ead53abc.pdf
  | format = pdf
  | doi = 
  | accessdate = 25 March 2010}}</ref>}}

==External links==
*[http://classtools.net/samples/sample.php?livingGraph Interactive Living Graph Templates in Flash]
*[http://www.face-online.org.uk/index.php?option=com_content&task=view&id=57&Itemid=172 FACE Living Graph Exercise]

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Quality control tools]]
{{comm-design-stub}}
<=====doc_Id=====>:750
<=====title=====>:
Category:Argument mapping
<=====text=====>:
{{Cat main|Argument map}}

[[Category:Knowledge representation]]
[[Category:Informal arguments]]
<=====doc_Id=====>:753
<=====title=====>:
Historical Thesaurus of the Oxford English Dictionary
<=====text=====>:
{{italic title}}
{{Infobox website
| name            = ''Historical Thesaurus of English''
| logo            = Historical Thesaurus of English logo.png
| logo_size       = <!-- default is 250px -->
| logo_alt        =
| url             = {{URL|www.glasgow.ac.uk/thesaurus}}
| commercial      = No
| type            = Academic
| registration    = None
| content_licence = Free for personal and non-commercial research<ref name=hte-using>{{cite web|title=Using ''Historical Thesaurus'' Data|url=http://historicalthesaurus.arts.gla.ac.uk/using-data/|website=The Historical Thesaurus of English|publisher=University of Glasgow|accessdate=25 October 2014}}</ref>
| programming_language = 
| owner           = [[University of Glasgow]]
| author          = Marc Alexander and Christian Kay<ref name=hte-cite>{{cite web|last1=Alexander|first1=Marc|last2=Kay|first2=Christian|title=How to Cite|url=http://historicalthesaurus.arts.gla.ac.uk/how-to-cite/|website=The Historical Thesaurus of English, version 4.2|publisher=University of Glasgow|accessdate=25 October 2014}}</ref>
| editor          = [[Christian Kay]], Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], Irené Wotherspoon, and Marc Alexander (editors)
| current_status  = Version 4.2, since September, 2014<ref name=hte-versions />
| footnotes       = 
}}
{{Infobox book
|name           = Historical Thesaurus of the Oxford English Dictionary : with additional material from "A Thesaurus of Old English"
|image          = Historical Thesaurus.jpg
|caption        = Print edition of version 1.0 of the ''Historical Thesaurus of English''<ref name="hte-versions" />
|alt            = Printed boxed set
|author         = Christian Kay, Jane Roberts, [[Michael Samuels (academic)|Michael Samuels]], and Irené Wotherspoon (editors)
|title_working  = Historical Thesaurus of English
|country        = Great Britain
|language       = English
|subject        = [[History of the English language]]
|genre          = [[Thesaurus|Thesauri]] 
|published      = 2009 ([[Oxford University Press]])
|pages          = 4,448
|awards         = Scottish Research Book of the Year Award, [[Saltire Society Literary Awards]], 2009
|isbn           = 978-0199208999
|oclc           = 318409912
|dewey          = 
|congress       =  PE1591 .H55 2009
}}

The '''''Historical Thesaurus of the Oxford English Dictionary''''' ('''''HTOED''''') is the print edition of the largest [[thesaurus]] in the world, the '''''Historical Thesaurus of English''''' ('''''HTE'''''), conceived and compiled by the English Language Department of the [[University of Glasgow]]. The ''HTE'' is a complete database of all the words in the second edition of [[Oxford English Dictionary|''The Oxford English Dictionary'']], arranged by [[semantic field]] and date. In this way, the ''HTE'' arranges the whole vocabulary of [[English language|English]], from the earliest written records in [[Old English language|Old English]] to the present, alongside types and dates of use. It is the first historical thesaurus to be compiled for any of the world's languages and contains 800,000 meanings for 600,000 words, within 230,000 categories, covering more than 920,000 words and meanings.<ref name="Woolcock">{{cite news| url=http://entertainment.timesonline.co.uk/tol/arts_and_entertainment/books/article6644646.ece | location=London | work=The Times | first=Nicola | last=Woolcock | title=After a 44-year labour of love worlds biggest thesaurus is born | date=2009-07-06}}{{subscription required}}</ref><ref>{{cite news|last1=Hitchings|first1=Henry|authorlink1=Henry Hitchings|title=Historical Thesaurus is a masterpiece worth waiting 40 years for|url=http://www.telegraph.co.uk/comment/personal-view/6413166/Historical-Thesaurus-is-a-masterpiece-worth-waiting-40-years-for.html|accessdate=25 October 2014|publisher=The Telegraph|date=23 October 2009|ref=Hitchings-2009|location=London}}</ref>  As the ''HTE'' website states, "in addition to providing hitherto unavailable information for linguistic and textual scholars, the ''Historical Thesaurus'' online is a rich resource for students of social and cultural history, showing how concepts developed through the words that refer to them."<ref name="hte">{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/ |title=Home page |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}</ref>

The ambitious project was announced at a 1965 meeting of the [[Philological Society]] by its originator, [[Michael Samuels (academic)|Michael Samuels]].<ref name=Crystal-2014>{{cite book|last1=Crystal|first1=David|authorlink1=David Crystal|title=Words in Time and Place: Exploring Language Through the ''Historical Thesaurus of the Oxford English Dictionary''|date=2014|publisher=Oxford University Press|location=Oxford|isbn=0199680477|page=vii}}</ref>  Work on the ''HTE'' started in 1965.

On 22 October 2009, after 44 years of work, version 1.0 was published as a two-volume set as ''HTOED''.<ref>{{cite news|url=http://news.bbc.co.uk/1/hi/england/oxfordshire/8136122.stm |title=UK &#124; England &#124; Oxfordshire &#124; Forty-year wait for new thesaurus |publisher=BBC News |date=2009-07-06 |accessdate=2010-04-15}}</ref> It consists of two slipcased hardcover volumes, totaling nearly 4,000 pages. The ''HTE'', released as version 4.2 in September 2014, is freely available online from the University of Glasgow.<ref name="hte-versions">{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/versions-and-changes/ |title=Versions of the Thesaurus |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-24}}</ref>

==Main sections==
The work is divided into three main sections: the External World, the Mind, and Society. These are broken down into successively narrower domains. The text eventually discriminates more than 236,000 categories.
The second order categories are:<ref>{{cite web |url=http://historicalthesaurus.arts.gla.ac.uk/classification/ |title=Classification |website=The Historical Thesaurus of English |publisher=University of Glasgow |accessdate=2014-10-22}} An oversize, one-page listing of all categories in top three tiers is available for download here.</ref>
{{col-begin-small}}
{{col-break}}
;I. The External World
 
# The Earth
# Life
# Physical sensibility
# Matter
# Existence
# Relative properties
# The Supernatural
{{col-break}} 
;II. The Mind
 
# Soul, spirit, mind
# Emotion/feeling
# Judgement, opinion
# Aesthetics
# Will/faculty of will
# Expectation
# Having/possession
# Languages
{{col-break}}
;III. Society
 
# Society/life in association with others
# Inhabiting/dwelling
# Relations between social groups
# Authority
# Law
# Education
# Religion
# Communications
# Travel/travelling
# Work / Serious occupation
# Leisure/The Arts
{{col-end}}

==References==
{{reflist}}

==External links==
* {{cite web|title=Search|url=http://historicalthesaurus.arts.gla.ac.uk/search/|website=The Historical Thesaurus of English|publisher=University of Glasgow}} {{open access}}
{{Dictionaries of English}}

[[Category:Thesauri]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Language histories]]
[[Category:History of the English language]]
<=====doc_Id=====>:756
<=====title=====>:
Default logic
<=====text=====>:

'''Default logic''' is a [[non-monotonic logic]] proposed by [[Raymond Reiter]] to formalize reasoning with default assumptions.

Default logic can express facts like “by default, something is true”; by contrast, standard logic can only express that something is true or that something is false. This is a problem because reasoning often involves facts that are true in the majority of cases but not always. A classical example is: “birds typically fly”. This rule can be expressed in standard logic either by “all birds fly”, which is inconsistent with the fact that penguins do not fly, or by “all birds that are not penguins and not ostriches and ... fly”, which requires all exceptions to the rule to be specified. Default logic aims at formalizing inference rules like this one without explicitly mentioning all their exceptions.

==Syntax of default logic==
A default theory is a pair <math>\langle W, D \rangle</math>. {{mvar|W}} is a set of logical formulae, called ''the background theory'', that formalize the facts that are known for sure. {{mvar|D}} is a set of ''default rules'', each one being of the form:

: <math>\frac{\mathrm{Prerequisite : Justification}_1, \dots , \mathrm{Justification}_n}{\mathrm{Conclusion}}</math>

According to this default, if we believe that {{math|Prerequisite}} is true, and each of <math>\mathrm{Justification}_i</math> is consistent with our current beliefs, we are led to believe that {{math|Conclusion}} is true.

The logical formulae in {{mvar|W}} and all formulae in a default were originally assumed to be [[first-order logic]] formulae, but they can potentially be formulae in an arbitrary formal logic. The case in which they are formulae in [[propositional logic]] is one of the most studied.

===Examples===
The default rule “birds typically fly” is formalized by the following default:

:<math>D = \left\{ \frac{\mathrm{Bird}(X) : \mathrm{Flies}(X)}{\mathrm{Flies}(X)} \right\}</math>

This rule means that, if {{mvar|X}} is a bird, and it can be assumed that it flies, then we can conclude that it flies. A background theory containing some facts about birds is the following one:

:<math>W = \{ \mathrm{Bird}(\mathrm{Condor}), \mathrm{Bird}(\mathrm{Penguin}), \neg \mathrm{Flies}(\mathrm{Penguin}), \mathrm{Flies}(\mathrm{Bee}) \}</math>.

According to this default rule, a condor flies because the precondition {{math|Bird(Condor)}} is true and the justification {{math|Flies(Condor)}} is not inconsistent with what is currently known. On the contrary, {{math|Bird(Penguin)}} does not allow concluding {{math|Flies(Penguin)}}: even if the precondition of the default {{math|Bird(Penguin)}} is true, the justification {{math|Flies(Penguin)}} is inconsistent with what is known.
From this background theory and this default, {{math|Bird(Bee)}} cannot be concluded because the default rule only allows deriving
{{math|Flies(''X'')}} from {{math|Bird(''X'')}}, but not vice versa. Deriving the antecedents of an inference rule from the consequences is a form of explanation of the consequences, and is the aim of [[abductive reasoning]].

A common default assumption is that what is not known to be true is believed to be false. This is known as the [[Closed World Assumption]], and is formalized in default logic using a default like the following one for every fact {{mvar|F}}.

: <math>\frac{:{\neg}F}{{\neg}F}</math>

For example, the computer language [[Prolog]] uses a sort of default assumption when dealing with negation: if a negative atom cannot be proved to be true, then it is assumed to be false.
Note, however, that Prolog uses the so-called [[negation as failure]]: when the interpreter has to evaluate the atom <math>\neg F</math>, it tries to prove that {{mvar|F}} is true, and conclude that <math>\neg F</math> is true if it fails. In default logic, instead, a default having <math>\neg F</math> as a justification can only be applied if <math>\neg F</math> is consistent with the current knowledge.

===Restrictions===

A default is categorical or prerequisite-free if it has no prerequisite (or, equivalently, its prerequisite is [[tautology (logic)|tautological]]). A default is normal if it has a single justification that is equivalent to its conclusion. A default is supernormal if it is both categorical and normal. A default is seminormal if all its justifications entail its conclusion. A default theory is called categorical, normal, supernormal, or seminormal if all defaults it contains are categorical, normal, supernormal, or seminormal, respectively.

==Semantics of default logic==

A default rule can be applied to a theory if its precondition is entailed by the theory and its justifications are all '''''consistent with''''' the theory.  The application of a default rule leads to the addition of its consequence to the theory.  Other default rules may then be applied to the resulting theory.  '''When the theory is such that no other default can be applied, the theory is called an extension of the default theory.'''  The default rules may be applied in different order, and this may lead to different extensions. The [[Nixon diamond]] example is a default theory with two extensions:

:<math>
\left\langle
\left\{
\frac{\mathrm{Republican}(X):\neg \mathrm{Pacifist}(X)}{\neg \mathrm{Pacifist}(X)},
\frac{\mathrm{Quaker}(X):\mathrm{Pacifist}(X)}{\mathrm{Pacifist}(X)}
\right\},
\left\{\mathrm{Republican}(\mathrm{Nixon}), \mathrm{Quaker}(\mathrm{Nixon})\right\}
\right\rangle
</math>

Since [[Richard Nixon|Nixon]] is both a [[American Republican|Republican]] and a [[Quaker]], both defaults can be applied. However, applying the first default leads to the conclusion that Nixon is not a pacifist, which makes the second default not applicable. In the same way, applying the second default we obtain that Nixon is a pacifist, thus making the first default not applicable. This particular default theory has therefore two extensions, one in which {{math|Pacifist(Nixon)}} is true, and one 
in which {{math|Pacifist(Nixon)}} is false. 

The original semantics of default logic was based on the [[Fixed point (mathematics)|fixed point]] of a function. The following is an equivalent algorithmic definition. If a default contains formulae with free variables, it is considered to represent the set of all defaults obtained by giving a value to all these variables. A default <math>\frac{\alpha:\beta_1,\ldots,\beta_n}{\gamma}</math> is applicable to a propositional theory {{mvar|T}} if <math>T \models \alpha</math> and
all theories <math>T \cup \{\beta_i\}</math> are consistent. The application of this default to {{mvar|T}} leads to the theory <math>T \cup \{\gamma\}</math>. An extension can be generated by applying the following algorithm:

 T=W           /* current theory */
 A=0           /* set of defaults applied so far */
 &nbsp;
               /* apply a sequence of defaults */
 '''while''' there is a default d that is not in A and is applicable to T
   add the consequence of d to T
   add d to A
 &nbsp;
               /* final consistency check */
 '''if''' 
   for every default d in A
     T is consistent with all justifications of d
 '''then'''
   output T

This algorithm is [[nondeterministic algorithm|non-deterministic]], as several defaults can alternatively be applied to a given theory {{mvar|T}}. In the Nixon diamond example, the application of the first default leads to a theory to which the second default cannot be applied and vice versa. As a result, two extensions are generated: one in which Nixon is a pacifist and one in which Nixon is not a pacifist.

The final check of consistency of the justifications of all defaults that have been applied implies that some theories do not have any extensions. In particular, this happens whenever this check fails for every possible sequence of applicable defaults. The following default theory has no extension:

:<math>
\left\langle 
\left\{
\frac{:A(b)}{\neg A(b)}
\right\},
\emptyset
\right\rangle
</math>

Since <math>A(b)</math> is consistent with the background theory, the default can be applied, thus leading to the conclusion that <math>A(b)</math> is false. This result however undermines the assumption that has been made for applying the first default. Consequently, this theory has no extensions.

In a normal default theory, all defaults are normal: each default has the form <math>\frac{\phi : \psi}{\psi}</math>. A normal default theory is guaranteed to have at least one extension. Furthermore, the extensions of a normal default theory are mutually inconsistent, i.e., inconsistent with each other.

===Entailment===

A default theory can have zero, one, or more extensions. [[Entailment]] of a formula from a default theory can be defined in two ways:

; Skeptical : a formula is entailed by a default theory if it is entailed by all its extensions;

; Credulous : a formula is entailed by a default theory if it is entailed by at least one of its extensions.

Thus, the Nixon diamond example theory has two extensions, one in which Nixon is a pacifist and one in which he is not a pacifist. Consequently, neither {{math|Pacifist(Nixon)}} nor {{math|&not;Pacifist(Nixon)}} are skeptically entailed, while both of them are credulously entailed. As this example shows, the credulous consequences of a default theory may be inconsistent with each other.

===Alternative default inference rules===
<!-- these are the alternative default inference rules that are based on the same original syntax of default logic -->

The following alternative inference rules for default logic are all based on the same syntax as the original system.

; Justified: differs from the original one in that a default is not applied if thereby the set {{mvar|T}} becomes [[inconsistent]] with a justification of an applied default;

; Concise: a default is applied only if its consequence is not already entailed by {{mvar|T}} (the exact definition is more complicated than this one; this is only the main idea behind it);

; Constrained: a default is applied only if the set composed of the background theory, the justifications of all applied defaults, and the consequences of all applied defaults (including this one) is consistent;

; Rational: similar to constrained default logic, but the consequence of the default to add is not considered in the consistency check;

; Cautious: defaults that can be applied but are conflicting with each other (like the ones of the Nixon diamond example) are not applied.

The justified and constrained versions of the inference rule assign at least an extension to every default theory.

==Variants of default logic==
<!-- these are the variants of default logic that differ from the original one both in syntax and semantics -->

The following variants of default logic differ from the original one on both syntax and semantics.

; Assertional variants : An assertion is a pair <math>\langle p: \{r_1,\ldots,r_n\} \rangle</math> composed of a formula and a set of formulae. Such a pair indicates that {{mvar|p}} is true while the formulae <math>r_1,\ldots,r_n</math> have been assumed consistent to prove that {{mvar|p}} is true. An assertional default theory is composed of an assertional theory (a set of assertional formulae) called the background theory and a set of defaults defined as in the original syntax. Whenever a default is applied to an assertional theory, the pair composed of its consequence and its set of justifications is added to the theory. The following semantics use assertional theories:

*Cumulative default logic
*Commitment to assumptions default logic
*Quasi-default logic

; Weak extensions : rather than checking whether the preconditions are valid in the theory composed of the background theory and the consequences of the applied defaults, the preconditions are checked for validity in the extension that will be generated; in other words, the algorithm for generating extensions starts by guessing a theory and using it in place of the background theory; what results from the process of extension generation is actually an extension only if it is equivalent to the theory guessed at the beginning. This variant of default logic is related in principle to [[autoepistemic logic]], where a theory <math>\Box x \rightarrow x</math> has the model in which {{mvar|x}} is true just because, assuming <math>\Box x</math> true, the formula <math>\Box x \rightarrow x</math> supports the initial assumption.

; Disjunctive default logic : the consequence of a default is a set of formulae instead of a single formula. Whenever the default is applied, at least one of its consequences is nondeterministically chosen and made true.

; Priorities on defaults : the relative priority of defaults can be explicitly specified; among the defaults that are applicable to a theory, only one of the most preferred ones can be applied. Some semantics of default logic do not require priorities to be explicitly specified; rather, more specific defaults (those that are applicable in fewer cases) are preferred over less specific ones.

; Statistical variant : a statistical default is a default with an attached upper bound on its frequency of error; in other words, the default is assumed to be an incorrect inference rule in at most that fraction of times it is applied.

==Translations==

Default theories can be translated into theories in other logics and vice versa. The following conditions on translations have been considered:

; Consequence-Preserving : the original and the translated theories have the same (propositional) consequences;

; Faithful : this condition only makes sense when translating between two variants of default logic or between default logic and a logic in which a concept similar to extension exists, e.g., models in modal logic; a translation is faithful if there exists a mapping (typically, a bijection) between the extensions (or models) of the original and translated theories;

; Modular : a translation from default logic to another logic is modular if the defaults and the background theory can be translated separately; moreover, the addition of formulae to the background theory only leads to adding the new formulae to the result of the translation;

; Same-Alphabet : the original and translated theories are built on the same alphabet;

; Polynomial : the running time of the translation or the size of the generated theory are required to be polynomial in the size of the original theory.

Translations are typically required to be faithful or at
least consequence-preserving, while the conditions of
modularity and same alphabet are sometimes ignored.

The translatability between propositional default logic and
the following logics have been studied:

* classical propositional logic;
* autoepistemic logic;
* propositional default logic restricted to seminormal theories;
* alternative semantics of default logic;
* circumscription.

Translations exist or not depending on which conditions are imposed. Translations from propositional default logic to classical propositional logic cannot always generate a polynomially sized propositional theory, unless the [[polynomial hierarchy]] collapses. Translations to autoepistemic logic exists or not depending on whether modularity or the use of the same alphabet is required.

==Complexity==

The [[Analysis of algorithms|computational complexity]] of the following problems about default logic is known:

; Existence of extensions : deciding whether a propositional default theory has at least one extension is <math>\Sigma^P_2</math>-complete;

; Skeptical entailment : deciding whether a propositional default theory skeptically entails a [[propositional formula]] is <math>\Pi^P_2</math>-complete;

; Credulous entailment : deciding whether a propositional default theory credulously entails a propositional formula is <math>\Sigma^P_2</math>-complete;

; Extension checking : deciding whether a propositional formula is equivalent to an extension of a propositional default theory is <math>\Delta^{P[log]}_2</math>-complete;

; Model checking : deciding whether a propositional interpretation is a model of an extension of a propositional default theory is <math>\Sigma^P_2</math>-complete.

==Implementations==

Three systems implementing default logics are 
[ftp://www.cs.engr.uky.edu/cs/manuscripts/deres.ps DeReS],
[http://www.cs.uni-potsdam.de/wv/xray/ XRay] and
[http://www.info.univ-angers.fr/pub/stephan/Research/GADEL/GADEL_prolog.html GADeL]
<!-- algorithms? other implemented systems? -->

==See also==
* [[Answer set programming]]
* [[Defeasible logic]]
* [[Non-monotonic logic]]

==References==
* G. Antoniou (1999). A tutorial on default logics. ''ACM Computing Surveys'', 31(4):337-359.
* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (2000). Space efficiency of propositional knowledge representation formalisms. ''Journal of Artificial Intelligence Research'', 13:1-31.
* P. Cholewinski, V. Marek, and M. Truszczynski (1996). Default reasoning system DeReS. In ''Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR'96)'', pages 518-528.
* J. Delgrande and T. Schaub (2003). On the relation between Reiter's default logic and its (major) variants. In ''Seventh European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty (ECSQARU 2003)'', pages 452-463.
* J. P. Delgrande, T. Schaub, and W. K. Jackson (1994). Alternative approaches to default logic. ''Artificial Intelligence'', 70:167-237.
* G. Gottlob (1992). Complexity results for nonmonotonic logics. ''Journal of Logic and Computation'', 2:397-425.
* G. Gottlob (1995). Translating default logic into standard autoepistemic logic. ''Journal of the ACM'', 42:711-740.
* T. Imielinski (1987). Results on translating defaults to circumscription. ''Artificial Intelligence'', 32:131-146.
* T. Janhunen (1998). On the intertranslatability of autoepistemic, default and priority logics, and parallel circumscription. In ''Proceedings of the Sixth European Workshop on Logics in Artificial Intelligence (JELIA'98)'', pages 216-232.
* T. Janhunen (2003). Evaluating the effect of semi-normality on the expressiveness of defaults. ''Artificial Intelligence'', 144:233-250.
* H. E. Kyburg and C-M. Teng (2006). Nonmonotonic Logic and Statistical Inference. ''Computational Intelligence'', 22(1): 26-51.
* P. Liberatore and M. Schaerf (1998). The complexity of model checking for propositional default logics. In ''Proceedings of the Thirteenth European Conference on Artificial Intelligence (ECAI'98)'', pages 18–22.
* W. Lukaszewicz (1988). Considerations on default logic: an alternative approach. ''Computational Intelligence'', 4(1):1-16.
* W. Marek and M. Truszczynski (1993). ''Nonmonotonic Logics: Context-Dependent Reasoning''. Springer.
* A. Mikitiuk and M. Truszczynski (1995). Constrained and rational default logics. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1509-1517.
* P. Nicolas, F. Saubion and I. Stéphan (2001). Heuristics for a Default Logic Reasoning System. ''International Journal on Artificial Intelligence Tools'', 10(4):503-523.
* R. Reiter (1980). A logic for default reasoning. ''Artificial Intelligence'', 13:81-132.
* T. Schaub, S. Brüning, and P. Nicolas (1996). XRay: A prolog technology theorem prover for default reasoning: A system description. In ''Proceedings of the Thirteenth International Conference on Automated Deduction (CADE'96)'', pages 293-297.
* G. Wheeler (2004). A resource bounded default logic. In ''Proceedings of the 10th International Workshop on Non-Monotonic Reasoning (NMR-04)'', Whistler, British Columbia, 416-422.
* G. Wheeler and C. Damasio (2004). An Implementation of Statistical Default Logic. In ''Proceedings of the 9th European Conference on Logics in Artificial Intelligence (JELIA 2004)'', LNCS Series, Springer, pages 121-133.

==External links==
* Schmidt, Charles F. [http://www.rci.rutgers.edu/~cfs/472_html/Logic_KR/DefaultTheory.html RCI.Rutgers.edu], Default Logic. Retrieved August 10, 2004.
* Ramsay, Allan (1999). [http://www.ccl.umist.ac.uk/teaching/material/5005/node33.html UMIST.ac.uk], Default Logic. Retrieved August 10, 2004.
* [http://plato.stanford.edu/entries/reasoning-defeasible/ Stanford.edu], Defeasible reasoning, [[Stanford Encyclopedia of Philosophy]].

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Logical calculi]]
[[Category:Non-classical logic]]
<=====doc_Id=====>:759
<=====title=====>:
Event calculus
<=====text=====>:
The '''event calculus''' is a [[logic]]al language for representing and reasoning about events and their effects first presented by [[Robert Kowalski]] and [[Marek Sergot]] in 1986.
It was extended by [[Murray Shanahan]] and [[Rob Miller (Computer Scientist)|Rob Miller]] in the 1990s. Similar to other languages for reasoning about change, the event calculus represents the effects of [[Action (artificial intelligence)|action]]s on [[fluent (artificial intelligence)|fluent]]s. However, [[Event (computing)|event]]s can also be external to the system. In the event calculus, one can specify the value of fluents at some given time points, the events that take place at given time points, and  their effects.

==Fluents and events==

In the event calculus, fluents are [[Reification (knowledge representation)|reified]]. This means that they are not formalized by means of [[Predicate (mathematics)|predicate]]s but by means of [[function (mathematics)|function]]s. A separate predicate <math>HoldsAt</math> is used to tell which fluents hold at a given time point. For example, <math>HoldsAt(on(box,table),t)</math> means that the box is on the table at time <math>t</math>; in this formula, <math>HoldsAt</math> is a predicate while <math>on</math> is a function.

Events are also represented as terms. The effects of events are given using the predicates <math>Initiates</math> and <math>Terminates</math>. In particular, <math>Initiates(e,f,t)</math> means that,
if the event represented by the term <math>e</math> is executed at time <math>t</math>,
then the fluent <math>f</math> will be true after <math>t</math>.
The <math>Terminates</math> predicate has a similar meaning, with the only difference 
being that <math>f</math> will be false and not true after <math>t</math>.

==Domain-independent axioms==

Like other languages for representing actions, the event calculus formalizes the correct evolution of the fluent via formulae telling the value of each fluent after an arbitrary action has been performed. The event calculus solves the [[frame problem]] in a way that is similar to the [[successor state axiom]]s of the [[situation calculus]]: a fluent is true at time <math>t</math> if and only if it has been made true in the past and has not been made false in the meantime.
 
:<math>HoldsAt(f,t) \leftarrow
[Happens(e,t_1) \wedge Initiates(e,f,t_1) 
\wedge (t_1<t) \wedge \neg Clipped(t_1,f,t)]</math>

This formula means that the fluent represented by the term <math>f</math> is true at time <math>t</math> if:

# an event <math>e</math> has taken place: <math>Happens(e,t_1)</math>;
# this took place in the past: <math>t_1<t</math>;
# this event has the fluent <math>f</math> as an effect: <math>Initiates(e,f,t_1)</math>; 
# the fluent has not been made false in the meantime: <math>Clipped(t_1,f,t)</math>

A similar formula is used to formalize the opposite case in which a fluent is false at a given time. Other formulae are also needed for correctly formalizing fluents before they have been effects of an event. These formulae are similar to the above, but <math>Happens(e,t_1) \wedge Initiates(e,f,t_1)</math> is replaced by <math>HoldsAt(f,t_1)</math>.

The <math>Clipped</math> predicate, stating that a fluent has been made false during an interval, can be axiomatized, or simply taken as a shorthand, as follows:

:<math>Clipped(t_1,f,t_2) \equiv
\exists e,t 
[Happens(e,t) \wedge (t_1 \leq t < t_2) \wedge Terminates(e,f,t)]</math>

==Domain-dependent axioms==

The axioms above relate the value of the predicates <math>HoldsAt</math>, <math>Initiates</math> and <math>Terminates</math>, but do not specify which fluents are known to be true and which events actually make fluents true or false. This is done by using a set of domain-dependent axioms. The known values of fluents are stated as simple literals <math>HoldsAt(f,t)</math>. The effects of events are stated by formulae relating the effects of events with their preconditions. For example, if the event <math>open</math> makes the fluent <math>isopen</math> true, but only if <math>haskey</math> is currently true, the corresponding formula in the event calculus is:

:<math>Initiates(e,f,t) \equiv
[ e=open \wedge f=isopen \wedge HoldsAt(haskey, t)] \vee \cdots
</math>

The right-hand expression of this equivalence is composed of a disjunction: for each event and fluent that can be made true by the event, there is a disjunct saying that <math>e</math> is actually that event, that <math>f</math> is actually that fluent, and that the precondition of the event is met.

The formula above specifies the [[truth value]] of <math>Initiates(e,f,t)</math> for every possible event and fluent. As a result, all effects of all events have to be combined in a single formulae. This is a problem, because the addition of a new event requires modifying an existing formula rather than adding new ones. This problem can be solved by the application of [[Circumscription (logic)|circumscription]] to a set of formulae each specifying one effect of one event:

: <math>Initiates(open, isopen, t) \leftarrow HoldsAt(haskey, t)</math>
: <math>Initiates(break, isopen, t) \leftarrow HoldsAt(hashammer, t)</math>
: <math>Initiates(break, broken, t) \leftarrow HoldsAt(hashammer, t)</math>

These formulae are simpler than the formula above, because each effect of each event can be specified separately. The single formula telling which events <math>e</math> and fluents <math>f</math> make <math>Initiates(e,f,t)</math> true has been replaced by a set of smaller formulae, each one telling the effect of an event on a fluent.
 
However, these formulae are not equivalent to the formula above. Indeed, they only specify sufficient conditions for <math>Initiates(e,f,t)</math> to be true, which should be completed by the fact that <math>Initiates</math> is false in all other cases. This fact can be formalized by simply circumscribing the predicate <math>Initiates</math> in the formula above. It is important to note that this circumscription is done only on the formulae specifying <math>Initiates</math> and not on the domain-independent axioms. The predicate <math>Terminates</math> can be specified in the same way <math>Initiates</math> is.

A similar approach can be taken for the <math>Happens</math> predicate. The evaluation of this predicate can be enforced by formulae specifying not only when it is true and when it is false:

:<math>Happens(e,t) \equiv
(e=open \wedge t=0) \vee (e=exit \wedge t=1) \vee \cdots</math>

Circumscription can simplify this specification, as only necessary conditions can be specified:

:<math>Happens(open, 0)</math>
:<math>Happens(exit, 1)</math>

Circumscribing the predicate <math>Happens</math>, this predicate will be false at all points in which it is not explicitly specified to be true. This circumscription has to be done separately from the circumscription of the other formulae. In other words, if <math>F</math> is the set of formulae of the kind <math>Initiates(e,f,t) \leftarrow \cdots</math>, <math>G</math> is the set of formulae <math>Happens(e, t)</math>, and <math>H</math> are the domain independent axioms, the correct formulation of the domain is:

:<math>Circ(F; Initiates, Terminates) \wedge
Circ(G; Happens) \wedge H</math>

==The event calculus as a logic program==

The event calculus was originally formulated as a set of [[Horn clauses]] augmented with [[negation as failure]] and could be run as a [[Prolog]] program. 
In fact, circumscription is one of the several semantics that can be given to negation as failure, and is closely related to the completion semantics (in which "if" is interpreted as "if and only if" &mdash; see [[logic programming]]).

==Extensions and applications==

The original event calculus paper of Kowalski and Sergot focused on applications to database updates and narratives. Extensions of the event 
calculus can also formalize non-deterministic actions, concurrent actions, actions with delayed effects, gradual changes, actions with duration, continuous change, and non-inertial fluents.

Kave Eshghi showed how the event calculus can be used for planning, using [[Abduction (logic)|abduction]] to generate hypothetical events in [[Abductive Logic Programming|abductive logic programming]]. Van Lambalgen and Hamm showed how the event calculus can also be used to give an algorithmic semantics to tense and aspect in natural language using constraint logic programming.

==Reasoning tools==

In addition to Prolog and its variants, several other tools for reasoning using the event calculus are also available:
* [http://www.doc.ic.ac.uk/~mpsha/planners.html Abductive Event Calculus Planners]
* [http://decreasoner.sourceforge.net/ Discrete Event Calculus Reasoner]
* [http://reasoning.eas.asu.edu/ecasp/ Event Calculus Answer Set Programming]
* [https://www.inf.unibz.it/~montali/tools.html Reactive Event Calculus]

==See also==

* [[First-order logic]]
* [[Frame problem]]
* [[Situation calculus]]

==References==
* Brandano, S. (2001) "[http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?isnumber=20130&arnumber=930691&count=35&index=2 The Event Calculus Assessed,]" ''IEEE TIME Symposium'': 7-12.
* Eshghi, K. (1988) "Abductive Planning with Event Calculus," ''ICLP/SLP'': 562-79.
* Kowalski, R. (1992) "Database updates in the event calculus," ''Journal of Logic Programming 12 (162)'': 121-46.
* -------- and M. Sergot (1986) "[http://www.doc.ic.ac.uk/~rak/papers/event%20calculus.pdf A Logic-Based Calculus of Events,]" ''New Generation Computing 4'': 67–95.
* -------- and F. Sadri (1995) "Variants of the Event Calculus," ''ICLP'': 67-81.
* Miller, R., and M. Shanahan (1999) "[http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html The event-calculus in classical logic — alternative axiomatizations,]" ''[[Electronic Transactions on Artificial Intelligence]]'' 3(1): 77-105.
* Mueller, Erik T. (2015). ''Commonsense Reasoning: An Event Calculus Based Approach (2nd Ed.)''. Waltham, MA: Morgan Kaufmann/Elsevier. ISBN 978-0128014165. (Guide to using the event calculus)
* Shanahan, M. (1997) ''Solving the frame problem: A mathematical investigation of the common sense law of inertia''. MIT Press.
* -------- (1999) "[http://www.springerlink.com/content/1bxk8gd0n6pajxbq/?p=8f3428a89bad4589a949d74b6f0ec98d&pi=0 The Event Calculus Explained,]" Springer Verlag, LNAI (1600): 409-30.
* Van Lambalgen, M., and F. Hamm (2005) ''The proper treatment of events''. Oxford and Boston: Blackwell Publishing.

[[Category:1986 introductions]]
[[Category:Logic in computer science]]
[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Logical calculi]]
<=====doc_Id=====>:762
<=====title=====>:
SERVQUAL
<=====text=====>:
{{marketing}}

'''SERVQUAL''' is a multi-dimensional research instrument, designed to capture consumer expectations and perceptions of a service along the five dimensions that are believed to represent service quality. SERVQUAL is built on the expectancy-disconfirmation paradigm, which in simple terms means that service quality is understood as the extent to which consumers' pre-consumption expectations of quality are confirmed or disconfirmed by their actual perceptions of the service experience. When the SERVQUAL questionnaire was first published in 1988 by a team of academic researchers, A. Parasurman, [[Valarie Zeithaml]] and [[Leonard Berry (professor)|Leonard L. Berry]]  to measure quality in the service sector,<ref>Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vo. 62,  no. 1, 1988, pp 12-40 <online:https://www.researchgate.net/publication/225083802_SERVQUAL_A_multiple-_Item_Scale_for_measuring_consumer_perceptions_of_service_quality></ref> it represented a breakthrough in the measurement methods used for service quality research. The diagnostic value of the instrument is supported by the ''model of service quality'' which forms the conceptual framework for the development of the scale (i.e. instrument or questionnaire). The instrument has been widely applied in a variety of contexts and cultural settings and found to be relatively robust. It has become the dominant measurement scale in the area of service quality. In spite of the long-standing interest in SERVQUAL and its myriad of context-specific applications, it has attracted some criticism from researchers.

==The SERVQUAL instrument==

[[File:Measuring service quality using SERVQUAL model (Kumar et al, 2009).png|thumb|The five dimensions of service quality)]]

SERVQUAL is a multidimensional instrument (i.e. questionnaire or measurement scale) designed to measure service quality by capturing respondents’ expectations and perceptions along the five dimensions of service quality.<ref>Parasuraman, A., Berry, L.L. and Zeithaml, V.A.,  “Refinement and Reassessment of the SERVQUAL scale,” ''Journal of Retailing,'' Vol. 67, no. 4, 1991, pp 57-67</ref>  The questionnaire consists of matched pairs of items; 22 expectation items and 22 perceptions items, organised into five dimensions which are believed to  align with the consumer’s mental map of service quality dimensions. Both the expectations component and the perceptions component of the questionnaire consist a total of 22 items, comprising 4 items to capture tangibles, 5 items to capture reliabiility, 4 items for responsiveness, 5 items for assurance and 5 items to capture empathy.<ref>Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, p. 25</ref> The questionnaire is designed to be administered in a face-to-face interview and requires a moderate to large size sample for statistical reliability. In practice, it is customary to add additional items such as the respondent's demographics, prior experience with the brand or category and behavioural intentions (intention to revisit/ repurchase, loyalty intentions and propensity to give word-of-mouth referrals). Thus, the final questionnaire may have up to 60 items and typically takes at least one hour, per respondent, to administer. The length of the questionnaire combined with sample size requirements contribute to substantial costs in administration and data analysis.


{| class="wikitable"
|+ Summary of SERVQUAL items <ref>Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, p. 22, 25 and 29</ref>
|-
! Dimension
! No. of Items in Questionnaire
! Definition
|-
| '''Reliability'''
| colspan="1" style="text-align: center;" | 5 
| The ability to perform the promised service dependably and accurately 
|-
| '''Assurance'''
| colspan="1" style="text-align: center;" | 5 
| The knowledge and courtesy of  employees and their ability to convey trust and confidence
|-
| '''Tangibles'''
| colspan="1" style="text-align: center;" | 4 
| The appearance of physical facilities, equipment, personnel and communication materials 
|-
| '''Empathy'''
| colspan="1" style="text-align: center;" | 5 
| The provision of caring, individualized attention to customer
|-
| '''Responsiveness'''
| colspan="1" style="text-align: center;" | 4 
| The willingness to help customers and to provide prompt service
|}

The instrument was developed over a five year period; was tested, pre-tested and refined before appearing in its final form. The instrument's developers, claim that it is a highly reliable and valid instrument.<ref>Zeithaml, V., Parasuraman, A. and Berry, L.L., ''Delivering Service Quality: Balancing Customer Perceptions and Expectations,'' N.Y., The Free Press, 1990</ref> Certainly, it has been widely used and adapted in service quality research for numerous industries and various geographic regions. In application, many researchers are forced to make minor modifications to the instrument as necessary for context-specific applications. Some researchers label their revised instruments with innovative labels such as EDUQUAL (educational context),<ref>Mahapatra, S.S. and Khan, M.S., "A Methodology for Evalution of Service Quality Using Neural Networks," in ''Proceedings of the International Conference on Global Manufacturing and Innovation,' July 27–29, 2006</ref> HEALTHQUAL (hospital context) <ref>Lee, D., "HEALTHQUAL: a multi-item scale for assessing healthcare service quality," Service Business, 2016; pp 1-26, doi:10.1007/s11628-016-0317-2</ref> and ARTSQUAL (art museum).<ref>Higgs, B., Polonsky M.J. and Hollick, M., “Measuring Expectations: Pre and Post Consumption: Does It Matter?” ''Journal of Retailing and Consumer Services'', vol. 12, no. 1, 2005</ref>


{| class="wikitable"
|+ Examples of matched pairs of items in the SERVQUAL questionnaire <ref>Based on Parasuraman, A, Ziethaml, V. and Berry, L.L., "SERVQUAL: A Multiple- Item Scale for Measuring Consumer Perceptions of Service Quality' ''Journal of Retailing,'' Vol. 62,  no. 1, 1988, [Appendix: SERVQUAL questionnaire, pp 37-40</ref>
|-
! Dimension
! Sample expectations item
! Sample perceptions item
|-
| '''Reliability'''
| When excellent telephone companies promise to do something by a certain time, they do so
| XYZ company provides it services at the promised time 
|-
| '''Assurance'''
| The behaviour of employees in excellent banks will instill confidence in customers
| The behaviour of employees in the XYZ bank instils confidence in you.
|-
| '''Tangibles'''
| Excellent telephone companies will have modern looking equipment
| XYZ company has modern looking equipment
|-
| '''Empathy'''
| Excellent banks will have operating hours convenient to customers
| XYZ bank has convenient operating hours
|-
| '''Responsiveness'''
| Employees of excellent telephone companies will never be too busy to help a customer
| XYZ employees are never too busy to help you
|}

The SERVQUAL questionnaire has been described as "the most popular standardized questionnaire to measure service quality." <ref>Caruanaa,A., Ewing, M.T and Ramaseshanc, B., "Assessment of the Three-Column Format SERVQUAL: An Experimental Approach," ''Journal of Business Research,'' Vol. 49, no. 1, July 2000, pp 57–65</ref> It is widely used by service firms, most often in conjunction with other measures of service quality and customer satisfaction. The SERVQUAL instrument was developed as part of a broader conceptualisation of how customers understand service quality. This conceptualisation is known as the ''model of service quality'' or more popularly as the ''gaps model.''

==The model of service quality==

The model of service quality, popularly known as the ''gaps model'' was developed by a group of American authors, A. Parasuraman, [[Valarie Zeithaml|Valarie A. Zeithaml]] and [[Leonard Berry (professor)|Len Berry]], in a systematic research program carrie out between 1983 and 1988. The model identifies the principal dimensions (or components) of service quality; proposes a scale for measuring service quality (SERVQUAL) and suggests possible causes of service quality problems. The model's developers originally identified [[SERVQUAL#Determinants|ten dimensions of service quality]], but after testing and retesting, some of the dimensions were found to be autocorrelated and the total number of dimensions was reduced to five, namely - reliability, assurance, tangibles, empathy and responsiveness.  These five dimensions are thought to represent the dimensions of service quality across a range of industries and settings. <ref>Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," ''Journal of Marketing,'' Vol. 52, No. 2, 1988, pp. 35-48 </ref> Among students of marketing, the memnonic,  '''RATER''', an acronym formed from the first letter of each of the five dimensions is often used as an aid to recall.

[[File:Servqual.jpg|thumb|A simplified model of service quality]]

Businesses use the SERVQUAL instrument (i.e. questionnaire) to measure potential service quality problems and the model of service quality to help diagnose possible causes of the problem. The model of service quality is built on the ''expectancy-confirmation paradigm'' which suggests that consumers perceive quality in terms of their perceptions of how well a given service delivery meets their expectations of that delivery.<ref>Oliver, R.L., Balakrishnan, P.V. S. and Barry, B., "Outcome Satisfaction in Negotiation: A Test of Expectancy Disconfirmation," ''Organizational Behavior and Human Decision Processes,'' Vol. 60, no. 2, 1994, Pages 252-275</ref> Thus, service quality can be conceptualised as a simple equation:

'''SQ = P- E'''
: where;

: '''SQ''' is service quality
: '''P''' is the individual's perceptions of given service delivery
: '''E''' is the individual's expectations of a given service delivery

When customer expectations are greater than their perceptions of received delivery, service quality is deemed low. When perceptions exceed expectations then service quality is high. The model of service quality identifies five gaps that may cause customers to experience poor service quality. In this model, gap 5 is the service quality gap and is the ''only'' gap that can be directly measured. In other words, the SERVQUAL instrument was specifically designed to capture gap 5. In contrast, Gaps 1-4 cannot be measured, but have diagnostic value.


{| class="wikitable"
|+ Summary of Gaps with Diagnostic Indications <ref>Based on Zeithaml, V.A., Berry, L.L. and Parasuraman, A., "Communication and Control Processes in the Delivery of Service Quality," ''Journal of Marketing,'' Vol. 52, No. 2, 1988, pp. 35-48 </ref>
|-
! '''Gap'''
! '''Brief description'''
! '''Probable Causes
|-
| '''Gap 1'''
The Knowledge Gap
| Difference between the target market’s expected service and management’s perceptions of the target market’s expected service
| 
* Insufficient marketing research
* Inadequate upward communications
* Too many layers of management
|-
| '''Gap 2'''
The standards Gap 
| Difference between management’s perceptions of customer expectations and the translation into service procedures and specifications
| 
* Lack of management commitment to service quality
* Employee perceptions of infeasibility
* Inadequate goal setting
* Inadequate task standardisation
|-
|'''Gap 3''' 
The Delivery Gap
| Difference between service quality specifications and the service actually delivered
| 
* Technical breakdowns or malfunctions
*  Role conflict/ ambiguity
*  Lack of perceived control
* Poor employee-job fit
* Poor technology- fit
* Poor supervision or training
|-
| '''Gap 4''' 
The Communications Gap
| Difference between service delivery intentions and what is communicated to the customer
| 
*Lack of horizontal communications
* Poor communication with advertising agency
* Inadequate communications between sales and operations
* Differences in policies and procedures across branches or divisions of an entity
* Propensity to overpromise
|}

== Development of the model ==

The development of the model of service quality involved a systematic research undertaking which began in 1983 and after various refinements resulted in the publication of the SERVQUAL instrument in 1988.<ref>Parasuraman, A., Berry, L.L.,  Zeithaml, V. A., "Understanding Customer Expectations of Service," ''Sloan Management Review,'' Vol. 32, no. 3, 1991, p. 39</ref> The model's developers began with an exhaustive literature search in order to identify items that were believed to impact on perceived service quality. This initial search identified some 100 items which were used in the first rounds of consumer testing. Prelimiary data analysis, using a data reduction technique known as [[factor analysis]] (also known as [[principal components analysis]]) revealed that these items loaded onto ten dimensions (or components) of service quality. The initial ten dimensions that were believed to represent service quality were:

# '''[[competence (human resources)|Competence]]''' is the possession of the required skills and knowledge to perform the service. For example, there may be competence in the knowledge and skill of contact personnel, knowledge and skill of operational support personnel and research capabilities of the organization.
# '''Courtesy''' is the consideration for the customer's property and a clean and neat appearance of contact personnel, manifesting as politeness, respect, and friendliness.
# '''Credibility''' includes factors such as trustworthiness, belief and honesty. It involves having the customer's best interests at prime position. It may be influenced by company name, company reputation and the personal characteristics of the contact personnel.
# '''Security''' enables the customer to feel free from danger, risk or doubt including physical safety, financial security and confidentiality.
# '''Access''' is approachability and ease of contact. For example, convenient office operation hours and locations.
# '''Communication''' means both informing customers in a language they are able to understand and also listening to customers. A company may need to adjust its language for the varying needs of its customers. Information might include for example, explanation of the service and its cost, the relationship between services and costs and assurances as to the way any problems are effectively managed.
# '''Knowing the customer''' means making an effort to understand the customer's individual needs, providing individualized attention, recognizing the customer when they arrive and so on. This in turn helps to delight the customers by rising above their expectations. 
# '''Tangibles''' are the physical evidence of the service, for instance, the  appearance of the physical facilities, tools and equipment used to provide the service; the appearance of personnel and communication materials and the presence of other customers in the service facility.
# '''Reliability'''  is the ability to perform the promised service in a dependable and accurate manner. The service is performed correctly on the first occasion, the accounting is correct, records are up to date and schedules are kept.
# '''Responsiveness''' is the readiness and willingness of employees to help customers by providing prompt timely services, for example, mailing a transaction slip immediately or setting up appointments quickly.

Further testing suggested that some of the ten preliminary dimensions of service quality were closely related or autocorrelated. Thus the ten initial dimensions were reduced and the labels amended to accurately reflect the revised dimensions. By the early 1990s, the authors had refined the model to five factors which in testing, appear to be relatively stable and robust.

# '''Reliability:''' the ability to perform the promised service dependably and accurately
# '''Assurance:''' the knowledge and courtesy of employees and their ability to convey trust and confidence
# '''Tangibles:''' the appearance of physical facilities, equipment, personnel and communication materials
# '''Empathy:''' the provision of caring, individualized attention to customers
# '''Responsiveness:''' the willingness to help customers and to provide prompt service

These are the five dimensions of service quality that form the basis of the individual items in the SERVQUAL research instrument (questionnaire). The acronym RATER, is often used to help students of marketing remember the five dimensions of quality explicitly mentioned in the research instrument.

Nyeck, Morales, Ladhari, and Pons (2002) stated the SERVQUAL measuring tool “appears to remain the most complete attempt to conceptualize and measure service quality” (p.&nbsp;101). The SERVQUAL measuring tool has been used by many researchers across a wide range of service industries and contexts, such as healthcare, banking, financial services, and education (Nyeck, Morales, Ladhari, & Pons, 2002).

== Criticisms of SERVQUAL and the model of service quality ==

Although the SERVQUAL instrument has been widely applied in a variety of industry and cross-cultural contexts, there are many criticisms of the approach. Francis Buttle published one of the most comprehensive criticisms of the model of service quality and the associated SERVQUAL instrument in 1996 in which both operational and theoretical concerns were identified.<ref>Buttle, F., “SERVQUAL: Review, Critique, Research Agenda,"  ''European Journal of Marketing,'' Vol. 30, no.  1, pp. 8-32 1996</ref> Some of the more important criticisms include:

: ''Face validity'': The model of service quality has its roots in the expectancy-disconfimation paradigm that informs customer satisfaction.<ref>Oliver, R.L., ''Satisfaction: A Behavioural Perspective on the Consumer,'' Boston, MA, Irwin McGraw-Hill, 1996</ref> A number of researchers have argued that the research instrument actually captures ''satisfaction'' rather than ''service quality''.<ref>Souca, Ma. L., "SERVQUAL - Thirty years of research on service quality with implications for customer satisfaction," in ''Marketing - from Information to Decision,'' [Proceedings of the International Conference], Cluj-Napoca: Babes Bolyai University, 2011, pp 420 -429</ref> Other researchers have questioned the validity of conceptualising service quality as a gap.<ref>van Dyke, T.P.,   Kappelman, L.A. and Prybutok, V.R.,"Measuring Information Systems Service Quality: Concerns on the Use of the SERVQUAL Questionnaire," ''MIS Quarterly,'' Vol. 21, No. 2, 1997, pp. 195-208, <Online:  http://www.jstor.org/stable/249419></ref>

: ''Construct validity'': The model's developers tested and retested the SERVQUAL scale for reliability and validity. However, at the same time the model's developers recommended that applied use of the instrument should modify or adapt the for specific contexts. Any attempt to adapt or modify the scale will have implications for the validity of items with implications for the validity of the dimensions of reliability, assurance, tangibles, empathy and repsonsiveness.<ref>Smith, A.M., "Measuring Service Quality: Is SERVQUAL now redundant?  ''Journal of Marketing Management,'' [Special Issue: Marketing in the Services Sector],  Vol 11, no. 1, 1995, pp 257-276</ref>
 
: ''Ambiguity of expectations construct'': SERVQUAL is designed to be administered after respondents have experienced a service. They are therefore asked to ''recall'' their pre-experience expectations. However,  recall is not always accurate, raising concerns about whether the research design accurately captures true pre-consumption expectations. In addition, studies show that expectations actually change over time. Consumers are continually modifying their expectations as they gain experience with a product category or brand.<ref>Parasuraman, A.; Berry, Leonard L.; Zeithaml, Valarie A., "Understanding Customer Expectations of Service," ''Sloan Management Review,'' Vol. 32, no. 3, 1991, pp 39 - 48</ref> In light of these insights, concerns have been raised about whether the act of experiencing the service might colour respondents' expectations.

: The way that expectations has been operationalised also represents a concern for theorists investigating the validity of the gaps model. The literature identifies different types of expectations.<ref>Parasuraman, A., Zeithaml, V. A., Berry, L. L., "Reassessment of Expectations as a Comparison Standard in Measuring Service Quality: Implications for Further Research," ''Journal of Marketing,'' Vol. 58 January 1994, pp 111–124</ref> Of these, there is an argument that only ''forecast expectations'' are true expectations. Yet, the SERVQUAL instrument appears to elicit ''ideal expectations''.<ref>Johnson, C. and Mathews, B.P. , "The influence of experience on service expectations", ''International Journal of Service Industry Management,'' Vol. 8 no. 4, pp 290-305</ref>  Note the wording in the questionnaire in the preceding figure which grounds respondents in their expectations of what ''excellent'' companies will do. Subtle use of words can elicit different types of expectations.  Capturing true expectations is important because it has implications for service quality scores. When researchers elicit ideal expectations, overall service quality scores are likely to be lower, making it much more difficult for marketers to deliver on those expectations.<ref>Boulding, W., Kalra, A., Staelin, R. and Zeithaml, V. A., "Dynamic Process Model of Service Quality: From Expectations to Behavioral Intentions," ''Journal of Marketing Research,'' Vol. 30, no 1, 1993, pp 7-27</ref>

:''Questionnaire length:''  The matched pairs design of the questionnaire (total of 22 expectation items plus 22 perception items= 44 total items) makes for a very long questionnaire. If researchers add demographic and other behavioural items such as prior experience with product or category and the standard battery of demographics including: age, gender, occupation, educational attainment etc. then the average questionnaire will have around 60 items. In practical terms, this means that the questionnaire would take more than one hour per respondent to administer in a face-to-face interview. Lengthy questionnaires are known to induce ''respondent fatigue'' which may have potential implications for data reliability. In addition, lengthy questionnaires add to the time and cost involved in data collection and data analysis. Coding, collation and interpretation of data is very time consuming and in the case of lengthy questionnaires administered across large samples, the findings cannot be used to address urgent quality-related problems. In some cases, it may be necessary to carry out 'quick and dirty' research while waiting for the findings of studies with superior research design.

: Some analysts have pointed out that the SERVPERF instrument, developed by Cronin and Taylor,<ref>Cronin, J. J. and Taylor, S. A., "Measuring Service Quality: A Re-examination and Extension," ''Journal of Marketing,'' Vol. 56, no. 3, 1992, pp 55-68.</ref><ref>Cronin J.J., Steven, J. and Taylor, A., "SERVPERF versus SERVQUAL: Reconciling performance based  and  perceptions-minus-expectations  measurement  of  service  quality," ''Journal  of  Marketing,''  Vol.  58, January, 1994, pp. 125-131</ref> and which reduced the number of questionnaire items by half (22 perceptions items only), achieves results that correlate well with SERVQUAL, with no reduction in diagnostic power, improved data accuracy through reductions in respondent boredom and fatigue and savings in the form of reduced administration costs.

:''Dimensional instability'':  A number of studies have reported that the five dimensions of service quality implicit in the model (reliability, assurance, tangibles, empathy and responsiveness) do not hold up when the research is replicated in different countries, different industries, in different market segments or even at different time periods.<ref>Carman, J.M., "Consumer Perceptions of Service Quality: An assessment of the SERVQUAL dimensions," ''Journal of Retailing,'' Vol. 66, no 1, 1990</ref><ref>Lam, S. K and Woo, K. S.,  "Measuring Service Quality: A test-retest reliability investigation of SERVQUAL,"  ''Journal of the Market Research Society, '' Vol. 39, no. 2, 1997, pp 381-396</ref> Some studies report that the SERVQUAL items do not always load onto the same factors. In some empirical research, the items load onto fewer dimensions, while other studies report tht the items load onto more than five dimensions of quality. In statistical terms, the robustness of the factor loadings is known as a model's ''dimensional stability.'' Across a wide range of empirical studies, the factors implicit in the SERVQUAL instrument have been shown to be unstable.<ref>Niedricha, R.W., Kiryanovab, E. and Black, W.C., "The Dimensional Stability of the Standards used in the Disconfirmation Paradigm," ''Journal of Retailing,'' Vol. 81, no. 1, 2005, pp 49–57</ref> Problems associated  with  the stability of the factor loadings may be attributed, at least in part, to the  requirement  that  each  new  SERVQUAL  investigation needed  to  make  context-sensitive  modifications to  the  instrument  in  order  to  accommodate the  unique  aspects  of  the focal service setting or problem. However, it has also been hypothesised that the dimensions of service quality represented by the SERVQUAL research instrument fail to capture the true dimensionality of the service quality construct and that there may not be a universal set of service quality dimensions that are relevant across all service industries.<ref>Miller, R.E., Hardgrave, B.C. and Jones, R.W., "SERVQUAL Dimensionality: An investigation of presentation order effect," ''International Journal of Services and Standards,'' Vol. 7, no. 1 DOI: 10.1504/IJSS.2011.040639</ref>

In spite of these criticisms, the SERVQUAL instrument, or any one of its variants (i.e. modified forms), dominates current research into service quality.<ref>Ladhari, R., "A review of twenty years of SERVQUAL research", ''International Journal of Quality and Service Sciences,'' Vol. 1 no. 2, pp.172 - 198</ref>  In a review of more than 40 articles that made use of SERVQUAL. a team of researchers found that  “few researchers concern themselves with the validation of the measuring tool”.<ref>Nyeck, S., Morales, M., Ladhari, R., & Pons, F., "10 Years of Service Quality  Measurement: Reviewing the use of the SERVQUAL Instrument," ''Cuadernos de Difusion,'' Vol. 7, no 13, pp 101-107.</ref>  SERVQUAL is not only the subject of academic papers, but it is also widely used by industry practitioners.<ref>Asubonteng, P., McCleary, K.J. and Swan, J.E.,  "SERVQUAL revisited: a critical review of service quality", ''Journal of Services Marketing,'' Vol. 10, no 6, 1996, pp 62-81</ref>

== See also==

* [[Customer satisfaction]]
* [[Customer satisfaction research]]
* [[Disconfirmed expectancy]]
* [[Quality management]]
* [[Service quality]]
* [[Services marketing]]

== References ==
{{Reflist}}

== External links ==

*[http://www.farrell-associates.com.au/BSOM/Papers/SERVQUAL.doc/ SERVQUAL questionnaire]  - Annotated copy of SERVQUAL questionnaire
*[http://www.kinesis-cem.com/pdf/ServQual.pdf/ SERVQUAL Instructions] - Detailed instructions for administering the SERVQUAL questionnaire

==Further reading==
* Luis Filipe Lages & Joana Cosme Fernandes, 2005, "The SERPVAL scale: A multi-item instrument for measuring service personal values", ''Journal of Business Research,'' Vol.58, Issue 11, pp 1562–1572.
* Deborah McCabe, Mark S. Rosenbaum, and Jennifer Yurchisin (2007), “Perceived Service Quality and Shopping Motivations:  A Dynamic Relationship,” ''Services Marketing Quarterly,'' 29 (1), pp 1–21.

[[Category:Knowledge representation]]
[[Category:Quality management]]
[[Category:Service industries]]
<=====doc_Id=====>:765
<=====title=====>:
Allen's interval algebra
<=====text=====>:
{{Use dmy dates|date=June 2013}}
''For the type of boolean algebra called interval algebra, see [[Boolean algebra (structure)#Examples|Boolean algebra (structure)]]''

'''Allen's interval algebra''' is a [[calculus (disambiguation)|calculus]] for [[spatial-temporal reasoning|temporal reasoning]] that was introduced by [[James F. Allen]] in 1983.

The calculus defines possible relations between time intervals and provides a composition table that can be used as a basis
for reasoning about temporal descriptions of events.

==Formal description==

=== Relations ===

The following 13 base relations capture the possible relations between two intervals.

{| class="wikitable"
 !Relation
 !Illustration
 !Interpretation
 |-
 |<math>X \,\mathrel{\mathbf{<}}\, Y</math>
<math>Y \,\mathrel{\mathbf{>}}\, X</math>
 |[[Image:Allen calculus before.png|X takes place before Y]]
 |X takes place before Y
 |-
 |<math>X \,\mathrel{\mathbf{m}}\, Y</math>
<math>Y \,\mathrel{\mathbf{mi}}\, X</math>
 |[[Image:Allen calculus meet.png|X meets Y]]
 |X meets Y (''i'' stands for '''''i'''nverse'')
 |-
 |<math>X \,\mathrel{\mathbf{o}}\, Y</math>
<math>Y \,\mathrel{\mathbf{oi}}\, X</math>
 |[[Image:Allen calculus overlap.png|X overlaps with Y]]
 |X overlaps with Y
 |-
 |<math>X \,\mathrel{\mathbf{s}}\, Y</math>
<math>Y \,\mathrel{\mathbf{si}}\, X</math>
 |[[Image:Allen calculus start.png|X starts with Y]]
 |X starts Y
 |-
 |<math>X \,\mathrel{\mathbf{d}}\, Y</math>
<math>Y \,\mathrel{\mathbf{di}}\, X</math>
 |[[Image:Allen calculus during.png|X during Y]]
 |X during Y
 |-
 |<math>X \,\mathrel{\mathbf{f}}\, Y</math>
<math>Y \,\mathrel{\mathbf{fi}}\, X</math>
 |[[Image:Allen calculus finish.png|X finishes with Y]]
 |X finishes Y
 |-
 |<math>X \,\mathrel{\mathbf{=}}\, Y</math>
 |[[Image:Allen calculus equal.png|X is equal to Y]]
 |X is equal to Y

 |}Using this calculus, given facts can be formalized and then used for automatic reasoning. Relations between intervals are formalized as sets of base relations.

The sentence
: ''During dinner, Peter reads the newspaper. Afterwards, he goes to bed.''
is formalized in Allen's Interval Algebra as follows:

<math>\mbox{newspaper } \mathbf{\{ \operatorname{d}, \operatorname{s}, \operatorname{f} \}} \mbox{ dinner}</math>

<math>\mbox{dinner } \mathbf{\{ \operatorname{<}, \operatorname{m} \}} \mbox{ bed}</math>

In general, the number of different relations between n intervals is 1, 1, 13, 409, 23917, 2244361... [http://oeis.org/A055203 OEIS A055203]. The special case shown above is for n=2.

===Composition of relations between intervals===
For reasoning about the relations between temporal intervals, Allen's Interval Algebra provides a [[Relation composition|composition]] table. Given the relation between <math>X</math> and <math>Y</math> and the relation between <math>Y</math> and <math>Z</math>, the composition table allows for concluding about the relation between <math>X</math> and <math>Z</math>. Together with a [[Inverse relation|converse]] operation, this turns Allen's Interval Algebra into a [[relation algebra]].

For the example, one can infer <math>\mbox{newspaper } \mathbf{\{ \operatorname{<}, \operatorname{m} \}} \mbox{ bed}</math>.

==Extensions==
Allen's Interval Algebra can be used for the description of both temporal intervals and spatial configurations. For the latter use, the relations are interpreted as describing the relative position of spatial objects. This also works for three-dimensional objects by listing the relation for each coordinate separately.

==Implementation==
* [https://code.google.com/p/allenintervalrelationships/ A simple java library implementing the concept of Allen's temporal relations and the path consistency algorithm]

==See also==
* [[Temporal logic]]
* [[Logic]]
* [[Region Connection Calculus]].
* [[Spatial relation]] (analog)
* [[Commonsense reasoning]]

==References==
* James F. Allen: ''Maintaining knowledge about temporal intervals''. In: ''Communications of the ACM''. 26 November 1983. ACM Press. pp.&nbsp;832–843, ISSN 0001-0782
* [[Bernhard Nebel]], Hans-Jürgen Bürckert: ''Reasoning about Temporal Relations: A Maximal Tractable Subclass of Allen's Interval Algebra.'' In: ''Journal of the ACM'' 42, pp.&nbsp;43–66. 1995.
* Peter van Beek, Dennis W. Manchak: ''The design and experimental analysis of algorithms for temporal reasoning.'' In: ''Journal of Artificial Intelligence Research'' 4, pp.&nbsp;1–18, 1996.

[[Category:Knowledge representation]]
[[Category:Constraint programming]]
<=====doc_Id=====>:768
<=====title=====>:
Open Knowledge Base Connectivity
<=====text=====>:
{{Unreferenced stub|auto=yes|date=December 2009}}
'''Open Knowledge Base Connectivity''' ('''OKBC''')  is a [[protocol (computer science)|protocol]] and an [[application programming interface|API]] for accessing knowledge in [[knowledge representation]] systems such as [[ontology (computer science)|ontology]] repositories and [[object-relational database]]s. It is somewhat complementary to the [[Knowledge Interchange Format]] that serves as a general representation language for knowledge. It is developed by [[SRI International]]'s [[Artificial Intelligence Center]] for [[DARPA]]'s High Performance Knowledge Base program (HPKB).

==External links==
* [http://www.ai.sri.com/~okbc/ Open Knowledge Base Connectivity Home Page]

[[Category:SRI International software]]
[[Category:Knowledge representation]]

{{Comp-sci-stub}}
<=====doc_Id=====>:771
<=====title=====>:
Digital curation
<=====text=====>:
'''Digital curation''' is the selection,<ref name="ALA, Scime" >{{Cite web
  |title=The Content Strategist as Digital Curator
  |author=Erin Scime
  |date=8 December 2009
  |publisher=[[A List Apart]]
  |url=http://www.alistapart.com/articles/content-strategist-as-digital-curator/
}}</ref> [[Preservation (library and archival science)|preservation]], maintenance,  collection and [[archiving]] of [[Digital data|digital]] assets.<ref name="paper">{{Cite book | last1 = Rusbridge | first1 = C. | last2 = Buneman | first2 = P. | authorlink2 = Peter Buneman| last3 = Burnhill | first3 = P. | last4 = Giaretta | first4 = D. | last5 = Ross | first5 = S. | last6 = Lyon | first6 = L. | last7 = Atkinson | first7 = M. | authorlink7 = Malcolm Atkinson| chapter = The Digital Curation Centre: A Vision for Digital Curation | doi = 10.1109/LGDI.2005.1612461 | title = 2005 IEEE International Symposium on Mass Storage Systems and Technology | pages = 31 | year = 2005 | url = http://eprints.erpanet.org/82/01/DCC_Vision.pdf| isbn = 0-7803-9228-0 | pmid =  | pmc = }}</ref><ref name="dccdefn">{{cite web |title=What is Digital Curation? |publisher=[[Digital Curation Centre]] |url=http://www.dcc.ac.uk/about/what |accessdate=2008-04-01}}</ref><ref>{{cite web |title=Digital curation |author=Elizabeth Yakel |publisher=Emerald Group Publishing |year=2007 |url=http://www.ingentaconnect.com/content/mcb/164/2007/00000023/00000004/art00003 |accessdate=2008-04-01}}</ref>
Digital curation establishes, maintains and adds value to repositories of digital data for present and future use.<ref name="dccdefn"/> This is often accomplished by [[archivist]]s, librarians, scientists, historians, and scholars. Enterprises are starting to use digital curation to improve the quality of information and data within their operational and strategic processes.<ref>E. Curry, A. Freitas, and S. O'Riáin, [http://3roundstones.com/led_book/led-curry-et-al.html "The Role of Community-Driven Data Curation for Enterprises,"] in Linking Enterprise Data, D. Wood, Ed. Boston, MA: Springer US, 2010, pp. 25-47.</ref> Successful digital curation will mitigate digital obsolescence, keeping the information accessible to users indefinitely. 

The term ''[[curator|curation]]'' in the past commonly referred to museum and library professionals. It has since been applied to interaction with [[social media]] including compiling digital images, web links and movie files.

==Approaches==
===Create new representation===
For some topics, knowledge is embodied in forms that have not been conducive to print, such as how choreography of dance or of the motion of skilled workers or artisans is difficult to encode. New digital approaches such as 3d holograms and other computer-programmed expressions are developing. 

For mathematics, it seems possible for a new common language to be developed that would express mathematical ideas in ways that can be digitally stored, linked, and made accessible. The [[Global Digital Mathematics Library]] is a project to define and develop such a language.

===Convert print resources===
The process of converting printed resources into digital collections has been epitomized to some degree by librarians and related specialists. For example,
The [[Digital Curation Centre]] is claimed to be a "world leading centre of expertise in digital information curation"<ref name="Digital Curation Centre">{{cite web|last=Digital Curation Centre|title=About the DCC|url=http://www.dcc.ac.uk/about-us|work=Website|publisher=Digital Curation Centre|accessdate=6 March 2013}}</ref> that assists higher education research institutions in such conversions. The DCC, based in the UK, began operations in early 2004 and suggests the following as a general outline of their approach to digital curation:

* Conceptualize: Consider what digital material you will be creating and develop storage options. Take into account websites, publications, email, among other types of digital output.
* Create: Produce digital material and attach all relevant metadata, typically the more metadata the more accessible the information.
* Access and use: Determine the level of accessibility for the range of digital material created. Some material may be accessible only by password and other material may be freely accessible to the public.
* Appraise and select: Consult the mission statement of the institution or private collection and determine what digital data is relevant. There may also be legal guidelines in place that will guide the decision process for a particular collection.
* Dispose: Discard any digital material that is not deemed necessary to the institution.
* Ingest: Send digital material to the predetermined storage solution. This may be an archive, repository or other facility.
* Preservation action: Employ measures to maintain the integrity of the digital material.
* Reappraise: Reevaluate material to ensure that is it still relevant and is true to its original form.
* Store: Secure data within the predetermined storage facility.
* Access and reuse: Routinely check that material is still accessible for the intended audience and that the material has not been compromised through multiple uses.
* Transform: If desirable or necessary the material may be transferred into a different digital format.

==="Sheer curation"===
''Sheer curation'' is an approach to digital curation where curation activities are quietly integrated into the normal work flow of those creating and managing data and other digital assets. The word sheer is used to emphasize the lightweight and virtually transparent nature of these curation activities. The term ''sheer curation'' was coined by Alistair Miles in the ImageStore project,<ref>[http://imageweb.zoo.ox.ac.uk/wiki/index.php/The_ImageStore_Project The ImageStore Project - ImageWeb<!-- Bot generated title -->]</ref> and the UK Digital Curation Centre's SCARP project.<ref>[http://www.dcc.ac.uk/scarp/ Digital Curation Centre: DCC SCARP Project<!-- Bot generated title -->]</ref> The approach depends on curators having close contact or 'immersion' in data creators' working practices. An example is the case study of a neuroimaging research group by Whyte et al., which explored ways of building its digital curation capacity around the apprenticeship style of learning of neuroimaging researchers, through which they share access to datasets and re-use experimental procedures.<ref>Whyte, A., Job, D., Giles, S. and Lawrie, S. (2008) '[http://www.ijdc.net/index.php/ijdc/article/view/74/53 Meeting Curation Challenges in a Neuroimaging Group]', The International Journal of Digital Curation Issue 1, Volume 3, 2008</ref>      

Sheer curation depends on the hypothesis that good data and digital asset management at the point of creation and primary use is also good practice in preparation for sharing, publication and/or [[long-term preservation]] of these assets. Therefore, sheer curation attempts to identify and promote tools and good practices in local data and digital asset management in specific domains, where those tools and practices add immediate value to the creators and primary users of those assets. Curation can best be supported by identifying existing practices of sharing, stewardship and re-use that add value, and augmenting them in ways that both have short-term benefits, and in the longer term reduce risks to digital assets or provide new opportunities to sustain their long-term accessibility and re-use value.    

The aim of sheer curation is to establish a solid foundation for other curation activities which may not directly benefit the creators and primary users of digital assets, especially those required to ensure long-term preservation. By providing this foundation, further curation activities may be carried out by specialists at appropriate institutional and organisation levels, whilst causing the minimum of interference to others.

A similar idea is ''curation at source'' used in the context of Laboratory Information Management Systems [[Laboratory information management system|LIMS]]. This refers more specifically to automatic recording of [[metadata]] or information about data at the point of capture, and has been developed to apply semantic web techniques to integrate laboratory instrumentation and documentation systems.<ref>Frey, J. [http://www.allhands.org.uk/2008/programme/jeremyfrey.cfm 'Sharing and Collaboration' keynote presentation at UK e-Science All Hands Meeting], 8–11 September 2008, Edinburgh</ref> Sheer curation and curation-at-source can be contrasted with post hoc [[digital preservation]], where a project is initiated to preserve a collection of digital assets that have already been created and are beyond the period of their primary use.

===Channelisation===
''Channelisation'' is curation of digital assets on the web, often by brands and media companies, into continuous flows of content, turning the user experience from a lean-forward interactive medium, to a lean-back passive medium. The curation of content can be done by an independent third party, that selects media from any number of on-demand outlets from across the globe and adds them to a playlist to offer a digital "channel" dedicated to certain subjects, themes, or interests so that the end user would see and/or hear a continuous stream of content.

==Challenges==
* Storage format evolution and obsolescence<ref name=ijdcpw200711>{{cite web|title=Digital Preservation Theory and Application: Transcontinental Persistent Archives Testbed Activity |publisher=The International Journal of Digital Curation |url=http://www.ijdc.net/./ijdc/article/view/43/50 |author=Paul Watry |date=November 2007 |accessdate=2008-04-01 |deadurl=yes |archiveurl=https://web.archive.org/web/20080315030030/http://www.ijdc.net:80/ijdc/article/view/43/50 |archivedate=2008-03-15 |df= }}</ref> 
* Rate of creation of new data and data sets
* Maintaining accessibility to data through links and search results
* Comparability of [[semantic]] and [[ontology|ontological]] definitions of data sets<ref name="ijdcpw200711"/>

===Responses===
* Specialized research institutions<ref>[http://www.dcc.ac.uk/ Digital Curation Centre]</ref><ref>[http://www.dpconline.org/ Digital Preservation Coalition]</ref>
* Academic courses
* Dedicated symposia<ref>[http://www.ils.unc.edu/digccurr2007/ DigCCurr 2007 - an international symposium on Digital Curation, April 18-20, 2007]</ref><ref>[http://stardata.nrf.ac.za/nadicc 1st African Digital Management and Curation Conference and Workshop - Date: 12-13 February 2008]</ref>
* Peer reviewed technical and industry journals<ref>[http://www.ijdc.net/ International Journal of Digital Curation]</ref>

==See also==
* [[Biocurator|Biocuration]]
* [[Curator]]
* [[Data curation]]
* [[Data format management]]
* [[Digital artifactual value]]
* [[Digital asset management]]
* [[Digital obsolescence]]
* [[International Society for Biocuration]]

==References==
{{reflist|33em}}

==External links==
*[https://www.youtube.com/watch?v=pbBa6Oam7-w Animations introducing digital preservation and curation]
*[http://www.alistapart.com/articles/content-strategist-as-digital-curator/ Content Strategist as Digital Curator], A List Apart Journal, December 2009
*[http://www.dcc.ac.uk/ Digital Curation Centre]
*[http://journals.tdl.org/jodi/article/view/229/183 Digital Curation and Trusted Repositories: Steps Toward Success]*[http://www.digcur-education.org DigCurV] A project funded by the European Commission to establish a curriculum framework for vocational training in digital curation.

[[Category:Archival science]]
[[Category:Databases]]
[[Category:Knowledge representation]]
[[Category:Digital libraries]]
[[Category:Digital preservation]]
<=====doc_Id=====>:774
<=====title=====>:
New Classification Scheme for Chinese Libraries
<=====text=====>:
{{Unreferenced|date=May 2009}}
The '''New Classification Scheme for Chinese Libraries''' is a system of [[library classification]] developed by Yung-Hsiang Lai since 1956.  It is modified from "[[:zh:中国图书分类法|A System of Book Classification for Chinese Libraries]]" of [[Liu Guojun]], which is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. 

The scheme is developed for Chinese books, and commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]]. 

==Main classes==
*000 Generalities
*100 [[Philosophy]]
*200 [[Religion]]
*300 [[Sciences]]
*400 [[Applied sciences]]
*500 [[Social sciences]]
*600-700 [[History]] and [[Geography]]
*800 [[Linguistics]] and [[Literature]]
*900 [[Arts]]

==Outline of the classification tables==
*'''000 Generalities'''
**000 Special collections
**010 [[Bibliography]]; [[Literacy]] ([[Documentation]])
**020 [[Library science|Library]] and [[information science]]; Archive management
**030 [[Sinology]]
**040 [[General encyclopedia]]
**050 [[Serial (publishing)|Serial publications]]; [[Periodicals]]
**060 General [[organization]]; [[Museology]]
**070 General collected [[essays]]
**080 General [[Book series|series]]
**090 Collected [[Chinese classics]]
*'''100 [[Philosophy]]'''
**100 Philosophy: general
**110 [[Thought]]; [[Learning]]
**120 [[Chinese philosophy]]
**130 [[Oriental philosophy]]
**140 [[Western philosophy]]
**150 [[Logic]]
**160 [[Metaphysics]]
**170 [[Psychology]]
**180 [[Esthetics]]
**190 [[Ethics]]
*'''200 [[Religion]]'''
**200 Religion: general
**210 Science of religion
**220 [[Buddhism]]
**230 [[Taoism]]
**240 [[Christianity]]
**250 [[Islam]] ([[Mohammedanism]])
**260 [[Judaism]]
**270 Other religions
**280 [[Mythology]]
**290 [[Astrology]]; [[Superstition]]
*'''300 [[Sciences]]'''
**300 Sciences: general
**310 [[Mathematics]]
**320 [[Astronomy]]
**330 [[Physics]]
**340 [[Chemistry]]
**350 [[Earth science]]; [[Geology]]
**360 [[Biological science]]
**370 [[Botany]]
**380 [[Zoology]]
**390 [[Anthropology]]
*'''400 [[Applied sciences]]'''
**400 Applied sciences: general
**410 [[Medical sciences]]
**420 [[Home economics]]
**430 [[Agriculture]]
**440 [[Engineering]]
**450 [[Mining]] and [[metallurgy]]
**460 [[Chemical engineering]]
**470 [[Manufacture]]
**480 [[Commerce]]: various business
**490 Commerce: [[Administration (business)|administration]] and [[management]]
*'''500 [[Social sciences]]'''
**500 Social sciences: general
**510 [[Statistics]]
**520 [[Education]]
**530 [[Rite]] and [[Convention (norm)|custom]]
**540 [[Sociology]]
**550 [[Economy]]
**560 [[Finance]]
**570 [[Political science]]
**580 [[Law]]; [[Jurisprudence]]
**590 [[Military science]]
*'''600-700 [[History]] and [[geography]]'''
**600 History and geography: General 
*'''History and geography of [[China]]'''
**610 General [[history of China]]
**620 Chinese history by period
**630 History of Chinese civilization
**640 Diplomatic history of China
**650 Historical sources
**660 [[Geography of China]]
**670 Local history
**680 Topical topography
**690 Chinese travels
*'''[[World history]] and geography'''
**710 World: general history and geography
**720 [[Oceans]] and [[sea]]s
**730 [[Asia]]: history and geography
**740 [[Europe]]: history and geography
**750 [[Americas|America]]: history and geography
**760 [[Africa]]: history and geography
**770 [[Oceania]]: history and geography
**780 [[Biography]]
**790 [[Antiquities]] and [[archaeology]]
*'''800 [[Linguistics]] and [[literature]]'''
**800 Linguistics: general
**810 Literature: general
**820 [[Chinese literature]]
**830 Chinese literature: general collections
**840 Chinese literature: individual works
**850 Various Chinese literature
**860 Oriental literature
**870 [[Western literature]]
**880 Other countries literatures
**890 [[Journalism]]
*'''900 [[Arts]]'''
**900 Arts: general
**910 [[Music]]
**920 [[Architecture]]
**930 [[Sculpture]]
**940 [[Drawing]] and [[painting]]; [[Calligraphy]]
**950 [[Photography]]; [[Computer art]]
**960 [[Decorative arts]]
**970 [[Arts and Crafts movement]]
**980 [[Theatre]]
**990 [[Recreation]] and [[leisure]]

==See also==
===Decimal systems===
*[[Dewey Decimal Classification]]
*[[Nippon Decimal Classification]]
*[[Korean Decimal Classification]] 
===Non-decimal systems===
*[[Library of Congress Classification]]
*[[Chinese Library Classification]]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:777
<=====title=====>:
Knowledge integration
<=====text=====>:
'''Knowledge integration''' is the process of synthesizing multiple [[knowledge model]]s (or representations) into a common model (representation).

Compared to [[information integration]], which involves merging information having different schemas and representation models, knowledge integration focuses more on synthesizing the understanding of a given subject from different perspectives.

For example, multiple interpretations are possible of a set of student grades, typically each from a certain perspective. An overall, integrated view and understanding of this information can be achieved if these interpretations can be put under a common model, say, a student performance index.

The [http://wise.berkeley.edu Web-based Inquiry Science Environment (WISE)], from the [[University of California at Berkeley]] has been developed along the lines of knowledge integration theory.

'''Knowledge integration''' has also been studied as the process of incorporating new information into a body of existing knowledge with an [[interdisciplinary]] approach.  This process involves determining how the new information and the existing knowledge interact, how existing knowledge should be modified to accommodate the new information, and how the new information should be modified in light of the existing knowledge.

A learning agent that actively investigates the consequences of new information can detect and exploit a variety of learning opportunities; e.g., to resolve knowledge conflicts and to fill knowledge gaps.  By exploiting these learning opportunities the learning agent is able to learn beyond the explicit content of the new information.

The [[machine learning]] program KI, developed by Murray and Porter at the [[University of Texas at Austin]], was created to study the use of automated and semi-automated knowledge integration to assist [[knowledge engineers]] constructing a large [[knowledge base]].

A possible technique which can be used is [[semantic matching]]. More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).

The [[University of Waterloo]] operates a Bachelor of Knowledge Integration [[undergraduate degree]] program as an academic major or minor. The program started in 2008.

==See also==
* [[Knowledge value chain]]

==References==
{{Reflist}}<!--added under references heading by script-assisted edit-->

==Further reading==
* Linn, M. C. (2006) The Knowledge Integration Perspective on Learning and Instruction. R. Sawyer (Ed.). In ''The Cambridge Handbook of the Learning Sciences.'' Cambridge, MA. Cambridge University Press
* Murray, K. S. (1996) KI: A tool for Knowledge Integration. Proceedings of the Thirteenth National Conference on Artificial Intelligence
* Murray, K. S. (1995) [http://www.ai.sri.com/pubs/files/1636.pdf Learning as Knowledge Integration], Technical Report TR-95-41, The University of Texas at Austin
* Murray, K. S. (1990) Improving Explanatory Competence, Proceedings of the Twelfth Annual Conference of the Cognitive Science Society
* Murray, K. S., Porter, B. W. (1990) Developing a Tool for Knowledge Integration: Initial Results. International Journal for Man-Machine Studies, volume 33
* Murray, K. S., Porter, B. W. (1989) Controlling Search for the Consequences of New Information during Knowledge Integration. Proceedings of the Sixth International Machine Learning Conference

[[Category:Knowledge representation]]
[[Category:Learning]]
[[Category:Machine learning]]
<=====doc_Id=====>:780
<=====title=====>:
Category:Knowledge representation software
<=====text=====>:
[[Category:Knowledge representation]]
[[Category:Application software]]
<=====doc_Id=====>:783
<=====title=====>:
Paradigm classification
<=====text=====>:
{{Multiple issues|
{{Orphan|date=October 2015}}
{{no footnotes|date=April 2013}}
}}

'''Paradigm classification''' in [[ontology]] is a two-dimensional classification scheme, such as a spreadsheet. It is a subset of [[faceted classification]].

== Overview ==
Paradigm classification deals with the large subset of faceted classification where an item may be classified within two dimensions. Examples might include [[genealogy]], where individuals are classified by their gender and relations with other individuals.

== References ==
{{reflist|2}}

== External links ==
{{Commons category|Ontology}}
*[http://www.miskatonic.org/library/facet-web-howto.html How to Make a Faceted Classification and Put It On the Web]
*[http://annotalia.com/philosophy/ontology Ontology]

{{Philosophy topics}}

[[Category:Knowledge representation]]
[[Category:Ontology]]


{{Ontology-stub}}
<=====doc_Id=====>:786
<=====title=====>:
Upper ontology
<=====text=====>:
{{multiple issues|
{{more footnotes|date=February 2011}}
{{essay-like|date=October 2010}}
}}

In [[information science]], an '''upper ontology''' (also known as a '''top-level ontology''' or '''foundation ontology''') is an [[Ontology (computer science)|ontology]] (in the sense used in information science) which consists of very general terms (such as "object", "property", "relation") that are common across all domains.  An important function of an upper ontology is to support broad [[semantic interoperability]] among a large number of domain-specific ontologies by providing a common starting point for the formulation of definitions. Terms in the domain ontology are ranked "under" the terms in the upper ontology, and the former stand to the latter in subclass relations.

A number of upper ontologies have been proposed, each with its own proponents. Each upper ontology can be considered as a computational implementation of [[natural philosophy]], which itself is a more empirical method for investigating the topics within the philosophical discipline of [[physical ontology]].

[[Library classification]] systems predate upper ontology systems. Though library classifications organize and categorize knowledge using general concepts that are the same across all knowledge domains, neither system is a replacement for the other.

==Development==

Any standard foundational ontology is likely to be contested among different groups, each with their own idea of "what exists". One factor exacerbating the failure to arrive at a common approach has been the lack of open-source applications that would permit the testing of different ontologies in the same computational environment.  The differences have thus been debated largely on theoretical grounds, or are merely the result of personal preferences. Foundational ontologies can however be compared on the basis of adoption for the purposes of supporting interoperability across domain ontologies.

No particular upper ontology has yet gained widespread acceptance as a [[de facto]] standard.  Different organizations have attempted to [[#Available ontologies|define standards]] for specific domains.  The '[[Process Specification Language]]' (PSL) created by the [[National Institute for Standards and Technology]] (NIST) is one example.

Another important factor leading to the absence of wide adoption of any existing upper ontology is the complexity. Some upper ontologies -- [[Cyc]] is often cited as an example in this regard -- are very large, ranging up to thousands of elements (classes, relations), with complex interactions among them and with a complexity similar to that of a human [[natural language]], and the learning process can be even longer than for a natural language because of the unfamiliar format and logical rules.  The motivation to overcome this learning barrier is largely absent because of the paucity of publicly accessible examples of use.  As a result, those building domain ontologies for local applications tend to create the simplest possible domain-specific ontology, not related to any upper ontology.  Such domain ontologies may function adequately for the local purpose, but they are very time-consuming to relate accurately to other domain ontologies. 

To solve this problem some genuinely top level ontologies have been developed, which are deliberately designed to have minimal overlap with any domain ontologies. Examples are [[Basic Formal Ontology]] and the [[Domain Ontology for Linguistic and Cognitive Engineering | DOLCE]] (see below).

===Arguments for the infeasibility of an upper ontology===
{{unreferenced section|date=December 2016}}
Historically, many attempts in many societies{{which?|date=December 2016}} have been made to impose or define a single set of concepts as more primal, basic, foundational, authoritative, true or rational than all others. A common objection{{By whom?|date=December 2016}} to such attempts points out that humans lack the sort of  transcendent perspective - or ''[[God's eye view]]'' - that would be required to achieve this goal. Humans are bound by language or culture, and so lack the sort of objective perspective from which to observe the whole terrain of concepts and derive any one standard.

Another objection is the problem of formulating definitions. Top level ontologies are designed to maximize support for interoperability across a large number of terms. Such ontologies must therefore consist of terms expressing very general concepts, but such concepts are so basic to our understanding that there is no way in which they can be defined, since the very process of definition implies that a less basic (and less well understood) concept is defined in terms of concepts that are more basic and so (ideally) more well understood. Very general concepts can often only be elucidated, for example by means of examples, or paraphrase. 

* There is no self-evident way of dividing the world up into [[concept]]s, and certainly no non-controversial one
* There is no neutral ground that can serve as a means of translating between specialized (or "lower" or "application-specific") ontologies
* Human [[language]] itself is already an arbitrary approximation of just one among many possible conceptual maps.  To draw any ''necessary correlation'' between [[English language|English]] words and any number of intellectual concepts we might like to represent in our ontologies is just asking for trouble. ([[WordNet]], for instance, is successful and useful precisely because it does not pretend to be a general-purpose upper ontology; rather, it is a tool for semantic / syntactic / linguistic disambiguation, which is richly embedded in the particulars and peculiarities of the English language.)
* Any hierarchical or topological representation of concepts must begin from some ontological, [[epistemology|epistemological]], linguistic, cultural, and ultimately pragmatic perspective.  Such pragmatism does not allow for the exclusion of politics between persons or groups, indeed it requires they be considered as perhaps more basic primitives than any that are represented.

Those{{who?|date=December 2016}} who doubt the feasibility of general purpose ontologies are more inclined to ask “what specific purpose do we have in mind for this conceptual map of entities and what practical difference will this ontology make?”  This pragmatic philosophical position surrenders all hope of devising the encoded ontology version of “everything that is the case,” ([[Wittgenstein]], [[Tractatus Logico-Philosophicus]]).

Finally there are objections similar to those against [[artificial intelligence]]{{From whom?|date=December 2016}}.  Technically, the complex concept acquisition and the social / linguistic interactions of human beings suggests any axiomatic foundation of "most basic" concepts must be cognitive, biological or otherwise difficult to characterize since we don't have axioms for such systems.  Ethically, any general-purpose ontology could quickly become an actual tyranny by recruiting adherents into a political program designed to propagate it and its funding means, and possibly defend it by violence.  Historically, inconsistent and irrational belief systems have proven capable of commanding obedience to the detriment or harm of persons both inside and outside a society that accepts them.  How much more harmful would a consistent rational one be, were it to contain even one or two basic assumptions incompatible with human life?

===Arguments for the feasibility of an upper ontology===
{{unreferenced section|date=November 2014}}
Many of those who doubt the possibility of developing wide agreement on a common upper ontology fall into one of two traps:
# they assert that there is no possibility of universal agreement on any conceptual scheme; but they argue that a practical common ontology does not need to have universal agreement, it only needs a large enough user community (as is the case for human languages) to make it profitable for developers to use it as a means to general interoperability, and for third-party developer to develop utilities to make it easier to use; and
# they point out that developers of data schemes find different representations congenial for their local purposes; but they do not demonstrate that these different representation are in fact logically inconsistent.

In fact, different representations of assertions about the real world (though not philosophical models), if they accurately reflect the world, must be logically consistent, even if they focus on different aspects of the same physical object or phenomenon.  If any two assertions about the real world are logically inconsistent, one or both must be wrong, and that is a topic for experimental investigation, not for ontological representation.  In practice, representations of the real world are created as and known to be approximations to the basic reality, and their use is circumscribed by the limits of error of measurements in any given practical application.  Ontologies are entirely capable of representing approximations, and are also capable of representing situations in which different approximations have different utility.  Objections based on the different ways people perceive things attack a simplistic, impoverished view of ontology.  The objection that there are logically incompatible models of the world are true, but in an upper ontology those different models can be represented as different theories, and the adherents of those theories can use them in preference to other theories, while preserving the logical consistency of the ''necessary'' assumptions of the upper ontology.  The ''necessary'' assumptions provide the logical vocabulary with which to specify the meanings of all of the incompatible models.  It has never been demonstrated that incompatible models cannot be properly specified with a common, more basic set of concepts, while there are examples of incompatible theories that can be logically specified with only a few basic concepts.

Many of the objections to upper ontology refer to the problems of life-critical decisions or non-axiomatized  problem areas such as law or medicine or politics that are difficult even for humans to understand.  Some of these objections do not apply to physical objects or standard abstractions that are defined into existence by human beings and closely controlled by them for mutual good, such as standards for electrical power system connections or the signals used in traffic lights.  No single general [[metaphysics]] is required to agree that some such standards are desirable.  For instance, while time and space can be represented many ways, some of these are already used in interoperable artifacts like maps or schedules.

Objections to the feasibility of a common upper ontology also do not take into account the possibility of forging agreement on an ontology that contains all of the ''primitive'' ontology elements that can be combined to create any number of more specialized concept representations.  Adopting this tactic permits effort to be focused on agreement only on a limited number of ontology elements. By agreeing on the meanings of that inventory of basic concepts, it becomes possible to create and then accurately and automatically interpret an infinite number of concept representations as combinations of the basic ontology elements.  Any domain ontology or database that uses the elements of such an upper ontology to specify the meanings of its terms will be automatically and accurately interoperable with other ontologies that use the upper ontology, even though they may each separately define a large number of domain elements not defined in other ontologies.  In such a case, proper interpretation will require that the logical descriptions of domain-specific elements be transmitted along with any data that is communicated; the data will then be automatically interpretable because the domain element descriptions, based on the upper ontology, will be properly interpretable by any system that can properly use the upper ontology.  In effect elements in different domain ontologies can be *translated* into each other using the common upper ontology. An upper ontology based on such a set of primitive elements can include alternative views, provided that they are logically compatible.  Logically incompatible models can be represented as alternative theories, or represented in a specialized extension to the upper ontology.  The proper use of alternative theories is a piece of knowledge that can itself be represented in an ontology.  Users that develop new domain ontologies and find that there are semantic primitives needed for their domain but missing from the existing common upper ontology can add those new primitives by the accepted procedure, expanding the common upper ontology as necessary.

Most proponents{{Who|date=April 2015}} of an upper ontology argue that several good ones may be created with perhaps different emphasis.  Very few are actually arguing to discover just one within natural language or even an academic field.  Most are simply standardizing some existing communication.  Another view advanced is that there is almost total overlap of the different ways that upper ontologies have been formalized, in the sense that different ontologies focus on a different aspect of the same entities, but the different views are complementary and not contradictory to each other; as a result, an internally consistent ontology that contains all the views, with means of translating the different views into the other, is feasible.  Such an ontology has not thus far been constructed, however, because it would require a large project to develop so as to include all of the alternative views in the separately developed upper ontologies, along with their translations.  The main barrier to construction of such an ontology is not the technical issues, but the reluctance of funding agencies to provide the funds for a large enough consortium of developers and users.

Several common arguments against upper ontology can be examined more clearly by separating issues of concept definition (ontology), language (lexicons), and facts (knowledge).  For instance, people have different terms and phrases for the same concept.  However, that does not necessarily mean that those people are referring to different concepts.  They may simply be using different language or idiom.  Formal ontologies typically use linguistic labels to refer to concepts, but the terms that label ontology elements mean no more and no less than what their axioms say they mean.  Labels are similar to variable names in software, evocative rather than definitive.  The proponents of a common upper ontology point out that the meanings of the elements (classes, relations, rules) in an ontology depend only on their [[logical form]], and not on the labels, which are usually chosen merely to make the ontologies more easily usable by their human developers.  In fact, the labels for elements in an ontology need not be words - they could be, for example, images of instances of a particular type, or videos of an action that is represented by a particular type.  It cannot be emphasized too strongly that words are *not* what are represented in an ontology, but entities in the real world, or abstract entities (concepts) in the minds of people.  Words are not equivalent to ontology elements, but words *label* ontology elements.  There can be many words that label a single concept, even in a single language (synonymy), and there can be many concepts labeled by a single word (ambiguity).  Creating the mappings between human language and the elements of an ontology is the province of Natural Language Understanding.  But the ontology itself stands independently as a logical and computational structure.  For this reason, finding agreement on the structure of an ontology is actually easier than developing a controlled vocabulary, because all different interpretations of a word can be included, each *mapped* to the same word in the different terminologies.

A second argument is that people believe different things, and therefore can't have the same ontology.  However, people can assign different truth values to a particular assertion while accepting the validity of certain underlying claims, facts, or way of expressing an argument with which they disagree. (Using, for instance, the issue/position/argument form.) This objection to upper ontologies ignores the fact that a single ontology can represent different belief systems, representing them as different belief systems, without taking a position on the validity of either.

Even arguments about the existence of a thing require a certain sharing of a concept, even though its existence in the real world may be disputed.  Separating belief from naming and definition also helps to clarify this issue, and show how concepts can be held in common, even in the face of differing belief.  For instance, [[wiki]] as a medium may permit such confusion but disciplined users can apply [[dispute resolution]] methods to sort out their conflicts.  It is also argued that most people share a common set of "semantic primitives", fundamental concepts, to which they refer when they are trying to explain unfamiliar terms to other people.  An ontology that includes representations of those semantic primitives could in such a case be used to create logical descriptions of any term that a person may wish to define logically.   That ontology would be one form of upper ontology, serving as a logical "interlingua" that can translate ideas in one terminology to its [[logical equivalence|logical equivalent]] in another terminology.

Advocates{{Who|date=April 2015}} argue that most disagreement about the viability of an upper ontology can be traced to the conflation of ontology, language and knowledge, or too-specialized areas of knowledge: many people, or agents or groups will have areas of their respective internal ontologies that do not overlap.  If they can cooperate and share a conceptual map at all, this may be so very useful that it outweighs any disadvantages that accrue from sharing.  To the degree it becomes harder to share concepts the deeper one probes, the more valuable such sharing tends to get.  If the problem is as basic as opponents of upper ontologies claim, then, it also applies to a group of humans trying to cooperate, who might need machine assistance to communicate easily.

If nothing else, such ontologies are implied by [[machine translation]], used when people cannot practically communicate.  Whether "upper" or not, these seem likely to proliferate.

==Available upper ontologies==

===Basic Formal Ontology (BFO)===
{{main|Basic Formal Ontology}}
The Basic Formal Ontology (BFO) framework developed by [[Barry Smith (academic and ontologist)|Barry Smith]] and his associates consists of a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: relating to continuant entities such as three-dimensional enduring objects, and occurrent entities (primarily) processes conceived as unfolding in successive phases through time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. A continuant domain ontology descending from BFO can be conceived as an inventory of entities existing at a time. Each occurrent ontology can be conceived as an inventory of processes unfolding through a given interval of time. Both BFO itself and each of its extension sub-ontologies can be conceived as a window on a certain portion of reality at a given level of granularity. More than [http://ifomis.uni-saarland.de/bfo/users 200 extension ontologies] of BFO have been created, applying the BFO architecture to different domains through the strategy of downward population. The Cell Ontology, for example, populates downward from BFO by importing the BFO branch terminating with object, and defining a cell as a subkind of object. Other examples of ontologies extending BFO are the [[Ontology for Biomedical Investigations]] (OBI) and the ontologies of the [[OBO Foundry|Open Biomedical Ontologies Foundry]]. In addition to these examples, BFO and extensions are increasingly being use in defense and security domains, for example in the [http://milportal.org AIRS framework]. BFO serves as the upper level of the Sustainable Development Goals (SDG) Interface Ontology developed by the [http://uneplive.unep.org/portal United Nations Environment Programme]. BFO has been documented in the textbook [http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology], published by MIT Press in 2015.

===BORO===
{{main|BORO}}
Business Objects Reference Ontology is an upper ontology designed for developing ontological or semantic models for large complex operational applications that consists of a top ontology as well as a process for constructing the ontology.  It is built upon a series of clear [[metaphysical choices]]  to provide a solid (metaphysical) foundation. A key choice was for an [[Extension (metaphysics)|extensional]] (and hence, [[Spacetime|four-dimensional]]) [[ontology]] which provides it a simple [[criteria of identity]]. Elements of it have appeared in a number of standards. For example, the ISO standard, [[ISO 15926]] – Industrial automation systems and integration – was heavily influenced by an early version. The [[IDEAS Group|IDEAS]] (International Defence Enterprise Architecture Specification for exchange) standard is based upon BORO, which in turn was used to develop [[DODAF]] 2.0.

===CIDOC Conceptual Reference Model===
{{main|CIDOC Conceptual Reference Model}}
Although "CIDOC object-oriented Conceptual Reference Model" (CRM) is a [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontology]], specialised to the purposes of representing cultural heritage, a subset called CRM Core is a generic upper ontology, including:<ref>{{cite web|title=Graphical Representation of core CRM form|url=http://www.cidoc-crm.org/cidoc_core_graphical_representation/graphical_representation.html|publisher=[[CIDOC]]}}</ref><ref>{{cite web|title=Definition of the CIDOC Conceptual Reference Model, Version 5.0.4|url=http://www.cidoc-crm.org/html/5.0.4/cidoc-crm.html#_Toc310250785|publisher=[[CIDOC]]|date=November 2011}}</ref>
* Space-Time – title/identifier, place, era/period, time-span, relationship to persistent items
* Events – title/identifier, beginning/ending of existence, participants (people, either individually or in groups), creation/modification of things (physical or conceptional), relationship to persistent items
* Material Things – title/identifier, place, the information object the material thing carries, part-of relationships, relationship to persistent items
* Immaterial Things – title/identifier, information objects (propositional or symbolic), conceptional things, part-of relationships

A persistent item is a physical or conceptional item that has a persistent identity recognized within the duration of its existence by its identification rather than by its continuity or by observation. A persistent item is comparable to an endurant.<br/>A propositional object is a set of statements about real or imaginary things.<br/>A symbolic object is a sign/symbol or an aggregation of signs or symbols.

===COSMO===
[[Common Semantic Model|COSMO]] (COmmon Semantic MOdel, available at http://micra.com/COSMO/COSMO.owl) is an ontology that was initiated as a project of the COSMO working group of the Ontology and taxonomy Coordinating Working Group, with the goal of developing a foundation ontology that can serve to enable broad general [[Semantic Interoperability]]. The current version is an OWL ontology, but a Common-Logic compliant version is anticipated in the future. The ontology and explanatory files are available at the COSMO site. The goal of the COSMO working group was to develop a foundation ontology by a collaborative process that will allow it to represent all of the basic ontology elements that all members feel are needed for their applications. The development of COSMO is fully open, and any comments or suggestions from any sources are welcome. After some discussion and input from members in 2006, the development of the COSMO has been continued primarily by Patrick Cassidy, the chairman of the COSMO Working Group. Contributions and suggestions from any interested party are still welcome and encouraged. Many of the types (OWL classes) in the current COSMO have been taken from the OpenCyc OWL version 0.78, and from the SUMO. Other elements were taken from other ontologies (such as BFO and DOLCE), or developed specifically for COSMO. Development of the COSMO initially focused on including representations of all of the words in the [[Longman Dictionary of Contemporary English]] (LDOCE) controlled [[defining vocabulary]] (2148 words). These words are sufficient to define (linguistically) all of the entries in the LDOCE. It is hypothesized that the ontological representations of the concepts represented by those terms will be sufficient to specify the meanings of any specialized ontology element, thereby serving as a basis for general [[Semantic Interoperability]].  Interoperability via COSMO is enabled by using the COSMO (or an ontology derived from it) as an interlingua by which other domain ontologies can be translated into each other's terms and thereby accurately communicate. As new domains are linked into COSMO, additional semantic primitives may be recognized and added to its structure.  The current (January 2016) OWL version of COSMO has over 8000 types (OWL classes), over 1000 relations, and over 3000 restrictions.  The COSMO itself (COSMO.owl) and other related and explanatory files can be obtained at http://micra.com/COSMO.

===Cyc===
{{main|Cyc}}
A well-known and quite comprehensive ontology available today is [[Cyc]], a proprietary system under development since 1986, consisting of a foundation ontology and several domain-specific ontologies (called ''microtheories''). A subset of that ontology has been released for free under the name [[Cyc#OpenCyc|OpenCyc]], and a more or less unabridged version is made available for free non-commercial use under the name [[Cyc#ResearchCyc|ResearchCyc]].

=== DOLCE ===
Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) is the first module of the WonderWeb foundational ontologies library,<ref>http://www.loa-cnr.it/old/Papers/D18.pdf</ref> developed by Nicola Guarino and his associates at the Laboratory for Applied Ontology (LOA). As implied by its acronym, DOLCE has a clear ''cognitive bias'', in that it aims at capturing the ontological categories underlying [[natural language]] and human [[common sense]]. DOLCE, however, does not commit to a strictly [[referentialist]] metaphysics related to the intrinsic nature of the world. Rather, the categories it introduces are thought of as cognitive artifacts, which are ultimately depending on human perception, cultural imprints and social conventions. In this sense, they intend to be just ''descriptive'' (vs ''prescriptive'') notions, that assist in making already formed conceptualizations explicit.

===General Formal Ontology (GFO)===
{{main|General formal ontology}}
The general formal ontology (GFO), developed by Heinrich Herre and his colleagues of the research group Onto-Med in [[Leipzig]], is a realistic ontology integrating processes and objects. It attempts to include many aspects of recent philosophy, which is reflected both in its taxonomic tree and its axiomatizations. GFO allows for different axiomatizations of its categories (such as the existence of [[atomic time-interval]]s vs. [[dense time]]). The basic principles of GFO are published in the Onto-Med Report Nr. 8 and in "General Formal Ontology (GFO): A Foundational Ontology for Conceptual Modelling".<ref>http://www.onto-med.de/Archiv/ontomed2002/en/publications/scientific-reports/om-report-no8.pdf</ref><ref>http://www.onto-med.de/publications/2010/gfo-basic-principles.pdf</ref>

Two GFO specialties, among others, are its account of persistence and its time model. Regarding persistence, the distinction between endurants (objects) and perdurants (processes) is made explicit within GFO by the introduction of a special category, a persistent<!-- [sic]?, not persistAnt? -->.<ref>http://www.onto-med.de/en/theories/gfo/part1/node20.html</ref> A persistant<!-- [sic], not persistEnt --> is a special category with the intention that its instances "remain identical" (over time). With respect to time, time intervals are taken as primitive in GFO, and time-points (called "time boundaries") as derived. Moreover, time-points may coincide, which is convenient for modelling instantaneous changes.

===gist===

gist is developed and supported by Semantic Arts.  gist (not an acronym – it means to get the essence of) is a “minimalist upper ontology”.  gist is targeted at enterprise information systems, although it has been applied to healthcare delivery applications.  
The major attributes of gist are:
# it is small (there are 140 classes and 127 properties)
# it is comprehensive (most enterprises will not find the need to create additional primitive classes, but will find that most of their classes can be defined and derived from gist)
# it is robust – all the classes descend from 12 primitive classes, which are mostly mutually disjoint.  This aids a great deal in subsequent error detection.  There are 1342 axioms, and it uses almost all of the DL constructs (it is SROIQ(D) )
# it is concrete – most upper ontologies start with abstract philosophical concepts that users must commit to in order to use the ontology.  Gist starts with concrete classes that most people already do, or reasonably could agree with, such as Person, Organization, Document, Time, UnitOfMeasure and the like) 
# it is unambiguous – ambiguous terms (such as “term”) have been removed as they are often overloaded and confused.  Also terms that frequently have different definitions at different enterprises (such as customer and order) have been removed, also to reduce ambiguity.
# it is understandable – in addition to being built on concrete, generally understood primitives, it is extremely modular.  The 140 classes are implemented in 18 modular ontologies, each can easily be understood in its entirety, and each imports only the other modules that it needs. 
gist has been used to build Enterprise Ontologies for a number of major commercial and governmental agencies including:  Procter & Gamble, Sentara Healthcare, Washington State Department of Labor & Industries, LexisNexis, Sallie Mae and two major Financial Services firms.
gist is freely available with a Creative Commons share alike license.  There are 18 small ontologies that make up gist.  Gist can be downloaded all at once by loading or importing gistCore at gist7.   
gist is actively maintained, and has been in use for 10 years. As of May 2015 it is at version 7.1.1.<ref>{{cite web|url=http://semanticarts.com/gist|title=gist home page|author=Semantic Arts}}</ref>

gist was the subject of a paper exploring how to bridge modeling differences between ontologies <ref>{{cite web|url=http://www.researchgate.net/profile/Anthony_Cohn/publication/221235042_Utility_Ontology_Development_with_Formal_Concept_Analysis/links/0912f50cbb29adba1f000000.pdf#page=163|title=Complexity of Reasoning with Expressive Ontology Mappings|author1=Chiara Ghidini |author2=Luciano Serafini |author3=Segio Tessaris }}</ref>
In a paper describing the OQuaRE methodology for evaluating ontologies, the gist unit of measure ontology scored the highest in the manual evaluation against 10 other unit of measure ontologies,  and scored above average in the automated evaluation.  The authors stated "This ontology could easily be tested and validated, its knowledge could be effectively reused and adapted for different specified environments" <ref>{{cite web|url=http://www.acs.org.au/__data/assets/pdf_file/0015/14118/JRPIT43.2.159.pdf|title=OQuaRE: A SQuaRE-based Approach for Evaluating the Quality of Ontologies|author1=Astrid Duque-Ramos  |author2=Jesualdo Tomas Fernandez-Breis |lastauthoramp=yes }}</ref>

===IDEAS===
The upper ontology developed by the [[IDEAS Group]] is [[higher-order]], [[extensional]] and [[4D ontology|4D]]. It was developed using the [[BORO Method]]. The IDEAS ontology is not intended for reasoning and inference purposes; its purpose is to be a precise model of business.

===ISO 15926===
{{main|ISO 15926}}
ISO 15926 is an International Standard for the representation of process plant life-cycle information. This representation is specified by a generic, conceptual data model that is suitable as the basis for implementation in a shared database or data warehouse. The data model is designed to be used in conjunction with reference data: standard instances that represent information common to a number of users, process plants, or both. The support for a specific life-cycle activity depends on the use of appropriate reference data in conjunction with the data model. To enable integration of life-cycle information the model excludes all information constraints that are appropriate only to particular applications within the scope.
ISO 15926-2 defines a generic model with 201 entity types. It has been prepared by Technical Committee ISO/TC 184, Industrial automation systems and integration, Subcommittee SC 4, Industrial data.

===MarineTLO===
MarineTLO is an upperontology for the marine domain (also applicable to the terrestrial domain), developed by the Information Systems Laboratory at the Institute of Computer Science,
Foundation for Research and Technology - Hellas ([[FORTH-ICS]]).
Its purpose is to tackle the need for having integrated sets of facts about marine species,
and thus to assist research about species and [[biodiversity]].
It provides a unified and coherent core model for schema mapping which enables formulating and
answering queries which cannot be answered by any individual source.<ref>{{cite web|url=http://www.ics.forth.gr/isl/MarineTLO|title=MarineTLO - A Top Level Ontology for the Marine/Biodiversity Domain|work=forth.gr|accessdate=22 April 2015}}</ref><ref>{{cite journal|author=Tzitzikas, Y. and Alloca, C. and Bekiari, C. and Marketakis, Y. and Fafalios, P. and Doerr, M. and Minadakis, N. and Patkos, T. and Candela, L.|title=Integrating Heterogeneous and Distributed Information about Marine Species through a Top Level Ontology|url=http://link.springer.com/chapter/10.1007/978-3-319-03437-9_29|location=Institute of Computer Science, FORTH-ICS, Greece|publisher=Springer|year=2013|pages=289–301|doi=10.1007/978-3-319-03437-9_29|journal=Communications in Computer and Information Science}}</ref>

===PROTON===
PROTON (PROTo ONtology) is a basic [[subsumption hierarchy]] which provides coverage of most of the upper-level concepts   necessary for semantic annotation, indexing, and retrieval.{{citation needed|date=November 2014}}

===SUMO (Suggested Upper Merged Ontology)===
{{main|Suggested Upper Merged Ontology}}
The [[Suggested Upper Merged Ontology]] (SUMO) is another comprehensive ontology project.  It includes an [[Standard upper ontology|upper ontology]], created by the [[IEEE]] working group P1600.1 (originally by [[Ian Niles]] and [[Adam Pease]]). It is extended with many domain ontologies and a complete set of links to WordNet. It is open source.

===UMBEL===
{{main|UMBEL}}
Upper Mapping and Binding Exchange Layer ([[UMBEL]]) is an ontology of 28,000 reference concepts that maps to a simplified subset of the [[OpenCyc]] ontology, that is intended to provide a way of linking the precise OpenCyc ontology with less formal ontologies.<ref>{{cite web|url=http://www.mkbergman.com/441/the-role-of-umbel-stuck-in-the-middle-with-you/|title=The Role of UMBEL: Stuck in the Middle with You . . .|author=Mike Bergman|accessdate=2010-10-26}}</ref> It also has formal mappings to [[Wikipedia]], [[DBpedia]], [[PROTON Ontology|PROTON]] and [[GeoNames]]. It has been developed and maintained as [[open source]] by Structured Dynamics.

===UFO (Unified Foundational Ontology)===
The Unified Foundational Ontology (UFO), developed by Giancarlo Guizzardi and associates, incorporating developments from GFO, DOLCE and the Ontology of Universals underlying OntoClean in a single coherent foundational ontology. The core categories of UFO (UFO-A) have been completely formally characterized in Giancarlo Guizzardi's Ph.D. thesis and further extended at the Ontology and Conceptual Modelling Research Group (NEMO) in Brazil with cooperators from Brandenburg University of Technology (Gerd Wagner) and Laboratory for Applied Ontology (LOA). UFO-A has been employed to analyze structural conceptual modeling constructs such as object types and taxonomic relations, associations and relations between associations, roles, properties, datatypes and weak entities, and parthood relations among objects. More recent developments incorporate an ontology of events in UFO (UFO-B), as well as an ontology of social and intentional aspects (UFO-C). The combination of UFO-A, B and C has been used to analyze, redesign and integrate reference conceptual models in a number of complex domains such as, for instance, Enterprise Modeling, Software Engineering, Service Science, Petroleum and Gas, Telecommunications, and Bioinformatics. Another recent development aimed towards a clear account of services and service-related concepts, and provided for a commitment-based account of the notion of service (UFO-S),<ref>Nardi, J. C., Falbo, R. D. A., Almeida, J. P. A., Guizzardi, G., Pires, L. F., van Sinderen, M. J., & Guarino, N. (2013, September). "Towards a commitment-based reference ontology for services". In Enterprise Distributed Object Computing Conference (EDOC), 2013 17th IEEE International (pp. 175-184). IEEE.</ref>
UFO is the foundational ontology for [[OntoUML]], an ontology modeling language.

===WordNet===
{{main|WordNet}}
[[WordNet]], a freely available database originally designed as a [[semantic network]] based on [[psycholinguistic]] principles, was expanded by addition of definitions and is now also viewed as a [[dictionary]]. It qualifies as an upper ontology by including the most general concepts as well as more specialized concepts, related to each other not only by the [[subsumption relation]]s, but by other semantic relations as well, such as part-of and cause. However, unlike Cyc, it has not been formally axiomatized so as to make the logical relations between the concepts precise. It has been widely used in [[Natural language processing]] research.

===YAMATO (Yet Another More Advanced Top Ontology)===
YAMATO is developed by Riichiro Mizoguchi, formerly at the Institute of Scientific and Industrial Research of the [[University of Osaka]], and now at the [[Japan Advanced Institute of Science and Technology]]. Major features of YAMATO are:
# an advanced description of quality, attribute, property, and quantity,<ref>http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/YAMATO101216.pdf</ref> 
# an ontology of representation,<ref>{{cite journal|url=http://link.springer.com/article/10.1007/BF03040960#page-1|title=Part 3: Advanced course of ontological engineering|work=springer.com|accessdate=22 April 2015|doi=10.1007/BF03040960|volume=22|pages=193–220}}</ref> 
# an advanced description of processes and events,<ref>{{cite journal | doi = 10.3233/AO-2009-0067 }}</ref> 
# the use of a theory of roles.<ref>{{cite journal | last1 = Mizoguchi | first1 = R. | last2 = Sunagawa | first2 = E. | last3 = Kozaki | first3 = K. | last4 = Kitamura | first4 = Y. | year = | title = A Model of Roles within an Ontology Development Tool: Hozo | url = http://iospress.metapress.com/content/w67u25284x0l206v/ | journal = J. of Applied Ontology | volume = 2 | issue = 2| pages = 159–179 }}</ref>

YAMATO has been extensively used for developing other, more applied, ontologies such as a medical ontology,<ref>http://ceur-ws.org/Vol-833/paper9.pdf</ref> an ontology of gene,<ref>http://ceur-ws.org/Vol-897/session1-paper05.pdf</ref> an ontology of learning/instructional theories,<ref>{{cite web|url=http://edont.qee.jp/omnibus/doku.php|title=start    [OMNIBUS project]|date=6 December 2014|work=qee.jp|accessdate=22 April 2015}}</ref> an ontology of sustainability science,<ref>{{cite web|url=http://link.springer.com/article/10.1007%2Fs11625-008-0063-z|title=Toward knowledge structuring of sustainability science based on ontology engineering|work=springer.com|accessdate=22 April 2015}}</ref> and an ontology of the cultural domain.

== Upper/Foundational Ontology tools==

===ONSET===
{{unreferenced section|date=November 2014}}
ONSET, the foundational ontology selection and explanation tool, assists the domain ontology developer in selecting the most appropriate foundational ontology. The domain ontology developer provides the requirements/answers one or more questions, and ONSET computes the selection of the appropriate foundational ontology and explains why. The current version (v2 of 24 April 2013) includes DOLCE, BFO, GFO, SUMO, YAMATO and GIST.

===ROMULUS===
{{unreferenced section|date=November 2014}}
ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO. Features of ROMULUS include:
# It provides a high-level view of the foundational ontologies with only the most general concepts common to all implemented foundational ontologies. 
# Foundational ontologies in ROMULUS are modularised.
# Foundational ontology mediation has been performed. This includes alignment, mapping, merging, searchable metadata and an interchangeability method for foundational ontologies. 
# ROMULUS provides detailed taxonomies of each foundational ontology to allow easy browsing of foundational ontologies. 
# ROMULUS allows you to download each foundational ontology module including the integrated foundational ontologies. 
# Searchable metadata of each foundational ontology is available. 
# A comparison of the included foundational ontologies is available.

==See also==
* [[Authority control]]
* [[Formal ontology]]
* [[Foundations of mathematics]]
* [[Knowledge Organization Systems]]
* [[Library classification]]
* [[Ontology (information science)]]
* [[Physical ontology]]
* [[Process ontology]]
* [[Semantic interoperability]]
* [[Commonsense knowledge]]

==References==
{{Reflist}}

==External links==
{{External links|date=April 2015}}
* [http://www.onto-med.de/ontologies/gfo General Formal Ontology (GFO) homepage]
* [http://www.loa.istc.cnr.it/ Laboratory of Applied Ontology (LOA) homepage]
* [http://proton.semanticweb.org/ PROTON Ontology]
* [http://ontolog.cim3.net/cgi-bin/wiki.pl?UpperOntologySummit Upper Ontology Summit (March 2006)]
* [http://ontogenesis.knowledgeblog.org/740 What is an upper level ontology?] Knowledge Blog article, 2010.
* [http://www.ics.forth.gr/isl/MarineTLO/ The MarineTLO ontology] What, Why, Who, Current applications, How to exploit it, Documents and Publications, Provide feedback.
* [http://www.thezfiles.co.za/ROMULUS/Onset/webonset.html ONSET]
* [http://www.thezfiles.co.za/ROMULUS/ ROMULUS]
{{Computable knowledge}}

{{DEFAULTSORT:Upper Ontology (Information Science)}}
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
<=====doc_Id=====>:789
<=====title=====>:
Category:Minimum Information Standards
<=====text=====>:
{{Cat main|Minimum Information Standards}}

[[Category:Knowledge representation]]
[[Category:Standards by type]]
<=====doc_Id=====>:792
<=====title=====>:
Reification (knowledge representation)
<=====text=====>:
{{multiple issues|{{refimprove|date=December 2009}}
{{Expert-subject|Information Science|ex2=Cyc}}}}


'''Reification''' in [[knowledge representation]] is the process of turning a predicate into an object.<ref>{{cite web |last=Hunt |first=Matthew |title=Notes on Semantic Nets and Frames |url=http://www.eecs.qmul.ac.uk/~mmh/AINotes/AINotes4.pdf |date=1996 |access-date=15 June 2016}}</ref> Reification involves the representation of factual assertions that are referred to by ''other'' assertions, which might then be manipulated in some way; e.g., comparing [[logical assertion]]s from different [[witness]]es in order to determine their [[credibility]].

The message "John is six feet tall" is an assertion involving truth that commits the speaker to its factuality, whereas the reified statement "Mary reports that John is six feet tall" defers such commitment to Mary. In this way, the statements can be incompatible without creating contradictions in [[reasoning]]. For example, the statements "John is six feet tall" and "John is five feet tall" are mutually exclusive (and thus incompatible), but the statements "Mary reports that John is six feet tall" and "Paul reports that John is five feet tall" are not incompatible, as they are both governed by a conclusive rationale that either Mary or Paul is (or both are), in fact, incorrect.

In Linguistics, reporting, telling, and saying are recognised as ''verbal processes that project a wording (or locution)''. If a person says that "Paul told x" and "Mary told y", this person stated only that the telling took place. In this case, the person who made these two statements did not represent a person inconsistently. In addition, if two people are talking to each other, let's say Paul and Mary, and Paul tells Mary "John is five feet tall" and Mary rejects Paul's statement by saying "No, he is actually six feet tall", the socially constructed model of John does not become inconsistent. The reason for that is that statements are to be understood as an attempt to convince the addressee of something (Austin's How to do things with words), alternatively as a request to add some attribute to the model of Paul. The response to a statement can be an acknowledgement, in which case the model is changed, or it can be a statement rejection, in which case the model does not get changed. Finally, the example above for which John is said to be "five feet tall" or "six feet tall" is only incompatible because John can only be a single number of feet tall. If the attribute were a possession as in "he has a dog" or "he also has a cat", a model inconsistency would not happen. In other words, the issue of model inconsistency has to do with our model of the domain element (John) and not with the ascription of different range elements (measurements such as "five feet tall" or "six feet tall") nor with statements.

==See also==
*[[Reification (computer science)]]
*[[Reification (fallacy)]]
*[[Reification (linguistics)]]

==References==
{{Reflist}}

{{DEFAULTSORT:Reification (Knowledge Representation)}}
[[Category:Knowledge representation]]
<=====doc_Id=====>:795
<=====title=====>:
GNOWSYS
<=====text=====>:
{{Refimprove|date=June 2011}}
{{Infobox software
|  name = GNOWSYS
|  logo = [[Image:Gnowsys-logo.png|100px]]
|  developer = [[GNU|The GNU Project]]
|  latest_release_version = 1.0 rc1
|  operating_system = [[Cross-platform]]
|  genre = [[Semantic web|Semantic computing]]
|  license = [[GNU General Public License|GPL]]
|  website = [https://www.gnu.org/software/gnowsys/ www.gnu.org/software/gnowsys/]
}}
'''GNOWSYS''' (Gnowledge Networking and Organizing system) is a specification for a generic [[distributed network]] based memory/[[knowledge management]]. It is developed as an application for developing and maintaining [[semantic web]] content. It is written in [[Python (programming language)|Python]]. It is implemented as a [[Django (web framework)|Django]] app.

The memory of GNOWSYS is designed as a node-oriented space. A node is described by other nodes to which it has links. The nodes are organized and processed according to a complex data structure called the neighborhood.<ref name="gnu">[https://www.gnu.org/software/gnowsys/] GNOWSYS: A Kernel for Semantic Computing.</ref>

==Applications==

The application can be used for web-based knowledge representation and content management projects, for developing structured knowledge bases, as a collaborative authoring tool, suitable for making electronic glossaries, dictionaries and encyclopedias, for managing large web sites or links, developing an online catalogue for a library of any thing including books, to make ontologies, classifying and networking any objects, etc. This tool is also intended to be used for an on-line tutoring system with dependency management between various concepts or software packages.  For example, the dependency relations between [[Debian GNU/Linux]] packages have been represented by the [http://www.gnowledge.org/search_debmap?val=1 gnowledge portal].

==Component Classes==
The kernel is designed to provide support to persistently store highly granular nodes of knowledge representation like terms, predicates and very complex propositional systems like arguments, rules, axiomatic systems, loosely held paragraphs, and more complex structured and consistent compositions. All the component classes in GNOWSYS are classified according to complexity into three groups, where the first two groups are used to express all possible well formed formulae permissible in a first order logic.<ref name="conceptPaper">[http://www.hbcse.tifr.res.in/gn/concept_paper.pdf GNOWSYS: A System for Semantic Computing ]</ref>

===Terms===
‘Object’, ‘Object Type’ for declarative knowledge, ‘Event’, ‘Event Type’, for temporal objects, and ‘Meta Types’ for expressing upper ontology. The
objects in this group are essentially any thing about which the [[knowledge engineer]] intends to express and store in the knowledge base, i.e., they are the objects of discourse. The instances of these component classes can be stored with or without expressing ‘instance of’ or ‘sub-class of’ relations among them.

===Predicates===
This group consists of ‘Relation’, and ‘Relation Type’ for expressing declarative knowledge, and ‘Function’ and ‘Function Type’ for expressing procedural knowledge. This group is to express qualitative and quantitative relations among the various instances stored in the knowledge base. While instantiating the predicates can be characterized by their logical properties of relations, quantifiers and cardinality as monadic predicates
of these predicate objects.

===Structures===
‘System’, ‘Encapsulated Class’, ‘Program’, and ‘Process’, are other base classes for complex structures, which can be combined iteratively to produce more complex systems. The component class ‘System’ is to store in the knowledge base a set of propositions composed into ontologies, axiomatic systems, complex systems like say a human body, an artifact like a vehicle etc., with or without consistency check. An ‘Encapsulated Class’ is to com-
pose declarative and behavioural objects in a flexible way to build classes. A ‘Program’ is not only to store the logic of any complete program or a component class, composed from the already available behavioural instances in the knowledge base with built-in connectives (conditions, and loops), but also execute them as web services. A ‘Process’ is to structure temporal objects with sequence, concurrency, synchronous or asynchronous specifications.

Every node in the database keeps the neighbourhood information, such as its super-class, sub-class, instance-of, and other relations, in which the object has a role, in the form of predicates. This feature makes computation of drawing graphs and inferences, on the one hand, and dependency and navigation paths on the other hand very easy.  All the data and metadata is indexed in a central catalogue making query and locating resources efficient.

==References==
{{Reflist}}

==External links==
{{Portal|Free software}}
* [http://www.gnowledge.org/ Welcome to Gnowledge!]
* [https://www.gnu.org/software/gnowsys/ GNOWSYS is part of the GNU project.]

{{GNU}}

{{DEFAULTSORT:Gnowsys}}
[[Category:Cross-platform free software]]
[[Category:Free network-related software]]
[[Category:GNU Project software]]
[[Category:Knowledge representation]]
[[Category:Semantic Web]]
<=====doc_Id=====>:798
<=====title=====>:
Enterprise interoperability
<=====text=====>:
'''Enterprise interoperability''' is the ability of an enterprise—a company or other large organization—to functionally link activities, such as [[product design]], [[supply chains]], manufacturing, in an efficient and competitive way. 

The research in interoperability of enterprise practised in is various domains itself ([[Enterprise Modelling]], [[Ontologies]], [[Information systems]], Architectures and Platforms) which it is a question of positioning.<ref name="Doumei2008"/>

== Enterprise interoperability topics ==
===Interoperability in enterprise architecture===
Enterprise architecture (EA) presents a high level design of enterprise capabilities that defines successful IT projects in coherence with enterprise principals and business related requirements. EA covers mainly (i) the business capabilities analysis and validation; (ii) the development of business, application, data and technical architectures and solutions, and finally (iii) the control of programme and project implementation and governance. The application of EA methodology feeds the enterprise repository reference frame with sets of building blocks used to compose the targeted system.

The interoperability can be considered either as a principal, requirement or constraint that impact the definition of patterns to compose building blocks in the definition of targeted architectural roadmap. In this scope, EA within the TOGAF perspective,<ref name="TOGAF2011"/> aims to reconcile interoperability requirements with potential solutions that make developed systems interoperable.
So as to maintain the interoperability challenge quite present in the next steps of system’s lifecycle, several models and Frameworks are developed under the topic enterprise interoperability.

===Enterprise interoperability frameworks===
To preserve interoperability, several [[Enterprise Interoperability Framework|enterprise interoperability frameworks]] can be identified in the literature:
* 2003: IDEAS:<ref name="IDEAS"/> Interoperability Developments for Enterprise Application and Software.
* 2004: EIF:<ref name="EIF"/> The European Interoperability Framework
* 2004: e-GIF:<ref name="eGIF"/> e-Government Interoperability Framework
* 2006: FEI:<ref name="FEI"/> The Framework for Enterprise Interoperability
* 2006: C4IF:<ref name="Peristeras2006"/> Connection, Communication, Consolidation, Collaboration Interoperability Framework
* 2007: AIF:<ref name="ATHENA"/> Athena Interoperability Framework
* 2007:<ref name="EntSysArch2007"/>  Enterprise Architecture Framework for Agile and Interoperable Virtual Enterprises

The majority of these frameworks considers enterprise at several aspects, viewpoints or abstraction levels: business, process, knowledge, application, technology, data, technic, etc. and proposes guidelines to support modeling and connection capabilities between these levels. The semantic challenge is considered as transversal to all these abstraction levels.
Setting up and applying guidelines and methodologies developed within these frameworks requires modeling efforts that identify and connect artifacts.

===Interoperability in software engineering===
The evolution of IT technologies aims to outsource IT capabilities to vendors to manage for use on demand. The evolution pathway starts form packaged solutions and goes through Infrastructure as a service (Iaas), Platform as a service (Paas), Software as a service (Saas) and recently the Cloud. Interoperability efforts are still mainly expected among these levels:
* strategy to business 
* business to processes
* processes to application

Dealing with business process definition, alignment, collaboration and interoperability, several international standards propose methodologies and guidelines in these perspectives:
* ISO 15704—Requirements for enterprise-reference architectures and methodologies
* CEN-ISO DIS 19439—Framework for Enterprise Modeling
* CEN-ISO WD 19440—Constructs for Enterprise Modeling
* ISO 18629—Process specification language
* ISO/IEC 15414—ODP Reference Model—Enterprise Language 
In addition, recent standards (BPMN, BPEL, etc.) and their implementation technologies propose relevant integration capabilities. Furthermore, model driven-engineering <ref name="MDAguide"/> provides capabilities that connect, transform and refine models to support interoperability.

===Metrics for interoperability maturity assessment===
The following approaches propose some metrics to assess the interoperability maturity,<ref name="Ford2008"/><ref name="GUEDRIA"/> 
* LISI: Levels of Information Systems Interoperability
* OIM: Organizational Interoperability Model
* NMI: NC3TA reference Model for Interoperability
* LCIM: Levels of Conceptual Interoperability Model
* EIMM: Enterprise Interoperability Maturity Model
* Smart Grid Interoperability Maturity Model Rating System

For the several interoperability aspects identified previously, the listed maturity approaches define interoperability categories (or dimensions) and propose qualitative as well as qualitative cross cutting issues to assess them. While interoperability aspects are not covered by a single maturity approach, some propositions go deeply in the definition of metric dimensions at one interoperability aspect such as the business interoperability measurement proposed by Aneesh.<ref name="Zutshi"/>

== See also ==
* [[INTEROP-VLab]]

== References ==
{{reflist|
refs=
<ref name=Doumei2008>Chen, D., [[Guy Doumeingts|Doumeingts]], G., and [[François Vernadat|Vernadat, F.]] 2008. Architectures for enterprise integration and interoperability: Past, present and future. ''Comput. Ind.'' 59, 7 (Sep. 2008), 647–659. {{en icon}} [http://dx.doi.org/10.1016/j.compind.2007.12.016 : DOI]</ref>
<ref name=TOGAF2011>TOGAF® 9 Certified, 2nd edition. The Open Group, 2011.</ref>
<ref name=IDEAS>“A Contribution to Enterprise Interoperability Maturity Assessment”</ref>
<ref name=EIF>EIF 2.0 http://ec.europa.eu/idabc/servlets/Docb0db.pdf</ref>
<ref name=eGIF>http://edina.ac.uk/projects/interoperability/e-gif-v6-0_.pdf</ref>
<ref name=FEI>http://chen33.free.fr/M2/Elearning/CIGI2009.Chen.final.pdf</ref>
<ref name=Peristeras2006>Peristeras, V., and K. Tarabanis (2006): The Connection, Communication, Consolidation, Collaboration Interoperability Framework (C4IF) for Information Systems Interoperability, International Journal of Interoperability in Business Information Systems (IBIS), Vol. 1, No. 1, pp. 61-72.</ref>
<ref name=ATHENA>http://www.asd-ssg.org/html/ATHENA/Deliverables/Deliverables%20provided%20to%20EC%206th%206%20Months/070306_ATHENA_DA82_V10.pdf</ref>
<ref name=EntSysArch2007>Handbook of Enterprise Systems Architecture in Practice, 2007</ref>
<ref name=MDAguide>http://www.omg.org/cgi-bin/doc?omg/03-06-01.pdf</ref>
<ref name=Ford2008>Ford T., et al. Measuring System Interoperability: An i-Score Improvement. Proceedings of the 6th Annual Conference on Systems Engineering Research. Los Angeles, CA, April 4–5, 2008</ref>
<ref name=GUEDRIA>GUEDRIAhttp://ori-oai.u-bordeaux1.fr/pdf/2012/GUEDRIA_WIDED_2012.pdf</ref>
<ref name=Zutshi>http://run.unl.pt/bitstream/10362/2646/1/Zutshi_2010.pdf</ref>
}}

== External links ==
* [http://www.interop-vlab.eu INTEROP-VLab]

[[Category:Interoperability]]
[[Category:Enterprise modelling]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:801
<=====title=====>:
Lumpers and splitters
<=====text=====>:
{{Refimprove|date=January 2014}}

'''Lumpers''' and '''splitters''' are opposing factions in any [[discipline]] that has to [[Categorization|place individual examples into rigorously defined categories]]. The lumper-splitter problem occurs when there is the need to create classifications and assign examples to them, for example schools of [[literature]], [[biology|biological]] [[taxon|taxa]] and so on. A "lumper" is an individual who takes a [[wikt:gestalt|gestalt]] view of a definition, and assigns examples broadly, assuming that differences are not as important as signature similarities. A "splitter" is an individual who takes precise definitions, and creates new categories to classify samples that differ in key ways.

==Origin of the terms==
The earliest use of these terms was by [[Charles Darwin]], in a letter to [[J. D. Hooker]] in 1857, "(Those who make many species are the 'splitters,' and those who make few are the 'lumpers.')" They were introduced more widely by [[George G. Simpson]] in his 1945 work "The Principles of Classification and a Classification of Mammals." As he put it, "splitters make very small units – their critics say that if they can tell two animals apart, they place them in different genera … and if they cannot tell them apart, they place them in different species. … Lumpers make large units – their critics say that if a [[Carnivora|carnivore]] is neither a dog nor a bear, they call it a cat."<ref>{{Cite journal|last=Simpson|first=George G.|title=The Principles of Classification and a Classification of Mammals|journal=Bulletin of the AMNH|volume=85|page=23|year=1945|publisher=American Museum of Natural History|location=New York}}</ref>

Another early use can be found in the title of a 1969 paper by the medical geneticist [[Victor McKusick]]: "On lumpers and splitters, or the nosology of genetic disease."<ref>McKusick VA. On lumpers and splitters, or the nosology of genetic disease. Perspect Biol Med. 1969 Winter;12(2):298-312.</ref>

Reference to lumpers and splitters also appeared in a debate in 1975 between [[J. H. Hexter]] and [[John Edward Christopher Hill|Christopher Hill]], in the ''[[Times Literary Supplement]]''. It followed from Hexter's detailed review of Hill's book ''Change and Continuity in Seventeenth Century England'', in which Hill developed [[Max Weber]]'s argument that the rise of capitalism was facilitated by [[Calvinist]] Puritanism. Hexter objected to Hill's 'mining' of sources to find evidence that supported his theories. Hexter argued that Hill plucked quotations from sources in a way that distorted their meaning. Hexter explained this as a mental habit that he called 'lumping'. According to him, lumpers rejected differences and chose to emphasize similarities. Any evidence that did not fit their arguments was ignored as aberrant. Splitters, by contrast, emphasised differences, and resisted simple schemes. While lumpers consistently tried to create coherent patterns, splitters preferred incoherent complexity.<ref>{{Cite journal|last=Chase|first=Bob|title=Upstart Antichrist|journal=History Workshop Journal|issue=60|date=Autumn 2005|pages=202-206}}</ref>

==Usage in various fields==

===Biology===
{{anchor|Lumping and splitting in biology}}
{{main|Biological classification}}
The categorization and naming of a particular species should be regarded as a ''hypothesis'' about the [[evolution]]ary relationships and distinguishability of that group of organisms. As further information comes to hand, the hypothesis may be confirmed or refuted. Sometimes, especially in the past when communication was more difficult, taxonomists working in isolation have given two distinct names to individual [[organism]]s later identified as the same species. When two named species are agreed to be of the same species, the older species name is almost always retained dropping the newer species name honoring a convention known as "priority of nomenclature".  This form of lumping is technically called synonymization. Dividing a taxon into multiple, often new, taxa is called splitting. Taxonomists are often referred to as "lumpers" or "splitters" by their colleagues, depending on their personal approach to recognizing differences or commonalities between organisms.

=== History ===
{{main|Periodization}}

In history, lumpers are those who tend to create broad definitions that cover large periods of time and many disciplines, whereas splitters want to assign names to tight groups of inter-relationships. Lumping tends to create a more and more unwieldy definition, with members having less and less mutually in common. This can lead to definitions which are little more than conventionalities, or groups which join fundamentally different examples. Splitting often leads to "[[Distinction without a difference|distinctions without difference]]," ornate and fussy categories, and failure to see underlying similarities.

For example, in the arts, "[[Romanticism|Romantic]]" can refer specifically to a period of [[Germany|German]] poetry roughly from 1780–1810, but would exclude the later work of [[Goethe]], among other writers. In music it can mean every composer from [[Johann Nepomuk Hummel|Hummel]] through [[Sergei Rachmaninoff|Rachmaninoff]], plus many that came after.

=== Software modelling ===
[[Software engineering]] often proceeds by building models (sometimes known as [[model-driven architecture]]). A lumper is keen to generalize, and produces models with a small number of broadly defined objects. A splitter is reluctant to generalize, and produces models with a large number of narrowly defined objects. Conversion between the two styles is not necessarily symmetrical. For example, if error messages in two narrowly defined classes behave in the same way, the classes can be easily combined. But if some messages in a broad class behave differently, every object in the class must be examined before the class can be split. This illustrates the principle that "splits can be lumped more easily than lumps can be split."<ref name=Pugh2005>{{cite book|last1=Pugh|first1=Ken|title=Prefactoring|date=2005|publisher=O'Reilly Media|pages=14–15|url=https://books.google.com/books?id=8nykB7qerJYC&pg=PA15#v=onepage&q&f=false|accessdate=2014-10-21}}</ref>

=== Language classification ===
{{main|Language classification}}

There is no agreement among [[Historical linguistics|historical linguists]] about what amount of evidence is needed for two languages to be safely classified in the same [[language family]]. For this reason, many language families have had lumper–splitter controversies, including [[Altaic languages|Altaic]], [[Pama–Nyungan languages|Pama–Nyungan]], [[Nilo-Saharan]], and most of the larger [[Classification schemes for indigenous languages of the Americas|families of the Americas]]. At a completely different level, the splitting of a [[mutually intelligible]] [[dialect continuum]] into different languages, or lumping them into one, is also an issue that continually comes up, though the consensus in contemporary linguistics is that there is no completely objective way to settle the question.

Splitters regard the [[comparative method (linguistics)|comparative method]] (meaning not comparison in general, but only reconstruction of a common ancestor or [[protolanguage]]) as the only valid proof of kinship, and consider [[genetic (linguistics)|genetic]] relatedness to be the question of interest. American linguists of recent decades tend to be splitters.

Lumpers are more willing to admit techniques like [[mass lexical comparison]] or [[lexicostatistics]], and mass typological comparison, and to tolerate the uncertainty of whether relationships found by these methods are the result of [[linguistic divergence]] (descent from common ancestor) or [[language convergence]] (borrowing). Much long-range comparison work has been from Russian linguists like [[Vladislav Illich-Svitych]] and [[Sergei Starostin]]. In the US, [[Joseph Greenberg|Greenberg]]'s and [[Merritt Ruhlen|Ruhlen]]'s work has been met with little acceptance from linguists. Earlier American linguists like [[Morris Swadesh]] and [[Edward Sapir]] also pursued large-scale classifications like [[Classification schemes for indigenous languages of the Americas#Sapir .281929.29: Encyclop.C3.A6dia Britannica|Sapir's 1929 scheme for the Americas]], accompanied by controversy similar to that today.<ref>http://www.nostratic.ru/books/(137)ruhlen12.pdf [[Merritt Ruhlen]]: Is Algonquian Amerind?</ref>

=== Liturgical studies ===
[[Paul F. Bradshaw]] suggests that the same principles of lumping and splitting apply to the study of early Christian [[liturgy]]. Lumpers, who tend to predominate, try to find a single line of texts from the apostolic age to the fourth century (and later). Splitters see many parallel and overlapping strands which intermingle and flow apart so that there is not a single coherent path in development of liturgical texts. Liturgical texts must not be taken solely at face value; often there are hidden agendas in texts.<ref name="bradshaw">Bradshaw, Paul F., ''The Search for the Origins of Christian Worship'', Oxford University Press, 2002, p. ix. ISBN 0-19-521732-2</ref>

The Hindu religion is essentially a lumper's concept, sometimes also known as [[Smartism]].  Hindu splitters, and individual adherents, often identify themselves as adherents of a religion such as [[Shaivism]], [[Vaishnavism]], or [[Shaktism]] according to which deity they believe to be the supreme creator of the universe.{{Citation needed|date=June 2012}}

===Philosophy===
[[Freeman Dyson]] has suggested that "observers of the philosophical scene" can be broadly, if over-simplistically, divided into splitters and lumpers, roughly corresponding to [[materialists]], who imagine the world as divided into atoms, and [[Platonists]], who regard the world as made up of ideas.

== See also ==
* [[Evolutionary biology]]
* [[Heterarchy]]
* [[Prototype theory]]
* [[Sorites paradox]]

==References==
{{Reflist|30em}}

==External links==
* [http://www.users.globalnet.co.uk/~rxv/infomgt/abstraction.htm#lumpersplitter Abstraction: Lumpers and Splitters]
* [http://www.tvtropes.org/pmwiki/pmwiki.php/Main/LumperVsSplitter Lumper Vs. Splitter] on [[TV Tropes|TV Tropes, a wiki dedicated to recurring themes in fiction, metafiction, and real life]]

{{DEFAULTSORT:Lumpers And Splitters}}
[[Category:Knowledge representation]]
[[Category:Taxonomy]]
<=====doc_Id=====>:804
<=====title=====>:
IDIS (software)
<=====text=====>:
{{Unreferenced|date=December 2009}}

'''IDIS''' is a [[software]] tool for direct [[data exchange]] between [[CDS/ISIS]] and [[IDAMS]]. It is developed, maintained and disseminated by [[UNESCO]].

==See also==
*[[CDS/ISIS]] - database software

{{DEFAULTSORT:Idis (Software)}}
[[Category:Knowledge representation]]


{{network-software-stub}}
<=====doc_Id=====>:807
<=====title=====>:
Category:Ontology (information science)
<=====text=====>:
{{Cat main|Ontology (information science)}}
{{Commons category|Ontology}}

[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Ontology|Information science]]
<=====doc_Id=====>:810
<=====title=====>:
AGROVOC
<=====text=====>:
'''AGROVOC''' (a portmanteau of agriculture and vocabulary) is a multilingual controlled vocabulary covering all areas of interest to the [[Food and Agriculture Organization of the United Nations]] (FAO), including food, nutrition, agriculture, fisheries, forestry and the environment. The vocabulary consists of over 32,000 concepts with up to 40,000 terms in 23 languages: Arabic, Chinese, Czech, English, French, German, Hindi, Hungarian, Italian, Japanese, Korean, Lao, Malay, Persian, Polish, Portuguese, Russian, Slovak, Spanish, Telugu, Thai, Turkish and Ukrainian. It is a collaborative effort, edited by a community of experts and coordinated by FAO.

AGROVOC is made available by FAO as an [[Resource Description Framework|RDF]]/[[SKOS]]-XL concept scheme and published as a [[linked data]] set aligned to 16 other vocabularies.

==History==
AGROVOC was first published at the beginning of the 1980s by FAO in English, Spanish and French to serve as a controlled vocabulary to index publications in agricultural science and technology, especially for [[AGRIS]].

In the 1990s, AGROVOC abandoned paper printing and went digital with data storage handled by a relational database. In 2004, preliminary experiments with expressing AGROVOC into the [[Web Ontology Language]](OWL) took place. At the same time a web based editing tool was developed, then called WorkBench, nowadays VocBench. In 2009 AGROVOC became an SKOS resource.

Today, AGROVOC is available in 23 languages as an SKOS-XL concept scheme and published as a Linked Open Data (LOD) set aligned to 16 other data sets related to agriculture.

==Users==
AGROVOC is used by researchers, librarians and information managers for indexing, retrieving and organizing data in agricultural information systems and web pages.<ref>[http://aims.fao.org/standards/agrovoc/uses-agrovoc AGROVOC Uses]</ref> Within the context of the [[Semantic Web]] also new users are emerging, like software developers and ontology builders.

==Access==
AGROVOC is accessible in various ways:

* Online: Search <ref>[http://aims.fao.org/standards/agrovoc/functionalities/search AGROVOC search]</ref> and browse AGROVOC on the [[Agricultural Information Management Standards]] (AIMS) website. 
* Download: RDF-SKOS (AGROVOC only or AGROVOC LOD).<ref>[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC Releases]</ref> 
* Live: SPARQL endpoint <ref>[http://202.45.139.84:10035/catalogs/fao/repositories/agrovoc AGROVOC SPARQL endpoint]</ref> and the AGROVOC Web services.<ref>[https://aims-fao.atlassian.net/wiki/display/AGV/Releases AGROVOC releases]</ref>

==Maintenance==
The AGROVOC team, located at FAO Headquarter, coordinates the editorial activities related to the maintenance of AGROVOC. The actual maintenance is carried out by a community of editors and institutions<ref>[http://aims.fao.org/standards/agrovoc/community AGROVOC people]</ref> for each of the 23 language versions.

The tool used by the community to edit and maintain AGROVOC is Vocbench, which was designed to meet the needs of the Semantic Web and linked data environments. VocBench provides tools and functionalities that facilitate both collaborative editing and multilingual terminology. It also includes administration and group management features that permit flexible roles for maintenance, validation and quality assurance.

FAO also facilitates the technical maintenance of AGROVOC, including its publication as a LOD resource. Technical support is provided by the University of Tor Vergata<ref>[http://art.uniroma2.it/ Tor Vergata University]</ref> (Rome, Italy) which leads the technical development of VocBench. The technical infrastructure for the online publication of AGROVOC is hosted by MIMOS Berhad<ref>[http://www.mimos.my/ MIMOS Berhad]</ref> (Kuala Lumpur, Malaysia).

==Structure==
All 32,000+ concepts of the AGROVOC thesaurus are hierarchically organized under 25 top concepts. AGROVOC top concepts are very general and high level concepts, like “activities”, “organisms”, “location”, “products” etc. More than half of the total number of concepts (20,000+) fall under the top concept “organism”, which confirms how AGROVOC is largely oriented towards the agricultural sector.

AGROVOC is an RDF/SKOS-XL concept scheme, meaning the conceptual and terminological level are separated. The basic notions for such a concept scheme are: concepts, their labels and relations.

*'''Concepts''' 
Concepts are anything we want to represent or “talk about” in our domain. Concepts are represented by terms. A concept could also be considered as the set of all terms used to express it in various languages.
In SKOS, concepts are formalized as skos:Concept, identified by dereferenceable URIs (= URL). For example, the AGROVOC concept with URI http://aims.fao.org/aos/agrovoc/c_12332 is for ''maize''.

*'''Terms''' 
Terms are the actual terms used to name a concept. For example ''maize'', ''maïs'', ''玉米'', ''ข้าวโพด'' are all terms used to refer to the same concept in English, French, Chinese and Hindi respectively.

AGROVOC terms are expressed by means of the SKOS extension for labels, SKOS-XL. The predicates used are:
skosxl:prefLabel, used for preferred terms (“descriptors” in thesaurus terminology), and 
skosxl:altLabel, used for non- preferred terms.

*'''Relations'''
In SKOS, hierarchical relations between concepts are expressed by the predicates skos:broader, skos:narrower. They correspond to the classical thesaurus relations broader/narrower (BT/NT).

Non-hierarchical relations express a notion of “relatedness” between concepts. AGROVOC uses the SKOS relation skos:related (corresponding to the classical thesaurus RT), and a specific vocabulary of relations called Agrontology.<ref>[https://aims-fao.atlassian.net/wiki/display/AGV/Agrontology Agrontology]</ref>

AGROVOC also allows for relations between labels (i.e. terms), thanks to the SKOS-XL extension to SKOS.

==Linked data==
AGROVOC is available as a linked data set and is aligned (linked) with 16 vocabularies related to agriculture (see table down below). The linked data version of AGROVOC is exposed as RDF and HTML, through a content-negotiation mechanism. It is also exposed through a SPARQL endpoint.

The advantage of having a thesaurus like AGROVOC published as LOD is that once thesauri are linked, the resources they index are linked as well. A good example is AGRIS, a mash-up web application that links the AGRIS bibliographic repository (indexed with AGROVOC) to related web resources (indexed with vocabularies linked to AGROVOC).

{| class="wikitable"
|-
! Resource !! Topics !! Linked concpets !! Languages !! Linked data !! Type of link
|-
| ASFA || Fisheries|| 1784|| || || skos:closeMatch
|-
| FAO Biotechnology Glossary || Biotechnologies|| 810|| EN, ES, FR, +3 more|| Yes|| skos:closeMatch
|-
| Chinese Agriculture Thesaurus (CAT)|| Agriculture|| || || Yes|| skos:closeMatch
|-
| EARTh|| Environment|| 1363 || EN+|| Yes|| skos:closeMatch
|-
| EUROVOC|| General EU || 1,297 || EN, ES, FR + 21 more || Yes || skos:exactMatch
|-
| GEMET || Environment || 1,191 || EN, ES, FR + 30 more || Yes|| skos:exactMatch
|-
| Library of Congress Subject Headings (LCSH)|| General || 1,093 || EN || Yes || skos:exactMatch
|-
| NAL Thesaurus || Agriculture || 13,390 || EN,ES || Yes || skos:exactMatch
|-
| RAMEAU Répertoire d'autorité-matière encyclopedique et alphabetique unifie  || General || 686 || FR || Yes || skos:exactMatch
|-
| STW - Thesaurus for Economics || Economy || 1,136 || EN, DE || Yes || skos:exactMatch
|-
| TheSoz - Thesaurus for the Social Sciences || Social sciences || 846 || EN,DE || Yes || skos:exactMatch
|-
| Geopolical Ontology || Geopolitical entities || 253 || AR, CH, EN, ES, FR, RU || Yes || skos:exactMatch
|-
| Dewey Decimal Classification (DDC) || General || 409 || EN, ES, FR + 8 more || Yes || skos:exactMatch
|-
| DBpedia || General || 10,989 || EN, ES, FR + 8 more || Yes || skos:exactMatch
skos:closeMatch
|-
| SWD (Schlagwortnormdatei)|| General || 6,245 || DE || Yes || skos:exactMatch
skos:closeMatch
skos:broadMatch
skos:narrowMatch
|-
| GeoNames || Geographical entities || 212 || EN, ES, FR + 63 more || Yes || skos:exactMatch
|}

==Copyright and license==
The copyright for the AGROVOC thesaurus content in English, French, Russian and Spanish stays with FAO and is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 3.0 Unported License.<ref>[http://creativecommons.org/licenses/by-sa/3.0/ Creative Commons Attribution ShareAlike 3.0 Unported]</ref> For any other language, the copyright rests with the institution responsible for its production.

==Related links==
* [[Agricultural Information Management Standards]]
* [[AGRIS]]
* [[Food and Agriculture Organization]]
* [[Geopolitical ontology]]

==External links==
* [http://agris.fao.org/ AGRIS]
* [http://aims.fao.org/standards/agrovoc AGROVOC]
* [http://aims.fao.org/ AIMS]
* [http://www.fao.org/home/en/ FAO]
* [https://www.w3.org/2001/sw/wiki/VocBench VocBench/Agricultural Ontology Server]

==Further reading==
* [http://aims.fao.org/standards/agrovoc/publications AGROVOC Publications]

==References==
{{reflist}}

{{DEFAULTSORT:Agrovoc}}
[[Category:Agricultural databases]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Food and Agriculture Organization]]
[[Category:Thesauri]]
<=====doc_Id=====>:813
<=====title=====>:
Semantic parameterization
<=====text=====>:
'''Semantic parameterization''' is a conceptual modeling process for expressing natural language descriptions of a domain in first-order predicate logic.<ref>Travis D. Breaux and Annie I. Antón (2004). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2004-36.pdf Deriving Semantic Models from Privacy Policies]. North Carolina State University Computer Science Technical Report TR-2004-36.</ref><ref>Travis D. Breaux and Annie I. Antón (2008). [http://theprivacyplace.org/blog/wp-content/uploads/2008/07/tr-2005-31.pdf "Mining Rule Semantics to Understand Legislative Compliance"]. North Carolina State University Computer Science Technical Report TR-2005-31.</ref><ref name="Breaux">T.D. Breaux, A.I. Anton, J. Doyle, [http://www4.ncsu.edu/~tdbreaux/publications/tdbreaux-tosem09.pdf "Semantic parameterization: a process for modeling domain descriptions"], ''ACM Transactions on Software Engineering Methodology'', vol. 18, no. 2, Article 5, 2008.</ref> The process yields a formalization of natural language sentences in [[Description Logic]] to answer the ''who,'' ''what'' and ''where'' questions in the Inquiry-Cycle Model (ICM) developed by Colin Potts and his colleagues at the Georgia Institute of Technology.<ref name="Potts">C. Potts, K. Takahashi, and A.I. Anton, "Inquiry-based requirements analysis", ''IEEE Software'' 11(2): 21–32, 1994.</ref> The parameterization process complements the Knowledge Acquisition and autOmated Specification (KAOS) method,<ref>A. Dardenne, A. van Lamsweerde and S. Fickas, "Goal-Directed Requirements Acquisition", ''Science of Computer Programming'' v. 20, North Holland, 1993, pp. 3-50.</ref> which formalizes answers to the ''when'', ''why'' and ''how'' ICM questions in [[Temporal Logic]], to complete the ICM formalization. The artifacts used in the parameterization process include a dictionary that aligns the domain lexicon with unique concepts, distinguishing between [[synonyms]] and [[polysemes]], and several natural language patterns that aid in mapping common domain descriptions to formal specifications.

== Relationship to other theories ==

Semantic Parameterization defines a meta-model consisting of eight roles that are domain-independent and reusable. Seven of these roles correspond to Jeffrey Gruber's [[thematic relations]]<ref>J. Gruber, ''Lexical Structures in Syntax and Semantics'', North Holland, New York, 1976.</ref> and [[case role]]s in Charles Fillmore's [[case grammar]]:<ref>C. Fillmore, "The Case for Case", ''Universals in Linguistic Theory'', Holt, Rhinehart and Winston, New York, 1968.</ref>

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ Meta-model Mapping to Case Frames and Thematic Relations
! Breaux's Meta-model
! Fillmore's Case Roles
! Thematic Relations
|-
| Subject
| Agentive
| Agent
|-
| Action
|
|-
| Object
| Objective/ Factitive
| Theme/ Patient
|-
| Target
| Dative
| Goal
|-
| Source
| Source
| Source
|-
| Instrument
| Instrumental
| Instrument
|-
| Purpose
|
| Purposive
|-
| Location
| Locative
| Location
|-
|
| Comitative
| Accompaniment
|}

The Inquiry-Cycle Model (ICM) was introduced to drive elicitation between engineers and stakeholders in requirements engineering.<ref name="Potts" /> The ICM consists of ''who'', ''what'', ''where'', ''why'', ''how'' and ''when'' questions. All but the ''when'' questions, which require a [[Temporal Logic]] to represent such phenomena, have been aligned with the meta-model in semantic parameterization using [[Description Logic]] (DL).

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ Mapping from DL roles to questions in the Inquiry-Cycle Model
! DL Role in Meta-model
! ICM Question
|-
| isSubjectOf.Activity
| Who performs the action?
|-
| isObjectOf.Activity
| Upon what is the action performed?
|-
| isTargetOf.Activity
| With whom is the transaction performed?
|-
| isPurposeOf.Activity
| Why is the action performed?
|-
| isInstrumentOf.Activity
| How is the action performed?
|-
| isLocationOf.Activity
| Where is the action performed?
|}

== Introduction with Example ==

The semantic parameterization process is based on [[Description Logic]], wherein the TBox is composed of words in a ''dictionary'', including nouns, verbs, and adjectives, and the ABox is partitioned into two sets of assertions: 1) those assertions that come from words in the natural language statement, called the ''grounding'', and 2) those assertions that are inferred by the (human) modeler, called the ''meta-model''. Consider the following unstructured natural language statement (UNLS) (see Breaux et al.<ref name="Breaux" /> for an extended discussion):

;UNLS<sub>1.0</sub>: The customer<sub>1,1</sub> must not share<sub>2,2</sub> the access-code<sub>3,3</sub> of the customer<sub>1,1</sub> with someone<sub>4,4</sub> who is not the provider<sub>5,4</sub>.

The modeler first identifies intensional and extensional polysemes and synonyms, denoted by the subscripts: the first subscript uniquely refers to the intensional index, i.e., the same first index in two or more words refer to the same concept in the TBox; the second subscript uniquely refers to the extensional index, i.e., two same second index in two or more words refer to the same individual in the ABox. This indexing step aligns words in the statement and concepts in the dictionary. Next, the modeler identifies concepts from the dictionary to compose the meta-model. The following table illustrates the complete DL expression that results from applying semantic paramterization.

{| class="wikitable" border="1" cellpadding="3" align="center"
|+ The grounding {{mvar|G}} and meta-model {{mvar|M}} derived from UNLS<sub>1.0</sub>
! Grounding ({{mvar|G}})
! Meta-model ({{mvar|M}})
|-valign="top"
| {{math|Customer(''p''<sub>1</sub>) <br />
⨅ Share(''p''<sub>2</sub>) <br />
⨅ isAccessCodeOf(''p''<sub>3</sub>, ''p''<sub>1</sub>) <br />
⨅ Someone(''p''<sub>4</sub>) <br />
⨅ Provider(''p''<sub>4</sub>)}}
| {{math|Activity(''p''<sub>5</sub>) <br />
⨅ hasSubject(''p''<sub>5</sub>, ''p''<sub>1</sub>) <br />
⨅ hasAction(''p''<sub>5</sub>, ''p''<sub>2</sub>) <br />
⨅ hasObject(''p''<sub>5</sub>, ''p''<sub>3</sub>) <br />
⨅ hasTarget(''p''<sub>5</sub>, ''p''<sub>4</sub>) <br />
⨅ isRefrainmentOf(''p''<sub>5</sub>, ''p''<sub>1</sub>)}}
|}

== References ==
{{Reflist}}

{{DEFAULTSORT:Semantic Parameterization}}
[[Category:Knowledge representation]]
<=====doc_Id=====>:816
<=====title=====>:
AgMES
<=====text=====>:
{{Refimprove|date=January 2015}}
The '''AgMES''' (Agricultural Metadata Element set) initiative was developed by the [[Food and Agriculture Organization]] (FAO) of the [[United Nations]] and aims to encompass issues of semantic standards in the domain of agriculture with respect to description, resource discovery, interoperability and data exchange for different types of information resources.

There are numerous other metadata schemas for different types of information resources. The following list contains a list of a few examples:

* Document-like Information Objects (DLIOs): [[Dublin Core]], Agricultural Metadata Element Set (AgMES)
* Events: [[VCalendar]]
* Geographic and Regional Information: Geographic information—Metadata ISO/IEC 11179 Standards<ref>{{cite web|url=http://isotc.iso.org/livelink/livelink/fetch/2000/2489/Ittf_Home/PubliclyAvailableStandards.htm |title=Freely Available Standards |publisher=Isotc.iso.org |date= |accessdate=2013-07-10}}</ref>
* Persons: [[FOAF (software)|Friend-of-a-friend]] (FOAF), [[vCard]]
* Plant Production and Protection: Darwin Core (1.0 and 2.0) (DwC)

AgMES as a namespace is designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]], AGLS<ref>http://www.naa.gov.au/recordkeeping/gov_online/agls/summary.html</ref> etc. Thus to be used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc., it will have to be used in conjunction with the standard namespaces mentioned before.  The AgMES initiative strives to achieve improved interoperability between information resources in agricultural domain by enabling means for exchange of information.

Describing a DLIO with AgMES means exposing its major characteristics and contents in a standard way that can be reused easily in any information system. The more institutions and organizations in the agricultural domain that use AgMES to describe their DLIOs, the easier it will be to interchange data in between information systems like digital libraries and other repositories of agricultural information.

== Use of AgMES ==
Metadata on agricultural Document-like Information Objects (DLIOs) can be created and stored in various formats:
* embedded in a web site (in the manor as with the HTML meta tag)
* in a separate metadata database
* in an XML file
* in an RDF file

AgMES defines elements that can be used to describe a DLIO that can be used together with other metadata standards such as the Dublin Core, the Australian Government Locator Service. A complete list of all elements, refinements and schemes endorsed by AgMES is available from the AgMES website.<ref>{{cite web|url=http://aims.fao.org/standards/agmes/namespace-specification |title=AgMES 1.1 Namespace Specification &#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-10}}</ref>

=== Creating application profiles ===
[[Application profile]]s are defined as schemas which consist of data elements drawn from one or more namespaces, combined by implementers, and optimized for a particular local application. Application profiles share the following four characteristics:
* They draw upon existing pool of metadata definition standards to extract suitable application- or requirement oriented elements.
* An application profile cannot create new elements.
* Application profiles specify the application specific details such as the schemes or controlled vocabularies. An application profile also contains information such as the format for the element value, cardinality or [[data type]].
* Lastly, an application profile can refine standardized definitions as long as it is "semantically narrower or more specific". This capability of application profiles caters to situations where a domain specific terminology is needed to replace a more general one.

=== Sample application profiles using AgMES ===
* The AGRIS Application Profile<ref>{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref> is a standard created specifically to enhance the description, exchange and subsequent retrieval of agricultural Document-like Information Objects (DLIOs). It is a format that allows sharing of information across dispersed bibliographic systems and is based on well-known and accepted metadata standards.
* The Event Application Profile<ref>{{cite web|url=http://www.fao.org/aims/ap_applied.jsp |title=Agricultural Information Management Standards (AIMS) &#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref> is a standard created to allow members of the Agricultural community to 'know' about an upcoming event and guide them to the event Web site where they can find further information. The information communicated is thus minimum yet interoperable across domains and organizations.

== AgMES and the semantic web ==

One of the advantages of the AgMES metadata schema is the ability to link between the [[Data element|metadata element]] and [[Controlled vocabulary|controlled vocabularies]]. The use of controlled vocabulary provides a "known" set of options to the indexer (and the search programmer) as to how the field can be filled out. Often the values may come from a specific thesaurus (e.g. [[AGROVOC]]) or classification schemes (e.g. the AGRIS/CARIS classification scheme) etc.<ref>{{cite web|url=http://www.fao.org/aims/index_en.jsp?callingPage=ag_classifschemes.jsp |title=Agricultural Information Management Standards (AIMS) &#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref>

Thanks to the possibility to use controlled vocabularies for metadata elements, the user is provided with the most precise information. In this context, work is also being carried out on exploiting the power of controlled vocabularies expressed as using URIs and machine-understandable semantics.  In this context, FAO is promoting the [[Agricultural Ontology Service]] (AOS) initiative with the objective of expressing more semantics within the traditional thesaurus AGROVOC and build a Concept Server<ref>{{cite web|url=http://www.fao.org/aims/cs.htm |title=Agricultural Information Management Standards (AIMS) &#124; Interoperability, reusability and cooperation |publisher=Fao.org |date= |accessdate=2013-07-10}}</ref> as a repository from which it will be always possible to extract traditional KOS.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AGRIS]]
* [[AGROVOC]]

==References==
{{Reflist}}

== External links ==
* {{Official website|https://web.archive.org/web/20060519100855/http://www.fao.org:80/aims/agmes_intro.jsp }}
* [http://dublincore.org/ Dublin Core Metadata Initiative]
* [http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]
* [http://xml.coverpages.org/ni2003-05-12-a.html FAO's AgMES Project Releases a New Application Profile for Encoding Metadata.] (''Cover Pages'', May 2003)
* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&SERIES=339 Agricultural Information and Knowledge Management Papers]
* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&status=10&dateto=31/12/2006&lang=en&sites=1 RSS feed of news and events]
* [https://web.archive.org/web/20060617132639/http://www.dgroups.org:80/groups/fao/agstandards/index.cfm?op=dsp_join Agstandards Discussion List]: This is a forum established for discussing metadata standards and the development of multilingual thesauri and ontologies.

{{DEFAULTSORT:Agmes}}
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Semantic Web]]
[[Category:Food and Agriculture Organization]]
<=====doc_Id=====>:819
<=====title=====>:
Mental mapping
<=====text=====>:
{{about|the geographical concept|the diagram|Mind map}}
In [[behavioral geography]], a '''mental map''' is a person's [[Perspective (cognitive)|point-of-view]] perception of their area of interaction. Although this kind of subject matter would seem most likely to be studied by fields in the [[social sciences]], this particular subject is most often studied by modern day [[geographers]]. They study it to determine [[Subjectivity|subjective]] qualities from the public such as personal preference and practical uses of geography like driving directions. [[Mass media]] also have a virtually direct effect on a person's mental map of the geographical world.<ref>[http://mentalmaps.info Mental Maps Resource Site]</ref> The perceived geographical dimensions of a foreign nation (relative to one's own nation) may often be heavily influenced by the amount of time and relative news coverage that the news media may spend covering news events from that foreign region. For instance, a person might perceive a small island to be nearly the size of a continent, merely based on the amount of news coverage that he or she is exposed to on a regular basis.<ref>[http://geography.about.com/cs/culturalgeography/a/mentalmaps.htm Mental Maps on About.com]</ref>

In [[Experimental psychology|psychology]], the term names the information maintained in the mind of an organism by means of which it may plan activities, select routes over previously traveled territories, etc. The rapid traversal of a familiar [[maze]] depends on this kind of mental map if scents or other markers laid down by the subject are eliminated before the maze is re-run.

==Background==
Mental maps are an outcome of the field of behavioral geography. The imagined maps are considered one of the first studies that intersected geographical settings with human action.<ref name="Gregory">{{cite book|last=Gregory|first=Derek|title=Dictionary of Human Geography: Mental maps/Cognitive Maps|year=2009|publisher=Wiley-Blackwell|location=Hoboken|edition=5th |author2=Johnston, Rom |author3=Pratt, Geraldine |page=455}}</ref>  The most prominent contribution and study of mental maps was in the writings of [[Kevin A. Lynch|Kevin Lynch]]. In ''[[The Image of the City]]'', Lynch used simple sketches of maps created from memory of an urban area to reveal five elements of the city; nodes, edges, districts, paths and landmarks.<ref>{{cite book|last=Lynch|first=Kevin|title=The Image of the City|year=1960|publisher=MIT Press|location=Cambridge MA}}</ref>  Lynch claimed that “Most often our perception of the city is not sustained, but rather partial, fragmentary, mixed with other concerns. Nearly every sense is in operation, and the image is the composite of them all.” (Lynch, 1960, p 2.) The creation of a mental map relies on memory as opposed to being copied from a preexisting map or image. In ''The Image of the City'', Lynch asks a participant to create a map as follows: “Make it just as if you were making a rapid description of the city to a stranger, covering all the main features. We don’t expect an accurate drawing- just a rough sketch.” (Lynch 1960, p 141) In the field of human geography mental maps have led to an emphasizing of social factors and the use of social methods versus quantitative or positivist methods.<ref name="Gregory" /> Mental maps have often led to revelations regarding social conditions of a particular space or area. Haken and Portugali (2003) developed an information view, which 
argued that the face of the city is its information <ref>{{cite journal|last=Haken|first=Herman|author2=Portugali, Juval|title=The face of the city is its information|journal=Journal of Environmental Psychology|date=August 2003|volume=23|pages=385–408|doi=10.1016/s0272-4944(03)00003-3}}</ref>
. Bin Jiang (2012) argued that the image of the city (or mental map) arises out of the scaling of city artifacts and locations.<ref>{{cite journal|last=Jiang|first=Bin|title=The image of the city out of the underlying scaling of city artifacts or locations|journal=Annals of the Association of American Geographers|year=2012|volume=103|pages= 1552–1566| doi = 10.1080/00045608.2013.779503 |arxiv=1209.1112}}</ref> He addressed that why the image of city can be formed 
<ref>{{cite journal|last=Jiang|first=Bin|title=Why can the image of the city be formed| arxiv=1212.3703}}</ref>
, and he even suggested ways of computing the image of the city, or more precisely the kind of collective image of the city, using increasingly available geographic information such as Flickr and Twitter
<ref>{{cite journal|last=Jiang|first=Bin|title=Computing the image of the city| arxiv=1212.0940}}</ref>
.

==Research applications==
Mental maps have been used in a collection of spatial research. Many studies have been performed that focus on the quality of an environment in terms of feelings such as fear, desire and stress. A study by Matei et al. in 2001 used mental maps to reveal the role of media in shaping urban space in Los Angeles. The study used Geographic Information Systems (GIS) to process 215 mental maps taken from seven neighborhoods across the city. The results showed that people’s fear perceptions in Los Angeles are not associated with high crime rates but are instead associated with a concentration of certain ethnicities in a given area.<ref>{{cite journal|last=Matei|first=Sorin |author2=Ball-Rokeach, Sandra |author3=Qiu Linchuan, Jack|title=Fear and Misperception of Los Angeles Urban Space: A Spatial-Statistical Study of Communication-Shaped Mental Maps|journal=Communication Research|date=August 2001|volume=28|issue=4|pages=429–463|accessdate=4 November 2012|url=http://mentalmap.org/files/matei_fear_CR.pdf|doi=10.1177/009365001028004004}}</ref>  The mental maps recorded in the study draw attention to these areas of concentrated ethnicities as parts of the urban space to avoid or stay away from. 
	
Mental maps have also been used to describe the urban experience of children. In a 2008 study by Olga den Besten mental maps were used to map out the fears and dislikes of children in Berlin and Paris. The study looked into the absence of children in today’s cities and the urban environment from a child’s perspective of safety, stress and fear.<ref>{{cite journal|last=Den Besten|first=Olga den|title=Local belonging and ‘geographies of emotions’: Immigrant children’s experience of their neighbourhoods in Paris and Berlin|journal=Childhood|date=May 2010|volume=17|issue=2|pages=181–195|url=http://chd.sagepub.com/content/17/2/181|accessdate=4 November 2012|doi=10.1177/0907568210365649}}</ref>

Peter Gould and Rodney White have performed prominent analyses in the book “Mental Maps.” The book is an investigation into people’s spatial desires. The book asks of its participants: “Suppose you were suddenly given the chance to choose where you would like to live- an entirely free choice that you could make quite independently of the usual constraints of income or job availability. Where would you choose to go?” (Gould, 1974, p 15) Gould and White use their findings to create a surface of desire for various areas of the world. The surface of desire is meant to show people’s environmental preferences and regional biases.<ref>{{cite book|last=Gould|first=Peter|title=Mental Maps|year=1993|publisher=Rutledge|location=New York|author2=White, Rodney|page=93}}</ref>

In an experiment done by [[Edward C. Tolman]], the development of a mental map was seen in rats.<ref>Goldstein, B. (2011). ''Cognitive Psychology: Connecting Mind, Research, and Everyday Experience--with coglab manual. (3rd ed.).'' Belmont, CA: Wadsworth.</ref> A rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial mental map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.

The idea of mental maps is also used in strategic analysis. David Brewster, an Australian strategic analyst, has applied the concept to strategic conceptions of South Asia and Southeast Asia.  He argues that popular mental maps of where regions begin and end can have a significant impact on the strategic behaviour of states.<ref>{{cite web|author=David Brewster|url=https:// www.academia.edu/7697999/Dividing_Lines_Evolving_Mental_Maps_of_the_Bay_of_Bengal|title=Dividing Lines: Evolving Mental Maps of the Bay of Bengal. Retrieved 21 September 2014}}</ref>

==References==
{{Reflist}}

[[Category:Knowledge representation]]
[[Category:Cognitive psychology]]
[[Category:Psychology terminology]]
<=====doc_Id=====>:822
<=====title=====>:
Geopolitical ontology
<=====text=====>:
The '''FAO geopolitical ontology''' is an [[Ontology (information science)|Ontology]] developed by the [[FAO|Food and Agriculture Organization of the United Nations (FAO)]] to describe, manage and exchange data related to geopolitical entities such as countries, territories, regions and other similar areas.

==Definitions and examples==
An [[ontology (information science)|ontology]] is a kind of dictionary that describes information  in a certain domain using concepts and relationships. It is often implemented using [[Web Ontology Language|OWL]] (Web Ontology Language), an [[XML]]-based standard language  that can be interpreted by computers.

* A ''Concept'' is defined as abstract knowledge. For example, in the geopolitical ontology a [[United Nations list of Non-Self-Governing Territories|non-self-governing territory]] or a [[geographical region|geographical group]] are concepts. Concepts are explicitly implemented in the ontology with individuals and classes:
** An ''individual'' is defined as an object perceived from the real world. In the geopolitical domain   [[Ethiopia]] or the [[least developed countries]] group are individuals.
** A ''class'' is defined as a set of individuals sharing common properties. In the geopolitical domain, [[Ethiopia]], [[Republic of Korea]] or [[Italy]] are individuals of the class ''self-governing'' territory; and [[least developed countries]] is an individual of the class ''special group''.
* Relationships between concepts are explicitly implemented by:
** ''[[Object (computer science)|Object]] properties'' between individuals of two classes. For example, ''has member'' and ''is in group'' properties, as shown in Figure 1.
** ''[[Datatype]] properties'' between individuals and literals or [[XML]] datatypes. For example, the individual [[Afghanistan]] has the datatype property ''CodeISO3'' with the value "AFG".
** ''Restrictions'' in classes and/or properties. For example, the property ''official English name'' of the class ''self-governing'' territory has been restricted to have only ''one'' value, this means that a self-governing territory (or country) can only have one internationally recognized official English name.<ref>Official names of countries from [http://www.fao.org/faoterm/nocs/pages/homeNocs.jsp?members=allC&lang=en&lang2=en FAO terminology database]</ref>

[[Image:Concepts November 19 2008 v 2.png||thumb|600px|center|Figure 1. An example of concepts and relationship in the geopolitical ontology.]]

The advantage of describing information in an ontology is that it enables to acquire domain knowledge by defining hierarchical structures of classes, adding individuals, setting object properties and datatype properties, and assigning restrictions.

==FAO ontology==
The geopolitical ontology provides names in seven languages (Arabic, Chinese, French, English, Spanish, Russian and Italian) and identifiers in various international coding systems ([[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], [[List of UNDP country codes|UNDP]] and [[DBPedia]]ID codes) for territories and groups. Moreover, the [[FAO]] geopolitical ontology tracks historical changes from 1985 up until today;<ref>Country or area code changes since 1982:  [http://unstats.un.org/unsd/methods/m49/m49chang.htm United Nations Statistics Division - country or area codes added or changed]</ref> provides [[geolocation]] (geographical coordinates); implements relationships among [[countries]] and countries, or countries and groups, including properties such as ''has border with'', ''is predecessor of'', ''is successor of'', ''is administered by'', ''has members'', and ''is in group''; and disseminates country statistics including country area, land area, agricultural area, [[GDP]] or [[population]].

The FAO geopolitical ontology provides a structured description of data sources. This includes: source name, source identifier, source creator and source's update date. Concepts are described using the [[Dublin Core]] vocabulary (http://purl.org/dc/elements/1.1/description).

In summary, the main objectives of the FAO geopolitical ontology are:

* To provide the most updated geopolitical information (names, codes, relationships, statistics)
* To track historical changes in geopolitical information
* To improve information management and facilitate standardized data sharing of geopolitical information
* To demonstrate the benefits of the geopolitical ontology to improve [[interoperability]] of [[corporate]] [[information systems]]

It is possible to '''download''' the FAO geopolitical ontology in [http://aims.fao.org/geopolitical.owl OWL] and [http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ RDF] formats. Documentation is available in the [[FAO Country Profiles]] [http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information] web page.

==Features of the FAO ontology==
The geopolitical ontology contains :

*Area types:<ref>When an area (territory or group) changed but kept the same name, the ontology differentiates the two areas by sub-fixing the name of the obsolete one with the year (e.g. “FAO 2006”). The year indicates the beginning of validity of that particular area.</ref>
**Territories: [[List of sovereign states|self-governing]], [[United Nations list of Non-Self-Governing Territories|non-self-governing]], [[Disputed area|disputed]], other.<ref>The area type ''Other'' is used for [[Antarctica]] which has no government and belongs to no country. See also [[Antarctica#Politics|Politics in Antarctica]].</ref>
**Groups: [[organizations]], [[subregion|geographic]], [[economic union|economic]] and special groups.<ref>Special groups term is used for non-economical or greographical territory groups like the [[Small Island Developing States]], [[Landlocked countries|Land Locked Countries]], Low Income Food Deficit Countries, [[Least Developed Countries]], etc.</ref>
*Names <ref>UN official names: [http://unstats.un.org/unsd/geoinfo/uncsgnreports.htm Reports of the United Nations Conference on the Standardization of Geographical Names]</ref> (official, short and names for lists) in Arabic, Chinese, English, French, Spanish, Russian and Italian.
*International codes: UN code – M49, [[ISO 3166]] Alpha-2 and Alpha-3, [[List of UNDP country codes|UNDP code]], [[Global Administrative Unit Layers (GAUL)|GAUL]] code, FAOSTAT, [[AGROVOC]] FAOTERM and [[DBPedia]]ID.
*Coordinates: maximum [[latitude]], minimum [[latitude]], maximum [[longitude]], minimum [[longitude]].
*Basic country statistics: country area, land area, agricultural area, GDP, population.
*Currency names and codes.
*Adjectives of nationality.
*Relations:
**Groups membership.
**Neighbours (land [[border]]), administration of [[United Nations list of Non-Self-Governing Territories|non-self-governing]].
**Historic changes: predecessor, successor, valid since,<ref>The value of the datatype property "validSince" is the first year of validity  of a territory or group. The geopolitical ontology traces back historic changes only until 1985. Therefore if an area has a validSince = 1985, this indicates that the area is valid at least since 1985.</ref> valid until.<ref>The value of the datatype property "validUntil" is the last year of validity of the territory or group. In case the area is currently valid, this value is set by default to 9999.</ref>

==Implementation into  OWL==
The [[FAO]] geopolitical ontology is implemented in [[Web Ontology Language|OWL]]. It consists of classes, properties, individuals and restrictions. Table 1 shows all classes, gives a brief description and lists some individuals that belong to each class. Note that the current version of the geopolitical ontology does not provide individuals of the class "disputed" territories. Table 2 and Table 3 illustrate datatype properties and object properties.

<!-- Deleted image removed: [[Image:Class and instances in the geopolitical ontology v 1.png||thumb|667px|center|Table 1. Classes and instances in the geopolitical ontology.]] -->

<!-- Deleted image removed: [[Image:Datatype properties in the geopolitical ontology v 1.png||thumb|667px|center|Table 2. Datatype properties in the geopolitical ontology.]] -->

[[Image:Object properties in the geopolitical ontology v 1.png||thumb|674px|center|Table 3. Object properties in the geopolitical ontology.]]

== Geopolitical ontology in Linked Open Data ==
<!-- Deleted image removed: [[File:Geopol LOD.png|thumb|200px|left|Figure 2. RDF version of FAO geopolitical ontology]]  -->

The FAO Geopolitical ontology is embracing the [http://linkeddata.org W3C Linked Open Data (LOD) initiative] and released its RDF version of the geopolitical ontology in March 2011. 
The term 'Linked Open Data' refers to a set of best practices for publishing and connecting structured data on the Web. The key technologies that support Linked Data are URIs, HTTP and RDF.

The RDF version of the geopolitical ontology is compliant with all [http://www.w3.org/DesignIssues/LinkedData.html Linked data principles] to be included in the [http://richard.cyganiak.de/2007/10/lod/ Linked Open Data cloud], as explained in the following.

==Resolvable http:// URIs ==
Every resource in the OWL format of the FAO Geopolitical Ontology has a unique URI. Dereferenciation was implemented to allow for three different URIs to be assigned to each resource as follows:  
* URI identifying the non-information  resource
* Information resource with an RDF/XML representation
* Information resource with an HTML representation
In addition the current URIs used for OWL format needed to be kept to allow for backwards compatibility for other systems that are using them. Therefore, the new URIs for the FAO Geopolitical Ontology in LOD were carefully created, using  “Cool URIs for Semantic Web”  and considering other good practices for URIs, such as DBpedia URIs.

==New URIs==
The URIs of the geopolitical ontology need to be permanent, consequently all transient information, such as year, version, or format was avoided in the definition of the URIs. 
The new URIs can be accessed at
http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ 
For example, for the resource “Italy” the URIs are the following: 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy
: identifies the non-information resource. 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/data/Italy
: identifies the resource with an RDF/XML representation. 
;http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy
:identifies the information resource with an HTML representation.
In addition, “owl:sameAs” is used to map the new URIs to the OWL representation.

==Dereferencing URIs==
When a non-information resource is looked up without any specific representation format, then the server needs to redirect the request to information resource with an HTML representation. 
For example, to retrieve the resource “Italy” (http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/Italy), which is a non-information resource, the server redirects to the html page of “Italy” (http://www.fao.org/countryprofiles/geoinfo/geopolitical/page/Italy).

==At least 1000 triples in the datasets==
The total number of triple statements in FAO Geopolitical Ontology is 22,495. 
At least 50 links to a dataset already in the current LOD Cloud:  
FAO Geopolitical Ontology has 195 links to [http://www.dbpedia.org DBpedia], which is already part of the LOD Cloud.

==Access to the entire dataset==
FAO Geopolitical Ontology provides the entire dataset as a RDF dump. It is available at http://www.fao.org/countryprofiles/geoinfo/geopolitical/data

The RDF version of the FAO Geopolitical Ontology has been already registered in CKAN (http://ckan.net/package/fao-geopolitical-ontology) and it was requested to add it into the LOD Cloud.

==Example of use==
[[Image:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right|Figure 3. a website of introducing the geopolitical ontology in FAO Country Profiles.]]

The [[FAO Country Profiles]] is an information retrieval tool which groups the FAO's vast archive of information on its global activities in [[agriculture]] and [[rural development]] in one single area and catalogues it exclusively by country.

The [[FAO Country Profiles]] system provides access to country-based heterogeneous data sources.<ref>[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]</ref> By using the  geopolitical ontology in the system, the following benefits are expected:<ref>[http://semanticweb.com/integrating-country-based-heterogeneous-data-at-the-united-nations-fao-s-geopolitical-ontology-and-services_b10681 Integrating country-based heterogeneous data at the United Nations: FAO's geopolitical ontology and services.]</ref>

* Enhanced system functionality for content aggregation and synchronization from the multiple source repositories.
* Improved information access and browsing through comparison of data in neighbor countries and groups.

Figure 3 shows a page in the [[FAO Country Profiles]] where the geopolitical ontology is described.

==See also==
*[[Agricultural Information Management Standards]]
*[[AGROVOC]]
*[[Country code]]
*[[FAO Country Profiles]]
*[[Global Administrative Unit Layers (GAUL)|Global Administrative Unit Layers]] (GAUL)
*[[International Organization for Standardization]] (ISO)

==References==
{{reflist|2}}

==External links==
*[http://aims.fao.org/geopolitical.owl Geopolitical ontology in OWL format]
*[http://www.fao.org/countryprofiles/geoinfo/geopolitical/resource/ Geopolitical ontology in RDF format]
*[http://www.fao.org/countryprofiles/geoinfo.asp?lang=en Geopolitical information in the FAO Country Profiles]
*[http://www.slideshare.net/faoaims/faos-geopolitical-ontology-and-services FAO’s Geopolitical Ontology and Services] (Slides about FAO's geopolitical ontology)
*[http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]
*[http://www.fao.org/faoterm FAO Terminology] (FAOTERM)
*[http://faostat.fao.org FAOSTAT]
*[http://unstats.un.org/unsd/methods/m49/m49.htm UN Statistics Division - M49 codes]
*[http://www.iso.org/iso/english_country_names_and_code_elements ISO - Maintenance Agency for ISO 3166 country codes]

{{DEFAULTSORT:Geopolitical Ontology}}
[[Category:Ontology]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Country codes]]
<=====doc_Id=====>:825
<=====title=====>:
ISO 15926
<=====text=====>:
The '''ISO 15926''' is a standard for [[data integration]], sharing, exchange, and hand-over between [[computer system]]s.

The title, "''Industrial automation systems and integration&mdash;Integration of life-cycle data for process plants including oil and gas production facilities''",  is regarded too narrow by the present ISO 15926 developers. Having developed a [[generic data model]] and Reference Data Library for process plants, it turned out that this subject is already so wide, that actually any state information may be modelled with it.

== History ==
In 1991 a European Union [[European Strategic Program on Research in Information Technology|ESPRIT]]-, named ProcessBase, started. The focus of this research project was to develop a [[data model]] for lifecycle information of a facility that would suit the requirements of the process industries. At the time that the project duration had elapsed, a consortium of companies involved in the process industries had been established: [[EPISTLE]] (European Process Industries STEP Technical Liaison Executive). Initially individual companies were members, but later this changed into a situation where three national consortia were the only members: PISTEP (UK), POSC/Caesar (Norway), and USPI-NL (Netherlands). (later PISTEP merged into POSC/Caesar, and USPI-NL was renamed to USPI).

EPISTLE took over the work of the ProcessBase project. Initially this work involved a standard called ISO 10303-221 (referred to as "[[ISO_10303|STEP]] AP221"). In that AP221 we saw, for the first time, an Annex M with a list of standard instances of the AP221 data model, including types of objects. These standard instances would be for reference and would act as a knowledge base with knowledge about the types of objects.
In the early nineties EPISTLE started an activity to extend Annex M to become a library of such object classes and their relationships: STEPlib. In the STEPlib activities a group of approx. 100 domain experts from all three member consortia, spread over the various expertises (e.g. Electrical, Piping, Rotating equipment, etc.), worked together to define the "core classes".

The development of STEPlib was extended with many additional classes and relationships between classes and published as [[Open Source]] data. Furthermore, the concepts and relation types from the AP221 and ISO 15926-2 data models were also added to the STEPlib dictionary. This resulted in the development of [[Gellish English]], whereas STEPlib became the [[Gellish English dictionary]]. Gellish English is a structured subset of natural English and is a [[modeling language]] suitable for [[knowledge modeling]], [[product modeling]] and [[data exchange]]. It differs from conventional modeling languages ([[meta language]]s) as used in information technology as it not only defines generic concepts, but also includes an English dictionary. The semantic expression capability of Gellish English was significantly increased by extending the number of relation types that can be used to express knowledge and information.

For modelling-technical reasons POSC/Caesar proposed another standard than [[ISO 10303]], called ISO 15926. EPISTLE (and ISO) supported that proposal, and continued the modelling work, thereby writing Part 2 of ISO 15926. This Part 2 has official ISO IS (International Standard) status since 2003.

POSC/Caesar started to put together their own RDL (Reference Data Library). They added many specialized classes, for example for [[American National Standards Institute|ANSI]] (American National Standards Institute) pipe and pipe fittings. Meanwhile STEPlib continued its existence, mainly driven by some members of USPI. Since it was clear that it was not in the interest of the industry to have two libraries for, in essence, the same set of classes, the Management Board of EPISTLE decided that the core classes of the two libraries shall be merged into Part 4 of ISO 15926. This merging process has been finished. Part 4 should act as reference data for part 2 of ISO 15926 as well as for ISO 10303-221 and replaced its Annex M. On June 5, 2007 ISO 15926-4 was signed off as a TS (Technical Specification).

In 1999 the work on an earlier version of Part 7 started. Initially this was based on [[XML Schema (W3C)|XML Schema]] (the only useful W3C Recommendation available then), but when [[Web Ontology Language|Web Ontology Language (OWL)]] became available it was clear that provided a far more suitable environment for Part 7. Part 7 passed the first ISO ballot by the end of 2005, and an implementation project started. A formal ballot for TS (Technical Specification) was planned for December 2007. However, it was decided then to split Part 7 into more than one part, because the scope was too wide.

== The standard ==
{{External links|date=May 2012}}
ISO 15926 has eleven parts (as of June 2009):

* Part 1 [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=29556] - Introduction, information concerning engineering, construction and operation of production facilities is created, used and modified by many different organizations throughout a facility's lifetime. The purpose of ISO 15926 is to facilitate integration of data to support the lifecycle activities and processes of production facilities.
* Part 2 [http://www.stanford.edu/group/narratives/classes/08-09/CEE215/ReferenceLibrary/FIATECH%20ISO%2015926/ISO/pack/ISO%2015926%20part2%20pack/ECM4.5/lifecycle_integration_schema.html]- Data Model. a generic 4D model that can support all disciplines, supply chain company types and life cycle stages, regarding information about functional requirements, physical solutions, types of objects and individual objects as well as activities.
* Part 3 - Reference data for geometry and topology.
* Parts 4 [http://data.posccaesar.org/rdl/] - Reference Data, the terms used within facilities for the process industry.
* Part 7 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 7: Implementation methods for the integration of distributed systems: Template methodology.
* Part 8 [http://www.15926.org] - Integration of life-cycle data for process plants including oil and gas production facilities - Part 8: Implementation methods for the integration of distributed systems: [[Web Ontology Language]] (OWL/RDF) implementation.
* Part 9 (in development)- Implementation standards, with the focus on Façades, standard web servers, web services, and security.
* Part 10 (in development)- Test Methods.
* Part 11 (in development)- Industrial Usage Guidelines.
* Part 12 (in development)- Life cycle integration ontology in [[Web Ontology Language]] (OWL2).
* Part 13 (in development)- Integrated lifecycle asset planning.

=== Description ===
The model and the library are suitable for representing lifecycle information about technical installations and their components.

They can also be used for defining the terms used in product catalogs in [[e-commerce]]. Another, more limited, use of the standard is as a reference classification for harmonization purposes between shared databases and product catalogues that are not based on ISO 15926.

The purpose of ISO 15926 is to provide a [[Lingua franca|Lingua Franca]] for computer systems, thereby integrating the information produced by them. Although set up for the process industries with large projects involving many parties, and involving plant operations and maintenance lasting decades, the technology can be used by anyone willing to set up a proper vocabulary of reference data in line with Part 4.

In Part 7 the concept of Templates is introduced. These are semantic constructs, using Part 2 entities, that represent a small piece of information. These constructs then are mapped to more efficient classes of n-ary relations that interlink the Nodes that are involved in the represented information.

In Part 8 the data model of Part 2 is mapped to OWL, and so are, in concept, the Reference Data of Part 4 and the templates of Part 7. For validation and reasoning purposes all are represented in First-Order Logic as well.

In Part 9 these Node and Template instances are stored in Façades. A Façade is an RDF [[quad store]], set up to a standard schema and an API. Any Façade only stores the data for which the Façade owner is responsible.

Each participating computer system maps its data from its internal format to such ISO-standard Node and Template instances. These are stored in a System Façade, each system its own Façade.

Data can be "handed over" from one Façade to another in cases where data custodianship is handed over (e.g. from a contractor to a plant owner, or from a manufacturer to the owners of the manufactured goods). Hand-over can be for a part of all data, whilst maintaining full referential integrity.

Façades can be set up for the consolidation of data by handing over data produced by various participating computer systems and stored in their System Façades. Examples are: a Façade for a project discipline, a project, a plant).

Documents are user-definable. They are defined in [[XML Schema]] and they are, in essence, only a structure containing cells that make reference to instances of Templates. This represents a view on all lifecycle data: since the data model is a 4D (space-time) model, it is possible to present the data that was valid at any given point in time, thus providing a true historical record. It is expected that this will be used for Knowledge Mining.

Data can be queried by means of [[SPARQL]]. In any implementation a restricted number of Façades can be involved, with different access rights. This is done by means of creating a CPF Server (= Confederation of Participating Façades). An [[Ontology (computer science)|Ontology]] Browser allows for access to one or more Façades in a given CPF, depending on the access rights.

== Projects and applications ==
{{External links|date=May 2012}}
There are a number of projects working on the extension of the ISO 15926 standard in different application areas.

=== Capital-intensive projects ===

Within the application of Capital Intensive projects, some cooperating implementation projects are running:

* The EDRC Project of [http://www.fiatech.org FIATECH] [http://www.fiatech.org/images/stories/projects/Project_Resumes/EDRC_Resume_v8_Sept_13_2013.pdf Capturing Equipment Data Requirements Using ISO 15926 and Assessing Conformance]. [http://techinvestlab.ru/EDRCDemo Example data and videos.]
* The ADI Project of [http://www.fiatech.org FIATECH], to build the tools (which will then be made available in the public domain)
** The tools and deliverables can be seen on the ISO 15926 knowledge base: [http://15926.org]
* The IDS Project of [http://www.posccaesar.org POSC Caesar Association], to define product models required for data sheets
* A joint ADI-IDS project is the [[ISO 15926 WIP]]
* The DEXPI project: The objective of DEXPI is to develop and promote a general standard for the process industry covering all phases of the lifecycle of a (petro-)chemical plant, ranging from specification of functional requirements to assets in operation. See more at [http://www.dexpi.org dexpi.org]

=== Upstream Oil and Gas industry ===

The [[Norwegian Oil Industry Association]] (OLF) has decided to use ISO 15926 (also known as the [[Oil and Gas Ontology]]) as the instrument for integrating data across disciplines and business domains for the [[Upstream (oil industry)|Upstream Oil and Gas industry]]. It is seen as one of the enablers of what has been called the next (or second) generation of [[Integrated operations]], where a better integration across companies is the goal.<ref>{{cite web |url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf |title=Integrated Operations and the Oil and Gas Ontology |author=The Norwegian Oil Industry Association (OLF) |accessdate=2009-05-06 }}</ref>

The following projects are currently running (May 2009):

* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].
* The [http://trac.posccaesar.org/wiki/EW Environment Web] project to include environmental reporting terms and definitions  as used in [http://www.epim.no EPIM]'s [http://www.epim.no/default.asp?id=945 EnvironmentWeb] in ISO 15926.

Finalised projects include:

* The [http://trac.posccaesar.org/wiki/IIP Integrated Information Platform (IIP)] project working on establishing a real-time information pipeline based on open standards. It worked among others on:
** [http://www.posccaesar.org/wiki/NcsDdr Daily Drilling Report (DDR)] to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008<ref>{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-05-05 }}</ref> for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and [http://www.ptil.no/main-page/category9.html Safety Authority Norway (PSA)]. NPD says that the quality of the reports has improved considerably since.
** [http://www.posccaesar.org/wiki/NcsDpr Daily Production Report (DPR)] to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and [http://www.statoilhydro.com/en/ouroperations/explorationprod/ncs/aasgard/pages/default.aspx Åsgard] ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]’ [[PRODML]] standard.

== Some technical background ==
One of the main requirements was (and still is) that the scope of the data model covers the entire lifecycle of a facility (e.g. oil refinery) and its components (e.g. pipes, pumps and their parts, etc.). Since such a facility over such a long time entails many different types of activities on a myriad of different objects it became clear that a generic and data-driven data model would be required.

A simple example will illustrate this. There are thousands of different types of physical objects in a facility (pumps, compressors, pipes, instruments, fluids, etc). Each of these has many properties. If all combinations would be modelled in a "hard-coded" fashion, the number of combinations would be staggering, and unmanageable.

The solution is a "template" that represents the semantics of: "This object has a property of  X yyyy" (where yyyy is the unit of measure). Any instance of that template refers to the applicable reference data:
* physical object (e.g. my Induction Motor)
* indirect property type (e.g. the class "cold locked rotor time")
* base property type (here: time)
* scale (here: seconds)

Without being able to make reference to those classes, via the Internet, it will be impossible to express this information.

==References==
{{Reflist}}

== External links ==
* [http://15926.org 15926.org]: A forum for ISO 15926 discussions and team collaboration.
* [http://iringug.org/wiki/index.php?title=Main_Page iringug.org]: -An online community of users, companies, and organizations that have common interest in solutions that implement ISO 15926 reference data and protocols.
* [http://iringtoday.com iringtoday.com]: - An online ISO 15926 thought leadership community geared toward engineering management.
* [http://techinvestlab.ru/ISO15926en .15926 Editor] Open source software to view, edit and verify ISO 15926 data.
* [http://wings.buffalo.edu/philosophy/ontology/bfo/west.pdf Against Idiosyncrasy in Ontology Development]: A critical study of ISO 15926 and of the claims made on its behalf.
* [http://www.matthew-west.org.uk/publications/ResponseToBarrySmithCommentsOnISO15926.pdf A Response to "Against Idiosyncrasy in Ontology Development"]: A rebuttal of "Against Idiosyncracy in Ontology Development".

{{ISO standards}}

{{DEFAULTSORT:Iso 15926}}
[[Category:ISO standards|#15926]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:828
<=====title=====>:
NeOn Toolkit
<=====text=====>:
'''The NeOn Toolkit''' is an open source, multi-platform [[ontology editor]], which supports the development of ontologies in [[Web Ontology Language|OWL]]/[[Resource Description Framework|RDF]]. The editor is based on the [[Eclipse (software)|Eclipse platform]] and provides a set of plug-ins (currently 20 plug-ins are available for the latest version, v2.4) covering a number of ontology engineering activities, including Annotation and Documentation, Modularization and Customization, Reuse, Ontology Evolution, translation<ref name="Espinoza2008">M. Espinoza, A. Gomez-Perez, and E. Mena. [http://sid.cps.unizar.es/PUBLICATIONS/POSTSCRIPTS/eswc08-localization.pdf Enriching an ontology with multilingual information]. In Proc. of 5th European Semantic Web Conference (ESWC'08), Tenerife, (Spain), June 2008.</ref> and others. 

The NeOn Toolkit has been developed in the course of the EU-funded NeOn project and is currently maintained and distributed by the NeOn Technologies Foundation.

==References==
<references/>

== External links ==
* [http://www.neon-foundation.org/ NeOn Technologies Foundation]
* [http://neon-toolkit.org/ NeOn Toolkit Website]
* [http://www.neon-project.org/ NeOn Project Website]

[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Free software programmed in Java (programming language)]]
<=====doc_Id=====>:831
<=====title=====>:
Category:Thesauri
<=====text=====>:
{{cat main|Thesaurus}}
{{Commons category|Thesauri}}
[[Category:Knowledge representation]]
[[Category:Reference works]]
[[Category:Dictionaries by type]]
[[Category:Information science]]
<=====doc_Id=====>:834
<=====title=====>:
OntoWiki
<=====text=====>:
{{Infobox Software | name = OntoWiki
| logo = 
| screenshot =<!-- Deleted image removed:  [[Image:OntoWiki-screenshot.jpg|180px]] -->
| caption = 
| founder = Sören Auer
| current maintainer = Sebastian Tramp
| latest_release_version = 0.9.11
| latest_release_date = January 31, 2014
| operating_system = [[Cross-platform]]
| programming language=[[PHP]]
| database=[[MySQL]]
| genre = [[knowledge management system]]
| license = [[GNU General Public License|GPL]]
| website = http://ontowiki.net
}}

'''OntoWiki''' is a free, [[open source software|open-source]] [[semantic wiki]] application, meant to serve as an [[ontology (computer science)|ontology]] editor and a [[knowledge management|knowledge acquisition]] system. It is a web-based application written in [[PHP]] and using either a [[MySQL]] database or a [[Virtuoso Universal Server|Virtuoso triple store]]. OntoWiki is form-based rather than syntax-based, and thus tries to hide as much of the complexity of knowledge representation formalisms from users as possible. OntoWiki is mainly being developed by the [http://aksw.org Agile Knowledge Engineering and Semantic Web (AKSW) research group] at the [[University of Leipzig]], a group also known for the [[DBpedia]] project among others, in collaboration with volunteers around the world.

In 2009 the AKSW research group got a budget of €425,000 from the [[Federal Ministry of Education and Research (Germany)|Federal Ministry of Education and Research of Germany]] for the development of the OntoWiki.<ref>[http://idw-online.de/pages/de/news300375 "OntoWiki" hilft Daten im Web zu verknüpfen] (German)</ref>

In 2010 OntoWiki became part of the technology stack supporting the [[Framework Programmes for Research and Technological Development#LOD2|LOD2]] (Linked Open Data) project. Leipzig University is one of the consortium members of the project, which is funded by a €6.5m EU grant.<ref>{{cite web |url=http://cordis.europa.eu/fetch?CALLER=PROJ_ICT&ACTION=D&CAT=PROJ&RCN=95562 |publisher=European Commission |title=CORDIS FP7 ICT Projects - LOD2 |date=2010-04-20}}</ref>

==See also==
* [[Semantic MediaWiki]]
* [[DBpedia]]

== External links ==
* {{official website|http://OntoWiki.net}}
* [https://github.com/AKSW/OntoWiki#ontowiki About page on GitHub]
* [http://blog.aksw.org AKSW blog]

== References ==
<references/>

{{DEFAULTSORT:Ontowiki}}
[[Category:Semantic wiki software]]
[[Category:Free integrated development environments]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
<=====doc_Id=====>:837
<=====title=====>:
Agricultural Information Management Standards
<=====text=====>:
{{Infobox website
|name=Agricultural Information Management Standards (AIMS)
|logo =[[File:Agricultural Information Management Standards (AIMS) logo.jpg|100px|AIMS logo]]
|slogan = ''Standards, Tools, Services & Advice''
|url ={{URL|http://aims.fao.org}}
|type = [[Community of Practice]]
|commercial      = No
|registration    = Optional
|language        = English
|launch date = 2006
|current status = Online
|screenshot     = 
}}

[http://aims.fao.org/ '''Agricultural Information Management Standards'''], abbreviated to '''AIMS''' is a space for accessing and discussing agricultural information management standards, tools and methodologies connecting information workers worldwide to build a global community of practice. Information management standards, tools and good practices can be found on AIMS:

* to support the implementation of structured and linked information and knowledge to enable institutions and individuals from different technical backgrounds to build open and interoperable information systems;
* to provide advice on how to best manage, disseminate, share and exchange agricultural scientific information;
* to promote good practices widely applicable and easy to implement, and;
* to foster communities of practices centered on interoperability, reusability and cooperation.

== Users ==

AIMS is primarily intended for information workers—librarians, information managers, software developers—but is also of interest to those who are simply passionate about knowledge and information sharing. The success of AIMS depends upon its communities reaching a critical mass to show that the investment in interoperability standards has a return.

== Community ==

AIMS holds [http://aims.fao.org/communities-aims 9 communities of practice]. They are intended to discuss and share information about the different ongoing initiatives under the AIMS umbrella. AIMS supports collaboration through forums and blogs amongst institutions and individuals that wish to share expertise on how to use tools, standards and methodologies. Moreover, news and events are published on AIMS as part of its ‘one-stop” access to interoperability and reusability of information resources. The AIMS communities are aimed at the global agricultural community, including information providers, from research institutes, academic institutions, educational and extension institutions and also the private sector.

== Content ==

=== Vocabularies ===

* [[AGROVOC]] is a comprehensive multilingual vocabulary that contains close to 40,000 concepts in over 20 languages covering subject fields in agriculture, forestry and fisheries together with cross-cutting themes such as land use, rural livelihoods and food security.<ref>{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai144e/ai144e00.pdf |title=Basic Guidelines for Managing AGROVOC|year=2008 |accessdate=2011-08-01}}</ref> It standardizes data description to enable a set of core integration goals: interoperability, reusability and cooperation.<ref>{{cite web|url=http://www.fao.org/docrep/008/af238e/af238e04.htm |title=Agricultural Information Systems and Common Exchange Standards|year=2005 |accessdate=2011-08-01}}</ref> In this spirit of collaboration, [[AGROVOC]] also works with other organizations that are using [[Linked Open Data]] techniques to connect vocabularies and build the backbone of the next generation of internet data; data that is marked up not just for style but for meaning. It is maintained by a global community of librarians, terminologists, information managers and software developers<ref>{{cite web|url=http://aims.fao.org/standards/agrovoc/community |title=AGROVOC Community |accessdate=2011-08-01}}</ref> using [http://aims.fao.org/tools/vocbench-2 VocBench], a multilingual, web-based vocabulary editor and workflow management tool that allows for simultaneous, distributed editing.<ref>{{cite web|url=http://aims.fao.org/tools/vocbench-2 |title=VocBench Homepage |accessdate=2011-08-01}}</ref>
* In addition to AGROVOC, AIMS provides access to other vocabularies like the [[Geopolitical ontology]] and [http://aims.fao.org/standards/agvocabularies/fisheries-ontology Fisheries Ontologies]. The [[Geopolitical ontology]] is used to facilitate data exchange and sharing in a standardized manner among systems managing information about countries and/or regions. The network of fisheries ontologies was created as a part of the [http://www.neon-project.org/nw/Welcome_to_the_NeOn_Project NeOn Project] and it covers the following areas: Water areas: for statistical reporting, jurisdictional ([[EEZ]]), environmental (LME), Species: taxonomic classification, ISSCAAP commercial classification, Aquatic resources, Land areas, Fisheries commodities, Vessel types and size, Gear types, [[AGROVOC]], ASFA.<ref>{{cite web|url=http://www.fao.org/docrep/field/009/ai254e/ai254e00.htm |title=Revised and enhanced fisheries ontologies |accessdate=2011-08-01}}</ref>
* [[AgMES]] is as a namespace designed to include agriculture specific extensions for terms and refinements from established standard metadata namespaces like [[Dublin Core]] or [[Australian Government Locator Service|AGLS]], used for Document-like Information Objects, for example like publications, articles, books, web sites, papers, etc.<ref>{{cite web|url=ftp://193.43.36.44/gi/gil/gilws/aims/publications/workshops/coherence0/ppt/agmes.pdf |title=Agricultural Metadata Element Set: Standardization and Information Dissemination|accessdate=2011-08-01}}</ref>
* Linked Open Data (LOD) - Enabled Bibliographic Data [http://aims.fao.org/lode/bd (LODE-BD) Recommendations 2.0] are a reference tool that assists bibliographic data providers in selecting appropriate encoding strategies according to their needs in order to facilitate metadata exchange by, for example, constructing crosswalks between their local data formats and widely used formats or even with a [[Linked Data]] representation

=== Tools ===
* [http://aims.fao.org/tools/agridrupal AgriDrupal] is both a suite of solutions for agricultural information management and a community of practice around these solutions.  The AgriDrupal community is made up of people who work in the community of agricultural information management specialists and have been experimenting with IM solutions in [[Drupal]].<ref>{{cite web|url=http://www.fao.org/docrep/article/am642e.pdf |title=AgriDrupal: repository management integrated into a content management system |accessdate=2011-08-01}}</ref>
* [http://aims.fao.org/agriocean-dspace AgriOcean DSpace] is a joint initiative of the [[United Nations]] agencies of [[FAO]] and [[UNESCO]]-IOC/IODE to provide a customized version of [[DSpace]]. It uses standards for [[metadata]], [[thesauri]] and other [[controlled vocabularies]] for [[oceanography]], [[marine science]], food, agriculture, development, [[fisheries]], [[forestry]], [[natural resources]] and other related sciences.<ref>{{cite web|url=http://eprints.rclis.org/handle/10760/15812 |title=AgriOcean DSpace : FAO and UNESCO/IOC-IODE Combine Efforts in their Support of Open Access |accessdate=2011-08-01}}</ref>
* [http://aims.fao.org/tools/vocbench-2 VocBench] is a web-based multilingual vocabulary management tool developed by [[FAO]] and hosted by [[MIMOS Berhad]]. It transforms thesauri, authority lists and glossaries into [[SKOS]]/[[Resource Description Framework|RDF]] concept schemes for use in a linked data environment. VocBench also manages the workflow and editorial processes implied by vocabulary evolution such as user rights/roles, validation and versioning. VocBench  supports a growing set of user communities, including the global, distributed group of terminologists who manage [[AGROVOC]].<ref>{{cite web|url=http://semtech2011.semanticweb.com/uploads/handouts/MON_600_Jaques_3910.pdf |title=VocBench: vocabulary editing and workflow management |accessdate=2011-08-01}}</ref>
* [http://aims.fao.org/tools/webagris-2 WebAGRIS] is a multilingual Web-based system for distributed data input, processing and dissemination (through the Internet or on CD-Rom), of agricultural bibliographic information. It is based on common standards of data input and dissemination formats ([[XML]], [[Html|HTML]], ISO2709), as well as subject categorization schema and [[AGROVOC]].<ref>{{cite web|url=ftp://ftp.fao.org/docrep/fao/010/ai161e/ai161e00.pdf |title=FAO’s experience in metadata exchange from CDS/ISIS bibliographic databases using XML format, compliant to Dublin Core standard |accessdate=2011-08-01}}</ref>

=== Services ===
* [http://www.agrifeeds.org/ AgriFeeds] is a service that allows users to search and filter news and events from several agricultural information sources and to create custom feeds based on the filters applied.<ref>{{cite web|url=ftp://ftp.fao.org/docrep/fao/011/ak182e/ak182e00.pdf |title=AgriFeeds: The Agricultural News and Events Aggregator |accessdate=2011-08-01}}</ref> AgriFeeds was designed in the context on [http://www.ciard.net/ CIARD] (Coherence in Information for Agricultural Research for Development). Within CIARD, the partners who designed and implemented AgriFeeds are [[FAO]] and [[Global Forum on Agricultural Research|GFAR]]. AgriFeeds is currently maintained by [[FAO]].
* [[AGRIS]] is a global public domain database with nearly 3 million structured bibliographical records on agricultural science and technology. The database is maintained by [[FAO]], with the content provided by more than 100 participating institutions from 65 countries.<ref>{{cite web|url=http://agris.fao.org/knowledge-and-information-sharing-through-agris-network |title=Knowledge and information sharing through the AGRIS Network |accessdate=2011-08-01}}</ref>
* [http://ring.ciard.net/ CIARD Routemap to Information Nodes and Gateways (RING)] is a project implemented within CIARD and is led by [[Global Forum on Agricultural Research|GFAR]]. The RING is a global registry of web-based services that give access to any kind of information pertaining to agricultural research for development (ARD). It allows information providers to register their services in various categories and so facilitate the discovery of sources of agriculture-related information across the world.<ref>{{cite web|url=http://www.fao.org/docrep/012/al207e/al207e00.pdf |title=The CIARD RING, an infrastructure for interoperability of agricultural research information services |year=2010 |accessdate=2011-08-01}}</ref>
* Since January 2011, AIMS supports [[E-lis|E-LIS]], the international electronic [[Open Archives Initiative|archive]] for [[Library science|library and information science]] (LIS). E-LIS is established, managed and maintained by an international team of 73 librarians and information scientists from 47 countries and support for 22 languages. It is freely accessible, aligned with the [[Open access (publishing)|Open Access]] (OA) movement and is a voluntary enterprise. Currently it is the largest international repository in the LIS field. Searching or browsing E-LIS is a kind of multilingual, multicultural experience, an example of what could be accomplished through open access archives to bring the people of the world together.<ref>{{cite web|url=http://eprints.rclis.org/handle/10760/6634 |title=E-LIS: an international open archive towards building open digital libraries |year=2005 |accessdate=2011-08-02}}</ref>
* [http://aims.fao.org/vest-registry VEST Registry] is a catalog of controlled vocabularies (such as authority files, classification systems, [[concept maps]], controlled lists, dictionaries, [[ontologies]] or subject headings); [[metadata]] sets ([[metadata]] element sets, namespaces and application profiles); and tools (such as library management software, content management systems or document repository software). It is concerned primarily with collecting and maintaining a consistent set of [[metadata]] for each resource. The scope of the VEST Registry is to provide a clearing house for tools, [[metadata]] sets and vocabularies used in food, [[agriculture]], development, [[fisheries]], [[forestry]] and [[natural resources]] [[information management]] context.

==See also==
* [[AGRIS]]
* [[AGROVOC]]
* [[E-LIS]]
* [[IMARK]]
* [[Geopolitical ontology]]

== References ==

{{Reflist|2}}

== External links ==
* [http://aims.fao.org/home Agricultural Information Management Standards]

[[Category:Agriculture]]
[[Category:Food and Agriculture Organization]]
[[Category:Standards]]
[[Category:Information science]]
[[Category:Knowledge]]
[[Category:Knowledge representation]]
[[Category:Library science]]
<=====doc_Id=====>:840
<=====title=====>:
Personal knowledge base
<=====text=====>:
{{about|knowledge management software|the general concept|Personal knowledge management}}
{{Copypaste|url=http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf|date=June 2016}}
A '''personal knowledge base''' ('''PKB''') is an electronic tool used to express, capture, and later retrieve the personal knowledge of an individual. It differs from a traditional [[database]] in that it contains subjective material particular to the owner, that others may not agree with nor care about. Importantly, a PKB consists primarily of knowledge, rather than [[information]]; in other words, it is not a collection of documents or other sources an individual has encountered, but rather an expression of the distilled knowledge the owner has extracted from those sources.<ref name = "Davies 2005"/><ref name = "Davies 2011"/>

== Definition ==

The term ''personal knowledge base'' was mentioned as early as the 1980s,<ref name = "Brooks 1985"/><ref name = "Kruger 1986"/><ref name = "Forman 1988"/><ref name = "Smith 1991"/> but the term came to prominence when it was described at length in publications by computer scientist Stephen Davies and colleagues,<ref name = "Davies 2005"/><ref name = "Davies 2011"/> who defined the term as follows:{{efn|An earlier version of this article incorrectly stated that the term ''personal knowledge base'' was coined in 2011; in fact, Stephen Davies and colleagues wrote a paper on the subject in 2005,<ref name = "Davies 2005"/> and publications by other authors had mentioned the term as early as the 1980s.<ref name = "Brooks 1985"/><ref name = "Kruger 1986"/><ref name = "Forman 1988"/><ref name = "Smith 1991"/> Much of the present article closely follows the publications by Davies and colleagues.}}

* '''personal''': a PKB is intended for private use, and its contents are custom-tailored to the individual. It contains trends, relationships, categories, and personal observations that its owner perceives but which no one else may agree with. It can be shared, just as one can explain one's own opinion to a hearer, but it is not jointly ''owned'' by anyone else any more than explaining one's opinion to a friend causes the friend to own one's mind.
* '''knowledge''': a PKB contains knowledge, not merely information. Its purpose is not simply to aggregate all the information sources one has seen, but to preserve the knowledge that one has ''learned'' from those sources. When a user returns to a PKB to retrieve knowledge she has stored, she is not merely pointed back to the original documents, where she must relocate, reread, reparse, and relearn the relevant passages. Instead, she is returned to the distilled version of the particular truth she is seeking, so that the mental model she originally had in mind can be easily reformed.
* '''base''': a PKB is a consolidated, integrated knowledge store. It is a reflection of its owner's memory, which, as Bush and many others have observed, can freely associate any two thoughts together, without restriction. Hence a PKB does not attempt to partition a user's field of knowledge into multiple segments that cannot reference one another. Rather, it can connect any two concepts without regard for artificial boundaries, and acts as a single, unified whole.

=== Contrast with other classes of systems ===

The following classes of systems ''cannot'' be classified as PKBs:<ref name = "Davies 2005"/><ref name = "Davies 2011"/>

* collaborative efforts to build a universal objective space (as opposed to an individual's personal knowledge.) The World Wide Web itself is in this category, as were its predecessors HyperTIES<ref name = "Schneiderman 1987"/> and Xanadu,<ref name = "Nelson 1987"/> Web categorization systems like the [[Open Directory Project]], and collaborative information collections like [[Wikipedia]].
* search systems like Enfish and the Stuff I've Seen project<ref name="Dumais et al 2003" /> that index and search one's information sources on demand, but do not give the user the ability to craft and express personal knowledge.
* tools whose goal is to produce a design artifact rather than to maintain knowledge for its own sake. Systems like ART<ref name="Nakakoji et al 2000" /> and Writing Environment<ref name="Smith et al 1987" /> use intermediate knowledge representations as a means to an end, abandoning them once a final artifact has been produced, and hence are not suitable as PKBs.
* systems that focus on capturing transient information, rather than archiving knowledge that has long-term value. Examples would be Web logs<ref name="Godwin-Jones 2003"/> and e-diaries.<ref name="Kovalainen et al 1998" />  Tools whose information domain is mostly limited to [[time management]] tasks (calendars, action items, contacts, etc.) rather than "general knowledge". Blandford and [[Thomas R.G. Green|Green]]<ref name="Blandford and Green 2001" /> and Palen<ref name="Palen 1999" /> give excellent surveys; common commercial examples would be [[Microsoft Outlook]], [[IBM Lotus Notes|Lotus Notes]], and [[Evolution (software)|Novell Evolution]].
* similarly, tools developed for a specific domain, such as bibliographic research rather than for "general knowledge".

==== Personal information management ====

PKM is similar to [[personal information management]], but is a distinct topic based on the "information" vs. "knowledge" difference. PKBs are about recording and managing the knowledge one derives from documents, whereas PIM is more about managing and retrieving the documents themselves.<ref name = "Davies 2005"/><ref name = "Davies 2011"/>

== Historical influences ==

Non-electronic personal knowledge bases have probably existed in some form since the dawn of written language: [[Leonardo da vinci#Journals and notes|Da Vinci's notebooks]] are a famous example. More commonly, card files and personal annotated libraries have served this function in the pre-electronic age.

=== Bush's Memex ===

Undoubtedly the most famous early formulation of an electronic PKB was [[Vannevar Bush]]'s description of the "[[Memex]]" in 1945.<ref name="Bush 1945" /> Bush surveyed the post-World-War-II landscape and laid out what he viewed as the most important forthcoming challenges to humankind in ''[[The Atlantic Monthly]]''. The Memex was a theoretical (never implemented) design or a system to help tackle the [[information overload]] problem, already formidable in 1945. In Bush's own words:

<blockquote>Consider a future device for individual use, which is a sort of mechanized private file and library. ... [A] device in which an individual stores all his books, records, and communications, and which is mechanized so that it may be consulted with exceeding speed and flexibility. It is an enlarged intimate supplement to his memory.  </blockquote>

Bush envisioned collaborative aspects as well, and even a world-wide system that scientists could freely consult.{{Citation needed|date=November 2012}} But an important emphasis throughout the article was on expanding our own powers of recollection: "Man needs to mechanize his record more fully," he says, if he is not to "become bogged down...by overtaxing his limited memory". With the Memex, the user could "add marginal notes and comments," and "build a trail of his interest" through the larger information space. She could share trails with friends, identify related works, and create personal annotations. Bush's Memex would give each individual the ability to create, categorize, classify, and relate his own set of information corresponding to his unique personal viewpoint.  Much of that information would in fact consist of bits and pieces from public documents, just as the majority of the knowledge inside our own heads has been imbibed from what we read and hear. But the Memex also allowed for the specialized recording of information that each individual perceived and needed to retain. The idea of supplementing our memory" was not a one-size-fits-all proposition, since no two people have the same interests, opinions, or memories.  Instead, it demanded a subjective expression of knowledge, unique to each individual.

=== Graphical knowledge capture tools ===

Great emphasis is placed on the pictorial nature of diagrams to represent abstract knowledge; the use of spatial layout, color, and images is said to strengthen understanding and promote creativity. Each of the three primary schools—[[mind map]]ping, [[concept map]]ping, and [[cognitive map]]ping—prescribes its own data model and procedures, and each boasts a number of software applications designed specifically to create compatible diagrams.

==== Mind mapping ====
[[Mind map]]ping was promoted by pop psychologist [[Tony Buzan]] in the 1960s, and commands the allegiance of an impressive number of adherents worldwide.  A mind map is essentially nothing more than a visual outline, in which a main idea or topic is written in the center of the diagram, and subtopics radiate outwards in increasing levels of specificity. The primary value is in the freeform, spatial layout (rather than a sequential, numbered outline), the ability for a software application to hide or reveal select levels of detail, and as mentioned above, graphical adornments. The basic data model is a [[Tree (graph theory)|tree]], rather than a [[Graph (discrete mathematics)|graph]], with all edges implicitly labeled "supertopic/subtopic". Numerous tools are available for constructing mind maps.

==== Concept mapping ====

[[Concept map]]s were developed by Cornell Professor [[Joseph D. Novak|Joseph Novak]],<ref name="Novak 2003" /> and based on [[David Ausubel]]'s assimilation theory of learning.<ref name="Ausubel 1968" /> An essential tenet is that newly encountered knowledge must be related to one's prior knowledge in order to be properly understood. Concept maps help depict such connections graphically. Like mind maps, they feature evocative words or phrases in boxes connected by lines. There are two principal differences, however: first, a concept map is properly a graph, not a tree, permitting arbitrary links between nodes rather than only parent/child relationships; and second, the links are labeled to identify the nature of the inter-concept relationship, typically with a verb phrase. In this way, the links on a diagram can be read as English sentences, with the upstream node as the subject and the downstream node as the direct object of the sentence.

There are many applications available that could be used for drawing these diagrams, not all of which directly acknowledge their support for concept maps in particular.<ref name="Canas et al 2005"/><ref name="Gaines and Shaw 1995" />

A concept map is virtually identical to the notion of a "[[semantic network]]",<ref name="Woods 1985" /> which has served as a cornerstone for much artificial intelligence work since its inception. Semantic networks, too, are directed graphs in which the nodes represent concepts and labeled edges the relationships between them. Much psychology research has strengthened the idea that the human mind internalizes knowledge in something very like this sort of framework. This likely explains the ease with which concept mapping techniques have been adopted by the uninitiated, since concept maps and semantic networks can be considered equivalent.

==== Cognitive mapping ====

[[Cognitive mapping]], developed by Fran Ackermann and Colin Eden at the University of Strathclyde, uses the same data model as does concept mapping, but with a new set of techniques. In cognitive maps, element names have two parts, separated by an ellipsis that is read "as opposed to" in order to further clarify the semantics of the node. ("Cold...hot" is different from "cold...freezing," for example.) Links are of three types—causal, temporal, connotative—the first of which is the most common and is read as "may lead to". Generally cognitive mapping is best suited to domains involving arguments and [[decision making]]. Cognitive mapping is not nearly as widespread as the other two paradigms. Together, these and related methods have brought into the mainstream the idea of breaking down knowledge into its fundamental elements, and representing them graphically. Students and workers from widely diverse backgrounds have experienced success in better articulating and examining their own knowledge, and in discovering how it relates to what else they know. Although architectural considerations prevent any of these tools from functioning as bona fide PKBs, the ideas they have contributed to a front-end interface mechanism cannot be overestimated.

=== Hypertext systems ===

Many in the hypertext community [[Hypertext#History|reference]] Vannevar Bush's article as the cornerstone of their heritage. Hence the development of hypertext techniques, while seldom applied specifically towards PKB solutions, is important.  There have basically been three types of hypertext systems: those that exploit features of non-linear text to create a dynamic, but coherent "hyperdocument";<ref name="Schneiderman 1987" /><ref name="Goodman 1988" /> those
that prescribe ways of linking existing documents together for navigation and expression of affinities;<ref name="Davis et al 1993" /><ref name="Garrett et al 1986" /><ref name="Pearl 1989" /> and those that use the hypertext model specifically to model abstract knowledge. Though the first and especially the second category have dominated research efforts (and public enthusiasm) over the past several decades, it is this third class that is closest in spirit to the original vision of hypertext by its founders.

In a similar vein to [[Vannevar Bush|Bush]], [[Doug Engelbart]]'s focus was to develop computer systems to "help people think better".  He sought data models that more closely paralleled the human thought process, and settled on using hypertext as a way to represent and store abstract human knowledge. Although his "[[NLS (computer system)|Augment]]" system underwent many changes, the original purpose closely aligned with that of PKBs.<ref name="Engelbart 1953"/>

More recently, Randall Trigg's TextNet<ref name="Trigg and Weiser 1986" /> and [[NoteCards]]<ref name="Halasz et al 1987" /> systems further explored this idea. TextNet revolved around "primitive pieces of text connected with typed links to form a network similar in many ways to a semantic network".<ref name="Conklin and Begeman 1988"/> Though text-centric, it was clear that Trigg's goal was to model the associations between primitive ideas and hence to reflect the mind's understanding. "By using...structure, meaning can be extracted from the relationships between chunks (small pieces of text) rather than from the words making them up."<ref name="Trigg and Weiser 1986" /> The subsequent [[NoteCards]] effort was similarly designed to "formulate, structure, compare, and manage ideas". It was useful for "analyzing information, constructing models, formulating arguments, designing artifacts, and generally processing ideas".

Conklin and Begeman's [[gIBIS]] system was another early effort into true knowledge representation, specifically for the field of design deliberations and arguments.<ref name="Conklin and Begeman 1988"/> The project lived on in the later project QuestMap<ref name="Selvin 1999" /> and the more modern [[Compendium (software)|Compendium]], which has been primarily used for capturing group knowledge expressed in face-to-face meetings. In all these cases, systems use semantic hypertext in an attempt to capture shared knowledge in its most basic form. Other examples of knowledge-based hypertext tools include Mental Link,<ref name="Dede and Jayaram 1990" /> Aquanet,<ref name="Marshall et al 1991" /> and SPRINT,<ref name="Carlson and Ram 1990" /> as well
as a few current commercial tools such as [[PersonalBrain]] and [[Tinderbox (application software)|Tinderbox]]<ref name="Bernstein 2003" /> and open source tools such as [[TiddlyWiki]].

=== Note-taking applications ===

[[Electronic Notetaking|Note-taking applications]] allow a user to create snippets of text and then organize or categorize them in some way. These tools can be used to form PKBs that are composed of such text snippets.

Most of these tools are based on a [[Tree (graph theory)|tree]] hierarchy, in which the user can write pages of notes and then organize them into sections and subsections. The higher level sections or chapters often receive a colored tab exactly as a physical three-ring notebook might. Other designers eschew the tree model for a more flexible category-based approach (see section [[#Data models|data models]]). The primary purpose of all these tools is to offer the benefits of freeform note-taking with none of the deficiencies: users are free to brainstorm and jot down anything from bullet points to polished text, while still being able to search, rearrange, and restructure the entire notebook easily.

An important subcategory of note-taking tools is outliners (e.g., [[OmniOutliner]]), or applications specifically designed to organize ideas in a hierarchy. These tools typically show a two-pane display with a tree-like navigation widget in the left-pane and a list of items in the right-pane. Topics and subtopics can be rearranged, and each outline stored in its own file. Modern outliners feature the ability to add graphics and other formatting to an item, and even hyper links to external websites or documents. The once abandoned (but now resurrected) Ecco system was among the first to allow items to have typed attributes, displayed in columns. This gives the effect of a custom spreadsheet per topic, with the topic's items as rows and the columns as attributes. It allows the user to gracefully introduce structure to their information as it is identified.

Of particular interest are applications optimized for subsuming portions of an information space realm into a PKB, where they can be clustered and arranged according to the user's own perceptions. The Virtual Notebook System (VNS)<ref name="Burger et al 1991" /> was one of the first to emphasize this. VNS was designed for sharing information among scientists at the Baylor College of Medicine; a user's "personal notebook" could make references to specific sections of a "community notebook," and even include arbitrary segments of other documents through a cut-and-paste mechanism.

=== Document management systems ===
{{main|Document management system}}

Another influence on PKBs are systems whose primary purpose is to help users organize documents, rather than personal knowledge derived from those documents. Such systems do not encode subjective knowledge per se, but they do create a personal knowledge base of sorts by allowing users to organize and cross-reference their information artifacts.

These efforts provide alternative indexing mechanisms to the limited "directory path and file name" approach. Presto<ref name="Dourish et al 1999" /> replaces the directory hierarchy entirely with attributes that users assign to files. These key-value pairs represent user-perceived properties of the documents, and are used as a flexible means for retrieval and organization. William Jones' Memory Extender<ref name="Jones 1986" /> was similar in spirit, but it dynamically varied the "weight" of a file's keywords according to the user's context and perceived access patterns. In [[Haystack (MIT project)|Haystack]],<ref name="Adar et al 1999" /> users—in conjunction with automated software agents—build a graph-based network of associative links through which documents can be retrieved.

Metadata and multiple
categorization can also be applied to provide multiple retrieval paths customized to the way the individual thinks and works with their information sources. WebTop<ref name="Wolber et al 2002" /> allowed the user to create explicit links between documents, but then also merged these user-defined relationships with other types of associations. These included the hyperlinks contained in the documents, associations implied by structural relationships, and content similarities discovered by text analysis. The idea was that any way in which items can be considered "related" should be made available to the user for help with retrieval.

A subclass of these systems integrate the user's personal workspace with a search facility, blurring the distinction between information retrieval and information organization.  SketchTrieve,<ref name="Hendry and Harper 1997" /> DLITE,<ref name="Cousins et al 1997" /> and Garnet<ref name="Buchanan et al 2004" /> each materialized elements from the retrieval domain (repositories, queries, search results) into tangible, manipulatable screen objects. These could be introduced directly into a spatial layout that also included the information sources themselves. These systems can be seen as combining a spatial hypertext interface as in VIKI<ref name="Marshall and Shipman 1995" /> with direct access to digital library search facilities.  NaviQue<ref name="Furnas and Rauch 1998" /> was largely in the same vein, though it incorporated a powerful similarity engine to proactively aid the user in organization. CYCLADES<ref name="Renda and Straccia 2005" /> let users organize Web pages into folders, and then attempted to infer what each folder "means" to that user, based on a statistical textual analysis of its contents. This helps users locate other items similar to what's already in a folder, learn what other users have found interesting and have grouped together, etc.

All of these document management systems are principally concerned with organizing objective information sources rather than the expression of subjective knowledge. Yet their methods are useful to consider with respect to PKB systems, because such a large part of our knowledge comprises things we remember, assimilate, and repurpose from objective sources. Search environments like SketchTrieve, as well as snippet gatherers like YellowPen, address an important need in [[knowledge management]]: bridging the divide between the subjective and objective realms, so that the former can make reference to and bring structure to the latter.

== Claims and benefits ==

PKB systems make various claims about the advantages of using them. These can be classified as follows:<ref name = "Davies 2005"/><ref name = "Davies 2011"/>

* '''Knowledge generation and formulation.''' Here the emphasis is on procedure, not persistence; it is the act of simply using the tool to express one's knowledge that helps, rather than the ability to retrieve it later.
* '''[[Knowledge capture]].''' PKBs do not merely allow one to express knowledge, but also to capture it before it elusively disappears. Often the emphasis is on a streamlined user interface, with few distractions and little encumbrance.  The point is to lower the burden of jotting down one's thoughts so that neither task nor thought process is interrupted.
* '''Knowledge organization.''' A 2003 study on note-taking habits found that "better organization" was the most commonly desired improvement in people's own information recording practices.<ref name="Hayes et al 2003" />
* '''Knowledge management and [[knowledge retrieval|retrieval]].''' Perhaps the most critical aspect of a PKB is that the knowledge it stores is permanent and accessible, ready to be retrieved at any later time.
* '''[[Knowledge integration|Integrating]] heterogeneous sources.''' Recognizing that the knowledge people form comes from a variety of different places, many PKB systems emphasize that the information from diverse sources and of different types can be integrated into a single database and interface.

== Data models ==
{{see|Data model}}
PKB systems can be compared along a number of different axes, the most important of which is the underlying data model they support. This is what prescribes and constrains the nature of the knowledge they can contain: what types of knowledge elements are allowed, how they can be structured, and how the user perceives them and can interact with them.<ref name = "Davies 2005"/><ref name = "Davies 2011"/>

Three aspects of data models can be identified: the ''structural framework'', which prescribes rules about how knowledge elements can be structured and interrelated; the ''knowledge elements'' themselves, or basic building blocks of information that a user creates and works with; and ''schema'', which involves the level of formal semantics introduced into the data model.

=== Structural frameworks ===

The following structural frameworks have been featured in one or more prominent PKB systems.

==== Tree ====

Systems that support a [[Tree (data structure)|tree]] model allow knowledge elements to be organized into a containment hierarchy, in which each element has one and only one "parent". This takes advantage of the mind's natural tendency to classify objects into groups, and to further break up each classification into subclassifications. It also mimics the way that a document can be broken up into chapters, sections, and subsections.  It tends to be natural for users to understand.

All of the applications for creating Buzan [[Mind Map|mind maps]] are based on a tree model, because a mind map ''is'' a tree. Each mind map has a "root" element in the center of the diagram (often called a "main topic") from which all other elements emanate as descendents.  Every knowledge element has one and only one place in this structure. Some tools, such as [[MindManager]], extend this paradigm by introducing "floating topics", which are not anchored to the hierarchy, and permitting "crosslinks" to arbitrary topics, similar to those in concept maps.

Other examples of tree-based systems are most personalized search interfaces,<ref name="Renda and Straccia 2005" /><ref name="Di Giacomo et al 2001" /><ref name="Reyes-Farfan and Sanchez 2003"/> outliners, and most of the "notebook-based" note-taking systems. By allowing them to partition their notes into sections and subsections, note-taking tools channel users into a tree hierarchy. In recognition of this confining limitation, many of these tools also permit a kind of "crosslink" between items, or employ some form of transclusion (see below) to allow items to co-exist in several places. The dominant paradigm in such tools, however, remains the simple parent-child hierarchy.

==== Graph ====

Graph-based systems allow users to create knowledge elements and then to interconnect them in arbitrary ways. The elements of a [[Graph (discrete mathematics)|graph]] are traditionally called "vertices," and connected by "arcs," though the terminology used by graph-based systems varies widely (see Table 1) and the hypertext community normally uses the terms "nodes" and "links". There are no restrictions on how many arcs one vertex can have with others, no notion of a "parent/child" relationship between vertices (unless the user chooses to label an arc with those semantics), and normally no "root" vertex. In many systems, arcs can optionally be labeled with a word or phrase indicating the nature of the relationship, and adorned with arrowheads on one or both ends to indicate navigability. (Neither of these adornments is necessary with a tree, since all relationships are implicitly labeled "parent/child" and are navigable from parent to child.) A graph is a more general form of a tree, and hence a strictly more powerful form of expression.

{| class="wikitable"
|+Terminology employed by a sampling of graph-based knowledge tools.
! System !! Vertex !! Arc !! Graph
|-
|[[Axon Idea Processor]]||object||link||diagram
|-
|Banxia Decision Explorer||concept||link||view
|-
|[[Compendium (software)|Compendium]]||node||link||view
|-
|[[Haystack (MIT project)|Haystack]]||needle||tie||bale
|-
|Idea Graph||idea||connection||ideagraph
|-
|Knowledge Manager||concept||relation||map
|-
|[[MyLifeBits]]||resource||link/annotation||story
|-
|[[NoteCards]]||note card||link||browser
|-
|[[PersonalBrain]]||thought||link||brain
|-
|RecallPlus||idea||association||diagram
|-
|SMART Ideas||symbol||connector||level
|}

This model is the defining characteristic of hypertext systems<ref name="Halasz and Schwartz 1994" /> including many of those used for document management.<ref name="Wolber et al 2002" /><ref name="Adar et al  1999" /> It is also the underpinning of all concept-mapping tools, whether they actually acknowledge the name "concept maps"<ref name="Canas et al 2005" /><ref name="Gaines and Shaw 1995" /> or advertise themselves simply as tools to draw knowledge diagrams. As mentioned previously, graphs draw their power from the fact that humans are thought to model knowledge as graphs (or equivalently, semantic networks) internally. In fact, it could be argued that all human knowledge can be ultimately reduced to a graph of some kind, which argues strongly for its sufficiency as a structural framework.<ref name="Quillian 1968" /><ref name="Nosek and Roth 1990" />

An interesting aspect of graph-based systems is whether or not they require a ''[[Connectivity (graph theory)|fully connected]]'' graph.  A fully connected graph is one in which every vertex can be reached from any other by simply performing enough arc traversals. There are no "islands" of vertices that are severed from each other. Most graph-based tools allow non-fully-connected graphs: knowledge elements are added to the system, and connected arbitrarily to each other, without constraint.  But a few tools, such as [[PersonalBrain]] and [[Compendium (software)|Compendium]], actually require a single network of information in which every knowledge element must be indirectly connected to every other. If one attempts to remove the last link that connects a body of nodes to the original root, the severed elements are either "forgotten" or else moved to a deleted objects heap where they can only be accessed by restoring a connection to the rest of the graph.

Some hypertext systems<ref name="Garrett et al 1986" /><ref name="Delisle and Schwartz 1986" /> add precision to the basic linking mechanism by allowing nodes to reference not only other nodes, but sections within nodes.<ref name="Halasz and Schwartz 1994" /> This ability is especially useful if the nodes themselves contain sizeable content, and also for PKB elements making reference to fragments of objective sources.

==== Tree plus graph ====

Although graphs are a strict superset of trees, trees offer some important advantages in their own right: simplicity, familiarity, ease of navigation, and the ability to conceal details at any level of abstraction. Indeed, the problem of "disorientation" in hypertext navigation<ref name="Conklin and Begeman 1988"/><ref name="Mantei 1982" /> largely disappears with the tree model; one is never confused about "where one is" in the larger structure, because traversing the parent hierarchy gives the context of the larger surroundings. For this reason, several graph-based systems have incorporated special support for trees as well, to combine the advantages of both approaches.  For instance, in concept mapping techniques, a generally hierarchical paradigm is prescribed, after which users are encouraged to identify "crosslinks" between distant concepts. Similarly, some systems using the mind mapping paradigm permit arbitrary relationships between nodes.

One of the earliest systems to combine tree and graph primitives was TEXTNET,<ref name="Trigg and Weiser 1986" /> which featured two types of nodes: "chunks" (which contained content to be browsed and organized) and "table of contents" nodes (or "tocs".) Any node could freely link to any other, permitting an unrestricted graph. But a group of tocs could be combined to form a tree-like hierarchy that bottomed out in various chunk nodes. In this way, any number of trees could be superimposed upon an arbitrary graph, allowing it to be viewed and browsed as a tree, with all the requisite advantages. Strictly speaking, a network of tocs formed a [[Directed acyclic graph|DAG]] rather than a tree. This means that a "chunk" could be represented in multiple places in the tree, if two different traversal paths ended up referring to the same chunk. A DAG is essentially the result of applying transclusion to the tree model. This is also true of NoteCards. NoteCards<ref name="Halasz et al 1987" /> offered a similar mechanism, using "FileBoxes" as the tree component that was overlaid upon the semantic network of notecards.

Brown University's IGD project explored various ways to combine and display unrestricted graphs with hierarchy, and used a visual metaphor of spatial containment to convey both graph and tree structure.<ref name="Feiner 1988" /> Their notion of "link inheritance" simplifies the way in which complex dual structures are displayed while still faithfully depicting their overall trends. Commercially, both [[PersonalBrain]] and Multicentrix<ref name="Koy 1997" /> provide explicit support for parent/child relationships in addition to arbitrary connections between elements, allowing tree and graph notions to coexist.  Some note-taking tools, while essentially tree-based, also permit crosslinks between notes.

==== Spatial ====
Some designers have shunned links between elements altogether, favoring instead spatial positioning as the sole organizational paradigm. Capitalizing on the human's tendency to implicitly organize through clustering, making piles, and spatially arranging, some tools offer a 2D workspace for placing and grouping items. This provides a less formal (and perhaps less intimidating) way for a user to gradually introduce structure into a set of items as it is discovered.

This approach originated from the spatial hypertext community, demonstrated in various projects,<ref name="diSessa and Abelson 1986" /> and VIKI/VKB<ref name="Marshall and Shipman 1995" /><ref name="Shipman et al 2000" /> With these programs, users place information items on a canvas and can manipulate them to convey organization imprecisely. Some project<ref name="Marshall and Shipman 1995" /> could infer the structure from a user's freeform layout: a spatial parser examines which items have been clustered together, colored or otherwise adorned similarly, etc., and makes judgments about how to turn these observations into machine-processible assertions. While others (Pad<ref name="Perlin and Fox 1993" />) allowed users to view different objects in varying levels of detail as they panned around the workspace.

Certain note-taking tools<ref name="Burger et al 1991" /><ref name="Akscyn et al 1987" /> combine an overarching tree structure with spatial freedom on each "frame" or "page". Users can access a particular page of the notebook with basic search or tree navigation facilities, and then lay out notes and images on the page as desired. Many graph-based approaches (such as concept mapping tools) also allow for arbitrary spatial positioning of elements. This allows both kinds of relationships to be expressed: explicit links and less formal expression through creative use of the screen.

==== Categories ====

In category-based structural frameworks, rather than being described in terms of their relationships to other elements (as with a tree or graph), items are simply grouped together in one or more categories, indicating that they have something in common. This scheme is based on the branch of pure mathematics called [[set theory]], in which each of a body of objects either has, or does not have, membership in each of some number of sets. There is normally no restriction as to how many different categories a given item can belong to, as is the case with mathematical sets.

Users may think of categories as collections, in which the category somehow encloses or "owns" the items within it. Indeed, some systems depict categories in this fashion, such as the Vista interface<ref name="Dourish et al 1999" /> where icons standing for documents are enclosed within ovals that represent categories. This is merely a convention of display, however, and fundamentally, categories are the same as simple keywords.

The most popular application to embrace the category approach was the original [[Lotus Agenda|Agenda]].<ref name="Kaplan et al 1990" /> All information retrieval in Agenda was performed in terms of category membership. Users specified queries that were lists of categories to include (or exclude), and only items that satisfied those criteria were displayed. Agenda was particularly sophisticated in that the categories themselves formed a tree hierarchy, rather than a flat namespace. Assigning an item to a category also implicitly assigned it to all ancestors in the hierarchy.

[[Personal Knowbase]] is a more modern commercial product based solely on a keyword (category) paradigm, though it uses a simple flat keyword structure rather than an inheritance hierarchy like Agenda. [[Haystack (MIT project)|Haystack]]<ref name="Adar et al 1999" /> and [[Open Source Applications Foundation|Chandler]] are other information management tools which use categorization in important ways. William Jones' Memory Extender<ref name="Jones 1986" /> took an artificial intelligence twist on the whole notion of keywords/categories, by allowing an item's keywords to be weighted, and adjusted over time by both the user and the system. This allowed the strength of category membership to vary dynamically for each of an item's assignments, in an attempt to yield more precise retrieval.

==== Chronological ====

Yale University's Lifestreams project<ref name="Fertig et al 1996" /> used timestamps as the principal means of organization and retrieval of personal documents. In Fertig et al.'s own words:

<blockquote>A [[lifestreaming|lifestream]] is a time-ordered stream of documents that functions as a diary of your electronic life; every document you create is stored in your lifestream, as are the documents other people send you. The tail of your stream contains documents from the past, perhaps starting with your electronic birth certificate.  Moving away from the tail and toward the present, your stream contains more recent documents such as papers in progress or the latest electronic mail you've received...</blockquote>

Documents are thus always ordered and accessed chronologically. Metadata-based queries on the collection produce "substreams," or chronologically ordered subsets of the original documents. The rationale for time-based ordering is that "time is a natural guide to experience; it is the attribute that comes closest to a universal skeleton-key for stored experience".<ref name="Freeman and Gelernter 1996" /> Whether chronology is our principal or even a common natural coding mechanism psychologically can be debated. But since any PKB system can easily create such an index, it seems worthwhile to follow Lifestreams' lead and allow the user to sort and retrieve based on time, as many systems have done. If nothing else, it relieves the user from having to create names for knowledge elements, since the timestamp is always an implicit identifying mark. PlanPlus, based on the Franklin-Covey planner system, is also chronologically modeled, and a number of products based on other data models<ref name="Kaplan et al 1990" /> offer chronological indexing in addition to their core paradigm.

==== Aquanet's framework ====

Though advertised as a hypertext system, Marshall ''et al.'''s Aquanet<ref name="Marshall et al 1991" /> went far beyond the traditional node-link graph model.  Knowledge expressed in Aquanet is centered around "relations," or n-ary links between objects in which the semantics of each participant in the relation is specified by the relation type. Each type of relation specifies a physical display (i.e., how it will be drawn on the screen, and the spatial positioning of each of its participants), and a number of "slots" into which participants can be plugged. Each participant in a relation can be either a base object, or another relation. Users can thus define a schema of relation types, and then build a complex semantic model out of relations and objects. Since relation types can be specified to associate any number of nodes (instead of just two, as in the graph model), this potentially allows more complex relationships to be expressed.

It should be noted, however, that the same effect can be achieved in the basic graph model by simply taking the n-ary relations and "reifying" them (i.e., turning them into nodes in their own right.) For instance, suppose we define a relation type "assassination," with slot types of "assassin," "victim," "location," and "weapon". We could then create a relation based on this type where the participants are "John Wilkes Booth," "Abraham Lincoln," "Ford's Theatre," and "derringer". This allows us to express a complex relationship between multiple objects in Aquanet. But we can express the same knowledge with the basic graph model by simply creating a node called "Lincoln's assassination" and then creating typed links between that node and the other four labeled "assassin," "victim," etc. Aquanet's biggest achievement in this area is the ability to express the schema of relation types, so that the types of objects an "assassination" relation can connect are consistent and enforced.

=== Knowledge elements ===

There are several options for specifying what knowledge elements consist of, and what kind of internal structure, if any, they possess:

# '''Word/phrase/concept'''.  Most systems engineered for knowledge representation encourage structures to be composed of very simple elements, usually words or phrases.  This is in the spirit of both mind mapping and concept mapping, where users are encouraged to use simple phrases to stand for mental concepts.
# '''Free text notes'''. Nearly all systems permit large amounts of free text to exist in the PKB, either as the contents of the elements themselves (NoteCards,<ref name="Halasz et al 1987" /> Hypercard,<ref name="Goodman 1988" /> TreePad) or attached to elements as separate, supplementary pages (Agenda,<ref name="Kaplan et al 1990" /> Zoot, HogBay).
# '''Links to an information space'''.  Since a user's knowledge base is to correspond to her mental perceptions, it seems profitable for the PKB to point to entities in the information space from which she formed those perceptions.  Many systems do in fact allow their knowledge elements to point to the original sources in some way. There are three common techniques:
##The knowledge element actually ''represents'' an original source.  This is the case for document management systems (WebTop,<ref name="Wolber et al 2002" /> MyLifeBits,<ref name="Gemmell et al 2002" /> Haystack<ref name="Adar et al 1999" />), integrated search facilities (NaviQue,<ref name="Furnas and Rauch 1998" /> CYCLADES<ref name="Renda and Straccia 2005" />), VIKI/VKB.<ref name="Marshall and Shipman 1995" /><ref name="Shipman et al 2000" />  Tinderbox<ref name="Bernstein 2003" /> will also allow one of its notes to be a URL, and the user can control whether its contents should be captured once, or "auto-fetched" as to receive constant web updates.  Many systems, in addition to storing a page of free text for each knowledge element, also permit any number of hyperlinks to be attached to a knowledge element (e.g., [[FreeMind|Freemind]], [[PersonalBrain]], Inspiration). VNS,<ref name="Burger et al  1991" /> which allows users to point to a community notebook page from within their personal notebook, gives similar functionality.
## The knowledge element is a repurposed snippet from an original source. This is potentially the most powerful form, but is rare among fully featured PKB systems.  Cartagio, Hunter-Gatherer,<ref name="Schraefel et al 2002" /> and YellowPen all allow Web page excerpts to be assimilated and organized, although they primarily only do that, without allowing them to easily be combined with other subjective knowledge. DEVONThink and MyBase's WebCollect plug-in add similar functionality to their more general-purpose, tree-based information managers. Both of these systems, when a snippet is captured, archive the entire Web page locally so it can be returned to later. The user interfaces of CircusPonies and StickyBrain have been heavily optimized towards grabbing information from other applications and bringing them into the PKB without disturbing the user's workflow.
# '''Composites''' Some programs allow a user to embed knowledge elements (and perhaps other information as well) inside a knowledge element to form an implicit hierarchy. Trees by themselves fall into this category, of course, since each node in the tree can be considered a "composite" of its content and children. But a few graph-based tools offer composite functionality as well. In Aquanet,<ref name="Marshall et al  1991" /> "relations" form the fundamental means of connection, and the units that are plugged into a relation can be not only objects, but other relations as well. This lends a recursive quality to a user's modeling. VIKI/VKB's spatial environment offers "subspaces" which let a user partition their visual workspace into subregions, whose internal contents can be viewed at a glance from the parent. Boxer's<ref name="diSessa and Abelson 1986" /> paradigm is similar. Tinderbox is a graph-based tool that supports hierarchical composite structures, and [[Compendium (software)|Compendium]] extends this even further by allowing transclusion of "views" as well as of nodes. Unlike the other tools, in Compendium the composite hierarchy does not form a [[Directed acyclic graph|DAG]], but rather an arbitrary graph: view A can appear on view B, and B can in turn appear on A. The user's intuitive notion of "inside" must be adapted somewhat in this case.

=== Schema ===

In the context of PKBs, "schema" means the ability for a user to specify types and introduce structure to aspects of the data model. It is a form of metadata whereby more precise semantics can be applied to various elements of the system. This facilitates more formal knowledge expression, ensures consistency across items of the same kind, and can better allows automated agents to process the information.

Both knowledge elements, and links, can contain various aspects of schema.

==== Schema for knowledge elements ====

===== Types, and related schema =====

In a PKB, a "[[type system]]" allows users to specify that a knowledge element is a member of a specific class or category or items, to provide a built-in method of organization and retrieval. Generally speaking, systems can make knowledge elements untyped, rigidly typed, or flexibly typed. In addition, they can incorporate some notion of inheritance among elements and their types.  There is a distinction between types and categories here. A category-based scheme, typically allows any number of categories/keywords to be assigned to an item. There are two differences between this and the notion of type. First, items are normally restricted to being of a single type, and this usually indicates a more intrinsic, permanent property of an item than simply its presence in a category collection.  (For example, one could imagine an item called "XYZ Corporation" shifting into and out of categories like "competitors", "overseas distributors," or "delinquent debtors" over time, but its core type of "company" would probably be static for all time.) Second, types often carry structural specifications with them: if an item is of a given type, this means it will have values for certain attributes appropriate to that type.  Some systems that do not allow typing offer the ability to approximate this function through categories.

Untyped elements are typical among informal knowledge capture tools, since they are designed to stimulate brainstorming and help users discover their nascent mental models.  These tools normally want to avoid forcing the user to commit to structure prematurely.  Most mind mapping and many concept mapping tools are in this category: a concept is simply a word or phrase, with no other semantic information (e.g., [[Visual Mind]]). Note-taking tools also usually take this approach, with all units of information being of the same type "note".

At the other extreme are tools which, like older relational database technology, require all items to be declared as of a specific type when they are created. Often this type dictates the internal structure of the element. These tools are better suited to domains in which the structure of knowledge to be captured is predictable, well-understood, and known in advance. For PKB systems, they are probably overly restrictive. KMap<ref name="Gaines and Shaw 1995" /> and Compendium are examples of tools that allow (and require) each item to be typed; in their case, the type controls the visual appearance of the item, rather than any internal structure.

In between these two poles are systems that permit typed and untyped elements to co-exist. NoteTaker is such a product; it holds simple free-text pages of notes, without any structure, but also lets the user define "templates" with predefined fields that can be used to instantiate uniformly structured forms. TreePad has a similar feature. Some other systems blur the distinction between typed and untyped, allowing the graceful introduction of structure as it is discovered. VKB,<ref name="Shipman et al 2000" /> for example, supports an elegant, flexible typing scheme, well suited to PKBs.  Items in general consist of an arbitrary number of [[attribute–value pair]]s. But when consistent patterns emerge across a set of objects, the user can create a type for that group, and with it a list of expected attributes and default values. This structure can be selectively overridden by individual objects, however, which means that even objects assigned to a particular type have flexible customization available to them. Tinderbox offers an alternate way of achieving this flexibility, as described below.

Finally, the [[object-oriented]] notion of [[Inheritance (computer science)|type inheritance]] is available in a few solutions. The different card types in NoteCards are arranged into an inheritance hierarchy, so that new types can be created as extensions of old. Aquanet extends this to multiple inheritance among types; the "slots" that an object contains are those of its type, plus those of all supertypes. SPRINT and Tinderbox also use a frame-based approach, and allow default values for attributes to be inherited from supertypes. This way, an item need not define values for all its attributes explicitly: unless overridden, an item's slot will have the shared, default value for all items of that type.

===== Other forms of schema =====

In addition to the structure that is controlled by an item's type, other forms of metadata and schema can be applied to knowledge elements.

* '''Keywords'''. Many systems let users annotate items with user-defined keywords. Here the distinction between an item's contents and the overall knowledge structure becomes blurred, since an item keyword could be considered either a property of the item, or an organizational mechanism that groups it into a category with like items. Systems using the category data model (e.g., Agenda) can employ keywords for the latter purpose. Some systems based on other data models also use keywords to achieve category-like functionality.
* '''Attribute/value pairs'''. Arbitrary attribute/value pairs can also be attached to elements in many systems, which gives a PKB the ability to define semantic structure that can be queried.  Frame-based systems like SPRINT and Aquanet are examples, as well as NoteTaker, VKB, and Tinderbox. MindPad[AKS-Labs 2005] is notable for taking the basic concept mapping paradigm and introducing schema to it via its "model editor". As mentioned earlier, adding user-defined attribute/value pairs to the items in an outliner yields spreadsheet-like functionality, as in Ecco and [[OmniOutliner]]. Some systems feature attribute/value pairs, but only in the form of system-defined attributes, not user-defined ones.
* '''Knowledge element appearance'''. Some tools modify a knowledge element's visual appearance on the screen in order to convey meaning to the user. SMART Ideas and [[Visual Mind]] let the user freely choose each element's icon from a variety of graphics, while KMap<ref name="Gaines and Shaw 1995" /> ties the icon directly to its underlying type. Other graphical aspects that can be modified include color (VIKI<ref name="Marshall and Shipman 1995" />), the set of attributes shown in a particular context (VKB<ref name="Shipman et al 2000" />), and the spatial positioning of objects in a relation (Aquanet<ref name="Marshall et al 1991" />).

==== Schema for links ====

In addition to prescribing schema for knowledge elements, many systems allow some form of information to be attached to the links that connect them.

In most of the early hypertext systems, links were unnamed and untyped, their function being merely to associate two items in an unspecified manner. The mind mapping paradigm also does not name links, but for a different reason: the implicit type of every link is one of generalization/specialization, associating a topic with a subtopic. Hence specifying types for the links would be redundant, and labeling them would clutter the diagram.

Concept mapping prescribes the naming of links, such that the precise nature of the relationship between two concepts is made clear. As mentioned above, portions of a concept map are meant to be read as English sentences, with the name of the link serving as a verb phrase connecting the two concepts. Numerous systems thus allow a word or phrase to decorate the links connecting elements.

Named links can be distinguished from ''typed'' links, however. If the text attached to a link is an arbitrary string of characters, unrelated to that of any other link, it can be considered the link name. Some systems, however, encourage the re-use of link names that the user has defined previously. In [[PersonalBrain]], for instance, before specifying the nature of a link, the user must create an appropriate "link type" (associated with a color to be used in presentation) in the system-wide database, and then assign that type to the link in question. This promotes consistency among the names chosen for links, so that the same logical relationship types will hopefully have the same tags throughout the knowledge base. This feature also facilitates searches based on link type, among other things. Other systems, especially those suited for specific domains such as decision modeling ([[gIBIS]]<ref name="Conklin and Begeman 1988" /> and Banxia Decision Explorer), predefine a set of link types that can be assigned (but not altered) by the user.

Some more advanced systems allow links to bear attribute/value pairs themselves, and even embedded structure, similar to those of the items they connect. In Haystack<ref name="Adar et al 1999" /> this is the case, since links ("ties") and nodes ("needles") are actually defined as subtypes of a common type ("straw").

KMap similarly defines a link as a subclass of node, which allows links to represent n-ary relationships between nodes, and enables recursive structure within a link itself. It is unclear how much value this adds in knowledge modeling, or how often users take advantage of such a feature. Neptune<ref name="Delisle and Schwartz 1986" /> and Intermedia<ref name="Garrett et
al 1986" /> are two older systems that also support attributes for links, albeit in a simpler manner.

Another aspect of links that generated much fervor in the early hypertext systems was that of link ''precision'': rather than merely connecting one element to another, systems like Intermedia defined anchors within documents, so that a particular snippet within a larger element could be linked to another snippet. The Dexter model<ref name="Halasz and Schwartz 1994" /> covers this issue in detail. For PKB purposes, this seems to be most relevant as regards links to the objective space, as discussed previously. If the PKB truly contains knowledge, expressed in appropriately fine-grained parts, then link precision between elements in the knowledge base is much less of a consideration.

This discussion on links has only considered connections between knowledge elements in the system, where the system has total control over both ends of the connection. As described in the previous section, numerous systems provide the ability to "link" from a knowledge element inside the system to some external resource: a file or a URL, say. These external links typically cannot be enhanced with any additional information, and serve only as convenient retrieval paths, rather than as aspects of knowledge representation.

== Architecture ==

The idea of a PKB gives rise to some important architectural considerations. While not constraining the nature of what knowledge can be expressed, the architecture nevertheless affects more mundane matters such as availability and workflow. But even more importantly, the system's architecture determines whether it can truly function as a lifelong, integrated knowledge store—the "base" aspect of the personal knowledge base defined above.

=== File-based ===

Traditionally, most electronic PKB systems have employed a simple storage mechanism based on flat files in a filesystem. This is true of virtually all of the mind mapping tools ([[MindManager]]), concept mapping tools, and even a number of hypertext tools (NoteCards,<ref name="Halasz et al 1987" /> Hypercard,<ref name="Goodman 1988" /> Tinderbox<ref name="Bernstein 2003" />). Typically, the main "unit" of a user's knowledge design—whether that be a mind map, a concept map, an outline, or a "notebook"—is stored in its own file somewhere in the filesystem. The application can find and load such files via the familiar "File | Open..." paradigm, at which point it typically maintains the entire knowledge structure in memory.

The advantage of such a paradigm is familiarity and ease of use; the disadvantage is a possibly negative influence on knowledge formulation. Users must choose one of two basic strategies: either store all of their knowledge in a single file; or else break up their knowledge and store it across a number of different files, presumably according to subject matter and/or time period. The first choice can result in scalability problems—consider how much knowledge a user might collect over a decade, if they stored things related to their personal life, hobbies, relationships, reading materials, vacations, academic course notes, multiple work-related projects, future planning, etc. It seems unrealistic to keep adding this kind of volume to a single, ever-growing multi-gigabyte file. The other option, however, is also constraining: each bit of knowledge can be stored in only one of the files (or else redundantly, which leads to synchronization problems), and the user is forced to choose this at knowledge capture time.

=== Database-based ===

If a PKB's data is stored in a database system, then knowledge elements reside in a global space, which allows any idea to relate to any other: now a user can relate a book he read on productivity not only to other books on productivity, but also to "that hotel in Orlando that our family stayed in last spring," because that is where he remembers having read the book. Though such a relationship may seem "out of bounds" in traditional knowledge organization, it is exactly the kind of retrieval path that humans often employ in retrieving memories.<ref name="Lorayne and Lucas 1974" /><ref name="Anderson 1990" /><ref name="Conway et al 1991" /> The database architecture enables a PKB to truly form an integrated knowledge base, and contain the full range of relationships.

Agenda<ref name="Kaplan et al 1990" /> and [[gIBIS]]<ref name="Conklin and Begeman 1988"/> were two early tools that subsumed a database backend in their architecture. More recently, the MyLifeBits project<ref name="Gemmell et al 2002" /> uses Microsoft SQL Server as its storage layer, and [[Compendium (software)|Compendium]] interfaces with the open source MySQL database.  A few note-taking applications also store information in an integrated database rather than in user-named files. The only significant drawback to this architectural choice (other than the modest footprint of the database management system) is that data is more difficult to copy and share across systems.  This is one true advantage of files: it is a simple matter to copy them across a network, or include them as an e-mail attachment, where they can be read by the same application on a different machine. This problem is solved by some of the following architectural choices.

=== Client–server ===

Decoupling the actual knowledge store from the PKB user interface can achieve architectural flexibility. As with all client-server architectures, the benefits include load distribution, platform interoperability, data sharing, and ubiquitous availability.  Increased complexity and latency are among the liabilities, which can indeed be considerable factors in PKB design.

One of the earliest and best examples of a client-server knowledge base was the Neptune hypertext system.<ref name="Delisle and Schwartz 1986" /> Neptune was tailored to the task of maintaining shared information within software engineering teams, rather than to personal knowledge storage, but the elegant implementation of its "Hypertext Abstract Machine" (HAM) was a significant and relevant achievement. The HAM was a generic hypertext storage layer that provided node and link storage and maintained version history of all changes. Application layers and user interfaces were to be built on top of the HAM. Architecturally, the HAM provided distributed network access so that client applications could run from remote locations and still access the central store. Another, more recent example, is the Scholarly Ontologies Project<ref name="Uren et al  2004" /><ref name="Sereno et al 2005" /> whose ClaiMapper and ClaiMaker components form a similar distributed solution in order to support collaboration.

These systems implemented a distributed architecture primarily in order to share data among colleagues. For PKBs, the prime motive is rather user mobility. This is a key consideration, since if a user is to store all of their knowledge into a single integrated store, they will certainly need access to it in a variety of settings.  MyBase Networking Edition is one example of how this might be achieved. A central server hosts the user's data, and allows network access from any client machine. Clients can view the knowledge base from within the MyBase application, or through a Web browser (with limited functionality.)

The Haystack project<ref name="Adar et al 1999" /> outlines a three-tiered architecture, which allows the persistent store, the Haystack data model itself, and the clients that access it to reside on separate machines. The interface to the middle tier is flexible enough that a number of different persistent storage models can be used, including relational databases, semistructured databases, and object-oriented databases. Presto's architecture<ref name="Dourish et al 1999" /> exhibits similar features.

==== Web-based ====

A variation of the client-server approach is Web-based systems, in which the client system consists of nothing but a (possibly enhanced) browser. This gives the same ubiquitous availability that client-server approaches do, while minimizing (or eliminating) the setup and installation required on each client machine.

KMap<ref name="Gaines and Shaw 1995" /> was one of the first knowledge systems to integrate with the World Wide Web. It allowed concept maps to be shared, edited, and remotely stored using the HTTP protocol. Concept maps were still created using a standalone client application for the Macintosh, but they could be uploaded to a central server, and then rendered in browsers as "clickable GIFs". Clicking on a concept within the map image in the browser window would have the same navigation effect as clicking on it locally inside the client application.  The user's knowledge expressions are stored on a central server in nearly all cases, rather than locally on the browser's machine.

=== Handheld devices ===

Lastly, mobile devices are a possible PKB architecture. Storing all of one's personal knowledge on a PDA would solve the availability problem, of course, and even more completely than would a client-server or web-based architecture.  The safety of the information is an issue, since if the device were to be lost or destroyed, the user could face irrevocable data loss; this is easily remedied, however, by periodically synchronizing the device's contents with a host computer.

Most handheld applications are simple note-taking software, with far fewer features than their desktop counterparts. BugMe! is an immensely popular note-taking tool that simply lets users enter text or scribble onto "notes" (screenfulls of space) and then organize them in primitive ways. Screen shots can be captured and included as graphics, and the tool features an array of drawing tools, clip art libraries, etc. The value add for this and similar tools is purely the size and convenience of the handheld device, not the ability to manage large amounts of information.

Perhaps the most effective use of a handheld architecture would be as a satellite data capture and retrieval utility. A user would normally employ a fully functional desktop application for personal knowledge management, but when "on the go," they could capture knowledge into a compatible handheld application and upload it to their PKB at a later convenient time. To enable mobile knowledge retrieval, either select information would need to be downloaded to the device before the user needed it, or else a wireless client-server solution could deliver any part of the PKB on demand. This is essentially the approach taken by software like KeySuite, which supplements a feature-rich desktop information management tool (e.g. [[Microsoft outlook|Microsoft Outlook]]) by providing access to that information on the mobile device.

== See also ==
* [[Commonplace book]]
* [[Lifelog]]
* [[Notetaking]]
** [[Comparison of notetaking software]]
* [[Outliner]]
* [[Personal knowledge management]]
* [[Personal wiki]]
** {{section link|List of wiki software|Personal wiki software}}
* {{section link|Tag (metadata)|Knowledge tags}}

== Notes ==
{{notelist}}

== References ==
{{reflist|30em|
refs=
<!-- Converted to LDR format
     using [[User:PleaseStand/References segregator]] -->

<ref name = "Davies 2005">Davies, S., Velez-Morales, J. and King, R. [http://www.cs.colorado.edu/department/publications/reports/docs/CU-CS-997-05.pdf Building the Memex sixty years later: trends and directions in personal knowledge bases]. Technical Report CU-CS-997-05. Boulder, Colorado: Department of Computer Science, University of Colorado at Boulder, August 2005.</ref>

<ref name = "Davies 2011">Davies, S. Still Building the Memex. ''Communications of the ACM'', vol. 53, issue 2, February 2011, 80-88.</ref>

<ref name = "Brooks 1985">Brooks, T. New technologies and their implications for local area networks. ''Computer Communications'', vol. 8, no. 2, 1985, 82-87.</ref>

<ref name = "Kruger 1986">Krüger, G. Future information technology—motor of the "information society". in ''Employment and the Transfer of Technology''. Berlin: Springer, 1986, 39-52.</ref>

<ref name = "Forman 1988">Forman, G. Making intuitive knowledge explicit through future technology. in ''Constructivism in the Computer Age''. Hillsdale, New Jersey: L. Erlbaum, 1988, 83-101.</ref>

<ref name = "Smith 1991">Smith, C.F. Reconceiving hypertext. in ''Evolving Perspectives on Computers and Composition Studies: Questions for the 1990s''. Urbana, Illinois: National Council of Teachers of English, 1991, 224-260.</ref>

<ref name="Schneiderman 1987">Schneiderman, B., User interface design for the Hyperties electronic encyclopedia. in ''Proceedings of the ACM Conference on Hypertext''. Chapel Hill, North Carolina, 1987, 189-194.</ref>

<ref name = "Nelson 1987">Nelson, T.H. ''Literary machines: the report on, and of, Project Xanadu concerning word processing, electronic publishing, hypertext, thinkertoys, tomorrow's intellectual revolution, and certain other topics including knowledge, education and freedom''. Swarthmore, Pennsylvania: Theodor H. Nelson, 1987.</ref>

<ref name="Dumais et al 2003">Dumais, S.T., Cutrell, E., Cadiz, J., Jancke, G., Sarin, R. and Robbins, D.C. Stuff I've Seen: a system for personal information retrieval and re-use. in ''Proceedings of the 26th annual international ACM SIGIR conference on Research and development in information retrieval''. Toronto, Canada, 2003, 72-79.</ref>

<ref name="Nakakoji et al 2000">Nakakoji, K., Yamamoto, Y., Takada, S. and Reeves, B.N. Two-dimensional spatial positioning as a means for reflection in design. in ''Proceedings of the Conference on Designing Interactive Systems: Processes, Practices, Methods, and Techniques''. New York: ACM, 2000, 145-154.</ref>

<ref name="Smith et al 1987">Smith, J.B., Weiss, S.F. and Ferguson, G.J. A hypertext writing environment and its cognitive basis. in ''Proceedings of the ACM Conference on Hypertext''. Chapel Hill, North Carolina, 1987, 195-214.</ref>

<ref name="Godwin-Jones 2003">Godwin-Jones, B. Blogs and wikis: environments for on-line collaboration. ''Language Learning and Technology'', vol. 7, no. 2 (May 2003), 12-16.</ref>

<ref name="Kovalainen et al 1998">Kovalainen, M., Robinson, M. and Auramaki, E. Diaries at work. in ''Proceedings of the 1998 ACM Conference on Computer Supported Collaborative Work'', Seattle, Washington, 1998, 49-58.</ref>

<ref name="Blandford and Green 2001">Blandford, A.E. and [[Thomas R.G. Green|Green, T.R.G.]]. Group and individual time management tools: what you get is not what you need. ''Personal and Ubiquitous Computing'', vol. 5, no. 4 (December 2001), 213-230.</ref>

<ref name="Palen 1999">Palen, L. Social, individual and technological issues for groupware calendar systems. In ''Proceedings of the SIGCHI conference on Human Factors in Computing Systems'', pp. 17-24. ACM, 1999.</ref>

<ref name="Bush 1945">Bush, V. As we may think. ''The Atlantic Monthly'', July 1945, 101-108.</ref>

<ref name="Novak 2003">Novak, J.D. The theory underlying concept maps and how to construct them. Institute for Human and Machine Cognition, University of West Florida, 2003.</ref>

<ref name="Ausubel 1968">Ausubel, D.P. ''Educational Psychology: A Cognitive View''. New York: Holt, Rinehart, and Winston, 1968</ref>

<ref name="Canas et al 2005">Cañas, A.J., Hill, G., Carff, R., Suri, N., Lott, J., Gomez, G., Eskridge, T.C., Arroyo, M. and Carvajal, R. CmapTools: a knowledge modeling and sharing environment. in ''Proceedings of the First International Conference on Concept Mapping'', Pamplona, Spain, 2005, 125-133</ref>

<ref name="Woods 1985">Woods, W.A. What's in a Link: Foundations for Semantic Networks. in Brachman, R.J. and Levesque, J. eds. ''Readings in Knowledge Representation'', Morgan Kaufmann, 1985.</ref>

<ref name="Goodman 1988">Goodman, D. ''The Complete Hypercard Handbook''. New York: Bantam Books, 1988.</ref>

<ref name="Garrett et al 1986">Garrett, L.N., Smith, K.E. and Meyrowitz, N. Intermedia: Issues, strategies, and tactics in the design of a hypermedia document system. in ''Proceedings of the Conference on Computer-Supported Cooperative Work'', 1986, 163-174.</ref>

<ref name="Davis et al 1993">Davis, H., Hall, W., Heath, I., Hill, G. and Wilkins, R. MICROCOSM: an open hypermedia environment for information integration. in ''Proceedings of the INTERCHI Conference on Human Factors in Computing Systems''. ACM Press, 1993.</ref>

<ref name="Pearl 1989">Pearl, A. Sun's Link Service: a protocol for open linking. in ''Proceedings of the Second Annual ACM Conference on Hypertext'', Pittsburgh, Pennsylvania, 1989, 137-146.</ref>

<ref name="Engelbart 1953">Engelbart, D.C. A conceptual framework for the augmentation of man's intellect. in Howerton, P.W. ed. ''Vistas in Information Handling'', Spartan Books, Washington, D.C., 1963, 1-29.</ref>

<ref name="Trigg and Weiser 1986">Trigg, R.H. and Weiser, M. TEXTNET: a network-based approach to text handling. ''ACM Transactions on Information Systems'', vol. 4, no. 1, 1986, 1-23.</ref>

<ref name="Halasz et al 1987">Halasz, F.G., Moran, T.P. and Trigg, R.H. NoteCards in a Nutshell. ''ACM SIGCHI Bulletin'', 17, 1986, 45-52.</ref>

<ref name="Conklin and Begeman 1988">Conklin, J. and Begeman, M.L. gIBIS: a hypertext tool for exploratory policy discussion. in ''Proceedings of the 1988 ACM Conference on Computer-supported Cooperative Work'', Portland, Oregon, 1988, 140-152.</ref>

<ref name="Dede and Jayaram 1990">Dede, C.J. and Jayaram, G. Designing a training tool for imaging mental models. Air Force Human Resources Laboratory, Brooks Air Force Base, Texas, 1990.</ref>

<ref name="Marshall et al 1991">Marshall, C., Halasz, F.G., Rogers, R.A. and Janssen, W.C. Aquanet: a hypertext took to hold your knowledge in place. in ''Proceedings of the Third Annual ACM Conference on Hypertext'', San Antonio, Texas, 1991, 261-275.</ref>

<ref name="Carlson and Ram 1990">Carlson, D.A. and Ram, S. HyperIntelligence: the next frontier. ''Communications of the ACM'', vol. 33, no. 3, 1990, 311-321.</ref>

<ref name="Bernstein 2003">Bernstein, M. Collages, composites, construction. in ''Proceedings of the Fourteenth ACM Conference on Hypertext and Hypermedia'', Nottingham, UK, August 2003, 121-123.</ref>

<ref name="Kaplan et al 1990">Kaplan, S.J., Kapor, M.D., Belove, E.J., Landsman, R.A. and Drake, T.R. Agenda: a personal information manager. ''Communications of the ACM'', vol. 33, no. 7, 1990, 105-116.</ref>

<ref name="Burger et al 1991">Burger, A.M., Meyer, B.D., Jung, C.P. and Long, K.B. The virtual notebook system. in ''Proceedings of the Third Annual ACM Conference on Hypertext'', San Antonio, Texas, 1991, 395-401.</ref>

<ref name="Schraefel et al 2002">Schraefel, M.C., Zhu, Y., Modjeska, D., Widgdor, D. and Zhao, S. Hunter Gatherer: interaction support for the creation and management of within-Webpage collections. in ''Proceedings of the Eleventh International Conference on the World Wide Web'', 2002, 172-181.</ref>

<ref name="Dourish et al 1999">Dourish, P., Edwards, W.K., LaMarca, A. and Salisbury, M. Presto: an experimental architecture for fluid interactive document spaces. ''ACM Transactions on Computer-Human Interaction'', 6, 2, 133-161.</ref>

<ref name="Jones 1986">Jones, W.P. The Memory Extender personal filing system. in ''Proceedings of the SIGCHI Conference on Human Factors in Computing Systems'', Boston, Massachusetts, 1986, 298-305.</ref>

<ref name="Adar et al 1999">Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in ''Proceedings of the Eighth International Conference on Information Knowledge Management'', Kansas City, Missouri, 1999, 413-422.</ref>

<ref name="Wolber et al 2002">Wolber, D., Kepe, M. and Ranitovic, I. Exposing document context in the personal web. in Proceedings of the 7th International Conference on Intelligent User Interfaces, San Francisco, California, 2002, 151-158.</ref>

<ref name="Hendry and Harper 1997">Hendry, D.G. and Harper, D.J. An informal information-seeking environment. ''Journal of the American Society for Information Science'', vol. 48, no. 11, 1997, 1036-1048.</ref>

<ref name="Cousins et al 1997">Cousins, S.B., Paepcke, A., Winograd, T., Bier, E.A. and Pier, K. The digital library integrated task environment (DLITE). in ''Proceedings of the Second ACM International Conference on Digital Libraries'', Philadelphia, Pennsylvania, 1997, 142-151.</ref>

<ref name="Buchanan et al 2004">Buchanan, G., Blandford, A.E., Thimbleby, H. and Jones, M. Integrating information seeking and structuring: exploring the role of spatial hypertext in a digital library. in ''Proceedings of the Fifteenth ACM Conference on Hypertext and Hypermedia'', Santa Cruz, California, 2004, 225-234.</ref>

<ref name="Marshall and Shipman 1995">Marshall, C. and Shipman, F. Spatial hypertext: designing for change. ''Communications of the ACM'', vol. 38, no. 8, 1995, 88-97.</ref>

<ref name="Furnas and Rauch 1998">Furnas, G.W. and Rauch, S.J. Considerations for information environments and the NaviQue workspace. in ''Proceedings of the ACM Conference on Digital Libraries'', 1998, 79-88.</ref>

<ref name="Renda and Straccia 2005">Renda, M.E. and Straccia, U. A personalized collaborative digital library environment: a model and an application. ''Information Processing and Management: an International Journal'', vol. 41, no. 1, 2005, 5-21.</ref>

<ref name="Hayes et al 2003">Hayes, G., Pierce, J.S. and Abowd, G.D. Practices for capturing short important thoughts. in ''CHI '03 Extended Abstracts on Human Factors in Computing Systems'', Ft. Lauderdale, Florida, 2003, 904-905.</ref>

<ref name="Reyes-Farfan and Sanchez 2003">Reyes-Farfan, N. and Sanchez, J.A. Personal spaces in the context of OAI. in ''Proceedings of the Third ACM/IEEE-CS Joint Conference on Digital Libraries'', 2003, 182-183.</ref>

<ref name="Halasz and Schwartz 1994">Halasz, F.G. and Schwartz, M. The Dexter hypertext reference model. ''Communications of the ACM'', vol. 37, no. 2, February 1994, 30-39.</ref>

<ref name="Adar et al 1999">Adar, E., Karger, D. and Stein, L.A. Haystack: per-user information environments. in ''Proceedings of the Eighth International Conference on Information Knowledge Management'', Kansas City, Missouri, 1999, 413-422.</ref>

<ref name="Gaines and Shaw 1995">Gaines, B.R. and Shaw, M.L.G. Concept maps as hypermedia components. ''International Journal of Human Computer Studies'', vol. 43, no. 3, 1995, 323-361.</ref>

<ref name="Quillian 1968">Quillian, M.R. Semantic memory. in ''Semantic Information Processing'', Cambridge, Massachusetts: MIT Press, 1968, 227-270.</ref>

<ref name="Nosek and Roth 1990">Nosek, J.T. and Roth, I. A comparison of formal knowledge representationschemes as communication tools: predicate logic vs semantic network. ''International Journal of Man-Machine Studies'', vol. 33, no. 2, 1990, 227-239.</ref>

<ref name="Delisle and Schwartz 1986">Delisle, N. and Schwartz, M. Neptune: a hypertext system for CAD applications. in ''Proceedings of the 1986 ACM SIGMOD International Conference on Management of Data'', Washington, D.C., 1986, 132-143.</ref>

<ref name="Mantei 1982">Mantei, M.M. ''Disorientation behavior in person–computer interaction''. Ph.D. thesis. Communications Department, University of Southern California, 1982.</ref>

<ref name="Feiner 1988">Feiner, S. Seeing the forest for the trees: hierarchical display of hypertext structure. in ''Proceedings of the ACM SIGOIS and IEEECS TC-OA 1988 Conference on Office Information Systems'', Palo Alto, California, 1988, 205-212.</ref>

<ref name="Koy 1997">Koy, A.K. Computer aided thinking. in ''Proceedings of the 7th International Conference on Thinking'', Singapore, 1997.</ref>

<ref name="Selvin 1999">Selvin, A.M. Supporting collaborative analysis and design with hypertext functionality. ''Journal of Digital Information'', vol. 1, no. 4, 1999.</ref>

<ref name="Di Giacomo et al 2001">Di Giacomo, M., Mahoney, D., Bollen, J., Monroy-Hernandez, A. and Meraz, C.M.R. MyLibrary, a personalization service for digital library environments. In ''DELOS Workshop: Personalisation and Recommender Systems in Digital Libraries'', June 2001.</ref>

<ref name="Akscyn et al 1987">Akscyn, R., McCracken, D. and Yoder, E. KMS: a distributed hypermedia system for managing knowledge in organizations. in ''Proceedings of the ACM Conference on Hypertext'', Chapel Hill, North Carolina, 1987.</ref>

<ref name="Fertig et al 1996">Fertig, S., Freeman, E. and Gelernter, D. Lifestreams: An alternative to the desktop metaphor. in ''Proceedings of the Conference on Human Factors in Computing Systems (CHI96)'', Vancouver, British Columbia, 1996, 410-411.</ref>

<ref name="Freeman and Gelernter 1996">Freeman, E. and Gelernter, D. Lifestreams: a storage model for personal data. ACM SIGMOD Record, 25, 1.(March 1996), 80-86.</ref>

<ref name="Gemmell et al 2002">Gemmell, J., Bell, G., Lueder, R., Drucker, S. and Wong, C. MyLifebits: Fulfilling the Memex vision. in ''Proceedings of the 2002 ACM Workshops on Multimedia'', 2002, 235-238.</ref>

<ref name="Shipman et al 2000">Shipman, F., Hsieh, H. and Airhart, R. Analytic workspaces: supporting the emergence of interpretation in the Visual Knowledge Builder. Department of Computer Science and Center for the Study of Digital Libraries, Texas A&M University, 2000.</ref>

<ref name="diSessa and Abelson 1986">diSessa, A.A. and Abelson, H. Boxer: a reconstructible computational medium. ''Communications of the ACM'', vol. 29, no. 9, 1986, 859-868.</ref>

<ref name="Anderson 1990">Anderson, J.R. ''Cognitive Psychology and Its Implications'', 3rd Ed. New York: W.H. Freeman, 1990.</ref>

<ref name="Lorayne and Lucas 1974">Lorayne, H. and Lucas, J. ''The Memory Book''. New York: Stein and Day, 1974.</ref>

<ref name="Conway et al 1991">Conway, M.A., Kahney, H., Bruce, K. and Duce, H. Imaging objects, routines, and locations. in Logie, R.H. and Denis, M. eds. ''Mental Images in Human Cognition'', New York: Elsevier Science Publishing, 1991, 171-182.</ref>

<ref name="Uren et al 2004">Uren, V., Buckingham Shum, S., Li, G. and Bachler, M. Sensemaking tools for understanding research literatures: design, implementation, and user evaluation. Knowledge Media Institute, The Open University, 2004, 1-42.</ref>

<ref name="Sereno et al 2005">Sereno, B., Buckingham Shum, S. and Motta, E. ClaimSpotter: an environment to support sensemaking with knowledge triples. in ''Proceedings of the International Conference on Intelligent User Interfaces'', San Diego, California, 2005, 1999-1206.</ref>

<ref name="Perlin and Fox 1993">Perlin, K. and Fox, D. Pad: an alternative approach to the computer interface. in ''Proceedings of the 20th annual conference on computer graphics and interactive techniques'', 1993, 57-64.</ref>
}}

{{Computable knowledge}}

[[Category:Knowledge representation]]
<=====doc_Id=====>:843
<=====title=====>:
Social History and Industrial Classification
<=====text=====>:
{{Underlinked|date=April 2014}}

'''Social History and Industrial Classification''' (SHIC) is a classification system used by many British museums for social history and industrial collections.
It was first published in 1983.<ref>{{cite web|title=SHIC Home|url=http://www.holm.demon.co.uk/shic/}}</ref>

==Purpose==
SHIC was classifies materials (books, objects, recordings etc.) by their interaction with the people who used them. For example, a carpenter's hammer is classified with other tools of the carpenter, and not with a blacksmith's hammer.<ref>{{cite web|title=SHIC Section A|url=http://www.holm.demon.co.uk/shic/shicint.htm}}</ref> In contrast other classification systems, for example the [[Dewey Decimal Classification]], might class all hammers together and close to the classification for other percussive tools. The specialist subject network, Social History Curator's Group (SHCG), obtained funding in 2012 to develop an on-line version, now on their website http://www.shcg.org.uk/<ref>{{cite web|title=Social History Curators' Group - SHCG|url=http://www.shcg.org.uk/About-SHIC|accessdate=29 October 2012}}</ref>

==Scheme==
Materials are classified under four major category numbers:
#Community life
#Domestic and family life
#Personal life
#Working life
 
Further classification within a category is by the use of further numbers after the decimal point.<ref>{{cite web|title=SHIC Section B|url=http://www.holm.demon.co.uk/shic/shicint.htm}}</ref> 

It is permissible to assign more than one classification in cases where the object had more than one use.<ref>{{cite web|title=SHIC Section F |url=http://www.holm.demon.co.uk/shic/shicint.htm}}</ref>

==References==
{{reflist}}
*''Social history and industrial classification (SHIC), a subject classification for museum collections'', University of Sheffield, 1983

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Social history]]
[[Category:Industrial history]]
<=====doc_Id=====>:846
<=====title=====>:
Library classification
<=====text=====>:
{{refimprove|date=March 2012}}
[[Image:HK Wan Chai Library Inside Bookcase a.jpg|thumb|A library book shelf in [[Hong Kong]] arranged using the [[Dewey Decimal Classification|Dewey classification]]]]

A '''library classification''' is a [[system]] by which library resources are arranged according to subject. Library classifications use a notational system that represents the order of topics in the classification and allows items to be stored in that order. Library classification systems group related materials together, typically arranged in a hierarchical tree structure. A different kind of classification system, called a [[faceted classification]] system, is also widely used which allows the assignment of multiple classifications to an object, enabling the classifications to be ordered in multiple ways. The library classification numbers can be considered identifiers for resources but are distinct from the [[International Standard Book Number]] (ISBN) or [[International Standard Serial Number]] (ISSN) system.

== Description ==
Library classification is an aspect of [[library and information science]]. It is distinct from [[taxonomy (general)|scientific classification]] in that it has as its goal to provide a useful ordering of documents rather than a theoretical organization of [[knowledge]].<ref>{{Citation
 | first =Ganesh
 | last =Bhattacharya
 | first2 =S R
 | last2 =Ranganathan
 | author2-link=S R Ranganathan
 | editor-last =Wojciechowski
 | editor-first =Jerzy A.
 | title =From knowledge classification to library classification
 | series =Ottawa Conference on the Conceptual Basis of the Classification of Knowledge, 1971
 | year =1974
 | pages =119–143
 | place =Munich
 | publisher =Verlag Dokumentation
}}</ref> Although it has the practical purpose of creating a physical ordering of documents, it does generally attempt to adhere to accepted scientific knowledge.<ref>{{cite book
 | last = Bliss
 | first = Henry Evelyn
 | title = The organization of knowledge in libraries
 | publisher = H. W. Wilson
 | location = New Yorka
 | date = 1933
}}</ref>

Library classification is distinct from the application of [[Index term|subject headings]] in that classification organizes knowledge into a systematic order, while subject headings provide access to intellectual materials through vocabulary terms that may or may not be organized as a knowledge system.<ref name=chan >{{Citation
 |publisher = The Scarecrow Press, Inc.
 |isbn = 9780810859449
 |title = Cataloging and classification
 |url = http://openlibrary.org/books/OL9558667M/Cataloging_and_Classification
 |author = Lois Mai Chan
 |edition = Cataloging and Classification
 |publication-date = September 28, 2007
 |id = 0810859440
 }}</ref>

==History==


Library classifications were preceded by classifications used by bibliographers such as [[Conrad Gessner]]. The earliest library classification schemes organized books in broad subject categories. The increase in available printed materials made such broad classification unworkable, and more granular classifications for library materials had to be developed in the nineteenth century.<ref name=shera>{{cite book|last1=Shera|first1=Jesse H|title=Libraries and the organization of knowledge|date=1965|publisher=Archon Books|location=Hamden, Conn.}}</ref>

Although libraries created order within their collections from as early as the fifth century B.C.,<ref name=shera /> the Paris Bookseller's classification, developed in 1842 by [[Jacques Charles Brunet]], is generally seen as the first of the modern book classifications. Brunet provided five major classes: theology, jurisprudence, sciences and arts, belles-lettres, and history.<ref name=sayers>{{cite book|last1=Sayers|first1=Berwick|title=An introduction to library classification|date=1918|publisher=H. W. Wilson|location=New York}}</ref>

==Types== 
There are many standard systems of library classification in use, and many more have been proposed over the years. However, in general, classification systems can be divided into three types depending on how they are used:

* '''Universal schemes''' which cover all subjects, for example the [[Dewey Decimal Classification]], [[Universal Decimal Classification]] and [[Library of Congress Classification]]
* '''Specific classification schemes''' which cover particular subjects or types of materials, for example Iconclass, [[British Catalogue of Music Classification]], and [[Dickinson classification]], or the [[NLM Classification]] for medicine. 
* '''National schemes''' which are specially created for certain countries, for example the [[Sweden|Swedish]] library classification system, SAB (Sveriges Allmänna Biblioteksförening).

In terms of functionality, classification systems are often described as:

*'''[[Enumeration|enumerative]]''': subject headings are listed alphabetically, with numbers assigned to each heading in alphabetical order.
*'''[[Hierarchy|hierarchical]]''': subjects are divided hierarchically, from most general to most specific.
*'''[[Faceted classification|faceted]]''' or '''analytico-synthetic''': subjects are divided into mutually exclusive orthogonal facets.

There are few completely enumerative systems or faceted systems; most systems are a blend but favouring one type or the other. The most common classification systems, LCC and DDC, are essentially enumerative, though with some hierarchical and faceted elements (more so for DDC), especially at the broadest and most general level. The first true faceted system was the [[Colon classification]] of [[S. R. Ranganathan]].

==Methods or Systems==

Classification types denote the classification or categorization according the form or characteristics or qualities of a classification scheme or schemes. Method and system has similar meaning. Method or methods or system means the classification schemes like Dewey Decimal Classification or Universal Decimal Classification. The types of classification is for identifying and understanding or education or research purposes while classification method means those classification schemes like DDC, UDC.
 
===English language universal classification systems===
The most common systems in [[English language|English]]-speaking countries are:
* [[Dewey Decimal Classification]] (DDC)
* [[Library of Congress Classification]] (LCC)
* [[Colon classification]] (CC)
* [[Universal Decimal Classification]] (UDC)

Other systems include:
* [[Harvard-Yenching Classification]], an English classification system for Chinese language materials.
* V-LIB 1.2 (2008 Vartavan Library Classification for over 700 fields of knowledge, currently sold under license in the UK by Rosecastle Ltd. (see http://rosecastle.atspace.com/index_files/VartavanLibrary.htm)).

===Non-English universal classification systems===
* A system of book classification for Chinese libraries (Liu's Classification) library classification for user
** [[New Classification Scheme for Chinese Libraries]]
* [[Nippon Decimal Classification]] (NDC)
* [[Chinese Library Classification]] (CLC)
* [[Korean Decimal Classification]] (KDC)
* Russian [[:ru:Библиотечно-библиографическая классификация|Library-Bibliographical Classification]] (BBK)

===Universal classification systems that rely on synthesis (faceted systems)===
* [[Bliss bibliographic classification]]
* [[Colon classification]]
* [[Cutter Expansive Classification]]
* [[Universal Decimal Classification]]

Newer classification systems tend to use the principle of synthesis (combining codes from different lists to represent the different attributes of a work) heavily, which is comparatively lacking in LC or DDC.

==The practice of classifying==

Library classification is associated with library (descriptive) cataloging under the rubric of ''cataloging and classification'', sometimes grouped together as ''technical services''. The library professional who engages in the process of cataloging and classifying library materials is called a ''cataloger'' or ''catalog librarian''. Library classification systems are one of the two tools used to facilitate [[subject access]].  The other consists of alphabetical indexing languages such as Thesauri and Subject Headings systems.

Library classification of a piece of work consists of two steps. Firstly, the "aboutness" of the material is ascertained. Next, a call number (essentially a book's address) based on the classification system in use at the particular library will be assigned to the work using the notation of the system.

It is important to note that unlike subject heading or thesauri where multiple terms can be assigned to the same work, in library classification systems, each work can only be placed in one class. This is due to shelving purposes: A book can have only one physical place. However, in classified catalogs one may have main entries as well as added entries. Most classification systems like the [[Dewey Decimal Classification]] (DDC) and [[Library of Congress Classification]] also add a [[cutter number]] to each work which adds a code for the author of the work.

Classification systems in libraries generally play two roles. Firstly, they facilitate [[subject access]] by allowing the user to find out what works or documents the library has on a certain subject.<ref>{{cite web|url=http://www.iva.dk/bh/lifeboat_ko/concepts/subject_access_points.htm|title=Subject access points|work=iva.dk}}</ref> Secondly, they provide a known location for the information source to be located (e.g. where it is shelved).

Until the 19th century, most libraries had closed stacks, so the library classification only served to organize the subject [[library catalog|catalog]]. In the 20th century, libraries opened their stacks to the public and started to shelve library material itself according to some library classification to simplify subject browsing.

Some classification systems are more suitable for aiding subject access, rather than for shelf location. For example, [[Universal Decimal Classification]], which uses a complicated notation of pluses and colons, is more difficult to use for the purpose of shelf arrangement but is more expressive compared to DDC in terms of showing relationships between subjects. Similarly [[faceted classification]] schemes are more difficult to use for shelf arrangement, unless the user has knowledge of the citation order.

Depending on the size of the library collection, some libraries might use classification systems solely for one purpose or the other. In extreme cases, a public library with a small collection might just use a classification system for location of resources but might not use a complicated subject classification system. Instead all resources might just be put into a couple of wide classes (travel, crime, magazines etc.). This is known as a "mark and park" classification method, more formally called reader interest classification.<ref>Lynch, Sarah N., and Eugene Mulero. [http://www.nytimes.com/2007/07/14/us/14dewey.html "Dewey? At This Library With a Very Different Outlook, They Don't"] ''[[The New York Times]]'', July 14, 2007.</ref>

== Comparing classification systems ==
As a result of differences in notation, history, use of enumeration, hierarchy, and facets, classification systems can differ in the following ways:
* Type of Notation: Notation can be pure (consisting of only numerals, for example) or mixed (consisting of letters and numerals, or letters, numerals, and other symbols). 
* Expressiveness: This is the degree to which the notation can express relationship between concepts or structure.
* Whether they support mnemonics: For example, the number 44 in DDC notation often means it concerns some aspect of France. For example, in the Dewey classification 598.0944 concerns "Birds in France", the 09 signifies geographical division, and 44 represents France.
* Hospitality: The degree to which the system is able to accommodate new subjects.
* Brevity: The length of the notation to express the same concept.
* Speed of updates and degree of support: The better classification systems are frequently being reviewed.
* Consistency 
* Simplicity
* Usability

== See also ==<!-- PLEASE RESPECT ALPHABETICAL ORDER -->
{{Wikipedia books}}
* [[Attribute-value system]]
* [[Categorization]]
* [[Document classification]]
* [[Knowledge organization]]
* [[Library management]]
* [[Library of Congress Subject Headings]]

==Notes==
{{reflist}}

==References==
{{commons category|Library cataloging and classification}}
* Chan, Lois Mai. (1994)''Cataloging and Classification: An Introduction'', second ed. New York: McGraw-Hill, . ISBN 978-0-07-010506-5, ISBN 978-0-07-113253-4.

{{Library classification systems}}
{{Computable knowledge}}

{{Authority control}}

{{DEFAULTSORT:Library Classification}}
[[Category:Library cataloging and classification| ]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:849
<=====title=====>:
Thesaurus
<=====text=====>:
{{about|thesauri for general or literary applications|thesauri designed for information retrieval|Thesaurus (information retrieval)|the Clare Fischer album|Thesaurus (album)}}
[[File:Historical Thesaurus.jpg|thumb|''Historical Thesaurus of the Oxford English Dictionary'', two-volume set]]
In general usage, a '''thesaurus''' is a [[reference work]] that lists words grouped together according to similarity of meaning (containing [[synonyms]] and sometimes [[antonyms]]), in contrast to a [[dictionary]], which provides [[definitions]] for words, and generally lists them in alphabetical order. The main purpose of such reference works is to help the user "to find the word, or words, by which [an] idea may be most fitly and aptly expressed"&nbsp;– to quote [[Peter Mark Roget]], architect of the best known thesaurus in the English language.<ref name="Roget">Roget, Peter. 1852. ''Thesaurus of English Language Words and Phrases''.</ref>

Although including synonyms, a thesaurus should not be taken as a complete list of all the synonyms for a particular word. The entries are also designed for drawing distinctions between similar words and assisting in choosing exactly the right word. Unlike a [[dictionary]], a thesaurus entry does not give the definition of words.

In [[library science]] and [[information science]], thesauri have been widely used to specify domain models.  Recently, thesauri have been implemented with [[Simple Knowledge Organization System]] (SKOS).{{citation needed|date=January 2016}}

== Etymology ==
The word "thesaurus" is derived from 16th-century [[New Latin]], in turn from [[Latin]] ''[[wikt:en:thesaurus#Latin|thēsaurus]]'', which is the [[Latinisation (literature)|Latinisation]] of the [[Ancient Greek|Greek]] {{lang|grc|[[wikt:en:θησαυρός#Ancient Greek|θησαυρός]]}} (''thēsauros''), "treasure, treasury, storehouse".<ref name="Harper">[http://www.etymonline.com/index.php?term=thesaurus "thesaurus"]. ''[[Online Etymology Dictionary]]''.</ref> The word ''thēsauros'' is of uncertain etymology. [[Douglas Harper]] derives it from the root of the Greek verb τιθέναι ''tithenai'', "to put, to place."<ref name="Harper" /> [[Robert S. P. Beekes|Robert Beekes]] rejected an [[Proto-Indo-European language|Indo-European]] derivation and suggested a [[Pre-Greek]] suffix {{nowrap|''*-ar<sup>w</sup>o-''}}.<ref>[[Robert S. P. Beekes|R. S. P. Beekes]], ''Etymological Dictionary of Greek'', Brill, 2009, p. 548.</ref>

From the 16th to the 19th centuries, the term "thesaurus" was applied to any [[dictionary]] or [[encyclopedia]], as in the ''[[Thesaurus linguae latinae]]'' (1532), and the ''[[Thesaurus linguae graecae]]'' (1572). The meaning "collection of words arranged according to sense" is first attested in 1852 in Roget's title and ''thesaurer'' is attested in [[Middle English]] for "[[treasurer]]".<ref name="Harper" />

== History ==
[[File:Roget P M.jpg|150px|right|thumb|[[Peter Mark Roget]], author of the first thesaurus.]]
In antiquity, [[Philo of Byblos]] authored the first text that could now be called a thesaurus. In [[Sanskrit]], the [[Amarakosha]] is a thesaurus in verse form, written in the 4th century.

The first modern thesaurus was ''[[Roget's Thesaurus]]'', first compiled in 1805 by [[Peter Mark Roget]], and published in 1852. Since its publication it has never been out of print and is still a widely used work across the English-speaking world.<ref>http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199254729.001.0001/acprof-9780199254729-chapter-1</ref> Entries in ''Roget's Thesaurus'' are listed conceptually rather than alphabetically.
Roget described his thesaurus in the foreword to the first edition:

<blockquote>It is now nearly fifty years since I first projected a system of verbal classification similar to that on which the present work is founded. Conceiving that such a compilation might help to supply my own deficiencies, I had, in the year 1805, completed a classed catalogue of words on a small scale, but on the same principle, and nearly in the same form, as the Thesaurus now published.<ref>Lloyd 1982, p. xix{{Full citation needed|date=August 2014}}</ref>
</blockquote>

== See also ==
* [[AGRIS]]
* [[Controlled vocabulary]]
* [[Knowledge Organization Systems]]
* [[Ontology (computer science)]]
* [[Simple Knowledge Organisation System]]
* [[ISO 25964]]

== References ==
{{Reflist}}

== External links ==
* {{Wiktionary-inline|thesaurus}}

{{Lexicography}}

[[Category:Thesauri| ]]
[[Category:Information science]]
[[Category:Knowledge representation]]
[[Category:Reference works]]
[[Category:Dictionaries by type]]
[[Category:Lexical semantics]]
<=====doc_Id=====>:852
<=====title=====>:
Prezi
<=====text=====>:
{{Infobox Website
| name           = Prezi
| logo           = [[File:Prezi logo transparent 2012.svg|frameless|150px]]
| url            = {{url|https://www.prezi.com/}}
| origin         = [[Hungary]]
| founder        = [[Adam Somlai-Fischer]]<br>Peter Halacsy<br>[[Peter Arvai]]
| language       = [[English language|English]], [[Portuguese language|Portuguese]], [[Spanish language|Spanish]], [[Korean language|Korean]], [[Japanese language|Japanese]], [[German language|German]], [[Italian language|Italian]], [[French language|French]], [[Hungarian language|Hungarian]]
| type           = [[Presentation Software|Presentation]] [[Collaboration tool|Collaboration]]
| launch date    = {{start date and age|2009|4|5}}
| current status = Active
}}

'''Prezi''' is a visual storytelling software alternative to traditional slide-based presentation formats. Prezi presentations feature a map-like, schematic overview that lets users pan between topics at will, zoom in on desired details, and pull back to reveal context.

This freedom of movement enables “conversational presenting,” a new presentation style in which presentations follow the flow of dialogue, instead of vice-versa.

Founded in 2009, and with offices in San Francisco, Budapest, and Mexico City, Prezi now fosters a community of over 75 million users with more than 260 million prezis around the world.

The company launched Prezi Business in 2016, with a suite of creation, collaboration, and analytics tools for teams. Prezi Business is an HTML5 application that runs on JavaScript.

The word ''Prezi'' is the short form of “presentation” in Hungarian.

== History ==

Prezi was founded in 2008 in Budapest, Hungary by Adam Somlai-Fischer, Peter Halacsy, and Peter Arvai. 

The earliest zooming presentation prototype had been previously developed by Somlai-Fischer to showcase his media-art pieces. Halacsy, an engineer, saw one of these presentations and proposed to improve the software. They were joined by entrepreneur and future CEO Arvai with the goal of making Prezi a globally recognized SaaS company.

The company established incorporation on May 20, 2009 and received its first major investment from TED two months later. A San Francisco office was opened that December. 

Early 2011 saw the launch of Prezi’s first iPad application, followed by $14M in Series B funding led by Accel Partners. A Prezi iPhone app was launched in late 2012. 

In March of 2014, Prezi pledged $100M in free licenses to Title 1 schools as part of the Obama administration’s ConnectED program. November of that year saw the announcement of $57M in new funding from Spectrum Equity and Accel Partners. 

Prezi for Android was launched in 2015, and in June of 2016, the company launched Prezi Business. As of June 2, 2016, Prezi reports 75 million registered users and 1 billion ‘prezi’ presentation views worldwide.

== Products and features ==
[[File:Path Tool.png|thumb|Prezi Path Tool]]

=== Prezi ZUI ===
The Prezi online and offline ZUI editors employ a common tool palette, allowing users to pan and zoom, and to size, rotate, or edit an object. The user places objects on a canvas and navigates between videos, images, texts and other presentation media. Frames allow grouping of presentation media together as a single presentation object. Paths are navigational sequences that connect presentation objects for the purposes of structuring a linear presentation.

=== Prezi Desktop ===
Prezi Desktop<ref>{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-06-05}}</ref> allows Prezi Pro or Edu Pro subscribers to work off-line and create and save their presentations on their own [[Microsoft Windows|Windows]] or [[Mac OS X|Mac]] systems. Prezi Desktop Editor allows users to work on the presentation off-line in a .pez file format. Users can have files up to 500 MB in size when signing up with a school-affiliated e-mail address. This storage capability doesn't affect when users use an appropriate third-party conversion software with [[FLV]] or [[SWF]] format.<ref>{{cite web|url=http://prezi.com/desktop/ |title=Desktop |publisher=Prezi |date= |accessdate=2012-07-23}}</ref>

=== Prezi Collaborate ===
Prezi Collaborate is an online collaboration feature that allows up to ten people (co-located or geographically separated) to co-edit and show their presentations in real time. Users participate in a prezi simultaneously, and each is visually represented in the presentation window by a small avatar. Although Prezi Meetings can be done simultaneously, that is not the only option. Participants can be invited to edit the Prezi presentation at a later time if they wish. A link will be sent and the participant has up to ten days to edit the presentation. Prezi Meeting is included in all license types.

===Prezi Viewer for iPad===
Prezi Viewer<ref>{{cite web|url=http://prezi.com/ipad/ |title=Viewer for iPad |publisher=Prezi |date= |accessdate=2012-06-05}}</ref> is an app developed for the [[iPad]] for viewing prezis created on one's Prezi online account. The iPad [[touchscreen]] and [[multi-touch]] [[user interface]] enables users to pan, and pinch to zoom in or out of their media.

Prezzip also offers templates for PreziU, with tool kits and visuals for file presentations.<ref>{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/prezi-ipad-viewer/|title=Prezi iPad viewer |publisher=Prezzip |accessdate=25 July 2012}}</ref><ref>{{cite web|url=http://www.prezzip.com/index.php/footer-pages/about/what-we-do/ |title=what we do |publisher=Prezzip |date= |accessdate=2012-07-25}}</ref>

== Revenue model ==
Prezi uses the [[freemium]] model. Customers who use the product's Public license must publish their work on the Prezi.com website, which is publicly viewable. Customers who pay for a Prezi Enjoy or Prezi Pro can make their presentations private. Only Pro license users have access to Prezi Desktop, which enables offline editing. Prezi also offers an educational license for students and educators.

== Uses ==

=== Business and conferences ===
Some users at the [[World Economic Forum]] are currently using Prezi for their presentations.<ref>{{cite web|url=http://blog.prezi.com/2009/11/03/how-to-create-a-good-prezi-for-the-world-economic-forum/ |title=zoomintoprezi - Latest - How to create a good prezi - World Economic Forum |publisher=Blog.prezi.com |date=2009-11-03 |accessdate=2012-06-05}}</ref> Many [[TED (conference)|TED Conference]] speakers have used Prezi, including TED curator [[Chris Anderson (entrepreneur)|Chris Anderson]], who used a Prezi for his TEDGlobal 2010 presentation: How Web Video Powers Global Innovation.<ref>{{cite web|author= |url=https://www.youtube.com/watch?annotation_id=annotation_962757&feature=iv&src_vid=X6Zo53M0lcY&v=LnQcCgS7aPQ |title=Chris Anderson: How YouTube is driving innovation |publisher=YouTube |date=2010-09-14 |accessdate=2015-05-06}}</ref> Michael Chasen, President/CEO of [[Blackboard, Inc.]], used Prezi to deliver the keynote at their BbWorld 2011 annual users' conference.<ref>{{cite web|author= |url=https://www.youtube.com/watch?v=rlGA9_p_--c |title=BbWorld 2011 Corporate Keynote |publisher=YouTube |date=2011-07-26 |accessdate=2012-06-05}}</ref> [[FBLA]] members have recently started using this software.{{citation needed|reason=|date=September 2013}}

=== Education ===
Prezi is used at [[Oregon State University]],<ref>{{cite web|author= not fuly true. should expand further|url=http://calendar.oregonstate.edu/event/63614 |title=Prezi in the  classroom |publisher=Oregon University State University calendar |date= |accessdate=2012-07-24}}</ref> as well as at the [[Dwight School]]<ref>{{cite news|last=Anderson |first=Jenny |url=http://cityroom.blogs.nytimes.com/2011/06/21/at-a-private-school-virtual-learning-and-the-rock/ |title=At Dwight School, Virtual Learning and the Rock - NYTimes.com |location=Manhattan (NYC) |publisher=Cityroom.blogs.nytimes.com |date=2011-06-21 |accessdate=2012-06-05}}</ref> and elsewhere in primary education and higher education.<ref>{{cite web|author=Zoltan Radnai|url=http://edu.prezi.com/article/27827/-Prezi-makes-you-stop-and-think/|title=Prezi makes you stop and think|publisher=Prezi|accessdate=23 July 2012}}</ref> It can be used by teachers and students to collaborate on presentations with multiple users able to access and edit the same presentation,<ref>{{cite web|author=Tilt |url=https://www.youtube.com/watch?v=lZyv6MTVsjc |title=Student Web 2.0 |publisher=YouTube |date= |accessdate=2012-07-18}}</ref> and to allow students to construct and present their knowledge in different learning styles.<ref>{{cite web|url=http://www.nactateachers.org/attachments/article/1060/NACTA%20Journal%20Vol%2055%20Sup%201.pdf/|title=Thinking outside of slide |publisher=NACTA |accessdate=24 July 2012}}</ref> The product is also being used in [[e-learning]] and [[edutainment]].<ref>{{cite web|author= |url=https://www.youtube.com/watch?v=s7nDT_KgPpk |title=Daniel Gallichan - 1. Platz beim 1. Freiburger Science Slam |publisher=YouTube |date= |accessdate=2012-06-05}}</ref> However note that Prezi is considered by Web2Access to be an 'inaccessible service'.<ref>{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}</ref> Educators have been advised that Prezi is not ADA/508 compliant and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.<ref>{{cite web|url=http://webaccessibility.gmu.edu/prezi.html |title=Prezi Known Accessibility Issues|publisher=George Mason University |date= |accessdate=2014-03-01}}</ref>

=== Information visualization ===
In July 2011, ''[[The Guardian]]'' used Prezi to publish a new world map graphic on their website, for an article about the newly independent South Sudan.<ref>{{cite news|author=Simon Rogers, Jenny Ridley |url=https://www.theguardian.com/news/datablog/interactive/2011/jul/08/world-map-new-south-sudan |title=The new world map: download it for yourself &#124; World news &#124; guardian.co.uk |publisher=Guardian |date= 2011-07-08|accessdate=2012-06-05 |location=London}}</ref>

== Platform compatibility ==
Prezi is developed in Adobe Flash, Adobe AIR and built on top of Django. It is compatible with most modern computers and web browsers. 

Prezi Business is an HTML5 application which runs on JavaScript. It also is compatible with most modern systems.

== Criticism ==
The company has acknowledged that the “[[zooming user interface]] (ZUI)” has the potential to induce nausea, and offers tutorials with recommendations for use of layout to avoid excessive visual stimulation.<ref>{{cite web|url=http://prezi.com/learn/grouping-and-layering/ |title=Why the Best Prezis use Grouping & Layering &#124; Prezi Learn Center |publisher=Prezi.com |date= |accessdate=2012-06-05}}</ref>
There has also been criticism of Prezi’s lack of font and color options. Notably, Presentation Zen author Garr Reynolds once stated that he had never seen a good presentation using Prezi and was looking for one;<ref>{{cite web|url=http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |title=Have you ever seen a great talk given with the help of Prezi? Do you have a link? - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-10 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080355/http://garr.posterous.com/have-you-ever-seen-a-great-talk-given-with-th |archivedate=April 2, 2012 }}
</ref> in a later post, he refers to Chris Anderson’s talk at TED Global 2010 as one of the best TED talks ever, commenting that it was a good use of Prezi.<ref>
{{cite web|url=http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |title=On train to Tokyo watching one of the best TED talks ever - Garr's posterous |publisher=Garr.posterous.com |date=2010-09-14 |accessdate=2012-06-05 |deadurl=unfit |archiveurl=https://web.archive.org/web/20120402080641/http://garr.posterous.com/on-train-to-tokyo-watching-one-of-the-best-te |archivedate=April 2, 2012 }}
</ref>

As Prezi is a Flash-based online zooming tool, most elements of the presentation cannot be read aloud by users with disabilities by means of a screen reader (e.g. it is not possible to add [[alt attribute]]s to images and [[iframe]]s used for the page design, and templates have been built to work without [[accessibility]] options). Prezi is considered by Web2Access to be an 'inaccessible service'.<ref>{{cite web|url=http://www.web2access.org.uk/product/172/ |title=Results for Prezi|publisher=Web2Access JISC TechDis |date= |accessdate=2014-03-01}}</ref> American educators have been advised that Prezi is not compliant with the Americans With Disabilities Act (ADA/508) and that an accessible PowerPoint version of the presentation should be provided online for students where a Prezi has been used.<ref>{{cite web|url=http://barrydahl.com/2015/01/08/accessibility-concerns-of-using-prezi-in-education/|title=Accessibility Concerns of Using Prezi in Education |publisher=Barry Dahl |date= |accessdate=2015-10-07}}</ref>

==See also==
* [[Scientific visualization]]
* [[Data Presentation Architecture]]

==References==
{{reflist|30em}}

==External links==
{{Commons category|Mind maps}}
* {{Official website|https://www.prezi.com}}

{{Mindmaps}}
{{Presentation software}}
{{Notetaking softwares}}

[[Category:Zoomable user interfaces]]
[[Category:Panorama viewers]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Note-taking software]]
<=====doc_Id=====>:855
<=====title=====>:
SMW+
<=====text=====>:
{{Infobox software
|name= SMW+
|screenshot=[[File:Fr Smw plus screenshot.png|300px|SMW+]]
|caption=
|developer=[http://www.diqa-pm.com/en/Main_Page DIQA-Projektmanagement GmbH]
|latest release version = 1.7.0
|latest release date = {{release date|2012|04|25}}
|discontinued = yes
|operating_system= [[Cross-platform]]
|programming language=[[PHP]]
|database=[[MySQL]]
|genre=[[MediaWiki]] extension
|license=[[GNU General Public License|GPL]]
|website=[http://semanticweb.org/wiki/SMW%2B SMW+ homepage]
}}

'''SMW+''' is an [[open source]] [[Software suite|software bundle]] composed of the [[wiki software|wiki application]] [[MediaWiki]] along with a number of its extensions, that was developed by the [[Germany|German]] software company [[Ontoprise GmbH]] from 2007 to 2012. In 2012, Ontoprise GmbH filed for bankruptcy<ref>{{cite web |url=http://www.econo.de/no_cache/nachrichten/einzelansicht/article/ontoprise-stellt-insolvenzantrag.html|title=Ontoprise stellt Insolvenzantrag |trans_title=Ontoprise starts insolvency proceedings |language=German |date=3 May 2012 |publisher=econo |accessdate=30 July 2012}}</ref> and went out of business. DIQA-Projektmanagement GmbH, a start-up founded by former Ontoprise employees,<ref>{{cite web |author=Michael Erdmann |title=SMW+ is dead, long live SMW+ |url=http://sourceforge.net/mailarchive/message.php?msg_id=29589354 |work=[[MediaWiki]] users mailing list |date=25 July 2012 |accessdate=30 July 2012}}</ref> now offers support for the software in SMW+, though under the name "[http://diqa-pm.com/en/DataWiki DataWiki]".

== Details ==

SMW+'s extensions include, most notably, [[Semantic MediaWiki]] and the [http://semanticweb.org/wiki/Halo_Extension Halo Extension]. Cumulatively, SMW+ functions as a [[semantic wiki]], and is also meant to serve as an [[enterprise wiki]] for use within companies, for applications such as [[knowledge management]] and [[project management]].

The SMW+ platform was available in a number of formats including a Windows installer, Linux installer and [[VMware]] image.

SMW+ emerged from [[Project Halo]], a research project meant to provide a platform for collaborative knowledge engineering for [[Subject matter expert|domain experts]] in the [[biology]], [[chemistry]] and [[physics]] at the first stage.

SMW+ is used by the [[Intergovernmental Oceanographic Commission]] of [[UNESCO]] to power an online encyclopedia for [[oceanography]].{{citation needed|date=April 2013}}

== References ==
{{reflist}}
* [http://smwplus.net/index.php/Business_applications_with_SMW%2B ''Business applications with SMW+, a Semantic Enterprise Wiki'']. Michael Erdmann, Daniel Hansch.
* [http://smwplus.net/index.php/Practical_applications_of_Semantic_MediaWiki_in_commercial_environments ''Practical applications of Semantic MediaWiki in commercial environments - Case Study: semantic-based project management'']. Daniel Hansch, Hans-Peter Schnurr. Presented at the ESTC 2009.
* [http://swui.webscience.org/SWUI2008CHI/Pfisterer.pdf ''User-Centered Design and Evaluation of Interface Enhancements to the Semantic MediaWiki'']. Frederik Pfisterer, Markus Nitsche, Anthony Jameson and Catalin Barbu. Presented at the CHI2008 (Computer Human Interaction Conference).
* [http://www.academypublisher.com/jetwi/vol1/no1/jetwi01019496.pdf ''Semantic Wikis: A Comprehensible Introduction with Examples from the Health Sciences'']. [[Maged N. Kamel Boulos]]. Journal of Emerging Technologies in Web Intelligence, Vol. 1, No. 1, August 2009
* [http://smwplus.net/index.php/Towards_a_Collaborative_Semantic_Wiki-based_Approach_to_IT_Service_Management ''Towards a Collaborative Semantic Wiki-based Approach to IT Service Management'']. Frank Kleiner, Andreas Abecker. Proceedings of I-SEMANTICS ’09.

==External links==
* [http://semanticweb.org/wiki/SMW%2B SMW+] on semanticweb.org
* [http://www.semanticweb.com/main/semantic_mediawiki_development_picks_up_steam_138918.asp Article at semanticweb.com about SMW+]
* [http://videolectures.net/iswc08_greaves_swfttsotsw/ "Semantic Wikis: Fusing the two strands of the Semantic Web"] - Talk given by Mark Greaves at the ISWC 2008

{{DEFAULTSORT:Smw}}
[[Category:Semantic wiki software]]
[[Category:Knowledge representation]]
[[Category:Free software programmed in PHP]]
[[Category:MediaWiki extensions]]
<=====doc_Id=====>:858
<=====title=====>:
Harvard–Yenching Classification
<=====text=====>:
[[Alfred Kaiming Chiu]] <ref>{{zh|t=裘開明 | s=裘开明 |p=Qiú Kāimíng|w=Ch'iu<sup>2</sup> K'ai<sup>1</sup>-Ming<sup>2</sup>}}.</ref> (1898–1977) was a pioneer of establishing a library classification system for Chinese language materials in the United States of America.  The system devised by him was known as '''Harvard–Yenching Classification System'''.  The system was primarily created for the classification of Chinese language materials in the [[Harvard-Yenching Library]] which was founded in 1927 at the [[Harvard-Yenching Institute]].<ref>Eugene W. Wu, "The Founding of the Harvard-Yenching Library," ''Journal of East Asian Libraries'' 101.1  (1993):  65-69.     [http://scholarsarchive.byu.edu/jeal/vol1993/iss101/16/]</ref>

During that early period other systems, such as the early edition of the [[Library of Congress Classification]], did not consist of appropriate subject headings to classify the Chinese language materials, particularly the ancient published materials.  As many American libraries started to collect the ancient and contemporary published materials from China, a number of American libraries subsequently followed Harvard University to adopt Harvard–Yenching classification system, such as the East Asian Library of the University of California in Berkeley, Columbia University, The University of Chicago, Washington University in St. Louis etc.

In addition to American libraries, the libraries of other universities in the world including England, Australia, New Zealand, Hong Kong, Singapore etc. also followed Harvard University to adopt the system.  During the period from the 1930s to the 1970s, the use of the system became popular for classifying not only Chinese language materials but also other East Asian materials including Korean and Japanese language materials.

During the period from the 1970s to the 1980s, a comprehensive subset of subject headings for Chinese language materials was gradually established in the Library of Congress Classification System so that almost a full spectrum of ancient and contemporary Chinese topics can be widely covered. As a result of this, the Library of Congress Classification System eventually replaced the Harvard–Yenching Classification System for all Chinese language materials acquired after the 1970s in many American Libraries.

Though the system has largely been phased out, the system is still being used in some libraries for Chinese language materials acquired prior to the Library of Congress update. Such previously acquired books are normally stored in separate stacks in libraries. However, some of the university libraries in the Commonwealth countries of the United Kingdom such as England, Australia and New Zealand still continue to use the Harvard-Yenching system; for example, the Institute for Chinese Studies Library of the University of Oxford, University of Sydney, University of Melbourne, and University of Auckland.

== The Harvard–Yenching classification system ==
The key classes of the system are listed as follows:

===Key classes===
** 0100–0999 Chinese Classics
** 1000–1999 Philosophy and Religion
** 2000–3999 Historical Sciences
** 4000–4999 Social Sciences
** 5000–5999 Language and Literature
** 6000–6999 Fine and Recreative Arts
** 7000–7999 Natural Sciences
** 8000–8999 Agriculture and Technology
** 9000–9999 Generalia and Bibliography

===Subjects of sub-classes===
{{Empty section|date=January 2011}}

===0100 to 0999 Chinese Classics===
** 0100–0199 Chinese classics in general
** 0200–0299 I Ching
** 0300–0399 Shu Ching
** 0400–0499 Shih Ching
** 0500–0669 San Li
** 0680–0799 Ch’un Ch’iu
** 0800–0849 Hsiao Ching
** 0850–0999 Ssu Shu

===1000 to 1999 Philosophy and Religion===
** 1000–1008 Philosophy & religion in general
** 1010–1429 Chinese philosophy
** 1470–1499 Hindu philosophy
** 1500–1539 Occident philosophy
** 1540–1569 Philosophical problems and systems
** 1570–1609 Logic
** 1610–1649 Metaphysics
** 1650–1699 Ethics
** 1700–1729 Religion in general
** 1730–1738 Mythology
** 1739–1749 Occultism numerology
** 1750–1779 History of religions
** 1780–1799 Chinese state cults
** 1800–1919 Buddhism
** 1920–1939 Taoism
** 1975–1987 Christianity
** 1988–1999 Other religions

===2000 to 3999 Historical Sciences===
** 2000–2049 Archaeology, Antiquities in general
** 2060–2159 China archaeology
** 2200–2249 Ethnology, ethnography
** 2250–2299 Genealogy and biography
** 2300–2349 World history
** 2350–2399 World geography
** 2400–2440 Asian history and geography
** 2450–2459 History of China in general
** 2461–2469 Chinese historiography
** 2470–2479 History of Chinese civilisation
** 2480–2509 Diplomatic history of China
** 2510–2519 General China history
** 2520–2533 Ancient history of China in general
** 2535 Ch’in, Han and 3 Kingdom in general
** 2536–2543 Ch’in Dynasty
** 2545–2559 Han Dynasty
** 2560–2567 The Three Kingdom
** 2570 Chin Dynasty and the Southern / Northern Dynasties
** 2571–2578 Chin Dynasty (265–420)
** 2581–2588 The Southern Dynasties
** 2590–2599 The Northern Dynasties
** 2605–2618 Sui, T’ang & the Five Dynasties in general
** 2605–2619 Sui Dynasty
** 2620–2639 T’ang Dynasty
** 2640–2649 Epoch of the Five Dynasties (North)
** 2650–2660 The Ten Kingdoms (South)
** 2662 Sung, Liao, Chin and Yuan Dynasties in general
** 2665–2684 Sung Dynasty (960–1279)
** 2685–2688 The Liao Kingdom (916–1201)
** 2690 The Chin Kingdom (1115–1234)
** 2695 The [[Hsi Hsia Kingdom]] (982–1227)
** 2700–2713 Yuan Dynasty (1280–1268)
** 2718 Ming and Ching Dynasties in general
** 2720–2739 Ming Dynasty
** 2740–2969 Ch’ing Dynasty
** 2970 Period of Republic, 1912
** 3000–3019 China: geography & history in general
** 3020–3031 General system treatises
** 3032–3049 Special works of geography: China
** 3507–3079 China: local description and travel
** 3080–3109 Maps, Atlas of China
** 3110–3299 Gazetteers of China
** 3300–3479 [[Japanese history]]
** 3400–3479 [[Geography of Japan|Japanese geography]]
** 3480–3489 [[Korean history]]
** 3490–3499 [[Hong Kong]], [[Macau]] history and geography
** 3500–3599 Other counties in Asia: history and geography
** 3600–3799 Europe: history and geography
** 3800–3899 America: history and geography
** 3900–3999 Africa, Oceania: history and geography

===4000 to 4999 Social Sciences===
** 4000–4019 Social sciences in general
** 4020–4099 Statistics
** 4100–4299 Sociology
** 4300–4599 Economics
** 4600–4899 Politics and Law
** 4900–4999 Education

===5000 to 5999 Language and Literature===
** 5000–5039 Linguistics in general
** 5040–5059 Literature in general
** 5060–5069 Chinese language in general
** 5070–5089 Semantic studies
** 5090–5119 Graphic studies
** 5120–5139 Phonological Studies
** 5140–5149 Grammar
** 5150–5159 Dialects
** 5160–5169 Texts: learning the language
** 5170–5199 Lexicography dictionaries
** 5200–5209 Chinese literature in general
** 5210–5217 Chinese literature: literary criticism
** 5218–5229 Chinese literature: history & biography
** 5230–5235 Chinese literature: collection of individual complete works
** 5236–5241 Chinese literature: general anothlogies
** 5242–5569 Collected Chinese literart works of individual authors
** 5570–5649 Tz’u
** 5650–5730 Lyrical works and drama
** 5731–5769 Chinese Fiction
** 5770–5779 Letters
** 5780–5799 Miscellany: proverbs, fables, juv. lit.
** 5800–5809 Minor languages in China
** 5810–5859 Japanese language
** 5860–5959 Japanese literature
** 5973 Korean language and literature
** 5975–5993 Indo-European language and literature
** 5994–5999 Other language and literature

===6000 to 6999 Fine and Recreative Arts===
** 6000–6019 Fine and recreative arts in general
** 6020–6029 Aesthetics
** 6030–6069 History of arts
** 6070–6289 Chinese & Japanese Calligraphy and painting
** 6290–6299 Materials & instruments
** 6300–6349 Western painting
** 6350–6359 Engraving Prints
** 6360–6399 Photography
** 6400–6499 Sculpture
** 6500–6599 Architecture
** 6600–6699 Industrial arts
** 6700–6799 Music
** 6800–6899 Amusements & games
** 6900–6999 Physical training & sports

===7000 to 7999 Natural Sciences===
** 7000–7019 Natural science in general
** 7020–7099 [[Mathematics]]
** 7100–7199 [[Astronomy]]
** 7200–7299 [[Physics]]
** 7300–7399 [[Chemistry]]
** 7400–7499 [[Geology|Geological science]]
** 7500–7599 [[Natural history]]
** 7600–7699 [[Botany]]
** 7700–7799 [[Zoology]]
** 7800–7869 Anthropology (Physical)
** 7870–7899 Psychology
** 7900–7999 Medical science

===8000 to 8999 Agriculture and Technology===
** 8000–8009 Agriculture & technology in general
** 8020–8239 Agriculture
** 8240–8289 Home economics (Domestic)
** 8290–8299 Technology in general
** 8300–8349 Handicrafts & artisan trades
** 8400–8499 Manufactures
** 8500–8599 Chemical technology
** 8600–8699 Mining & Metallurgy
** 8700–8899 Engineering
** 8900–8999 Military & Naval science

===9000 to 9999 Generalia and Bibliography===
** 9000–9007 Generalia and bibliography in general
** 9100 Chinese general series of composite nature
** 9101–9109 Chinese general series of a special type
** 9110 Chinese series of particular locality
** 9111–9120 Chinese family & individual author
** 9130–9163 Sinology
** 9164–9179 Japanese general series
** 9180–9199 Japanese individual polygraphic books
** 9200–9229 General periodicals & society publications
** 9230–9289 General congresses & museums
** 9290–9339 General encyclopedias and reference books
** 9401–9409 Bibliography in general
** 9410–9510 Bibliography
** 9511–9519 Subject bibliographies
** 9520–9539 Chinese collective bibliographies
** 9540–9549 Other general bibliographies of various countries
** 9550–9559 Reading lists & best books, periodical index
** 9562–9569 Special bibliographies
** 9570–9579 Bibliographies of critical reviews
** 9600–9629 Library catalogues
** 9630–9639 Dealers’ & publishers’ catalogues
** 9640–9684 Japanese bibliographies
** 9696–9699 Bibliographies of Western countries
** 9700–9929 Librarianship
** 9930–9999 Journalism, newspapers

== See also ==
The official library classification in China is:

* [[Chinese Library Classification]] (CLC)

The other library classifications for Chinese materials outside China are:
* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books drawn up by Profs. Haloun and P. van der Loon for Cambridge University, UK.
* ''University of Leeds Classification of Books in Chinese, UK'' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])

== Notes==

<references />

==References==
* [http://www.lib.unimelb.edu.au/collections/asian/Harvard-Yenching.html Harvard-Yenching Classification in the University of Melbourne] - Subject headings in both Chinese and English.
* [http://research.dils.tku.edu.tw/joemls/41/41-2/139-162.pdf PDF] - Brief history of the Harvard–Yenching Classification System, and an overview of the collections of Chinese language materials in Columbia University, Cornell University, Harvard University, the Library of Congress, Princeton University and Yale University.
* [http://www.news.harvard.edu/gazette/2003/10.30/19-yenching.html Yenching, The singular history of a singular library, Ken Gewertz, Harvard News Office] - A brief history of the Harvard-Yenching Library and the Harvard–Yenching Classification System.

== External links ==
* [https://www.lib.uchicago.edu/e/easia/shelf.html Examples of the Harvard-Yenching classification system]
* [http://www.library.ucla.edu/libraries/eastasian/collect.htm East Asian Library of the University of California in Los Angeles]

{{Library classification systems}}

{{DEFAULTSORT:Harvard-Yenching Classification}}
[[Category:Library cataloging and classification]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Chinese culture]]
[[Category:Harvard University]]
[[Category:Yenching University]]
<=====doc_Id=====>:861
<=====title=====>:
Composite portrait
<=====text=====>:
[[File:Composite portraiture Galton.jpg|thumb|Composite portraiture, [[Francis Galton]], 1883.]]
'''Composite portraiture''' (also known as [[composite photograph]]s) is a technique invented by Sir [[Francis Galton]] in the 1880s after a suggestion by [[Herbert Spencer]] for registering photographs of human faces on the two eyes to create an "average" photograph of all those in the photographed group.<ref>Benson, P., & Perrett, D. (1991). [http://www.abebooks.com/9781854890368/Photovideo-1854890360/plp Computer averaging and manipulations of faces.] In P. Wombell (ed.), ''Photovideo: Photography in the age of the computer'' (pp. 32–38). London: Rivers Oram Press.</ref><ref>Galton, F. (1878). [http://www.galton.org/essays/1870-1879/galton-1879-jaigi-composite-portraits.pdf Composite portraits.] ''Journal of the Anthropological Institute of Great Britain and Ireland, 8'', 132–142.</ref>

Spencer had suggested using onion paper and line drawings, but Galton devised a technique for multiple exposures on the same photographic plate.  He noticed that these composite portraits were more attractive than any individual member, and this has generated a large body of research on human [[attractiveness]] and [[averageness]] one hundred years later.  He also suggested in a [[Royal Society]] presentation in 1883 that the composites provided an interesting concrete representation of human [[ideal type]]s and [[concept]]s.  He discussed using the technique to investigate characteristics of common types of humanity, such as criminals.  In his mind, it was an extension of the statistical techniques of [[average]]s and [[correlation]].  In this sense, it represents one of the first implementations of [[convolution]] [[factor analysis]] and [[neural network]]s in the understanding of [[knowledge representation]] in the human mind. Galton also suggested that the technique could be used for creating natural types of common objects.

During the late 19th century, English psychometrician [[Sir Francis Galton]] attempted to define [[Physiognomy|physiognomic]] characteristics of health, disease, beauty, and criminality, via a method of composite photography. Galton's process involved  the photographic superimposition of two or more faces by multiple exposures. After averaging together photographs of violent criminals, he found that the composite appeared "more respectable" than any of the faces comprising it; this was likely due to the irregularities of the skin across the constituent images being averaged out in the final blend. With the advent of computer technology during the early 1990s, Galton's composite technique has been adopted and greatly improved using computer graphics software.<ref>Yamaguchi, M. K., Hirukawa, T., & Kanazawa, S. (1995). [http://www.perceptionweb.com/abstract.cgi?id=p240563 Judgment of gender through facial parts.] ''Perception, 24'', 563–575.</ref>

== References ==
<references />

==External links==
* [http://www.medienkunstnetz.de/works/composite-fotografie/ Samples of Galton's composites]
* [http://www.compositeportraits.com/ A Visual History of Composite Portraiture in Photography. Edited by Jake Rowland]

[[Category:Portrait art]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:864
<=====title=====>:
Retrievability
<=====text=====>:
'''Retrievability''' is a term associated with the ease with which information can be found or retrieved using an information system, specifically a [[search engine]] or [[information retrieval]] system.

A document (or information object) has high retrievability if there are many queries which retrieve the document via the search engine, and the document is ranked sufficiently high that a user would encounter the document. Conversely, if there are few queries that retrieve the document, or when the document is retrieved the documents are not high enough in the ranked list, then the document has low retrievability.

Retrievability can be considered as one aspect of [[findability]].

Applications of retrievability include detecting [[Web search engine#Search engine bias|search engine bias]], measuring algorithmic bias, evaluating the influence of search technology, tuning information retrieval systems and evaluating the quality of documents in a [[text corpus|collection]].

==See also==
* [[Information retrieval]]
* [[Knowledge mining]]
* [[Search engine optimization]]
* [[Findability]]

==References==
*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Retrievability: an evaluation measure for higher order information access tasks| title=Proceedings of the 17th ACM conference on Information and knowledge management| year=2008| pages=561–570| publisher=ACM| location=Napa Valley, California, USA| series=CIKM '08| doi=10.1145/1458082.1458157| url=http://doi.acm.org/10.1145/1458082.1458157| accessdate=5 June 2013}}
*{{cite book|author1=Azzopardi, L.  |author2=Vinay, V.  |lastauthoramp=yes | chapter=Accessibility in information retrieval| title=Proceedings of the IR research, 30th European conference on Advances in information retrieval| year=2008| pages=482–489| publisher=Springer| location=Glasgow,UK| series=ECIR '08| url=http://dl.acm.org/citation.cfm?id=1793333| accessdate=7 Dec 2016}}

[[Category:Web design]]
[[Category:Knowledge representation]]
[[Category:Information science]]
<=====doc_Id=====>:867
<=====title=====>:
DOAP
<=====text=====>:
{{one source|date=October 2012}}

'''DOAP''' ('''Description of a Project''') is an [[RDF Schema]] and [[XML]] vocabulary to describe  software projects, in particular [[free and open source software]].

It was created and initially developed by [[Edd Dumbill]] to convey semantic information associated with open source software projects.

== Adoption ==

There are currently generators, [[validator]]s, viewers, and converters to enable more projects to be able to be included in the [[semantic web]]. [[Freshmeat]]'s 43 000 projects are now available published with DOAP.<ref>{{ cite web | url = http://fgiasson.com/blog/index.php/2007/08/04/freshmeatnet-now-available-in-doap-43-000-new-doap-projects/ | title = Freshmeat.net now available in DOAP: 43 000 new DOAP projects | first = Frederick | last = Giasson | accessdate = 2010-04-08 }}</ref> It is currently used in the [[Mozilla Foundation]]'s project page and in several other software repositories, notably the [[Python Package Index]].

Major properties include: doap:homepage, doap:developer, doap:programming-language, doap:os

== Examples ==

The following is an example in RDF/XML:

<source lang="xml">
 <rdf:RDF xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:doap="http://usefulinc.com/ns/doap#">
  <doap:Project>
   <doap:name>Example project</doap:name>
   <doap:homepage rdf:resource="http://example.com" />
   <doap:programming-language>javascript</doap:programming-language>
   <doap:license rdf:resource="http://example.com/doap/licenses/gpl"/>
  </doap:Project>
 </rdf:RDF>
</source>

Other properties include <code>Implements specification, anonymous root, platform, browse, mailing list, category, description, helper, tester, short description, audience, screenshots, translator, module, documenter, wiki, repository, name, repository location, language, service endpoint, created, download mirror, vendor, old homepage, revision, download page, license, bug database, maintainer, blog, file-release</code> and <code>release.</code>{{citation needed|date=January 2013}}

==References==

{{reflist}}

==External links==
* {{github|edumbill/doap/|Doap Project}}
* [http://www.oss-watch.ac.uk/resources/doap.xml OSS Watch DOAP Briefing Note]
* [http://crschmidt.net/semweb/doapamatic/ doapamatic]: DOAP generator

{{Semantic Web}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]
<=====doc_Id=====>:870
<=====title=====>:
HiLog
<=====text=====>:
'''HiLog''' is a programming [[logic]] with higher-order syntax, which allows arbitrary terms to appear in predicate and function positions. However, the [[model theory]] of HiLog is first-order. Although syntactically HiLog strictly extends [[first order logic]], HiLog can be embedded into this logic.

HiLog is described in detail in
<ref name="hilog-jlp">
W. Chen, M. Kifer and D.S. Warren (1993), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.52.7860 ''HiLog: A Foundation for Higher-Order Logic Programming'']. Journal of Logic Programming, 1993.
</ref>
.<ref>
W. Chen, M. Kifer and D.S. Warren (1989), [http://citeseerx.ist.psu.edu/showciting?cid=2016805 ''HiLog: A first order semantics for higher-order logic programming constructs'']. Proc. North American Logic Programming Conference, 1989.
</ref> 
It was later extended in the direction of [[many-sorted logic]] in.<ref>
W. Chen and M. Kifer (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.56.4332 ''Sorted HiLog: Sorts in Higher-Order Logic Data Languages'']. Int’l Conference on Database Theory, Springer Lecture Notes in Computer Science #893.
</ref>
Other contributions to the theory of HiLog include
<ref>
K.A. Ross (1994), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.55.2148 ''On Negation in HiLog'']. Journal of Logic Programming, 1994.
</ref>
.<ref>
J. de Bruijn and S. Heymans (2008), [http://www.kr.tuwien.ac.at/staff/bruijn/priv/publications/frames-predicates-fi.pdf ''On the Relationship between Description Logic-based and F-Logic-based Ontologies'']. Fundamenta Informaticae 82:3, 2008, pp. 213-236.
</ref>

The [[XSB|XSB System]] parses HiLog syntax, but the integration of HiLog into XSB is only partial. In particular, HiLog is not integrated with the XSB module system. A full implementation of HiLog is available in the [[Flora-2|Flora-2 system]].

In,<ref name="hilog-jlp"/> it has been shown that HiLog can be embedded into [[first-order logic]] through a fairly simple transformation. For instance, <tt>p(X)(Y,Z(V)(W))</tt> gets embedded as the following first-order term:

  apply(p(X),Y,apply(apply(Z,V),W))

Details can be found in.<ref name="hilog-jlp"/>

The [[Rule Interchange Format#FLD|Framework for Logic-Based Dialects]] (RIF-FLD) of the [[Rule Interchange Format]] (RIF) is largely based on the ideas underlying HiLog and [[F-logic]].

== Examples ==

In all the examples, below, capitalized symbols denote variables and the comma denotes [[logical conjunction]], as in most [[logic programming]] languages. The first and the second examples show that variables can appear in predicate positions. Predicates can even be complex terms, such as <tt>closure(P)</tt> or <tt>maplist(F)</tt> below. The third example shows that variables can also appear in place of atomic formulas, while the fourth example illustrates the use of variables in place of function symbols. The first example defines a generic transitive closure operator, which can be applied to an arbitrary binary predicate. The second example is similar. It defines a [[LISP]]-like mapping operator, which applies to an arbitrary binary predicate. The third example shows that the [[Prolog]] meta-predicate <tt>call/1</tt> can be expressed in HiLog in a natural way and without the use of extra-logical features. The last example defines a predicate that traverses arbitrary binary trees represented as [[Term (logic)|first-order term]]s.
<source lang="prolog">
  closure(P)(X,Y) <- P(X,Y).
  closure(P)(X,Y) <- P(X,Z), closure(P)(Z,Y).

  maplist(F)([],[]).
  maplist(F)([X|R],[Y|Z]) <- F(X,Y), maplist(F)(R,Z).

  call(X) <- X.

  traverse(X(L,R)) <- traverse(L), traverse(R).
</source>

==References==
{{reflist}}

[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:873
<=====title=====>:
Enactivism
<=====text=====>:
'''Enactivism''' argues that [[cognition]] arises through a dynamic interaction between an acting [[organism]] and its environment.<ref name="Evan Thompson"/> It claims that our environment is one which we selectively create through our capacities to interact with the world.<ref name=Rowlands/> "Organisms do not passively receive information from their environments, which they then translate into internal representations. Natural cognitive systems...participate in the generation of meaning ...engaging in transformational and not merely informational interactions: ''they enact a world''."<ref name=Jaegher1/> These authors suggest that the increasing emphasis upon enactive terminology presages a new era in thinking about cognitive science.<ref name=Jaegher1/> How the actions involved in enactivism relate to age-old questions about [[free will]] remains a topic of active debate.<ref name=Manetti/>

The term 'enactivism' is close in meaning to 'enaction', defined as "the manner in which a subject of perception creatively matches its actions to the requirements of its situation".<ref name=Tascano0/> The introduction of the term ''enaction'' in this context is attributed to  [[Francisco Varela]], [[Evan Thompson]], and [[Eleanor Rosch]],<ref name=Tascano0/><ref name=RWilson/> who proposed the name to "emphasize the growing conviction that cognition is not the representation of a pre-given world by a pre-given mind but is rather the enactment of a world and a mind on the basis of a history of the variety of actions that a being in the world performs".<ref name=Varela/> This was further developed by Thompson and others,<ref name="Evan Thompson"/> to place emphasis upon the idea that experience of the world is a result of mutual interaction between the sensorimotor capacities of the organism and its environment.<ref name=RWilson/>

The initial emphasis of enactivism upon sensorimotor skills has been criticized as "cognitively marginal",<ref name=ClarkA/> but it has been extended to apply to higher level cognitive activities, such as social interactions.<ref name="Jaegher1"/> "In the enactive view,... knowledge is constructed: it is constructed by an agent through its sensorimotor interactions with its environment, co-constructed between and within living species through their meaningful interaction with each other. In its most abstract form, knowledge is co-constructed between human individuals in socio-linguistic interactions...Science is a particular form of social knowledge construction...[that] allows us to perceive and predict events beyond our immediate cognitive grasp...and also to construct further, even more powerful scientific knowledge."<ref name=Rohde/>

Enactivism is closely related to [[situated cognition]] and [[embodied cognition]], and is presented as an alternative to [[cognitivism (psychology)|cognitivism]], [[computationalism]], and [[Cartesian dualism]].

==Philosophical aspects<!--'Enaction (philosophy)' redirects here-->==

Enactivism is one of  a cluster of related theories sometimes known as the ''4Es'', As described by [[Mark Rowlands]], mental processes are:<ref name=Rowlands/>
* '''Embodied''' involving more than the brain, including a more general involvement of bodily structures and processes.
* '''Embedded''' functioning only in a related external environment.
* '''Enacted''' involving not only neural processes, but also things an organism ''does''.
* '''Extended''' into the organism's environment.

Enactivism proposes an alternative to [[Dualism (philosophy of mind)|dualism]] as a philosophy of mind, in that it emphasises the interactions between mind, body and the environment, seeing them all as inseparably intertwined in mental processes.<ref name=EThompson/> The self arises as part of the process of an embodied entity interacting with the environment in precise ways determined by its physiology.   In this sense, individuals can be seen to "grow into" or arise from their interactive role with the world.<ref name=Burman/>
:"Enaction is the idea that organisms create their own experience through their actions. Organisms are not passive receivers of input from the environment, but are actors in the environment such that what they experience is shaped by how they act."<ref name=Hutchins/>

In ''The Tree of Knowledge'' Maturana & Varela proposed the term ''enactive''<ref name=Maturana/> "to evoke the view of knowledge that what is known is brought forth, in contraposition to the more classical views of either cognitivism<ref group=Note name=Cognitivism/> or connectionism.<ref group=Note name=Connectionism/> They see enactivism as providing a middle ground between the two extremes of [[representationalism]] and [[solipsism]]. They seek to "confront the problem of understanding how our existence-the [[Praxis (process)|praxis]] of our living- is coupled to a surrounding world which appears filled with regularities that are at every instant the result of our biological and social histories.... to find a ''via media'': to understand the regularity of the world we are experiencing at every moment, but without any point of reference independent of ourselves that would give certainty to our descriptions and cognitive assertions.  Indeed the whole mechanism of generating ourselves, as describers and observers tells us that our world, as the world which we bring forth in our coexistence with others, will always have precisely that mixture of regularity and mutability, that combination of solidity and shifting sand, so typical of human experience when we look at it up close."[''Tree of Knowledge'', p.&nbsp;241]

Enactivism also addresses the [[hard problem of consciousness]], referred to by Thompson as part of the ''[[explanatory gap]]'' in explaining how consciousness and subjective experience are related to brain and body.<ref name=EThompson2/>  "The problem with the dualistic concepts of consciousness and life in standard formulations of the hard problem is that they exclude each other by construction".<ref name=EThompson3/> Instead, according to Thompson's view of enactivism, the study of consciousness or [[Phenomenology (philosophy)|phenomenology]] as exemplified by [[Husserl]] and [[Merleau-Ponty]] is to complement science and its objectification of the world. "The whole universe of science is built upon the world as directly experienced, and if we want to subject science itself to rigorous scrutiny and arrive at a precise assessment of its meaning and scope, we must begin by reawakening the basic experience of the world of which science is the second-order expression" (Merleau-Ponty, ''The phenomenology of perception'' as quoted by Thompson, p.&nbsp;165). In this interpretation, enactivism asserts that science is formed or enacted as part of humankind's interactivity with its world, and by embracing phenomenology "science itself is properly situated in relation to the rest of human life and is thereby secured on a sounder footing."<ref name=EThompson4/><ref name=Baldwin/>

Enaction has been seen as a move to conjoin [[representationalism]] with [[phenomenalism]], that is, as adopting a [[constructivist epistemology]], an epistemology centered upon the active participation of the subject in constructing reality.<ref name=Mutelesi/><ref name=Chiari/> However, 'constructivism' focuses upon more than a simple 'interactivity' that could be described as a minor adjustment to 'assimilate' reality or 'accommodate' to it.<ref name=Glaserfeld/> Constructivism looks upon interactivity as a radical, creative, revisionist process in which the knower ''constructs'' a personal 'knowledge system' based upon their experience and tested by its viability in practical encounters with their environment. Learning is a result of perceived anomalies that produce dissatisfaction with existing conceptions.<ref name=Glasersfeld2/>

How does constructivism relate to enactivism? From the above remarks it can be seen that [[Ernst von Glasersfeld|Glasersfeld]] expresses an interactivity between the knower and the known quite acceptable to an enactivist, but does not emphasize  the structured probing of the environment by the knower that leads to the "perturbation relative to some expected result" that then leads to a new understanding.<ref name=Glasersfeld2/> It is this probing activity, especially where it is not accidental but deliberate, that characterizes enaction, and invokes ''affect'',<ref name=Ward2/> that is, the motivation and planning that lead to doing and to fashioning the probing, both observing and modifying the environment, so that "perceptions and nature condition one another through generating one another."<ref name=Diettrich/> The questioning nature of this probing activity is not an emphasis of [[Jean Piaget|Piaget]] and Glasersfeld.

Sharing enactivism's stress upon both action and embodiment in the incorporation of knowledge, but giving Glasersfeld's mechanism of viability an [[Introduction to evolution|evolutionary]] emphasis,<ref name=Diettrich2/> is [[evolutionary epistemology]]. Inasmuch as an organism must reflect its environment well enough for the organism to be able to survive in it, and to be competitive enough to be able to reproduce at sustainable rate, the structure and reflexes of the organism itself embody knowledge of its environment. This biology-inspired theory of the growth of knowledge is closely tied to [[universal Darwinism]], and is associated with evolutionary epistemologists such as [[Karl Popper]], [[Donald T. Campbell]], [[Peter Munz]], and [[Gary Cziko]].<ref name=Gontier/> According to Munz, "an organism is an ''embodied theory'' about its environment... Embodied theories are also no longer expressed in language, but in anatomical structures or reflex responses, etc."<ref name=Gontier/><ref name=Munz/>

==Psychological aspects==
McGann & others<ref name=McGann>{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203–209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935}}
</ref> argue that enactivism attempts to mediate between the explanatory role of the coupling between cognitive agent and environment and the traditional emphasis on brain mechanisms found in neuroscience and psychology.  In the interactive approach to social cognition developed by De Jaegher &  others,<ref name=Gallagher0>{{cite journal |author=Shaun Gallagher |year=2001 |title=The practice of mind |journal=Journal of Consciousness Studies |volume=8 |issue=5–7 |pages=83–107 |url=http://www.ummoss.org/Gallagher01.pdf}}
</ref><ref name=Gallager1>
{{cite book |author=Shaun Gallagher |isbn=978-0199204168 |edition=Paperback |year=2006 |title=How the Body Shapes the Mind |publisher=Oxford University Press |url=https://books.google.com/books/about/How_the_Body_Shapes_the_Mind.html?id=zhv5F-GYm98C}}
</ref><ref name=Ratcliffe>
{{cite book |author=Matthew Ratcliffe |year=2008 |title=Rethinking Commonsense Psychology: A Critique of Folk Psychology, Theory of Mind and Simulation |publisher=Palgrave Macmillan |isbn=978-0230221208 |url=https://books.google.com/books/about/Rethinking_Commonsense_Psychology.html?id=-JNyQgAACAAJ}}
</ref> the dynamics of interactive processes are seen to play significant roles in coordinating interpersonal understanding, processes that in part include what they call [[#Participatory sense-making|''participatory sense-making'']].<ref name=DeJaeger0>
{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |url=http://www.enactionschool.com/resources/papers/DeJaegherDiPaolo2007.pdf |year=2007 |title=Participatory Sense-Making: An enactive approach to social cognition |journal=Phenomenology and the Cognitive Sciences |volume=6 |issue=4 |pages=485–507 |doi=10.1007/s11097-007-9076-9}}
</ref><ref name=DeJaegher1>
{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |year=2010 |title=Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |volume=14 |issue=10 |pages=441–447 |url=http://ezequieldipaolo.files.wordpress.com/2011/10/dejaegher_dipaolo_gallagher_tics_2010.pdf |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}
</ref> Recent developments of enactivism in the area of social neuroscience involve the proposal of ''The Interactive Brain Hypothesis''<ref name=DiPaolo3>
{{cite journal |url=http://journal.frontiersin.org/Journal/10.3389/fnhum.2012.00163/full |author1=Ezequiel Di Paolo |author2=Hanne De Jaegher |date=June 2012 |title= The Interactive Brain Hypothesis |journal=Frontiers in Human Neuroscience |volume=7 |issue=6 |doi=10.3389/fnhum.2012.00163}}</ref> where social cognition brain mechanisms, even those used in non-interactive situations, are proposed to have interactive origins.

===Enactive views of perception===
In the enactive view, perception "is not conceived as the transmission of information but more as an exploration of the world by various means. Cognition is not tied into the workings of an 'inner mind', some cognitive core, but occurs in directed interaction between the body and the world it inhabits."<ref name=McGann2>
{{cite book |title=Consciousness & Emotion: Agency, conscious choice, and selective perception |page=184 |author1=Marek McGann |author2=Steve Torrance |chapter=Doing It and Meaning It: And the relation between the two  |isbn=9789027294616 |publisher=John Benjamins Publishing |year=2005 |url=https://books.google.com/books?id=LZk6AAAAQBAJ&pg=PA184 |editor1=Ralph D. Ellis |editor2=Natika Newton }}
</ref>

[[Alva Noë]] in advocating an enactive view of perception<ref name=Noe>
{{cite book |author=Alva Noë |title= Action in Perception |url=https://books.google.com/books?id=kFKvU2hPhxEC&pg=PA1 |pages=1 ''ff'' |chapter=Chapter 1: The enactive approach to perception: An introduction |isbn=9780262140881 |year=2004 |publisher=MIT Press}}
</ref> sought to resolve how we perceive three-dimensional objects, on the basis of two-dimensional input.  He argues that we perceive this solidity (or 'volumetricity') by appealing to patterns of sensorimotor expectations. These arise from our agent-active  'movements and interaction' with objects, or 'object-active' changes in the object itself. The solidity is perceived through our expectations and skills in knowing how the object's appearance would change with changes in how we relate to it. He saw all perception as an active exploration of the world, rather than being a passive process, something which happens to us.

Noë's idea of the role of 'expectations' in three-dimensional perception has been opposed by several philosophers, notably by [[Andy Clark]].<ref name=ClarkA1/> Clark points to difficulties of the enactive approach.  He points to internal processing of visual signals, for example, in the ventral and dorsal pathways, [[Two-streams hypothesis|the two-streams hypothesis]]. This results in an integrated perception of objects (their recognition and location, respectively) yet this processing cannot be described as an action or actions. In a more general criticism, Clark suggests that perception is not a matter of expectations about sensorimotor mechanisms guiding perception. Rather, although the limitations of sensorimotor mechanisms constrain perception, this sensorimotor activity is drastically filtered to fit current needs and purposes of the organism, and it is these imposed 'expectations' that govern perception, filtering for the 'relevant' details of sensorimotor input (called "sensorimotor summarizing").<ref name=ClarkA1/>

Another application of enaction to perception is analysis of the human hand. The many remarkably demanding uses of the hand are not learned by instruction, but through a history of engagements that lead to the acquisition of skills. According to one interpretation, it is suggested that "the hand [is]...an organ of cognition", not a faithful subordinate working under top-down instruction, but a partner in a "bi-directional interplay between manual and brain activity."<ref name=Hutto>
{{cite book |title= Radicalizing Enactivism: Minds without content |author=[[Daniel D Hutto]], Erik Myin |pages=46 ''ff'' |chapter=A helping hand |url=https://books.google.com/books?id=pAj-96LlBuMC&pg=PA46 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}
</ref> According to [[Daniel Hutto]]: "Enactivists are concerned to defend the view that our most elementary ways of engaging with the world and others - including our basic forms of perception and perceptual experience - are mindful in the sense of being phenomenally charged and intentionally directed, despite being non-representational and content-free."<ref name=Hutto2>
{{cite book |title= Radicalizing Enactivism: Minds without content |author1=Daniel D Hutto |author2=Erik Myin |pages=12–13  |chapter=Chapter 1: Enactivism: The radical line |url=https://books.google.com/books?id=pAj-96LlBuMC&pg=PA12 |isbn= 9780262018548 |year=2013 |publisher=MIT Press}}
</ref> Hutto calls this position 'REC' (<u>R</u>adical <u>E</u>nactive <u>C</u>ognition): "According to REC, there is no way to distinguish neural activity that is imagined to be genuinely content involving (and thus truly mental, truly cognitive) from other non-neural activity that merely plays a supporting or enabling role in making mind and cognition possible."<ref name=Hutto2/>

===Participatory sense-making===

[[Hanne De Jaegher]] and [[Ezequiel Di Paolo]] (2007)<ref name="DeJaeger0"/> have extended the enactive concept of sense-making<ref name="EThompson3"/> into the social domain. The idea takes as its departure point the process of interaction between individuals in a social encounter.<ref name=DeJaegher_etal>{{cite journal |author1=Hanne De Jaegher |author2=Ezequiel Di Paolo |author3=Shaun Gallagher |title= Can social interaction constitute social cognition? |journal=Trends in Cognitive Sciences |year=2010  |volume=14 |issue=10 |pages=441–447 |doi=10.1016/j.tics.2010.06.009 |pmid=20674467}}
</ref> De Jaegher and Di Paolo argue that the interaction process itself can take on a form of autonomy (operationally defined). This allows them to define social cognition as the generation of meaning and its transformation through interacting individuals.

The notion of participatory sense-making has led to the proposal that interaction processes can sometimes play constitutive roles in social cognition (De Jaegher, Di Paolo, Gallagher, 2010).<ref name="DeJaegher1"/> It has been applied to research in [[social neuroscience]]''<ref name="DiPaolo3"/><ref name=SchilbachTimmermans>
{{cite journal |author1=Leonhard Schilbach |author2=Bert Timmermans |author3=Vasudevi Reddy |author4=Alan Costall |author5=Gary Bente |author6=Tobias Schlicht |author7=Kai Vogeley |title= Toward a second-person neuroscience |journal=Behavioral and Brain Sciences |year=2013  |volume=36 |issue=4 |pages=393–414 |doi=10.1017/S0140525X12000660}}</ref>'' and [[autism]].''<ref name="DeJaegher_autism">{{cite journal |author= Hanne De Jaegher |title= Embodiment and sense-making in autism|journal=Frontiers in Integrative Neuroscience |year=2012  |volume=7 |pages=15 |doi=10.3389/fnint.2013.00015}}
</ref>''

In a similar vein, "an inter-enactive approach to agency holds that the behavior of agents in a social situation unfolds not only according to their individual abilities and goals, but also according to the conditions and constraints imposed by the autonomous dynamics of the interaction process itself".<ref name=STorrance>
{{cite journal |title=An Inter-Enactive Approach to Agency: Participatory Sense-Making, Dynamics, and Sociality |author1=Steve Torrance |author2=Tom Froese |url=http://sacral.c.u-tokyo.ac.jp/pdf/froese_humana_2011.pdf |journal=Human Mente |volume=15 |pages=21–53 |year=2011 }} 
</ref> According to Torrance, enactivism involves five interlocking themes related to the question "What is it to be a (cognizing, conscious) agent?" It is:<ref name=STorrance/>
:1. to be a biologically autonomous ([[Autopoiesis|autopoietic]]) organism
:2. to generate ''significance'' or ''meaning'', rather than to act via...updated internal representations of the external world
:3. to engage in sense-making via dynamic coupling with the environment
:4. to 'enact' or 'bring forth' a world of significances by mutual co-determination of the organism with its enacted world
:5. to arrive at an experiential awareness via lived embodiment in the world.

Torrance adds that "many kinds of agency, in particular the agency of human beings, cannot be understood separately from understanding the nature of the interaction that occurs between agents." That view introduces the social applications of enactivism. "Social cognition is regarded as the result of a special form of action, namely ''social interaction''...the enactive approach looks at the circular dynamic within a dyad of embodied agents."<ref name=FuchsT>
{{cite book |url=https://books.google.com/books?id=Olm10GVwV74C&pg=PA206 |page=206 |chapter=Non-representational intersubjectivity |author1=Thomas Fuchs |author2=Hanne De Jaegher |isbn=9783794527915 |year=2010 |publisher=Schattauer Verlag |title=The Embodied Self: Dimensions, Coherence and Disorders |editor1=Thomas Fuchs |editor2=Heribert C. Sattel |editor3=Peter Henningsen }}
</ref>
 
In [[cultural psychology]], enactivism is seen as a way to uncover cultural influences upon feeling, thinking and acting.<ref name=Verheggen>{{cite book |chapter=Chapter 8: Enactivism |author1=Cor Baerveldt |author2=Theo Verheggen |title=The Oxford Handbook of Culture and Psychology  |url=https://books.google.com/books?id=WljI1r2e-SUC&pg=PA165 |pages=165''ff'' |doi=10.1093/oxfordhb/9780195396430.013.0009 |isbn=9780195396430 |date=May 2012 |quote= Whereas the enactive approach in general has focused on sense-making as an embodied and situated activity, enactive cultural psychology emphasizes the expressive and dynamically enacted nature of cultural meaning.}}</ref>  Baerveldt and Verheggen argue that "It appears that seemingly natural experience is thoroughly intertwined with sociocultural realities." They suggest that the social patterning of experience is to be understood through enactivism, "the idea that the reality we have in common, and in which we find ourselves, is neither a world that exists independently from us, nor a socially shared way of representing such a pregiven world, but a world itself brought forth by our ways of communicating and our joint action....The world we inhabit is manufactured of 'meaning' rather than 'information'.<ref name=Baerveldt>
{{cite journal |title=Enactivism and the experiential reality of culture: Rethinking the epistemological basis of cultural psychology |author1=Cor Baerveldt |author2=Theo Verheggen |url=https://docs.google.com/file/d/0Bz8cVS8LoO7OTk9ZUkVqazFiU1U/edit |journal=Culture & Psychology |volume=5 |issue=2 |pages=183–206 |year=1999 |doi=10.1177/1354067x9952006}}
</ref>

[[Niklas Luhmann|Luhmann]] attempted to apply Maturana and Varela's notion of autopoiesis to social systems.<ref name=Luhmann>
{{cite book |title=Social systems |url=https://books.google.com/books?id=zVZQW4gxXk4C&pg=PA34&lpg=PA34 |isbn= 9780804726252 |year=1995 |publisher=Stanford University Press |author=Niklas Luhmann}}
</ref> "A core concept of social systems theory is derived from biological systems theory: the concept of ''autopoiesis''. Chilean biologist Humberto Maturana come up with the concept to explain how biological systems such as cells are a product of their own production." "Systems exist by way of operational closure and this means that they each construct themselves and their own realities."<ref name=Moeller>
{{cite book  |chapter=Part 1: A new way of thinking about society |pages= 12 ''ff'' |author=Hans-Georg Moeller |year=2011 |isbn= 978-0812695984 |publisher=Open Court |title=Luhmann Explained: From Souls to Systems |url=https://books.google.com/books?id=tuKsEvpcj9MC&pg=PA12}}
</ref>

==Educational aspects==
The first definition of enaction was introduced by psychologist [[Jerome Bruner]],<ref name=Pugliese>
{{cite book |title=Intelligent Virtual Agents: |chapter=A framework for motion based bodily enaction with virtual characters; §2.1 Enaction |author1=Roberto Pugliese |author2=Klaus Lehtonen |url=https://books.google.com/books?id=QU9b_IjVMF4C&pg=PA163 |page=163 |isbn=9783642239731 |publisher=Springer |year=2011}}
</ref><ref name=Beck>
{{cite book |url=https://books.google.com/books?id=8V9BAAAAQBAJ&pg=PA104 |page=104 |title=From Diagnostics to Learning Success: Proceedings in Vocational Education and Training |isbn=978-9462091894 |edition=Paperback |year=2013 |publisher=Springer Science & Business |author=Stephanie A Hillen |chapter=Chapter III: What can research on technology for learning in vocational educational training teach media didactics? |editor1=Klaus Beck |editor2=Olga Zlatkin-Troitschanskaia }}
</ref> who introduced enaction as 'learning by doing' in his discussion of how children learn, and how they can best be helped to learn.<ref name=Bruner>{{cite book |author=[[Jerome Bruner]]|year=1966 |title=Toward a theory of instruction |publisher=Belknap Press of Harvard University Press |isbn=978-0674897007}}</ref><ref name=Bruner2>{{cite book |author=Jerome Bruner |year=1968 |title=Processes of cognitive growth: Infancy |publisher= Crown Pub |isbn= 978-0517517482}}{{oclc|84376}}</ref> He associated enaction with two other ways of knowledge organization: [[Cultural icon|Iconic]] and [[Symbol]]ic.<ref name=Bruner3>Quote from
{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}} as quoted from {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor & Francis |isbn= 0415326982 |edition=Paperback}}</ref>

:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully  (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"
The term 'enactive framework' was elaborated upon by [[Francisco Varela]] and [[Humberto Maturana]].<ref name=Bopry>
{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |author=Jeanette Bopry |chapter=Providing a warrant for constructivist practice: the contribution of Francisco Varela |quote=Varela's enactive framework beginning with his collaboration on [[autopoiesis]] theory with his mentor Humberto Maturana [and the development of] enaction as a framework within which these theories work as a matter of course. |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |year=2007 |publisher=Greenwood Publishing Group |isbn=9780313331237 |url=https://books.google.com/books?id=O1ugEIEid6YC&pg=PA474 |pages=474 ''ff''}}
</ref>

Sriramen argues that enactivism provides "a rich and powerful explanatory theory for learning and being."<ref name= Sriraman>
{{cite book |title=Theories of Mathematics Education: Seeking New Frontiers |author1=Bharath Sriraman |author2=Lyn English |isbn=3642007422 |year=2009 |publisher=Springer |url=https://books.google.com/books?id=Kd_LgW2AXIoC&pg=PA42 |pages=42 ''ff'' |chapter=Enactivism}}</ref> and that it is closely related to both the [[Piaget's theory of cognitive development|ideas of cognitive development]] of [[Jean Piaget|Piaget]], and also the [[social constructivism]] of [[Vygotsky]].<ref name=Sriraman/> Piaget focused on the child's immediate environment, and suggested cognitive structures like spatial perception emerge as a result of the child's interaction with the world.<ref name=Roth>
{{cite book |title=Geometry as Objective Science in Elementary School Classrooms: Mathematics in the Flesh |author=Wolff-Michael Roth |isbn=1136732209 |year=2012 |publisher=Routledge |pages=41 ''ff'' |url=https://books.google.com/books?id=cXSsAgAAQBAJ&pg=PT41 |chapter=Epistemology and psychology: Jean Piaget and modern constructivism}}
</ref> According to Piaget, children ''construct'' knowledge, using what they know in new ways and testing it, and the environment provides feedback concerning the adequacy of their construction.<ref name= Cziko>
{{cite book |title=Without Miracles: Universal Selection Theory and the Second Darwinian Revolution |author=Gary Cziko |chapter=Chapter 12: Education; The provision and transmission of truth, or the selectionist growth of fallible knowledge? |page=222 |url=https://books.google.com/books?id=v1JEypylerUC&pg=PA222&lpg=PA222 |year=1997 |isbn=9780262531474 |publisher=MIT Press}}
</ref> In a cultural context, Vygotsky suggested that the kind of cognition that can take place is not dictated by the engagement of the isolated child, but is also a function of social interaction and dialogue that is contingent upon a sociohistorical context.<ref name=Kincheloe>
{{cite book |title=The Praeger Handbook of Education and Psychology, Volume 1 |chapter=Interpretivists drawing on the power of enactivism |url=https://books.google.com/books?id=O1ugEIEid6YC&pg=PA24 |pages=24 ''ff'' |publisher=Greenwood Publishing Group |year=2007 |editor1=Joe L. Kincheloe |editor2=Raymond A. Horn |author=Joe L Kincheloe |isbn=0313331235}}
</ref>  Enactivism in educational theory "looks at each learning situation as a complex system consisting of teacher, learner, and context, all of which frame and co-create the learning situation."<ref name=Vithal>
{{cite book |editor1=Renuka Vithal |editor2=Jill Adler |editor3=Christine Keitel |title=Researching Mathematics Education in South Africa: Perspectives, Practices and Possibilities |chapter=Chapter 9: Dilemmas of change: seeing the complex rather than the complicated?  |page=240 |author=Chris Breen |isbn=0796920478 |publisher=HSRC Press |year=2005 |url=https://books.google.com/books?id=byWHt_NVUEgC&pg=RA6-PA240}}
</ref> Enactivism in education is very closely related to [[situated cognition]],<ref name=VanDeGevel>
{{cite book |title=The nexus between artificial intelligence and economics |chapter=§3.2.2 Enactive artificial intelligence |quote=''Enactivism'' may be considered as the most developed model of embodied situated cognition...Knowing is inseparable from doing. |url=https://books.google.com/books?id=uek_AAAAQBAJ&pg=PA21 |page=21 |author=Ad J. W. van de Gevel, Charles N. Noussair |isbn=3642336477 |publisher=Springer |year=2013}}
</ref> which holds that "knowledge is situated, being in part a product of the activity, context, and culture in which it is developed and used."<ref name=Collins>
{{cite journal |title=Situated cognition and the culture of learning |author1=John Seely Brown |author2=Allan Collins |author3=Paul Duguid |url=http://www.exploratorium.edu/ifi/resources/museumeducation/situated.html |journal=Educational Researcher |volume=18 |number=1 |pages=32–42 |date=Jan–Feb 1989 |doi=10.3102/0013189x018001032}}
</ref> This approach challenges the "separating of what is learned from how it is learned and used."<ref name=Collins/>

==Artificial intelligence aspects==
{{importance section|date=May 2014}}
{{main|Enactive interfaces}}
The ideas of enactivism regarding how organisms engage with their environment have interested those involved in [[Cognitive robotics|robotics]] and [[Human–computer interaction|man-machine interfaces]]. The analogy is drawn that a robot can be designed to interact and learn from its environment in a manner similar to the way an organism does that,<ref name=Sandini>
{{cite book |chapter=The ''iCub'' cognitive humanoid robot: An open-system research platform for enactive cognition |author1=Giulio Sandini |author2=Giorgio Metta |author3=David Vernon |title=50 Years of Artificial Intelligence: Essays Dedicated to the 50th Anniversary of Artificial Intelligence |editor1=Max Lungarella |editor2=Fumiya Iida |editor3=Josh Bongard |editor4=Rolf Pfeifer |publisher=Springer |year=2007 |isbn= 9783540772958}}
</ref> and a human can interact with a computer-aided design tool or data base using an interface that creates an enactive environment for the user, that is, all the user's tactile, auditory, and visual capabilities are enlisted in a mutually explorative engagement, capitalizing upon all the user's abilities, and not at all limited to cerebral engagement.<ref name=Bordegoni>
{{cite book |title=Emotional Engineering: Service Development |chapter=§4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&pg=PA78 |pages=78 ''ff'' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}
</ref> In these areas it is common to refer to [[affordance]]s as a design concept, the idea that an environment or an interface affords opportunities for enaction, and good design involves optimizing the role of such affordances.<ref name=Norman>
{{cite book |title=The Design of Everyday Things |edition=Revised and expanded |quote=An affordance is a relationship between the properties of an object and the capabilities of the agent that determine just how the object could possibly be used. |url=https://books.google.com/books?id=nVQPAAAAQBAJ&pg=PT17 |year=2013 |page=11 |isbn=978-0465050659 |publisher=Basic Books |author=Don Norman |chapter=Affordances }}
</ref><ref name=Kim>
{{cite book |title=Encyclopedia of human computer interaction |chapter=The use and evolution of affordance in HCI  |url=https://books.google.com/books?id=h9iZh_I1YREC&pg=PA668 |pages=668 ''ff'' |isbn=9781591407980 |year=2006 |publisher=Idea Group Inc |author=Georgios S Christou |editor=Claude Ghaoui}}
</ref><ref name=Kaipainen>
{{cite journal |title=Enactive Systems and Enactive Media: Embodied Human-Machine Coupling beyond Interfaces |url=http://www.mitpressjournals.org/doi/abs/10.1162/LEON_a_00244#.U3_JKygT0cs  |journal=Leonardo |volume=44 |pages=433–438 |date=October 2011 |issue=5 |doi=10.1162/LEON_a_00244 |author1=Mauri Kaipainen |author2=Niklas Ravaja |author3=Pia Tikka |display-authors=etal}} 
</ref><ref name=Boy>

{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&pg=PA118&lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118 |quote=The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela's sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself.}}

</ref><ref name=Thannhuber>
{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-Jörg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&rct=j&q=&esrc=s&sa=X&ei=N-h_U6HtHIiEogSy_oHAAw&ved=0CCcQgAMoADAA&usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 ''ff'' |doi=10.1016/s0007-8506(07)62129-5}}</ref>

The activity in the AI community also has influenced enactivism as whole. Referring extensively to modeling techniques for [[evolutionary robotics]] by Beer,<ref name=Beer>
{{cite journal |author=Randall D Beer |year=1995 |title=A dynamical systems perspective on agent-environment interaction.
 |journal= Artificial Intelligence |volume=72 |pages=173–215 |url=http://dx.doi.org/10.1016/0004-3702%2894%2900005-L |doi=10.1016/0004-3702(94)00005-l}}
</ref> the modeling of learning behavior by Kelso,<ref name=Kelso>
{{cite book |author=James AS Kelso |year=2009 |chapter=Coordination dynamics |editor=R. A. Meyers |title= Encyclopedia of complexity and system science |pages= 1537–1564 |isbn=978-0-387-75888-6 |url=http://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30440-3_101}}
</ref> and to modeling of sensorimotor activity by Saltzman,<ref name=Saltzman>
{{cite book |author=Eliot L. Saltzman |year=1995 |chapter=Dynamics and coordinate systems in skilled sensorimotor activity |editor1=T. van Gelder |editor2=R. F. Port |title= Mind as motion: Explorations in the dynamics of cognition  |publisher= MIT Press |isbn=9780262161503 |url=https://books.google.com/books?id=e6HUM6V8QbQC&pg=PA151 |page=151 ''ff''}}
</ref> McGann, De Jaegher, and Di Paolo discuss how this work makes the dynamics of coupling between an agent and its environment, the foundation of enactivism, "an operational, empirically observable phenomenon."<ref name=McGann3>{{cite journal |author1=Marek McGann |author2=Hanne De Jaegher |author3=Ezequiel Di Paolo |year= 2013 |title=Enaction and psychology |journal=Review of General Psychology |volume=17 |issue=2 |pages=203–209 |url= http://www.academia.edu/4993021/Enaction_and_Psychology |doi= 10.1037/a0032935 |quote=Such modeling techniques allow us to explore the parameter space of coupling between agent and environment...to the point that their basic principles (the universals, if such there are, of enactive psychology) can be brought clearly into view.}}
</ref> That is, the AI environment invents examples of enactivism using concrete examples that, although not as complex as living organisms, isolate and illuminate basic principles.

==See also==
{{colbegin}}
*[[Action-specific perception]]
*[[Autopoesis]]
*[[Biosemiotics]]
*[[Cognitive science]]
*[[Cognitive psychology]]
*[[Computational theory of mind]]
*[[Connectivism]]
*[[Cultural psychology]]
*[[Distributed cognition]]
*[[Embodied cognition]]
*[[Embodied embedded cognition]]
*[[Enactive interfaces]]
*[[Extended cognition]]
*[[Extended mind]]
*[[Externalism#Enactivism and embodied cognition]]
*[[Mind–body problem]]
*[[Phenomenology (philosophy)]]
*[[Representationalism]]
*[[Situated cognition]]
*[[Social cognition]]
{{colend}}

==References==
{{reflist|30em|refs=

<ref name=Baldwin>
{{cite book |author=Thomas Baldwin |title=[[Maurice Merleau-Ponty]]: Basic Writings |chapter-url=https://books.google.com/books?id=OS8FM-AFvvsC&pg=PA65 |chapter=Part One: Merleau-Ponty's prospectus of his work |page=65  |quote=Science has not and never will have, by its nature, the same significance ''qua'' form of being as the world which we perceive, for the simple reason that it is a rationale or explanation of that world. |isbn= 978-0415315869 |year=2003 |publisher=Routledge}}
</ref>

<ref name=Burman>
{{cite journal |author=Jeremy Trevelyan Burman  |year=2006 |journal=Journal of Consciousness Studies |title=Book reviews: ''Consciousness & Emotion'' |url=http://www.imprint.co.uk/pdf/13_12_br.pdf  |volume=13 |issue=12 |pages=115–124}}  From a review of {{cite book |title=Consciousness & Emotion: Agency, conscious choice, and selective perception |editor1=Ralph D. Ellis |editor2=Natika Newton |url=https://books.google.com/books?id=LZk6AAAAQBAJ&printsec=frontcover |isbn=9789027294616 |year=2005 |publisher=John Benjamins Publishing}}
</ref>

<ref name=Chiari>
{{cite web |title=Constructivism |author1=Gabriele Chiari |author2=M. Laura Nuzzo |work=The Internet Encyclopaedia of Personal Construct Psychology |url=http://www.pcp-net.org/encyclopaedia/constructivism.html}}
</ref>

<ref name=ClarkA>
{{cite journal |author1=Andy Clark |author2=Josefa Toribio |title=Doing without representing |journal =Synthese |volume=101 |pages=401–434 |year=1994 |url=http://www.philosophy.ed.ac.uk/people/clark/pubs/DoingW-O-rep.pdf |doi=10.1007/bf01063896}}
</ref>

<ref name=ClarkA1>
{{cite journal  |author=Andy Clark   |title=Vision as Dance? Three Challenges for Sensorimotor Contingency Theory |journal= Psyche |volume=12 |issue=1 |date=March 2006 |url= https://www.era.lib.ed.ac.uk/bitstream/1842/1444/1/Psyche%20Clark.pdf}}
</ref>

<ref name=Diettrich>
{{cite book |author=Olaf Diettrich |chapter=The biological boundary conditions for our classical physical world view |title=Evolutionary Epistemology, Language and Culture |page=88 |year=2006 |publisher=Springer |editor1=Nathalie Gontier |editor2=Jean Paul van Bendegem |editor3=Diederik Aerts |isbn=9781402033957 |url=https://books.google.com/books?id=hp2JiTDBbWkC&pg=PA88}}
</ref>

<ref name=Diettrich2>
"The notion of 'truth' is replaced with 'viability' within the subjects' experiential world." From {{cite book |title= The handbook of evolution: The evolution of human societies and culture |author=Olaf Diettrich |chapter=Cognitive evolution; footnote 2 |page=61 |url=https://books.google.com/books?id=Ex5c_pyOsTwC&pg=PA61&lpg=PA61#v=onepage&q&f=false |editor1=Franz M. Wuketits |editor2=Christoph Antweiler |year=2008 |publisher=Wiley-Blackwell}} and in ''Evolutionary Epistemology, Language and Culture'' cited above, p. 90.
</ref>

<ref name=Glaserfeld>
{{cite book |author= Ernst von Glasersfeld |title=Epistemology and education |chapter=Report no. 14: Piaget and the Radical Constructivist Epistemology |url=http://www.vonglasersfeld.com/034 |editor1=CD Smock |editor2=E von Glaserfeld |publisher=Follow Through Publications |pages=1–24 |year=1974}} 
</ref>

<ref name=Glasersfeld2>
{{cite journal |author= Ernst von Glasersfeld |url=http://www.univie.ac.at/constructivism/EvG/papers/118.pdf |title=Cognition, construction of knowledge and teaching |journal=Synthese |volume=80 |issue=1 |pages=121–140 |year=1989 |doi=10.1007/bf00869951}}
</ref>

<ref name=Gontier>
{{cite web |author=Nathalie Gontier |title=Evolutionary Epistemology |url=http://www.iep.utm.edu/evo-epis/ |work=Internet Encyclopedia of Philosophy |year=2006}}
</ref>

<ref name=Hutchins>	
{{cite book |title=Cognition in the Wild |author=Edwin Hutchins |url= |isbn=9780262581462 |year=1996 |page=428 |publisher=MIT Press }} Quoted by {{cite journal |title=Cognitive, embodied or enacted? :Contemporary perspectives for HCI and interaction  |url=http://trans-techresearch.net/wp-content/uploads/2010/11/Rocha-01.pdf |author=Marcio Rocha |year=2011 |publisher=Transtechnology Research Reader |isbn=978-0-9538332-2-1}}	
</ref>

<ref name=Jaegher1>
{{cite book |author1=Ezequiel A Di Paolo |author2=Marieke Rhohde |author3=Hanne De Jaegher |chapter=Horizons for the enactive mind: Values, social interaction, and play |title=Enaction: Toward a New Paradigm for Cognitive Science |editor1=John Stewart |editor2=Oliver Gapenne |editor3=Ezequiel A Di Paolo |url=https://books.google.com/books?id=UtFDJx-gysQC&pg=PA39 |pages=33 ''ff'' |isbn=  978-0262526012 |publisher=MIT Press |year=2014}}
</ref>

<ref name=Manetti>
A collection of papers on this topic is introduced by {{cite journal |title=Agency: From embodied cognition to free will |author1=Duccio Manetti |author2=Silvano Zipoli Caiani |journal=Humana Mente |volume=15 |date=January 2011 |pages=''V''-''XIII'' |url=http://www.humanamente.eu/PDF/Issue15_CompletePDF.pdf}}
</ref>

<ref name=Maturana>
{{cite book |author1=Humberto R Maturana |author2=Francisco J Varela |year=1992 |title= The tree of knowledge: the biological roots of human understanding |edition=Revised |publisher=Shambhala Publications Inc |chapter=Afterword |page=255 |isbn=978-0877736424}}
</ref>

<ref name=Munz>
{{cite book |url=https://books.google.com/books?id=tMuIAgAAQBAJ&pg=PA154&lpg=PA154 |page=154 |author=Peter Munz |title=Philosophical Darwinism: On the Origin of Knowledge by Means of Natural Selection |year=2002 |isbn=9781134884841 |publisher=Routledge}}
</ref>

<ref name=Mutelesi>
{{cite journal |title=Radical constructivism seen with Edmund Husserl as starting point |author=Edmond Mutelesi |url=http://www.univie.ac.at/constructivism/journal/2/1/006.mutelesi |journal=Constructivist foundations |volume=2 |issue=1 |pages=6–16 |date=November 15, 2006}}
</ref>

<ref name=Rohde>
{{cite book |title=Enaction, Embodiment, Evolutionary Robotics: Simulation Models for a Post-Cognitivist Science of Mind  |chapter= §3.1 The scientist as observing subject |pages=30 ''ff'' |author=Marieke Rohde |isbn=978-9078677239 |publisher=Atlantis Press |year=2010 |url=https://books.google.com/books?id=LlpZjLMPiHYC&pg=PA30}}
</ref>

<ref name=Rowlands>
{{cite book |author=Mark Rowlands |chapter=Chapter 3: The mind embedded §5 The mind enacted |pages=70 ''ff'' |year=2010 |isbn=0262014556 |publisher=MIT Press |url=https://books.google.com/books?id=AiwjpL-0hDgC&pg=PA70 |title=The new science of the mind: From extended mind to embodied phenomenology}} Rowlands attributes this idea to {{cite book |author=D M MacKay |year=1967 |chapter=Ways of looking at perception |title=Models for the perception of speech and visual form (Proceedings of a symposium) |editor=W Watthen-Dunn |publisher=MIT Press |pages=25 ''ff'' |url=https://books.google.com/books?id=Ts9JAAAAMAAJ&focus=searchwithinvolume&q=MacKay+Ways+of+looking+at+perception}}
</ref>

<ref name=EThompson>
{{cite book |title= Mind in life |chapter=The enactive approach |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=13 ''ff'' |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA13 |publisher=Harvard University Press |year=2007 }}  ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here]
</ref>

<ref name=EThompson2>
{{cite book |title= Mind in life |chapter=Autonomy and emergence |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |pages=37 ''ff'' |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA13 |publisher=Harvard University Press |year=2007}} See also the Introduction, p. ''x''.
</ref>

<ref name=EThompson3>
{{cite book |title= Mind in life |chapter=Chapter 8: Life beyond the gap |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=225  |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA225 |publisher=Harvard University Press |year=2007}}
</ref>

<ref name=EThompson4>
{{cite book |title= Mind in life |chapter=Life can be known only by life |author=Evan Thompson |isbn=978-0674057517 |edition=Paperback |page=165  |url=https://books.google.com/books?id=OVGna4ZEpWwC&pg=PA165 |publisher=Harvard University Press |year=2007}}
</ref>

<ref name="Evan Thompson">
{{cite book |title=Mind in life:Biology, phenomenology, and the sciences of mind |author=Evan Thompson |url=http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf |publisher=Harvard University Press |isbn= 978-0674057517 |chapter=Chapter 1: The enactive approach |year=2010}} ToC, first 65 pages, and index [http://lchc.ucsd.edu/MCA/Mail/xmcamail.2012_03.dir/pdf3okBxYPBXw.pdf found here].
</ref>

<ref name=Tascano0>
{{cite book |title=A Dictionary of Continental Philosophy |editor=John Protevi |url=https://books.google.com/books?id=kRUZ61uISUMC&pg=PA169 |pages=169–170 |chapter = Enaction |isbn=9780300116052 |publisher=Yale University Press |year=2006}}
</ref>

<ref name=Varela>
{{cite book |title=The embodied mind: Cognitive science and human experience |author1=Francisco J Varela |author2=Evan Thompson |author3=Eleanor Rosch |url=https://books.google.com/books?id=QY4RoH2z5DoC&printsec=frontcover#v=snippet&q=We%20propose%20as%20a%20%20name%20the%20term%20%20enactive&f=false |year=1992 |publisher=MIT Press |page=9 |quote= |isbn=978-0262261234}}</ref>

<ref name=Ward2>
"The underpinnings of cognition are inextricable from those of affect, that the phenomenon of cognition itself is essentially bound up with affect.." See p. 104: {{cite book |author1=Dave Ward |author2=Mog Stapleton |year=2012 |url=https://books.google.com/books?id=Y1E7FogqvJ0C&pg=PA89 |chapter=Es are good. Cognition as enacted, embodied, embedded, affective and extended |editor= Fabio Paglieri |title=Consciousness in Interaction: The role of the natural and social context in shaping consciousness |publisher=John Benjamins Publishing |pages=89 ''ff'' |isbn=978-9027213525}} [http://philpapers.org/archive/WAREAG.pdf On-line version here].
</ref>

<ref name=RWilson>
{{cite web |author1=Robert A Wilson |author2=Lucia Foglia |title=Embodied Cognition: §2.2 Enactive cognition |work=The Stanford Encyclopedia of Philosophy (Fall 2011 Edition) |editor=Edward N. Zalta |url = http://plato.stanford.edu/archives/fall2011/entries/embodied-cognition/#EnaCog |date=July 25, 2011}}
</ref>

}}

==Further reading==
* {{cite journal |author1=De Jaegher H. |author2=Di Paolo E. A. | year = 2007 | title = Participatory sense-making: An enactive approach to social cognition | url = | journal = Phenomenology and the Cognitive Sciences | volume = 6 | issue = 4| pages = 485–507 | doi=10.1007/s11097-007-9076-9}}
* Di Paolo, E. A., Rohde, M. and De Jaegher, H., (2010). ''Horizons for the Enactive Mind: Values, Social Interaction, and Play.'' In J. Stewart, O. Gapenne and E. A. Di Paolo (eds), Enaction: Towards a New Paradigm for Cognitive Science, Cambridge, MA: MIT Press, pp.&nbsp;33 – 87. ISBN 9780262014601
* [[Daniel Hutto|Hutto, D. D.]] (Ed.)  (2006).  ''Radical Enactivism: Intentionality, phenomenology, and narrative.''  In R. D. Ellis & N. Newton (Series Eds.), ''Consciousness & Emotion, vol. 2.'' ISBN 90-272-4151-1
* McGann, M. & Torrance, S. (2005).  Doing it and meaning it (and the relationship between the two).  In R. D. Ellis & N. Newton, ''Consciousness & Emotion, vol. 1: Agency, conscious choice, and selective perception''. Amsterdam: John Benjamins. ISBN 1-58811-596-8
* {{cite journal |title=The enactive approach: Theoretical sketches from cell to society |author1=Tom Froese |author2=Ezequiel A DiPaolo |citeseerx = 10.1.1.224.5504 |journal=Pragmatics and Cognition |volume=19 |issue=1 |year=2011 |pages=1–36 |doi=10.1075/pc.19.1.01fro}}
* {{cite journal |author1=Steve Torrance |author2=Tom Froese |title=An inter-enactive approach to agency: participatory sense-making, dynamics, and sociality. |journal=Humana. Mente |volume=15 |year=2011 |pages=21–53 |citeseerx = 10.1.1.187.1151 }}

==Notes==
{{reflist |group=Note |refs=
<ref group=Note name=Cognitivism>
Cognition as information processing like that of a digital computer. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} ''Cognitivism'', p. 4; See also {{cite web |title=The computational theory of mind |author=Steven Horst |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/spr2011/entries/computational-mind/ |date=December 10, 2009}}
</ref>

<ref group=Note name=Connectionism>
Cognition as emergent patterns of activity in a neural network. From {{cite book |author=Evan Thompson |title=Mind in Life |isbn=978-0674057517}} ''Connectionism'', p. 8; See also {{cite web |title=Connectionism |author=James Garson |work= The Stanford Encyclopedia of Philosophy (Spring 2011 Edition) |editor=Edward N. Zalta |url=http://plato.stanford.edu/archives/win2012/entries/connectionism/ |date=July 27, 2010}}
</ref>

}}

==External links==
*{{cite web |title=Consciousness as the emergent property of the interaction between brain, body, & environment: the crucial role of haptic perception  |author=Pietro Morasso |url=http://www.consciousness.it/iwac2005/Material/Morasso.pdf |year=2005 }} Slides related to a chapter on [[haptic perception]] (recognition through touch): {{cite book |editor1=Antonio Chella |editor2=Riccardo Manzotti |author=Pietro Morasso |chapter=Chapter 14: The crucial role of haptic perception |page=234 ''ff'' |title= Artificial Consciousness |publisher= Academic  |year=2007 |isbn=978-1845400705 |url=https://www.google.com/search?tbo=p&tbm=bks&q=isbn:1845400704&num=10}}
*{{cite web |title=Questioning Life and Cognition: Some Foundational Issues in the Paradigm of Enaction |url=http://www.enactionseries.com/library/bookjs/co/Original_book_JS.html#Pk1qsEYBVxgUwAM6tVeiff |author=John Stewart |work=Enaction Series: Online Collaborative Publishing |editor1=Olivier Gapenne |editor2=Bruno Bachimont |publisher=Enaction Series |accessdate=April 27, 2014}} 
*{{cite web |title=Educational Multimedia Task Force – MM 1045, REPRESENTATION |url=http://halshs.archives-ouvertes.fr/docs/00/00/18/64/PDF/REPRDel1.pdf |publisher= |author1=George-Louis Baron |author2=Eric Bruillard |author3=Christophe Dansac |date=January 1999}} An overview of the rationale and means and methods for the study of representations that the learner constructs in his/her attempt to understand knowledge in a given field. See in particular §1.2.1.4 ''Toward social representations'' (p.&nbsp;24)
*{{cite web |author=Randall Whittaker |year=2001 |title=Autopoiesis and enaction |url=http://www.enolagaia.com/AT.html |publisher=Observer Web}} An extensive but uncritical introduction to the work of [[Francisco Varela]] and [[Humberto Maturana]]
*{{cite journal|title=Enactivism: Arguments & Applications.|journal=Avant|date=Autumn 2014|volume= V| issue =  2/2014|doi=10.12849/50202014.0109.0002|url=http://avant.edu.pl/en/22014-2|accessdate=27 November 2014}} Entire journal issue on enactivism's status and current debates.

[[Category:Behavioral neuroscience]]
[[Category:Cognitive science]]
[[Category:Consciousness]]
[[Category:Educational psychology]]
[[Category:Enactive cognition]]
[[Category:Epistemology of science]]
[[Category:Knowledge representation]]
[[Category:Metaphysics of mind]]
[[Category:Motor cognition]]
[[Category:Neuropsychology]]
[[Category:Perception]]
[[Category:Philosophical theories]]
[[Category:Philosophy of psychology]]
[[Category:Psychological concepts]]
[[Category:Psychological theories]]
[[Category:Sociology of knowledge]]
[[Category:Action (philosophy)]]
<=====doc_Id=====>:876
<=====title=====>:
Enterprise Interoperability Framework
<=====text=====>:
{{Underlinked|date=June 2014}}
The '''Enterprise Interoperability Framework''' is used as a guideline for collecting and structuring knowledge/solution for [[enterprise interoperability]]. The Enterprise Interoperability Framework defines the domain and sub-domains for [[interoperability]] research and development in order to identify a set of pieces of knowledge for solving enterprise interoperability problems by removing barriers to interoperability.

==Existing Interoperability Frameworks==

Some existing works on interoperability have been carried out to define interoperability framework or reference models, in particular, the LISI <ref name="C4ISR1998"/> reference model, [[European Interoperability Framework]] (EIF),<ref name="EIF2004"/> IDEAS interoperability framework,<ref name=" IDEAS2003"/> [[Model Driven Interoperability|ATHENA]] interoperability framework,<ref name="ATHENA2003"/> and E-Health Interoperability Framework.<ref name="NEHTA2006"/> These existing approaches constitute the basis for the Enterprise Interoperability Framework.

The necessity to elaborate the Enterprise Interoperability Framework has been discussed in.<ref name="INTEROP"/> Existing interoperability frameworks do not explicitly address barriers to interoperability, which is a basic assumption of this research; they are not aimed at structuring interoperability knowledge with respect to their ability to remove various barriers.

The Enterprise Interoperability framework has three basic [[dimensions]]:

# Interoperability concerns defined the content (or aspect) of interoperation that may take place at various levels of the [[Business|enterprise]]. In the domain of Enterprise Interoperability, the following four interoperability concerns are identified : Data, Service, Process, and Business.<ref name=" Guglielmina2005"/> [[File:Interoperability Concerns Data, Service, Process, and Business.jpg|thumb|Interoperability Concerns:  Data, Service, Process, and Business]]
# Interoperability barriers: Interoperability barrier is a fundamental concept in defining the interoperability domain. Many interoperability issues are specific to particular application domains. These can be things like support for particular attributes, or particular access control regimes. Nevertheless, general barriers and problems of interoperability can be identified; and most of them being already addressed,<ref name="EIF2004"/><ref name=" Kasunic2004"/><ref name="ERISA2004"/> Consequently, the objective is to identify common barriers to interoperability. By the term ‘barrier’ we mean an ‘incompatibility’ or ‘mismatch’ which obstructs the sharing and exchanging of information. Three categories of barriers are identified: conceptual, technological and organisational.
# Interoperability approaches represents the different ways in which barriers can be removed (integrated, unified, and federated) 
 [[File:Basic Approaches to Develop Interoperability.jpg|thumb|Basic Approaches to Develop Interoperability]]

The Enterprise Interoperability Framework with its three basic dimensions is shown.
[[File:Enterprise Interoperability Framework.jpg|thumb|Enterprise Interoperability Framework]]

== Enterprise Interoperability Framework Use ==

The Enterprise Interoperability Framework allows to:

* Capture and structure interoperability knowledge/solutions in the framework through a barrier-driven approach 
* Provide support to enterprise interoperability engineers and industry end users to carry out their interoperability projects.

The Enterprise Interoperability Framework not only aims at structuring concepts, defining research domain and capturing knowledge, but also at helping [[industry|industries]] to solve their interoperability problems. When carrying out an interoperability project involving two particular enterprises, interoperability concerns and interoperability barriers between the two enterprises will be identified first and mapped to this Enterprise Interoperability Framework. Using the [[Enterprise architecture framework|framework]], existing interoperability degree can be characterised and targeted interoperability degree can be defined as the [[Goal|objective]] to meet. Then knowledge/solutions associated to the barriers and concerns can be searched in the framework, and solutions found will be proposed to users for possible adaptation and/or combination with other solutions to remove the identified barriers so that the required interoperability can be established.

== References ==
{{reflist|
refs=
<ref name= C4ISR1998>C4ISR (1998), Architecture Working Group (AWG), Levels of Information Systems Interoperability (LISI), 30 March 1998.</ref>
<ref name= EIF2004>EIF (2004), European Interoperability Framework for PAN-European EGovernment services, IDA working document - Version 4.2 – January 2004.</ref>
<ref name= IDEAS2003>IDEAS (2003), IDEAS Project Deliverables (WP1-WP7), Public reports, www.ideas-road map.net.</ref>
<ref name= ATHENA2003 >ATHENA (2003): Advanced Technologies for Interoperability of Heterogeneous Enterprise Networks and their Applications, FP6-2002-IST-1, Integrated Project Proposal, April 2003.  Deriverable.</ref>
<ref name=NEHTA2006>NEHTA (2006), Towards a Health Interop Framework, ({{cite web|url=http://www |title=Archived copy |accessdate=2011-05-24 |deadurl=yes |archiveurl=https://web.archive.org/web/20060326070849/http://www |archivedate=2006-03-26 |df= }}. providersedge.com/ehdocs/.../Towards_an_Interoperability_Framework.pdf)</ref>
<ref name= INTEROP >INTEROP D1.1, Knowledge map of research in interoperability in the INTEROP NoE, WP1, Version 1, August 11th 2004.</ref>
<ref name= Guglielmina2005>C. Guglielmina and A. Berre, Project A4 (Slide presentation), ATHENA Intermediate Audit 29.-30. September 2005, Athens, Greece.</ref>
<ref name= Kasunic2004>Kasunic, M., Anderson, W.,: Measuring systems interoperability: challenges and opportunities, Software engineering measurement and analysis initiative, April 2004</ref>
<ref name= EIF2004>EIF: European Interoperability Framework, Write Paper, Brussels, 18, Feb. 2004, http://www.comptia.org</ref>
<ref name= ERISA2004>ERISA (The European Regional Information Society Association), A guide to Interoperability for Regional Initiatives, Brussels, September 2004.</ref>
}}

== External links ==
* [http://www.interop-vlab.eu INTEROP-VLab]
* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-2-enterprise-interoperability-framework-and-knowledge-corpus/ DI.2.Enterprise Interoperability Framework and knowledge corpus]
* [http://interop-vlab.eu/ei_public_deliverables/interop-noe-deliverables/di-domain-interoperability/di-3_enterprise-interoperability-framework-and-knowledge-corpus/ DI.3.Enterprise Interoperability Framework and knowledge corpus]

[[Category:Interoperability]]
[[Category:Enterprise modelling]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:879
<=====title=====>:
Closed-world assumption
<=====text=====>:
{{more footnotes|date=August 2011}}
The '''closed-world assumption''' (CWA), in a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], is the presumption that a statement that is true is also known to be true. Therefore, conversely, what is not currently known to be true, is false. The same name also refers to a [[formal logic|logical]] formalization of this assumption by [[Raymond Reiter]]. The opposite of the closed-world assumption is the [[open-world assumption]] (OWA), stating that lack of knowledge does not imply falsity. Decisions on CWA vs. OWA determine the understanding of the actual semantics of a conceptual expression with the same notations of concepts. A successful formalization of natural language semantics usually cannot avoid an explicit revelation of whether the implicit logical backgrounds are based on CWA or OWA.

[[Negation as failure]] is related to the closed-world assumption, as it amounts to believing false every predicate that cannot be proved to be true.

== Example ==

In the context of [[knowledge management]], the closed-world assumption is used in at least two situations: (1) when the knowledge base is known to be complete (e.g., a corporate database containing records for every employee), and (2) when the knowledge base is known to be incomplete but a "best" definite answer must be derived from incomplete information. For example, if a [[database]] contains the following table reporting editors who have worked on a given article, a query on the people not having edited the article on Formal Logic is usually expected to return "Sarah Johnson".

{| border="1" cellspacing="0" cellpadding="2" align="center"
! colspan="2" style="background:#ffdead;" | Edit
|-
! style="background:#efefef;" | Editor
! style="background:#efefef;" | Article
|-
| John Doe || Formal Logic
|-
| Joshua A. Norton || Formal Logic
|-
| Sarah Johnson || Introduction to Spatial Databases
|-
| Charles Ponzi || Formal Logic
|-
| Emma Lee-Choon || Formal Logic
|}
<br /> <!-- The preceding tag creates a whitespace line separating the table above from the text paragraph below -->
In the closed-world assumption, the table is assumed to be [[Complete knowledge base|complete]] (it lists all editor-article relationships), and Sarah Johnson is the only editor who has not edited the article on Formal Logic. In contrast, with the open-world assumption the table is not assumed to contain all editor-article tuples, and the answer to who has not edited the Formal Logic article is unknown. There is an unknown number of editors not listed in the table, and an unknown number of articles edited by Sarah Johnson that are also not listed in the table.

==Formalization in logic==

The first formalization of the closed-world assumption in [[logic|formal logic]] consists in adding to the knowledge base the negation of the literals that are not currently [[logical consequence|entailed]] by it. The result of this addition is always [[consistent]] if the knowledge base is in [[Horn clause|Horn form]], but is not guaranteed to be consistent otherwise. For example, the knowledge base
:<math>\{English(Fred) \vee Irish(Fred)\}</math>
entails neither <math>English(Fred)</math> nor <math>Irish(Fred)</math>.

Adding the negation of these two literals to the knowledge base leads to
:<math>\{English(Fred) \vee Irish(Fred), \neg English(Fred), \neg Irish(Fred)\}</math>
which is inconsistent. In other words, this formalization of the closed-world assumption sometimes turns a consistent knowledge base into an inconsistent one. The closed-world assumption does not introduce an inconsistency on a knowledge base <math>K</math> exactly when the intersection of all [[Herbrand model]]s of <math>K</math> is also a model of <math>K</math>; in the propositional case, this condition is equivalent to <math>K</math> having a single minimal model, where a model is minimal if no other model has a subset of variables assigned to true.

Alternative formalizations not suffering from this problem have been proposed. In the following description, the considered knowledge base <math>K</math> is assumed to be propositional. In all cases, the formalization of the closed-world assumption is based on adding to <math>K</math> the negation of the formulae that are “free for negation” for <math>K</math>, i.e., the formulae that can be assumed to be false. In other words, the closed-world assumption applied to a [[propositional formula]] <math>K</math> generates the formula:
:<math>K \wedge \{\neg f ~|~ f \in F\}</math>.
The set <math>F</math> of formulae that are free for negation in <math>K</math> can be defined in different ways, leading to different formalizations of the closed-world assumption. The following are the definitions of <math>f</math> being free for negation in the various formalizations.

; CWA (closed-world assumption) : <math>f</math> is a positive literal not entailed by <math>K</math>;

; GCWA (generalized CWA) : <math>f</math> is a positive literal such that, for every positive clause <math>c</math> such that <math>K \not\vdash c</math>, it holds <math>K \not\vdash c \vee f</math>;<ref>{{Citation
 | first = Jack | last = Minker
 | author-link = Jack Minker
 | title = On indefinite databases and the closed world assumption
 | publisher = [[Springer Berlin Heidelberg]]
 | series = Lecture Notes in Computer Science
 | volume = 138
 | year = 1982
 | pages = 292–308
 | url = http://link.springer.com/chapter/10.1007/BFb0000066
 | doi = 10.1007/BFb0000066
 | isbn = 978-3-540-11558-8 }}</ref>

; EGCWA (extended GCWA): same as above, but <math>f</math> is a conjunction of positive literals;

; CCWA (careful CWA): same as GCWA, but a positive clause is only considered if it is composed of positive literals of a given set and (both positive and negative) literals from another set;

; ECWA (extended CWA): similar to CCWA, but <math>f</math> is an arbitrary formula not containing literals from a given set.

The ECWA and the formalism of [[Circumscription (logic)|circumscription]] coincide on propositional theories. The complexity of query answering (checking whether a formula is entailed by another one under the closed-world assumption) is typically in the second level of the [[polynomial hierarchy]] for general formulae, and ranges from [[P (complexity)|P]] to [[coNP]] for [[Horn clause|Horn formulae]]. Checking whether the original closed-world assumption introduces an inconsistency requires at most a logarithmic number of calls to an [[Oracle machine|NP oracle]]; however, the exact complexity of this problem is not currently known.

==See also==

* [[Open-world assumption]]
* [[Non-monotonic logic]]
* [[Circumscription (logic)]]
* [[Negation as failure]]
* [[Default logic]]
* [[Stable model semantics]]
* [[Unique name assumption]]

==References==
{{Reflist}}
*{{cite journal |last1=Cadoli |first1=Marco |last2=Lenzerini |first2=Maurizio |title=The complexity of propositional closed world reasoning and circumscription |journal=Journal of Computer and System Sciences |date=April 1994 |volume=48 |issue=2 |pages=255–310 |doi=10.1016/S0022-0000(05)80004-2 |url=http://www.sciencedirect.com/science/article/pii/S0022000005800042 |accessdate=20 February 2013 |issn=0022-0000}}
*{{cite journal |last1=Eiter |first1=Thomas |last2=Gottlob |authorlink2=Georg Gottlob |first2=Georg |title=Propositional circumscription and extended closed-world reasoning are <math>\Pi^p_2</math>-complete |journal=Theoretical Computer Science |date=June 1993 |volume=114 |issue=2 |pages=231–245 |doi=10.1016/0304-3975(93)90073-3 |url=http://www.sciencedirect.com/science/article/pii/0304397593900733 |accessdate=20 February 2013 |issn=0304-3975}}
*{{cite journal |last1=Rajasekar |first1=Arcot |last2=Lobo |first2=Jorge |last3=Minker |first3=Jack |authorlink3=Jack Minker |title=Weak Generalized Closed World Assumption |journal=Journal of Automated Reasoning |date=September 1989 |volume=5 |issue=3 |pages=293–307 |doi=10.1007/BF00248321 |url=http://link.springer.com/article/10.1007/BF00248321 |accessdate=20 February 2013 |publisher=Kluwer Academic Publishers |issn=0168-7433}}
*{{cite journal |last=Lifschitz |first=Vladimir |authorlink=Vladimir Lifschitz |title=Closed-world databases and circumscription |journal=Artificial Intelligence |date=November 1985 |volume=27 |issue=2 |pages=229–235 |doi=10.1016/0004-3702(85)90055-4 |url=http://www.sciencedirect.com/science/article/pii/0004370285900554 |accessdate=20 February 2013 |issn=0004-3702}}
*{{cite book |last=Reiter |first=Raymond |authorlink=Raymond Reiter |editor1-last=Gallaire |editor1-first=Hervé |editor2-last=Minker |editor2-first=Jack |editor2-link=Jack Minker |title=Logic and Data Bases |year=1978 |publisher=Plenum Press |isbn=9780306400605 |url=http://www.springer.com/computer/security+and+cryptology/book/978-0-306-40060-5 |accessdate=21 February 2013 |chapter=On Closed World Data Bases |pages=119–140}}
*{{cite journal |last1=Duan |first1=Yucong |last2=Cruz |first2=Christophe |title=Formalizing Semantic of Natural Language through Conceptualization from Existence |journal=International Journal of Innovation, Management and Technology |date=February 2011 |volume=2 |issue=1 |pages=37–42 |doi=10.7763/IJIMT.2011.V2.100 |url=http://ijimt.org/abstract/100-E00187.htm |accessdate=21 February 2013 |issn=2010-0248}}

==External links==
* https://web.archive.org/web/20090624113015/http://www.betaversion.org:80/~stefano/linotype/news/91/
* [http://owl1-1.googlecode.com/svn-history/r374/trunk/www.webont.org/owled/2005/sub12.pdf Closed World Reasoning in the Semantic Web through Epistemic Operators]
* [http://books.hammerpig.com/the-closed-world-assumption-of-databases.html Excerpt from Reiter's 1978 talk on the closed world assumption]

{{DEFAULTSORT:Closed-world assumption}}
[[Category:Logic programming]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:882
<=====title=====>:
Babelfy
<=====text=====>:
{{multiple issues|
{{context|date=August 2016}}
{{notability|Products|date=August 2016}}
}}
{{Infobox software
 |name = Babelfy
 |logo = [[File:Babelfy_logo.png|140px|Babelfy logo.]]
 |screenshot =
 |caption = Babelfy
 |developer = 
 |released = 
 |latest_release_version = Babelfy 1.0
 |latest_release_date = June 2014
 |genre = {{Flatlist|
* [[Word sense disambiguation]]
* [[Entity linking]]
}}
 |programming language = 
 |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]
 |website = {{URL|babelfy.org}}
 |alexa   = 
}}

'''Babelfy''' is an [[algorithm]] for the disambiguation of text written in any language. Specifically, Babelfy performs the tasks of [[Multilinguality|multilingual]] [[Word Sense Disambiguation]] (i.e., the disambiguation of common nouns, verbs, adjectives and adverbs) and [[Entity Linking]] (i.e. the disambiguation of mentions to encyclopedic entities like people, companies, places, etc.).<ref>A. Moro, A. Raganato, R. Navigli. [http://wwwusers.di.uniroma1.it/~navigli/pubs/TACL_2014_Babelfy.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.</ref> Babelfy is based on the [[BabelNet]] multilingual semantic network and performs disambiguation and entity linking in three steps:

* It associates with each [[Vertex (graph theory)|vertex]] of the BabelNet semantic network, i.e., either [[concept]] or [[named entity]], a semantic signature, that is, a set of related vertices. This is a preliminary step which needs to be performed only once, independently of the input text.
* Given an input [[Written text|text]], it extracts all the linkable fragments from this text and, for each of them, lists the possible [[meaning (linguistics)|meanings]] according to the [[semantic network]].
* It creates a [[Graph (data structure)|graph-based]] semantic interpretation of the whole text by linking the candidate meanings of the extracted fragments using the previously-computed semantic signatures. It then extracts a dense [[Glossary of graph theory#Subgraphs|subgraph]] of this representation and selects the best candidate meaning for each fragment.

As a result, the text, written in any of the 271 [[language]]s supported by BabelNet, is output with possibly overlapping semantic annotations.

==See also==
* [[BabelNet]]
* [[Entity linking]]
* [[Multilinguality]]
* [[Word sense disambiguation]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://babelfy.org}}

[[Category:Lexical semantics]]
[[Category:Semantics]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Artificial intelligence]]
[[Category:Multilingualism]]


{{prog-lang-stub}}
<=====doc_Id=====>:885
<=====title=====>:
Flint toolkit
<=====text=====>:
{{COI|date=September 2014}}
Flint is a software toolkit which supports various modes of uncertainty handling, namely, [[Fuzzy_logic| Fuzzy]], [[Bayesian_inference| Bayesian]] and [[Expert_system#Certainty_factors| Certainty Theory]].

Along with [[Flex_expert_system| Flex]], Flint was licensed to the Open University as part of T396: 'Artificial intelligence for technology'.

Much of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.<ref name = "AI Toolkit">{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists}} by Adrian Hopgood </ref>

Flint is produced by [[Logic Programming Associates|LPA]] and runs on PCs and web servers.

==External links==
*[http://www.generation5.org/content/2003/lpainference.asp The Shape of Inference] by Clive Spenser & Charles Langley, Generation5
*[http://www.generation5.org/content/2003/probmodulation.asp Probability Modulation and Non-linearity in Bayesian Networks] by Clive Spenser & Charles Langley, Generation5
*[http://www.generation5.org/content/2004/defuzz.asp Defuzzification Options in Flex] by Clive Spenser & Charles Langley, Generation5
*[http://www.degruyter.com/view/j/pomr.2007.14.issue-3/v10012-007-0012-2/v10012-007-0012-2.xml Application of fuzzy inference to assessment of degree of hazard to ship power plant operator] by Tomasz Kowalewski, Antoni Podsiadło & Wiesław Tarełko
*[http://www.slaai.lk/proc/2007/2007.pdf#page=42 Computational Modeling in Conceptual Models: Widening Scope of Artificial Life] by Mendis, Asoka. S. Karunananda, Samaratunga
*[http://www.lpa.co.uk/fln.htm Flint Overview], LPA

== References ==
{{Reflist}}

[[Category:Expert_systems]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}
<=====doc_Id=====>:888
<=====title=====>:
WYSIWYM (interaction technique)
<=====text=====>:
{{for|the editing paradigm|WYSIWYM}}

'''What you see is what you meant''' ('''WYSIWYM''') is a text editing [[interaction technique]] that emerged from two projects at [[University of Brighton]]. It allows users to create abstract [[knowledge representation]]s such as those required by the [[Semantic Web]] using a natural language interface. [[Natural language understanding]] (NLU) technology is not employed. Instead,  [[natural language generation]] (NLG) is used in a highly interactive manner.

The text editor accepts repeated refinement of a selected span of text as it becomes increasingly less vacuous of authored semantics. Using a mouse, a text property held in the evolving text can be further refined by a set of options derived by NLG from a built-in [[ontology]]. An invisible representation of the semantic knowledge is created which can be used for multilingual document generation, formal knowledge formation, or any other task that requires formally specified information.<ref>{{Cite web|url = http://oro.open.ac.uk/39116/1/Thesis_Final.pdf|title = Generating Natural Language Explanations For Entailments In Ontologies|date = 2013|accessdate = 10 November 2014|website = Open Research Online|publisher = The Open University|last = Nguyen|first = Tu}}</ref>

The two projects at Brighton worked in the field of Conceptual Authoring to lay a foundation for further research and development of a Semantic Web Authoring Tool (SWAT). This tool has been further explored as a means for developing a knowledge base by those without prior experience with Controlled Natural Language tools.<ref>{{Cite journal|url = http://oro.open.ac.uk/40385/|title = How easy is it to learn a controlled natural language for building a knowledge base?|last = Williams|first = Sandra|date = 13 June 2014|journal = Fourth Workshop on Controlled Natural Language, 20–22 August 2014, Galway, Ireland (forthcoming), Springer International Publishing AG.|accessdate = 10 November 2014|doi = |pmid = }}</ref>

==See also==
*[[Semantic markup]]
* [[Web Ontology Language|OWL [Web Ontology Language]]]
* [[WYSIWYM]]
* [[Protégé (software)]]

== References ==
<references />

==External links==
* Nguyen, Tu (2013).  ''[http://oro.open.ac.uk/39116/ Generating Natural Language Explanations For Entailments In Ontologies.]  ''PhD thesis The Open University.
* [http://mcs.open.ac.uk/nlg/research/Conceptual_Authoring.html Conceptual Authoring] at Natural Language Generation group of the Open University
* [http://mcs.open.ac.uk/nlg/SWAT/ SWAT: Semantic Web Authoring Tool] research project
* [http://mcs.open.ac.uk/nlg/old_projects/wysiwym/ WYSIWYM home page]

{{DEFAULTSORT:WYSIWYM (interaction technique)}}
[[Category:Knowledge representation]]
[[Category:Natural language generation]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Semantic Web]]


{{software-stub}}
<=====doc_Id=====>:891
<=====title=====>:
Issue-Based Information System
<=====text=====>:
'''Issue-Based Information System''' (IBIS)  was invented by Werner Kunz and  [[Horst Rittel]] as an argumentation-based approach to tackling [[wicked problem]]s &ndash; complex, ill-defined problems that involve multiple [[stakeholder (corporate)|stakeholders]].<ref>Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe für Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)</ref> 

To quote from their original paper, ''"Issue-Based Information Systems (IBIS) are meant to support coordination and [[planning]] of [[political]] decision processes. IBIS guides the identification, structuring, and settling of issues raised by problem-solving groups, and provides information pertinent to the [[discourse]]..."''. 

Subsequently, the understanding of [[planning]] and [[design]] as a process of [[argumentation]] (of the designer with himself or with others) has led to the use of IBIS as a [[design rationale]].<ref>Noble, Douglas and Rittel, Horst W.J.  1988, Issue-Based Information Systems for Design, Proceedings of the ACADIA `88 Conference, Association for Computer Aided Design in Architecture, University of Michigan, October 1988. Also published as Working Paper #492, Institute of Urban and Regional Development, College of Environmental Design, University of California, Berkeley. November 1988.</ref>

The basic structure of IBIS is a [[Graph (discrete mathematics)|graph]]. It is therefore quite suitable to be manipulated by [[computer]].

==Overview==
The elements of IBIS are issues (or questions that need to be answered), each of which are associated with alternative positions (or possible answers).  These in turn are associated with arguments which support or object to a given position (or another argument).  In the course of the treatment of issues, new issues come up which are treated likewise.

Issue-Based Information Systems are used as a means of widening the coverage of a problem.  By encouraging a greater degree of participation, particularly in the earlier phases of the process, the designer is increasing the opportunity that difficulties of his proposed solution, unseen by him, will be discovered by others.  Since the problem observed by a designer can always be treated as merely a symptom of another higher-level problem, the argumentative approach also increases the likelihood that someone will attempt to attack the problem from this point of view.  Another desirable characteristic of the Issue-Based Information System is that it helps to make the design process “transparent.”  Transparency here refers to the ability of observers as well as participants to trace back the process of decision-making. 

IBIS is used in issue mapping,<ref>Okada, A., Shum, S.J.B. and Sherborne, T. (Eds.),  "Knowledge Cartography: software tools and mapping techniques,"  Springer;  2008, ISBN 978-1-84800-148-0</ref>  an argument visualization technique related to [[argument mapping]]. It is also the basis of a facilitation technique called dialogue mapping.<ref>Conklin, J., "Dialog Mapping: Reflections on an Industrial Strength Case Study", in Visualizing Argumentation – Tools for Collaborative and Educational Sense-Making, P. Kirschner, S.J.B Shum,C.S. Carr (Eds), Springer-Verlag, London (2003)</ref>

==History==
Rittel’s interest lay in the area of public policy and planning, which is also the context in which he defined [[wicked problem]]s.<ref>Rittel, Horst, and Melvin Webber; "Dilemmas in a General Theory of Planning," pp. 155-169, Policy Sciences, Vol. 4, Elsevier Scientific Publishing Company, Inc., Amsterdam, 1973. Reprinted in N. Cross (ed.), Developments in Design Methodology, J. Wiley & Sons, Chichester, 1984, pp. 135-144</ref> So it is no surprise that Rittel and Kunz envisaged IBIS as the: 

''"...type of information system meant to support the work of cooperatives like governmental or administrative agencies or committees, planning groups, etc., that are confronted with a problem complex in order to arrive at a plan for decision..."''.<ref>Werner, Kunz and Rittel, Horst, Issues as Elements of Information Systems, Working paper No. 131, Studiengruppe für Systemforschung, Heidelberg, Germany, July 1970 (Reprinted May 1979)</ref> 

When the paper was written, there were three manual, paper-based IBIS-type systems in use—two in government agencies and one in a university. 

A renewed interest in IBIS-type systems came about in the following decade, when advances in technology made it possible to design relatively inexpensive, computer-based IBIS-type systems. Jeff Conklin and co-workers adapted the IBIS structure for use in software engineering, creating the [[gIBIS]] (graphical IBIS) hypertext system in the late 1980s.<ref>Conklin, J. and Begeman, M.L., gIBIS: A hypertext tool for team design deliberation, Proceedings of the ACM conference on Hypertext, 1987</ref>   Several other graphical IBIS-type systems were developed once it was realised that such systems facilitated collaborative design and problem solving.<ref>Shum, S.J.B.,Selvin, Albert, M.,  Sierhuis, M., Conklin, J., Haley, C. B. and Nuseibeh, B.,  Hypermedia support for argumentation-based rationale: 15 years on from gIBIS and QOC, Rationale Management in Software Engineering, Springer, 2006</ref> These efforts culminated in the creation of the open source [[Compendium (software)]]  tool which supports—among other things—a graphical IBIS notation. Similar tools which do not rely on a database for storage include DRed <ref>http://www3.imperial.ac.uk/designengineering/tools/dred</ref> and designVUE.<ref>http://www3.imperial.ac.uk/portal/page/portallive/designengineering/tools/designvue</ref>

In recent years, there has been a renewed interest in IBIS-type systems, particularly in the context of [[sensemaking]] and collaborative problem solving in a variety of social and technical contexts. Of particular note is facilitation method called dialogue mapping which uses the IBIS notation to map out a design (or any other) dialogue as it evolves.<ref>Conklin, Jeff; "Dialogue Mapping: Building Shared Understanding of Wicked Problems," Wiley; 1st edition, 18 November 2005, ISBN 978-0-470-01768-5</ref>

Lately, online versions of dialogue- and issue-mapping tools have appeared, for example, Glyma and bCisive (see the links below).

==See also==
{{colbegin||30em}}
*[[Argument mapping]]
*[[Collaborative software]]
*[[Compendium (software)]]
*[[Computational sociology]]
*[[Creative problem solving]]
*[[Critical thinking]]
*[[Decision making]]
*[[Design]]
*[[Design rationale]]
*[[Graph database]]
*[[Knowledge base]]
*[[Planning]]
*[[Problem solving]]
*[[Wicked problem]]
{{colend}}{{clear right}}

==References==
<references />

== External links ==
* {{cite web | title = Issues as elements of information systems | citeseerx = 10.1.1.134.1741 }}
* [http://cognexus.org/issue_mapping.htm Cognexus Institute – Issue Mapping]
* [http://www.cleverworkarounds.com/2009/03/04/the-one-best-practice-to-rule-them-all-part-4/ Cleverworkarounds – the one best practice to rule them all – part 4]
* [http://eight2late.wordpress.com/2009/07/08/the-what-and-whence-of-issue-based-information-systems/ Eight to Late – The what and whence of issue-based information systems]
* [http://eight2late.wordpress.com/2009/06/25/visualising-arguments-using-issue-maps-an-example-and-some-general-comments/ Eight to Late – Visualising arguments using issue maps – an example and some general comments]
* [http://eight2late.wordpress.com/2009/04/07/issues-ideas-and-arguments-a-communication-centric-approach-to-tackling-project-complexity/ Eight to Late – Issues, Ideas and Arguments – a communication-centric approach to tackling project complexity]
* [http://bcisiveonline.com bCisive Online]
* [http://glyma.co/gettingstarted Glyma]

[[Category:Argument mapping]]
[[Category:Information systems]]
[[Category:Knowledge representation]]
[[Category:Problem structuring methods]]
<=====doc_Id=====>:894
<=====title=====>:
Category:Library of Congress Classification
<=====text=====>:
{{Cat main|Library of Congress Classification}}
[[Category:Library of Congress]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:897
<=====title=====>:
Faceted metadata
<=====text=====>:
#REDIRECT [[Faceted classification]]

{{R with possibilities}}

[[Category:Metadata|Faceted]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:900
<=====title=====>:
Mathematical model
<=====text=====>:
{{Distinguish2|the same term used in [[model theory]], a branch of [[mathematical logic]].}}
{{Refimprove|date=May 2008}} 
A '''mathematical model''' is a description of a [[system]] using [[mathematics|mathematical]] concepts and [[Language of mathematics|language]]. The process of developing a mathematical model is termed '''mathematical modeling'''. Mathematical models are used in the [[natural science]]s (such as [[physics]], [[biology]], [[earth science]], [[meteorology]]) and [[engineering]] disciplines (such as [[computer science]], [[artificial intelligence]]), as well as in the [[social sciences]] (such as [[economics]], [[psychology]], [[sociology]], [[political science]]).  [[Physicist]]s, [[engineer]]s, [[statistician]]s, [[operations research]] analysts, and [[economist]]s use mathematical models most extensively. A model may help to explain a system and to study the effects of different components, and to make predictions about behaviour.

== Elements of a mathematical model ==

Mathematical models can take many forms, including [[dynamical systems]], [[statistical model]]s, [[differential equations]], or [[Game theory|game theoretic models]].  These and other types of models can overlap, with a given  model involving a variety of abstract structures. In general, mathematical models may include [[model theory|logical models]].  In many cases, the quality of a scientific field depends on how well the mathematical models developed on the theoretical side agree with results of repeatable experiments.  Lack of agreement between theoretical mathematical models and experimental measurements often leads to important advances as better theories are developed.

In the [[physical sciences]], the traditional mathematical model contains four major elements. These are
# [[Governing equation]]s
# [[Defining equation (physics)|Defining equation]]s
# [[Constitutive equation]]s
# [[Constraint (mathematics)|Constraint]]s

== Classifications ==
Mathematical models are usually composed of relationships and ''[[variable (mathematics)|variables]]''. Relationships can be described by ''[[Operator (mathematics)|operators]]'', such as algebraic operators, functions, differential operators, etc. Variables are abstractions of system [[parameter (computer programming)|parameters]] of interest, that can be [[Quantification (science)|quantified]]. Several classification criteria can be used for mathematical models according to their structure:
* '''Linear vs. nonlinear:''' If all the operators in a mathematical model exhibit [[linear]]ity, the resulting mathematical model is defined as linear. A model is considered to be nonlinear otherwise. The definition of linearity and nonlinearity is dependent on context, and linear models may have nonlinear expressions in them.  For example, in a [[Linear model|statistical linear model]], it is assumed that a relationship is linear in the parameters, but it may be nonlinear in the predictor variables.  Similarly, a differential equation is said to be linear if it can be written with linear [[differential operator]]s, but it can still have nonlinear expressions in it.  In a [[Optimization (mathematics)|mathematical programming]] model, if the objective functions and constraints are represented entirely by [[linear equation]]s, then the model is regarded as a linear model.  If one or more of the objective functions or constraints are represented with a [[nonlinearity|nonlinear]] equation, then the model is known as a nonlinear model.<br>Nonlinearity, even in fairly simple systems, is often associated with phenomena such as [[Chaos theory|chaos]] and [[irreversibility]].  Although there are exceptions, nonlinear systems and models tend to be more difficult to study than linear ones.  A common approach to nonlinear problems is [[linearization]], but this can be problematic if one is trying to study aspects such as irreversibility, which are strongly tied to nonlinearity.
* '''Static vs. dynamic:''' A ''dynamic'' model  accounts for time-dependent changes in the state of the system, while a ''static'' (or steady-state) model calculates the system in equilibrium, and thus is time-invariant.  Dynamic models typically are represented by [[differential equation]]s or [[difference equation]]s.
* '''Explicit vs. implicit:''' If all of the input parameters of the overall model are known, and the output parameters can be calculated by a finite series of computations, the model is said to be ''explicit''. But sometimes it is the ''output'' parameters which are known, and the corresponding inputs must be solved for by an iterative procedure, such as [[Newton's method]] (if the model is linear) or [[Broyden's method]] (if non-linear). In such a case the model is said to be ''implicit''. For example, a [[jet engine]]'s physical properties such as turbine and nozzle throat areas can be explicitly calculated given a design [[thermodynamic cycle]] (air and fuel flow rates, pressures, and temperatures) at a specific flight condition and power setting, but the engine's operating cycles at other flight conditions and power settings cannot be explicitly calculated from the constant physical properties.
* '''Discrete vs. continuous:''' A [[discrete modeling|discrete model]] treats objects as discrete, such as the particles in a [[molecular model]] or the states in a [[statistical model]]; while a [[continuous model]] represents the objects in a continuous manner, such as the velocity field of fluid in pipe flows, temperatures and stresses in a solid, and electric field that applies continuously over the entire model due to a point charge.
* '''Deterministic vs. probabilistic (stochastic):''' A [[deterministic system|deterministic]] model is one in which every set of variable states is uniquely determined by parameters in the model and by sets of previous states of these variables; therefore, a deterministic model always performs the same way for a given set of initial conditions. Conversely, in a stochastic model—usually called a "[[statistical model]]"—randomness is present, and variable states are not described by unique values, but rather by [[probability]] distributions.
* '''Deductive, inductive, or floating:''' A deductive model is a logical structure based on a theory. An inductive model arises from empirical findings and generalization from them. The floating model rests on neither theory nor observation, but is merely the invocation of expected structure. Application of mathematics in social sciences outside of economics has been criticized for unfounded models.<ref>{{cite book |authorlink=Stanislav Andreski |first=Stanislav |last=Andreski |year=1972 |title=Social Sciences as Sorcery |publisher=[[St. Martin’s Press]] |isbn=0-14-021816-5 }}</ref> Application of [[catastrophe theory]] in science has been characterized as a floating model.<ref>{{cite book |authorlink=Clifford Truesdell |first=Clifford |last=Truesdell |year=1984 |title=An Idiot’s Fugitive Essays on Science |pages=121–7 |publisher=Springer |isbn=3-540-90703-3 }}</ref>

== Significance in the natural sciences ==
Mathematical models are of great importance in the natural sciences, particularly in [[physics]]. Physical [[theory|theories]] are almost invariably expressed using mathematical models.

Throughout history, more and more accurate mathematical models have been developed. [[Newton's laws of motion|Newton's laws]] accurately describe many everyday phenomena, but at certain limits [[relativity theory]] and [[quantum mechanics]] must be used; even these do not apply to all situations and need further refinement. It is possible to obtain the less accurate models in appropriate limits, for example relativistic mechanics reduces to Newtonian mechanics at speeds much less than the [[speed of light]]. Quantum mechanics reduces to classical physics when the quantum numbers are high. For example, the [[de Broglie wavelength]] of a tennis ball is insignificantly small, so classical physics is a good approximation to use in this case.

It is common to use idealized models in physics to simplify things. Massless ropes, point particles, [[ideal gases]] and the [[particle in a box]] are among the many simplified models used in physics. The laws of physics are represented with simple equations such as Newton's laws, [[Maxwell's equations]] and the [[Schrödinger equation]]. These laws are such as a basis for making mathematical models of real situations. Many real situations are very complex and thus modeled approximate on a computer, a model that is computationally feasible to compute is made from the basic laws or from approximate models made from the basic laws. For example, molecules can be modeled by [[molecular orbital]] models that are approximate solutions to the Schrödinger equation. In [[engineering]], physics models are often made by mathematical methods such as [[finite element analysis]].

Different mathematical models use different geometries that are not necessarily accurate descriptions of the geometry of the universe. [[Euclidean geometry]] is much used in classical physics, while [[special relativity]] and [[general relativity]] are examples of theories that use [[geometry|geometries]] which are not Euclidean.

== Some applications==<!--whatever this section is, it isn't background (original heading), and isn't general-->
Since [[prehistory|prehistorical times]] simple models such as [[map]]s and [[Mathematical diagram|diagrams]] have been used.

Often when engineers analyze a system to be controlled or optimized, they use a mathematical model. In analysis, engineers can build a descriptive model of the system as a hypothesis of how the system could work, or try to estimate how an unforeseeable event could affect the system. Similarly, in control of a system, engineers can try out different control approaches in [[simulation]]s.

A mathematical model usually describes a system by a [[Set (mathematics)|set]] of variables and a set of equations that establish relationships between the variables. Variables may be of many types; [[Real number|real]] or [[integer]] numbers, [[Boolean data type|boolean]] values or [[String (computing)|strings]], for example. The variables represent some properties of the system, for example, measured system outputs often in the form of [[Signal (electronics)|signals]], [[Chronometry|timing data]], counters, and event occurrence (yes/no). The actual model is the set of functions that describe the relations between the different variables.

== Building blocks ==
In [[business]] and [[engineering]], mathematical models may be used to maximize a certain output. The system under consideration will require certain inputs. The system relating inputs to outputs depends on other variables too: [[decision theory|decision variables]], [[state variable]]s, [[Exogeny|exogenous]] variables, and [[random variable]]s.

Decision variables are sometimes known as independent variables.  Exogenous variables are sometimes known as [[parameter]]s or [[constant (mathematics)|constant]]s.
The variables are not independent of each other as the state variables are dependent on the decision, input, random, and exogenous variables.  Furthermore, the output variables are dependent on the state of the system (represented by the state variables).

[[Goal|Objective]]s and [[constraint (mathematics)|constraint]]s of the system and its users can be represented as [[function (mathematics)|function]]s of the output variables or state variables.  The [[objective function]]s will depend on the perspective of the model's user.  Depending on the context, an objective function is also known as an ''index of performance'', as it is some measure of interest to the user.  Although there is no limit to the number of objective functions and constraints a model can have, using or optimizing the model becomes more involved (computationally) as the number increases.

For example, [[economist]]s often apply [[linear algebra]] when using [[input-output model]]s. Complicated mathematical models that have many variables may be consolidated by use of [[vector space|vectors]] where one symbol represents several variables.

== A priori information ==
[[File:Blackbox3D-withGraphs.png|thumb|480px|To analyse something with a typical "black box approach", only  the behavior of the stimulus/response will be accounted for, to infer the (unknown) ''box''. The usual representation of this ''black box system''  is a [[data flow diagram]] centered in the box.]]

Mathematical modeling problems are often classified into [[black box]] or [[White box (software engineering)|white box]] models, according to how much [[a priori (philosophy)|a priori]] information on the system is available. A black-box model is a system of which there is no a priori information available. A white-box model (also called glass box or clear box) is a system where all necessary information is available. Practically all systems are somewhere between the black-box and white-box models, so this concept is useful only as an intuitive guide for deciding which approach to take.

Usually it is preferable to use as much a priori information as possible to make the model more accurate. Therefore, the white-box models are usually considered easier, because if you have used the information correctly, then the model will behave correctly. Often the a priori information comes in forms of knowing the type of functions relating different variables. For example, if we make a model of how a medicine works in a human system, we know that usually the amount of medicine in the blood is an [[exponential decay|exponentially decaying]] function. But we are still left with several unknown parameters; how rapidly does the medicine amount decay, and what is the initial amount of medicine in blood? This example is therefore not a completely white-box model. These parameters have to be estimated through some means before one can use the model.

In black-box models one tries to estimate both the functional form of relations between variables and the numerical parameters in those functions. Using a priori information we could end up, for example, with a set of functions that probably could describe the system adequately. If there is no a priori information we would try to use functions as general as possible to cover all different models. An often used approach for black-box models are [[neural networks]] which usually do not make assumptions about incoming data. Alternatively the NARMAX (Nonlinear AutoRegressive Moving Average model with eXogenous inputs) algorithms which were developed as part of [[nonlinear system identification]] <ref name="SAB1">Billings S.A. (2013), ''Nonlinear System Identification: NARMAX Methods in the Time, Frequency, and Spatio-Temporal Domains'', Wiley.</ref> can be used to select the model terms, determine the model structure, and estimate the unknown parameters in the presence of correlated and nonlinear noise. The advantage of NARMAX models compared to neural networks is that NARMAX produces models that can be written down and related to the underlying process, whereas neural networks produce an approximation that is opaque.

=== Subjective information ===
Sometimes it is useful to incorporate subjective information into a mathematical model.  This can be done based on [[Intuition (knowledge)|intuition]], [[experience]], or [[expert opinion]], or based on convenience of mathematical form.  [[Bayesian statistics]] provides a theoretical framework for incorporating such subjectivity into a rigorous analysis: we specify a [[prior probability distribution]] (which can be subjective), and then update this distribution based on empirical data.

An example of when such approach would be necessary is a situation in which an experimenter bends a coin slightly and tosses it once, recording whether it comes up heads, and is then given the task of predicting the probability that the next flip comes up heads.  After bending the coin, the true probability that the coin will come up heads is unknown; so the experimenter would need to make a decision (perhaps by looking at the shape of the coin) about what prior distribution to use.  Incorporation of such subjective information might be important to get an accurate estimate of the probability.

== Complexity ==
[[File:Mathematical models for complex systems.jpg|300px|right|thumb|This is a schematic representation of three types of mathematical models of complex systems with the level of their mechanistic understanding.]]
In general, model complexity involves a trade-off between simplicity and accuracy of the model.  [[Occam's razor]] is a principle particularly relevant to modeling, its essential idea being that among models with roughly equal predictive power, the simplest one is the most desirable.  While added complexity usually improves the realism of a model, it can make the model difficult to understand and analyze, and can also pose computational problems, including [[numerical instability]].  [[Thomas Kuhn]] argues that as science progresses, explanations tend to become more complex before a [[paradigm shift]] offers radical simplification.

For example, when modeling the flight of an aircraft, we could embed each mechanical part of the aircraft into our model and would thus acquire an almost white-box model of the system. However, the computational cost of adding such a huge amount of detail would effectively inhibit the usage of such a model. Additionally, the uncertainty would increase due to an overly complex system, because each separate part induces some amount of variance into the model. It is therefore usually appropriate to make some approximations to reduce the model to a sensible size. Engineers often can accept some approximations in order to get a more robust and simple model. For example, [[Isaac Newton|Newton's]] [[classical mechanics]] is an approximated model of the real world. Still, Newton's model is quite sufficient for most ordinary-life situations, that is, as long as particle speeds are well below the [[speed of light]], and we study macro-particles only.

== Training ==
Any model which is not pure white-box contains some [[parameter]]s that can be used to fit the model to the system it is intended to describe. If the modeling is done by a [[neural network]] or other [[machine learning]], the optimization of parameters is called ''training'', while the optimization of model hyperparameters is called ''tuning'' and often uses [[cross-validation]]. In more conventional modeling through explicitly given mathematical functions, parameters are often determined by ''[[curve fitting]]''.

== Model evaluation ==
A crucial part of the modeling process is the evaluation of whether or not a given mathematical model describes a system accurately.  This question can be difficult to answer as it involves several different types of evaluation.

=== Fit to empirical data ===
Usually the easiest part of model evaluation is checking whether a model fits experimental measurements or other empirical data.  In models with parameters, a common approach to test this fit is to split the data into two disjoint subsets: training data and verification data. The training data are used to estimate the model parameters.  An accurate model will closely match the verification data even though these data were not used to set the model's parameters. This practice is referred to as [[cross-validation (statistics)|cross-validation]] in statistics.

Defining a [[Metric (mathematics)|metric]] to measure distances between observed and predicted data is a useful tool of assessing model fit.  In statistics, decision theory, and some [[economic model]]s, a [[loss function]] plays a similar role.

While it is rather straightforward to test the appropriateness of parameters, it can be more difficult to test the validity of the general mathematical form of a model.  In general, more mathematical tools have been developed to test the fit of [[statistical model]]s than models involving [[differential equations]].  Tools from [[non-parametric statistics]] can sometimes be used to evaluate how well the data fit a known distribution or to come up with a general model that makes only minimal assumptions about the model's mathematical form.

=== Scope of the model ===
Assessing the scope of a model, that is, determining what situations the model is applicable to, can be less straightforward.  If the model was constructed based on a set of data, one must determine for which systems or situations the known data is a "typical" set of data.

The question of whether the model describes well the properties of the system between data points is called [[interpolation]], and the same question for events or data points outside the observed data is called [[extrapolation]].

As an example of the typical limitations of the scope of a model, in evaluating Newtonian [[classical mechanics]], we can note that Newton made his measurements without advanced equipment, so he could not measure properties of particles travelling at speeds close to the speed of light.  Likewise, he did not measure the movements of molecules and other small particles, but macro particles only. It is then not surprising that his model does not extrapolate well into these domains, even though his model is quite sufficient for ordinary life physics.

=== Philosophical considerations ===
Many types of modeling implicitly involve claims about [[causality]].  This is usually (but not always) true of models involving differential equations.  As the purpose of modeling is to increase our understanding of the world, the validity of a model rests not only on its fit to empirical observations, but also on its ability to extrapolate to situations or data beyond those originally described in the model. One can think of this as the differentiation between qualitative and quantitative predictions. One can also argue that a model is worthless unless it provides some insight which goes beyond what is already known from direct investigation of the phenomenon being studied.

An example of such criticism is the argument that the mathematical models of [[Optimal foraging theory]] do not offer insight that goes beyond the common-sense conclusions of [[evolution]] and other basic principles of ecology.<ref>{{Cite journal | last1 = Pyke | first1 = G. H. | doi = 10.1146/annurev.es.15.110184.002515 | title = Optimal Foraging Theory: A Critical Review | journal = Annual Review of Ecology and Systematics | volume = 15 | pages = 523–575 | year = 1984 | pmid =  | pmc = }}</ref>

== Examples ==
* One of the popular examples in [[computer science]] is the mathematical models of various machines, an example is the [[deterministic finite automaton]] which is defined as an abstract mathematical concept, but due to the deterministic nature of a DFA, it is implementable in hardware and software for solving various specific problems. For example, the following is a DFA M with a binary alphabet, which requires that the input contains an even number of 0s.

[[File:DFAexample.svg|right|thumb|250px|The [[state diagram]] for ''M'']]
''M'' = (''Q'', Σ, δ, ''q''<sub>0</sub>, ''F'') where
*''Q'' = {''S''<sub>1</sub>, ''S''<sub>2</sub>},
*Σ = {0, 1},
*''q<sub>0</sub>'' = ''S''<sub>1</sub>,
*''F'' = {''S''<sub>1</sub>}, and
*δ is defined by the following [[state transition table]]:
:{| border="1" cell padding="1" cell spacing="0"
| || <center>'''0'''</center> || <center>'''1'''</center>
|-
|'''''S''<sub>1</sub>''' || ''S''<sub>2</sub> || ''S''<sub>1</sub>
|-
|'''''S''<sub>2</sub>''' || ''S''<sub>1</sub> || ''S''<sub>2</sub>
|}

The state ''S''<sub>1</sub> represents that there has been an even number of 0s in the input so far, while ''S''<sub>2</sub> signifies an odd number. A 1 in the input does not change the state of the automaton. When the input ends, the state will show whether the input contained an even number of 0s or not. If the input did contain an even number of 0s, ''M'' will finish in state ''S''<sub>1</sub>, an accepting state, so the input string will be accepted.

The language recognized by ''M'' is the [[regular language]] given by the [[regular expression]] 1*( 0 (1*) 0 (1*) )*, where "*" is the [[Kleene star]], e.g., 1* denotes any non-negative number (possibly zero) of symbols "1".

* Many everyday activities carried out without a thought are uses of mathematical models. A geographical [[map projection]] of a region of the earth onto a small, plane surface  is a model<ref>[http://www.landinfo.com/resources_dictionaryMP.htm landinfo.com, definition of map projection]</ref> which can be used for many purposes such as planning travel.
* Another simple activity is predicting the position of a vehicle from its initial position, direction and speed of travel, using the equation that distance traveled is the product of time and speed. This is known as [[dead reckoning]] when used more formally. Mathematical modeling in this way does not necessarily require formal mathematics; animals have been shown to use dead reckoning.<ref>{{cite book |last=Gallistel |first= |title=The Organization of Learning |location=Cambridge |publisher=The MIT Press |year=1990 |ISBN=0-262-07113-4 }}</ref><ref>{{Cite journal | last1 = Whishaw | first1 = I. Q. | last2 = Hines | first2 = D. J. | last3 = Wallace | first3 = D. G. | doi = 10.1016/S0166-4328(01)00359-X | title = Dead reckoning (path integration) requires the hippocampal formation: Evidence from spontaneous exploration and spatial learning tasks in light (allothetic) and dark (idiothetic) tests | journal = Behavioural Brain Research | volume = 127 | issue = 1–2 | pages = 49–69 | year = 2001 | pmid =  11718884| pmc = }}</ref>
* ''[[Population]] Growth''. A simple (though approximate) model of population growth is the [[Malthusian growth model]]. A slightly more realistic and largely used population growth model is the [[logistic function]], and its extensions. 
* ''Individual-based cellular automata models of [[population]] growth''
[[File:Logical deterministic individual-based cellular automata model of single species population growth.gif|left|thumb|150px]]
<br /><br /><br /><br /><br /><br /><br /><br /><br />
* ''Model of a particle  in a potential-field''. In this model we consider a particle as being a point of mass which describes a trajectory in space which is modeled by a function giving its coordinates in space as a function of time. The potential field is given by a function <math>V\!:\mathbb{R}^3\!\rightarrow\mathbb{R}</math> and the trajectory, that is a function <math>\mathbf{r}\!:\mathbb{R}\rightarrow\mathbb{R}^3</math>, is the solution of the [[differential equation]]:

::<math> -\frac{\mathrm{d}^2\mathbf{r}(t)}{\mathrm{d}t^2}m=\frac{\partial V[\mathbf{r}(t)]}{\partial x}\mathbf{\hat{x}}+\frac{\partial V[\mathbf{r}(t)]}{\partial y}\mathbf{\hat{y}}+\frac{\partial V[\mathbf{r}(t)]}{\partial z}\mathbf{\hat{z}}, </math>

that can be written also as:

::<math> m\frac{\mathrm{d}^2\mathbf{r}(t)}{\mathrm{d}t^2}=-\nabla V[\mathbf{r}(t)]. </math>

:Note this model assumes  the particle is a point mass, which is certainly known to be false in many cases in which we use this model; for example, as a model of planetary motion.

* ''Model of rational behavior for a consumer''.  In this model we assume a consumer faces a choice of ''n'' commodities labeled 1,2,...,''n'' each with a market price ''p''<sub>1</sub>, ''p''<sub>2</sub>,..., ''p''<sub>''n''</sub>. The consumer is assumed to have a ''cardinal'' utility function ''U'' (cardinal in the sense that it assigns numerical values to utilities), depending on the amounts of commodities ''x''<sub>1</sub>, ''x''<sub>2</sub>,..., ''x''<sub>''n''</sub> consumed.  The model further assumes that the consumer has a budget ''M'' which is used to purchase a vector ''x''<sub>1</sub>, ''x''<sub>2</sub>,..., ''x''<sub>''n''</sub> in such a way as to maximize ''U''(''x''<sub>1</sub>, ''x''<sub>2</sub>,..., ''x''<sub>''n''</sub>).  The problem of rational behavior in this model then becomes an [[Optimization (mathematics)|optimization]] problem, that is:
:: <math> \max U(x_1,x_2,\ldots, x_n) </math>
:: subject to:
:: <math> \sum_{i=1}^n p_i x_i \leq M.</math>
:: <math> x_{i} \geq 0   \; \; \; \forall i \in \{1, 2, \ldots, n \} </math>
: This model has been used in [[general equilibrium theory]], particularly to show existence and [[Pareto efficiency]] of economic equilibria.  However, the fact that this particular formulation assigns ''numerical values'' to levels of satisfaction is the source of criticism (and even ridicule).  However, it is not an essential ingredient of the theory and again this is an idealization.
* ''[[Neighbour-sensing model]]'' explains the [[mushroom]] formation from the initially chaotic [[fungus|fungal]] network.
* ''[[Computer science]]'': models in Computer Networks, data models, surface model,...
* ''[[Mechanics]]'': movement of rocket model,...

Modeling requires selecting and identifying relevant aspects of a situation in the real world.

== See also ==
{{Portal|Mathematics}}
{{div col|2}}
* [[Agent-based model]]
* [[Cliodynamics]]
* [[Computer simulation]]
* [[Conceptual model]]
* [[Decision engineering]]
* [[Grey box model]]
* [[Mathematical biology]]
* [[Mathematical diagram]]
* [[Mathematical psychology]]
* [[Mathematical sociology]]
* [[Model inversion]]
* [[Microscale and macroscale models]]
* [[Statistical Model]]
* [[System identification]]
* [[TK Solver]] - Rule Based Modeling
{{div col end}}

== References ==
{{Reflist}}

== Further reading ==

===Books===
* Aris, Rutherford [ 1978 ] ( 1994 ). ''Mathematical Modelling Techniques'', New York: Dover. ISBN 0-486-68131-9
* Bender, E.A. [ 1978 ] ( 2000 ). ''An Introduction to Mathematical Modeling'', New York: Dover. ISBN 0-486-41180-X
* Gershenfeld, N. (1998) ''The Nature of Mathematical Modeling'', [[Cambridge University Press]] ISBN 0-521-57095-6 .
* Lin, C.C. & Segel, L.A. ( 1988 ). ''Mathematics Applied to Deterministic Problems in the Natural Sciences'', Philadelphia: SIAM. ISBN 0-89871-229-7

===Specific applications===
* [[Korotayev]] A., Malkov A., Khaltourina D. (2006). [http://cliodynamics.ru/index.php?option=com_content&task=view&id=124&Itemid=70 ''Introduction to Social Macrodynamics: Compact Macromodels of the World System Growth'']. Moscow: [http://urss.ru/cgi-bin/db.pl?cp=&lang=en&blang=en&list=14&page=Book&id=34250 Editorial URSS] ISBN 5-484-00414-4 .
* {{Cite journal | last1 = Peierls | first1 = R. | doi = 10.1080/00107518008210938 | title = Model-making in physics | journal = Contemporary Physics | volume = 21 | pages = 3–17 | year = 1980 | pmid =  | pmc = |bibcode = 1980ConPh..21....3P }}
* ''[http://anintroductiontoinfectiousdiseasemodelling.com/ An Introduction to Infectious Disease Modelling]'' by Emilia Vynnycky and Richard G White.

== External links ==

;General reference

* Patrone, F. [http://www.fioravante.patrone.name/mat/u-u/en/differential_equations_intro.htm Introduction to modeling via differential equations], with critical remarks.
* [http://plus.maths.org/issue44/package/index.html Plus teacher and student package: Mathematical Modelling.] Brings together all articles on mathematical modeling from ''[[Plus Magazine]]'', the online mathematics magazine produced by the Millennium Mathematics Project at the University of Cambridge.

;Philosophical

* Frigg, R. and S. Hartmann, [http://plato.stanford.edu/entries/models-science/ Models in Science], in: The Stanford Encyclopedia of Philosophy, (Spring 2006 Edition)
* Griffiths, E. C. (2010) [https://sites.google.com/a/ncsu.edu/emily-griffiths/whatisamodel.pdf What is a model?]

{{Authority control}}

{{DEFAULTSORT:Mathematical Model}}
[[Category:Applied mathematics]]
[[Category:Collective intelligence]]
[[Category:Conceptual models]]
[[Category:Knowledge representation]]
[[Category:Mathematical modeling| ]]
[[Category:Mathematical terminology]]
<=====doc_Id=====>:903
<=====title=====>:
Categorization
<=====text=====>:
{{Refimprove|date=January 2017}}
{{selfref|For information about Wikipedia's article categories, see [[Help:Category]].}}
{{for|particular uses|Category (disambiguation)}}
{{Information science}}
'''Categorization''' is the process in which ideas and objects are recognized, differentiated, and understood.<ref>Cohen, H., & Lefebvre, C. (Eds.). (2005).[https://books.google.com/books?id=5WDfl14RgKMC ''Handbook of Categorization in Cognitive Science'']. Elsevier.</ref> Categorization implies that objects are grouped into categories, usually for some specific purpose. Ideally, a category illuminates a [[Binary relation|relationship]] between the [[subject (philosophy)|subject]]s and [[object (philosophy)|object]]s of knowledge. Categorization is fundamental in language, prediction, [[inference]], decision making and in all kinds of environmental interaction. It is indicated that categorization plays a major role in [[computer programming]].<ref>Frey, T., Gelhausen, M., & Saake (2011).[http://ecs.victoria.ac.nz/twiki/pub/Events/PLATEAU/Program/plateau2011-frey.pdf '' Categorization of Concerns – A Categorical Program Comprehension Model. In Proceedings of the Workshop on Evaluation and Usability of Programming Languages and Tools (PLATEAU) at the ACM Onward! and SPLASH Conferences. October, 2011. Portland, Oregon, USA''].</ref>

There are many categorization theories and techniques. In a broader historical view, however, three general approaches to categorization may be identified:
* Classical categorization
* Conceptual clustering
* Prototype theory

==The classical view==
{{main|Categories (Aristotle)}}
'''Classical categorization''' first appears in the context of [[Western Philosophy]] in the work of [[Plato]], who, in his [[Statesman (dialogue)|Statesman]] dialogue, introduces the approach of grouping objects based on their similar [[Property (philosophy)|properties]]. This approach was further explored and systematized by [[Aristotle]] in his [[Categories (Aristotle)|Categories]] treatise, where he analyzes the differences between [[Class (philosophy)|class]]es and [[Object (philosophy)|object]]s. Aristotle also applied intensively the classical categorization scheme in his approach to the classification of living beings (which uses the technique of applying successive narrowing questions such as "Is it an animal or vegetable?", "How many feet does it have?", "Does it have fur or feathers?", "Can it fly?"...), establishing this way the basis for natural [[Taxonomy (biology)|taxonomy]].

The classical [[Aristotelianism|Aristotelian]] view claims that categories are discrete entities characterized by a set of properties which are shared by their members. In [[analytic philosophy]], these properties are assumed to establish the conditions which are both [[necessary and sufficient condition]]s to capture meaning. 

According to the classical view, categories should be clearly defined, mutually exclusive and collectively exhaustive. This way, any entity of the given classification universe belongs unequivocally to one, and only one, of the proposed categories.

==Conceptual clustering==
{{main|Conceptual clustering}}
'''Conceptual clustering''' is a modern variation of the classical approach, and derives from attempts to explain how knowledge is represented. In this approach, [[Class (philosophy)|class]]es (clusters or entities) are generated by first formulating their conceptual descriptions and then classifying the entities according to the descriptions. 

Conceptual clustering developed mainly during the 1980s, as a machine paradigm for [[unsupervised learning]]. It is distinguished from ordinary [[Cluster analysis|data clustering]] by generating a concept description for each generated category. 

Categorization tasks in which category labels are provided to the learner for certain objects are referred to as supervised classification, [[supervised learning]], or [[concept learning]]. Categorization tasks in which no labels are supplied are referred to as unsupervised classification, [[unsupervised learning]], or [[Cluster analysis|data clustering]]. The task of supervised classification involves extracting information from the labeled examples that allows accurate prediction of class labels of future examples. This may involve the [[abstraction]] of a rule or concept relating observed object features to category labels, or it may not involve abstraction (e.g., [[Exemplar theory|exemplar model]]s). The task of clustering involves recognizing inherent structure in a data set and grouping objects together by similarity into classes. It is thus a process of ''generating'' a classification structure. 

Conceptual clustering is closely related to [[fuzzy set]] theory, in which objects may belong to one or more groups, in varying degrees of fitness.

==Prototype theory==
{{main|Prototype theory}}
Since the research by [[Eleanor Rosch]] and [[George Lakoff]] in the 1970s, categorization can also be viewed as the process of grouping things based on [[prototype]]s—the idea of necessary and sufficient conditions is almost never met in categories of naturally occurring things. It has also been suggested that categorization based on prototypes is the basis for human development, and that this learning relies on learning about the world via [[embodied cognition|embodiment]].

A [[cognition|cognitive]] approach accepts that natural categories are graded (they tend to be fuzzy at their boundaries) and inconsistent in the status of their constituent members.

Systems of categories are not objectively "out there" in the world but are rooted in people's experience. Conceptual categories are not identical for different cultures, or indeed, for every individual in the same culture. 

Categories form part of a hierarchical structure when applied to such subjects as [[Taxonomy (biology)|taxonomy]] in [[biological classification]]: higher level: life-form level, middle level: generic or [[genus]] level, and lower level: the [[species]] level. These can be distinguished by certain traits that put an item in its distinctive category. But even these can be arbitrary and are subject to revision.

Categories at the middle level are perceptually and conceptually the more salient. The generic level of a category tends to elicit the most responses and richest images and seems to be the psychologically basic level. Typical taxonomies in zoology for example exhibit categorization at the [[embodied cognition|embodied]] level, with similarities leading to formulation of "higher" categories, and differences leading to differentiation within categories.

== Miscategorization ==
Miscategorization can be a [[Fallacy|logical fallacy]] in which diverse and dissimilar objects, concepts, entities, etc. are grouped together based upon illogical common denominators, or common denominators that virtually any concept, object or entity have in common. A common way miscategorization occurs is through an over-categorization of concepts, objects or entities, and then miscategorization based upon characters that virtually all things have in common.

== See also ==
{{too many see alsos|date=October 2013}}
{{columns-list|3| 
* [[Lumpers and splitters]]
* [[Artificial neural network]]
* [[Category learning]]
* [[Categorical perception]]
* [[Classification in machine learning]]
* [[Family resemblance]]
* [[Fuzzy concept]]
* [[Language acquisition]]
* [[Library classification]]
* [[Machine learning]]
* [[Multi-label classification]]
* [[Natural kind]]
* [[Ontology]]
* [[Pattern recognition]]
* [[Perceptual learning]]
* [[Semantics]]
* [[Socrates]]
* [[Sortal]]
* [[Structuralism]]
* [[Symbol grounding]]
* [[Taxonomy (general)]]
}}

== References ==
{{Reflist}}

==External links==
{{Wiktionary}}
* [http://eprints.ecs.soton.ac.uk/11725/ To Cognize is to Categorize: Cognition is Categorization]
*[http://toolserver.org/~dapete/catgraph/ Wikipedia Categories Visualizer]
*[http://www.revue-emulations.net/archives/n8/categentretien Interdisciplinary Introduction to Categorization: Interview with Dvora Yanov (political sciences), Amie Thomasson (philosophy) and Thomas Serre (artificial intelligence)]

{{philosophy of language}}

[[Category:Knowledge representation]]
[[Category:Concepts in epistemology]]
[[Category:Semantics]]
[[Category:Cognition]]
<=====doc_Id=====>:906
<=====title=====>:
Brian Deer Classification System
<=====text=====>:
The '''Brian Deer Classification System''' is a [[library classification]] system created for use in Indigenous contexts by Canadian [[Mohawk people|Kahnawake Mohawk]] librarian A. Brian Deer.<ref name="auto">{{cite journal|last1=Doyle|first1=Ann M.|last2=Lawson|first2=Kimberley|last3=Dupont|first3=Sarah|title=Indigenization of Knowledge Organization at the Xwi7xwa Library|journal=Journal of Library and Information Studies|date=December 2015|volume=13|issue=2|page=112|doi=10.6182/jlis.2015.13(2).107|url=https://open.library.ubc.ca/cIRcle/collections/ubclibraryandarchives/29962/items/1.0103204|accessdate=11 March 2016}}</ref>

== History ==
Deer designed his classification system while working in the library of the [[National Indian Brotherhood]] from 1974-1976, with the goal of reflecting indigenous viewpoints and values in knowledge organization. Between 1978 and 1980, the system was adapted for use in [[British Columbia]] by Gene Joseph and Keltie McCall while working at the [[Union of British Columbia Indian Chiefs]].<ref name="auto"/>

Variations of the Brian Deer Classification are in use at the [[University of British Columbia Library|Xwi7xwa Library]] at the [[University of British Columbia]];<ref name="auto"/> the [[Union of British Columbia Indian Chiefs]] Resource Centre;<ref>{{cite journal|last1=Cherry|first1=Alissa|last2=Mukunda|first2=Keshav|title=A Case Study in Indigenous Classification: Revisiting and Reviving the Brian Deer Scheme|journal=Cataloging & Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=pages 548–567|doi=10.1080/01639374.2015.1008717|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1008717?journalCode=wccq20|accessdate=11 March 2016}}</ref> and the  Aanischaaukamikw Cree Cultural Institute in [[Oujé-Bougoumou, Quebec]].<ref>{{cite journal|last1=Swanson|first1=Raegan|title=Adapting the Brian Deer Classification System for Aanischaaukamikw Cree Cultural Institute|journal=Cataloging & Classification Quarterly|date=31 Jul 2015|volume=53|issue=5-6|pages=568–579|doi=10.1080/01639374.2015.1009669|url=http://www.tandfonline.com/doi/abs/10.1080/01639374.2015.1009669|accessdate=11 March 2016}}</ref>

== References ==
{{Reflist}}

== External links ==
* [http://xwi7xwa.library.ubc.ca/files/2011/09/deer.pdf Brian Deer Classification System]

<!--- Categories --->
[[Category:Library cataloging and classification]]
[[Category:Information science]]
[[Category:Knowledge representation]]
[[Category:Aboriginal peoples in Canada]]
<=====doc_Id=====>:909
<=====title=====>:
OntoUML
<=====text=====>:
{{Infobox technology standard
| name = OntoUML
| year_started = 2005
| domain = [[Conceptual_model|Conceptual Modeling]]
| base_standards = [[Unified Foundational Ontology (UFO)]]
| related_standards = [[Unified Modeling Language|UML]]
| organization = [[Ontology & Conceptual Modeling Research Group (NEMO)]]
| website = {{URL|http://nemo.inf.ufes.br/}}
}}
'''OntoUML''' is a [[Ontology|ontologically]] well-founded language for [[Ontology_(information_science)|Ontology]]-driven [[Conceptual_model|Conceptual Modeling]]. '''OntoUML''' is built as a [[Unified Modeling Language|UML]] extension based on the [[Unified Foundational Ontology (UFO)]]. UFO was created by Giancarlo Guizzardi in his Ph.D. thesis<ref>{{cite book|last1=Guizzardi|first1=Giancarlo|title=Ontological foundations for structural conceptual models|date=2005|publisher=Enschede: Telematica Instituut Fundamental Research Series|url=http://doc.utwente.nl/50826/1/thesis_Guizzardi.pdf}}</ref> and used to evaluate and re-design a fragment of the UML 2.0 metamodel. Giancarlo Guizzardi is a lead researcher at the [[Ontology & Conceptual Modeling Research Group (NEMO)]]<ref>{{cite web|title=Ontology & Conceptual Modeling Research Group (NEMO)|url=http://nemo.inf.ufes.br/}}</ref> located at the [[Federal University of Espírito Santo|Federal University of Espírito Santo (UFES)]] in [[Vitória,_Espírito_Santo|Vitória]] city, state of [[Espírito Santo]], [[Brazil]].

NEMO created an OntoUML infrastructure<ref>{{cite web|title=OntoUML infrastructure|url=http://code.google.com/p/rcarraretto/}}</ref> using [[Eclipse_Modeling_Framework|Eclipse EMF]] for '''OntoUML''' model manipulation which serve as a basis for its tool support. NEMO has been actively working on tool support for the '''OntoUML''' Conceptual Modeling Language, respectively on:

# extensions of [[Unified Modeling Language|UML]] production-grade tools to support OntoUML, namely, the MDG for [[Enterprise_Architect_(software)|Enterprise Architect]].
# a standalone tool called [[OntoUML Lightweight Editor (OLED)]]<ref>{{cite web|title=OntoUML lightweight editor (OLED) repository|url=https://github.com/nemo-ufes/ontouml-lightweight-editor}}</ref> to the development, evaluation and implementation of domain ontologies.
# a legacy OntoUML editor<ref>{{cite web|title=Legacy OntoUML editor|url=https://github.com/nemo-ufes/ontouml-editor-eclipse}}</ref> based on an old version of [[Graphical_Modeling_Framework|Eclipse/GMF]].

Check out a list of all publications of NEMO about ontologies and OntoUML: <ref>{{cite web|title=NEMO publications|url=http://nemo.inf.ufes.br/publications/}}</ref>

== References ==
<references />

[[Category:UML tools]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
<=====doc_Id=====>:912
<=====title=====>:
Consistency (knowledge bases)
<=====text=====>:
A [[knowledge base]] KB is '''consistent''' ''[[iff]]'' its negation is not a [[Tautology (logic)|tautology]].

I.e., a knowledge base KB is inconsistent (not consistent) [[iff]] there is no [[Interpretation (logic)|interpretation]] which [[entailment|entails]] KB.

Example of an inconsistent knowledge base:

KB := { a, ¬a }

Consistency in terms of knowledge bases is mostly the same as the natural understanding of [[consistency]].

[[Category:Knowledge representation]]
{{logic-stub}}
{{database-stub}}
<=====doc_Id=====>:915
<=====title=====>:
Library branch
<=====text=====>:
[[File:New York Public Library May 2011.JPG|200 px|thumb|right|The [[New York Public Library Main Branch]] in [[Bryant Park]], [[Manhattan]]]]
'''Library branches''' are libraries that form part of a [[library system]] but are not located in the same area, building or city, but use the same [[Library classification]] for their catalogs and are interconnected with all the branches of the system that form part of the systems and to library patrons through a [[integrated library system]].<ref>{{cite web|url=http://www.merriam-webster.com/dictionary/branch |title=Branch &#124; Definition of Branch by Merriam-Webster |website=Merriam-webster.com |date= |accessdate=2016-11-05}}</ref>

Most of [[County|counties]] of every country have their own [[library system]] that usually have between to 20 libraries on every city of their counties, some of them are; London Public Library (on Canada) with 16 library branches, [[Helsinki Metropolitan Area Libraries]] with 63 libraries,<ref>{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}</ref> [[National Library of Venezuela]] with 685 branches.

Some popular library branches includ [[New York Public Library Main Branch]], part of [[New York Public Library|New York Public Library System]], and [[Martin Luther King Jr. Memorial Library]], a branch of [[District of Columbia Public Library|District of Columbia Public Library System]].

==References==
{{Reflist}}

[[Category:Public libraries]]
[[Category:Private libraries]]
[[Category:Libraries]]
[[Category:Culture]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:918
<=====title=====>:
Plinian Core
<=====text=====>:
{{Multiple issues|
{{Orphan|date=June 2016}}
{{primary sources|date=January 2016}}
{{Underlinked|date=October 2016}}
}}

'''Plinian Core''' is a set of vocabulary terms that can be used to describe different aspects of [[biodiversity informatics|biological species information]]. Under "biological species Information" all kinds of properties or traits related to taxa—biological and non-biological—are included. Thus, for instance, terms pertaining descriptions,  legal aspects, conservation, management, demographics, nomenclature, or related resources are incorporated.

== Description ==

The '''Plinian Core''' is aimed to facilitate the exchange of information about the species and upper taxa.
	
What is in scope?
* 	Species level catalogs of any kind of biological objects or data. 
* 	Terminology associated with biological collection data. 
* 	Striving for compatibility with other biodiversity-related standards. 
* 	Facilitating the addition of components and attributes of biological data. 
	What is not in scope? 
* 	Data interchange protocols. 
* 	Non-biodiversity-related data. 
* 	Occurrence level data. 
	This standard is named after Pliny the Elder, a very influential figure in the study of the biological species.
	
'''Plinian Core''' design requirements includes: ease of use, to be self-contained, able to support data integration from multiple databases, and ability to handle different levels of granularity. Core terms can be grouped in its current version as follows:
* 	[[Metadata]]
* 	Base Elements
* 	Record Metadata
* 	[[Nomenclature#Nomenclature.2C classification and identification|Nomenclature and Classification]]
* 	[[Taxonomic description]]
* 	[[Natural history]]
* 	[[Invasive species]]
* 	[[Habitat|Habitat and Distribution]]
* 	[[Demography]] and Threats
* 	Uses, Management and Conservation
* 	associatedParty, MeasurementOrFact, References, AncillaryData

== Background ==
Plinian Core started as a collaborative project between [[Instituto Nacional de Biodiversidad]] and [[GBIF]] Spain in 2005. A series of iterations in which elements were defined and implanted in different projects resulted in a "Plinian Core Flat" [deprecated].

As a result, a new development was impulse to overcome them in 2012. New formal requirements, additional input and a will to better support the standard and its documentation, as well as to align it with the processes of [[TDWG]], the world reference body for biodiversity information standards.

A new version, '''Plinian Core v3.1''' was defined. This provides more flexibility to fully represent the information of a species in a variety of scenarios. New elements to deal with aspects such as IPR, related resources, referenced, etc. were introduced, and elements already included were better-defined and documented.

Partner for the development of Plinian Core in this new phase incorporated the [[University of Granada]] (UG, Spain), the [[Alexander von Humboldt Institute]] (IAvH, Colombia), the [[National Commission for the Knowledge and Use of Biodiversity]] (Conabio, Mexico) and the [[University of São Paulo]] (USP, Brazil).

A "Plinian Core Task Group" within TDWG "Interest Group on species Information"<ref>{{Cite web|title = TDWG: (TDWG) Species Information Interest Group - Charter|url = http://www.tdwg.org/activities/species-information/charter/|website = www.tdwg.org|access-date = 2016-01-29}}</ref> in being proposed.

== Levels of the standard ==
Plinian Core is presented in to levels: the '''abstract model''' and the '''application profiles'''.

The abstract model (AM), comprising the abstract model schema(xsd)  and the terms' URIs, is the normative part. It is all comprehensive, and allows for different levels of granularity in describing species properties. The AM should be taken as a "menu" from which to choose terms and level of detail needed in any specific project.

The subsets of the abstract model intended to be implemented in specific projects are the "application profiles" (APs). Besides containing part of the elements of the AM, APs can impose additional specifications on the included elements, such as controlled vocabularies.  Some exampes of APs in use follow:
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_CONABIO.xsd CONABIO]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_INBIO.xsd INBIO]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_GBIF_ES.xsd GBIF.ES]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_MAGRAMA.xsd MAGRAMA]
# Application profile [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/application%20profiles/stable%20version/PliC3.2.1_AP_SIB-COLOMBIA.xsd SIB-COLOMBIA]

== Relation to other standards ==
Plinian incorporates a number of elements already defined by other standards. The following table summarizes these standards and the elements used in Plinian Core:
{| class="wikitable"
!Standard
!Elements
|-
|[[Darwin Core]]
|taxonConceptID, Hierarchy, MeasurementOrFact, ResourceRelationShip.
|-
|[[Ecological Metadata Language]]
|associatedParty, keywordSet, coverage, dataset
|-
|[[Encyclopedia of Life]] Schema
|AncillaryData: DataObjectBase
|-
|[[Global Invasive Species Network]]
|origin, presence, persistence, distribution, harmful, modified, startValidDate, endValidDate, countryCode, stateProvince, county, localityName, county, language, citation, abundance...
|-
|[http://www.tdwg.org/schemas/tcs/1.01 Taxon Concept Schema. TCS]
|scientificName
|}

== External Links ==
* [http://www.github.com/PlinianCore Main page]
* [http://tools.gbif.org/dwca-validator/extensions.do An Implementation of Plinian Core as GBIF's IPT Extensions]
* [https://github.com/PlinianCore/Documentation/wiki/PlinianCore_Terms Plinian Core Terms Quick Reference Guide]
* [https://raw.githubusercontent.com/PlinianCore/Sources/master/xsd/abstract%20models/stable%20version/PlinianCore_AbstractModel_v3.2.1.xsd Plinian Core Abstract Model (xsd). Current version (3.2.1)]
* [http://www.tdwg.org/ Biodiversity Information Standards (TDWG)]
* [http://www.ingurumena.ejgv.euskadi.eus/r49-u95/es/contenidos/informacion/naturaeuskadi/es_def/adjuntos/plinian.pdf Sistema de información de la naturaleza de euskadi. Aplicación del estandar Plinian Core]
* [http://www.magrama.gob.es/es/biodiversidad/servicios/banco-datos-naturaleza/informacion-disponible/BDN_Modelos_Datos.aspx Estándar Plinian Core para la gestión integrada de la información sobre especies. Ministerio de Agricultura, Alimentación y Medio Ambiente de España]
* [http://patrimonio.ambiente.gob.ec/bndv/modelo.php Modelo conceptual de la Base Nacional de Datos de Vegetación. Ministerio del Ambiente, Ecuador]

== References ==
<references />

[[Category:Biodiversity informatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]
[[Category:Metadata standards]]
