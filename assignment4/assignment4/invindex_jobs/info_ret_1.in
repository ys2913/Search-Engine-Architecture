<=====doc_Id=====>:1
<=====title=====>:
Category:Data management
<=====text=====>:
{{Commons cat|Data management}}
*'''[[Data management]]''' — all the disciplines related to managing '''{{C|Data|data}}''' as a valuable resource.

{{clr}}
:::::{{Catmain|Data management}}
{{catdiffuse}}
{{CategoryTOC}}

{{Database}}
{{Databases}}

[[Category:Data|Management]]
[[Category:Computer data|Management]]
[[Category:Information retrieval]]
[[Category:Information technology management]]
<=====doc_Id=====>:4
<=====title=====>:
Category:Citation indices
<=====text=====>:
{{Cat main|Citation index}}
{{distinguish|Category:Bibliographic databases and indexes|Category:Citation metrics}}

[[Category:Bibliometrics|Indices]]
[[Category:Reference works]]
[[Category:Indexes]]
[[Category:Information retrieval]]
[[Category:Bibliographic databases and indexes| ]]
<=====doc_Id=====>:7
<=====title=====>:
Comprehensive Model of Information Seeking
<=====text=====>:
The '''Comprehensive Model of Information Seeking''', or CMIS, is a theoretical construct designed to predict how people will seek information.  It was first developed by J. David Johnson and has been utilized by a variety of disciplines including [[Library and Information Science]] and [[Health Communication]].

The CMIS has been empirically tested in health and organizational contexts<ref>Johnson, J. D., & Meischke, H. (1993). Cancer-related channel selection:  An extensionfor a sample of women who have had a mammogram. Women & Health, 20, 31-44.; Johnson, J. D., Donohue, W. A., Atkin, C. K., & Johnson, S. H. (1995). A comprehensive model of information seeking: Tests focusing on a technical organization. 
Science Communication, 16, 274-303.</ref> The CMIS has inherent strengths for studying how people react to health problems such as cancer.<ref name="auto">Johnson, J. D., Andrews, J. E. & Allard, S. (2001). A Model for Understanding and Affecting Genetics Information Seeking. Library and Information Science Research 23(4): 335-349.</ref> The CMIS specifies ''antecedents'' that explain why people become information seekers, ''information carrier characteristics'' that shape how people go about looking for information, and ''information seeking actions'' that reflect the nature of the search itself.

==Design==

[[File:Diagram of the Comprehensive Model of Information Seeking.jpg|thumb|right|The Comprehensive Model of Information Seeking]]
The CMIS has been quantitatively tested and performs well when it comes to health information seeking behaviors (HISB).<ref name="auto"/> There are three main schemas in the CMIS. These are:  Antecedents, information field, and information seeking actions.  The antecedents are those factors that determine how an information consumer will receive the information.  Those factors are:  Demographics, personal experience, salience, and beliefs.  These factors are fluid and can change during the health information seeking process.  The second schema is the information fields that consist of characteristics and utilities.  This schema is concerned with the channels and carriers of information.  A person’s understanding is developed through the information field.  The third schema involves the transformational processes and measured by the consumer’s understanding of the messages received through the information field.  The final schema involves information seeking actions.  This is what the consumer does as a result of the first two schemas through information seeking.  There are three major dimensions:  the scope, depth, and method of information seeking.<ref name="auto"/>

==Antecedents==
The CMIS antecedents—demographics, personal experience, salience, and beliefs—are factors that determine an individual's natural predisposition to search for information from particular information carriers. Certain types of health information seeking can be triggered by an individual's degree of personal experience with disease.<ref>Johnson, J. D. (1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref> In the CMIS framework, two personal relevance factors, salience and beliefs, are seen as the primary determinants in translating a perceived gap into an active search for information. Salience refers to the personal significance of health information to the individual, such as perceptions of risk to one's health, which are likely to result in information seeking action. However, people also may be motivated to gather information to determine the implications of health events for themselves and/or others related to their future activities, a factor directly related to the rapidly growing field of genetics. An individual's beliefs about the nature of a particular disease, its impacts, and level of control, all directly relate to self-efficacy, one of our key variables, and one that plays an important role in information seeking and people's more general pattern of actions related to health.<ref>Johnson, J. D.(1997). Cancer-related information seeking. Cresskill, NJ: Hampton Press.</ref>

==Information Carrier Characteristics==

The information carrier characteristics are drawn from a model of Media Exposure and Appraisal (MEA) that has been tested on a variety of information carriers, including both sources and channels, and in a variety of cultural settings. Following the MEA, the CMIS focuses on editorial tone, communication potential, and utility. In the CMIS, characteristics are composed of editorial tone, which reflects an audience member's perception of credibility, while communication potential relates to issues of style and comprehensiveness. Utility relates the characteristics of a medium directly to the needs of an individual, and shares much with the uses and gratifications perspectives. For example, is the information contained in the medium relevant, topical, and important for the individual's purposes? In general, utility is very important for health information seeking.<ref name="auto"/>

==Information Seeking Actions==

There are several types of information seeking actions that can result from the impetus provided by the factors identified by the CMIS. For example, search behavior can be characterized by its extent, or the number of activities carried out, which has two components: scope, the number of alternatives investigated; and, depth, the number of dimensions of an alternative investigated. There is also the method of the search, or channel, as another major dimension of the search.  For instance, an individual might choose the method of consulting a telephone information service, decide to have a narrow scope by only asking questions about smoking cessation clinics, but investigate every recommendation in detail, thus increasing the depth of the search.<ref name="auto"/>

==Stages in the CMIS==

A key concept from the CMIS is the notion of “stages,” or “cancer involvement”.  According to the CMIS, an individual may be at one of four stages regarding a cancer threat, and thereby have differing information needs and behaviors.

The first stage, ''Casual'', is characterized by a general lack of concern or interest. At this stage, individuals are not purposive in their search for cancer-related information; rather, their search is accidental and aimless, even apathetic.
 
The second stage is ''Purposive-Placid''. This is characterized by the question, “What can I do to prevent cancer?” Individuals here might have some passing interest in cancer or genetic information, but are generally still not affected or directly concerned.

The third stage is ''Purposive-Clustered''. Here, an individual will be in closer proximity to cancer. This is the point at which a person is motivated to look for practical information that will address the specific problem. For example, a first-degree relative of a recently diagnosed breast cancer patient may seek genetic screening or [[BRCA mutation|BRCA]] 1/2 testing. The person could clearly benefit from such information- seeking behavior since medical authorities acknowledge that early detection of cancer leads to earlier treatments and better treatment outcomes.

The fourth stage, ''Directed'', includes individuals who have been diagnosed as having cancer. Such individuals need knowledge for making informed decisions about treatment and management of the disease.<ref name="auto"/>

== References ==
{{Research help|Med}}
{{Reflist}}



[[Category:Communication]]
[[Category:Information retrieval]]
[[Category:Health sciences]]
<=====doc_Id=====>:10
<=====title=====>:
Category:Information retrieval systems
<=====text=====>:
[[Category:Information retrieval]]
<=====doc_Id=====>:13
<=====title=====>:
Category:Information retrieval genres
<=====text=====>:
[[Category:Information retrieval]]
<=====doc_Id=====>:16
<=====title=====>:
Evaluation measures (information retrieval)
<=====text=====>:
{{Orphan|date=June 2016}}

The '''evaluation measures''' of an information retrieval system is the process of assessing how well the search results satisfied the user's query intent. The metrics are often split in to multiple categories. Online metrics measure actual users' interactions with the search system. Offline metrics measure the relevance of the search engine by having expert judges measure how likely each result (or the SERP page as a whole) is to meet the information needs of the user.

The mathematical symbols used in the formulas below mean:
* <math>X \cap Y</math> - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in ''both'' sets X and Y
* <math>| X |</math> - [[Cardinality]] - in this case, the number of documents in set X
* <math>\int</math> - [[Integral]]
* <math>\sum</math> - [[Summation]]
* <math>\Delta</math> - [[Symmetric difference]]

== Online metrics ==
Online metrics are generally created from data mined from search logs. The metrics are often used to determine the success of an [[A/B testing|A/B test]].

=== Session abandonment rate ===
Session abandonment rate is a ratio of search session which do not result in a click.

=== Click-through rate ===
Click-through rate ('''CTR''') is the ratio of users who click on a specific link to the number of total users who view a page, email, or advertisement. It is commonly used to measure the success of an [[online advertising]] campaign for a particular website as well as the effectiveness of email campaigns.<ref name=AMA>[[American Marketing Association]] Dictionary. http://www.marketingpower.com/_layouts/Dictionary.aspx.{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} Retrieved 2012-11-02. The [[Marketing Accountability Standards Board (MASB)]] endorses this definition as part of its ongoing [http://www.commonlanguage.wikispaces.net/ Common Language in Marketing Project].</ref>

=== Session success rate ===
Session success rate measures the ratio of user sessions that lead to a success. Defining "success" is often dependent on context, but for search a successful result is often measured using [[Dwell time (information retrieval)|dwell time]] as a primary factor along with secondary user interaction, for instance, the user copying the result URL is considered a successful result, as is copy/pasting from the snippet.

=== Zero result rate ===
Zero result rate ('''ZRR''') is the ratio of [[Search engine results page|SERPs]] which returned with zero results. The metric either indicates a [[Precision and recall|recall]] issue, or that the information being searched for is not in the index.

== Offline metrics ==
Offline metrics are generally created from relevance judgement sessions where the judges score the quality of the search results.  The judges often score each result of a query as either binary (good/bad), or on a multi-level scale of satisfying the needs of the searcher. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy. For instance there is ambiguity in the query "mars", the judge does not know if the user is search for [[Mars]] the planet, [[Mars (chocolate bar)|Mars]] the chocolate bar, or [[Bruno Mars]] the singer.

=== Precision ===
{{main|Precision and recall}}

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:<math> \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} </math>

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===
{{main|Precision and recall}}

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:<math>\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} </math>

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:<math> \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} </math>

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\mbox{specificity})</math>. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-score / F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:<math>F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}</math>

This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.

The general formula for non-negative real <math>\beta</math> is:
:<math>F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,</math>

Two other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that <math>F_\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}</math>.  Their relationship is:
:<math>F_\beta = 1 - E</math> where <math>\alpha=\frac{1}{1 + \beta^2}</math>

F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}

=== Average precision ===
<!-- [[Average precision]] redirects here -->
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref>
:<math>\operatorname{AveP} = \int_0^1 p(r)dr</math>
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:<math>\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)</math>
where <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\Delta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />

This finite sum is equivalent to:
:<math> \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!</math>
where <math>\operatorname{rel}(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06–11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:<ref name="voc2010" /><ref name="nlpbook" />
:<math>\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)</math>
where <math>p_{\operatorname{interp}}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>:
:<math>p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})</math>.

An alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves] {{webarchive |url=https://web.archive.org/web/20121208201457/http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf |date=December 8, 2012 }}. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.</ref>

=== Precision at K ===

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.<ref name="stanford" />  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.

=== R-Precision ===

R-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, <math>R</math>, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant <math>r</math> turns that into a relevancy fraction: <math>r/R = r/15</math>.<ref name="trec15"/>

Precision is equal to recall at the '''R'''-th position.<ref name="stanford">{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze}}  Part of ''Introduction to Information Retrieval'' [http://nlp.stanford.edu/IR-book/]</ref>

Empirically, this measure is often highly correlated to mean average precision.<ref name="stanford" />

=== Mean average precision ===
<!-- [[Mean average precision]] redirects here -->
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:<math> \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!</math>
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position <math>p</math> is defined as:

:<math> \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. </math>

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:

:<math> \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. </math>

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other measures ===
{{Confusion matrix terms}}
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]
* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents<ref name="trec15">http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf</ref>
* GMAP - geometric mean of (per-topic) average precision<ref name="trec15" />
* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}

===Visualization===

Visualizations of information retrieval performance include:
* Graphs which chart precision on one axis and recall on the other<ref name="trec15" />
* Histograms of average precision over various topics<ref name="trec15" />
* [[Receiver operating characteristic]] (ROC curve)
* [[Confusion matrix]]

== Non-metrics ==

=== Top queries list ===
Top queries is noting the most common queries over a fixed amount of time. The top queries list assists in knowing the style of queries entered by users.

== Non-relevance metrics ==

=== Queries per time ===
Measuring how many queries are performed on the search system per (month/day/hour/minute/sec) tracks the utilization of the search system. It can be used for diagnostics to indicate an unexpected spike in queries, or simply as a baseline when comparing with other metrics, like query latency. For example, a spike in query traffic, may be used to explain a spike in query latency.

==References==
<references />

[[Category:Information retrieval]]
[[Category:Information retrieval evaluation]]
[[Category:Internet search engines]]
<=====doc_Id=====>:19
<=====title=====>:
Metadirectory
<=====text=====>:
A '''metadirectory''' system provides for the flow of data between one or more [[directory service]]s and [[database]]s, in order to maintain synchronization of that data, and is an important part of [[identity management]] systems. The data being synchronized typically are collections of entries that contain user profiles and possibly authentication or policy information. Most metadirectory deployments synchronize data into at least one [[Lightweight Directory Access Protocol|LDAP]]-based directory server, to ensure that LDAP-based applications such as [[single sign-on]] and portal servers have access to recent data, even if the data is mastered in a non-LDAP data source.

Metadirectory products support filtering and transformation of data in transit.

Most [[identity management]] suites from commercial vendors include a metadirectory product, or a [[provisioning#User provisioning|user provisioning]] product.

== See also ==
* [[Virtual directory]]
* [[Identity correlation]]
* [[Microsoft Identity Integration Server]]
* [[Novell Identity Manager]]
* [[Critical Path, Inc.|Critical Path Metadirectory]]

[[Category:Directory services]]
[[Category:Data management]]
<=====doc_Id=====>:22
<=====title=====>:
Category:Query languages
<=====text=====>:
This [[Wikipedia:Category|category]] lists those [[domain-specific programming language]]s targeted at performing [[database]] [[query language|queries]].

[[Category:Domain-specific programming languages]]
[[Category:Data management]]
[[Category:Databases]]
<=====doc_Id=====>:25
<=====title=====>:
Category:Computer file systems
<=====text=====>:
{{catdiffuse}}
{{Cat main|File system}}
{{Commonscat|File systems}}
A '''[[file system]]''' in computing, is a method for storing and organizing [[computer file]]s and the data they contain to make it easy to find and access them. File systems may use a [[data storage device]] such as a [[hard disk]] or [[CD-ROM]] and involve maintaining the physical location of the files, or they may be virtual and exist only as an access method for virtual data or for data over a network (e.g. [[Network File System (protocol)|NFS]]).

More formally, a file system is a set of [[abstract data type]]s that are implemented for the storage, hierarchical organization, manipulation, navigation, access, and retrieval of [[data]].

== See also ==
* [[:Category:Computer storage]]

[[Category:Data management]]
[[Category:Operating system technology|File systems]]
<!--[[Category:Computer systems|File systems]] deleted  "Computer systems" not an index of systems -->
[[Category:Storage software]]
<=====doc_Id=====>:28
<=====title=====>:
Data bank
<=====text=====>:
In [[telecommunication]]s, a '''data bank''' is a repository of information on one or more subjects that is organized in a way that facilitates local or remote information retrieval. A data bank may be either centralized or decentralized.
In computers the data bank is the same as in telecommunication (i.e. it is the repository of data. The data in the data bank can be things such as credit card transactions or it can be any data base of a company where large quantities of queries are being processed on daily bases). 
 
'''Data bank''' may also refer to an organization primarily concerned with the construction and maintenance of a [[database]].

== See also ==

* [[Star Wars Databank]]
* [[Protein Data Bank]]
* [[National Trauma Data Bank]]
* [[memory bank]]
* [[International Tree-Ring Data Bank]]
* [[Hazardous Substances Data Bank]]
* [[electron microscopy data bank]]
* [[Dortmund Data Bank]]
* [[Casio Databank]]
* [[conformational dynamics data bank]]
* [[Databank Systems Limited]] a former New Zealand banking agency

==Sources==
*{{FS1037C MS188}}
*''[[The American Heritage Dictionary of the English Language]], Fourth Edition''. [[Houghton Mifflin]], 2000.

==External links==
{{wiktionary}}

[[Category:Data management]]


{{telecomm-stub}}
<=====doc_Id=====>:31
<=====title=====>:
Category:Metadata
<=====text=====>:
{{Infobox library classification|CC = |DDC = |LCC = |UDC = 001.103.2}}
{{catdiffuse}}
{{Cat main|Metadata}}

[[Category:Data management]]
[[Category:Data]]
<=====doc_Id=====>:34
<=====title=====>:
Data Transformation Services
<=====text=====>:
'''Data Transformation Services''', or '''DTS''', is a set of objects and utilities to allow the automation of [[extract, transform, load|extract, transform and load]] operations to or from a database. The objects are DTS packages and their components, and the utilities are called DTS tools. DTS was included with earlier versions of [[Microsoft SQL Server]], and was almost always used with SQL Server databases, although it could be used independently with other databases.

DTS allows data to be transformed and loaded from [[heterogeneous]] sources using [[OLE DB]], [[ODBC]], or text-only files, into any supported [[database]]. DTS can also allow automation of data import or transformation on a scheduled basis, and can perform additional functions such as [[File Transfer Protocol|FTPing]] files and executing external programs. In addition, DTS provides an alternative method of version control and backup for packages when used in conjunction with a version control system, such as [[Microsoft Visual SourceSafe]].
[[Image:DTS Designer screenshot.PNG|right|thumb|300px|Here a DTS package is edited with DTS Designer in [[Windows XP]].]] DTS has been superseded by [[SQL Server Integration Services]] in later releases of Microsoft SQL Server though there was some backwards compatibility and ability to run DTS packages in the new SSIS for a time.

__TOC__
{{clear}}

==History==

In SQL Server versions 6.5 and earlier, [[database administrators]] (DBAs) used [[SQL Server Transfer Manager]] and [[Bulk Copy Program]], included with SQL Server, to transfer data. These tools had significant shortcomings, and many{{quantify|date=May 2014}} DBAs used third-party tools such as [[Pervasive Data Integrator]] to transfer data more flexibly and easily. With the release of SQL Server 7 in 1998, "Data Transformation Services" was packaged with it to replace all these tools.

SQL Server 2000 expanded DTS functionality in several ways. It introduced new types of tasks, including the ability to [[File Transfer Protocol|FTP]] files, move databases or database components, and add messages into [[Microsoft Message Queuing|Microsoft Message Queue]]. DTS packages can be saved as a Visual Basic file in SQL Server 2000, and this can be expanded to save into any COM-compliant language. Microsoft also integrated packages into [[Windows 2000 security]] and made DTS tools more user-friendly;  tasks can accept input and output parameters.

DTS comes with all editions of SQL Server 7 and 2000, but was superseded by [[SQL Server Integration Services]] in the Microsoft SQL Server 2005 release in 2005.

== DTS packages ==
The DTS package is the fundamental logical component of DTS; every DTS object is a [[Information hiding|child component]] of the package. Packages are used whenever one modifies data using DTS. All the [[metadata]] about the data transformation is contained within the package. Packages can be saved directly in a SQL Server, or can be saved in the [[Microsoft Repository]] or in [[Component Object Model|COM]] files. SQL Server 2000 also allows a programmer to save packages in a [[Visual Basic]] or other language file (when stored to a VB file, the package is actually scripted—that is, a VB script is executed to dynamically create the package objects and its component objects).

A package can contain any number of [[ActiveX Data Objects|connection objects]], but does not have to contain any. These allow the package to read data from any [[OLE DB]]-compliant data source, and can be expanded to handle other sorts of data. The functionality of a package is organized into ''tasks'' and ''steps''.

A DTS Task is a discrete set of functionalities executed as a single step in a DTS package. Each task defines a work item to be performed as part of the data movement and data transformation process or as a job to be executed.

Data Transformation Services supplies a number of tasks that are part of the DTS [[object model]] and that can be accessed graphically through the DTS Designer or accessed programmatically. These tasks, which can be configured individually, cover a wide variety of data copying, data transformation and notification situations. For example, the following types of tasks represent some actions that you can perform by using [[Data transformation service|DTS]]: executing a single SQL statement, sending an email, and transferring a file with FTP.

A step within a DTS package describes the order in which tasks are run and the precedence constraints that describe what to do in the case damage or of failure. These steps can be executed sequentially or in parallel.

Packages can also contain [[global variable]]s which can be used throughout the package. SQL Server 2000 allows input and output parameters for tasks, greatly expanding the usefulness of global variables. DTS packages can be edited, password protected, scheduled for execution, and retrieved by version.

==DTS tools==
DTS tools packaged with SQL Server include the DTS wizards, DTS Designer, and DTS Programming Interfaces.

===DTS wizards===
The DTS [[Wizard (software)|wizards]] can be used to perform simple or common DTS tasks. These include the ''Import/Export Wizard'' and the ''Copy of Database Wizard''. They provide the simplest method of copying data between [[OLE DB]] data sources. There is a great deal of functionality that is not available by merely using a wizard. However, a package created with a wizard can be saved and later altered with one of the other DTS tools.

A ''Create Publishing Wizard'' is also available to schedule packages to run at certain times. This only works if [[SQL Server Agent]] is running; otherwise the package will be scheduled, but will not be executed.

===DTS Designer===
The DTS Designer is a [[graphical tool]] used to build complex DTS Packages with workflows and event-driven logic. DTS Designer can also be used to edit and customize DTS Packages created with the DTS wizard.

Each connection and task in DTS Designer is shown with a specific [[Icon (computing)|icon]]. These icons are joined with precedence constraints, which specify the order and requirements for tasks to be run. One task may run, for instance, only if another task succeeds (or fails). Other tasks may run concurrently.

The DTS Designer has been criticized for having unusual quirks and limitations, such as the inability to visually [[copy and paste]] multiple tasks at one time. Many of these shortcomings have been overcome in [[SQL Server Integration Services]], DTS's successor.

===DTS Query Designer===
A graphical tool used to build [[Information retrieval|queries]] in DTS.

===DTS Run Utility===
DTS Packages can be run from the command line using the DTSRUN Utility.<BR/>
The utility is invoked using the following  syntax: 
<PRE>
dtsrun /S server_name[\instance_name]
        { {/[~]U user_name [/[~]P password]} | /E }
    ]
    {    
        {/[~]N package_name }
        | {/[~]G package_guid_string}
        | {/[~]V package_version_guid_string}
    }
    [/[~]M package_password]
    [/[~]F filename]
    [/[~]R repository_database_name]
    [/A global_variable_name:typeid=value] 
    [/L log_file_name]
    [/W NT_event_log_completion_status]
    [/Z] [/!X] [/!D] [/!Y] [/!C]
]
</PRE>

When passing in parameters which are mapped to Global Variables, you are required to include the typeid. This is rather difficult to find on the Microsoft site. Below are the TypeIds used in passing in these values.

{| class="wikitable sortable"
! Type !! typeid
|-
| Boolean
| align="right" | 11
|-
| Currency
| align="right" | 6
|-
| Date
| align="right" | 7
|-
| Decimal
| align="right" | 14
|-
| HRESULT
| align="right" | 25
|-
| Int
| align="right" | 22
|-
| Integer (1-byte)
| align="right" | 16
|-
| Integer (8-byte)
| align="right" | 20
|-
| Integer (small)
| align="right" | 2
|-
| LPWSTR
| align="right" | 31
|-
| Pointer
| align="right" | 26
|-
| Real (4-byte)
| align="right" | 4
|-
| Real (8-byte)
| align="right" | 5
|-
| String
| align="right" | 8
|-
| Unsigned int (1-byte)
| align="right" | 17
|-
| Unsigned int (2-byte)
| align="right" | 18
|-
| Unsigned int (4-byte)
| align="right" | 19
|-
| Unsigned int (1-byte)
| align="right" | 21
|-
| Unsigned int
| align="right" | 23
|}

==See also==

* [[OLAP]]
* [[Data warehouse]]
* [[Data mining]]
* [[SQL Server Integration Services]]
* [[Meta Data Services]]

==References==
* {{cite book |author1=Chaffin, Mark |author2=Knight, Brian |author3=Robinson, Todd | title=Professional SQL Server 2000 DTS | publisher=[[Wrox Press]] (Wiley Publishing, Inc.) | year=2003 | isbn=0-7645-4368-7}}

== External links ==
* [http://msdn2.microsoft.com/en-us/library/aa933484(SQL.80).aspx Microsoft SQL Server: Data Transformation Services (DTS)]
* [http://www.sqldts.com/ SQL DTS unique DTS information resource]
* [http://support.microsoft.com/kb/238912 Understanding Microsoft Repository]
* [http://pragmaticworks.com/Resources/webinars/Default.aspx DTS Videos & Training]
* [http://www.softrus.org/dts/ DTS Documenter]

[[Category:Microsoft database software]]
[[Category:Data management]]
[[Category:Extract, transform, load tools]]
[[Category:Microsoft server technology]]
<=====doc_Id=====>:37
<=====title=====>:
Control break
<=====text=====>:
{{unreferenced|date=August 2016}}
{{dablink|Not to be confused with 'Control-Break' displayed in MS-DOS when cancelling an ongoing task by pressing [[break key|Ctrl+Break]] key combination.}}
In [[computer programming]] a '''control break''' is a change in the value of one of the [[Key field|key]]s on which a file is sorted which requires some extra processing.  For example, with an input file sorted by post code, the number of items found in each postal district might need to be printed on a report, and a heading shown for the next district.  Quite often there is a hierarchy of nested control breaks in a program, e.g. streets within districts within areas, with the need for a grand total at the end. [[Structured programming]] techniques have been developed to ensure correct processing of control breaks in languages such as [[COBOL]] and to ensure that conditions such as empty input files and [[sequence error]]s are handled properly.

With [[fourth generation language]]s such as [[SQL]], the programming language should handle most of the details of control breaks automatically.

[[Category:Conditional constructs]]
[[Category:Data management]]
<=====doc_Id=====>:40
<=====title=====>:
Data architecture
<=====text=====>:
{{Refimprove|date=November 2008}}
In [[information technology]], '''data architecture''' is composed of models, policies, rules or standards that govern which data is collected, and how it is stored, arranged, integrated, and put to use in data systems and in organizations.<ref>[http://www.businessdictionary.com/definition/data-architecture.html Business Dictionary - Data Architecture]</ref>  Data is usually one of several [[architecture domain]]s that form the pillars of an [[enterprise architecture]] or [[solution architecture]].<ref>[http://www.learn.geekinterview.com/data-warehouse/data-architecture/what-is-data-architecture.html What is data architecture] GeekInterview, 2008-01-28, accessed 2011-04-28</ref>

== Overview ==
A data architecture should{{POV statement|date=March 2013}} set data standards for all its data systems as a vision or a model of the eventual interactions between those data systems. [[Data integration]], for example, should be dependent upon data architecture standards since data integration requires data interactions between two or more data systems. A data architecture, in part, describes the [[data structure]]s used by a business and its computer [[applications software]]. Data architectures address data in storage and data in motion; descriptions of data stores, data groups and data items; and [[data mapping|mappings]] of those data artifacts to data qualities, applications, locations etc.

Essential to realizing the target state, Data Architecture describes how data is processed, stored, and utilized in an [[information system]]. It provides criteria for [[data processing]] operations so as to make it possible to design [[data flow]]s and also control the flow of data in the system.

The [[data architect]] is typically responsible for defining the target state, aligning during development and then following up to ensure enhancements are done in the spirit of the original blueprint.

During the definition of the target state, the Data Architecture breaks a subject down to the atomic level and then builds it back up to the desired form. The data architect breaks the subject down by going through 3 traditional architectural processes:
* Conceptual - represents all business entities.
* Logical - represents the logic of how entities are related.
* Physical - the realization of the data mechanisms for a specific type of functionality.

The "data" column of the [[Zachman Framework]] for enterprise architecture &ndash;

{| border=1
|'''Layer''' || '''View''' || '''Data (What)''' || '''Stakeholder'''
|-
|1||'''Scope/Contextual''' || List of things and architectural standards<ref>[http://www.strins.com/data-architecture-standards.html Data Architecture Standards]</ref> important to the business || Planner
|-
|2||'''Business Model/Conceptual'''  || Semantic model or [[Entity-relationship model|Conceptual]]/[http://tdan.com/the-enterprise-data-model/5205 Enterprise Data Model] || Owner
|-
|3||'''System Model/Logical''' || Enterprise/[[Logical data model|Logical Data Model]] || Designer
|-
|4||'''Technology Model/Physical''' || [[Physical data model|Physical Data Model]] || Builder
|-
|5||'''Detailed Representations'''  ||  Actual [[database]]s || Subcontractor
|}

In this second, broader sense, data architecture includes a complete analysis of the relationships among an organization's functions, available [[technologies]], and [[data type]]s.

Data architecture should be defined in the '''planning phase''' of the design of a new data processing and storage system. The major types and sources of data necessary to support an enterprise should be identified in a manner that is complete, consistent, and understandable. The primary requirement at this stage is to define all of the relevant '''data entities''', not to specify [[computer hardware]] items. A data entity is any real or abstracted thing about which an organization or individual wishes to store data.

== Physical data architecture ==
Physical data architecture of an information system is part of a [[Technology roadmapping|technology plan]]. As its name implies, the technology plan is focused on the actual tangible [[element (mathematics)|elements]] to be used in the implementation of the data architecture [[design]]. Physical data architecture encompasses database architecture. Database architecture is a [[Model (abstract)|schema]] of the actual database technology that will support the designed data architecture.

== Elements of data architecture ==
Certain elements must be defined during the design phase of the data architecture schema. For example, administrative structure that will be established in order to manage the data resources must be described. Also, the methodologies that will be employed to store the data must be defined. In addition, a description of the database technology to be employed must be generated, as well as a description of the processes that will manipulate the data. It is also important to design [[interface (computing)|interfaces]] to the data by other systems, as well as a design for the [[infrastructure]] that will support common data operations (i.e. emergency procedures, [[data import]]s, [[data backup]]s, external [[data transfer|transfers of data]]).

Without the guidance of a properly implemented data architecture design, common data operations might be implemented in different ways, rendering it difficult to understand and control the flow of data within such systems. This sort of fragmentation is highly undesirable due to the potential increased cost, and the data disconnects involved. These sorts of difficulties may be encountered with rapidly growing enterprises and also enterprises that service different lines of [[business]] (e.g. [[insurance]] [[Product (business)|products]]).

Properly executed, the data architecture phase of information system planning forces an organization to precisely specify and describe both internal and external information flows. These are patterns that the organization may not have previously taken the time to conceptualize. It is therefore possible at this stage to identify costly information shortfalls, disconnects between departments, and disconnects between organizational systems that may not have been evident before the data architecture analysis.<ref>{{cite book|last=Mittal|first=Prashant|title=Author|year=2009|publisher=Global India Publications|location=pg 256|isbn=978-93-8022-820-4|pages=314|url=https://books.google.com/books?id=BpkhYDj4tm0C&dq=inauthor:%22PRASHANT+MITTAL%22&source=gbs_navlinks_s}}</ref>

== Constraints and influences ==
Various constraints and influences will have an effect on data architecture design. These include enterprise requirements, technology drivers, economics, business policies and data processing needs.

; Enterprise requirements: These will generally include such elements as economical and effective system expansion, acceptable performance levels (especially system access speed), [[Financial transaction|transaction]] reliability, and transparent [[data management]]. In addition, the [[Data conversion|conversion]] of raw data such as transaction [[Record (computer science)|records]] and [[image]] [[Computer file|files]] into more useful [[information]] forms through such features as [[data warehouse]]s is also a common organizational [[requirement]], since this enables managerial decision making and other organizational processes. One of the architecture techniques is the split between managing [[transaction data]] and (master) [[reference data]]. Another one is splitting [[Automatic identification and data capture|data capture systems]] from data retrieval systems (as done in a data warehouse).

; Technology drivers: These are usually suggested by the completed data architecture and database architecture designs. In addition, some technology drivers will derive from existing organizational integration frameworks and standards, organizational economics, and existing site resources (e.g. previously purchased [[software licensing]]).

; Economics: These are also important factors that must be considered during the data architecture phase. It is possible that some solutions, while optimal in principle, may not be potential candidates due to their cost. External factors such as the [[business cycle]], interest rates, market conditions, and legal considerations could all have an effect on decisions relevant to data architecture.

; Business policies: [[Business policies]] that also drive data architecture design include internal organizational policies, rules of [[regulatory agency|regulatory bodies]], professional standards, and applicable governmental [[laws]] that can vary by applicable [[government agency|agency]]. These policies and rules will help describe the manner in which enterprise wishes to process their data.

; Data processing needs: These include accurate and reproducible [[data transaction|transactions]] performed in high volumes, data warehousing for the support of management information systems (and potential [[data mining]]), repetitive periodic [[Data reporting|reporting]], ad hoc reporting, and support of various organizational initiatives as required (i.e. annual budgets, new [[Product (business)|product]] development).

== See also ==
* [[Enterprise Information Security Architecture]] - (EISA) positions data security in the enterprise information framework.
* [[FDIC Enterprise Architecture Framework]]
* [[Controlled vocabulary]]
* [[Information silo]]
* [[Disparate system]]
* [[Data Warehouse]]

== References ==
{{reflist}}

== Further reading ==
* Bass, L.; John, B.; & Kates, J. (2001). ''Achieving Usability Through Software Architecture'', Carnegie Mellon University.
* Lewis, G.; Comella-Dorda, S.; Place, P.; Plakosh, D.; & Seacord, R., (2001). ''Enterprise Information System Data Architecture Guide'' Carnegie Mellon University.
* Adleman, S.; Moss, L.; Abai, M. (2005). ''Data Strategy'' Addison-Wesley Professional.

== External links ==
{{commons category|Data architecture}}
* [http://www.sei.cmu.edu/library/abstracts/reports/01tr005.cfm Achieving Usability Through Software Architecture], sei.cmu.edu 2001
* [http://sunsite.uakom.sk/sunworldonline/swol-07-1998/swol-07-itarchitect.html The Logical Data Architecture], by Nirmal Baid

{{Data model}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:Enterprise architecture]]
<=====doc_Id=====>:43
<=====title=====>:
Data archaeology
<=====text=====>:
{{for|the computer-based analysis of archaeological data|Computational archaeology}}

'''Data archaeology''' refers to the art and science of recovering [[computer]] [[data]] encoded and/or encrypted in now obsolete [[Computer media|media]] or [[content format|formats]]. Data archaeology can also refer to recovering information from damaged electronic formats after natural or man made disasters.

The term originally appeared in 1993 as part of the [[Global Oceanographic Data Archaeology and Rescue Project]] (GODAR). The original impetus for data archaeology came from the need to recover computerized records of climatic conditions stored on old computer tape, which can provide valuable evidence for testing theories of [[climate change]]. These approaches allowed the reconstruction of an image of the Arctic that had been captured by the [[Nimbus program|Nimbus 2]] satellite on September 23, 1966, in higher resolution than ever seen before from this type of data.<ref>[http://nsidc.org/monthlyhighlights/january2010.html Techno-archaeology rescues climate data from early satellites] U.S. National Snow and Ice Data Center (NSIDC), January 2010 [http://www.webcitation.org/5xN1sNyDp Archived]</ref>

[[NASA]] also utilizes the services of data archaeologists to recover information stored on 1960s era vintage computer tape, as exemplified by the [[Lunar Orbiter Image Recovery Project]] (LOIRP).<ref>[http://www.nasa.gov/topics/moonmars/features/LOIRP/ LOIRP Overview] NASA website November 14, 2008 [http://www.webcitation.org/5xN1DjLG4 Archived]</ref>

==Recovery==
It is also important to make the distinction in data archaeology between data recovery, and data intelligibility. You may be able to recover the data, but not understand it. For data archaeology to be effective the data must be intelligible.<ref name="www.ukoln.ac.uk"> [http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf] Study on website October 23, 2011 </ref>

===Disaster recovery===
Data archaeologists can also use [[data recovery]] after natural disasters such as fires, floods, earthquakes, or even hurricanes. For example, in 1995 during [[Hurricane Marilyn]] the National Media Lab assisted the [[National Archives and Records Administration]] in recovering data at risk due to damaged equipment. The hardware was damaged from rain, salt water, and sand, yet it was possible to clean some of the disks and refit them with new cases thus saving the data within.<ref name="www.ukoln.ac.uk"/>

===Recovery techniques===
When deciding whether or not to try and recover data, the cost must be taken into account. If there is enough time and money, most data will be able to be recovered. In the case of [[magnetic media]], which are the most common type used for data storage, there are various techniques that can be used to recover the data depending on the type of damage.<ref name="www.ukoln.ac.uk"/>{{rp|17}}

Humidity can cause tapes to become unusable as they begin to deteriorate and become sticky.  In this case, a heat treatment can be applied to fix this problem, by causing the oils and residues to either be reabsorbed into the tape or evaporate off the surface of the tape.  However, this should only be done in order to provide access to the data so it can be extracted and copied to a medium that is more stable.<ref name="www.ukoln.ac.uk"/>{{rp|17–18}}

Lubrication loss is another source of damage to tapes. This is most commonly caused by heavy use, but can also be a result of improper storage or natural evaporation.  As a result of heavy use, some of the lubricant can remain on the read-write heads which then collect dust and particles.  This can cause damage to the tape.  Loss of lubrication can be addressed by re-lubricating the tapes.  This should be done cautiously, as excessive re-lubrication can cause tape slippage, which in turn can lead to media being misread and the loss of data.<ref name="www.ukoln.ac.uk"/>{{rp|18}}

Water exposure will damage tapes over time.  This often occurs in a disaster situation.  If the media is in salty or dirty water, it should be rinsed in fresh water.  The process of cleaning, rinsing, and drying wet tapes should be done at room temperature in order to prevent heat damage.  Older tapes should be recovered prior to newer tapes, as they are more susceptible to water damage.<ref name="www.ukoln.ac.uk"/>{{rp|18}}

==Prevention==
To prevent the need of data archaeology, creators and holders of digital documents should take care to employ [[digital preservation]].

==See also==
* [[Data degradation|Bit rot]]
* [[Digital dark age]]
* [[Knowledge discovery]]

==References==
<references />
*[http://www.worldwidewords.org/turnsofphrase/tp-dat1.htm World Wide Words: Data Archaeology]
*O'Donnell, James Joseph.  ''Avatars of the Word:  From Papyrus to Cyperspace''  Harvard University Press, 1998.
* {{cite book | last1  = Ross | first1 = Seamus | last2  = Gow | first2 = Ann| lastauthoramp = yes| title = Digital archaeology : rescuing neglected and damaged data resources| publisher = British Library and Joint Information Systems Committee| place     = London & Bristol| series = Electronic libraries programme studies
| year    = 1999| | language = EN| url = http://www.ukoln.ac.uk/services/elib/papers/supporting/pdf/p2.pdf| isbn = 1-90050-851-6}}

[[Category:Data management|Archaeology]]
[[Category:Digital preservation]]
[[Category:Archaeological sub-disciplines]]
<=====doc_Id=====>:46
<=====title=====>:
Data steward
<=====text=====>:
{{merge|Data custodian|date=February 2016}}

A '''data steward''' is a person responsible for the management and fitness of [[data element]]s  - both the content and [[metadata]]. Data stewards have a specialist role that incorporates processes, policies, guidelines and responsibilities for administering organizations' entire data in compliance with policy and/or regulatory obligations. A data steward may share some responsibilities with a [[data custodian]].

The overall objective of a data steward is [[data quality]], in regard to the key/critical data elements existing within a specific enterprise operating structure, of the elements in their respective domains. This includes capturing/documenting (meta)information for their elements (such as: definitions, related rules/governance, physical manifestation, related data models, etc. With most of these properties being specific to an attribute/concept relationship), identifying owners/custodians/various  responsibilities, relations insight pertaining to attribute quality, aiding with project requirement data facilitation and documentation of capture rules. 

Data stewards begin the [[stewardship|stewarding]] process with the identification of the elements which they will steward, with the ultimate result being standards, [[Control (disambiguation)|control]]s and [[Data entry clerk|data entry]].{{citation needed|date=October 2014}}  The steward works closely with business glossary standards analysts (for standards), with [[data architect]]/[[Data modeling|modeler]]s (for standards), with  [[Data quality|DQ]] analysts (for controls) and with [[Computer operator|operations team member]]s (good-quality data going in per business rules) while entering data.

Data stewardship roles are common when organizations attempt to exchange data precisely and consistently between computer systems and to reuse data-related resources.{{citation needed|date=October 2014}}  [[Master data management]] often{{quantify|date=October 2014}} makes references to the need for data stewardship for its implementation to succeed. Data stewardship must have precise purpose, fit for purpose or fitness.

==Data Steward Responsibilities==
A data steward ensures that each assigned data element:
# Has clear and unambiguous [[data element definition]].
# Does not conflict with other data elements in the metadata registry (removes duplicates, overlap etc.)
# Has clear enumerated value definitions if it is of type [[Code (metadata)|Code]].
# Is still being used (remove unused data elements)
# Is being used consistently in various computer systems
# Is being used, fit for purpose = Data Fitness.
# Has adequate documentation on appropriate usage and notes
# Documents the origin and sources of authority on each metadata element
# Is protected against unauthorised access or change

==Benefits of data stewardship==

Systematic data stewardship can foster fitness through:

# consistent use of data management resources
# easy mapping of data between computer systems and exchange documents
# lower costs associated with migration to (for example) [[Service Oriented Architecture]] (SOA)

Assignment of each data element to a person sometimes seems like an unimportant process. But many groups{{Which|date=July 2010}} have found that users have greater trust and usage rates in systems where they can contact a person with questions on each data element.

== Examples ==
{{Expand section|date=July 2010}}

The [http://www.epa.gov/edr [[United States Environmental Protection Agency|EPA]] metadata registry] furnishes an example of data stewardship.  Note that each data element therein has a "POC"  (point of contact).

== Data Stewardship Applications ==
A new market for data governance applications is emerging, one in which both technical and business staff — stewards — manage policies. These new applications, like previous generations, deliver a strong business glossary capability, but they don't stop there. Vendors are introducing additional features addressing the roles of business in addition to technical stewards' concerns.<ref>{{Cite web|url=https://www.forrester.com/report/The+Forrester+Wave+Data+Governance+Stewardship+Applications+Q1+2016/-/E-RES117915|title=The Forrester Wave™: Data Governance Stewardship Applications, Q1 2016|website=www.forrester.com|access-date=2016-12-20}}</ref>

Information stewardship applications are business solutions used by business users acting in the role of information steward (interpreting and enforcing information governance policy, for example). These developing solutions represent, for the most part, an amalgam of a number of disparate, previously IT-centric tools already on the market, but are organized and presented in such a way that information stewards (a business role) can support the work of information policy enforcement as part of their normal, business-centric, day-to-day work in a range of use cases.

The initial push for the formation of this new category of packaged software came from operational use cases — that is, use of business data in and between transactional and operational business applications. This is where most of the master data management (MDM) efforts are undertaken in organizations. However, there is also now a faster-growing interest in the new data lake arena for more analytical use cases.<ref>{{Cite web|url=https://www.gartner.com/document/3284717?ref=TypeAheadSearch&qid=744b6ad6c678d064cc2d6eb831a4c959|title=Market Guide for Information Stewardship Applications|last=De Simoni|first=Guido|date=15 April 2016|website=www.gartner.com|publisher=Gartner|access-date=}}</ref>

==See also==
* [[Metadata]]
* [[Metadata registry]]
* [[Data curation]]
* [[Data element]]
* [[Data element definition]]
* [[Representation term]]
* [[ISO/IEC 11179]]

==References==
* ''Universal Meta Data Models'', by David Marco and Michael Jennings, Wiley, 2004, page 93-94 ISBN 0-471-08177-9
* ''Metadata Solution'' by Adrinne Tannenbaum, Addison Wesley, 2002, page 412
* ''Building and Managing the Meta Data Repository'', by David Marco, Wiley, 2000, pages 61–62
* ''The Data Warehouse Lifecycle Toolkit'', by [[Ralph Kimball]] et. el., Wiley, 1998, also briefly mentions the role of data steward in the context of data warehouse project management on page 70.
* ''Developing Geospatial Intelligence Stewardship for Multinational Operations'', by Jeff Thomas, US Army Command General Staff College, 2010, www.dtic.mil/dtic/tr/fulltext/u2/a524227.pdf.

==Notes==
{{reflist}}

[[Category:Data management]]
[[Category:Information technology governance]]
[[Category:Knowledge representation]]
[[Category:Library occupations]]
[[Category:Metadata]]
[[Category:Technical communication]]
<=====doc_Id=====>:49
<=====title=====>:
Single source publishing
<=====text=====>:
{{redirect|Single source}}
'''Single source publishing''', also known as '''single sourcing publishing''', is a [[content management]] method which allows the same source [[Content (media)|content]] to be used across different forms of [[Media (communication)|media]] and more than one time.<ref>Kay Ethier, ''XML and FrameMaker'', pg. 19. [[New York City|New York]]: [[Apress]], 2004. ISBN 9781430207191</ref><ref>Lucas Walsh, "The Application of Single-Source Publishing to E-Government." Taken from ''Encyclopedia of Digital Government'', pg. 64. Eds. Ari-Veikko Anttiroiko and Matti Mälkiä. [[Hershey, Pennsylvania|Hershey]]: IGI Global, 2007. ISBN 9781591407904</ref><ref>[http://www.stylusstudio.com/single_source_publishing.html Single Source Publishing] at [[Stylus Studio]]. Copyright © 2005-2013 [[Progress Software]]. Accessed June 11, 2013.</ref><ref name=petra>[http://www.writersua.com/articles/singlesource/ Single Source Publishing with Flare]. Copyright © 2010 WritersUA. Published November 16, 2010; accessed June 11, 2013.</ref> The labour-intensive and expensive work of [[Technical editing#Technical editing|editing]] need only be carried out once, on only one document;<ref name=cms>Barry Schaeffer, [http://www.cmswire.com/cms/information-management/single-source-publishing-creating-customized-output-015069.php Single Source Publishing: Creating Customized Output]. CMS Wire, 3 April 2012. Accessed 10 June 2013.</ref> that source document can then be stored in one place and reused.<ref>[[Ann Rockley]] and Charles Cooper, [https://books.google.com/books?id=82X6jGY_dHMC&pg=PT75&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CDoQ6AEwBg#v=onepage&q=single%20source%20publishing&f=false Managing Enterprise Content: A Unified Content Strategy], Chapter 5: Product content. 2nd ed. [[Berkeley, California|Berkeley]]: [[New Riders Press]], 2012. ISBN 9780132931649</ref> This reduces the potential for error, as corrections are only made one time in the source document.<ref>Janet Mackenzie, ''The Editor's Companion'', pg. 92. [[Cambridge]]: [[Cambridge University Press]], 2011. ISBN 9781107402188</ref>

The benefits of single source publishing primarily relate to the editor rather than the [[User (computing)|user]]. The user does benefit from consistent terminology and information, but this consistency is also a potential weakness of single source publishing if the content manager does not have an organized [[Conceptualization (information science)|conceptualization]].<ref name=petra/> Single-source publishing is sometimes used synonymously with '''multi-channel publishing''' though whether or not the two terms are synonymous is a matter of discussion.<ref name=mek>[http://www.mekon.com/index.php/pages/knowledge_zone/single-sourcing-multi-channel-publishing/technology_standards Single Source & Multi-Channel Publishing]. © 2013 Mekon, accessed 23 June 2013.</ref>

==Definition==
While there is a general definition of single source publishing, there is no single official delineation between single source publishing and multi-channel publishing, nor are there any official governing bodies to provide such a delinieation. Single source publishing is most often understood as the creation of one source document in [[Microsoft Word]] or [[Adobe FrameMaker]] and converting that document into different [[file format]]s or human [[language]]s (or both) multiple times with minimal effort. Multi-channel publishing can either be seen as synonymous with single source publishing, or similar in that there is one source document but the process itself results in more than a mere reproduction of that source.<ref name=mek/>

==History==
The origins of single-source publishing lie, indirectly, with the release of [[Windows 3.0]] in 1990.<ref name=bob162>Bob Boiko, [https://books.google.com/books?id=p6nUDn3ZaBoC&pg=PA162&dq=Single+source+publishing&hl=en&sa=X&ei=CUiqU6LEDaLV0QXd44CQDw&ved=0CCoQ6AEwAzgK#v=onepage&q=Single%20source%20publishing&f=false Content Management Bible], pg. 162. [[Hoboken, New Jersey|Hoboken]]: [[John Wiley & Sons]], 2005. ISBN 9780764583643</ref> With the eclipsing of [[MS-DOS]] by [[graphical user interface]]s, help files went from being unreadable text along the bottom of the screen to hypertext systems such as [[WinHelp]]. On-screen help interfaces allowed software companies to cease the printing of large, expensive help manuals with their products, reducing costs for both producer and consumer. This system raised opportunities as well, and many developers fundamentally changed the way they thought about publishing. Writers of [[software documentation]] did not simply move from being writers of traditional bound books to writers of [[electronic publishing]], but rather they became authors of central documents which could be reused multiple times across multiple formats.<ref name=bob162/>

The first single-source publishing project was started in 1993 by Cornelia Hofmann at [[Schneider Electric]] in [[Seligenstadt]], using software based on [[Interleaf]] to automatically create paper documentation in multiple languages based on a single original source file.<ref>[https://books.google.com/books?id=inCeft4AkXcC&pg=PA65&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CCoQ6AEwAw#v=onepage&q=single%20source%20publishing&f=false Translating Into Success: Cutting-edge Strategies for Going Multilingual in a Global Age], pg. 227. Eds. Robert C. Sprung and Simone Jaroniec. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9789027231871</ref>

[[XML]], developed during the mid- to late-1990s, was also significant to the development of single source publishing as a method. XML, a markup language, allows developers to separate their documentation into two layers: a shell-like layer based on presentation and a core-like layer based on the actual written content. This method allows developers to write the content only one time while switching it in and out of multiple different formats and delivery methods.<ref>Doug Wallace and Anthony Levinson, "The XML e-Learning Revolution: Is Your Production Model Holding You Back?" Taken from [https://books.google.com/books?id=4RK7tJ-OO3cC&pg=PA65&dq=Single+source+publishing&hl=en&sa=X&ei=CUiqU6LEDaLV0QXd44CQDw&ved=0CEgQ6AEwCDgK#v=onepage&q=Single%20source%20publishing&f=false Best of The eLearning Guild's Learning Solutions: Articles from the eMagazine's First Five Years], pg. 63. Ed. Bill Brandon. Hoboken: John Wiley & Sons, 2008. ISBN 9780470277157</ref>

In the mid-1990s, several firms began creating and using single source content for technical documentation (Boeing Helicopter, Sikorsky Aviation and Pratt & Whitney Canada) and user manuals (Ford owners manuals) based on tagged SGML and XML content generated using the Arbortext Epic editor with add-on functions developed by a contractor.  The concept behind this usage was that complex, hierarchical content that did not lend itself to discrete componentization could be used across a variety of requirements by tagging the differences within a single document using the capabilities built into SGML and XML.
Ford, for example, was able to tag its single owner's manual files so that 12 model years could be generated via a resolution script running on the single completed file.  Pratt & Whitney, likewise, was able to tag up to 20 subsets of its jet engine manuals in single source files, calling out the desired version at publication time.  World Book Encyclopedia also used the concept to tag its articles for American and British versions of English.

Starting from the early 2000s, single source publishing was used with an increasing frequency in the field of [[technical translation]]. It is still regarded as the most efficient method of publishing the same material in different languages.<ref>Bert Esselink, "Localisation and translation." Taken from [https://books.google.com/books?id=a4W7lWgCqYoC&pg=PA73&dq=Single+source+publishing&hl=en&sa=X&ei=CUiqU6LEDaLV0QXd44CQDw&ved=0CCUQ6AEwAjgK#v=onepage&q=Single%20source%20publishing&f=false Computers and Translation: A Translator's Guide], pg. 73. Ed. H. L. Somers. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2003. ISBN 9789027216403</ref> Once a printed manual was translated, for example, the online help for the software program which the manual accompanies could be automatically generated using the method.<ref>Burt Esselink, ''A Practical Guide to Localization'', pg. 228. Volume 4 of Language international world directory. [[Amsterdam]]: [[John Benjamins Publishing Company]], 2000. ISBN 9781588110060</ref> [[Metadata]] could be created for an entire manual and individual pages or files could then be translated from that metadata with only one step, removing the need to recreate information or even database structures.<ref>Cornelia Hofmann and Thorsten Mehnert, "Multilingual Information Management at Schneider Automation." Taken from ''Translating Into Success'', pg. 67.</ref>

Although single source publishing is now decades old, its importance has increased urgently as of the 2010s. As consumption of information products rises and the number of target audiences expands, so does the work of developers and content creators. Within the industry of software and its documentation, there is a perception that the choice is to embrace single source publishing or render one's operations obsolete.<ref name=cms/>

==Criticism==
Single-source publishing has been criticized due to the quality of work, being compared to as the "conveyor belt assembly" of content creation by its critics.<ref>Mick Hiatt, [http://mashstream.com/mashups/the-myth-of-single-source-authoring/ The Myth of Single-Source Authoring]. Mashstream, November 18, 2009.</ref> 

While heavily used in technical translation, there are risks of error in regard to [[Index (publishing)|indexing]]. While two words might be [[synonym]]s in English, they may not be synonyms in another language. In a document produced via single sourcing, however, the index will be translated automatically and the two words will be rendered as synonyms because they are in the [[Source language (translation)|source language]], while in the [[Target language (translation)|target language]] they are not.<ref>Nancy Mulvany, [https://books.google.com/books?id=G0Eqm8FbiTMC&pg=PA312&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CEcQ6AEwCA#v=onepage&q=single%20source%20publishing&f=false Indexing Books], pg. 154. 2nd ed. [[Chicago]]: [[University of Chicago Press]], 2009. ISBN 9780226550176</ref>

==See also==
* [[Content management]]
* [[Darwin Information Typing Architecture]]
* [[EPUB]]
* [[Markup language]]

===List of single-source publishing tools=== 
* [[Adobe FrameMaker]]<ref>Sarah S. O'Keefe, Sheila A. Loring, Terry Smith and Lydia K. Wong, [https://books.google.com/books?id=b-yEKgcQmN8C&pg=PA6&dq=single+source+publishing&hl=en&sa=X&ei=rRehU_TxFs2W0QXT6YCQAQ&ved=0CE0Q6AEwCQ#v=onepage&q=single%20source%20publishing&f=false Publishing Fundamentals: Unstructured FrameMaker 8], pg. 6. Scriptorium Publishing, 2008. ISBN 9780970473349</ref>
* [[Apache Cocoon]]
* [[Apache Forrest]]
* [[Altova]]
* [[Booktype]]
* [[DocBook XSL]]
* [[DITA Open Toolkit]]
* [[Help & Manual]]
* [[MadCap Flare]]
* [[Oxygen XML Editor|Oxygen XML editor]]
* [[Scenari]]
* [[Sphinx (documentation generator)|Sphinx]]<ref>{{cite web | url = http://pythonic.pocoo.org/2008/3/21/sphinx-is-released | title = Sphinx is released! &raquo; And now for something completely Pythonic... | publisher = Georg Brandl| work = And now for something completely Pythonic... | accessdate = 2011-04-03 }}</ref>
* [[XPLM Publisher]]

==References==
{{reflist}}

==Further reading==
* {{cite book | last = Ament | first = Kurt | authorlink = | title = Single Sourcing: Building Modular Documentation | publisher = William Andrew | date = 2007-12-17 |  location = | pages = 245 | url = | doi = | id = | isbn = 0-8155-1491-3 }}
* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}
* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath | title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}
* {{cite book | last = Maler | first = Eve | authorlink = |author2=Jeanne El Andaloussi  | title = Developing SGML DTDs: From Text to Model to Markup | publisher = Prentice Hall PTR | date = 1995-12-15 | location = | pages = 560 | url = | doi = | id = | isbn = 0-13-309881-8 }} (the "bible" for Data Modeling)

==External links==
* [http://www.elkera.com/cms/articles/seminars_and_presentations/planning_a_single_source_publishing_application_for_business_documents/ Planning a Single Source Publishing Application for Business Documents] (A paper presented by Peter Meyer at OpenPublish, Sydney, on 29 July 2005)
* [https://www.tug.org/TUGboat/tb29-1/tb91sojka.pdf Single-source publishing in multiple formats for different output devices]
* [http://www.agilemodeling.com/essays/singleSourceInformation.htm Single Sourcing Information - An Agile Practice for Effective Documentation]
* [http://www.stcsig.org/ss Society for Technical Communication Single-sourcing Special Interest Group]
* [http://www.wisegeek.com/what-is-single-source-publishing.htm What Is Single Source Publishing?] at WiseGeek
* [http://www.technical-communication.org/topics/information-development.html tekom Europe] (Articles about Information Development and Single Source Publishing)

[[Category:Technical communication]]
[[Category:Computer file systems]]
[[Category:Data management]]
<=====doc_Id=====>:52
<=====title=====>:
Workflow engine
<=====text=====>:
A '''workflow engine''' is a [[software application]] that manages business processes. It is a key component in [[workflow technology]] and typically makes use of a [[database server]].

A workflow engine manages and monitors the state of activities in a [[workflow]], such as the processing and approval of a loan application form, and determines which new activity to transition to according to defined processes (workflows).<ref>http://docs.oracle.com/cd/B13789_01/workflow.101/b10286/wfapi.htm</ref> The actions may be anything from saving an application form in a [[document management system]] to sending a reminder e-mail to users or escalating overdue items to management. A workflow engine facilitates the flow of information, tasks, and events. Workflow engines may also be referred to as Workflow Orchestration Engines.<ref>http://pic.dhe.ibm.com/infocenter/tivihelp/v48r1/index.jsp?topic=%2Fcom.ibm.sco.doc_2.2%2Fenablement%2Fworkfloworchestration.html</ref>

Workflow engines mainly have three functions:
*	Verification of the current status: Check whether the command is valid in executing a task.
*	Determine the authority of users: Check if the current user is permitted to execute the task.
*	Executing condition script: After passing the previous two steps, the workflow engine begins to evaluate the condition script in which the two processes are carried out, if the condition is true, workflow engine execute the task, and if the execution successfully completes, it returns the success, if not, it reports the error to trigger and roll back the change.<ref>The Workflow Engine Model. [http://msdn.microsoft.com/en-us/library/aa188337%28office.10%29.aspx  The Workflow Engine Model] Accessed 1 Dec. 2010.</ref>

A workflow engine is a core technique for task allocation software, such as [[business process management]], in which the workflow engine allocates tasks to different executors while communicating data among participants. A workflow engine can execute any arbitrary sequence of steps, for example, a healthcare data analysis.<ref name=hf2010>{{Cite journal | last1 = Huser | first1 = V. | last2 = Rasmussen | first2 = L. V. | last3 = Oberg | first3 = R. | last4 = Starren | first4 = J. B. | title = Implementation of workflow engine technology to deliver basic clinical decision support functionality | doi = 10.1186/1471-2288-11-43 | journal = BMC Medical Research Methodology | volume = 11 | pages = 43 | year = 2011 | pmid = 21477364 | pmc = 3079703}}</ref>

== See also ==
*[[Business rules engine]]
*[[Business rule management system]]
*[[Comparison of BPEL engines]]
*[[Inference engine]]
*[[Java Rules Engine API]]
*[[Rete algorithm]]
*[[Ripple down rules]]
*[[Semantic reasoner]]
*[[BPEL|Business Process Execution Language]]
*[[Production system (computer science)|Production system]]
*[[Workflow management system]]

== References ==
{{Reflist}}

[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Workflow technology]]
[[Category:Workflow software]]
<=====doc_Id=====>:55
<=====title=====>:
Long-running transaction
<=====text=====>:
{{More sources|date=October 2015}}
'''Long-running transactions''' (also known as Saga transactions) are computer [[database transaction]]s that avoid [[lock (computer science)|locks]] on non-local resources, use compensation to handle failures, potentially aggregate smaller [[ACID]] transactions (also referred to as [[atomic transaction]]s), and typically use a coordinator to complete or abort the transaction. In contrast to [[rollback (data management)|rollback]] in ACID transactions, compensation restores the original state, or an equivalent, and is business-specific. For example, the compensating action for making a hotel reservation is canceling that reservation, possibly with a penalty.

A number of protocols have been specified for long-running transactions using Web services within business processes. OASIS Business Transaction Processing<ref>http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=business-transaction</ref> and WS-CAF<ref>http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=ws-caf</ref> are examples. These protocols use a coordinator to mediate the successful completion or use of compensation in a long-running transaction.

==See also==
*[[Optimistic concurrency control]]
*[[Long-lived transaction]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:58
<=====title=====>:
Data field
<=====text=====>:
{{Unreferenced|date=March 2007}}
A '''data field''' is a place where you can store [[data]].  Commonly used to refer to a column in a [[database]] or a field in a [[Data entry clerk|data entry]] form or web form.

The field may contain data to be entered as well as data to be displayed.  

==See also== 
{{wiktionary|Data}}
*[[Data dictionary]]
*[[Data element]]
*[[Data acquisition]]
*[[Data hierarchy]]

[[Category:Data management]]

[[it:Campo (informatica)]]
<=====doc_Id=====>:61
<=====title=====>:
Comparison of ADO and ADO.NET
<=====text=====>:
''Note: The following content requires a knowledge of [[database]] technologies.''

The following is a comparison of two different database access technologies from [[Microsoft]], namely, [[ActiveX Data Objects|ActiveX Data Objects (ADO)]] and [[ADO.NET]]. Before comparing the two technologies, it is essential to get an overview of [[Microsoft Data Access Components]] (MDAC) and the [[.NET Framework]]. [[Microsoft Data Access Components]] provide a uniform and comprehensive way of developing applications for accessing almost any data store entirely from [[Managed code#Managed and unmanaged|unmanaged code]]. The [[.NET Framework]] is an [[Virtual machine#Application virtual machine|application virtual machine]]-based software environment that provides security mechanisms, [[memory management]], and [[exception handling]] and is designed so that developers need not consider the capabilities of the specific CPU that will execute the .NET application. The .NET [[Virtual machine#Application virtual machine|application virtual machine]] turns [[intermediate language]] (IL) into machine code. High-level language compilers for [[C Sharp (programming language)|C#]], [[Visual Basic .NET|VB.NET]] and [[C++]] are provided to turn source code into IL. [[ADO.NET]] is shipped with the Microsoft NET Framework.

[[ActiveX Data Objects|ADO]] relies on [[Component Object Model|COM]] whereas [[ADO.NET]] relies on managed-providers defined by the .NET [[Common Language Runtime|CLR]]. ADO.NET does not replace ADO for the COM programmer; rather, it provides the .NET programmer with access to relational data sources, XML, and application data.

{| class="wikitable"
|-
! 
! ADO
! ADO.NET
|-
| Business Model
| Connection-oriented Models used mostly
| Disconnected models are used:Message-like Models.
|-
| Disconnected Access
| Provided by Record set
| Provided by Data Adapter and Data set
|-
| [[XML]] Support
| Limited
| Robust Support
|-
|Connection Model
|Client application needs to be connected always to data-server while working on the data, unless using client-side cursors or a disconnected Record set
|Client disconnected as soon as the data is processed. DataSet is disconnected at all times.
|-
|Data Passing
|ADO objects communicate in binary mode.
|ADO.NET uses XML for passing the data.
|-
|Control of data access behaviors
|Includes implicit behaviors that may not always be required in an application and that may therefore limit performance.
|Provides well-defined, factored components with predictable behavior, performance, and semantics.
|-
|Design-time support
|Derives information about data implicitly at run time, based on metadata that is often expensive to obtain.
|Leverages known metadata at design time in order to provide better run-time performance and more consistent run-time behavior.
|}

== References ==
* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO programmer]

[[Category:Data management]]
[[Category:.NET Framework]]
[[Category:Microsoft application programming interfaces]]
[[Category:SQL data access]]
[[Category:Software comparisons|ADO and ADO.NET]]
<=====doc_Id=====>:64
<=====title=====>:
Content re-appropriation
<=====text=====>:
{{Orphan|date=February 2009}}
Fundamental to modern [[information architecture]]s, and driven by [http://www.webreference.com/internet/semantic/ semantic Web] technologies, '''content re-appropriation''' is the act of searching, filtering, gathering, grouping, and aggregation which allows information to be related, classified and identified.  This is achieved by applying syntactic or semantic meaning though intelligent tagging or artificial interpretation of fragmented content (see [[Resource Description Framework]]).  Hence, all information becomes valuable and interpretable.

==Domain==
Since the domain of Content applies to areas of [[software applications]], [[document]]s, and [[Computer media|media]], these can be processed though a pipeline of generation, aggregation, transform-many, and serialization (see [http://www.w3.org/TR/xml-pipeline/ XML Pipeline]).  The output of this can viewed in a medium most effect for decision making.

The desired outcomes of content re-appropriation are:

*Seamless, Integrated, and Shared User experiences
*[[Software visualization|Visualization]]
*Detection, Analysis & Investigation
*[[Personalization]] unique to the User
*Inbound or Outbound [[web syndication|Syndication]] of Information
*[[Publish]] or [[Subscribe]] to Information
*Dynamically adapted output to Users medium

Essentially to make ''information'' disparities transparent to the [[user (computing)|user]] - getting to the bottom line … quickly.

==Areas of Use==
Content re-appropriation is effective across the [[Content-Tier]], that is places where Content exists:

*Identity & Directory Management e.g. [[Lightweight Directory Access Protocol|LDAP]], [[Security Assertion Markup Language|SAML]] & [[JNDI]]
*Content Management e.g. [http://jakarta.apache.org/slide/ Apache Slide]
*Content Systems e.g. [[File System]]s, [[E-mail]], [[Network share]]s, [[Storage Area Network|SAN]] & [[Database]]
*Business Systems e.g. [[Enterprise resource planning|ERP]] & [[Customer Relationship Management|CRM]]
*Data Warehouse e.g. [[OLAP]]
*Internet & Web Services e.g. [[HyperText Transfer Protocol|HTTP]] & [[Simple Object Access Protocol|SOAP]]
*[[Instant messenger|Presence]] and [[peer-To-Peer]]

== See also ==
* [[Knowledge visualization]]
* [[Web indexing]]
* [[Taxonomy (general)|Taxonomy]]

[[Category:Data management]]
[[Category:Technical communication]]
<=====doc_Id=====>:67
<=====title=====>:
CA Gen
<=====text=====>:
'''CA Gen''' is a [[Computer Aided Software Engineering]] (CASE) application development environment marketed by [[CA Technologies]]. Gen was previously known as '''IEF''' ('''Information Engineering Facility'''), '''Composer by IEF''', '''Composer''', '''COOL:Gen''', '''Advantage:Gen''' and '''AllFusion Gen'''.

The toolset originally supported the [[information engineering]] methodology developed by [[Clive Finkelstein]], [[James Martin (author)|James Martin]] and others in the early 1980s. Early versions supported IBM's [[IBM DB2|DB2]] database, [[IBM 3270|3270]] 'block mode' screens and generated [[COBOL]] code.

In the intervening years the toolset has been expanded to support additional development techniques such as [[Component-based software engineering|component-based development]]; creation of [[Client–server model|client/server]] and [[web application]]s and generation of [[C (programming language)|C]], [[Java (programming language)|Java]] and [[C Sharp (programming language)|C#]]. In addition, other platforms are now supported such as many variants of *ix-like Operating Systems (AIX, HP-UX, Solaris, Linux) as well as Windows.

Its range of supported database technologies have widened to include [[Oracle Database|ORACLE]], [[Microsoft SQL Server]], [[ODBC]], [[Java Database Connectivity|JDBC]] as well as the original DB2.

The toolset is fully integrated - objects identified during analysis carry forward into design without redefinition. All information is stored in a repository (central encyclopedia). The encyclopedia allows for large team development - controlling access so that multiple developers may not change the same object simultaneously.<ref>https://communities.ca.com/web/ca-gen-edge-global-user-community/wiki/-/wiki/EDGE+User+Group+CA+Gen+Wiki/What+is+CA+Gen?&#p_36</ref>

==Overview==
It was initially produced by [[Texas Instruments]], with input from [[James Martin (author)|James Martin]] and his consultancy firm James Martin Associates, and was based on the Information Engineering Methodology (IEM). The first version was launched in 1987.

IEF became popular among large government departments and public utilities. It initially supported a [[CICS]]/COBOL/DB2 target environment.  However, it now supports a wider range of relational databases and operating systems. IEF was intended to shield the developer from the complexities of building complete multi-tier cross-platform applications.

In 1995, Texas Instruments decided to change their marketing focus for the product. Part of this change included a new name - "Composer".

By 1996, IEF had become a popular tool. However, it was criticized by some IT professionals for being too restrictive, as well as for having a high per-workstation cost ($15K USD). But it is claimed that IEF reduces development time and costs by removing complexity and allowing rapid development of large scale enterprise transaction processing systems.

In 1997, Composer had another change of branding, Texas Instruments sold the [[Texas Instruments Software]] division, including the Composer rights, to [[Sterling Software]]. Sterling software changed the well known name "Information Engineering Facility" to "COOL:Gen". COOL was an acronym for "Common Object Oriented Language" - despite the fact that there was little [[Object-oriented programming|object orientation]] in the product.

In 2000, Sterling Software was acquired by [[Computer Associates]] (now CA). CA has rebranded the product three times to date and the product is still used widely today. Under CA, recent releases of the tool added support for the CA-[[DATACOM/DB|Datacom]] DBMS, the Linux operating system, C# code generation and [[ASP.NET]] web clients. The current version is known as CA Gen - version 8 being released in May 2010, with support for customised web services, and more of the toolset being based around the [[Eclipse (software)|Eclipse framework]].

There are a variety of "add-on" tools available for CA Gen, including Project Phoenix from Jumar - a collection of software tools and services focused on the modernisation and re-platforming of existing/legacy CA Gen applications to new environments,<ref>[http://www.jumar-solutions.com/ Jumar]</ref> GuardIEn - a [[Configuration Management]] and Developer Productivity Suite,<ref>[http://www.iet.co.uk IET Ltd]</ref> QAT Wizard,<ref>[http://www.qat.com/qat_wizard.asp QAT Wizard]</ref> an interview style wizard that takes advantage of the meta model in Gen, products for multi-platform application reporting and XML/SOAP enabling of Gen applications.,<ref>[http://www.canamsoftware.com/ Canam Software Labs]</ref> and developer productivity tools such as Access Gen, APMConnect, QA Console and Upgrade Console from Response Systems <ref>[http://www.response-systems.com Response Systems]</ref>
Recently CA GEN has released its latest version 8.5.

==References==
{{Reflist}}

==External links==
* [http://www.uk.capgemini.com/public-sector/tax-welfare/regenerate Capgemini REGENERATE offering] - Support, Update, Migrate
* [http://www.edgeusergroup.org EDGE User Group] - the user group for CA Gen
* [http://www.edgeusergroup.org/wiki CA Gen Wiki] - sponsored by the EDGE User Group
* [http://www.gentalk.biz gentalk.biz] - CA Gen Blog - inactive
* [http://www.qat.com QAT Global] - CA Gen Services and Training Provider (USA)
* [http://www.iet.co.uk IET] - CA Gen Product and Services Provider (UK)
* [http://www.jumar-solutions.com/ Jumar Solutions] - CA Gen Product and Services Provider (UK)
* [http://www.response-systems.com/ Response Systems] - CA Gen Product and Services Provider (UK)
* [http://www.facet.com.au/ Facet Consulting] - CA Gen Services Provider (Australia)
* [http://www.canamsoftware.com/ Canam Software Labs, Inc.] - CA Gen Product and Service Provider (Canada)

[[Category:Computer-aided software engineering tools]]
[[Category:Data management]]
[[Category:CA Technologies]]


Edited by: Sambit Mishra
<=====doc_Id=====>:70
<=====title=====>:
Data Reference Model
<=====text=====>:
[[Image:DRM Collaboration Process.jpg|thumb|320px|The DRM Collaboration Process.]]
The '''Data Reference Model''' ('''DRM''') is one of the five reference models of the [[Federal Enterprise Architecture]] (FEA). 

== Overview ==
The DRM is a framework whose primary purpose is to enable information sharing and reuse across the [[United States federal government]] via the standard description and discovery of common data and the promotion of uniform data management practices. The DRM describes artifacts which can be generated from the data architectures of federal government agencies. The DRM provides a flexible and standards-based approach to accomplish its purpose. The scope of the DRM is broad, as it may be applied within a single agency, within a [[Community of interest (computer security)|Community of Interest]] (COI)1, or cross-COI.

== Data Reference Model topics ==
=== DRM structure ===
The DRM provides a standard means by which [[data]] may be described, categorized, and shared. These are reflected within each of the DRM’s three standardization areas:

* ''Data Description'': Provides a means to uniformly describe data, thereby supporting its discovery and sharing.
* ''Data Context'': Facilitates discovery of data through an approach to the categorization of data according to taxonomies. Additionally, enables the definition of authoritative data assets within a COI.
* ''Data Sharing'': Supports the access and exchange of data where access consists of ''ad hoc'' requests (such as a query of a data asset), and exchange consists of fixed, re-occurring transactions between parties. Enabled by capabilities provided by both the Data Context and Data Description standardization areas.

===DRM Version 2 ===
The Data Reference Model version 2 released in November 2005 is a 114 page document with detailed architectural diagrams and an extensive glossary of terms.

The DRM also make many references to ISO standards specifically the [[ISO/IEC 11179]] metadata registry standard.

=== DRM usage ===
The DRM is not technically a published technical interoperability standard such as web services, it is an excellent starting point for data architects within federal and state agencies.  Any federal or state agencies that are involved with exchanging information with other agencies or that are involved in [[Data warehouse]]ing efforts should use this document as a guide.

==See also==
* [[Enterprise architecture framework]]
* [[Enterprise application integration]]
* [[Enterprise service bus]]
* [[Federal Enterprise Architecture]]
* [[ISO/IEC 11179]]
* [[Metadata publishing]]
* [[Semantic spectrum]]
* [[Semantic web]]
* [[Synonym ring]]
<!--
== References ==
{{reflist}}-->

==External links==
* [https://web.archive.org/web/20070617034325/http://www.defenselink.mil/cio-nii/docs/DoD_DRM_V04_5aug.pdf US Department of Defense Data Reference Model]
* [http://www.whitehouse.gov/sites/default/files/omb/assets/egov_docs/DRM_2_0_Final.pdf US Federal Enterprise Architecture Program Data Reference Model Version 2.0]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Reference models]]
<=====doc_Id=====>:73
<=====title=====>:
Tagsistant
<=====text=====>:
{{POV|date=October 2012}}

{{Infobox software
| name                   = Tagsistant
| logo                   = [[file:Tagsistant logo.png|300px]]
| developer              = Tx0 <tx0@strumentiresistenti.org>
| latest release version = 0.6
| frequently_updated     = yes<!-- Release version update? Don't edit this page, just click on the version number! -->
| programming language   = [[C (programming language)|C]]
| operating system       = Linux kernel
| language               = English
| genre                  = [[Semantic file system]]
| license                = [[GNU General Public License|GNU GPL]]
| website                = http://www.tagsistant.net/
}}
{{Infobox filesystem
| name = Tagsistant
| developer = Tx0
| full_name =
| introduction_date =
| introduction_os =
| partition_id =
| directory_struct =
| file_struct =
| bad_blocks_struct =
| max_file_size =
| max_files_no =
| max_filename_size =
| max_volume_size =
| dates_recorded =
| date_range =
| date_resolution =
| forks_streams =
| attributes =
| file_system_permissions =
| compression =
| encryption =
| OS =
}}
'''Tagsistant''' is a [[semantic file system]] for the [[Linux kernel]], written in [[C (programming language)|C]] and based on [[Filesystem in Userspace|FUSE]]. Unlike traditional [[file systems]] that use hierarchies of directories to locate objects, Tagsistant introduces the concept of [[Tag (metadata)|tags]].

==Design and differences with hierarchical file systems==

In computing, a [[file system]] is a type of data store which could be used to store, retrieve and update [[Computer file|files]]. Each file can be uniquely located by its [[Path (computing)|path]]. The user must know the path in advance to access a file and the path does not necessarily include any information about the content of the file.

Tagsistant uses a complementary approach based on [[Tag (metadata)|tags]]. The user can create a set of tags and apply those tags to files, [[File directory|directories]] and other objects ([[Device file|devices]], [[Named pipe|pipes]], ...). The user can then search all the objects that match a subset of tags, called a query. This kind of approach is well suited for managing user contents like pictures, audio recordings, movies and text documents but is incompatible with system files (like libraries, commands and configurations) where the univocity of the path is a [[Computer security|security]] requirement to prevent the access to a wrong content.

==The tags/ directory==

A Tagsistant file system features four main directories:

:archive/
:relations/
:stats/
:tags/

Tags are created as sub directories of the <code>tags/</code> directory and can be used in queries complying to this syntax:

:<code>tags/subquery/[+/subquery/[+/subquery/]]/@/</code><ref>{{cite web|title=tags/ and relations/ directories|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&start=3}}</ref>

where a subquery is an arbitrarily long list of tags, concatenated as directories:

:<code>tag1/tag2/tag3/.../tagN/</code>

The portion of a path delimited by <code>tags/</code> and <code>@/</code> is the actual query. The <code>+/</code> operator joins the results of different sub-queries in one single list. The <code>@/</code> operator ends the query.

To be returned as a result of the following query:

:<code>tags/t1/t2/+/t1/t4/@/</code>

an object must be tagged as both <code>t1/</code> and <code>t2/</code> or as both <code>t1/</code> and <code>t4/</code>. Any object tagged as <code>t2/</code> or <code>t4/</code>, but not as <code>t1/</code> will not be retrieved.

The query syntax deliberately violates the [[POSIX]] file system semantics by allowing a path token to be a descendant of itself, like in <code>tags/t1/t2/+/t1/t4/@</code> where <code>t1/</code> appears twice. As a consequence a recursive scan of a Tagsistant file system  will exit with an error or endlessly loop, as done by [[UNIX]] <code>[[Find|find]]</code>:

<syntaxhighlight lang="bash">
~/tagsistant_mountpoint$ find tags/
tags/
tags/document
tags/document/+
tags/document/+/document
tags/document/+/document/+
tags/document/+/document/+/document
tags/document/+/document/+/document/+
[...]
</syntaxhighlight>

This drawback is balanced by the possibility to list the tags inside a query in any order. The query <code>tags/t1/t2/@/</code> is completely equivalent to <code>tags/t2/t1/@/</code> and <code>tags/t1/+/t2/t3/@/</code> is equivalent to <code>tags/t2/t3/+/t1/@/</code>.

The <code>@/</code> element has the precise purpose of restoring the POSIX semantics: the path <code>tags/t1/@/directory/</code> refers to a traditional directory and a recursive scan of this path will properly perform.

==The reasoner and the relations/ directory==

Tagsistant features a simple [[Semantic Reasoner|reasoner]] which expands the results of a query by including objects tagged with related tags. A relation between two tags can be established inside the <code>relations/</code> directory following a three level pattern:

:<code>relations/tag1/rel/tag2/</code>

The <code>rel</code> element can be ''includes'' or ''is_equivalent''. To include the ''rock'' tag in the ''music'' tag, the UNIX command <code>mkdir</code> can be used:

:<code>mkdir -p relations/music/includes/rock</code>

The reasoner can recursively resolve relations, allowing the creation of complex structures:

:<code>mkdir -p relations/music/includes/rock</code>
:<code>mkdir -p relations/rock/includes/hard_rock</code>
:<code>mkdir -p relations/rock/includes/grunge</code>
:<code>mkdir -p relations/rock/includes/heavy_metal</code>
:<code>mkdir -p relations/heavy_metal/includes/speed_metal</code>

The web of relations created inside the <code>relations/</code> directory constitutes a basic form of [[Ontology (information science)|ontology]].

==Autotagging plugins==

Tagsistant features an autotagging plugin stack which gets called when a file or a symlink is written.<ref>{{cite web|title=How to write a plugin for Tagsistant?|url=http://www.tagsistant.net/documents-about-tagsistant/coding-and-debugging/7-how-to-write-a-plugin-for-tagsistant}}</ref> Each plugin is called if its declared [[Mime type|MIME type]] matches

The list of working plugins released with Tagsistant 0.6 is limited to:

* text/html: tags the file with each word in <code><title></code> and <code><keywords></code> elements and with ''document'', ''webpage'' and ''html'' too
* image/jpeg: tags the file with each [[Exchangeable image file format|Exif]] tag

==The repository==

Each Tagsistant file system has a corresponding repository containing an <code>archive/</code> directory where the objects are actually saved and a <code>tags.sql</code> file holding tagging information as an [[SQLite]] database. If the [[MySQL]] database engine was specified with the <code>--db</code> argument, the <code>tags.sql</code> file will be empty. Another file named <code>repository.ini</code> is a [[GLib]] ini store with the repository configuration.<ref>{{cite web|title=Key-value file parser|url=https://developer.gnome.org/glib/2.32/glib-Key-value-file-parser.html}}</ref>

Tagsistant 0.6 is compatible with the MySQL and Sqlite dialects of SQL for tag reasoning and tagging resolution. While porting its logic to other SQL dialects is possible, differences in basic constructs (especially the INTERSECT SQL keyword) must be considered.

==The archive/ and stats/ directories==

The <code>archive/</code> directory has been introduced to provide a quick way to access objects without using tags. Objects are listed with their inode number prefixed.<ref>{{cite web|title=Tagsistant 0.6 howto - Inodes|url=http://www.tagsistant.net/documents-about-tagsistant/0-6-howto?showall=&start=6}}</ref>

The <code>stats/</code> directory features some read-only files containing usage statistics. A file <code>configuration</code> holds both compile time information and current repository configuration.

==Main criticisms==

It has been highlighted that relying on an external database to store tags and tagging information could cause the complete loss of metadata if the database gets corrupted.<ref>{{cite web|title=Extended attributes and tag file systems|url=http://www.lesbonscomptes.com/pages/tagfs.html}}</ref>

It has been highlighted that using a flat namespace tends to overcrowd the <code>tags/</code> directory.<ref>{{cite web|title=The major problem with this approach is scalability|publisher=https://news.ycombinator.com/item?id=2573318}}</ref> This could be mitigated introducing [[Tag (metadata)#Triple tags|triple tags]].

==See also==
{{Portal|Free software}}
[[Semantic file system]]

==References==
{{Reflist}}

==External links==
* {{Official website|http://www.tagsistant.net/}}
* [https://aur.archlinux.org/packages.php?ID=54644 Arch Linux package]
* [https://news.ycombinator.com/item?id=2573318 Discussion on Hacker News]
* [http://www.lesbonscomptes.com/pages/tagfs.html Extended attributes and tag file systems]
* [http://lakm.us/logit/2010/03/tagsistant-on-production-2/ Tagsistant On Production]

[[Category:Computer file systems]]
[[Category:Data management]]
[[Category:Semantic file systems]]
<=====doc_Id=====>:76
<=====title=====>:
Microsoft SQL Server Master Data Services
<=====text=====>:
{{multiple issues|
{{Advert|date=March 2011}}
{{Update|inaccurate=yes|date=April 2010}}
}}

'''Microsoft SQL Server Master Data Services''' is a [[Master Data Management]] (MDM) product from [[Microsoft]] that ships as a part of the [[Microsoft SQL Server]] relational database management system.<ref>https://msdn.microsoft.com/en-us/library/ms130214.aspx</ref>  Master Data Services (MDS) is the SQL Server solution for master data management. Master data management (MDM) enables your organization to discover and define non-transactional lists of data, and compile maintainable, reliable master lists. Master Data Services first shipped with Microsoft SQL Server 2008 R2.  Microsoft SQL Server 2016 includes many enhancements to Master Data Services, such as improved performance and security, and the ability to clear transaction logs, create custom indexes, share entity data between different models, and support for many-to-many relationships. For more information, see [https://msdn.microsoft.com/en-us/library/ff929136.aspx What's New in Master Data Services (MDS)]

==Overview==
In Master Data Services, the model is the highest level container in the structure of your master data. You create a model to manage groups of similar data. A model contains one or more entities, and entities contain members that are the data records. An entity is similar to a table.

Like other MDM products, Master Data Services aims to create a centralized data source and keep it synchronized, and thus reduce redundancies, across the applications which process the data.{{cn|date=January 2015}}

Sharing the architectural core with Stratature +EDM, Master Data Services uses a [[Microsoft SQL Server]] database as the physical data store. It is a part of the ''Master Data Hub'', which uses the database to store and manage data [[Entity Data Model|entities]].{{cn|date=January 2015}} It is a database with the software to validate and manage the data, and keep it synchronized with the systems that use the data.<ref name="arch">{{cite web | url = http://msdn2.microsoft.com/en-us/library/bb410798.aspx | title = Master Data Management (MDM) Hub Architecture | author = Roger Walter | publisher = MSDN TechNet | accessdate = 2007-09-25}}</ref> The master data hub has to extract the data from the source system, validate, sanitize and shape the data, remove duplicates, and update the hub repositories, as well as synchronize the external sources.<ref name="arch"/> The entity schemas, attributes, data hierarchies, validation rules and access control information are specified as [[metadata]] to the Master Data Services runtime. Master Data Services does not impose any limitation on the data model. Master Data Services also allows custom ''Business rules'', used for validating and sanitizing the data entering the data hub, to be defined, which is then run against the data matching the specified criteria. All changes made to the data are validated against the rules, and a log of the transaction is stored persistently. Violations are logged separately, and optionally the owner is notified, automatically. All the data entities can be [[Revision control system|versioned]].{{cn|date=January 2015}}

Master Data Services allows the master data to be categorized by hierarchical relationships, such as employee data are a subtype of organization data. Hierarchies are generated by relating data attributes. Data can be automatically categorized using rules, and the categories are introspected programmatically. Master Data Services can also expose the data as [[Microsoft SQL Server]] [[view (database)|views]], which can be pulled by any [[SQL]]-compatible client. It uses a role-based access control system to restrict access to the data. The views are generated dynamically, so they contain the latest data entities in the master hub. It can also push out the data by writing to some external journals. Master Data Services also includes a web-based UI for viewing and managing the data. It uses [[AJAX]] in the front-end and [[ASP.NET]] in the back-end.{{cn|date=January 2015}}

Master Data Services also includes certain features not available in the Stratature +EDM product. It gains a [[Web service]] interface to expose the data, as well as an [[API]], which internally uses the exposed web services, exposing the feature set, programmatically, to access and manipulate the data. It also integrates with [[Active Directory]] for authentication purposes. Unlike +EDM, Master Data Services supports [[Unicode]] characters, as well as support multilingual user interfaces.{{cn|date=January 2015}}

There has been a significant [http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever performance increase] in Master Data Services in SQL Server 2016 as well as the Excel Add-In.<ref>http://www.faceofit.com/why-is-sql-server-2016-is-faster-than-ever</ref>

== Terminology ==

* ''Model'' is the highest level of an MDS instance. It is the primary container for specific groupings of master data. In many ways it is very similar to the idea of a database. 
* ''Entities'' are containers created within a model. Entities provide a home for members, and are in many ways analogous to database tables. (e.g. Customer)
* ''Members'' are analogous to the records in a database table (Entity) e.g. Will Smith. Members are contained within entities. Each member is made up of two or more attributes. 
* ''Attributes'' are analogous to the columns within a database table (Entity) e.g. Surname. Attributes exist within entities and help describe members (the records within the table). Name and Code attributes are created by default for each entity and serve to describe and uniquely identify leaf members. Attributes can be related to other attributes from other entities which are called 'domain-based' attributes. This is similar to the concept of a foreign key.
Other attributes however, will be of type 'free-form' (most common) or 'file'.
* ''Attribute Groups'' are explicitly defined collections of particular attributes. Say you have an entity "customer" that has 50 attributes &mdash; too much information for many of your users. Attribute groups enable the creation of custom sets of hand-picked attributes that are relevant for specific audiences. (e.g. "customer - delivery details" that would include just their name and last known delivery address). This is very similar to a database view.
*  ''Hierarchies'' organize members into either Derived or Explicit hierarchical structures. Derived hierarchies, as the name suggests, are derived by the MDS engine based on the relationships that exist between attributes. Explicit hierarchies are created by hand using both leaf and consolidated members.
*  ''Business Rules'' can be created and applied against model data to ensure that custom business logic is adhered to. In order to be committed into the system data must pass all business rule validations applied to them. e.g. Within the Customer Entity you may want to create a business rule that ensures all members of the 'Country' Attribute contain either the text "USA" or "Canada". The Business Rule once created and ran will then verify all the data is correct before it accepts it into the approved model.
*  ''Versions'' provide system owners / administrators with the ability to Open, Lock or Commit a particular version of a model and the data contained within it at a particular point in time. As the content within a model varies, grows or shrinks over time versions provide a way of managing metadata so that subscribing systems can access to the correct content.

== References ==
{{Reflist}}

==External links==
*[https://msdn.microsoft.com/en-us/library/ee633763.aspx Microsoft SQL Server 2016 Master Data Services]

[[Category:Data management]]
[[Category:Microsoft software|SQL Server Master Data Services]]
[[Category:2010 software]]
<=====doc_Id=====>:79
<=====title=====>:
Microsoft Office PerformancePoint Server
<=====text=====>:
{{multiple issues|
{{more footnotes|date=August 2013}}
{{refimprove|date=August 2013}}
}}

{{Infobox Software
| name = Microsoft Office PerformancePoint Server
| developer = [[Microsoft]]
| released = {{Start date|2007|11}}
| latest_release_version = 1.0 SP2
| latest_release_date = 2008
| operating_system = [[Microsoft Windows]]
| genre = [[Enterprise Performance Management]]
| license = [[Proprietary software|Proprietary]] [[EULA]]
| website = [https://web.archive.org/web/20071016055516/http://www.microsoft.com/business/performancepoint/ www.microsoft.com/business/performancepoint]
}}
'''Microsoft Office PerformancePoint Server''' is a [[business intelligence]] [[Computer software|software]] product released in 2007 by [[Microsoft]]. Although discontinued in 2009, the dashboard, scorecard, and analytics capabilities of PerformancePoint Server were incorporated into [[Sharepoint 2010|SharePoint 2010]] and later versions.

PerformancePoint Server also provided a planning and budgeting component directly integrated with Excel.

==History==

Microsoft offered preview releases of PerformancePoint Server starting in mid-2006. Previews of the product were formed from [[Business Scorecard Manager 2005]] and the Planning Server component. Acquisitions [[ProClarity Corporation|ProClarity]] and [[Great Plains Software|Great Plains]] brought additional analytics and planning/reporting capabilities, as well as companion products ProClarity 6.3 and [[Microsoft FRx|FRx]].

PerformancePoint Server was officially released in November 2007.

Microsoft discontinued PerformancePoint Server as an independent product in 2009 and folded its dashboard, scorecard and analytics capabilities into PerformancePoint Services in [[SharePoint Server 2010]].<ref>{{cite web |url=http://www.informationweek.com/news/business_intelligence/analytics/showArticle.jhtml?articleID=212902915&subSection=Business+Intelligence |title=Microsoft Makes Sweeping Changes To BI Software Strategy |date=January 27, 2009 |last=Weier |first=Mary Hayes |work=[[InformationWeek]]}}</ref>

==Monitoring Server Component==
Business monitoring capabilities, including dashboards, scorecards & key performance indicators, navigable reports for deeper analysis, strategy maps, and linked filtering, are provided by PerformancePoint's Monitoring Server component. A Dashboard Designer application that is distributed from Monitoring Server enables business analysts or IT Administrators to:

* create & test data source connections
* create views that use those data connections
* assemble the views into a dashboard
* deploy the dashboard as a [[SharePoint]] page

Dashboard Designer saved content and security information back to the Monitoring Server. Data source connections, such as OLAP cubes or relational tables, were also made through Monitoring Server.
 
After a dashboard has been published to the Monitoring Server database, it would be deployed as a SharePoint page and shared with other users as such. When the pages were opened in a web browser, Monitoring Server updated the data in the views by connecting back to the original data sources.

==Planning Server Component==
PerformancePoint's Planning Server component supported maintenance of logical business models, budget & approval workflows, enterprise data sources, and it followed [[Generally Accepted Accounting Principles]].

Planning Server made use of Excel for input and line-of-business reporting, as well as SQL Server for storing and processing business models.

==Management Reporter Component==
The Management Reporter component was designed to perform financial reporting and can read PerformancePoint Planning models directly. A development kit was also available to allow this component to read other models .{{which|date=August 2013}}

==References==
{{reflist}}

==External links==
* [http://msdn2.microsoft.com/en-us/office/bb660518.aspx PerformancePoint Server 2007 Developer Portal]
* [http://blogs.technet.com/datapuzzle Data Puzzle]
* [http://performancepointinsider.com/blogs/default.aspx PerformancePoint Insider]
* [http://alanwhitehouse.wordpress.com/2009/01/26/pps-planning-being-discontinued/ Performance Point Planning being discontinued]

{{Microsoft Office}}

[[Category:Microsoft Office servers]]
[[Category:Business intelligence]]
[[Category:Data management]]
<=====doc_Id=====>:82
<=====title=====>:
Category:Data structures
<=====text=====>:
{{catdiffuse}}

{{Commons cat|Data structures}}
{{Category see also|Data types}}

In [[computer science]], a '''[[data structure]]''' is a way of storing [[data]] in a computer so that it can be used efficiently. Often a carefully chosen data structure will allow a more efficient [[algorithm]] to be used. The choice of the data structure must begin from the choice of an [[abstract data structure]].

{{Cat main|Data structures}}

== See also ==

* [[:Category:Abstract data types]]
* [[:Category:Hashing]]
* [[:Category:Computer file formats]]

[[Category:Algorithms and data structures]]
[[Category:Computer data|Structures]]
[[Category:Data management|Structures]]
[[Category:Computer programming]]
<=====doc_Id=====>:85
<=====title=====>:
Category:Databases
<=====text=====>:
{{Commonscat|Databases}}
{{distinguish|Category:Database software}}

:*'''[[Database]]s'''

{{clr}}
::{{Cat main|Database}}
:::::::{{cat see also|Digital libraries}}

{{catdiffuse}}

{{Database}}
{{Databases}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:Information retrieval systems]]
<=====doc_Id=====>:88
<=====title=====>:
Category:Computer-aided software engineering tools
<=====text=====>:
{{Cat main|Computer-aided software engineering}}

[[Category:Data management]]
[[Category:Computer programming tools]]
<=====doc_Id=====>:91
<=====title=====>:
Transaction data
<=====text=====>:
{{Unreferenced|date=July 2010}}

'''Transaction data''' are data describing an event (the change as a result of a [[Transaction processing|transaction]]) and is usually described with verbs. Transaction data always has a time dimension, a numerical value and refers to one or more objects (i.e. the [[reference data]]).

Typical transactions are:
* Financial: orders, invoices, payments
* Work: Plans, activity records
* [[Logistics]]: Deliveries, storage records, travel records, etc.

Typical [[transaction processing system]]s (systems generating transactions) are [[SAP ERP|SAP]] and [[Oracle Financials]].

==Records management==
{{Main|Records management}}
Recording and retaining transactions is called [[records management]]. The record of the transaction is stored in a place where the [[wikt:retention|retention]] can be guaranteed and where data are archived/removed following a [[retention period]]. The format of the transaction can be data (to be stored in a database), but it can also be a document.

==Data Warehousing==
Transaction data can be summarised in a [[Data warehouse]], which helps accessibility and analysis of the data.

==See also==
* [[Data modeling]]
* [[Data architecture]]
* [[Information Lifecycle Management]]
* [[reference data]]

[[Category:Data management]]
[[Category:Transaction processing]]


{{compsci-stub}}
<=====doc_Id=====>:94
<=====title=====>:
Paper data storage
<=====text=====>:
{{refimprove|date=August 2012}}
'''Paper data storage''' refers to the use of [[paper]] as a [[data storage device]]. This includes [[writing]], [[illustrating]], and the use of data that can be interpreted by a machine or is the result of the functioning of a machine.  A defining feature of paper data storage is the ability of humans to produce it with only simple tools and interpret it visually.

Though this is now mostly obsolete, paper was once also an important form of [[computer data storage]].

==History==
Before paper was used for storing data, it had been used in several applications for storing instructions to specify a machine's operation.  The earliest use of paper to store instructions for a machine was the work of [[Basile Bouchon]] who, in 1725, used punched paper rolls to control textile looms.  This technology was later developed into the wildly successful [[Jacquard loom]].  The 19th century saw several other uses of paper for controlling machines.  In 1846, telegrams could be prerecorded on [[punched tape]] and rapidly transmitted using [[Alexander Bain (inventor)|Alexander Bain]]'s automatic telegraph.  Several inventors took the concept of a mechanical organ and used paper to represent the music.  

In the late 1880s [[Herman Hollerith]] invented the recording of data on a medium that could then be read by a machine.  Prior uses of machine readable media, above, had been for control ([[automaton]]s, [[piano roll]]s, [[Jacquard loom|looms]], ...), not data.  "After some initial trials with paper tape, he settled on [[punched card]]s..."<ref>[http://www.columbia.edu/acis/history/hollerith.html Columbia University Computing History - Herman Hollerith]</ref>  Hollerith's method was used in the 1890 census.<!-- The Census Bureau is not "an independent 3rd party" source - as required by Wikipedia - for Census Bureau performance claims. FOLLOWING CLAIM DELETED. --- and the completed results were "... finished months ahead of schedule and far under budget".<ref>[http://www.census.gov/history/www/technology/010873.html U.S. Census Bureau: Tabulation and Processing]</ref>-->  Hollerith's company eventually became the core of [[International Business Machines|IBM]]. 

Other technologies were also developed that allowed  machines to work with marks on paper instead of punched holes.  This technology was widely used for [[optical scan voting system|tabulating votes]] and grading [[scantron|standardized tests]].  [[Barcode]]s made it possible for any object that was to be sold or transported to have some computer readable information securely attached to it. Banks used magnetic ink on checks, supporting MICR scanning.

In an early electronic computing device, the [[Atanasoff-Berry Computer]], electric sparks were used to singe small holes in paper cards to represent binary data.  The altered [[dielectric constant]] of the paper at the location of the holes could then be used to read the binary data back into the machine by means of electric sparks of lower voltage than the sparks used to create the holes.  This form of paper data storage was never made reliable and was not used in any subsequent machine.

As of 2014, [[Universal Product Code]] barcodes, first used in 1974, are ubiquitous.

Some people recommend a width of at least 3 pixels for each minimum-width gap and each minimum-width bar for 1D barcodes;
and a width of at least 4 pixels—e.g., a 4&nbsp;×&nbsp;4 pixel = 16 pixel module for [[2D barcode]]s.<ref>
Accusoft.
[http://www.accusoft.com/whitepapers/barcodes/BarcodesinDocuments-BestPractices.pdf "Using Barcodes in Documents – Best Practices"].
2007.
Retrieved 2014-04-25.
</ref>
For a typical black-and-white barcode scanned by a typical 300 dpi [[image scanner]],
and assuming roughly half the space is occupied by finder patterns, fiducial alignment patterns, and error detection and correction codes, that recommendation gives a maximum data density of roughly 50 bits per linear inch (about 2 bit/mm) for 1D barcodes, and roughly 2 800 bits per square inch (about 4.4 bit/mm<sup>2</sup>).

==Limits==
The limits of data storage depend on the technology to write and read such data.  For example, an 8″&nbsp;×&nbsp;10″ (roughly A4 without margins) 300dpi 8-bit greyscale image map contains 7.2 megabytes of data—assuming a scanner can accurately reproduce the printed image to that resolution and [[color depth]], and a program can accurately interpret such an image.  A similarly sized image in 2400dpi 24-bit true color theoretically contains 1.38 gigabytes of information.

==See also==
{{div col|3}}
*[[Banknote]] read by a [[vending machine]]
*[[Book music]]
*[[Edge-notched card]]
*[[Index card]]
*[[Kimball tag]]
*[[Machine-readable medium]]
*[[Magnetic ink character recognition]]
*[[Mark sense]]
*[[Music roll]]
*[[Optical mark recognition]]
*[[Paper disc]]
*[[Perfin]]
*[[Perforation]]
*[[Punched tape]]
*[[Spindle (stationery)]]
*[[Stenotype]]
*[[Ticker tape]]
{{div col end}}

==References==
{{reflist}}

==External links==
* [http://www.microglyphs.com/english/html/dataglyphs.shtml DataGlyphs]
* [http://ollydbg.de/Paperbak/ PaperBack data storage]
{{Paper data storage media}}

[[Category:Data management]]
[[Category:Storage media]]
<=====doc_Id=====>:97
<=====title=====>:
Information architecture
<=====text=====>:
{{Information science}}
'''Information architecture''' ('''IA''') is the structural design of shared [[information]] environments; the art and science of organizing and labelling [[website]]s, [[intranet]]s, [[online communities]] and [[software]] to support usability and findability; and an emerging [[community of practice]] focused on bringing principles of [[design]] and [[architecture]] to the digital landscape.<ref name = "What">{{Cite journal | title = What is IA? | publisher = Information Architecture Institute | url = http://www.iainstitute.org/documents/learn/What_is_IA.pdf | format = [[PDF]] | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}.</ref>  Typically, it involves a [[Scientific modelling|model]] or [[concept]] of [[information]] that is used and applied to activities which require explicit details of complex [[information system]]s. These activities include [[library]] systems and [[database]] development.

Information architecture is considered to have been founded by [[Richard Saul Wurman]].<ref name = "Richard Saul Wurman, Cooper-Hewitt">{{cite web|title=Richard Saul Wurman awarded for Lifetime Achievement|url=http://wurman.com/rsw/|publisher=Smithsonian Cooper-Hewitt, National Design Museum|accessdate=19 April 2014}}</ref> Today there is a growing network of active IA specialists who constitute the [[Information Architecture Institute]].<ref>{{Cite journal | title = Join the IA Network | publisher = Information Architecture Institute | url = http://www.iainstitute.org/en/network/ | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}.</ref>

==Definition==
''Information architecture'' has somewhat different meanings in different branches of [[Information system|IS]] or [[Information technology|IT]]:
# The structural design of shared information environments.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}
# The art and science of organizing and labeling web sites, intranets, online communities, and software to support [[findability]] and [[usability]].<ref name="What"/><ref>Morville&nbsp;&amp; Rosenfeld (2007). p.&nbsp;4. "The art and science of shaping information products and experienced to support usability and findability."</ref>
# An emerging [[community of practice]] focused on bringing principles of design and architecture to the digital landscape.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}<ref>Resmini, A. & Rosati, L. (2012). A Brief History of Information Architecture. ''Journal of Information Architecture''. Vol. 3, No. 2. [Available at http://journalofia.org/volume3/issue2/03-resmini/]. Originally published in Resmini, A. & Rosati L. (2011). ''Pervasive Information Architecture''. Morgan Kauffman. (Edited by the authors).</ref>
# The combination of organization, labeling, search and navigation systems within websites and intranets.{{Sfn | Morville | Rosenfeld | 2007}}{{Rp|4}}
# Extracting required parameters/data of Engineering Designs in the process of creating a knowledge-base linking different systems and standards.  
# A subset of [[data architecture]] where usable data (a.k.a. information) is constructed in and designed or arranged in a fashion most useful or empirically holistic to the users of this data.
# The practice of organizing the information / content / functionality of a web site so that it presents the best user experience it can, with information and services being easily usable and findable (as applied to web design and development).<ref>{{Cite web|url=https://developer.mozilla.org/en-US/docs/Glossary/Information_architecture|title=Information Architecture|last=|first=|date=|website=|publisher=Mozilla Developer Network|access-date=}}</ref>

=== Debate ===
The difficulty in establishing a common definition for "information architecture" arises partly from the term's existence in multiple fields.  In the field of [[systems design]], for example, information architecture is a component of [[enterprise architecture]] that deals with the information component when describing the structure of an enterprise.

While the definition of information architecture is relatively well-established in the field of systems design, it is much more debatable within the context of online information systems (i.e., websites). Andrew Dillon refers to the latter as the "big IA–little IA debate".<ref>{{Cite journal | last = Dillon | first = A | year = 2002 | title = Information Architecture in JASIST: Just where did we come from? | journal = Journal of the American Society for Information Science and Technology | volume = 53 | pages = 821–23 | issue = 10 | doi = 10.1002/asi.10090 | postscript = <!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}.</ref> In the little IA view, information architecture is essentially the application of [[information science]] to [[web design]] which considers, for example, issues of classification and information retrieval. In the big IA view, information architecture involves more than just the organization of a website; it also factors in [[user experience]], thereby considering [[usability]] issues of [[information design]].

==Information architect==
About the term '''information architect''' [[Richard Saul Wurman]] wrote: "I mean architect as used in the words ''architect of foreign policy''. I mean architect as in the creating of systemic, structural, and orderly principles to make something work — the thoughtful making of either artifact, or idea, or policy that informs because it is clear."<ref>Wurman, "Introduction", in: ''Information Architects'' (1997). p. 16.</ref>

==Notable people in information architecture==

===Pioneers===
*[[Richard Saul Wurman]]
*[[Peter Morville]]
*[[Louis Rosenfeld]]

===First generation===
*Jorge Arango
*[[Jesse James Garrett]]
*[[Adam Greenfield]]
*[[Peter Merholz]]
*[[Eric Reiss]]
*[[Donna Spencer]]
*[[Christina Wodtke]]

===Second generation===
*[[Abby Covert]]
*[[Andrew Hinton]]
*[[Dan Klyn]]
*[[Andrea Resmini]]

===Influencers===
*[[David Weinberger]]

== See also ==
{{Div col}}
* [[Applications architecture]]
* [[Card sorting]]
* [[Chief experience officer]]
* [[Content management]]
* [[Content strategy]]
* [[Controlled vocabulary]]
* [[Data management]]
* [[Data presentation architecture]]
* [[Digital humanities]]
* [[Ecological interface design]]
* [[Enterprise information security architecture]]
* [[Faceted classification]]
* [[Human factors and ergonomics]]
* [[Informatics]]
* [[Interaction design]]
* [[Process architecture]]
* [[Site map]]
* [[Social information architecture]]
* [[Tree testing]]
* [[User experience design]]
* {{section link|Visualization (graphics)|Knowledge visualization}}
* [[Wayfinding]]
* [[Web graph]]
* [[Web literacy]] (Infrastructure)
{{Div col end}}

== References ==
{{reflist}}

== Bibliography ==
* {{Cite book | editor-last1 = Wurman | editor-first1 = Richard Saul | editor1-link = Richard Saul Wurman | isbn = 1-888-00138-0 | url = http://www.amazon.com/dp/1888001380 | year = 1997 | title = Information Architects | edition = 1st | publisher = Graphis Inc. }}
* {{Cite book | last2 = Rosenfeld | first2 = Louis | author2-link = Lou Rosenfeld | last1 = Morville | first1 = Peter | author1-link = Peter Morville | isbn = 0-596-52734-9 | url = https://books.google.com/books?id=2d2Ry2hZc2MC&printsec=frontcover&dq=information+architecture#v=onepage&q&f=false | year = 2007 | title = Information architecture for the World Wide Web | edition = 3rd|publisher = O'Reilly & Associates | place = Sebastopol, CA | ref=harv}}
* {{Cite book | last1 = Brown | first1 = Peter  | isbn = 0-471-48679-5 | url = http://www.amazon.com/dp/0471486795 | year = 2003 | title = Information Architecture with XML | edition = 1st | publisher = John Wiley & Sons Ltd. }}
* {{Cite book | last1 = Wodtke | first1 = Christina | author1-link = Christina Wodtke | isbn = 0-321-60080-0 | url = https://books.google.com/books?id=Tp40QFGCU2sC | year = 2009 | title = Information Architecture - Blueprints for the Web | edition = 2nd | publisher = New Riders }}
* {{Cite book | last1 = Resmini | first1 = Andrea | last2 = Rosati | first2 = Luca | isbn = 0-123-82094-4 | url = https://books.google.com/books?id=ntWc13nSiNkC | year = 2011 | title = Pervasive Information Architecture - Designing Cross-channel User Experiences | edition = 1st | publisher = Morgan Kauffman }}

== Further reading ==
* {{cite book|author1=Wei Ding|author2=Xia Lin|title=Information Architecture: The Design and Integration of Information Spaces|url=https://books.google.com/books?id=-wy3RhKoWWQC|date= 15 May 2009 | publisher=Morgan & Claypool |isbn=978-1-59829-959-5}}
* {{cite book|author1=Sue Batley|title=Information Architecture for Information Professionals|url=https://books.google.com/books?id=6g0PAQAAMAAJ|date=January 2007| publisher=Woodhead Publishing |isbn=978-1-84334-233-5}}
* {{cite book|author1=Earl Morrogh|title=Information Architecture: An Emerging 21st Century Profession
|url=https://books.google.com/books?id=JzlmQgAACAAJ&dq|year=2003| publisher=Prentice Hall |isbn=9780130967466}}
* {{cite book|author1=Peter Van Dijck|title=Information Architecture for Designers: Structuring Websites for Business Success |url=https://books.google.com/books?id=Wy2sb0r_udYC&dq|date=August 1, 2003| publisher=Rotovision|isbn=9782880467319}}
* {{cite book|author1=Alan Gilchrist|author2=Barry Mahon|title=Information Architecture: Designing Information Environments for Purpose|url=https://books.google.com/books?id=akxqAAAAMAAJ&q|year=2004| publisher=Facet|isbn=9781856044875}}

{{Semantic Web}}

[[Category:Data management]]
[[Category:Enterprise architecture]]
[[Category:Information architects]]
[[Category:Information governance]]
[[Category:Information science]]
[[Category:Information technology management]]
[[Category:Information technology]]
[[Category:Records management]]
[[Category:Technical communication]]
[[Category:Information architecture| ]]
<=====doc_Id=====>:100
<=====title=====>:
Category:Semantic Web
<=====text=====>:
{{Cat main|Semantic Web}}
{{Commons cat|Semantic Web}}

[[Category:Internet ages|Web 3]]
[[Category:World Wide Web Consortium]] <!-- the Semantic Web is a major W3C activity -->
[[Category:Knowledge representation]]
[[Category:Data management]]
<=====doc_Id=====>:103
<=====title=====>:
Vector-field consistency
<=====text=====>:
'''Vector-Field Consistency'''<ref group="nb"><sub>Designation coined by L. Veiga.</sub></ref> is a [[consistency model]] for replicated data (for example, objects), initially described in a paper<ref>{{cite conference |author1=Nuno Santos |author2=Luís Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~pjpf/middleware07vector.pdf | format=PDF}}</ref> which was awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007. It has since been enhanced for increased scalability and fault-tolerance in a recent paper.<ref>{{cite conference |author1=Luís Veiga |author2=André Negrão |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency 
| booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}</ref>

== Description ==
This consistency model was initially designed for replicated [[data management]] in adhoc gaming in order to minimize bandwidth usage without sacrificing playability. Intuitively, it captures the notion that although players require, wish, and take advantage of information regarding the whole of the game world (as opposed to a restricted view to rooms, arenas, etc. of limited size employed in many [[multiplayer game]]s), they need to know information with greater freshness, frequency, and accuracy as other game entities are located closer and closer to the player's position.

It prescribes a multidimensional divergence bounding scheme, based on a [[vector field]] that employs consistency vectors k=(θ,σ,ν), standing for maximum allowed '''t'''ime - or replica staleness, '''s'''equence - or missing updates, and '''v'''alue<ref group="nb"><sub>Since in the [[Greek alphabet]] there was no letter for the ''vee'' sound, the ''nu'' letter was preferred for its resemblance with the roman V, for ''v''alue, instead of β (''beta'') for the ''vee'' sound in contemporary Greek speaking.</sub></ref> - or user-defined measured replica divergence, applied to all space coordinates in game scenario or world.

The consistency vector-fields emanate from field-generators designated as pivots (for example, players) and [[Field strength|field intensity]] attenuates as distance grows from these pivots in concentric or square-like regions. This consistency model unifies locality-awareness techniques employed in message routing and consistency enforcement for multiplayer games, with divergence bounding techniques traditionally employed in replicated database and web scenarios.

== Notes ==
<references group="nb"/>

== References ==
<references/>

[[Category:Data management]]
<=====doc_Id=====>:106
<=====title=====>:
SIGMOD Edgar F. Codd Innovations Award
<=====text=====>:
The [[Association for Computing Machinery|ACM]] '''[[SIGMOD]] [[Edgar F. Codd]] Innovations Award''' is a lifetime research achievement award given by the ACM Special Interest Group on Management of Data, at its yearly flagship conference (also called SIGMOD). According to its homepage, it is given "for innovative and highly significant contributions of enduring value to the development, understanding, or use of database systems and databases".<ref>http://www.sigmod.org/sigmod-awards</ref> The award has been given since 1992.

== Recipients ==
{| class="wikitable"
! Year
! Name
|-
|2016
|[[Gerhard Weikum]]
|-
| 2015
| [[Laura M. Haas]]<ref>[http://www.sigmod.org/all-news/dr.-laura-haas-is-the-recipient-of-the-2015-sigmod-edgar-f.-codd-innovation-award Dr. Laura Haas is the recipient of the 2015 SIGMOD Edgar F. Codd Innovation Award], [[SIGMOD]], retrieved 2015-06-21.</ref>
|-
| 2014
| [[Martin L. Kersten]]<ref>{{cite web |url=https://www.cwi.nl/news/2014/international-innovation-award-big-data-research-martin-kersten |title=International innovation award to Martin Kersten |date=26 June 2014 |website=CWI Amsterdam |accessdate=26 June 2014}}</ref>
|-
| 2013
| [[Stefano Ceri]]
|-
| 2012
| Bruce Lindsay
|-
| 2011
| [[Surajit Chaudhuri]]
|-
| 2010
| [http://www.hpl.hp.com/people/umesh_dayal/ Umeshwar Dayal]
|-
| 2009
| [[Masaru Kitsuregawa]]
|-
| 2008
| [[Moshe Y. Vardi]]
|-
| 2007
| [[Jennifer Widom]]
|-
| 2006
| [[Jeffrey D. Ullman]]
|-
| 2005
| Michael Carey
|-
| 2004
| [[Ronald Fagin]]
|-
| 2003
| [[Don Chamberlin]]
|-
| 2002
| [[Patricia Selinger]]
|-
| 2001
| [[Rudolf Bayer]]
|-
| 2000
| [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]]
|-
| 1999
| [[Hector Garcia-Molina]]
|-
| 1998
| [[Serge Abiteboul]]
|-
| 1997
| [[David Maier]]
|-
| 1996
| [[C. Mohan]]
|-
| 1995
| [[David DeWitt]]
|-
| 1994
| [[Phil Bernstein|Philip Bernstein]]
|-
| 1993
| [[Jim Gray (computer scientist)|Jim Gray]]
|-
| 1992
| [[Michael Stonebraker]]
|}

== References ==
<references/>

[[Category:Association for Computing Machinery]]
[[Category:Awards established in 1992]]
[[Category:Computer science awards]]
[[Category:Data management]]
<=====doc_Id=====>:109
<=====title=====>:
Category:Document-oriented databases
<=====text=====>:
A '''document-oriented database''' is a [[database management system]] designed for document-oriented applications
{{Cat main|Document-oriented database}}
{{see also|Document-oriented database#Implementations}}
{{see also cat|Full text databases}}
{{see also cat|Key-value databases}}

[[Category:Data management]]
[[Category:Database management systems]]
[[Category:Types of databases]]
<=====doc_Id=====>:112
<=====title=====>:
Conference on Innovative Data Systems Research
<=====text=====>:
{{multiple issues|
{{notability|Events|date=September 2011}}
{{primary sources|date=September 2011}}
}}

{{Infobox Academic Conference
 | history = 2002–
 | discipline = [[Database]]
 | abbreviation = CIDR
 | publisher = CIDR Conference
 | country= [[United States]]
 | frequency = biennial
}}
The '''Conference on Innovative Data Systems Research''' ('''CIDR''') is a biennial [[computer science]] conference focused on research into new techniques for [[data management]]. It was started in 2002 by [[Michael Stonebraker]], [[Jim Gray (computer scientist)|Jim Gray]], and [[David DeWitt]], and is held at the [[Asilomar Conference Grounds]] in [[Pacific Grove, California]].

CIDR focuses on presenting work that is more speculative, radical, or provocative than what is typically accepted by the traditional database research conferences (such as the [[International Conference on Very Large Data Bases]] (VLDB) and the [[ACM SIGMOD Conference]]).

==See also==
* [[International Conference on Very Large Data Bases]] (VLDB)
* [[ACM SIGMOD Conference]]

==External links==
* [http://www-db.cs.wisc.edu/cidr/ CIDR website]

[[Category:Data management]]
[[Category:Computer science conferences]]


{{database-stub}}
{{compu-conference-stub}}
<=====doc_Id=====>:115
<=====title=====>:
Parchive
<=====text=====>:
{{Merge from|QuickPAR|date=March 2014}}

{{Infobox file format
| name = Parchive
| extension = .par, .par2, .p??, (.par3 future)
| mime =
| owner =
| creatorcode =
| genre = [[Erasure code]]
| containerfor =
| containedby =
| extendedfrom =
| extendedto =
}}

'''Parchive''' (a [[portmanteau]] of '''parity archive''', and formally known as '''Parity Volume Set Specification'''<ref>[https://www.livebusinesschat.com/smf/index.php?topic=5736.msg38234#msg38234 Re: Correction to Parchive on Wikipedia], reply #3, by Yutaka Sawada: "Their formal title are "Parity Volume Set Specification 1.0" and "Parity Volume Set Specification 2.0."</ref>) is an [[erasure code]] system that produces '''par''' files for [[checksum]] verification of [[data integrity]], with the capability to perform [[data recovery]] operations that can repair or regenerate corrupted or missing data. 

Parchive was originally written to solve the problem of reliable file sharing on [[Usenet]],<ref>{{cite web
| url         = http://parchive.sourceforge.net/#desc
| title       = Parchive: Parity Archive Volume Set
| accessdate  = 2009-10-29
| quote       = The original idea behind this project was to provide a tool to apply the data-recovery capability concepts of RAID-like systems to the posting and recovery of multi-part archives on Usenet.
}}</ref> but it is now commonly used for protecting any kind of data from [[data corruption]], [[disc rot]], [[data degradation|bit rot]], and accidental or malicious damage. Despite the name, Parchive uses more advanced techniques that do not utilize simplistic [[Parity bit|parity]] methods of [[error detection and correction]].

As of 2014, '''PAR1''' is obsolete, '''PAR2''' is mature for widespread use, and '''PAR3''' is an experimental version being developed by MultiPar author Yutaka Sawada.<ref>[http://www.livebusinesschat.com/smf/index.php?topic=5098.0 possibility of new PAR3 file]</ref><ref>[http://www.livebusinesschat.com/smf/index.php?topic=3339.0 Question about your usage of PAR3]</ref><ref>[http://www.livebusinesschat.com/smf/index.php?topic=5025.msg29912;topicseen#msg29912 Risk of undetectable intended modification]</ref><ref>[http://www.livebusinesschat.com/smf/index.php?topic=3527.msg8850;topicseen#msg8850 PAR3 specification proposal not finished as of April 2011]</ref>  The original SourceForge Parchive project has been inactive since November 9, 2010.<ref>{{cite web |url = http://sourceforge.net/projects/parchive/ |title = Parchive: Parity Archive Tool |accessdate = 2012-09-02}}</ref> 

== History ==
Parchive was intended to increase the reliability of transferring files via Usenet [[newsgroup]]s. Usenet was originally designed for informal conversations, and the underlying protocol, [[NNTP]] was not designed to transmit arbitrary binary data. Another limitation, which was acceptable for conversations but not for files, was that messages were normally fairly short in length and limited to 7-bit [[ASCII]] text.<ref>{{cite IETF
| title       = Network News Transfer Protocol
| rfc         = 977
| sectionname = Character Codes
| section     = 2.2
| page        = 5
| last1       = Kantor
| first1      = Brian
| authorlink1 =
| last2       = Lapsley
| first2      = Phil
| authorlink2 = Phil Lapsley
| year        = 1986
| month       = February
| publisher   = [[Internet Engineering Task Force|IETF]]
| accessdate  = 2009-10-29
}}</ref>

Various techniques were devised to send files over Usenet, such as [[uuencode|uuencoding]] and [[Base64]]. Later Usenet software allowed  8 bit [[Extended ASCII]], which permitted new techniques like [[yEnc]]. Large files were broken up to reduce the effect of a corrupted download, but the unreliable nature of Usenet remained.

With the introduction of Parchive, parity files could be created that were then uploaded along with the original data files. If any of the data files were damaged or lost while being propagated between Usenet servers, users could download parity files and use them to reconstruct the damaged or missing files. Parchive included the construction of small index files (*.par in version 1 and *.par2 in version 2) that do not contain any recovery data. These indexes contain [[hash function|file hash]]es that can be used to quickly identify the target files and verify their integrity.

Because the index files were so small, they minimized the amount of extra data that had to be downloaded from Usenet to verify that the data files were all present and undamaged, or to determine how many parity volumes were required to repair any damage or reconstruct any missing files. They were most useful in version 1 where the parity volumes were much larger than the short index files. These larger parity volumes contain the actual recovery data along with a duplicate copy of the information in the index files (which allows them to be used on their own to verify the integrity of the data files if there is no small index file available).

In July 2001, Tobias Rieper and Stefan Wehlus proposed the Parity Volume Set specification, and with the assistance of other project members, version 1.0 of the specification was published in October 2001.<ref>{{cite web|url=http://sourceforge.net/docman/display_doc.php?docid=7273&group_id=30568 |title=Parchive: Parity Volume Set specification 1.0 |accessdate=2009-04-07 |last=Nahas |first=Michael |date=2001-10-14 |deadurl=yes |archiveurl=https://web.archive.org/web/20081220184024/http://sourceforge.net/docman/display_doc.php?docid=7273&group_id=30568 |archivedate=December 20, 2008 }}</ref> Par1 used [[Reed–Solomon error correction]] to create new recovery files. Any of the recovery files can be used to rebuild a missing file from an incomplete [[download]].

Version 1 became widely used on Usenet, but it did suffer some limitations:
* It was restricted to handle at most 255 files.
* The recovery files had to be the size of the largest input file, so it did not work well when the input files were of various sizes. (This limited its usefulness when not paired with the proprietary RAR compression tool.)
* The recovery algorithm had a bug, due to a flaw<ref>{{cite web
| url         = http://web.eecs.utk.edu/~plank/plank/papers/CS-03-504.html
| title       = Note: Correction to the 1997 Tutorial on Reed-Solomon Coding
| accessdate  = 2009-10-29
| last        = Plank
| first       = James S.
|author2=Ding, Ying
|date=April 2003
}}</ref> in the academic paper<ref>{{cite web
| url         = http://web.eecs.utk.edu/~plank/plank/papers/SPE-9-97.html
| title       = A Tutorial on Reed-Solomon Coding for Fault-Tolerance in RAID-like Systems
| accessdate  = 2009-10-29
| last        = Plank
| first       = James S.
|date=September 1997
}}</ref> on which it was based.
* It was strongly tied to Usenet and it was felt that a more general tool might have a wider audience.

In January 2002, Howard Fukada proposed that a new Par2 specification should be devised with the significant changes that data verification and repair should work on blocks of data rather than whole files, and that the algorithm should switch to using 16 bit numbers rather than the 8 bit numbers that PAR 1 used. Michael Nahas and Peter Clements took up these ideas in July 2002, with additional input from Paul Nettle and Ryan Gallagher (who both wrote Par1 clients). Version 2.0 of the Parchive specification was published by Michael Nahas in September 2002.<ref>{{cite web
| url         = http://parchive.sourceforge.net/docs/specifications/parity-volume-spec/article-spec.html
| title       = Parity Volume Set Specification 2.0
| accessdate  = 2009-10-29
| last        = Nahas
| first       = Michael |author2=Clements, Peter |author3=Nettle, Paul |author4=Gallagher, Ryan
| date        = 2003-05-11
}}</ref>

Peter Clements then went on to write the first two Par2 implementations, [[QuickPar]] and par2cmdline. Abandoned since 2004, Paul Houle created phpar2 to supersede par2cmdline. Yutaka Sawada created MultiPar to supersede QuickPar. Sawada maintains par2cmdline to use as MultiPar's PAR engine backend.

On May 10, 2014, Sawada reported a hash collision security problem in par2cmdline (the backend for MultiPar):<ref name="livebusinesschat.com">[https://www.livebusinesschat.com/smf/index.php?topic=5579.0 v1.2.5.3 is public]</ref>

<blockquote>I'm not sure this problem can be used for DoS attack against automated Par2 usage. If someone has a skill to forge CRC-32, it is possible to make a set of source file and Par2 file, which freeze a Par2 client for several hours.</blockquote>

== Versions ==
Versions 1 and 2 of the [[file format]] are incompatible. (However, many clients support both.)

=== Parity Volume Set Specification 1.0 ===
For Par1, the files ''f1'', ''f2'', ..., ''fn'', the Parchive consists of an index file (''f.par''), which is CRC type file with no recovery blocks, and a number of "parity volumes" (''f.p01'', ''f.p02'', etc.). Given all of the original files except for one (for example, ''f2''), it is possible to create the missing ''f2'' given all of the other original files and any one of the parity volumes. Alternatively, it is possible to recreate two missing files from any two of the parity volumes and so forth.<ref>{{cite book
| last        = Wang
| first       = Wallace
| authorlink  =
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 – 167
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}</ref>

Par1 supports up to 256 recovery files. Each recovery file must be the size of the largest input file.

=== Parity Volume Set Specification 2.0 ===
Par2 files generally use this naming/extension system: ''filename.vol000+01.PAR2'', ''filename.vol001+02.PAR2'', ''filename.vol003+04.PAR2'', ''filename.vol007+06.PAR2'', etc. The +01, +02, etc. in the filename indicates how many blocks it contains, and the vol000, vol001, vol003 etc. indicates the number of the first recovery block within the PAR2 file. If an index file of a download states that 4 blocks are missing, the easiest way to repair the files would be by downloading ''filename.vol003+04.PAR2''. However, due to the redundancy, ''filename.vol007+06.PAR2'' is also acceptable. There is also an index file ''filename.PAR2'', it is identical in function to the small index file used in PAR1.

Par2 supports up to 65536 (2<sup>16</sup>) recovery blocks (however, par2cmdline, the official PAR2 implementation, it limited to 32767 blocks at once). Input files are split into multiple equal-sized blocks so that recovery files do not need to be the size of the largest input file.

Although [[Unicode]] is mentioned in the PAR2 specification as an option, most PAR2 implementations do not support unicode.<ref>[http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 QuickPar forum posting] {{webarchive |url=https://web.archive.org/web/20120302104523/http://www.quickpar.co.uk/forum/viewtopic.php?id=1065 |date=March 2, 2012 }}</ref>

Directory support is included in the PAR2 specification, but most or all implementations do not support it.

=== Parity Volume Set Specification 3.0 ===
Par3 is a planned improvement over Par2.<ref>{{cite web|url=http://hp.vector.co.jp/authors/VA021385/|title=MultiPar announcement|publisher=}}</ref><ref>[http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 QuickPar forum posting&nbsp;– status PAR3] {{webarchive |url=https://web.archive.org/web/20101127125317/http://www.quickpar.org.uk/forum/viewtopic.php?id=1264 |date=November 27, 2010 }}</ref><ref>[http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 QuickPar forum posting&nbsp;– PAR3 specifications] {{webarchive |url=https://web.archive.org/web/20120316104813/http://www.quickpar.co.uk/forum/viewtopic.php?id=1047 |date=March 16, 2012 }}</ref><ref>[http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm PAR3 proposal] {{webarchive |url=https://web.archive.org/web/20100911002706/http://hp.vector.co.jp/authors/VA021385/par3_spec_prop.htm |date=September 11, 2010 }}</ref> The authors intend to fix problems related to creating or repairing when the block count or block size is very high. Par3 also adds support for including directories (file folders) in a parchive and Unicode characters in file names. In addition, the authors plan to enable the Par3 algorithm to identify files that have been moved or renamed.<ref>http://www.livebusinesschat.com/smf/index.php?topic=4751.0 PAR3 move/rename brainstorming</ref>

== Software ==

=== Windows ===
* MultiPar (freeware) &nbsp;— Builds upon QuickPar's features and [[GUI]], and Yutaka Sawada's fork of par2cmdline as the PAR2 backend.<ref name="livebusinesschat.com"/> It has support for Par3, [[multithreading (software)|multithreading]], [[Symmetric multiprocessor system|multiple processors]], and the ability to recurse subfolders. MultiPar is able to add recovery data to [[Zip (file format)|ZIP]] and [[7-Zip]]<ref>{{cite web|url=https://sourceforge.net/p/sevenzip/feature-requests/1006/|title=7-Zip|publisher=}}</ref> files, with a few minor caveats.<ref>[http://www.livebusinesschat.com/smf/index.php?topic=4922.0 How to add recovery record to ZIP or 7-Zip archive]</ref> MultiPar is also verified to work with [[Wine (software)|Wine]] under [[TrueOS]], and may work with other operating systems too.<ref>[http://www.livebusinesschat.com/smf/index.php?topic=4902.0 MultiPar works with PCBSD 9.0]</ref> Although the Par2 and Par3 components are (or will be) open source, the MultiPar GUI on top of them is currently not open source.<ref>[https://www.livebusinesschat.com/smf/index.php?topic=5402.0 contacted you, asking about sourcecode]</ref>  Download from [https://www.livebusinesschat.com/smf/index.php?board=396.0 MultiPar forum]. 
* [[QuickPar]] (freeware)&nbsp;— unmaintained since 2004, superseded by MultiPar.
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] ([[GNU General Public License|GPLv2]])&nbsp;— a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]].
* Par-N-Rar ([[GNU General Public License|GPL]])
* [http://paulhoule.com/phpar2/index.php phpar2] &nbsp;— advanced par2cmdline with multithreading and highly optimized assemblercode (about 66% faster than QuickPar 0.9.1)
* Rarslave ([[GNU General Public License|GPLv2]])
* [[SmartPAR]] (freeware) &nbsp;— Unmaintained since 2002 and obsolete as this application written for Microsoft Windows only works with the original Par1 (PAR) Parchive format parity files. Superseded by QuickPar. It uses Reed–Solomon error correction to create new recovery files. SmartPAR is able to correct errors and recover missing parts of distributed files from PAR files.<ref>{{cite book
| last        = Wang
| first       = Wallace
| authorlink  = 
| title       = Steal this File Sharing Book
| url         = https://books.google.com/books?id=FGfMS5kymmcC&pg=PT183
| accessdate  = 2009-09-24
| edition     = 1st
| date        = 2004-10-25
| publisher   = [[No Starch Press]]
| location    = [[San Francisco, California]]
| isbn        = 1-59327-050-X
| pages       = 164 – 167 
| chapter     = Finding movies (or TV shows): Recovering missing RAR files with PAR and PAR2 files
}}</ref> Last stable release 0.13d1 dated {{Start date and age|2002|01|22}}<ref>{{cite web |url=http://parchive.sourceforge.net/ |title=Parchive: Parity archive tool |accessdate=2009-09-26}}</ref>
* [http://www.wehlus.de/mirror/index.html Mirror]&nbsp;— First PAR implementation, unmaintained since 2001.
* [http://parchive.sourceforge.net/ Original par2cmdline]&nbsp;— (obsolete).
* [https://github.com/Parchive/par2cmdline par2cmdline] by BlackIkeEagle.

=== Mac OS X ===
* [https://gp.home.xs4all.nl/Site/MacPAR_deLuxe.html MacPAR deLuxe 4.2]
* [http://www.unrarx.com/ UnRarX]
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.

=== [[Linux]] ===
* The [https://github.com/Parchive/par2cmdline par2] utility, which is a maintained fork of par2cmdline. 
* [http://pypar2.silent-blade.org/index.php?n=Main.HomePage PyPar2 1.4], a frontend for par2.
* [http://sourceforge.net/projects/parchive/ GPar2 2.03]
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later.
* [https://github.com/jkansanen/par2cmdline-mt par2cmdline-mt] is another multithreaded version of par2cmdline using [[OpenMP]], [[GNU General Public License|GPLv2]], or later.

=== [[FreeBSD]] ===
* [https://web.archive.org/web/20110311041855/http://chuchusoft.com/par2_tbb/ par2+tbb] is a concurrent (multithreaded) version of par2cmdline 0.4 using [[Threading Building Blocks|TBB]], [[GNU General Public License|GPLv2]], or later. It is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline-tbb/ par2cmdline-tbb].
* [http://parchive.sourceforge.net/ par2cmdline] is available in the [[FreeBSD Ports]] system as [https://www.freshports.org/archivers/par2cmdline/ par2cmdline].

=== [[POSIX]] ===
Software for POSIX conforming operating systems:
* [http://sourceforge.net/projects/ekpar2/ Par2 for KDE 4]

== See also ==
* [[Data degradation|Bit rot]]
* [[Disc rot]]
* [[Data corruption]]
* [[Checksum]]
* [[Comparison of file archivers]] – Some [[file archivers]] are capable of integrating parity data into their formats for error detection and correction:
* [[RAID]]&nbsp;– RAID levels at and above RAID 5 make use of parity data to detect and repair errors.

== References ==
{{Reflist|30em}}

== External links ==
* [http://parchive.sourceforge.net/ Parchive project - full specifications and math behind it]
* [http://www.ydecode.com/page_articles003.htm Introduction to PAR and PAR2]
* [http://www.slyck.com/Newsgroups_Guide_PAR_PAR2_Files Slyck's Guide To The Usenet Newsgroups: PAR & PAR2 Files]
* [http://www.warezfaq.com/allaboutpar.htm Another introduction to PAR and PAR2] and [http://www.warezfaq.com/more_info.htm more information from the same site]
* [http://www.binaries4all.com/quickpar/repair.php Guide to repair files using PAR2]
* [https://web.archive.org/web/20100912073937/http://chuchusoft.com/par2_tbb/ par2+tbb]
* [http://www.milow.net/public/projects/parnrar/parnrar.html Par-N-Rar]
* [http://www.irasnyder.com/devel/#rarslave Rarslave]

[[Category:Archive formats]]
[[Category:Data management]]
[[Category:Usenet]]
<=====doc_Id=====>:118
<=====title=====>:
Control flow diagram
<=====text=====>:
{{this|flow diagrams in business process modeling{{clarify|reason=From the lead, I'm unable to give a more informative characterization of 'Control flow diagram'.|date=January 2014}}|directed graphs representing the control flow of imperative computer programs|control flow graph}}
[[File:Performance seeking control flow diagram.jpg|thumb|240px|Example of a "performance seeking" control flow diagram.<ref name="GO92"> Glenn B. Gilyard and John S. Orme (1992) [http://www.nasa.gov/centers/dryden/pdf/88262main_H-1808.pdf ''Subsonic Flight Test Evaluationof a Performance Seeking ControlAlgorithm on an F-15 Airplane''] NASA Technical Memorandum 4400.</ref>]]
A '''control flow diagram''' ('''CFD''') is a [[diagram]] to describe the [[control flow]] of a [[business process]], [[process (engineering)|process]] or review

Control flow diagrams were developed in the 1950s, and are widely used in multiple [[engineering]] disciplines. They are one of the classic [[business process modeling]] methodologies, along with [[flow chart]]s, [[data flow diagram]]s, [[functional flow block diagram]], [[Gantt chart]]s, [[PERT]] diagrams, and [[IDEF]].<ref name="TD03"> Thomas Dufresne & James Martin (2003). [http://mason.gmu.edu/~tdufresn/paper.doc "Process Modeling for E-Business"]. INFS 770 Methods for Information Systems Engineering:  Knowledge Management and E-Business. Spring 2003</ref>

== Overview ==
A control flow diagram can consist of a subdivision to show sequential steps, with if-then-else conditions, repetition, and/or case conditions. Suitably annotated geometrical figures are used to represent operations, data, or equipment, and arrows are used to indicate the sequential flow from one to another.<ref>[http://www.fda.gov/ora/Inspect_ref/igs/gloss.html FDA glossary of terminology applicable to software development and computerized systems]. Accessed 14 Jan 2008.</ref>

There are several types of control flow diagrams, for example:
* Change control flow diagram, used in [[project management]]
* Configuration decision control flow diagram, used in [[configuration management]]
* [[Process control]] flow diagram, used in [[process management]]  
* Quality control flow diagram, used in [[quality control]].

In software and systems development control flow diagrams can be used in [[control flow analysis]], [[data flow analysis]], [[algorithm analysis]], and [[simulation]]. Control and data are most applicable for real time and data driven systems. These flow analyses transform logic and data requirements text into graphic flows which are easier to analyze than the text. PERT, state transition, and transaction diagrams are examples of control flow diagrams.<ref>Dolores R. Wallace et al. (1996). [http://hissa.nist.gov/HHRFdata/Artifacts/ITLdoc/234/val-proc.html ''Reference Information for the Software Verification and Validation Process''], NIST Special Publication 500-234.</ref>

== Types of Control Flow Diagrams ==
=== Process Control Flow Diagram ===
A flow diagram can be developed for the process [[control system]] for each critical activity. Process control is normally a closed cycle in which a [[sensor]] provides information to a process control [[software application]] through a [[communications system]]. The application determines if the sensor information is within the predetermined (or calculated) data parameters and constraints. The results of this comparison are fed to an actuator, which controls the critical component. This [[feedback]] may control the component electronically or may indicate the need for a manual action.<ref name="NIoJ02"> National Institute of Justice (2002). [http://www.ncjrs.gov/txtfiles1/nij/195171.txt '' A Method to Assess the Vulnerability of U.S. Chemical Facilities]''. Series: Special Report.</ref> 

This closed-cycle process has many checks and balances to ensure that it stays safe. The investigation of how the process control can be subverted is likely to be extensive because all or part of the process control may be
oral instructions to an individual monitoring the process. It may be fully computer controlled and automated, or it may be a hybrid in which only the sensor is automated and the action requires manual intervention. Further, some process control systems may use prior generations of hardware and software, while others are state of the art.<ref name="NIoJ02"/>

=== Performance seeking control flow diagram ===
The figure presents an example of a performance seeking control [[flow diagram]] of the algorithm. The control law consists of estimation, modeling, and optimization processes. In the [[Kalman filter]] estimator, the inputs, outputs, and residuals were recorded. At the compact propulsion system modeling stage, all the estimated inlet and engine parameters were recorded.<ref name="GO92"/>  

In addition to temperatures, pressures, and control positions, such estimated parameters as stall margins, thrust, and drag components were recorded. In the optimization phase, the operating condition constraints, optimal solution, and linear programming health status condition codes were recorded. Finally, the actual commands that were sent to the engine through the DEEC were recorded.<ref name="GO92"/>

== See also ==
* [[Data flow diagram]]
* [[Control flow graph]]
* [[DRAKON]]
* [[Flow process chart]]

== References ==
{{NIST-PD}}
{{reflist}}

[[Category:Information systems]]
[[Category:Data management]]
[[Category:Diagrams]]
[[Category:Systems analysis]]
{{DEFAULTSORT:Control Flow Diagram}}
<=====doc_Id=====>:121
<=====title=====>:
Data aggregation
<=====text=====>:
'''Data aggregation''' is the compiling of [[information]] from [[databases]] with intent to prepare combined datasets for [[data processing]].<ref>{{cite journal|author1=Stanley, Jay  |author2=Steinhardt, Barry|title=Bigger Monster, Weaker Chains: The Growth of an American Surveillance Society|publisher=American Civil Liberties Union|date=January 2003}}</ref>

==Description==
The source information for data aggregation may originate from public records and [[criminal]] databases. The information is packaged into aggregate reports and then sold to [[business]]es, as well as to [[Local government|local]], [[State government|state]], and government agencies. This information can also be useful for [[marketing]] purposes. In the United States, many data brokers' activities fall under the [[Fair Credit Reporting Act]] (FCRA) which regulates [[Credit bureau|consumer reporting agencies]]. The agencies then gather and package personal information into [[consumer]] reports that are sold to [[creditor]]s, [[employer]]s, [[insurer]]s, and other businesses.

Various reports of information are provided by database aggregators. Individuals may request their own consumer reports which contain basic [[biographical]] information such as name, date of birth, current address, and phone number. Employee [[background check]] reports, which contain highly detailed information such as past addresses and length of residence, [[professional]] [[Licensure|licenses]], and criminal history, may be requested by eligible and qualified third parties. Not only can this data be used in employee background checks, but it may also be used to make decisions about insurance coverage, pricing, and law enforcement. [[Privacy]] activists argue that database aggregators can provide erroneous information.<ref>{{cite web|url=http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |title=Data Aggregators: A Study of Data Quality and Responsiveness |author1=Pierce, Deborah |author2=Ackerman, Linda |publisher=Privacyactivism.org |date=2005-05-19 |accessdate=2007-04-02 |archiveurl=https://web.archive.org/web/20070319220412/http://www.privacyactivism.org/docs/DataAggregatorsStudy.html |archivedate=2007-03-19 |deadurl=yes |df= }}</ref>

==Role of the Internet==
The potential of the [[Internet]] to consolidate and manipulate information has a new application in data aggregation, also known as ''screen scraping''. The Internet gives users the opportunity to consolidate their [[username]]s and [[password]]s, or PINs. Such consolidation enables consumers to access a wide variety of PIN-protected [[website]]s containing personal information by using one master PIN on a single website. Online account providers include [[financial institution]]s, [[stockbroker]]s, [[airline]] and frequent flyer and other reward programs, and [[e-mail]] accounts. Data aggregators can gather account or other information from designated websites by using account holders' PINs, and then making the users' account information available to them at a single website operated by the aggregator at an account holder's request. Aggregation services may be offered on a standalone basis or in conjunction with other financial services, such as [[portfolio (finance)|portfolio]] tracking and [[Bill (payment)|bill]] payment provided by a specialized website, or as an additional service to augment the online presence of an enterprise established beyond the virtual world. Many established companies with an Internet presence appear to recognize the value of offering an aggregation service to enhance other web-based services and attract visitors. Offering a data aggregation service to a website may be attractive because of the potential that it will frequently draw users of the service to the hosting website.

==Local business data aggregation==
When it comes to compiling location information on local businesses, there are several major data aggregators that collect information such as the business name, address, phone number, website, description and hours of operation. They then validate this information using various validation methods. Once the business information has been verified to be accurate, the data aggregators make it available to publishers like [[Google]] and [[Yelp]].

When Yelp, for example, goes to update their Yelp listings, they will pull data from these local data aggregators. Publishers take local business data from different sources and compare it to what they currently have in their database. They then update their database it with what information they deem accurate.

==Legal implications==
Financial institutions are concerned about the possibility of [[legal liability|liability]] arising from data aggregation activities, potential [[security]] problems, infringement on [[intellectual property]] rights and the possibility of diminishing traffic to the institution's website. The aggregator and financial institution may agree on a data feed arrangement activated on the customer's request, using an Open Financial Exchange (OFX) standard to request and deliver information to the site selected by the customer as the place from which they will view their account data. Agreements provide an opportunity for institutions to negotiate to protect their customers' interests and offer aggregators the opportunity to provide a robust service. Aggregators who agree with information providers to extract data without using an OFX standard may reach a lower level of consensual relationship; therefore, "screen scraping" may be used to obtain account data, but for business or other reasons, the aggregator may decide to obtain prior consent and negotiate the terms on which customer data is made available. "Screen scraping" without consent by the content provider has the advantage of allowing subscribers to view almost any and all accounts they happen to have opened anywhere on the Internet through one website.

==Outlook==
Over time, the transfer of large amounts of account data from the account provider to the aggregator's server could develop into a comprehensive profile of a user, detailing their banking and [[credit card]] transactions, balances, securities transactions and portfolios, and [[travel]] history and preferences. As the sensitivity to data protection considerations grows, it is likely there will be a considerable focus on the extent to which data aggregators may seek to use this data either for their own purposes or to share it on some basis with the operator of a website on which the service is offered or with other third parties.<ref>{{cite web|url=http://www.ffhsj.com/bancmail/bmarts/aba_art.htm|title=Scrape It, Scrub It and Show It: The Battle Over Data Aggregation|author1=Ledig, Robert H.  |author2=Vartanian, Thomas P.|publisher=Fried Frank|date=2002-09-11|accessdate=2007-04-02}}</ref>

==References==
<references />

{{DEFAULTSORT:Data Aggregator}}
[[Category:Data management]]
[[Category:Information privacy]]
[[Category:Data laws]]
<=====doc_Id=====>:124
<=====title=====>:
Two-phase commit protocol
<=====text=====>:
{{Redirect|2PC|the play in American and Canadian football|Two-point conversion|the cryptographic protocol|Commitment scheme}}

In [[transaction processing]], [[database]]s, and [[computer networking]], the '''two-phase commit protocol''' ('''2PC''') is a type of [[Atomic commit|atomic commitment protocol]] (ACP). It is a [[distributed algorithm]] that coordinates all the processes that participate in a [[Distributed transaction|distributed atomic transaction]] on whether to ''[[Commit (data management)|commit]]'' or ''abort'' (''roll back'') the transaction (it is a specialized type of [[Consensus (computer science)|consensus]] protocol). The protocol achieves its goal even in many cases of temporary system failure (involving either process, network node, communication, etc. failures), and is thus widely used.<ref name="bernstein1987">[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman (1987): [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Chapter 7, Addison Wesley Publishing Company, ISBN 0-201-10715-5</ref><ref name="weikum2001">[[Gerhard Weikum]], Gottfried Vossen (2001): [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Chapter 19, Elsevier, ISBN 1-55860-508-8</ref><ref name=Bern2009>Philip A. Bernstein, Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition], Chapter 8, Morgan Kaufmann (Elsevier), ISBN 978-1-55860-623-4</ref>
However, it is not resilient to all possible failure configurations, and in rare cases, user (e.g., a system's administrator) intervention is needed to remedy an outcome. To accommodate recovery from failure (automatic in most cases) the protocol's participants use [[Server log|logging]] of the protocol's states. Log records, which are typically slow to generate but survive failures, are used by the protocol's [[recovery procedure]]s. Many protocol variants exist that primarily differ in logging strategies and recovery mechanisms. Though usually intended to be used infrequently, recovery procedures compose a substantial portion of the protocol, due to many possible failure scenarios to be considered and supported by the protocol.

In a "normal execution" of any single [[distributed transaction]] ( i.e., when no failure occurs, which is typically the most frequent situation), the protocol consists of two phases:
#The ''commit-request phase'' (or ''voting phase''), in which a ''coordinator'' process attempts to prepare all the transaction's participating processes (named ''participants'', ''cohorts'', or ''workers'') to take the necessary steps for either committing or aborting the transaction and to ''vote'', either "Yes": commit (if the transaction participant's local portion execution has ended properly), or "No": abort (if a problem has been detected with the local portion), and
#The ''commit phase'', in which, based on ''voting'' of the cohorts, the coordinator decides whether to commit (only if ''all'' have voted "Yes") or abort the transaction (otherwise), and notifies the result to all the cohorts. The cohorts then follow with the needed actions (commit or abort) with their local transactional resources (also called ''recoverable resources''; e.g., database data) and their respective portions in the transaction's other output (if applicable).

Note that the two-phase commit (2PC) protocol should not be confused with the [[two-phase locking]] (2PL) protocol, a [[concurrency control]] protocol.

==Assumptions==
The protocol works in the following manner: one node is a designated '''coordinator''', which is the master site, and the rest of the nodes in the network are designated the '''cohorts'''. The protocol assumes that there is [[stable storage]] at each node with a [[Write ahead logging|write-ahead log]], that no node crashes forever, that the data in the write-ahead log is never lost or corrupted in a crash, and that any two nodes can communicate with each other. The last assumption is not too restrictive, as network communication can typically be rerouted. The first two assumptions are much stronger; if a node is totally destroyed then data can be lost.

The protocol is initiated by the coordinator after the last step of the transaction has been reached. The cohorts then respond with an '''agreement''' message or an '''abort''' message depending on whether the transaction has been processed successfully at the cohort.

==Basic algorithm==

===Commit request phase===
or '''voting phase'''

#The coordinator sends a '''query to commit''' message to all cohorts and waits until it has received a reply from all cohorts.
#The cohorts execute the transaction up to the point where they will be asked to commit.  They each write an entry to their ''undo log'' and an entry to their ''[[redo log]]''.
#Each cohort replies with an '''agreement''' message (cohort votes '''Yes''' to commit), if the cohort's actions succeeded, or an '''abort''' message (cohort votes '''No''', not to commit), if the cohort experiences a failure that will make it impossible to commit.

===Commit phase===
or '''Completion phase'''

====Success====
If the coordinator received an '''agreement''' message from ''all'' cohorts during the commit-request phase:
#The coordinator sends a '''commit''' message to all the cohorts.
#Each cohort completes the operation, and releases all the locks and resources held during the transaction.
#Each cohort sends an '''acknowledgment''' to the coordinator.
#The coordinator completes the transaction when all acknowledgments have been received.

====Failure====
If ''any'' cohort votes '''No''' during the commit-request phase (or the coordinator's timeout '''expires'''):
#The coordinator sends a '''rollback''' message to all the cohorts.
#Each cohort undoes the transaction using the undo log, and releases the resources and locks held during the transaction.
#Each cohort sends an '''acknowledgement''' to the coordinator.
#The coordinator undoes the transaction when all acknowledgements have been received.

====Message flow====
<pre>
Coordinator                                         Cohort
                              QUERY TO COMMIT
                -------------------------------->
                              VOTE YES/NO           prepare*/abort*
                <-------------------------------
commit*/abort*                COMMIT/ROLLBACK
                -------------------------------->
                              ACKNOWLEDGMENT        commit*/abort*
                <--------------------------------  
end
</pre>
An * next to the record type means that the record is forced to stable storage.<ref name="mohan1986">[[C. Mohan]], Bruce Lindsay and R. Obermarck (1986): [http://dl.acm.org/citation.cfm?id=7266  "Transaction management in the R* distributed database management system"],''ACM Transactions on Database Systems (TODS)'', Volume 11 Issue 4, Dec. 1986, Pages 378 - 396</ref>

==Disadvantages==
The greatest disadvantage of the two-phase commit protocol is that it is a blocking protocol. If the coordinator fails permanently, some cohorts will never resolve their transactions: After a cohort has sent an '''agreement''' message to the coordinator, it will block until a '''commit''' or '''rollback''' is received.

==Implementing the two-phase commit protocol==

===Common architecture===
In many cases the 2PC protocol is distributed in a computer network. It is easily distributed by implementing multiple dedicated 2PC components similar to each other, typically named ''[[Transaction manager]]s'' (TMs; also referred to as ''2PC agents'' or Transaction Processing Monitors), that carry out the protocol's execution for each transaction (e.g., [[The Open Group]]'s [[X/Open XA]]). The databases involved with a distributed transaction, the ''participants'', both the coordinator and cohorts, ''register'' to close TMs (typically residing on respective same network nodes as the participants) for terminating that transaction using 2PC. Each distributed transaction has an ad hoc set of TMs, the TMs to which the transaction participants register. A leader, the coordinator TM, exists for each transaction to coordinate 2PC for it, typically the TM of the coordinator database. However, the coordinator role can be transferred to another TM for performance or reliability reasons. Rather than exchanging 2PC messages among themselves, the participants exchange the messages with their respective TMs. The relevant TMs communicate among themselves to execute the 2PC protocol schema above, "representing" the respective participants, for terminating that transaction. With this architecture the protocol is fully distributed (does not need any central processing component or data structure), and scales up with number of network nodes (network size) effectively.

This common architecture is also effective for the distribution of other [[atomic commitment protocol]]s besides 2PC, since all such protocols use the same voting mechanism and outcome propagation to protocol participants.<ref name="bernstein1987" /><ref name="weikum2001" />

===Protocol optimizations===
[[Database]] research has been done on ways to get most of the benefits of the two-phase commit protocol while reducing costs by ''protocol optimizations''<ref name="bernstein1987" /><ref name="weikum2001" /><ref name="Bern2009" /> and protocol operations saving under certain system's behavior assumptions.

====Presumed Abort and Presumed Commit====
''Presumed abort'' or ''Presumed commit'' are common such optimizations.<ref name="weikum2001" /><ref name=Bern2009/><ref name="mohan1983">[[C. Mohan]], Bruce Lindsay (1985): [http://portal.acm.org/citation.cfm?id=850772  "Efficient commit protocols for the tree of processes model of distributed transactions"],''ACM SIGOPS Operating Systems Review'',
19(2),pp. 40-52 (April 1985)</ref> An assumption about the outcome of transactions, either commit, or abort, can save both messages and logging operations by the participants during the 2PC protocol's execution. For example, when presumed abort, if during system recovery from failure no logged evidence for commit of some transaction is found by the recovery procedure, then it assumes that the transaction has been aborted, and acts accordingly. This means that it does not matter if aborts are logged at all, and such logging can be saved under this assumption. Typically a penalty of additional operations is paid during recovery from failure, depending on optimization type. Thus the best variant of optimization, if any, is chosen according to failure and transaction outcome statistics.

====Tree two-phase commit protocol====
The '''[[Tree (data structure)|Tree]] 2PC protocol'''<ref name="weikum2001" /> (also called ''Nested 2PC'', or ''Recursive 2PC'') is a common variant of 2PC in a [[computer network]], which better utilizes the underlying communication infrastructure. The participants in a distributed transaction are typically invoked in an order which defines a tree structure, the ''invocation tree'', where the participants are the nodes and the edges are the invocations (communication links). The same tree is commonly utilized to complete the transaction by a 2PC protocol, but also another communication tree can be utilized for this, in principle. In a tree 2PC the coordinator is considered the root ("top") of a communication tree (inverted tree), while the cohorts are the other nodes. The coordinator can be the node that originated the transaction (invoked recursively (transitively) the other participants), but also another node in the same tree can take the coordinator role instead. 2PC messages from the coordinator are propagated "down" the tree, while messages to the coordinator are "collected" by a cohort from all the cohorts below it, before it sends the appropriate message "up" the tree (except an '''abort''' message, which is propagated "up" immediately upon receiving it or if the current cohort initiates the abort).

The '''Dynamic two-phase commit''' (Dynamic two-phase commitment, D2PC) '''protocol'''<ref name="weikum2001" /><ref name="raz1995">[[Yoav Raz]] (1995): [http://www.springerlink.com/content/pv12p828kk616258/  "The Dynamic Two Phase Commitment (D2PC) protocol "],''Database Theory — ICDT '95'', ''Lecture Notes in Computer Science'', Volume 893/1995, pp. 162-176, Springer, ISBN 978-3-540-58907-5</ref> is a variant of Tree 2PC with no predetermined coordinator. It subsumes several optimizations that have been proposed earlier. '''Agreement''' messages ('''Yes''' votes) start to propagate from all the leaves, each leaf when completing its tasks on behalf of the transaction (becoming ''ready''). An intermediate (non leaf) node sends when ''ready'' an '''agreement''' message to the last (single) neighboring node from which '''agreement''' message has not yet been received. The coordinator is determined dynamically by racing '''agreement''' messages over the transaction tree, at the place where they collide. They collide either at a transaction tree node, to be the coordinator, or on a tree edge. In the latter case one of the two edge's nodes is elected as a coordinator (any node). D2PC is time optimal (among all the instances of a specific transaction tree, and any specific Tree 2PC protocol implementation; all instances have the same tree; each instance has a different node as coordinator): By choosing an optimal coordinator D2PC commits both the coordinator and each cohort in minimum possible time, allowing the earliest possible release of locked resources in each transaction participant (tree node).

==See also==
*[[Atomic commit]]
*[[Commit (data management)]]
*[[Three-phase commit protocol]]
*[[X/Open XA|XA]]
*[[Paxos algorithm]]
*[[Two Generals' Problem]]

==References==
{{Reflist}}

==External links==
*[http://exploredatabase.blogspot.in/2014/07/two-phase-commit-protocol-in-pictures.html Two Phase Commit protocol explained in Pictures] by exploreDatabase

{{DEFAULTSORT:Two-Phase Commit Protocol}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:127
<=====title=====>:
Data dictionary
<=====text=====>:
{{Distinguish|Dictionary (data structure)}}
{{Use dmy dates|date=July 2013}}
A '''data dictionary''', or [[metadata repository]], as defined in the ''IBM Dictionary of Computing'', is a "centralized repository of information about data such as meaning, relationships to other data, origin, usage, and format."<ref>ACM, [http://portal.acm.org/citation.cfm?id=541721 IBM Dictionary of Computing], 10th edition, 1993</ref> The term can have one of several closely related meanings pertaining to [[database]]s and [[database management system]]s (DBMS):

* A [[document]] describing a database or collection of databases
* An integral [[software component|component]] of a [[Database management system|DBMS]] that is required to determine its structure
* A piece of [[middleware]] that extends or supplants the native data dictionary of a DBMS

==Documentation==
The terms ''data dictionary'' and ''data repository'' indicate a more general software utility than a catalogue. A ''catalogue'' is closely coupled with the DBMS software. It provides the information stored in it to the user and the DBA, but it is mainly accessed by the various software modules of the DBMS itself, such as [[Data definition language|DDL]] and [[Data manipulation language|DML]] compilers, the query optimiser, the transaction processor, report generators, and the constraint enforcer. On the other hand, a ''data dictionary'' is a data structure that stores [[metadata]], i.e., (structured) data about information. The software package for a stand-alone data dictionary or data repository may interact with the software modules of the DBMS, but it is mainly used by the designers, users and administrators of a computer system for information resource management. These systems maintain information on system hardware and software configuration, documentation, application and users as well as other information relevant to system administration.<ref>Ramez Elmasri, Shamkant B. Navathe: ''Fundamentals of Database Systems'', 3rd. ed. sect. 17.5, p. 582</ref>

If a data dictionary system is used only by the designers, users, and administrators and not by the DBMS Software, it is called a ''passive data dictionary.'' Otherwise, it is called an ''active data dictionary'' or ''data dictionary.''  When a passive data dictionary is updated, it is done so manually and independently from any changes to a DBMS (database) structure. With an active data dictionary, the dictionary is updated first and changes occur in the DBMS automatically as a result.

Database [[User (computing)|users]] and [[Application software|application]] developers can benefit from an authoritative data dictionary document that catalogs the organization, contents, and conventions of one or more databases.<ref>TechTarget, ''SearchSOA'', [http://searchsoa.techtarget.com/sDefinition/0,,sid26_gci211896,00.html What is a data dictionary?]</ref> This typically includes the names and descriptions of various [[Table (database)|tables]] (records or Entities) and their contents ([[Column (database)|fields]]) plus additional details, like the [[Data type|type]] and length of each [[data element]].  Another important piece of information that a data dictionary can provide is the relationship between Tables.  This is sometimes referred to in Entity-Relationship diagrams, or if using Set descriptors, identifying which Sets database Tables participate in.

In an active data dictionary constraints may be placed upon the underlying data.  For instance, a Range may be imposed on the value of numeric data in a data element (field), or a Record in a Table may be FORCED to participate in a set relationship with another Record-Type.  Additionally, a distributed DBMS may have certain location specifics described within its active data dictionary (e.g. where Tables are physically located).

The data dictionary consists of record types (tables) created in the database by systems generated command files, tailored for each supported back-end DBMS. Command files contain SQL Statements for CREATE TABLE, CREATE UNIQUE INDEX, ALTER TABLE (for referential integrity), etc., using the specific statement required by that type of database.

There is no universal standard as to the level of detail in such a document.

==Middleware==
In the construction of database applications, it can be useful to introduce an additional layer of data dictionary software, i.e. [[middleware]], which communicates with the underlying DBMS data dictionary. Such a "high-level" data dictionary may offer additional features and a degree of flexibility that goes beyond the limitations of the native "low-level" data dictionary, whose primary purpose is to support the basic functions of the DBMS, not the requirements of a typical application. For example, a high-level data dictionary can provide alternative [[entity-relationship model]]s tailored to suit different applications that share a common database.<ref>U.S. Patent 4774661, [http://www.freepatentsonline.com/4774661.html Database management system with active data dictionary], 19 November 1985, AT&T</ref> Extensions to the data dictionary also can assist in [[query optimization]] against [[distributed database]]s.<ref>U.S. Patent 4769772, [http://www.freepatentsonline.com/4769772.html Automated query optimization method using both global and parallel local optimizations for materialization access planning for distributed databases], 28 February 1985, Honeywell Bull</ref>  Additionally, DBA functions are often automated using restructuring tools that are tightly coupled to an active data dictionary.

[[Software framework]]s aimed at [[rapid application development]] sometimes include high-level data dictionary facilities, which can substantially reduce the amount of programming required to build [[Menu (computing)|menus]], [[Form (programming)|forms]], reports, and other components of a database application, including the database itself. For example, PHPLens includes a [[PHP]] [[class library]] to automate the creation of tables, indexes, and [[foreign key]] constraints [[Portability (software)|portably]] for multiple databases.<ref>PHPLens, [http://phplens.com/lens/adodb/docs-datadict.htm ADOdb Data Dictionary Library for PHP]</ref> Another PHP-based data dictionary, part of the RADICORE toolkit, automatically generates program [[Object (computer science)|objects]], [[Scripting language|scripts]], and SQL code for menus and forms with [[data validation]] and complex [[join (SQL)|joins]].<ref>RADICORE, [http://www.radicore.org/viewarticle.php?article_id=5 What is a Data Dictionary?]</ref> For the [[ASP.NET]] environment, [[Base One International|Base One's]] data dictionary provides cross-DBMS facilities for automated database creation, data validation, performance enhancement ([[Cache (computing)|caching]] and index utilization), [[application security]], and extended [[data type]]s.<ref>Base One International Corp., [http://www.boic.com/b1ddic.htm Base One Data Dictionary]</ref>  [[Visual DataFlex]] features<ref>VISUAL DATAFLEX,[http://www.visualdataflex.com/features.asp?pageid=1030 features]</ref> provides the ability to use DataDictionaries as class files to form  middle layer between the user interface and the underlying database.   The intent is to create standardized rules to maintain data integrity and enforce business rules throughout one or more related applications.

==Platform-specific examples==
Developers use a ''data description specification'' (''DDS'') to describe data attributes in file descriptions that are external to the application program that processes the data, in the context of an [[IBM System i]].<ref>{{cite web |url=http://publib.boulder.ibm.com/infocenter/iseries/v5r3/topic/dds/rbafpddsmain.htm |title=DDS documentation for IBM System i V5R3}}</ref>

==See also==
*[[Data hierarchy]]
*[[Data modeling]]
*[[Database schema]]
*[[ISO/IEC 11179]]
*[[Metadata registry]]
*[[Semantic spectrum]]
*[[Vocabulary OneSource]]
*[[Metadata repository]]

==References==
{{Reflist|30em}}

==External links==
{{Commons category|Data dictionary}}
*Yourdon, ''Structured Analysis Wiki'', [http://yourdon.com/strucanalysis/wiki/index.php?title=Chapter_10 Data Dictionaries]

{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Data Dictionary}}
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
<=====doc_Id=====>:130
<=====title=====>:
Hybrid array
<=====text=====>:
A '''hybrid array''' is a form of [[hierarchical storage management]] that combines [[hard disk drive]]s (HDDs) with [[solid-state drive]]s (SSDs) for [[I/O]] speed improvements.

Hybrid storage arrays aim to mitigate the ever increasing price-performance gap between HDDs and [[DRAM]] by adding a non-volatile flash level to the [[memory hierarchy]].<ref name="MicheloniMarelli2012">{{cite book|author1=Rino Micheloni|author2=Alessia Marelli|author3=Kam Eshghi|title=Inside Solid State Drives (SSDs)|url=https://books.google.com/books?id=S8xRtkF7hUkC&pg=PA62|year=2012|publisher=Springer|isbn=978-94-007-5145-3|page=62}}</ref> Hybrid arrays thus aim to lower the cost per I/O, compared to using only SSDs for storage.  Hybrid architectures can be as simple as involving a single SSD [[Cache (computing)|cache]] for desktop or laptop computers, or can be more complex as configurations for [[data center]]s and [[cloud computing]].

== Implementations ==
<!-- please only add products covered in [[WP:SECONDARY]] sources at adequate depth -->
Some commercial products for building hybrid arrays include:
* [[Adaptec]] demonstrated the MaxIQ series in 2009.<ref>{{cite web |author=Charlie Demerjian |url= http://semiaccurate.com/2009/09/09/adaptecs-maxiq-caches-raids-ssds/ |title= Adaptec's MaxIQ caches RAIDs with SSDs |publisher= SemiAccurate |date= September 9, 2009 |accessdate= October 10, 2016 }}</ref>
* Apple's [[Fusion Drive]]
*  [[Linux]] software includes [[bcache]], [[dm-cache]], and [[Flashcache]] (and its fork EnhanceIO).
* Condusive's [[ExpressCache]] is marketed for laptops.
* [[EMC Corporation]] VFcache was announced in 2012.<ref>{{cite web |last= Larry Dignan |url= http://www.zdnet.com/blog/btl/emc-unveils-vfcache-targets-fusion-io/68657 |title=EMC unveils VFCache, targets Fusion-io |publisher= ZDNet |work= Between the Lines |date= February 5, 2012 |accessdate= October 10, 2016 }}</ref><ref>{{Cite news |title= One day later: EMC declares war on all-flash array, server flash card rivals: Rolls out XtremIO array, renamed VFCache |date= March 5, 2013 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2013/03/05/emc_xtremsf/ |accessdate= October 10, 2016 }}</ref>
* [[Fusion-io]] acquired ioTurbine in 2011,<ref name="io">{{cite web |url= http://www.theregister.co.uk/2013/06/25/fusionio_spins_ioturbine_faster/ |title=Fusion-io spins up ioTurbine, enhances server flash caching |work= The Register |accessdate= October 10, 2016 }}</ref> and the product line it acquired by buying NexGen in 2013.<ref>{{cite web|url=http://www.theregister.co.uk/2013/04/24/fusion_io_nexgen/|title=Fusion-io buys NexGen|work=theregister.co.uk |accessdate=2015-03-26}}</ref>
* [[Hitachi]] Accelerated Flash Storage (HAFS) used together with the Hitachi Dynamic Tiering software<ref>{{citation |url=http://www.computerweekly.com/feature/Big-storage-turns-the-tide-in-the-hybrid-flash-array-market |title=Big storage turns the tide in the hybrid flash array market |work=[[Computer Weekly]] |date=September 2013 |accessdate=2015-03-26}}</ref>
* [[IBM]] Flash Cache Storage Accelerator (FCSA) server software<ref>{{cite web|author=The SSD Guy |url=http://thessdguy.com/ibm-adds-server-side-caching/ |title=IBM Adds Server-Side Caching |publisher=The SSD Guy |date=2013-08-20 |accessdate=2013-12-23}}</ref>
* Intel's [[Smart Response Technology]] for desktop
* Intel's [[Cache Acceleration Software]] for servers and workstations 
* [[LSI Corporation|LSI]] CacheCade software for their controllers<ref>{{cite web|url=http://www.storagereview.com/lsi_megaraid_cachecade_pro_20_review|title=LSI MegaRAID CacheCade Pro 2.0 Review |accessdate=2015-03-26 |work=storagereview.com}}</ref>
* [[Marvell Technology Group|Marvell]]'s HyperDuo controllers<ref>{{cite web|url=http://www.cnet.com/8301-32254_1-20027657-283.html|title=Hands-on with the Marvell HyperDuo hybrid storage controller |accessdate=2015-03-26 |publisher=CBS Interactive|work=CNET}}</ref>
* Microsoft's [[Automated Tiering]] (since Windows 2012 R2)
* [[NetApp]]'s Flash Cache, Flash Pool, Flash Accel<ref>{{cite web|url=http://www.theregister.co.uk/2012/08/21/netapp_server_flash/|title=NetApp: Flash as a STORAGE tier? You must be joking |accessdate=2015-03-26 |work=theregister.co.uk}}</ref>
* [[Oracle Corporation]] markets products such as [[Exadata]] Smart Cache Flash, and the FS1 flash storage system.<ref>{{Cite news |title= Oracle crashes all-flash bash: Behold, our hybrid FS1 arrays: Mutant flash/disk box a pillar of storage: It's axiomatic |date= September 30, 2014 |work= The Register |author= Chris Mellor |url= http://www.theregister.co.uk/2014/09/30/the_fs1_pillar_of_oracle_arrays_storage/ |accessdate= October 10, 2016 }}</ref>
* Microsoft [[ReadyBoost]] allows personal computers to [[USB flash drive]]s as cache.
* Nvelo DataPlex SSD caching software was announced in 2011,<ref>{{cite web |url= http://www.thessdreview.com/our-reviews/nvelo-dataplex-ssd-caching-software-review-seven-msata-ssds-prove-an-amazing-concept/  |title= NVELO Dataplex SSD Caching Software Review - Seven mSATA SSDs Prove An Amazing Concept |work= The SSD Review |date= December 4, 2011 |author= Les Tokar |accessdate= October 10, 2016 }}</ref> and was acquired by [[Samsung]] in 2012.<ref>{{cite web |url= http://www.anandtech.com/show/6518/samsung-acquires-ssd-caching-company-nvelo |title=Samsung Acquires SSD Caching Company NVELO |publisher=AnandTech |author= Kristian Vättö |date= December 16, 2012 |accessdate= October 10, 2016 }}</ref>
* [[SanDisk]] FlashSoft for Windows, Linux, and [[vSphere]]<ref name="io"/>
* Products are offered by vendors like AMI [[StorTrends]],<ref>{{cite web|author=Ian Barker |url=http://betanews.com/2014/01/27/ami-stortrends-3500i-offers-high-performance-storage-for-smaller-enterprises/ |title=AMI StorTrends 3500i offers high performance storage for smaller enterprises |publisher=BetaNews |date=2014-01-27 |accessdate=2014-10-17}}</ref> [[Tegile Systems]], [[Reduxio]], and [[Tintri]].<ref>{{cite web|url=http://www.theregister.co.uk/2013/04/09/blind_spot/ |accessdate=2015-03-26 |title=Mutant array upstarts feast on EMC, NetApp's leavings|work= The Register }}</ref>
* [[ZFS]] using hybrid storage pools, are used for example in some Oracle Corporation products.<ref>{{cite web|url=http://www.enterprisestorageforum.com/san-nas-storage/oracles-flash-friendly-sun-zfs-storage-is-ready-for-new-sparcs.html|title=Oracle's Flash-Friendly Sun ZFS Storage Is Ready for New SPARCs|date=3 April 2013 |accessdate=2015-03-26 |work=enterprisestorageforum.com}}</ref>

== See also ==
* [[Hybrid drive]]{{spaced ndash}} built-in flash cache, handled by firmware
* [[Automated tiered storage]]{{spaced ndash}} another name for hierarchical storage management
* The "[[five-minute rule]]" for caching

== References ==
{{Reflist|30em}}

[[Category:Data management]]
[[Category:Solid-state caching]]
[[Category:Memory management software]]
<=====doc_Id=====>:133
<=====title=====>:
Data extraction
<=====text=====>:
'''Data extraction''' is the act or process of retrieving [[data]] out of (usually [[unstructured data|unstructured]] or poorly structured) data sources for further [[data processing]] or [[data storage device|data storage]] ([[data migration]]). The [[data import|import]] into the intermediate extracting system is thus usually followed by [[data transformation]] and possibly the addition of [[metadata]] prior to [[data export|export]] to another stage in the data [[workflow]].<ref>[http://www.extractingdata.com Definition of data extraction.]</ref>

Usually, the term data extraction is applied when ([[experiment]]al) data is first imported into a computer from primary sources, like [[measuring device|measuring]] or [[recording device]]s. Today's [[electronic device]]s will usually present an [[electrical connector]] (e.g. [[USB]]) through which '[[raw data]]' can be [[data stream|streamed]] into a [[personal computer]].

Typical unstructured data sources include web pages, emails, documents, PDFs, scanned text, mainframe reports, spool files, classifieds, etc. Which is further used for sales / marketing leads.<ref>[http://www.suntecdata.com/data-extraction-services.html Data Extraction Services] Retrieved, April 4, 2016</ref>  Extracting data from these unstructured sources has grown into a considerable technical challenge where as historically data extraction has had to deal with changes in physical hardware formats, the majority of current data extraction deals with extracting data from these unstructured data sources, and from different software formats.  This growing process of data extraction<ref>[http://www.loginworks.com/blogs/web-scraping-blogs/209-web-data-extraction/ data extraction.]</ref> from the web is referred to as [[Web scraping]].

The act of adding structure to unstructured data takes a number of forms
* Using text pattern matching such as [[regular expression]]s to identify small or large-scale structure e.g. records in a report and their associated data from headers and footers; 
* Using a table-based approach to identify common sections within a limited domain e.g. in emailed resumes, identifying skills, previous work experience, qualifications etc. using a standard set of commonly used headings (these would differ from language to language), e.g. Education might be found under Education/Qualification/Courses;
* Using text analytics to attempt to understand the text and link it to other information

==References==
{{Reflist}}

==External links==
* [http://www.etltools.org/extraction.html Data Extraction] as a part of the ETL process in a Data Warehousing environment

{{Data warehouse}}

{{DEFAULTSORT:Data Extraction}}
[[Category:Data management]]
[[Category:Data warehousing]]
<=====doc_Id=====>:136
<=====title=====>:
Operational system
<=====text=====>:
{{distinguish|Operating system}}
An '''operational system''' is a term used in [[data warehousing]] to refer to a system that is used to process the day-to-day transactions of an organization. These systems are designed in a manner that processing of day-to-day transactions is performed efficiently and the integrity of the transactional data is preserved.
== Synonyms ==
Sometimes operational systems are referred to as [[operational database]]s, [[transaction processing system]]s, or [[online transaction processing]] systems (OLTP). However, the use of the last two terms as synonyms may be confusing, because operational systems can be [[batch processing]] systems as well.

Any enterprise must necessarily maintain a lot of data about its operation. This is its "operational data".

{| class="wikitable" border="1"
|-
! Organization
! Probably
|-
| Manufacturing Company 
| Product data
|-
| Bank
| Account Data
|-
| Hospital
| Patient Data
|-
| University
| Student Data
|-
| Government Department
| Planning data
|}

==See also==
* [[Operating system]] (OS)
* [[Data warehouse#Data warehouses versus operational systems|Data warehouses versus operational systems]]

{{database-stub}}

{{DEFAULTSORT:Operational System}}
[[Category:Data warehousing]]
[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Business intelligence]]
<=====doc_Id=====>:139
<=====title=====>:
Disaster recovery
<=====text=====>:
{{About|1=[[business continuity planning]]|2=societal disaster recovery|3=emergency management}}
{{Other uses|DR (disambiguation)}}

{{merge to|business continuity|date=June 2015}}

'''Disaster recovery''' (DR) involves a set of policies and procedures to enable the recovery or continuation of vital technology infrastructure and systems following a [[natural disaster|natural]] or [[man-made hazards|human-induced]] [[disaster]]. Disaster recovery focuses on the IT or [[technology systems]] supporting critical business functions,<ref>[http://continuity.georgetown.edu/dr/ ''Systems and Operations Continuity: Disaster Recovery.''] Georgetown University. University Information Services. Retrieved 3 August 2012.</ref> as opposed to [[business continuity]], which involves keeping all essential aspects of a business functioning despite significant disruptive events.  Disaster recovery is therefore a subset of business continuity.<ref>[http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&expand=true&lc=en ''Disaster Recovery and Business Continuity, version 2011.''] {{webarchive |url=https://web.archive.org/web/20130111203921/http://www-304.ibm.com/partnerworld/gsd/solutiondetails.do?solution=44832&expand=true&lc=en |date=January 11, 2013 }} IBM. Retrieved 3 August 2012.</ref>

==History==
Disaster recovery developed in the mid- to late 1970s as computer center managers began to recognize the dependence of their organizations on their computer systems. At that time, most systems were [[Batch processing|batch]]-oriented [[Mainframe computer|mainframe]]s which in many cases could be [[downtime|down]] for a number of days before significant damage would be done to the organization.

As awareness of the potential business disruption that would follow an IT-related disaster, the disaster recovery industry developed to provide backup computer centers, with Sun Information Systems (which later became Sungard Availability Services) becoming the first major US commercial hot site vendor, established in 1978 in Philadelphia.

During the 1980s and 90s, customer awareness and industry both grew rapidly, driven by the advent of open systems and [[Real-time computing|real-time processing]] which increased the dependence of organizations on their IT systems. Regulations mandating business continuity and disaster recovery plans for organizations in various sectors of the economy, imposed by the authorities and by business partners, increased the demand and led to the availability of commercial disaster recovery services, including mobile data centers delivered to a suitable recovery location by truck.

With the rapid growth of the [[Internet]] through the late 1990s and into the 2000s, organizations of all sizes became further dependent on the continuous [[availability]] of their IT systems, with some organizations setting objectives of 2, 3, 4 or 5 nines (99.999%) availability of critical systems.{{citation needed|date=March 2016}} This increasing dependence on IT systems, as well as increased awareness from large-scale disasters such as tsunami, earthquake, flood, and volcanic eruption, spawned disaster recovery-related products and services, ranging from [[high-availability]] solutions to [[hot-site]] facilities.  Improved networking meant critical IT services could be served remotely, hence on-site recovery became less important.

The rise of cloud computing since 2010 continues that trend: nowadays, it matters even less where computing services are physically served, just so long as the network itself is sufficiently reliable (a separate issue, and less of a concern since modern networks are highly resilient by design).  'Recovery as a Service' (RaaS) is one of the security features or benefits of cloud computing being promoted by the Cloud Security Alliance.<ref>[https://cloudsecurityalliance.org/download/secaas-category-9-bcdr-implementation-guidance/ ''SecaaS Category 9 // BCDR Implementation Guidance''] CSA, retrieved 14 July 2014.</ref>

===Classification of disasters===
Disasters can be classified into two broad categories. The first is natural disasters such as floods, hurricanes, tornadoes or earthquakes. While preventing a natural disaster is impossible, risk management measures such as avoiding disaster-prone situations and good planning can help. The second category is man made disasters, such as hazardous material spills, infrastructure failure, bio-terrorism, and disastrous IT bugs or failed change implementations. In these instances, surveillance, testing and mitigation planning are invaluable.

==Importance of disaster recovery planning==
Recent research supports the idea that implementing a more holistic pre-disaster planning approach is more cost-effective in the long run. Every $1 spent on hazard mitigation(such as a [[disaster recovery plan]]) saves society $4 in response and recovery costs.<ref>{{cite web|first=Partnership for Disaster Resilience|title=Post-Disaster Recovery Planning Forum: How-To Guide|url=http://nthmp.tsunami.gov/Minutes/oct-nov07/post-disaster_recovery_planning_forum_uo-csc-2.pdf|publisher=University of Oregon's Community Service Center|accessdate=2013-05-23}}</ref>

As [[Information technology|IT systems]] have become increasingly critical to the smooth operation of a company, and arguably the economy as a whole, the importance of ensuring the continued operation of those systems, and their rapid recovery, has increased. For example, of companies that had a major loss of business data, 43% never reopen and 29% close within two years. As a result, preparation for continuation or recovery of systems needs to be taken very seriously. This involves a significant investment of time and money with the aim of ensuring minimal losses in the event of a disruptive event.<ref>{{cite web|url=http://www.ready.gov/business/implementation/IT|title=IT Disaster Recovery Plan|date=25 October 2012|publisher=FEMA|accessdate=11 May 2013}}</ref>

==Control measures==
Control measures are steps or mechanisms that can reduce or eliminate various threats for organizations. Different types of measures can be included in disaster recovery plan (DRP).

Disaster recovery planning is a subset of a larger process known as business continuity planning and includes planning for resumption of applications, data, hardware, electronic communications (such as networking) and other IT infrastructure. A business continuity plan (BCP) includes planning for non-IT related aspects such as key personnel, facilities, crisis communication and reputation protection, and should refer to the disaster recovery plan (DRP) for IT related infrastructure recovery / continuity.

IT disaster recovery control measures can be classified into the following three types:
# Preventive measures - Controls aimed at preventing an event from occurring.
# Detective measures - Controls aimed at detecting or discovering unwanted events.
# Corrective measures - Controls aimed at correcting or restoring the system after a disaster or an event.

Good disaster recovery plan measures dictate that these three types of controls be documented and exercised regularly using so-called "DR tests".

==Strategies==
Prior to selecting a disaster recovery strategy, a disaster recovery planner first refers to their organization's business continuity plan which should indicate the key metrics of [[recovery point objective]] (RPO) and [[recovery time objective]] (RTO) for various business processes (such as the process to run payroll, generate an order, etc.). The metrics specified for the business processes are then mapped to the underlying IT systems and infrastructure that support those processes.<ref>Gregory, Peter. CISA Certified Information Systems Auditor All-in-One Exam Guide, 2009. ISBN 978-0-07-148755-9. Page 480.</ref>

Incomplete RTOs and RPOs can quickly derail a disaster recovery plan. Every item in the DR plan requires a defined recovery point and time objective, as failure to create them may lead to significant problems that can extend the disaster’s impact.<ref>{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |title=Five Mistakes That Can Kill a Disaster Recovery Plan |publisher=Dell.com |accessdate=2012-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx |archivedate=2013-01-16 |df= }}</ref> Once the RTO and RPO metrics have been mapped to IT infrastructure, the DR planner can determine the most suitable recovery strategy for each system.  The organization ultimately sets the IT budget and therefore the RTO and RPO metrics need to fit with the available budget. While most business unit heads would like zero data loss and zero time loss, the cost associated with that level of protection may make the desired high availability solutions impractical. A [[cost-benefit analysis]] often dictates which disaster recovery measures are implemented.

Traditionally, a disaster recovery system involved cutover or switch-over recovery systems.{{citation needed|date=April 2016}} Such measures would allow an organization to preserve its technology and information, by having a remote disaster recovery location that produced backups on a regular basis. However, this strategy proved to be expensive and time-consuming. Therefore, more affordable and effective cloud-based systems were introduced.

Some of the most common strategies for [[Data recovery|data protection]] include: 
* backups made to tape and sent off-site at regular intervals
* backups made to disk on-site and automatically copied to off-site disk, or made directly to off-site disk
* replication of data to an off-site location, which overcomes the need to restore the data (only the systems then need to be restored or synchronized), often making use of [[storage area network]] (SAN) technology
* Private Cloud solutions which replicate the management data (VMs, Templates and disks) into the storage domains which are part of the private cloud setup. These management data are configured as an xml representation called OVF (Open Virtualization Format), and can be restored once a disaster occurs.
* Hybrid Cloud solutions that replicate both on-site and to off-site data centers.  These solutions provide the ability to instantly fail-over to local on-site hardware, but in the event of a physical disaster, servers can be brought up in the cloud data centers as well.
* the use of high availability systems which keep both the data and system replicated off-site, enabling continuous access to systems and data, even after a disaster (often associated with [[cloud storage]])<ref>{{cite web|url=http://www.inc.com/guides/201106/how-to-use-the-cloud-as-a-disaster-recovery-strategy.html|title=How to Use the Cloud as a Disaster Recovery Strategy|last=Brandon|first=John|date=23 June 2011|publisher=Inc. |accessdate=11 May 2013}}</ref>

In many cases, an organization may elect to use an outsourced disaster recovery provider to provide a stand-by site and systems rather than using their own remote facilities, increasingly via [[cloud computing]].

In addition to preparing for the need to recover systems, organizations also implement precautionary measures with the objective of preventing a disaster in the first place. These may include: 
* local mirrors of systems and/or data and use of disk protection technology such as [[RAID]]
* surge protectors — to minimize the effect of power surges on delicate electronic equipment
* use of an [[uninterruptible power supply]] (UPS) and/or backup generator to keep systems going in the event of a power failure
* fire prevention/mitigation systems such as alarms and fire extinguishers
* anti-virus software and other security measures

==See also==
* [[Backup site]]
* [[High availability]]
* [[Continuous data protection]]
* [[Data recovery]]
* [[Emergency management]]
* [[IT service continuity]]
* [[Remote backup service]]
* [[Seven tiers of disaster recovery]]
* [[Virtual tape library]]

==References==
{{reflist}}

==Further reading==
* ISO/IEC 22301:2012 (replacement of BS-25999:2007) Societal Security - Business Continuity Management Systems - Requirements
* ISO/IEC 27001:2013 (replacement of ISO/IEC 27001:2005 [formerly BS 7799-2:2002]) Information Security Management System
* ISO/IEC 27002:2013 (replacement of ISO/IEC 27002:2005 [renumbered ISO17799:2005]) Information Security Management - Code of Practice
* ISO/IEC 22399:2007 Guideline for incident preparedness and operational continuity management
* ISO/IEC 24762:2008 Guidelines for information and communications technology disaster recovery services
* IWA 5:2006 Emergency Preparedness—British Standards Institution --
* BS 25999-1:2006 Business Continuity Management Part 1: Code of practice
* BS 25999-2:2007 Business Continuity Management Part 2: Specification
* BS 25777:2008 Information and communications technology continuity management - Code of practice—Others --
* "A Guide to Business Continuity Planning" by James C. Barnes
* "Business Continuity Planning", A Step-by-Step Guide with Planning Forms on CDROM by Kenneth L Fulmer
* "Disaster Survival Planning: A Practical Guide for Businesses" by Judy Bell
* ICE Data Management (In Case of Emergency) made simple - by MyriadOptima.com
* Harney, J.(2004). Business continuity and disaster recovery: Back up or shut down.
* AIIM E-Doc Magazine, 18(4), 42-48.
* Dimattia, S. (November 15, 2001).Planning for Continuity. Library Journal,32-34.

==External links==
* [https://www.ready.gov/business/implementation/IT IT Disaster Recovery Plan from Ready.gov]
<!--========================({{No More Links}})============================
 | PLEASE BE CAUTIOUS IN ADDING MORE LINKS TO THIS ARTICLE. WIKIPEDIA |
 | IS NOT A COLLECTION OF LINKS NOR SHOULD IT BE USED FOR ADVERTISING. |
 | |
 | Excessive or inappropriate links WILL BE DELETED. |
 | See [[Wikipedia:External links]] & [[Wikipedia:Spam]] for details. |
 | |
 | If there are already plentiful links, please propose additions or |
 | replacements on this article's discussion page, or submit your link |
 | to the relevant category at the Open Directory Project (dmoz.org) |
 | and link back to that category using the {{dmoz}} template. |
 =======================({{No More Links}})=============================-->

{{Disasters}}

{{DEFAULTSORT:Disaster Recovery}}
[[Category:Disaster recovery]]
[[Category:Data management]]
[[Category:Backup]]
[[Category:IT risk management]]
<=====doc_Id=====>:142
<=====title=====>:
Database-centric architecture
<=====text=====>:
'''Database-centric Architecture''' or '''data-centric architecture''' has several distinct meanings, generally relating to [[software architecture]]s in which [[database]]s play a crucial role. Often this description is meant to contrast the design to an alternative approach. For example, the characterization of an architecture as "database-centric" may mean any combination of the following:

* using a standard, general-purpose [[relational database management system]], as opposed to customized in-[[Memory (computers)|memory]] or [[Computer file|file]]-based [[data structures]] and [[access method]]s. With the evolution of sophisticated [[Database management system|DBMS]] software, much of which is either free or included with the [[operating system]], application developers have become increasingly reliant on standard database tools, especially for the sake of [[rapid application development]].
* using dynamic, [[Table (database)|table]]-driven logic, as opposed to logic embodied in previously [[compiled]] [[Computer program|program]]s. The use of table-driven logic, i.e. behavior that is heavily dictated by the contents of a database, allows programs to be simpler and more flexible. This capability is a central feature of [[dynamic programming language]]s. See also [[control table]]s for tables that are normally coded and embedded within programs as [[data structures]] (i.e. not compiled statements) but could equally be read in from a [[flat file]], [[database]] or even retrieved from a [[spreadsheet]].
* using [[stored procedure]]s that run on [[database server]]s, as opposed to greater reliance on logic running in middle-tier [[application server]]s in a [[multi-tier architecture]]. The extent to which [[business logic]] should be placed at the back-end versus another tier is a subject of ongoing debate. For example, Toon Koppelaars presents a detailed analysis of alternative [[Oracle Database|Oracle-based]] architectures that vary in the placement of business logic, concluding that a database-centric approach has practical advantages from the standpoint of ease of development and maintainability.<ref>[https://web.archive.org/web/20060525094651/http://www.oracle.com/technology/pub/articles/odtug_award.pdf] A Database-centric approach to J2EE Application Development</ref>
* using a shared database as the basis for communicating between [[Parallel computing|parallel processes]] in [[distributed computing]] applications, as opposed to direct [[inter-process communication]] via [[message passing]] functions and [[message-oriented middleware]]. A potential benefit of database-centric architecture in [[distributed application]]s is that it simplifies the design by utilizing DBMS-provided [[transaction processing]] and [[Index (database)|indexing]] to achieve a high degree of reliability, performance, and capacity.<ref>{{Citation |author=Lind P, Alm M |title=A database-centric virtual chemistry system |journal=J Chem Inf Model |volume=46 |issue=3 |pages=1034–9 |year=2006 |pmid=16711722 |doi=10.1021/ci050360b |postscript=. }}</ref> For example, [[Base One]] describes a database-centric distributed computing architecture for [[Grid computing|grid]] and [[Computer cluster|cluster]] computing, and explains how this design provides enhanced security, fault-tolerance, and [[scalability]].<ref>[http://www.boic.com/dbgrid.htm Database-Centric Grid and Cluster Computing]</ref>
* an overall enterprise architecture that favors shared data models<ref>{{Cite news|url=http://tdan.com/the-data-centric-revolution/18780|title=The Data Centric Revolution|newspaper=TDAN.com|access-date=2017-01-09}}</ref> over allowing each application to have its own, idiosyncratic data model. 

==See also==
*[[Control table]]s
*[[:Category:Data-centric programming languages|Data-centric programming languages]]
*The [[data-driven programming]] paradigm, which makes the information used in a system the primary design driver.
*See the [http://datacentricmanifesto.org/ datacentricmanifesto.org] 

==References==
{{Reflist}}

{{Database}}

{{DEFAULTSORT:Database-Centric Architecture}}
[[Category:Software architecture]]
[[Category:Data management]]
[[Category:Distributed computing architecture]]
<=====doc_Id=====>:145
<=====title=====>:
Durability (database systems)
<=====text=====>:
{{Unreferenced|date=December 2009}}
In [[database system]]s, '''durability''' is the [[ACID]] property which guarantees that [[database transaction|transaction]]s that have committed will survive permanently. 
For example, if a flight booking reports that a seat has successfully been booked, then the seat will remain booked even if the system crashes.

Durability can be achieved by flushing the transaction's log records to [[non-volatile storage]] before acknowledging commitment.

In [[distributed transaction]]s, all participating servers must coordinate before commit can be acknowledged. This is usually done by a [[two-phase commit protocol]].

Many DBMSs implement durability by writing transactions into a [[transaction log]] that can be reprocessed to recreate the system state right before any later failure. A transaction is deemed committed only after it is entered in the log.

==See also==
* [[Atomicity (database systems)|Atomicity]]
* [[Consistency (database systems)|Consistency]]
* [[Isolation (database systems)|Isolation]]
* [[Relational database management system]]

{{DEFAULTSORT:Durability (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]


{{Compu-sci-stub}}
{{Database-stub}}
<=====doc_Id=====>:148
<=====title=====>:
Category:Open data
<=====text=====>:
{{cat main}}
[[Category:Open content]]
[[Category:Free software|Data]]
[[Category:Data management]]
<=====doc_Id=====>:151
<=====title=====>:
Virtual directory
<=====text=====>:
In [[computing]], the term '''virtual directory''' has a couple of meanings. It may simply designate (for example in [[Internet Information Services|IIS]]) a [[Folder (computing)|folder]] which appears in a [[Path (computing)|path]] but which is not actually a subfolder of the preceding folder in the path. However, this article will discuss the term in the context of [[directory service]]s and [[identity management]].

A virtual directory or '''virtual directory server''' in this context is a software layer that delivers a single access point for [[identity management]] applications and service platforms. A virtual directory operates as a high-performance, lightweight abstraction layer that resides between client applications and disparate types of identity-data repositories, such as proprietary and standard directories, databases, web services, and applications.

A virtual directory receives queries and directs them to the appropriate data sources by abstracting and virtualizing data. The virtual directory integrates identity data from multiple heterogeneous data stores and presents it as though it were coming from one source. This ability to reach into disparate repositories makes virtual directory technology ideal for consolidating data stored in a distributed environment. 

{{As of | 2011}}, virtual directory servers most commonly use the [[Lightweight Directory Access Protocol|LDAP]] protocol, but more sophisticated virtual directories can also support [[SQL]] as well as [[Directory Services Markup Language|DSML]] and [[Service Provisioning Markup Language|SPML]].

Industry experts have heralded the importance of the virtual directory in modernizing the identity infrastructure. According to Dave Kearns of Network World, "Virtualization is hot and a virtual directory is the building block, or foundation, you should be looking at for your next identity management project."<ref>{{cite web | url=http://www.networkworld.com/article/2305608/access-control/virtual-directory-finally-gains-recognition.html | title=Virtual directory finally gains recognition | publisher=NetworkWorld | date=7 August 2006 | accessdate=14 July 2014 | author=Kearns, Dave}}</ref> In addition, Gartner analyst, Bob Blakley<ref>The Emerging Architecture of Identity Management, Bob Blakley, April 16, 2010.</ref> said that virtual directories are playing an increasingly vital role. In his report, “The Emerging Architecture of Identity Management,” Blakley wrote: “In the first phase, production of identities will be separated from consumption of identities through the introduction of a virtual directory interface.”

==Capabilities==
Virtual directories can have some or all of the following capabilities:<ref>{{cite web|url=http://optimalidm.com/resources/blog/virtual-directory-server-2/|title=An Introduction To Virtual Directories|publisher=Optimal Idm|accessdate=15 July 2014}}</ref>
* Aggregate identity data across sources to create a single point of access.
* Create high-availability for authoritative data stores.
* Act as identity firewall by preventing [[denial-of-service attack]]s on the primary data stores through an additional virtual layer.
* Support a common searchable namespace for centralized authentication.
* Present a unified virtual view of user information stored across multiple systems.
* Delegate authentication to backend sources through source-specific security means.
* Virtualize data sources to support migration from legacy data stores without modifying the applications that rely on them.
* Enrich identities with attributes pulled from multiple data stores, based on a link between user entries. 

Some advanced identity virtualization platforms can also:
* Enable application-specific, customized views of identity data without violating internal or external regulations governing identity data. Reveal contextual relationships between objects through hierarchical directory structures.
* Develop advanced correlation across diverse sources using correlation rules. 
* Build a global user identity by correlating unique user accounts across various data stores, and enrich identities with attributes pulled from multiple data stores, based on a link between user entries. 
* Enable constant data refresh for real-time updates through a persistent cache.

==Advantages ==
Virtual directories:
* Enable faster deployment because users do not need to add and sync additional application-specific data sources 
* Leverage existing identity infrastructure and security investments to deploy new services 
* Deliver high availability of data sources 
* Provide application-specific views of identity data which can help avoid the need to develop a master enterprise schema
* Allow a single view of identity data without violating internal or external regulations governing identity data
* Act as identity firewalls by preventing denial-of-service attacks on the primary data-stores and providing further security on access to sensitive data
* Can reflect changes made to authoritative sources in real-time
* Present a unified virtual view of user information from multiple systems so that it appears to reside in a single system
* Can secure all backend storage locations with a single security policy

==Disadvantages==
An original disadvantage is public perception of "push & pull technologies" which is the general classification of "virtual directories" depending on the nature of their deployment. Virtual directories were initially designed and later deployed with "push technologies" in mind, which also contravened with [[privacy laws of the United States]]. This is no longer the case. There are, however, other disadvantages in the current technologies.

* The classical virtual directory based on proxy cannot modify underlying data structures or create new views based on the relationships of data from across multiple systems. So if an application requires a different structure, such as a flattened list of identities, or a deeper hierarchy for delegated administration, a virtual directory is limited. 
* Many virtual directories cannot correlate same-users across multiple diverse sources in the case of duplicate users
* Virtual directories without advanced caching technologies cannot scale to heterogeneous, high-volume environments.

==Sample terminology==
{{Overly detailed|section=yes|date=July 2014}}
* Unify metadata: Extract schemas from the local data source, map them to a common format, and link the same identities from different data silos based on a unique identifier.
* Namespace joining: Create a single large directory by bringing multiple directories together at the namespace level. For instance, if one directory has the namespace "ou=internal,dc=domain,dc=com" and a second directory has the namespace "ou=external,dc=domain,dc=com," then creating a virtual directory with both namespaces is an example of namespace joining.
* Identity joining: Enrich identities with attributes pulled from multiple data stores, based on a link between user entries.  For instance if the user joeuser exists in a directory as "cn=joeuser,ou=users" and in a database with a username of "joeuser" then the "joeuser" identity can be constructed from both the directory and the database.
* Data remapping: The translation of data inside of the virtual directory. For instance, mapping “uid” to “samaccountname,” so a client application that only supports a standard LDAP-compliant data source is able to search an Active Directory namespace, as well.
* Query routing: Route requests based on certain criteria, such as “write operations going to a master, while read operations are forwarded to replicas.”
* Identity routing: Virtual directories may support the routing of requests based on certain criteria (such as write operations going to a master while read operations being forwarded to replicas).
* Authoritative source: A "virtualized" data repository, such as a directory or database, that the virtual directory can trust for user data.
* Server groups: Group one or more servers containing the same data and functionality. A typical implementation is the multi-master, multi-replica environment in which replicas process "read" requests and are in one server group, while masters process "write" requests and are in another, so that servers are grouped by their response to external stimuli, even though all share the same data.

==Use cases==
The following are sample use cases of virtual directories:
* Integrating multiple directory namespaces to create a central enterprise directory.
* Supporting infrastructure integrations after mergers and acquisitions. 
* Centralizing identity storage across the infrastructure, making identity information available to applications through various protocols (including LDAP, JDBC, and web services). 
* Creating a single access point for [[Web Access Management|web access management]] (WAM) tools. 
* Enabling web [[single sign-on]] (SSO) across varied sources or domains.
* Supporting role-based, fine-grained authorization policies
* Enabling authentication across different security domains using each domain’s specific credential checking method.
* Improving secure access to information both inside and outside of the firewall.

==References==
<references/>

{{DEFAULTSORT:Virtual Directory}}
[[Category:Data management]]
<=====doc_Id=====>:154
<=====title=====>:
Network transparency
<=====text=====>:
{{Multiple issues|
{{Unreferenced|date=December 2009}}
{{Lead rewrite|date=July 2015}}
}}

'''Network transparency''' in its most general sense refers to the ability of a protocol to transmit data over the [[computer network|network]] in a manner which is [[Transparency (human–computer interaction)|transparent]] (invisible) to those using the applications that are using the protocol.

==X Window==
The term is often partially correctly applied in the context of the [[X Window System]], which is able to transmit graphical data over the network and integrate it seamlessly with applications running and displaying locally; however, certain extensions of the X Window System are not capable of working over the network.<ref>{{cite web |url=https://lwn.net/Articles/553415/ |title=The Wayland Situation: Facts About X vs. Wayland (Phoronix) |publisher=[[LWN.net]] |date=23 June 2013}}</ref>

==Databases==
In a [[centralized database system]], the only available resource that needs to be shielded from the user is the data (that is, the [[storage system]]). In a [[Distributed Database Management System|distributed DBMS]], a second resource needs to be managed in much the same manner: the [[computer network|network]]. Preferably, the user should be protected from the network operational details. Then there would be no difference between database applications that would run on the centralized database and those that would run on a distributed one. This kind of transparency is referred to as '''network transparency''' or '''distribution transparency'''. From a [[database management system]] (DBMS) perspective, distribution transparency requires that users do not have to specify where data is located.

Some have separated distribution transparency into location transparency and naming transparency.

Location transparency in commands used to perform a task is independent both of the locations of the data, and of the system on which an operation is carried out.

Naming transparency means that a unique name is provided for each object in the database.

==Firewalls==
{{See also|Proxy server#Transparent proxy}}

Transparency in firewall technology can be defined at the networking (IP or [[Internet Layer|Internet layer]]) or at the [[Internet Layer|application layer]].

Transparency at the IP layer means the client targets the real IP address of the server. If a connection is non-transparent, then the client targets an intermediate host (address), which could be a proxy or a caching server. IP layer transparency could be also defined from the point of server's view. If the connection is transparent, the server sees the real client IP. If it is non-transparent, the server sees the IP of the intermediate host.

Transparency at the application layer means the client application uses the protocol in a different way. An example of a transparent HTTP request for a server:

<syntaxhighlight lang="text">
GET / HTTP/1.1
Host: example.org
Connection: Keep-Alive
</syntaxhighlight>

An example non-transparent HTTP request for a proxy (cache):

<syntaxhighlight lang="text">
GET http://foo.bar/ HTTP/1.1
Proxy-Connection: Keep-Alive
</syntaxhighlight>

Application layer transparency is symmetric when the same working mode is used on both the sides. The transparency is asymmetric when the firewall (usually a proxy) converts server type requests to proxy type or vice versa. 

Transparency at the IP layer does not mean automatically application layer transparency.

== See also ==
{{Portal|Computer networking}}

* [[Data independence]]
* [[Replication transparency]]

==References==
{{Reflist}}

{{DEFAULTSORT:Network Transparency}}
[[Category:Telecommunications]]
[[Category:Data management]]
<=====doc_Id=====>:157
<=====title=====>:
Uniform information representation
<=====text=====>:
{{unreferenced|date=December 2009}}

'''Uniform information representation'''  allows information from several realms or disciplines to be displayed and worked with as if it came from the same realm or discipline.  It takes information from a number of sources, which may have used different methodologies and metrics in their data collection, and builds a single large collection of information, where some records may be more complete than others across all fields of data

Uniform information representation is particularly important in the fields of [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), where different departments of a large organization may have collected information for different purposes, with different labels and units, until one department realized that data already collected by those other departments could be re-purposed for their own needs—saving the enterprise the effort and cost of re-collecting the same information.

{{DEFAULTSORT:Uniform Information Representation}}
[[Category:Data management]]

{{Comp-sci-stub}}
<=====doc_Id=====>:160
<=====title=====>:
Recording format
<=====text=====>:
{{Unreferenced|date=December 2009}}
[[File:Cylinder Head Sector.svg|thumb|300px|right|A cylinder, head, and sector of a hard drive. The sectors are a recording container format. The digital data on the disks may be both secondary [[Container format (digital)|container file formats]] and raw digital data content formats such as digital audio or ASCII encoded text.]]
[[File:WorldMapLongLat-eq-circles-tropics-non.png|thumb|440px|A map of Earth showing lines of latitude (horizontally) and longitude (vertically). The lines are a grid, a method for dividing and containing recorded [[cartographical]] data. The land masses and oceans are cartographical data in a raw content ([[pictorial]] graphical) format. The text is in an [[alphanumeric]]al symbolic raw content format.]]
A '''recording format ''' is a [[content format|format]] for [[encoder|encoding]] data for storage on a [[storage medium]]. The format can be container information such as [[Cylinder-head-sector|sectors]] on a disk, or user/audience information ([[Content (media and publishing)|content]]) such as [[analog signal|analog]] [[stereo]] [[Sound recording and reproduction|audio]].  Multiple levels of encoding may be achieved in one format. For example, a text encoded page may contain [[HTML]] and [[XML]] encoding, combined in a [[plain text]] file format, using either [[EBCDIC]] or [[ASCII]] character encoding, on a [[Universal Disk Format|UDF]] [[Digital data|digital]]ly formatted disk.  

In [[electronic media]], the primary format is the encoding that requires hardware to interpret (decode) data; while secondary encoding is interpreted by secondary [[signal processing]] methods, usually [[computer software]]. 

==Recording container formats==
A container format is a system for dividing physical storage space or virtual space for data. Data space can be divided evenly by a [[systems of measurement|system of measurement]], or divided unevenly with [[meta data]]. A grid may divide physical or virtual space with physical or virtual (dividers) borders, evenly or unevenly.  Just as a physical container (such as a [[file cabinet]]) is divided by physical borders (such as [[drawer (furniture)|drawer]]s and [[file folder]]s), data space is divided by virtual borders. Meta data such as a [[unit of measurement]], [[Address (geography)|address]], or [[meta tags]] act as virtual borders in a container format. A template may be considered an abstract format for containing a solution as well as the content itself. 

* Systems of measurement
**[[Metric system]]
** [[Geographic coordinate system]]
**[[Grid (page layout)|Page grid]]
* [[Film formats]]
* [[Audio format|Audio data format]]
* [[Video tape|Video tape format]]
* [[Disk format]]
* [[File format]]
* [[Meta data]]
** [[Formatted text|Text formatting]]
** [[Template (file format)|Template]]
** [[Data structure]]

==Raw content formats==
{{Main|content format}}

A raw content format is a system of converting data to displayable [[information]].  Raw content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format. A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].

{{Audio format}}{{Homevid}}

{{DEFAULTSORT:Recording Format}}
[[Category:Communication]]
[[Category:Information science]]
[[Category:Data management]]
[[Category:Film and video technology]]
[[Category:Computer storage media]]
[[Category:Recording]]
<=====doc_Id=====>:163
<=====title=====>:
Comparison of OLAP Servers
<=====text=====>:
The following tables compare general and technical information for a number of [[online analytical processing]] (OLAP) servers supporting MDX language. Please see the individual products articles for further information.

==General information==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Company
! Website
! Latest stable version
! [[Software license]]
! License Pricing
|-
! [[TM1|IBM Cognos TM1]]
| [[IBM]]
|<ref>{{cite web|url=http://www-01.ibm.com/software/data/cognos/index.html|title=Cognos Business Intelligence and Financial Performance Management}}</ref>
| 10.2.2 FP4
| [[Proprietary software|Proprietary]]
| -
|-
! [[Essbase]]
| [[Oracle Corporation|Oracle]]
|<ref>{{cite web|url=http://www.oracle.com/us/solutions/ent-performance-bi/business-intelligence/essbase/index.html|title=Oracle Essbase}}</ref>
| 11.1.2.4
| [[Proprietary software|Proprietary]]
| [http://www.oracle.com/us/corporate/pricing/index.htm]
|-
! [[icCube]]
| [[icCube]]
|<ref>{{cite web|url=http://www.icCube.com|title=icCube OLAP Server}}</ref>
| 6.0
| [[Proprietary software|Proprietary]]
| community/[http://www.iccube.com//prices]
|-
! [[Jedox|Jedox OLAP Server]]
| [[Jedox]]
|<ref>{{cite web|url=http://www.jedox.com/en/home/overview.html |title=Jedox AG Business Intelligence |deadurl=yes |archiveurl=https://web.archive.org/web/20100514124342/http://www.jedox.com:80/en/home/overview.html |archivedate=2010-05-14 |df= }}</ref>
| 7.0
| [[GNU General Public License|GPL]] v2 or [[EULA]], [[Proprietary software|Proprietary]]
| -
|-
 ! Infor BI OLAP Server
| [[Infor]]
|<ref>{{cite web|url=http://www.infor.com|title=Infor}}</ref>
| 10.6.0
| [[Proprietary software|Proprietary]]
| -
|-
! [[Microsoft Analysis Services]]
| [[Microsoft]]
|<ref>{{cite web|url=http://www.microsoft.com/Sqlserver/2008/en/us/analysis-services.aspx|title=Microsoft SQL Server 2008 Analysis Services}}</ref>
| 2016
| [[Proprietary software|Proprietary]]
| [http://www.microsoft.com/sqlserver/2008/en/us/pricing.aspx]
|-
! MicroStrategy Intelligence Server
| [[MicroStrategy]]
|<ref>{{cite web|url=http://www.microstrategy.com/Software/Products/Intelligence_Server/|title=MicroStrategy Intelligence Server}}</ref>
| 9
| [[Proprietary software|Proprietary]]
| -
|-
! [[Mondrian OLAP server]]
| [[Pentaho]]
|<ref>{{cite web|url=http://mondrian.pentaho.org|title=Pentaho Analysis Services: Mondrian Project}}</ref>
| 3.7
| [[Eclipse Public License|EPL]]
| free
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| [[Oracle Corporation|Oracle]]
|<ref>{{cite web|url=http://www.oracle.com/technology/documentation/olap.html|title=Oracle OLAP Documentation}}</ref>
| 11g R2
| [[Proprietary software|Proprietary]]
| [http://www.oracle.com/us/corporate/pricing/index.htm]
|-
! [[SAS System|SAS OLAP Server]]
| [[SAS Institute]]
|<ref>{{cite web|url=http://www.sas.com/technologies/dw/storage/mddb/index.html|title=SAS OLAP Server}}</ref>
| 9.4
| [[Proprietary software|Proprietary]]
| -
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| [[SAP AG|SAP]]
|<ref>{{cite web|url=http://www.sap.com/usa/platform/netweaver/components/businesswarehouse/index.epx |title=Components & Tools}}</ref>
| 7.30
| [[Proprietary software|Proprietary]]
| -
|-
! [[Cubes (OLAP server)|Cubes]]
| [[Open source|Open source community]]
|<ref>{{cite web|url=http://cubes.databrewery.org|title=Cubes – Lightweight OLAP Python Toolkit}}</ref>
| 1.0.1
| [[MIT License|MIT]]
| -
|}

==Data storage modes==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! [[MOLAP]]
! [[ROLAP]]
! [[HOLAP]]
! In-Memory
! Offline
|-
! [[TM1|IBM Cognos TM1]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes| [http://www-01.ibm.com/support/knowledgecenter/SSVJ22_10.2.2/com.ibm.swg.ba.cognos.dsk_ug.10.2.2.doc/t_dsk_maintain_offline.html%23t_dsk_maintain_offline Cognos Insight Distributed mode]}}
|-
! [[Essbase]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|
|-
! [[icCube]]
| {{Yes}}
| {{No}}
| {{No}}
| 
| {{Yes | [http://www.iccube.com/support/documentation/user_guide/using/offline_cubes.html Offline Cubes]}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{No}}
| {{No}}
| {{Yes}}
|Local cubes
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|Local cubes,<br /> [[PowerPivot|PowerPivot for Excel]],<br />[[Power BI|Power BI Desktop]]}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes|[http://www.microstrategy.com/Software/Products/User_Interfaces/Office/ MicroStrategy Office],<br /> [http://www.microstrategy.com/Software/Products/Service_Modules/Report_Services/ Dynamic Dashboards]}}
|-
! [[Mondrian OLAP server]]
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
|
|-
! [[Cubes (OLAP server)]]
| {{No}}
| {{Yes}}
| {{No}}
|
|
|-
|}

==APIs and query languages==
APIs and query languages OLAP servers support.
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! [[XML for Analysis]]
! [[OLE DB for OLAP]]
! [[Multidimensional Expressions|MDX]]
! [[Stored procedures]]
! Custom functions
! [[SQL]]
! [[LINQ]]<ref name="linq">{{cite web|url=http://agiledesignllc.com/products|title=SSAS Entity Framework Provider}}</ref>
! Visualization
! [[JSON]]
! [[REST API]]
|-
! [[Essbase]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| SmartView (Excel-AddIn), WebAnalysis, Financial Reports
| {{dunno}}
| {{dunno}}
|-
! [[icCube]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|[[Java (programming language)|Java]],<ref>{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/java_integration.html|title=icCube Java integration documentation|publisher=[[icCube]]}}</ref> [[R (programming language)|R]]<ref>{{cite web|url=http://www.iccube.com/support/documentation/mdx_integration/r_integration.html|title=icCube R language integration documentation|publisher=[[icCube]]}}</ref>}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes|[[Java (programming language)|Java]], [[Javascript]]}}
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|OLAP Rules, Push Rules, Application Engine}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes|Application Studio}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|Cube Rules, SVS Triggers}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes|[[.NET framework|.NET]]}}<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/ms176113.aspx|title=SQL Server 2008 Books Online (October 2009)Defining Stored Procedures|publisher=[[MSDN]]}}</ref>
| {{Yes}}<ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/ms145486.aspx|title=SQL Server 2008 Books Online (October 2009)Using Stored Procedures|publisher=[[MSDN]]}}</ref>
| {{Yes}}<ref>{{cite web|url=http://support.microsoft.com/kb/218592/en-gb|title=How to perform a SQL Server distributed query with OLAP Server|publisher=[[MSDN]]}}</ref>
| {{Yes}}
| {{Yes|Microsoft Excel, SharePoint, Microsoft Power BI, and 70+ other visualization tools}}<ref>{{cite web|url=http://www.ssas-info.com/analysis-services-client-tools-frontend|title=A collection of SSAS frontend tools|publisher=[[SSAS-info.com]]}}</ref>
| {{dunno}}
| {{dunno}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes}}<ref>{{cite web|url=http://www.simba.com/news/Pentaho-Simba-Partner-for-Excel-Connectivity.htm|title=Pentaho and Simba Technologies Partner to Bring World's Most Popular Open Source OLAP Project to Microsoft Excel Users}}</ref>
| {{Yes}}
| {{Yes}}
| {{Yes}}<ref>{{cite web|url=http://mondrian.pentaho.org/documentation/schema.php#User-defined_function|title=How to Define a Mondrian Schema|publisher=Pentaho}}</ref>
| {{Yes}}
| {{Yes}}
| {{No}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{No}}
| {{Yes}}<ref name="oraclemdx">{{cite web|url=http://www.oracle.com/us/corporate/press/036550|title=Oracle and Simba Technologies Introduce MDX Provider for Oracle OLAP}}</ref>
| {{Yes}}<ref name="oraclemdx"/>
| {{Yes|[[Java (programming language)|Java]], PL/SQL, [[OLAP DML]]}}
| {{Yes}}
| {{Yes}}<ref>{{cite web|url=http://www.oracle.com/technology/products/bi/olap/11g/demos/olap_sql_demo.html|title=Querying Oracle OLAP Cubes: Fast Answers to Tough Questions Using Simple SQL}}</ref>
| {{No}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes}}
| {{Yes|Web Report Studio}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{No}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[TM1|Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
| TM1 Web/TM1 Contributor, IBM Cognos Insight, IBM Performance Modeler, IBM Cognos Cafe for Excel, Cognos BI, TM1 Perspectives for Excel
| {{dunno}}
| {{Yes}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{No}}
| {{No}}
| {{No}}
| {{No}}
| {{Yes}}
| {{No}}
| {{No}}
| Cubes Viewer<ref>{{cite web|url=https://github.com/jjmontesl/cubesviewer|title=Cubes Viewer|publisher=jjmontes}}</ref>
| {{Yes}}
| {{dunno}}
|}

==OLAP distinctive features==

A list of OLAP features that are not supported by all vendors. All vendors support features such as parent-child, multilevel hierarchy, drilldown.

Data processing, management and performance related features:

{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!Real Time
!Write-back
!Partitioning
!Usage Based Optimizations
!Load Balancing and Clustering
|-
! [[Essbase]]
| {{No}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[icCube]]
| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/walkthrough/walkthrough_rt.html|title=icCube Real Time walkthrough}}</ref>
| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/mdx/Update%20Cube.html|title=icCube Writeback/Update Cube}}</ref>
| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/reference/partitioning_edition.html|title=icCube Partitioning}}</ref>
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{dunno}}
| {{Yes}}<ref>{{cite web|url=http://www.microstrategy.com/Software/Products/Dev_Tools/SDK/extensions.asp|title=Common Extensions of the MicroStrategy Platform}}</ref>
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes2 | Planned}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{dunno}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{No}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS OLAP Server]]
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|}

Data modeling features:
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!Semi-additive measures
!Many-to-Many 
!Multi-Cube Model
!Perspectives
!KPI
!Translations
!Named Sets
!Multi-attribute Hierarchies
!Actions
|-
! [[Essbase]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[icCube]]
| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_aggregation.html|title=icCube Aggregatin types}}</ref>
| {{Yes}}<ref>{{cite web|url=http://www.iccube.com/support/documentation/user_guide/schemas_cubes/facts_many2many.html|title=icCube Many-to-Many}}</ref>
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}<ref>{{cite web|url=http://jira.pentaho.com/browse/MONDRIAN-962|title=Support for Non-Additive and Semi-Additive Measures}}</ref>
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[IBM Cognos BI]]
| {{Yes}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS OLAP Server]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{dunno}}
| {{Yes}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|}

==System limits==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!OLAP Server
!# cubes
!# measures
!# dimensions
!# dimensions in cube
!# hierarchies in dimension
!# levels in hierarchy
!# dimension members
|-
! [[Essbase]]<ref>{{cite web|url=http://docs.oracle.com/cd/E57185_01/epm.1112/essbase_db/frameset.htm?limits.html|title=Essbase Server Limits|publisher=Oracle}}</ref>
| ?
| ?
| ?
| 255
| 255
| ?
| 20,000,000 (ASO), 1,000,000 (BSO)
|-
! [[icCube]]<!-- Java Integer, 32 bits -->
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
| ?
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
|-
! Infor BI OLAP Server
| ?
| 10,000,000
| ?
| 30
| ?
| ?
| 10,000,000
|-
! [[Jedox|Jedox OLAP Server]]
| 2^32 (32 bits) / 2^64 (64 bits)
| 2^32
| 2^32 (32 bits) / 2^64 (64 bits)
| 250
| 2^32
| 2^32
| 2^32
|-
! [[Microsoft Analysis Services]]<ref>{{cite web|url=http://technet.microsoft.com/en-us/library/ms365363.aspx|title=SQL Server 2008 Books Online (October 2009)Maximum Capacity Specifications (Analysis Services - Multidimensional Data)|publisher=Microsoft}}</ref>
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647 (max. number of dimensions in a database)
| 2,147,483,647
| 2,147,483,647
| 2,147,483,647 (xOLAP)
Unrestricted (In-memory)
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]<!-- Unrestricted by server - based on hardware limits, infinite it's not possible ;-) -->
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| ?
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
| Unrestricted{{efn|name=fn0}}
|-
! [[SAS System|SAS OLAP Server]]<ref>{{cite web|url=http://support.sas.com/documentation/cdl/en/olapug/63148/HTML/default/viewer.htm#p0m66bhcbgqwjen1jyfhf6woysu3.htm|title=SAS OLAP Cube Size Specifications}}</ref>
| Unrestricted{{efn|name=fn0}}
| 1024
| 128
| ?
| 128
| 19
| 4,294,967,296
|-
! [[IBM Cognos TM1]]
| Unrestricted{{efn|name=fn0}}
| Unrestricted
| Unrestricted{{efn|name=fn0}}
| 256
| Unrestricted{{efn|name=fn0}}
| Unrestricted
| Unrestricted
|}
{{notelist|notes=
{{efn|name=fn0|Please update as 'unrestricted', is just not possible}}
}}

==Security==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
!rowspan="2"| OLAP Server
!rowspan="2"| Authentication
!rowspan="2"| Network encryption
!rowspan="2"| On-the-Fly{{efn|name=fn1}}
!colspan="3"| Data access
|-
!Cell security
!Dimension security
!Visual totals
|-
! [[Essbase]]
| {{Yes|Essbase authentication, [[LDAP]] authentication, [[Microsoft Active Directory]]}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[icCube]]
| {{Yes|HTTP Basic/Form Authentication, Windows SSO (NTLM,Kerberos)}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! Infor BI OLAP Server
| {{Yes|OLAP authentication, Infor Federation Services, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes|Jedox authentication, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[Microsoft Analysis Services]]
| {{Yes|[[NTLM]], [[Kerberos (protocol)|Kerberos]]}}
| {{Yes|[[Transport Layer Security|SSL]] and [[SSPI]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes|Host authentication, database authentication, [[LDAP]], <br />[[Microsoft Active Directory]], [[NTLM]], SiteMinder, Tivoli, SAP, [[Kerberos (protocol)|Kerberos]]}}
| {{Yes|[[Transport Layer Security|SSL]], AES<ref>[http://latam.microstrategy.com/Software/Products/Intelligence_Server/features.asp MicroStrategy Intelligence Server Features]</ref>}}
| ?
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes|Oracle Database authentication}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| ?
| {{Yes}}
| {{Yes}}
| {{dunno}}
|-
! [[SAS System|SAS OLAP Server]]<ref>{{cite web|url=http://support.sas.com/documentation/cdl/en/mdxag/59575/HTML/default/a003230130.htm|title=SAS OLAP Security Totals and Permission Conditions}}</ref>
| {{Yes|Host authentication,SAS token authentication, [[LDAP]], [[Microsoft Active Directory]]}}
| {{Yes}}<ref>{{cite web|url=http://support.sas.com/documentation/cdl/en/bisecag/61133/HTML/default/a003275910.htm|title=How to Change Over-the-Wire Encryption Settings for SAS Servers}}</ref>
| ?
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[IBM Cognos TM1]]
| {{Yes|Builtin, [[LDAP]], [[Microsoft Active Directory]], [[NTLM]], IBM Cognos BI authentication}}
| {{Yes|[[Transport Layer Security|SSL]]}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|}
{{notelist|notes=
{{efn|name=fn1|On-the-Fly : The ability to define authentication dynamically via programmatic interfaces. New users do not require restarting the server or redefining the security.}}
}}

==Operating systems==
The OLAP servers can run on the following [[operating system]]s:
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Windows
! Linux
! UNIX
! z/OS
! AIX
|-
! [[Essbase]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[icCube]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! Infor BI OLAP Server
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[Microsoft Analysis Services]]
| {{Yes}}
| {{No}}
| {{No}}
| {{No}}
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|-
! [[Mondrian OLAP server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{Yes}}
|-
! [[IBM Cognos TM1]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
| {{Yes}}
|-
! [[Cubes (OLAP server)|Cubes]]
| {{Yes}}
| {{Yes}}
| {{Yes}}
| {{No}}
|}
<cite id="os_java">Note (1):</cite>The server availability depends on [[JVM|Java Virtual Machine]] not on the [[operating system]]</cite>

==Support information==
{| class="wikitable sortable"  style="font-size: 100%; text-align: center; width: auto;"
|-
! OLAP Server
! Issue Tracking System
! Forum/Blog
! Roadmap
! Source code
|-
! [[Essbase]]
| {{Yes|[http://support.oracle.com myOracle Support]}}
| [http://forums.oracle.com/forums/main.jspa?categoryID=84]
| [http://communities.ioug.org/Portals/2/Oracle_Essbase_Roadmap_Sep_09.pdf]
| Closed
|-
! [[icCube]]
| {{Yes | [http://issues.iccube.com/ YouTrack]}}
| |[http://www.iccube.com/forum]
| 
| Open
|-
! Infor BI OLAP Server
| {{Yes|Infor Xtreme}}
| 
| Available upon request
| Closed
|-
! [[Jedox|Jedox OLAP Server]]
| {{Yes|[http://bugs.palo.net/mantis/main_page.php Mantis]}}
| [http://www.jedox.com/community/palo-forum/board.php?boardid=9]
|
| Open
|-
! [[Microsoft Analysis Services]]
| {{Yes|[https://connect.microsoft.com/SQLServer Connect]}}
| [http://social.msdn.microsoft.com/Forums/en-US/sqlanalysisservices/threads]
| -
| Closed
|-
! [[MicroStrategy|MicroStrategy Intelligence Server]]
| {{Yes | [https://resource.microstrategy.com/Support/MainSearch.aspx MicroStrategy Resource Center]}}
| [https://resource.microstrategy.com/Forum/]
| -
| Closed
|-
! [[Mondrian OLAP server]]
| {{Yes|[http://jira.pentaho.com/browse/MONDRIAN Jira]}}
| [http://forums.pentaho.org/forumdisplay.php?f=79]
| [http://mondrian.pentaho.org/documentation/roadmap.php]
| Open
|-
! [[Oracle OLAP|Oracle Database OLAP Option]]
| {{Yes|[http://support.oracle.com myOracle Support]}}
| [http://forums.oracle.com/forums/main.jspa?categoryID=84]
|
| Closed
|-
! [[SAS System|SAS OLAP Server]]
| {{Yes|[http://support.sas.com/forums/index.jspa Support]}}
| [http://blogs.sas.com/]
|
| Closed
|-
! [[SAP NetWeaver Business Intelligence|SAP NetWeaver BW]]
| {{Yes | [http://service.sap.com/ OSS]}}
| [http://forums.sdn.sap.com/index.jspa]
| [http://esworkplace.sap.com/socoview(bD1lbiZjPTAwMSZkPW1pbg==)/render.asp?id=2270EAD629814D05A7ECECECECC8D002&fragID=&packageid=DEE98D07DF9FA9F1B3C7001A64D3F462]
| Closed
|-
! [[IBM Cognos TM1]]
| {{Yes | [http://ibm.com/support/servicerequest/ IBM Service Request]}}
| [http://www.tm1forum.com/viewforum.php?f=3]
|
| Closed
|-
! [[Cubes (OLAP server)|Cubes]]
| {{Yes|[https://github.com/databrewery/cubes/issues Cubes – Github Issues]}}
| [https://groups.google.com/forum/#!forum/cubes-discuss]
| [https://github.com/DataBrewery/cubes/wiki/Roadmap]
| [https://github.com/DataBrewery/cubes Open]
|}

==See also==
* [[Cubes (OLAP server)|Cubes]] (light-weight open-source OLAP server)
* [[icCube]]
* [[Palo (OLAP database)]]

==References==
{{reflist}}

{{Data warehouse}}

{{DEFAULTSORT:Comparison Of Olap Servers}}
[[Category:Online analytical processing| ]]
[[Category:Software comparisons|OLAP Servers]]
[[Category:Data management]]
[[Category:Data warehousing products]]
<=====doc_Id=====>:166
<=====title=====>:
Photo recovery
<=====text=====>:
{{Advert|date=August 2010}}

'''Photo recovery''' is the process of salvaging digital photographs from damaged, failed, corrupted, or inaccessible [[Computer data storage#Secondary storage|secondary storage]] media when it cannot be accessed normally. Photo Recovery can be considered a subset of
the overall [[Data Recovery]] field. 

Photo loss or deletion failures may be due to both hardware or software failures.

==Recovering data after hardware failure==
An excellent explanation of hardware failures is provided in the section for [[Data Recovery|data recovery]]. Typically, if your
drive or card is so badly damaged that your computer can not recognize that a drive/card has been connected, you
will need to consult a data recovery service provider.

==Recovering data after logical failure==
Logical Damage or the inability to view photos can occur due to many reasons. The most common reasons are:

# Deletion of photos.
# Corruption of boot sector of media.
# Corruption of [[file system]].
# [[Disk formatting]].
# Move or Copy errors.

=== Photo Recovery Using File Carving ===
The majority of photo recovery programs work by using a technique called [[file carving|file carving (data carving)]].
There are many different file carving techniques that are used to recover photos. Most of these techniques
fail in the presence of [[file system fragmentation]]. Simson Garfinkel showed that on average 16% of [[JPEG]]s are fragmented,<ref name=garfinkel_dfrws2007>[[Simson Garfinkel]], ''Carving Contiguous and Fragmented Files with Fast Object Validation'', in Proceedings of the 2007 digital forensics research workshop, DFRWS, Pittsburgh, PA, August 2007</ref> which
means on average 16% of jpegs are recovered partially or appear corrupt when recovered using techniques that
can't handle fragmented photos.

==== Header-Footer Carving ====
In Header-Footer Carving, a recovery program attempts to recover photos based on the standard starting and ending byte
signature of the photo format. To take an example, all [[JPEG]]s always begin with the hex sequence "FFD8" and they must
end with the hex sequence "FFD9".

Header-Footer Carving cannot be used to recover fragmented photos, and fragmented
photos will appear to be partially recovered or corrupt if incorrect data is added. Header-Footer Carving, along
with Header-Size Carving, are by far the most common techniques for photo recovery. One of the first non-gui/console
based programs to use this technique is [[PhotoRec]].
Use of footers can often truncate a photo, as many JPEGs contain thumbnails as an embedded object.  If a file is terminated with a FFD9 it will be corrupted, unless nested FFD8/FFD9s are counted.

==== Header-Size Carving ====
In Header-Size Carving, a recovery program attempts to recover photos based on the standard starting byte signature of
the photo format, along with the size of the photo that is either derived or explicitly stated in the photo format.
To take an example all 24-bit Windows Bitmaps (*.bmp), begin with the letters "BM", and store the size of the file in
the header. Header-Size Carving cannot be used to recover fragmented photos, and fragmented photos will appear to be
partially recovered or corrupt if incorrect data is added.

==== File-Structure Based Carving ====
A more advanced form of carving, a recovery program attempts to recover photos based on detailed knowledge of the
structure rules of the photo format. This will enable a recovery program to identify when a photo is not complete or
fragmented, but more needs to be done to see if a fragmented photo can be recovered. This technique is rarely
used by most photo recovery programs.

==== Validated Carving ====
In validated carving, a decoder is used to detect any errors in recovery of a photo. More advanced forms of validated
carving occur when each part of the recovered photo is compared against the rest of the photo to see if it "fits"
visually. Validated carving is superb at detecting photos that are either fragmented or have parts over-written or
missing. Validated carving alone cannot be used to recover fragmented photos.<ref name=pal_ieee_ip>A. Pal and N. Memon, [http://digital-assembly.com/technology/research/pubs/ieee-trans-2006.pdf "Automated reassembly of file fragmented images using greedy algorithms"] in IEEE Transactions on Image processing, February 2006, pp 385393</ref>

==== Log Carving ====
Log Carving occurs when a recovery program uses information left over in either file system structures or the log
to recover a deleted photo. For example, occasionally NTFS will store in the logs the exact location of where the
file was located prior to its deletion. A program using Log Carving will be able to then recover the photo. To be
sure about the quality of recovery, Validated Carving or File-Structure based carving should also be used to
validate the recovered photo.

==== Bi-Fragment Gap Carving ====
A fragmented photo recovery technique where a header and footer are identified and then all combinations of blocks
between the header and footer are validated to determine which combination results in the correct recovery of the
photo.<ref name="garfinkel_dfrws2007"/> This technique will only work if the file is fragmented into two parts.

==== SmartCarving ====
A process by which fragmented photos are recovered by looking at blocks on the disk and determining which block
is the best visual match for the photo being recovered. This is done in parallel for all blocks that are not part
of a recovered file.<ref name=pal_dfrws2008>A. Pal, T. Sencar, N. Memon, [http://digital-assembly.com/technology/research/pubs/dfrws2008.pdf "Detecting File Fragmentation Point Using Sequential Hypothesis Testing"] Digital Forensic Research Workshop, August 2008</ref>

==References==
<references />

==Further reading==
* Tanenbaum, A. & Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
*[http://www.informationweek.com/news/windows/showArticle.jhtml?articleID=200000329 What To Do When Windows Vista Crashes: Little-Known Recovery Strategies], from Information Week

[[Category:Data recovery|photo]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Hard disk software|*]]
[[Category:Photography]]
<=====doc_Id=====>:169
<=====title=====>:
BBC Archives
<=====text=====>:
{{for|the Iron Maiden album|BBC Archives (album)}}
{{distinguish|BBC Motion Gallery}}
{{EngvarB|date=September 2013}}
{{Use dmy dates|date=September 2013}}
[[File:BBC Information and Archives Logo.svg|thumb|300px|BBC Information and Archives logo]]

'''BBC Information and Archives''' (sometimes known just as '''BBC Archives''') are collections documenting the [[BBC]]'s broadcasting history, including copies of [[BBC Television|television]] and [[BBC Radio|radio]] broadcasts, internal documents, photographs, [[BBC Online|online]] content, [[sheet music]], commercially available music, press cuttings and historic equipment.<ref name="BBCArchive TV 1"/> The original copies of these collections are permanently retained but are now in the process of being digitised, estimated to take until approximately 2015. Some collections are now being uploaded onto the BBC Archives website on [[BBC Online]] for viewers to see. The archive is one of the largest broadcast archives in the world with over 12 million items.<ref name=Perivale1>{{cite web|last=Hayes|first=Sarah|title=The new BBC Archive Centre in Perivale|url=http://www.bbc.co.uk/blogs/aboutthebbc/2011/10/the-new-bbc-archive-centre-at.shtml|work=About the BBC Blog|publisher=BBC Online|accessdate=17 January 2012}}</ref>

==Overview==
The BBC Archives encompass numerous different archives containing different materials produced or acquired by the BBC. The earliest material dates back to 1890 and now consists of 1 million hours of playable material, in addition to documents, photographs and equipment.<ref name="gutechweekly">{{cite news|last=Kiss|first=Jemima|title=In The BBC Archive|url=https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road|work=Tech Weekly|publisher=Guardian News & Media Ltd|accessdate=21 August 2010 | location=London | date=18 August 2010| archiveurl= https://web.archive.org/web/20100821165828/http://www.guardian.co.uk/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road?| archivedate= 21 August 2010 <!--DASHBot-->| deadurl= no}}</ref> The archives contain 12 million items on 66 miles of shelving spread over several sites.<ref name=gutechweekly /> The stock is managed using a [[bar code]] system, which help to locate material on the shelves and also track material that has been lent out.<ref name=gutechweekly /> The BBC says that the budget for managing, protecting and digitising the archive accounts for only a small part of the BBC's overall spend.<ref name=gutechweekly />

The BBC is engaging in an ongoing project to [[Digital reformatting|digitise]] archived programme material, converting recordings made on older [[Analog recording|analogue formats]] such as audio tape, [[videotape]] and film to electronic formats which are compatible with modern computer systems. Much of the audio-visual material was originally recorded on formats which are now obsolete and incompatible with modern broadcast equipment due to the fact that the machines used to reproduce many formats are no longer being manufactured. Additionally, some film and audio formats are slowly disintegrating, and digitisation also serves as a [[digital preservation]] programme. As of summer 2010 BBC Archive staff have spent approximately ten years digitising half of the media content<ref name=gutechweekly /><ref name="BBC Archive BBCInternetblog"/> and due to improving work practices expect to complete the other half in five years. Current estimates suggest the digitised archive would comprise approximately 52 [[petabyte]]s of information,<ref name=gutechweekly /> with one programme minute of video requiring 1.4 [[gigabyte]]s of storage.<ref name=gutechweekly /> The BBC uses the [[Material Exchange Format]] (MXF)<ref name=gutechweekly /> which is an uncompressed, non-proprietary format which the BBC has been publicising to mitigate the threat of the format becoming obsolete (as digital formats can and do).<ref name=gutechweekly />

The Archive digitisation a key part of the BBC's programme to engineer a fully digital and [[tapeless|tapeless production workflow]] across the entire Corporation. It was closely tied in with the ill-fated [[Digital Media Initiative]] (DMI), a scheme which ran from 2008 to 2013 and attempted to create a unified online archive search and programme production system.<ref name=bbc-dmi>{{cite web|title=Digital Media Initiative|url=http://www.bbc.co.uk/careers/divisions/digital-media-initiative|publisher=BBC|accessdate=15 February 2012|archiveurl=https://web.archive.org/web/20120310041000/http://www.bbc.co.uk/careers/divisions/digital-media-initiative|archivedate=10 March 2012|deadurl=yes}}</ref> After spiralling development costs and project delays, the problems with DMI came to public attention during coverage of the [[death and funeral of Margaret Thatcher]] in April 2013, when it was reported that the lack of digital ingest facilities provided for [[BBC News]] staff meant that tapes had to be sent by taxi from the Perivale centre to be digitised by independent companies in central London.<ref>{{cite news|title=BBC's Thatcher coverage highlights problems with non-digital archives|url=https://www.theguardian.com/media/2013/apr/11/bbc-thatcher-coverage|accessdate=3 May 2013|newspaper=The Guardian|date=11 February 2012|location=London|first=Tara|last=Conlan}}</ref> DMI was cancelled in 2013.<ref name=bbc_abandons>{{cite news|title=BBC abandons £100m digital project|url=http://www.bbc.co.uk/news/entertainment-arts-22651126|accessdate=25 May 2013|newspaper=BBC News|date=24 May 2013}}</ref>

The BBC Archive website was relaunched online in 2008 and has provided newly released historical material regularly since then.<ref>{{cite web|last=Sangster|first=Jim|title=A new homepage for BBC Archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/05/a_new_homepage_for_bbc_archive.html|work=BBC Internet Blog|publisher=BBC|accessdate=19 January 2012}}</ref> The BBC works in partnership with the [[British Film Institute]] (BFI), [[The National Archives]] and other partners in working with and using the materials.<ref name=gutechweekly /> A related project called "Genome" is expected to complete in 2011 and will make programme listings dating back to 1923, sourced from ''[[The Radio Times]]'', available to search online.<ref name=gutechweekly />

In July 2008, [[Roly Keating]] was appointed Director of Archive Content,<ref>{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/07_july/22/archive.shtml |title=Roly Keating appointed as Director of Archive Content |publisher=BBC Press Office|date=22 July 2008 |accessdate=1 July 2011}}</ref> with responsibility for increasing public access to the BBC’s archives. In October 2008, Keating appointed [[Tony Ageh]] Controller of Archive Development with "specific responsibility for developing ways of making the archive easily understandable and accessible to users".<ref>{{cite web|url=http://www.bbc.co.uk/pressoffice/pressreleases/stories/2008/10_october/10/ageh.shtml|publisher=BBC Press Office|date=10 October 2008|accessdate=1 July 2011|title=Tony Ageh appointed Controller of Archive Development}}</ref>

In 2012, BBC Archive Development produced a book - primarily aimed as BBC staff - titled 'BBC Archive Collections: What's In The Archive And How To Use Them'.<ref>'BBC Archive Collections: What's In The Archives, And How To Use Them' Edited by Jake Berger https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0</ref>  This book describes the BBC's archive collections and offers guidance around on how items from the collections can be reused online.  The book's references to 'Fabric', a system due to be delivered by the [[Digital Media Initiative]] are no longer accurate as the project was cancelled.

==Buildings==
From 1968 to 2010 the BBC Archive was housed at the Archive centre in Windmill Road, [[Brentford]], in [[W postcode area|west London]].<ref name=Perivale1/><ref name="BBC Archive BBCInternetblog"/><ref name="Perivale 2 AtBBCblog"/> The condition of the building deteriorated over the years and suffered occasional flooding incidents, and eventually the Archive was relocated to a new centre at Perivale Park, [[Perivale]], three miles north of the old site.<ref name="Perivale 2 AtBBCblog"/><ref name="Perivale Centre R4 Blog"/> The new BBC Archive Centre was opened in Summer 2010 and all material was successfully moved by March 2011.<ref name="Perivale 2 AtBBCblog"/><ref name="Perivale 3 S&Pblog"/> The cost of the refurbishment and of the move was approximately £16.6 million.<ref name=Perivale1/><ref name="Perivale 2 AtBBCblog">{{cite news|last=Skinner|first=Peter|title=A new home for the BBC Archive|url=http://www.bbc.co.uk/blogs/aboutthebbc/2010/08/a-warm-balmy-afternoon-in.shtml|accessdate=19 January 2012|newspaper=BBC About the BBC Blog|date=20 August 2010}}</ref><ref name="Perivale Centre R4 Blog">{{cite news|last=Bolton|first=Roger|title=Tears in Perivale – Feedback in the archives|url=http://www.bbc.co.uk/blogs/radio4/2011/09/tears_in_perivale_feedback_in_the_archives.html|accessdate=19 January 2012|newspaper=BBC Radio 4 and 4 Extra Blog|date=23 September 2011}}</ref><ref name="Perivale 3 S&Pblog">{{cite news|last=Kane|first=Chris|title=Preserving the past at Perivale|url=http://www.bbc.co.uk/blogs/spacesandplaces/2011/03/preserving_the_past_at_perival.shtml|accessdate=19 January 2012|newspaper=BBC Spaces & Places Blog|date=9 March 2011}}</ref><ref>{{cite web|url=http://downloads.bbc.co.uk/foi/classes/disclosure_logs/rfi20111170_cost_of_new_archive_centre.pdf|title=Freedom of Information Act 2000 – RFI20111170|last=Jupe|first=Steve|date=20 October 2011|work=Freedom of Information Request|publisher=BBC|accessdate=7 February 2012}}</ref><ref>{{cite web|title=The BBC Archive Centre has moved|url=http://www.bbc.co.uk/commissioning/news/the-bbc-archive-centre-has-moved.shtml|work=BBC Commissioning|publisher=BBC|accessdate=19 January 2012}}</ref>

Material is stored in thirteen vaults,<ref name="Perivale 2 AtBBCblog"/> controlled to match the best climate for the material inside them,<ref name=Perivale1/><ref name=gutechweekly /><ref name="BBC Archive BBCInternetblog"/><ref name="Perivale 2 AtBBCblog"/> and named after a different BBC personality depending on the content contained in them.<ref name="Perivale 2 AtBBCblog"/> In addition to the vaults, new editing and workrooms have been added so that the material can easily be transferred between formats as well as viewed and restored.<ref name="Perivale 2 AtBBCblog"/> The building has also been fitted with fire suppression systems to protect the archive in the event of an incident at the centre, so the total loss of the archive is avoided.<ref name="Perivale Centre R4 Blog"/>

==Television Archive==
The '''BBC Television Archive''' contains over 600,000 hours of television broadcast material<ref name="BBCArchive TV 1">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – What's in the BBC Archive|url=http://www.bbc.co.uk/archive/tv_archive.shtml|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> located on 600,000/650,000 film reels<ref name="Windmill Road">{{cite web|title=A Tour of the BBC Archive at Windmill Road|url=https://www.youtube.com/watch?v=S3Z2djrAW2M|publisher=[[BBC]]|accessdate=23 July 2015|date=13 Aug 2010}}</ref><ref name="BBC Archive BBCInternetblog">{{cite news|last=Williams|first=Adrian|title=Safeguarding the BBC's archive|url=http://www.bbc.co.uk/blogs/bbcinternet/2010/08/safeguarding_the_bbcs_archive.html|accessdate=19 January 2012|newspaper=BBC Internet Blog|date=18 August 2010}}</ref><ref name="BBCArchive PTV 4">{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – Film|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> and 2.4/2.7 million videotapes.<ref name="BBC Archive BBCInternetblog"/><ref name="Windmill Road"/> The archive itself holds extensive material from approximately the mid-1970s onwards, when important recordings at the broadcaster were retained for the future.<ref name="BBCArchives TV 6">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – When did the BBC start to ensure that important broadcasts were not destroyed|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>

Recordings from before this date are less comprehensively preserved; the process of [[Kinescope|telerecording]] was originally invented in 1947<ref name="BBCArchive TV 2">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – Why aren't there many recordings from the early days of television|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=2|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> while videotape recording was gradually introduced from the late 1950s onwards,<ref name="BBC Archive TV 4">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – When did the BBC start recording programmes regularly|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=4|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> but due to the expense of the tapes,<ref name="BBCArchives PTV 8">{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – Why was videotape invented|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> recording was seen for production use only with recordings subsequently being [[wiping|wiped]].<ref name="BBC Archive TV 4"/> or telerecordings being junked. The exceptions in the early years were usually occasions of great importance, such as the [[coronation of Queen Elizabeth II]].<ref name="BBCArchive TV 2"/> In addition, numerous programmes at the time were broadcast 'live' and so utilised no recording procedure in the production process.<ref name="BBCArchive TV 2"/> The earliest item in the collection is from 1936.<ref name="BBC Archive BBCInternetblog"/><ref name="Perivale 3 S&Pblog"/><ref name="BBCArchives PTV 6">{{cite web|last=Williams|first=Adrian|title=Preserving the Television Archive – The oldest BBC Television film clip|url=http://www.bbc.co.uk/archive/preserving.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>

In 2013 there were 340,000 D3 Tapes but the hardware they have could only transfer up to 130,000 D3 tapes.<ref name=Digitising>{{cite web|title=Digitising the BBC archive|url=http://www.bbc.co.uk/academy/technology/article/art20130704121742520|publisher=[[BBC]]|accessdate=23 July 2015|date=2013}}</ref> The BBC has had to be very selective of what they are transferring.<ref name=Digitising/>

Before anything is put into the archive a team of Digitisation Operators watch and listen to programs looking for problems with the tapes or transfers.<ref name="Sarah Bello">{{cite web|title=Sarah Bello, BBC Archive|url=https://www.youtube.com/watch?v=MyG1lSdsKQs|publisher=[[BBC]]|accessdate=23 July 2015|date=11 March 2013}}</ref> 

Today, the majority of programmes are kept, including news, entertainment, drama and a selection of other long-running programmes such as quiz shows.<ref name="BBCArchive TV 7">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – How does the BBC decide what to keep in its archive today|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=7|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The remaining material from the television archive is offered to the [[British Film Institute]] prior to being disposed of.<ref name="BBCArchive TV 8">{{cite web|last=Lee|first=Adam|title=BBC Television Archive – Does the BBC offer recordings it's not keeping for the archive to anyone else|url=http://www.bbc.co.uk/archive/tv_archive.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>

==Sound Archive==
{{main|BBC Sound Archive}}

The '''BBC Sound Archive''' contains the archived output from the BBC's radio output. Widespread recordings exist in the archive from the mid-1930s, when recording of programmes and speeches were kept for rebroadcast; the catalyst for this was the launch of the [[BBC Empire Service]] in 1932 and the subsequent rebroadcast of speeches from political leaders at a time convenient in the different time zones.<ref name="BBCArchive - Radio 3">{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Why did the BBC start making recordings|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Prior to this, the broadcast of recordings was seen as being false to the listener and was avoided.<ref name="BBCArchives Radio 2">{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Why aren't there many recordings from the early days of radio|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Any recordings made were frequently disposed of and it was the efforts of [[Marie Slocombe]], who founded the Sound Archive in 1937 when she retained recordings of prominent figures in the country, that the archive became into being officially when she was appointed the Sounds Recording Librarian in 1941.<ref name="BBCArchives - PRadio 6">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – How did the Sound Archive begin|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Today, all of the BBC's radio output is recorded for re-use,<ref name="BBCArchive - Radio 8">{{cite web|last=Rooks|first=Simon|title=BBC Sound Archive – Does the BBC keep copies of all programmes today|url=http://www.bbc.co.uk/archive/sound_archive.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> with approximately 66% of output being preserved in the Archives;<ref name="BBCArchive - Radio 8"/> programmes involving guests or live performances from artists are kept<ref name="BBCArchive - Radio 8"/> whereas programmes in which the DJ plays commercially available music are only sampled and not kept entirely.<ref name="BBCArchive - Radio 8"/> Prior to any material being disposed of, the material is offered to the [[British Library Sound Archive]].<ref name="BBCArchive TV 8"/>

The archive consists of a number of different formats including 200 [[Phonograph cylinder|wax cylinders]],<ref name="BBCArchive - PRadio 3">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – What are the earliest sound recordings|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> numerous [[gramophone record]]s made from both [[shellac]] and [[vinyl]]<ref name="BBCArchives - PRadio 4">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – Discs|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> as well as numerous more recordings on [[Reel-to-reel audio tape recording|tape]], CD and on [[digital audio tape]] (DAT).<ref name="BBCArchive PRadio 5">{{cite web|last=Weaver|first=Julia|title=Preserving the Sound Archive – Tape|url=http://www.bbc.co.uk/archive/preserving_sound.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The difficulty of these different formats is the availability of the machines required to play them; some of the vinyl records in the archive are 16 inches in size and require large [[phonograph]] units to play,<ref name="BBCArchives - PRadio 4"/> while the players for the wax cylinders and DATs are no longer in production.<ref name="BBCArchive PRadio 5"/> There are 700,00 vinyl records, 180,000 78's records, 400,000 [[LP record]] and 350,000 [[Compact disc|Cd's]] in the archive.<ref name="Windmill Road"/>

The oldest item is a wax cylinder containing a recording made by [[Florence Nightingale]], recorded on 30 July 1890.<ref name="BBCArchive - PRadio 3"/> Another unique item is the gramophone record from [[Mary of Teck|Queen Mary]]'s [[doll house]], which is approximately an inch in size and had the [[God Save the Queen|national anthem]] on it.<ref name="BBCArchives - PRadio 4"/>

The Sound Archive is based at the new BBC Archive Centre in Perivale, along with the television archive,<ref name=Perivale1/><ref name="Perivale 3 S&Pblog"/> and was previously based at Windmill Road, Brentford.

==Written Archives==
The '''BBC Written Archive''' contains all the internal written documents and communications from the corporation from the launch in 1922 to the present day.<ref name="Written Archives BBC Story">{{cite web|title=The Written Archives|url=http://www.bbc.co.uk/historyofthebbc/contacts/wac.shtml|work=The BBC Story|publisher=BBC|accessdate=19 January 2012}}</ref><ref name="BBCArchive Written 1">{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives – What are the BBC Written Archives|url=http://www.bbc.co.uk/archive/written.shtml|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Its collections shed light into the behind the scenes workings of the corporation and also elaborate on the difficulties of getting a television or radio programme to or off the air as the case may be.<ref name="BBC Archive Written 3">{{cite web|last=Kavanagh|first=Jacquie|title=BBC Written Archives – What do the documents reveal|url=http://www.bbc.co.uk/archive/written.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The archive guidelines state that access to files post-1980 is restricted due to the current nature of the files; the general exception to this rule are documents such as scripts and Programme as Broadcast records.

The Written Archives are located at the BBC Written Archive Centre in [[Caversham, Berkshire]], near [[Reading, Berkshire|Reading]].<ref name="Written Archives BBC Story"/> The centre houses the archive on four and a half miles of shelving along with reading rooms. The centre is different from the other BBC Archives in that the centre opens for writers and academic researchers in higher education.<ref name="Written Archives BBC Story"/>

==Photographic Library==
The '''BBC Photographic Library''' is responsible for approximately 10 million images,<ref name="BBCArchive Photo 1">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – What's in the BBC Photo Library|url=http://www.bbc.co.uk/archive/photo_library.shtml|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> dating back to 1922,<ref>{{cite web|title=BBC Pictures|url=http://www.bbc.co.uk/mediacentre/pictures/index.html|work=BBC Media Centre|publisher=BBC|accessdate=19 January 2012}}</ref> created for publicity purposes and subsequently kept for future use.<ref name="BBCArchive Photo 2">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Why does the BBC have photographs|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> In addition to programme promotion, a large number of images are of historic events which are often incorporate into the daily news bulletins; as a result, half the photographic library team work specifically with these images.<ref name="BBCArchive Photo 4">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – The Team|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The images themselves are kept as originals in the archive, with digitisation only utilised when a specific image is required for use, when the image is sent in a digital format.<ref name="BBCArchive Photo 5">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – What format are the images stored on|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> Copies of images are also used in case any images are damaged, notable due to [[vinegar syndrome]].<ref name="BBCArchive Photo 6">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Preservation|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=6|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The BBC Photographic library itself is based within [[BBC Television Centre]], London.

The most popular images from the Archive include [[Colin Firth]] in ''[[Pride and Prejudice (1995 TV series)|Pride and Prejudice]]'', [[Michael Parkinson]] interviewing [[Muhammad Ali]], [[Jimmy Savile]] presenting the first ''[[Top of the Pops]]'', [[Martin Bashir]] interviewing [[Diana, Princess of Wales]] and a picture of [[Delia Derbyshire]] at work in the Radiophonic workshop at the BBC.<ref name="BBCArchive Photo 8">{{cite web|last=Dewar|first=Natalie|title=Photographic Library – Our Top 10|url=http://www.bbc.co.uk/archive/photo_library.shtml?chapter=8|work=BBC Archive – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>

==Heritage Collection==
The '''BBC Heritage Collection''' is the newest of the BBC Archives and holds a variety of historic broadcast technology, art, props and merchandise.<ref name="BBCArchives Heritage 2">{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Where do the items come from|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=2|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> The collection was created out of personal collections and bequeaths by former staff members, as the BBC had no formal policy on the heritage collection until c.2003.<ref name="BBCArchives Heritage 2"/>

The collection includes, amongst other items, the BBC One Noddy Globe and clock,<ref name="BBCArchive Heritage 3">{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Broadcast technology|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=3|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> a [[BBC-Marconi Type A]] microphone,<ref name="BBCArchive Heritage 3"/> an early [[crystal radio]] made by the [[British Broadcasting Company]],<ref name="BBCArchive Heritage 3"/> a [[405-line television system|Marconi/EMI camera]] used in the early [[BBC Television]] experiments,<ref name="BBCArchive Heritage 3"/> a [[BBC Micro]] computer<ref name="BBCArchive Heritage 3"/> and a selection of items used to create [[Foley (filmmaking)|Foley]].<ref name="BBCArchive Heritage 3"/> In addition to all the broadcast technology, art is also kept, namely the portraits of all the BBC [[Director-General of the BBC|Director General]]s,<ref name="BBCArchive Heritage 4">{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Art|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=4|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> as well as props including an original [[TARDIS]] from ''[[Doctor Who]]''<ref name="BBCArchive Heritage 5">{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Costumes and Props|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=5|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref> and the children's television puppet [[Gordon the Gopher]].<ref name="BBCArchive Heritage 5"/>

The heritage collection itself has no one permanent home, as the majority of objects are on display, either around BBC properties or on loan to museums or other collections; the most notable museum housing the collection is the [[National Media Museum]] in [[Bradford]].<ref name="BBCArchive Heritage 8">{{cite web|last=O'Connell|first=Rory|title=BBC Heritage Collection – Where can I see items from the collection|url=http://www.bbc.co.uk/archive/heritage.shtml?chapter=8|work=BBC Archives – Meet the experts|publisher=BBC|accessdate=19 January 2012}}</ref>

==Archive Treasure Hunt==
At the turn of the millennium, the BBC launched the '''BBC Archive Treasure Hunt''', a public appeal to recover pre-1980s lost BBC radio and television productions.<ref>{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml |title=BBC Online – Cult – Treasure Hunt – About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100721235531/http://www.bbc.co.uk/cult/treasurehunt/about/about.shtml| archivedate= 21 July 2010 <!--DASHBot-->| deadurl= no}}</ref> Original material, featuring several popular programmes were lost due to the practice of [[wiping]], because of copyright issues and for technological reasons.<ref>{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/lost.shtml |title=BBC Online – Cult – Treasure Hunt – About the Campaign |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}</ref><ref>{{cite web|author=Stuart Douglas - www.thiswaydown.org |url=http://www.btinternet.com/~m.brown1/bbchunt.htm |title=missing episodes articles |publisher=Btinternet.com |date=7 July 1965 |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100814103420/http://www.btinternet.com/~m.brown1/bbchunt.htm| archivedate= 14 August 2010 <!--DASHBot-->| deadurl= no}}</ref>

The resolution of this appeal was that over one hundred productions were recovered<ref>{{cite web|url=http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html |title=No 4 2001 – Missing Believed Wiped |publisher=Fiat/Ifta |date= |accessdate=30 July 2010| archiveurl= https://web.archive.org/web/20100716032923/http://fiatifta.org/aboutfiat/news/old/2001/2001-04/03.light.html| archivedate= 16 July 2010 <!--DASHBot-->| deadurl= no}}</ref> including ''[[The Men from the Ministry]]'', ''[[Something to Shout About (film)|Something To Shout About]]'', ''[[Man and Superman]]'', ''[[The Doctor's Dilemma (play)|The Doctor's Dilemma]]'', ''[[I'm Sorry, I'll Read That Again]]'', ''[[Hancock's Half Hour]]'', ''[[I'm Sorry, I Haven't A Clue]]'' and ''[[The Ronnie Corbett Thing]]'' in addition to recording sessions with [[Elton John]], [[Ringo Starr]] and [[Paul Simon]].<ref name="BBCTH">{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online – Cult – Treasure Hunt – List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}</ref><ref>{{cite web|url=http://www.allbusiness.com/services/motion-pictures/4848337-1.html |title='hunt' Unearths BBC Treasures From Radio, Tv &#124; Business solutions from |publisher=AllBusiness.com |date=9 November 2001 |accessdate=30 July 2010}}</ref> Also, the Peter Sellers Estate Collection donated numerous recordings featuring [[Peter Sellers]].<ref name="BBCTH"/>

==Creative Archive Licence==
The BBC together with the [[British Film Institute]], the [[Open University]], [[Channel 4]] and [[Teachers' TV]] formed a collaboration, named the Creative Archive Licence Group, to create a copyright licence for the re-release of archived material.<ref name=CAL>{{cite web|title=Creative Archive pilot|url=http://www.bbc.co.uk/creativearchive/|publisher=BBC|accessdate=31 March 2016}}</ref>

The Licence was a trial, launched in 2005, and notable for the re-release of part of the [[BBC News]]' archive and programmes made by the [[BBC Natural History Unit]] for creative use by the public. While artists and teachers were encouraged to use the content to create works of their own, the terms of the licence were restrictive compared to [[copyleft]] licences. Use of Creative Archive content for commercial, "endorsement, campaigning, defamatory or derogatory purposes" was forbidden, any derivative works were to be released under the same licence, and content was only  to be used within the UK.<ref name=CAL/><ref>{{cite web|title=Creative Archive License|url=http://news.bbc.co.uk/1/hi/help/4527506.stm|publisher=BBC|accessdate=17 January 2012}}</ref> The trial ended in 2006 following a review by the [[BBC Trust]] and works released under the licence were withdrawn.<ref name=CAL/>

==Voices from the archives==
Voices from the Archives is a former [[BBC]] project, launched in partnership with [[BBC Four]] that provided free access to audio interviews with various notable people and professions from a variety of political, religious and social backgrounds. The website ceased to be updated in June 2005, and the concept was instead adopted by [[BBC Radio 4]] as a collection of film interviews from various programmes.

==Programme catalog==
{{main|BBC Programme Catalogue}}
Over the years there the BBC has used various Programme catalog databases to keep a record of the programmes in the archives. Internal databases include [[Infax]] and [[BBC Fabric|Fabric]], and publicly accessible databases include [[BBC Genome]] and [http://www.bbc.co.uk/programmes BBC Programmes].

==See also==
{{portal|BBC}}
* [[BBC Genome Project]]
* [[Lost film]]
* [[Film preservation]]
* [[Missing Believed Wiped]]
* [[Telerecording]]
* [[Doctor Who missing episodes|''Doctor Who'' missing episodes]]
* [[Timeline of the BBC]]

==References==
{{reflist|2}}

==External links==

===BBC Archives===
*{{bbc.co.uk|id=archive|title=BBC Archives}}
*{{bbc.co.uk|id=bbcfour/collections|title=BBC Four – Collections}}
*{{bbc.co.uk|id=archive/archive_pioneers|title=BBC Archive collection – Archive Pioneers: Saviours of sound at the BBC}}
*{{bbc.co.uk|id=programmes|title=BBC Programmes}}
*{{bbc.co.uk|id=informationandarchives|title=BBC Information and Archives}}
* [https://www.theguardian.com/technology/blog/audio/2010/aug/18/bbc-archive-roly-keating-windmill-road Tech Weekly podcast: In the BBC archives] from ''[[The Guardian]]'' website.
* [https://www.dropbox.com/s/rz1o57nzlsf1v04/BBC%20Archive%20Collections%20Guide%202012.pdf?dl=0 BBC Archive Collections: What's In The Archives, And How To Use Them]

===Wiped Material===
* [http://www.missing-episodes.com/ British TV Missing Episodes Index]
* [http://www.wipednews.com/ Wiped News.Com – A news and features website devoted to missing TV, Film & Radio]

{{BBC}}

[[Category:BBC]]
[[Category:BBC New Media|Archives]]
[[Category:Data management]]
[[Category:Broadcasting websites]]
[[Category:British websites]]
[[Category:BBC offices, studios and buildings|Archives]]
[[Category:Organisations based in Reading, Berkshire]]
[[Category:History of television in the United Kingdom]]
[[Category:History of radio]]
[[Category:BBC history]]
[[Category:Year of establishment missing]]
[[Category:Archives in Berkshire]]
[[Category:Television archives]]
<=====doc_Id=====>:172
<=====title=====>:
Data migration
<=====text=====>:
{{More footnotes|date=February 2013}}
'''Data migration''' is the process of [[data transfer|transferring]] [[data]] between [[computer data storage|computer storage]] types or [[file format]]s. It is a key consideration for any system implementation, upgrade, or consolidation. Data migration is usually performed programmatically to achieve an ''automated migration'', freeing up human resources from tedious tasks. Data migration occurs for a variety of reasons, including server or storage equipment replacements, maintenance or upgrades, [[Software modernization|application migration]], website consolidation and [[data center]] relocation.<ref>Janssen C, Data migration, http://www.techopedia.com/definition/1180/data-migration (retrieved 12 August 2013)</ref>

To achieve an effective data migration procedure, data on the old system is [[data mapping|mapped]] to the new system utilising a design for [[data extraction]] and [[data loading]]. The design relates old [[data format]]s to the new system's formats and requirements. Programmatic data migration may involve many phases but it minimally includes ''data extraction'' where data is read from the old system and ''data loading'' where data is written to the new system.

After loading into the new system, results are subjected to [[data verification]] to determine whether data was accurately translated, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].

Automated and manual data cleaning is commonly performed in migration to improve [[data quality]], eliminate [[data duplication|redundant]] or obsolete information, and match the requirements of the new system.

Data migration phases (design, [[extract, transform, load|extraction]], [[data cleansing|cleansing]], load, verification) for applications of moderate to high complexity are commonly repeated several times before the new system is deployed.

==Categories==

Data is stored on various media in [[Computer file|files]] or [[databases]], and is generated and consumed by [[software applications]] which in turn support [[business processes]]. The need to transfer and convert data can be driven by multiple business requirements and the approach taken to the migration depends on those requirements. Four major migration categories are proposed on this basis.

===Storage migration===
A business may choose to rationalize the physical media to take advantage of more efficient storage technologies. This will result in having to move physical blocks of data from one tape or disk to another, often using [[Storage virtualization|virtualization]] techniques. The data format and content itself will not usually be changed in the process and can normally be achieved with minimal or no impact to the layers above.

===Database migration===
{{main article|Schema migration}}
Similarly, it may be necessary to move from one [[database]] vendor to another, or to upgrade the version of database software being used. The latter case is less likely to require a physical data migration, but this can happen with major upgrades. In these cases a physical transformation process may be required since the underlying data format can change significantly. This may or may not affect behavior in the applications layer, depending largely on whether the data manipulation language or protocol has changed – but modern applications are written to be agnostic to the database technology so that a change from [[Sybase]], [[MySQL]], [[IBM DB2|DB2]] or [[Microsoft SQL Server|SQL Server]] to [[Oracle Database|Oracle]] should only require a testing cycle to be confident that both functional and non-functional performance has not been adversely affected.

===Application migration===
Changing application vendor – for instance a new [[Customer relationship management|CRM]] or [[Enterprise resource planning|ERP]] platform – will inevitably involve substantial transformation as almost every application or suite operates on its own specific data model and also interacts with other applications and systems within the [[enterprise application integration]] environment. Furthermore, to allow the application to be sold to the widest possible market, commercial off-the-shelf packages are generally configured for each customer using [[metadata]]. [[Application programming interfaces]] (APIs) may be supplied by vendors to protect the [[data integrity|integrity of the data]] they have to handle.

===Business process migration===
[[Business processes]] operate through a combination of human and application systems actions, often orchestrated by [[business process management]] tools. When these change they can require the movement of data from one store, database or application to another to reflect the changes to the organization and information about customers, products and operations. Examples of such migration drivers are mergers and acquisitions, business optimization and reorganization to attack new markets or respond to competitive threat.

The first two categories of migration are usually routine operational activities that the IT department takes care of without the involvement of the rest of the business. The last two categories directly affect the operational users of processes and applications, are necessarily complex, and delivering them without significant business downtime can be challenging. A highly adaptive approach, concurrent synchronization, a business-oriented audit capability and clear visibility of the migration for stakeholders are likely to be key requirements in such migrations.

===Project versus process===
There is a difference between data migration and [[data integration]] activities. Data migration is a project by means of which data will be moved or copied from one environment to another, and removed or decommissioned in the source. During the migration (which can take place over months or even years), data can flow in multiple directions, and there may be multiple migrations taking place simultaneously. The [[Extract, Transform, Load]] actions will be necessary, although the means of achieving these may not be those traditionally associated with the [[Extract, Transform, Load|ETL]] acronym.

Data integration, by contrast, is a permanent part of the IT architecture, and is responsible for the way data flows between the various applications and data stores - and is a process rather than a project activity. Standard ETL technologies designed to supply data from operational systems to data warehouses would fit within the latter category.

== Migration as a form of digital preservation ==
Migration, which focuses on the digital object itself, is the act of transferring, or rewriting data from an out-of-date medium to a current medium and has for many years been considered the only viable approach to long-term preservation of digital objects.<ref>{{cite journal|author1=van der Hoeven, Jeffrey|author2= Bram Lohman|author3=Remco Verdegem|title=Emulation for Digital Preservation in Practice: The Results|journal=The International Journal of Digital Curation|volume=2|issue=2|year=2007|pages=123-132|url=http://www.ijdc.net/index.php/ijdc/article/view/50|doi=10.2218/ijdc.v2i2.35}}</ref> Reproducing brittle newspapers onto [[microform|microfilm]] is an example of such migration.

=== Disadvantages ===
* Migration addresses the possible obsolescence of the data carrier, but does not address the fact that certain technologies which run the data may be abandoned altogether, leaving migration useless.
* Time-consuming – migration is a continual process, which must be repeated every time a medium reaches obsolescence, for all data objects stored on a certain media.
* Costly - an institution must purchase additional data storage media at each migration.<ref>{{cite journal|author=Muira, Gregory|title=Pushing the Boundaries of Traditional Heritage Policy: maintaining long-term access to multimedia content|journal=IFLA Journal|volume=33|year=2007|pages=323-326|url=http://www.ifla.org/files/assets/hq/publications/ifla-journal/ifla-journal-4-2007.pdf}}</ref>

As a result of the disadvantages listed above, technology professionals have begun to develop alternatives to migration, such as [[emulator|emulation]].

==See also==
* [[Data conversion]]
* [[Data transformation]]
* [[Extract, transform, load]]
* [[System migration]]

==References==
{{reflist}}

== External links ==
* {{dmoz|Computers/Software/Databases/Data_Warehousing/Extraction_and_Transformation|Data Migration}}

{{Authority control}}

[[Category:Data management]]
<=====doc_Id=====>:175
<=====title=====>:
SQL programming tool
<=====text=====>:
{{unreferenced|date=April 2010}}
In the field of [[software]], '''[[SQL]] programming tools''' provide platforms for [[database administrator]]s (DBAs) and [[application software|application]] developers to perform daily tasks efficiently and accurately.

Database administrators and application developers often face constantly changing environments  which they rarely completely control. Many changes result from new development projects or from modifications to existing code, which, when deployed to production, do not always produce the expected result.

For organizations to better manage development projects and the [[team]]s that develop code, suppliers of SQL programming tools normally provide more than facility to the database administrator or application developer to aid in database management and in quality [[Software deployment|code-deployment]] practices.

==Features==

SQL programming tools may include the following features:

===SQL editing===

SQL editors allow users to edit and execute SQL statements. They may support the following features:

* cut, copy, paste, undo, redo, find (and replace), bookmarks
* block indent, print, save file, uppercase/lowercase
* keyword highlighting
* auto-completion
* access to frequently used files
* output of query result
* editing query-results
* committing and rolling-back transactions
* inside cut paper

===Object browsing===

Tools may display information about [[database object]]s relevant to developers or to database administrators. Users may:

* view object descriptions
* view object [[Data Definition Language|definition]]s (DDL)
* create database objects
* enable and disable [[database trigger|trigger]]s and [[database constraints|constraint]]s
* recompile valid or invalid objects
* query or edit [[Table (database)|table]]s and [[View (database)|view]]s

Some tools also provide features to display dependencies among objects, and allow users to expand these dependent objects recursively (for example: packages may reference views, views generally reference tables, super/subtypes, and so on).

===Session browsing===

Database administrators and application developers can use session browsing tools to view the current activities of each user in the database. They can check the resource-usage of individual users, statistics information, locked objects and the current running SQL of each individual session.

===User-security management===

DBAs can create, edit, delete, disable or enable user-accounts in the database using security-management tools. DBAs can also assign [[database role|role]]s, system [[Privilege (computing)|privilege]]s, object privileges, and [[database storage|storage]]-quotas to users.

===Debugging===

Some tools offer features for the debugging of [[stored procedure]]s: [[program animation|Step In]], Step Over, Step Out, Run Until Exception, [[Breakpoint]]s, View & Set Variables, View Call Stack, and so on. Users can debug any program-unit without making any modification to it, including triggers and [[object type]]s.

===Performance monitoring===

Monitoring tools may show the database resources — usage summary, service time summary, recent activities, top sessions, session history or top SQL — in easy-to-read graphs. Database administrators can easily monitor the health of various components in the monitoring instance. Application developers may also make use of such tools to diagnose and correct application-performance problems as well as improve SQL server performance.

===Test Data===

Test data generation tools can populate the database by realistic test data for server or client side testing purposes. Also, this kind of software can upload sample BLOB files to database.

==See also==
* [[Comparison of database tools]]

{{DEFAULTSORT:Sql Programming Tool}}
[[Category:Data management]]
[[Category:Relational database management systems]]
<=====doc_Id=====>:178
<=====title=====>:
Lean integration
<=====text=====>:
'''Lean integration''' is a [[management system]] that emphasizes creating value for customers, continuous improvement, and eliminating waste as a sustainable [[data integration]] and [[system integration]] practice.  Lean integration has parallels with other lean disciplines such as [[lean manufacturing]], [[lean IT]], and [[lean software development]].  It is a specialized collection of tools and techniques that address the unique challenges associated with seamlessly combining information and processes from systems that were independently developed, are based on incompatible data models, and remain independently managed, to achieve a cohesive holistic operation.

==History==

Lean integration was first introduced by John Schmidt in a series of blog articles starting in January 2009 entitled 10 Weeks To Lean Integration.<ref>[http://blogs.informatica.com/perspectives/index.php/2009/01/14/10-weeks-to-lean-integration/ Original Lean Integration Blog Series]</ref>  This was followed by a white paper<ref>[http://www.cloudyintegration.com/uploads/LEAN_INTEGRATION_AFE_-_John_Schmidt.pdf Lean Integration White Paper]</ref> on the topic in April 2009 and the book ''Lean Integration, An Integration Factory Approach to Business Agility'' <ref name="Schmidt">John G.Schmidt, David Lyle (2010) ''Lean Integration: An Integration Factory Approach to Business Agility'', Addison-Wesley Pearson Education, ISBN 0-321-71231-5</ref> in May 2010.

==Overview==

Lean integration builds on the same set of principles that were developed for [[lean manufacturing]] and [[lean software development]] which is based on the [[Toyota Production System]]. Integration solutions can be broadly categorized as either Process Integration or Data Integration.  

The book<ref name="Schmidt"/> is based on the premise that Integration is an ongoing activity and not a one-time activity;  therefore integration should be viewed as a long term strategy for an organization.  John Schmidt and David Lyle initially articulated in their book the reasons for maintaining an efficient and sustainable integration team.  Lean integration as an integration approach must be ''sustainable'' and ''holistic'' unlike other integration approaches that either tackle only a part of the problem or tackle the problem for a short period of time.  Lean integration drives elimination of waste by adopting reusable elements, high automation and quality improvements.  Lean is a data-driven, fact-based methodology that relies on metrics to ensure that the quality and performance are maintained at a high level. 

An organizational focus is required for the implementation of lean integration principles. The predominant organizational model is the [[Integration Competency Center]] which may be structured as a central group or a more loosely coupled federated team.

==Lean integration principles==

The principles of Lean Integration may at first glance appear similar to that of [[Six Sigma]] but there are some very clear differences between them.  Six-Sigma is an ''analytical technique'' that focuses on quality and reduction of defects while Lean is a ''management system'' that focuses on delivering value to the end customer by continuously improving value delivery processes.  Lean provides a robust framework that facilitates improving efficiency and effectiveness by focusing on critical customer requirements.

As mentioned in lean integration there are seven core ''lean integration principles'' vital for deriving significant and sustainable business benefits. They are as below: 

# Focus on the customer and eliminate waste: Waste elimination should be viewed from the customer perspective and all activities that do not add value to the customer needs to be looked at closely and eliminated or reduced. In an integration context, the customer is often an internal sponsor or group within an organization that uses, benefits from, or pays for, the integrated capabilities.
# Continuously improve: A data driven cycle of hypothesis-validation-implementation should be used to drive innovation and continuously improve the end-to-end process.  Adopting and institutionalizing lessons learned and sustaining integration knowledge are related concepts that assist in the establishment of this principle.
# Empower the team: Creating cross-functional teams and sharing commitments across individuals empower the teams and individuals who have a clear understanding of their roles and the needs of their customers.  The team is also provided the support by senior management to innovate and try new ideas without fear of failure.
# Optimize the whole: Adopt a big-picture perspective of the end-to-end process and optimize the whole to maximize the customer value.  This may at times require performing individual steps and activities that appear to be sub-optimal when viewed in isolation, but aid in streamlining the end-to-end process.
# Plan for change: Application of mass customization techniques like leveraging automated tools, structured processes, and reusable and parameterized integration elements leads to reduction in cost and time in both the build and run stages of the integration life-cycle. Another key technique is a middleware services layer that presents applications with enduring abstractions of data through standardized interfaces, allowing the underlying data structures to change without necessarily impacting the dependent applications.
# Automate processes: Automation of tasks increases the ability to respond to large integration projects as effectively as small changes. In its ultimate form, automation eliminates integration dependencies from the critical implementation path of projects.
# Build quality in : Process excellence is emphasized and quality is built in rather than inspected in. A key metric for this principle is First Time Through (FTT) percentage which is a measure of the number of times an end-to-end process is executed without having to do any rework or repeat any of the steps.

==Benefits of lean integration==

The Lean integration practices transforms integration from an ''art'' into a ''science'', a repeatable and teachable methodology that shifts the focus from integration as a point-in-time activity to integration as a sustainable activity that enables organizational agility.  Once an organization adopts the integration as a science it enhances the organization’s ability to change rapidly without comprising on the IT risk or quality thereby transforming the organization into an agile data driven enterprise.  The following are the advantages derived by adopting the lean integration practices:

# Efficiency: typical improvements are in the scale of 50% labor productivity improvements and 90% lead-time reduction through continuous efforts to eliminate waste.
# Agility: Reusable components, highly automated processes and self-service delivery models improve the agility of the organization.
# Data quality: quality and reliability of data is enhanced and data becomes a real asset.
# Governance: metrics are established that drive continuous improvement.
# Innovation: innovation is facilitated by using fact-based approach.
# Staff Morale: IT staff is kept engaged with high morale driving bottom-up improvements.

==See also==

* [[Integration Competency Center]]
* [[Lean software development]]
* [[Lean IT]]
* [[Data Integration]]
* [[Toyota Production System]]

==References==

<references/>

==External links==
* [http://www.integrationfactory.com Lean Integration book microsite]
* [http://blogs.informatica.com/perspectives/index.php/2010/04/06/health-care-is-ready-for-lean-integration/ Application of Lean Integration to Health Care]
* [http://www.informatica.com/news_events/press_releases/Pages/02082010_lean.aspx  Press Release about Lean Integration Book]
* [http://www.linkedin.com/in/johnschmidt John Schmidt profile]
* [http://www.linkedin.com/in/davelyle David Lyle profile]
* [http://my.safaribooksonline.com/9780321712363 Lean Integration book publisher website]
* [http://www.baselinemag.com/c/a/IT-Management/How-IT-Runs-Lean-419352/ Slide show overview of Lean Integration]
* [http://www.linkedin.com/groups?gid=2302506 LinkedIn Group for Lean Integration Community]
* [http://www.itbusinessedge.com/cm/blogs/vizard/making-the-case-for-lean-integration/?cs=42547 Book review by Mike Vizard of ITBusinessEdge]
* [http://www.bcs.org/server.php?show=conBlogPost.1685 Book review by John Morris]
* [http://www.itbusinessedge.com/cm/blogs/lawson/lean-principles-can-make-it-better-at-integration/?cs=42041&utm_source=itbe&utm_medium=email&utm_campaign=EEB&nr=EEB John Schmidt and David Lyle Interview by Loraine Lawson]
* [http://www.insurancenetworking.com/blogs/insurance_technology_Lean_IT_manufacturing-25138-1.html Book review by Joe McKendrick]
* [http://www.information-management.com/dmradio/-10017194-1.html David Lyle Interview on DM Radio]

[[Category:Data management]]
[[Category:Software development philosophies]]
[[Category:Agile software development]]
[[Category:Information technology]]
[[Category:Quality]]
<=====doc_Id=====>:181
<=====title=====>:
Novell File Reporter
<=====text=====>:
{{Infobox software
|name                       = Novell File Reporter
|logo                       =
|screenshot                 =
|caption                    =
|collapsible                =
|author                     =
|developer                  = [[Novell]]
|released                   = {{Start date|2010|01}}
|discontinued               =
|latest release version     = 2.6.1
|latest release date        = {{Start date|2015|10|02}}
|latest preview version     =
|latest preview date        = <!-- {{Start date|YYYY|MM|DD}} -->
|frequently updated         =
|programming language       =
|operating system           =
|platform                   =
|size                       =
|language                   =
|status                     =
|genre                      = [[System Software]]
|license                    =
|website                    = [http://www.novell.com/products/file-reporter/ Novell File Reporter]
}}

'''Novell File Reporter''' (a.k.a. '''NFR''') is software that allows network administrators to identify files stored on the network and generates reports regarding the size of individual files, [[File format | file type]], when files were last accessed, and where duplicates exist. Additionally, the File Reporter tracks storage volume capacity and usage. It is a component of the [[Novell File Management Suite]].

==How It Works==

Novell File Reporter examines and reports on terabytes of data via a central reporting engine (NFR Engine) and distributed agents (NFR Agents). <ref>{{Citation| title = Novell File Reporter Reports on Terabytes of Data | url= http://www.novell.com/products/file-reporter/terabytes_data.html | accessdate = 31 July 2010}}</ref> The NFR Engine schedules the scans of file instances conducted by NFR Agents, processes and compiles the scans for reporting purposes, and provides report information to the user interface. 

In addition to the standard reports <ref>{{Citation| title = Novell File Report Standard Reports | url= http://www.novell.com/products/file-reporter/standard_reports.html | accessdate = 31 July 2010}}</ref> it can generate, the NFR Engine can also produce "trigger reports" in response to specific events (a server volume crossing a capacity threshold, for example). Accordingly, the NFR Engine monitors the data gathered by the NFR Agents in order to identify these "triggers."

The NFR Engine when working in either [[Novell eDirectory | eDirectory]] or [[Active Directory]] connects to the directory via a Directory Services Interface (DSI) and thus can monitor and check file permissions.<ref>{{Citation | last = Huber | first= Matthias | journal= Linux Magazine | title= Novell File Management Suite Optimizes Storage | date= 25 January 2010| url=http://www.linux-magazine.com/Online/News/Novell-File-Management-Suite-Optimizes-Storage | accessdate= 31 July 2010}}</ref>

==References==
{{Reflist}}

==External links==
*[http://www.novell.com/products/file-reporter/technicalinfo/ Novell File Reporter: Product page] Overview, features, and technical information
*[http://www.novell.com/documentation/filereporter2/ Novell File Reporter: Documentation]
*[http://www.filereportersupport.com/nfr/ Novell File Reporter: Support]

{{Novell}}

[[Category:Novell]]
[[Category:Novell software]]
[[Category:Storage software]]
[[Category:Data management]]
<=====doc_Id=====>:184
<=====title=====>:
ISO 8000
<=====text=====>:
'''[[International Organization for Standardization|ISO]] 8000''', the global standard for ''[[Data Quality and Enterprise Master Data]]'', describes the features and defines the requirements for the Data Quality and Portability of Enterprise Master Data.  Master Data is typically "internal" business information about clients, products and operations.  The standard is currently under development, but is quickly being adopted by many Fortune 500 corporations and certain public agencies involved in the regulation and supervision of financial markets around the world. ISO 8000 is one of the emerging technology standards that large and complex organizations are turning to in order to improve business processes and control operational costs.  The standard will be published as a number of separate documents, which [[International Organization for Standardization|ISO]] calls "parts".

ISO 8000 is being developed by [[ISO TC 184/SC 4|ISO technical committee TC 184, ''Automation systems and integration'', sub-committee SC 4, ''Industrial data'']]. Like other [[International Organization for Standardization|ISO]] and [[International Electrotechnical Commission|IEC]] standards, ISO 8000 is copyrighted and is not freely available.<ref>[http://www.iso.org/iso/copyright.htm ISO copyright policy]</ref>

== Published parts ==

The following part has already been published:

* ISO/TS 8000-1:2011, ''Data quality &mdash; Part 1: Overview''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50798 ISO catalogue page for ISO/TS 8000-1:2011]</ref>
* ISO 8000-2:2012, ''Data quality &mdash; Part 2: Vocabulary''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=57436 ISO catalogue page for ISO 8000-2:2012]</ref>
* ISO 8000-61:2016, ''Data quality &mdash; Part 61: Data quality management: Process reference model''<ref>[http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=63086 ISO catalogue page for ISO-61:2016]</ref>
* ISO/TS 8000-100:2009, ''Data quality &mdash; Part 100: Master data: Exchange of characteristic data: Overview''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=52129 ISO catalogue page for ISO/TS 8000-100:2009]</ref>
* ISO 8000-102:2009, ''Data quality &mdash; Part 102: Master data: Exchange of characteristic data: Vocabulary''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50799 ISO catalogue page for ISO 8000-102:2009]</ref>
* ISO 8000-110:2009, ''Data quality — Part 110: Master data: Exchange of characteristic data: Syntax, semantic encoding, and conformance to data specification''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51653 ISO catalogue page for ISO 8000-110:2009]</ref>
* ISO/TS 8000-120:2009, ''Data quality &mdash; Part 120: Master data: Exchange of characteristic data: Provenance''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50801 ISO catalogue page for ISO/TS 8000-120:2009]</ref>
* ISO/TS 8000-130:2009, ''Data quality &mdash; Part 130: Master data: Exchange of characteristic data: Accuracy''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=50802 ISO catalogue page for ISO/TS 8000-130:2009]</ref>
* ISO/TS 8000-140:2009, ''Data quality &mdash; Part 140: Master data: Exchange of characteristic data: Completeness''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-140:2009]</ref>
* ISO/TS 8000-150:2011, ''Data quality &mdash; Part 150: Master data: Quality management framework''<ref>[http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=53589 ISO catalogue page for ISO/TS 8000-150:2011]</ref>

== Further reading ==

{{Citation | last=Benson | first=Peter | journal=Real-World Decision Support (RWDS) Journal | year=2009 | volume=3 | issue=4 | title=ISO 8000 Data Quality — The Fundamentals, Part 1 | url=http://www.ewsolutions.com/resource-center/rwds_folder/rwds-archives/issue.2009-10-12.0790666855/document.2009-10-12.3367922336/view?searchterm=ISO%208000}}

{{Citation | last=Benson | first=Peter | title=NATO Codification System as the foundation for ISO 8000, the International Standard for data quality (Oil IT Journal)| year=2008 | url=http://www.oilit.com/papers/Benson.pdf}}

{{Citation | last=Benson | first=Peter | title=ISO 8000 &mdash; A new international standard for data quality | year=2009 | url=http://www.dataqualitypro.com/data-quality-home/iso-8000-a-new-international-standard-for-data-quality.html}}

{{Citation | last=Benson | first=Peter | title=Peter Benson discusses the certification options of ISO 8000 (Live Recording) | year=2010 | url=http://www.dataqualitypro.com/data-quality-home/peter-benson-discusses-the-certification-options-of-iso-8000.html}}

{{Citation | last=Grantner | first=Emily | title=ISO 8000 &mdash; A Standard for data quality | year=2007 | issue=Oct.-Dec. | journal=Logistics Spectrum | url=http://www.highbeam.com/doc/1P3-1518467381.html}}

{{Citation | last=West | first=Matthew | title=ISO 8000 &mdash; the Emerging Standard for Data Quality | year=2009 | journal=IAIDQ's Information and Data Quality Newsletter | volume=5 | issue=3 | url=http://iaidq.org/publications/west-2009-07.shtml}} (full article requires no cost registration to access)

== References ==
<references/>

{{ISO standards}}

{{DEFAULTSORT:Iso 8000}}
[[Category:ISO standards|#08000]]
[[Category:Data management]]
<=====doc_Id=====>:187
<=====title=====>:
Signed overpunch
<=====text=====>:
{{Refimprove|date=March 2008}}
A '''signed overpunch''' is a code used to store the [[Sign (mathematics)|sign]] of a number by changing the last digit. It is used in [[COBOL]], especially when using [[EBCDIC]]. Its purpose is to save a character that would otherwise be used by the sign digit.<ref name="EncycArk">{{Cite web|url=http://www.3480-3590-data-conversion.com/article-signed-fields.html |title=Tech Talk, COBOL Tutorials, EBCDIC to ASCII Conversion of Signed Fields |accessdate=2008-03-15}}</ref>  The code is derived from the [[punched card#IBM 80-column punched card formats and character codes|Hollerith Punched Card Code]], where both a digit and a sign can be entered in the same card column.

==The codes==
{| class="wikitable" style="text-align:center"
! Code !! Digit !! Sign
|-
| } || 0 || &minus;
|-
| J || 1 || &minus;
|-
| K || 2 || &minus;
|-
| L || 3 || &minus;
|-
| M || 4 || &minus;
|-
| N || 5 || &minus;
|-
| O || 6 || &minus;
|-
| P || 7 || &minus;
|-
| Q || 8 || &minus;
|-
| R || 9 || &minus;
|-
| { || 0 || +
|-
| A || 1 || +
|-
| B || 2 || +
|-
| C || 3 || +
|-
| D || 4 || +
|-
| E || 5 || +
|-
| F || 6 || +
|-
| G || 7 || +
|-
| H || 8 || +
|-
| I || 9 || +
|}

==Examples==
10} is -100<BR>
45A is 451

Decimal points are usually implied and not explicitly stated in the text. Using numbers with two decimal digits:

1000} is -100.00

==References==
{{Reflist}}

{{DEFAULTSORT:Signed Overpunch}}
[[Category:Computer programming]]
[[Category:Punched card]]
[[Category:Data management]]
[[Category:History of software]]
<=====doc_Id=====>:190
<=====title=====>:
Enterprise manufacturing intelligence
<=====text=====>:
{{unreferenced|date=February 2013}}
'''Enterprise manufacturing intelligence (EMI),''' or simply manufacturing intelligence (MI), is a term which applies to software used to bring a corporation's manufacturing-related data together from many sources for the purposes of reporting, analysis, visual summaries, and passing data between enterprise-level and plant-floor systems. As data is combined from multiple sources, it can be given a new structure or context that will help users find what they need regardless of where it came from.  The primary goal is to turn large amounts of manufacturing data into real knowledge and drive business results based on that knowledge.{{reflist}}

== Core functions ==

[[AMR Research]] has identified five core functions every Enterprise Manufacturing Intelligence application should possess:

* '''Aggregation:''' Making available data from many sources, most often databases.
* '''Contextualization:''' Providing a structure, or model, for the data that will help users find what they need.  Usually a folder tree utilizing a hierarchy such as the [[ISA-95]] standard.
* '''Analysis:''' Enabling users to analyze data across sources and especially across production sites.  This often includes the ability for true ''ad hoc'' reporting.
* '''Visualization:''' Providing tools to create visual summaries of the data to alert decision makers and call attention to the most important information of the moment.  The most common visualization tool is the [[Dashboard (business)|dashboard.]]
* '''Propagation:''' Automating the transfer of data from the plant-floor up to enterprise-level systems or vice versa.

{{DEFAULTSORT:Enterprise Manufacturing Intelligence}}
[[Category:Data management]]
<=====doc_Id=====>:193
<=====title=====>:
Category:NoSQL
<=====text=====>:
{{Cat main|NoSQL}}

[[Category:Databases]] 
[[Category:Data management]]
[[Category:Structured storage]]
<=====doc_Id=====>:196
<=====title=====>:
Grid-oriented storage
<=====text=====>:
{{primary sources|article|date=March 2009}}

'''Grid-oriented Storage''' ('''GOS''') was a term used for data storage by a university project during the era when the term [[grid computing]] was popular.

== Description ==
GOS was a successor of the term [[network-attached storage]] (NAS). GOS systems contained hard disks, often [[RAID]]s (redundant arrays of independent disks), like traditional file servers. 
[[Image:gosongrid.jpg |thumb |upright=1.4]]

GOS was designed to deal with long-distance, cross-domain and single-image file operations, which is typical in Grid environments. GOS behaves like a file server via the file-based GOS-FS protocol to any entity on the grid. Similar to [[Advanced Resource Connector|GridFTP]], GOS-FS integrates a parallel stream engine and [[Grid Security Infrastructure]] (GSI). 

Conforming to the universal VFS (Virtual Filesystem Switch), GOS-FS can be pervasively used as an underlying platform to best utilize the increased transfer bandwidth and accelerate the [[Network File System (protocol)|NFS]]/[[CIFS]]-based applications. GOS can also run over [[SCSI]], [[Fibre Channel]] or [[iSCSI]], which does not affect the acceleration performance, offering both file level protocols and block level protocols for [[storage area network]] (SAN) from the same system.

In a grid infrastructure, resources may be geographically distant from each other, produced by differing manufacturers, and have differing access control policies. This makes access to grid resources dynamic and conditional upon local constraints. Centralized management techniques for these resources are limited in their scalability both in terms of execution efficiency and fault tolerance. Provision of services across such platforms requires a distributed resource management mechanism and the peer-to-peer clustered GOS appliances allow a single storage image to continue to expand, even if a single GOS appliance reaches its capacity limitations. The cluster shares a common, aggregate presentation of the data stored on all participating GOS appliances. Each GOS appliance manages its own internal storage space. The major benefit of this aggregation is that clustered GOS storage can be accessed by users as a single mount point. 

GOS products fit the thin-server categorization. Compared with traditional “fat server”-based storage architectures, thin-server GOS appliances deliver numerous advantages, such as the alleviation of potential network/grid bottle-necks, CPU and OS optimized for I/O only, ease of installation, remote management and minimal maintenance, low cost and Plug and Play, etc. Examples of similar innovations include NAS, printers, fax machines, routers and switches.

An [[Apache server]] has been installed in the GOS operating system, ensuring an HTTPS-based communication between the GOS server and an administrator via a Web browser. Remote management and monitoring makes it easy to set up, manage, and monitor GOS systems.

== History ==
[[Frank Zhigang Wang]] and Na Helian proposed a funding proposal to the UK government titled “Grid-Oriented Storage (GOS): Next Generation Data Storage System Architecture for the Grid Computing Era” in 2003. The proposal was approved and granted one million pounds{{citation needed|date=March 2009}} in 2004. The first prototype was constructed in 2005 at Centre for Grid Computing, Cambridge-Cranfield High Performance Computing Facility. The first conference presentation was at IEEE Symposium on Cluster Computing and Grid (CCGrid), 9–12 May 2005, Cardiff, UK. As one of the five best work-in-progress, it was included in the IEEE Distributed Systems Online. In 2006, the GOS architecture and its implementations was published in IEEE Transactions on Computers, titled “Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture”.  
Starting in January 2007, demonstrations were presented at [[Princeton University]], Cambridge University Computer Lab and others.
By 2013, the Cranfield Centre still used future tense for the project.<ref name="Cranfield CGC">{{cite web |url= http://www.cranfield.ac.uk/soe/departments/appliedmaths/gridcomputing/index.html |title= Centre for Grid Computing |accessdate= June 14, 2013 |publisher=Cranfield University}} <!--  --></ref>

[[Peer-to-peer file sharing]]s use similar techniques.

==Notes==
{{reflist}}

==Further reading==
* Frank Wang, Na Helian, Sining Wu, Yuhui Deng, Yike Guo, Steve Thompson, Ian Johnson, Dave Milward & Robert Maddock, Grid-Oriented Storage, IEEE Distributed Systems Online,  Volume 6,  Issue 9, Sept. 2005.
* Frank Wang, Sining Wu, Na Helian, Andy Parker, Yike Guo, Yuhui Deng, Vineet Khare, Grid-oriented Storage: A Single-Image, Cross-Domain, High-Bandwidth Architecture, IEEE Transaction on Computers, Vol.56, No.4, pp.&nbsp;474–487, 2007.
* Frank Zhigang Wang, Sining Wu, Na Helian, An Underlying Data-Transporting Protocol for Accelerating Web Communications, International Journal of Computer Networks, Elsevier, 2007.
* Frank Zhigang Wang, Sining Wu, Na Helian, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Data Access to Nucleotide Sequence Database with 6x Improvement in Response Times, New Generation Computing, No.2, Vol.25, 2007.
* Frank Wang, Yuhui Deng, Na Helian, Evolutionary Storage: Speeding up a Magnetic Disk by Clustering Frequent Data, IEEE Transactions on Magnetics, Issue.6, Vol.43, 2007.
* Frank Zhigang Wang, Na Helian, Sining Wu, Yuhui Deng, Vineet Khare, Chris Thompson and Michael Parker, Grid-based Storage Architecture for Accelerating Bioinformatics Computing, Journal of VLSI Signal Processing Systems, No.1, Vol.48, 2007.
* Yuhui Deng and   Frank Wang, A Heterogeneous Storage Grid Enabled by Grid Service, ACM Operating System Review, No.1, Vol.41, 2007.
* Yuhui Deng & Frank Wang, Optimal Clustering Size of Small File Access in Network Attached Storage Device, Parallel Processing Letters, No.1, Vol.17, 2007.

{{DEFAULTSORT:Grid-Oriented Storage}}
[[Category:Data management]]
<=====doc_Id=====>:199
<=====title=====>:
Commitment ordering
<=====text=====>:
{{multiple issues|
{{expert subject|computer science|date=October 2012|reason=it is impossible to copy edit the article in its current state}}
{{notability|date=December 2011}}
{{more footnotes|date=November 2011}}
{{technical|date=November 2011}}
{{essay-like|date=November 2011}}
}}

'''Commitment ordering''' ('''CO''') is a class of interoperable ''[[serializability]]'' techniques in [[concurrency control]] of [[database]]s, [[transaction processing]], and related applications. It allows [[Serializability#Optimistic versus pessimistic techniques|optimistic]] (non-blocking) implementations. With the proliferation of [[multi-core processor]]s, CO has been also increasingly utilized in [[Concurrent computing|concurrent programming]], [[transactional memory]], and especially in [[software transactional memory]] (STM) for achieving serializability [[Optimistic concurrency control|optimistically]]. CO is also the name of the resulting transaction [[Schedule (computer science)|schedule]] (history) property, which was originally defined in 1988 with the name ''dynamic atomicity''.<ref name=Fekete1988>Alan Fekete, [[Nancy Lynch]], Michael Merritt, William Weihl (1988): [http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA200980&Location=U2&doc=GetTRDoc.pdf ''Commutativity-based locking for nested transactions'' (PDF)] MIT, LCS lab, Technical report MIT/LCS/TM-370, August 1988.</ref> In a CO compliant schedule the chronological order of commitment events of transactions is compatible with the [[Serializability#Testing conflict serializability|precedence]] order of the respective transactions. CO is a broad special case of ''[[serializability#View and conflict serializability|conflict serializability]]'', and effective means ([[Reliability engineering|reliable]], high-performance, distributed, and [[Scalability|scalable]]) to achieve [[global serializability]] (modular serializability) across any collection of database systems that possibly use different concurrency control mechanisms (CO also makes each system serializability compliant, if not already).

Each not-CO-compliant database system is augmented with a CO component (the commitment order coordinator—COCO) which orders the commitment events for CO compliance, with neither data-access nor any other transaction operation interference. As such CO provides a low overhead, general solution for global serializability (and distributed serializability), instrumental for [[global concurrency control]] (and [[distributed concurrency control]]) of multi database systems and other [[transactional object]]s, possibly highly distributed (e.g., within [[cloud computing]], [[grid computing]], and networks of [[smartphone]]s). An [[atomic commitment protocol]] (ACP; of any type) is a fundamental part of the solution, utilized to break global cycles in the conflict (precedence, serializability) graph. CO is the most general property (a [[necessary condition]]) that guarantees global serializability, if the database systems involved do not share concurrency control information beyond atomic commitment protocol (unmodified) messages, and have no knowledge whether transactions are global or local (the database systems are ''autonomous''). Thus CO (with its variants) is the only general technique that does not require the typically costly distribution of local concurrency control information (e.g., local precedence relations, locks, timestamps, or tickets). It generalizes the popular ''[[Two-phase locking|strong strict two-phase locking]]'' (SS2PL) property, which in conjunction with the ''[[two-phase commit protocol]]'' (2PC) is the [[de facto standard]] to achieve global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join such SS2PL based solutions for global serializability.

In addition, locking based ''global deadlocks'' are resolved automatically in a CO based multi-database environment, an important side-benefit (including the special case of a completely SS2PL based environment; a previously unnoticed fact for SS2PL).

Furthermore, '''strict commitment ordering''' (SCO; [[#Raz1991c|Raz 1991c]]), the intersection of ''[[Schedule (computer science)#Strict|Strictness]]'' and CO, provides better performance (shorter average transaction completion time and resulting better transaction [[throughput]]) than SS2PL whenever read-write conflicts are present (identical blocking behavior for write-read and write-write conflicts; comparable locking overhead). The advantage of SCO is especially significant during lock contention. Strictness allows both SS2PL and SCO to use the same effective ''database recovery'' mechanisms.

Two major generalizing variants of CO exist, '''extended CO''' (ECO; [[#Raz1993a|Raz 1993a]]) and '''multi-version CO''' (MVCO; [[#Raz1993b|Raz 1993b]]). They as well provide global serializability without local concurrency control information distribution, can be combined with any relevant concurrency control, and allow optimistic (non-blocking) implementations. Both use additional information for relaxing CO constraints and achieving better concurrency and performance. '''Vote ordering''' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]) is a container schedule set (property) and technique for CO and all its variants. Local VO is a necessary condition for guaranteeing global serializability, if the atomic commitment protocol (ACP) participants do not share concurrency control information (have the ''generalized autonomy'' property). CO and its variants inter-operate transparently, guaranteeing global serializability and automatic global deadlock resolution also together in a mixed, heterogeneous environment with different variants.

==Overview==

The ''Commitment ordering'' (CO; [[#Raz1990|Raz 1990]], [[#Raz1992|1992]], [[#Raz1994|1994]], [[#Raz2009|2009]]) schedule property has been referred to also as ''Dynamic atomicity'' (since 1988<ref name=Fekete1988/>), ''commit ordering'', ''commit order serializability'',  and ''strong recoverability'' (since 1991). The latter is a misleading name since CO is incomparable with ''[[serializability#Correctness - recoverability|recoverability]]'', and the term "strong" implies a special case. This means that a schedule with a strong recoverability property does not necessarily have the CO property, and vice versa.

In 2009 CO has been characterized as a major concurrency control method, together with the previously known (since the 1980s) three major methods: ''Locking'', ''Time-stamp ordering'', and ''Serialization graph testing'', and as an enabler for the interoperability of systems using different concurrency control mechanisms.<ref name=Bern2009>[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (pages 145, 360)</ref>

In a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple and possibly [[Distributed database]]s. Enforcing [[global serializability]] in such system is problematic. Even if every local schedule of a single database is serializable, still, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach conflict serializability would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. The problem of achieving global serializability effectively had been characterized as [[open problem|open]] until the public disclosure of CO in 1991 by its [[Invention|inventor]] [[Yoav Raz]] ([[#Raz1991a|Raz 1991a]]; see also [[Global serializability]]).

Enforcing CO is an effective way to enforce conflict serializability globally in a distributed system, since enforcing CO locally in each database (or other transactional object) also enforces it globally. Each database may use any, possibly different, type of concurrency control mechanism. With a local mechanism that already provides conflict serializability, enforcing CO locally does not cause any additional aborts, since enforcing CO locally does not affect the data access scheduling strategy of the mechanism (this scheduling determines the serializability related aborts; such a mechanism typically does not consider the commitment events or their order). The CO solution requires no communication overhead, since it uses (unmodified) ''[[atomic commitment]]'' protocol messages only, already needed by each distributed transaction to reach atomicity. An atomic commitment protocol plays a central role in the distributed CO algorithm, which enforces CO globally, by breaking global cycles (cycles that span two or more databases) in the global conflict graph.
CO, its special cases, and its generalizations are interoperable, and achieve global serializability while transparently being utilized together in a single heterogeneous distributed environment comprising objects with possibly different concurrency control mechanisms. As such, ''Commitment ordering'', including its special cases, and together with its generalizations (see CO variants below), provides a general, high performance, fully distributed solution (no central processing component or central data structure are needed) for guaranteeing global serializability in heterogeneous environments of multidatabase systems and other multiple transactional objects (objects with states accessed and modified only by transactions; e.g., in the framework of [[transactional processes]], and within Cloud computing and Grid computing). The CO solution scales up with network size and the number of databases without any negative impact on performance (assuming the statistics of a single distributed transaction, e.g., the average number of databases involved with a single transaction, are unchanged).

With the proliferation of [[Multi-core processor]]s, Optimistic CO (OCO) has been also increasingly utilized to achieve serializability in software transactional memory, and numerous STM articles and patents utilizing "commit order" have already been published (e.g., Zhang et al. 2006<ref name=Zhang2006/>).

==The commitment ordering solution for global serializability==

===General characterization of CO===

''Commitment ordering'' (CO) is a special case of conflict serializability. CO can be enforced with ''non-blocking'' mechanisms (each transaction can complete its task without having its data-access blocked, which allows [[optimistic concurrency control]]; however, commitment could be blocked). In a CO schedule the commitment events' ([[partial order|partial]]) precedence order of the transactions corresponds to the precedence (partial) order of the respective transactions in the ([[directed graph|directed]]) conflict graph (precedence graph, serializability graph), as induced by their conflicting access operations (usually read and write (insert/modify/delete) operations; CO also applies to higher level operations, where they are conflicting if [[noncommutative]], as well as to conflicts between operations upon multi-version data).

;Definition{{colon}} commitment ordering: Let <math>T_{1}, T_{2}</math> be two ''committed'' transactions in a schedule, such that <math>T_{2}</math> is ''in a conflict'' with <math>T_{1}</math> (<math>T_{1}</math> ''precedes'' <math>T_{2}</math>). The schedule has the '''Commitment ordering''' (CO) property, if for every two such transactions <math>T_{1}</math> commits before <math>T_{2}</math> commits.

The commitment decision events are generated by either a local commitment mechanism, or an atomic commitment protocol, if different processes need to reach consensus on whether to commit or abort. The protocol may be distributed or centralized. Transactions may be committed concurrently, if the commit partial order allows (if they do not have conflicting operations). If different conflicting operations induce different partial orders of same transactions, then the conflict graph has [[cycle (graph theory)|cycles]], and the schedule will violate serializability when all the transactions on a cycle are committed. In this case no partial order for commitment events can be found. Thus, cycles in the conflict graph need to be broken by aborting transactions. However, any conflict serializable schedule can be made CO without aborting any transaction, by properly delaying commit events to comply with the transactions' precedence partial order.

CO enforcement by itself is not sufficient as a concurrency control mechanism, since CO lacks the recoverability property, which should be supported as well.

===The distributed CO algorithm===

A fully distributed ''Global commitment ordering'' enforcement algorithm exists, that uses local CO of each participating database, and needs only (unmodified) Atomic commitment protocol messages with no further communication. The distributed algorithm is the combination of local (to each database) CO algorithm processes, and an atomic commitment protocol (which can be fully distributed).
Atomic commitment protocol is essential to enforce atomicity of each distributed transaction (to decide whether to commit or abort it; this procedure is always carried out for distributed transactions, independently of concurrency control and CO). A common example of an atomic commitment protocol is the ''[[two-phase commit protocol]]'', which is resilient to many types of system failure. In a reliable environment, or when processes usually fail together (e.g., in the same [[integrated circuit]]), a simpler protocol for atomic commitment may be used (e.g., a simple handshake of distributed transaction's participating processes with some arbitrary but known special participant, the transaction's coordinator, i.e., a type of ''one-phase commit'' protocol). An atomic commitment protocol reaches consensus among participants on whether to ''commit'' or ''abort'' a distributed (global) transaction that spans these participants. An essential stage in each such protocol is the '''YES vote''' (either explicit, or implicit) by each participant, which means an obligation of the voting participant to obey the decision of the protocol, either commit or abort. Otherwise a participant can unilaterally abort the transaction by an explicit NO vote. The protocol commits the transaction only if YES votes have been received from ''all'' participants, and thus typically a missing YES vote of a participant is considered a NO vote by this participant. Otherwise the protocol aborts the transaction. The various atomic commit protocols only differ in their abilities to handle different computing environment failure situations, and the amounts of work and other computing resources needed in different situations.

The entire CO solution for global serializability is based on the fact that in case of a missing vote for a distributed transaction, the atomic commitment protocol eventually aborts this transaction.

====Enforcing global CO====

In each database system a local CO algorithm determines the needed commitment order for that database. By the characterization of CO above, this order depends on the local precedence order of transactions, which results from the local data access scheduling mechanisms. Accordingly, YES votes in the atomic commitment protocol are scheduled for each (unaborted) distributed transaction (in what follows "a vote" means a YES vote). If a precedence relation (conflict) exists between two transactions, then the second will not be voted on before the first is completed (either committed or aborted), to prevent possible commit order violation by the atomic commitment protocol. Such can happen since the commit order by the protocol is not necessarily the same as the voting order. If no precedence relation exists, both can be voted on concurrently. This ''vote ordering strategy'' ensures that also the atomic commitment protocol maintains commitment order, and it is a ''necessary condition'' for guaranteeing Global CO (and the local CO of a database; without it both Global CO and Local CO (a property meaning that each database is CO compliant) may be violated).

However, since database systems schedule their transactions independently, it is possible that the transactions' precedence orders in two databases or more are not compatible (no global partial order exists that can [[Embedding|embed]] the respective local partial orders together). With CO precedence orders are also the commitment orders. When participating databases in a same distributed transaction do not have compatible local precedence orders for that transaction (without "knowing" it; typically no coordination between database systems exists on conflicts, since the needed communication is massive and unacceptably degrades performance) it means that the transaction resides on a global cycle (involving two or more databases) in the global conflict graph. In this case the atomic commitment protocol will fail to collect all the votes needed to commit that transaction: By the ''vote ordering strategy'' above at least one database will delay its vote for that transaction indefinitely, to comply with its own commitment (precedence) order, since it will be waiting to the completion of another, preceding transaction on that global cycle, delayed indefinitely by another database with a different order. This means a '''''voting-[[deadlock]]''''' situation involving the databases on that cycle.
As a result, the protocol will eventually abort some deadlocked transaction on this global cycle, since each such transaction is missing at least one participant's vote. Selection of the specific transaction on the cycle to be aborted depends on the atomic commitment protocol's abort policies (a [[timeout (telecommunication)|timeout]] mechanism is common, but it may result in more than one needed abort per cycle; both preventing unnecessary aborts and abort time shortening can be achieved by a dedicated abort mechanism for CO). Such abort will break the global cycle involving that distributed transaction. Both deadlocked transactions and possibly other in conflict with the deadlocked (and thus blocked) will be free to be voted on. It is worthwhile noting that each database involved with the voting-deadlock continues to vote regularly on transactions that are not in conflict with its deadlocked transaction, typically almost all the outstanding transactions. Thus, in case of incompatible local (partial) commitment orders, no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause of incompatibility. This means that the above ''vote ordering strategy'' is also a ''sufficient condition'' for guaranteeing Global CO.

The following is concluded:

*'''The Vote ordering strategy for Global CO Enforcing [[Theorem]]'''

:Let <math>T_{1}, T_{2}</math> be undecided (neither committed nor aborted) transactions in a database system that enforces CO for local transactions, such that <math>T_{2}</math> is ''global'' and ''in conflict'' with <math>T_{1}</math> (<math>T_{1}</math> ''precedes'' <math>T_{2}</math>). Then, having <math>T_{1}</math> ended (either committed or aborted) before <math>T_{2}</math> is voted on to be committed (the ''vote ordering strategy''), in each such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global CO (the condition guarantees Global CO, which may be violated without it).

:'''Comments:'''
# The ''vote ordering strategy'' that enforces global CO is referred to as <math>CD^3C</math> in ([[#Raz1992|Raz 1992]]).
#The Local CO property of a global schedule means that each database is CO compliant. From the necessity discussion part above it directly follows that the theorem is true also when replacing "Global CO" with "Local CO" when global transactions are present. Together it means that Global CO is guaranteed [[if and only if]] Local CO is guaranteed (which is untrue for Global conflict serializability and Local conflict serializability: Global implies Local, but not the opposite).

Global CO implies Global serializability.

The '''Global CO algorithm''' comprises enforcing (local) CO in each participating database system by ordering commits of local transactions (see [[Commitment ordering#Enforcing CO locally|Enforcing CO locally]] below) and enforcing the ''vote ordering strategy'' in the theorem above (for global transactions).

====Exact characterization of voting-deadlocks by global cycles====

The above global cycle elimination process by a '''voting deadlock''' can be explained in detail by the following observation:

First it is assumed, for simplicity, that every transaction reaches the ready-to-commit state and is voted on by at least one database (this implies that no blocking by locks occurs).
Define a ''"wait for vote to commit" graph'' as a directed graph with transactions as nodes, and a directed edge from any first transaction to a second transaction if the first transaction blocks the vote to commit of the second transaction (opposite to conventional edge direction in a [[wait-for graph]]). Such blocking happens only if the second transaction is in a conflict with the first transaction (see above). Thus this "wait for vote to commit" graph is identical to the global conflict graph. A cycle in the "wait for vote to commit" graph means a deadlock in voting. Hence there is a deadlock in voting if and only if there is a cycle in the conflict graph. Local cycles (confined to a single database) are eliminated by the local serializability mechanisms. Consequently, only global cycles are left, which are then eliminated by the atomic commitment protocol when it aborts deadlocked transactions with missing (blocked) respective votes.

Secondly, also local commits are dealt with: Note that when enforcing CO also waiting for a regular local commit of a local transaction can block local commits and votes of other transactions upon conflicts, and the situation for global transactions does not change also without the simplifying assumption above: The final result is the same also with local commitment for local transactions, without voting in atomic commitment for them.

Finally, blocking by a lock (which has been excluded so far) needs to be considered: A lock blocks a conflicting operation and prevents a conflict from being materialized. If the lock is released only after transaction end, it may block indirectly either a vote or a local commit of another transaction (which now cannot get to ready state), with the same effect as of a direct blocking of a vote or a local commit. In this case a cycle is generated in the conflict graph only if such a blocking by a lock is also represented by an edge. With such added edges representing events of blocking-by-a-lock, the conflict graph is becoming an ''augmented conflict graph''.

*'''Definition: augmented conflict graph'''

:An '''augmented conflict graph''' is a [[serializability#Testing conflict serializability|conflict graph]] with added edges: In addition to the original edges a directed edge exists from transaction <math>T_{1}</math> to transaction <math>T_{2}</math> if two conditions are met:
# <math>T_{2}</math> is blocked by a data-access lock applied by <math>T_{1}</math> (the blocking prevents the conflict of <math>T_{2}</math> with <math>T_{1}</math> from being materialized and have an edge in the regular conflict graph), and
# This blocking will not stop before <math>T_{1}</math> ends (commits or aborts; true for any locking-based CO)

:The graph can also be defined as the [[Union (set theory)|union]] of the (regular) ''conflict graph'' with the (reversed edge, regular) ''wait-for graph''

:'''Comments:'''
# Here, unlike the regular conflict graph, which has edges only for materialized conflicts, all conflicts, both materialized and non-materialized, are represented by edges.
# Note that all the new edges are all the (reversed to the conventional) edges of the ''wait-for graph''. The ''wait-for graph'' can be defined also as the graph of non-materialized conflicts. By the common conventions edge direction in a ''conflict graph'' defines time order between conflicting operations which is opposite to the time order defined by an edge in a ''wait-for graph''.
# Note that such global graph contains (has embedded) all the (reversed edge) regular local ''wait-for'' graphs, and also may include locking based global cycles (which cannot exist in the local graphs). For example, if all the databases on a global cycle are SS2PL based, then all the related vote blocking situations are caused by locks (this is the classical, and probably the only global deadlock situation dealt with in the database research literature). This is a global deadlock case where each related database creates a portion of the cycle, but the complete cycle does not reside in any local wait-for graph.

In the presence of CO the ''augmented conflict graph'' is in fact a (reversed edge) ''local-commit and voting wait-for graph'': An edge exists from a first transaction, either local or global, to a second, if the second is waiting for the first to end in order to be either voted on (if global), or locally committed (if local). All ''global cycles'' (across two or more databases) in this graph generate voting-deadlocks. The graph's global cycles provide complete characterization for voting deadlocks and may include any combination of materialized and non-materialized conflicts. Only cycles of (only) materialized conflicts are also cycles of the regular conflict graph and affect serializability. One or more (lock related) non-materialized conflicts on a cycle prevent it from being a cycle in the regular conflict graph, and make it a locking related deadlock. All the global cycles (voting-deadlocks) need to be broken (resolved) to both maintain global serializability and resolve global deadlocks involving data access locking, and indeed they are all broken by the atomic commitment protocol due to missing votes upon a voting deadlock.

'''Comment:''' This observation also explains the correctness of ''[[Commitment ordering#Extended CO (ECO)|Extended CO (ECO)]]'' below: Global transactions' voting order must follow the conflict graph order with vote blocking when order relation (graph path) exists between two global transactions. Local transactions are not voted on, and their (local) commits are not blocked upon conflicts. This results in same voting-deadlock situations and resulting global cycle elimination process for ECO.

The ''voting-deadlock'' situation can be summarized as follows:

*'''The CO Voting-Deadlock Theorem'''

:Let a multidatabase environment comprise CO compliant (which eliminates ''local cycles'') database systems that enforce, each, ''Global CO'' (using the condition in the theorem above). Then a ''voting-deadlock'' occurs if and only if a ''global cycle'' (spans two or more databases) exists in the ''Global augmented conflict graph'' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the ''global transactions'' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock); if a local transaction resides on the cycle, eventually it has its (local) commit blocked.

:'''Comment:''' A rare situation of a voting deadlock (by missing blocked votes) can happen, with no voting for any transaction on the related cycle by any of the database systems involved with these transactions. This can occur when local sub-transactions are [[Thread (computer science)|multi-threaded]]. The highest probability instance of such rare event involves two transactions on two simultaneous opposite cycles. Such global cycles (deadlocks) overlap with local cycles which are resolved locally, and thus typically resolved by local mechanisms without involving atomic commitment. Formally it is also a global cycle, but practically it is local (portions of local cycles generate a global one; to see this, split each global transaction (node) to local sub-transactions (its portions confined each to a single database); a directed edge exists between transactions if an edge exists between any respective local sub-transactions; a cycle is local if all its edges originate from a cycle among sub-transactions of the same database, and global if not; global and local can overlap: a same cycle among transactions can result from several different cycles among sub-transactions, and be both local and global).

Also the following locking based special case is concluded:

*'''The CO Locking-based Global-Deadlock Theorem'''

:In a CO compliant multidatabase system a locking-based global-deadlock, involving at least one data-access lock (non-materialized conflict), and two or more database systems, is a reflection of a global cycle in the ''Global augmented conflict graph'', which results in a voting-deadlock. Such cycle is not a cycle in the (regular) ''Global conflict graph'' (which reflects only materialized conflicts, and thus such cycle does not affect ''[[serializability]]'').

:'''Comments:'''
# Any blocking (edge) in the cycle that is not by a data-access lock is a direct blocking of either voting or local commit. All voting-deadlocks are resolved (almost all by ''Atomic commitment''; see comment above), including this locking-based type.
# Locking-based global-deadlocks can be generated also in a completely SS2PL-based distributed environment (special case of CO based), where all the vote blocking (and voting-deadlocks) are caused by data-access locks. Many research articles have dealt for years with resolving such global deadlocks, but none (except the CO articles) is known (as of 2009) to notice that ''atomic commitment'' automatically resolves them. Such automatic resolutions are regularly occurring unnoticed in all existing SS2PL based multidatabase systems, often bypassing dedicated resolution mechanisms.

Voting-deadlocks are the key for the operation of distributed CO.

Global cycle elimination (here voting-deadlock resolution by ''atomic commitment'') and resulting aborted transactions' re-executions are time consuming, regardless of concurrency control used. If databases schedule transactions independently, global cycles are unavoidable (in a complete analogy to cycles/deadlocks generated in local SS2PL; with distribution, any transaction or operation scheduling coordination results in autonomy violation, and typically also in substantial performance penalty). However, in many cases their likelihood can be made very low by implementing database and transaction design guidelines that reduce the number of conflicts involving a global transaction. This, primarily by properly handling hot spots (database objects with frequent access), and avoiding conflicts by using commutativity when possible (e.g., when extensively using counters, as in finances, and especially multi-transaction ''accumulation counters'', which are typically hot spots).

Atomic commitment protocols are intended and designed to achieve atomicity without considering database concurrency control. They abort upon detecting or [[Heuristic algorithm|heuristically]] finding (e.g., by timeout; sometimes mistakenly, unnecessarily) missing votes, and typically unaware of global cycles. These protocols can be specially enhanced for CO (including CO's variants below) both to prevent unnecessary aborts, and to accelerate aborts used for breaking global cycles in the global augmented conflict graph (for better performance by earlier release upon transaction-end of computing resources and typically locked data). For example, existing locking based global deadlock detection methods, other than timeout, can be generalized to consider also local commit and vote direct blocking, besides data access blocking. A possible compromise in such mechanisms is effectively detecting and breaking the most frequent and relatively simple to handle length-2 global cycles, and using timeout for undetected, much less frequent, longer cycles.

===Enforcing CO locally===

''Commitment ordering'' can be enforced locally (in a single database) by a dedicated CO algorithm, or by any algorithm/protocol that provides any special case of CO. An important such protocol, being utilized extensively in database systems, which generates a CO schedule, is the ''strong strict [[two phase locking]]'' protocol (SS2PL: "release transaction's locks only after the transaction has been either committed or aborted"; see below). SS2PL is a [[proper subset]] of the intersection of [[Two-phase locking|2PL]] and strictness.

====A generic local CO algorithm====

A '''generic local CO algorithm''' ([[#Raz1992|Raz 1992]]; Algorithm 4.1) is an algorithm independent of implementation details, that enforces exactly the CO property. It does not block data access (nonblocking), and consists of aborting a certain set of transactions (only if needed) upon committing a transaction. It aborts a (uniquely determined at any given time) minimal set of other undecided (neither committed, nor aborted) transactions that run locally and can cause serializability violation in the future (can later generate cycles of committed transactions in the conflict graph; this is the ABORT set of a committed transaction T; after committing T no transaction in ABORT at commit time can be committed, and all of them are doomed to be aborted). This set consists of all undecided transactions with directed edges in the conflict graph to the committed transaction. The size of this set cannot increase when that transaction is waiting to be committed (in ready state: processing has ended), and typically decreases in time as its transactions are being decided. Thus, unless [[Real-time computing|real-time]] constraints exist to complete that transaction, it is preferred to wait with committing that transaction and let this set decrease in size. If another serializability mechanism exists locally (which eliminates cycles in the local conflict graph), or if no cycle involving that transaction exists, the set will be empty eventually, and no abort of set member is needed. Otherwise the set will stabilize with transactions on local cycles, and aborting set members will have to occur to break the cycles. Since in the case of CO conflicts generate blocking on commit, local cycles in the ''augments conflict graph'' (see above) indicate local commit-deadlocks, and deadlock resolution techniques as in [[Serializability#Common mechanism - SS2PL|SS2PL]] can be used (e.g., like ''timeout'' and ''wait-for graph''). A local cycle in the ''augmented conflict graph'' with at least one non-materialized conflict reflects a locking-based deadlock. The local algorithm above, applied to the local augmented conflict graph rather than the regular local conflict graph, comprises the '''generic enhanced local CO algorithm''', a single local cycle elimination mechanism, for both guaranteeing local serializability and handling locking based local deadlocks. Practically an additional concurrency control mechanism is always utilized, even solely to enforce recoverability. The generic CO algorithm does not affect local data access scheduling strategy, when it runs alongside of any other local concurrency control mechanism. It affects only the commit order, and for this reason it does not need to abort more transactions than those needed to be aborted for serializability violation prevention by any combined local concurrency control mechanism. The net effect of CO may be, at most, a delay of commit events (or voting in a distributed environment), to comply with the needed commit order (but not more delay than its special cases, for example, SS2PL, and on the average significantly less).

The following theorem is concluded:

*'''The Generic Local CO Algorithm Theorem'''
:When running alone or alongside any concurrency control mechanism in a database system then
#The ''Generic local CO algorithm'' guarantees (local) CO (a CO compliant schedule).
#The ''Generic enhanced local CO algorithm'' guarantees both (local) CO and (local) locking based deadlock resolution.
: and (when not using ''timeout'', and no ''real-time'' transaction completion constraints are applied) neither algorithm aborts more transactions than the minimum needed (which is determined by the transactions' operations scheduling, out of the scope of the algorithms).

====Example: Concurrent programming and Transactional memory====
:See also ''[[The History of Commitment Ordering#Concurrent programming and Transactional memory|Concurrent programming and Transactional memory]]''

With the proliferation of Multi-core processors, variants of the Generic local CO algorithm have been also increasingly utilized in Concurrent programming, [[Transactional memory]], and especially in Software transactional memory for achieving serializability optimistically by "commit order" (e.g., Ramadan et al. 2009,<ref name=Ramadan2009>Hany E. Ramadan, Indrajit Roy, Maurice Herlihy, Emmett Witchel (2009): [http://portal.acm.org/citation.cfm?id=1504201 "Committing conflicting transactions in an STM"] ([http://www.cs.utexas.edu/~indrajit/pubs/ppopp121-ramadan.pdf PDF]) ''Proceedings of the 14th ACM SIGPLAN symposium on Principles and practice of parallel programming'' (PPoPP '09), ISBN 978-1-60558-397-6</ref> Zhang et al. 2006,<ref name=Zhang2006>Lingli Zhang, Vinod K.Grover, Michael M. Magruder, David Detlefs, John Joseph Duffy, Goetz Graefe (2006): [http://www.freepatentsonline.com/7711678.html  Software transaction commit order and conflict management] United States Patent 7711678, Granted 05/04/2010.</ref> von Parun et al. 2007<ref name=vonParun2007>Christoph von Praun, Luis Ceze, Calin Cascaval (2007) [http://portal.acm.org/citation.cfm?id=1229443 "Implicit Parallelism with Ordered Transactions"] ([http://www.cs.washington.edu/homes/luisceze/publications/ipot_ppopp07.pdf PDF]), ''Proceedings of the 12th ACM SIGPLAN symposium on Principles and practice of parallel programming'' (PPoPP '07), ACM New York ©2007, ISBN 978-1-59593-602-8 doi 10.1145/1229428.1229443</ref>). Numerous related articles and patents utilizing CO have already been published.

====Implementation considerations: The Commitment Order Coordinator (COCO)====

A database system in a multidatabase environment is assumed. From a [[software architecture]] point of view a CO component that implements the generic CO algorithm locally, the ''Commitment Order Coordinator'' (COCO), can be designed in a straightforward way as a [[mediator pattern|mediator]] between a (single) database system and an atomic commitment protocol component ([[#Raz1991b|Raz 1991b]]). However, the COCO is typically an integral part of the database system. The COCO's functions are to vote to commit on ready global transactions (processing has ended) according to the local commitment order, to vote to abort on transactions for which the database system has initiated an abort (the database system can initiate abort for any transaction, for many reasons), and to pass the atomic commitment decision to the database system. For local transactions (when can be identified) no voting is needed. For determining the commitment order the COCO maintains an updated representation of the local conflict graph (or local augmented conflict graph for capturing also locking deadlocks) of the undecided (neither committed nor aborted) transactions as a data structure (e.g., utilizing mechanisms similar to [[lock (computer science)|locking]] for capturing conflicts, but with no data-access blocking). The COCO component has an [[interface (computer science)|interface]] with its database system to receive "conflict," "ready" (processing has ended; readiness to vote on a global transaction or commit a local one), and "abort" notifications from the database system. It also interfaces with the atomic commitment protocol to vote and to receive the atomic commitment protocol's decision on each global transaction. The decisions are delivered from the COCO to the database system through their interface, as well as local transactions' commit notifications, at a proper commit order. The COCO, including its interfaces, can be enhanced, if it implements another variant of CO (see below), or plays a role in the database's concurrency control mechanism beyond voting in atomic commitment.

The COCO also guarantees CO locally in a single, isolated database system with no interface with an atomic commitment protocol.

===CO is a necessary condition for global serializability across autonomous database systems===

If the databases that participate in distributed transactions (i.e., transactions that span more than a single database) do not use any shared concurrency control information and use unmodified atomic commitment protocol messages (for reaching atomicity), then maintaining (local) ''commitment ordering'' or one of its generalizing variants (see below) is a [[necessary condition]] for guaranteeing global serializability (a proof technique can be found in ([[#Raz1992|Raz 1992]]), and a different proof method for this in ([[#Raz1993a|Raz 1993a]])); it is also a [[sufficient condition]]. This is a mathematical fact derived from the definitions of ''serializability'' and a ''[[Database transaction|transaction]]''. It means that if not complying with CO, then global serializability cannot be guaranteed under this condition (the condition of no local concurrency control information sharing between databases beyond atomic commit protocol messages). Atomic commitment is a minimal requirement for a distributed transaction since it is always needed, which is implied by the definition of transaction.

([[#Raz1992|Raz 1992]]) defines ''database autonomy'' and ''independence'' as complying with this requirement without using any additional local knowledge:
*'''Definition:''' (concurrency control based) '''autonomous database system'''
:A database system is '''Autonomous''', if it does not share with any other entity any concurrency control information beyond unmodified [[atomic commitment protocol]] messages. In addition it does not use for concurrency control any additional local information beyond conflicts (the last sentence does not appear explicitly but rather implied by further discussion in [[#Raz1992|Raz 1992]]).

Using this definition the following is concluded:

*'''The CO and Global serializability Theorem'''

#CO compliance of every ''autonomous'' database system (or transactional object) in a multidatabase environment is a ''necessary condition'' for guaranteeing Global serializability (without CO Global serializability may be violated).
#CO compliance of every database system is a ''sufficient condition'' for guaranteeing Global serializability.

However, the definition of autonomy above implies, for example, that transactions are scheduled in a way that local transactions (confined to a single database) cannot be identified as such by an autonomous database system. This is realistic for some transactional objects, but too restrictive and less realistic for general purpose database systems. If autonomy is augmented with the ability to identify local transactions, then compliance with a more general property, ''Extended commitment ordering'' (ECO, see below), makes ECO the necessary condition.

Only in ([[#Raz2009|Raz 2009]]) the notion of ''Generalized autonomy'' captures the intended notion of autonomy:
*'''Definition: generalized autonomy'''
:A database system has the ''Generalized autonomy'' property, if it does not share with any other database system any local concurrency information beyond (unmodified) atomic commit protocol messages (however any local information can be utilized).

This definition is probably the broadest such definition possible in the context of database concurrency control, and it makes CO together with any of its (useful: No concurrency control information distribution) generalizing variants (Vote ordering (VO); see CO variants below) the necessary condition for Global serializability (i.e., the union of CO and its generalizing variants is the necessary set VO, which may include also new unknown useful generalizing variants).

===Summary===

The ''Commitment ordering'' (CO) solution (technique) for global serializability can be summarized as follows:

If each ''database'' (or any other ''transactional object'') in a multidatabase environment complies with CO, i.e., arranges its local transactions' commitments and its votes on (global, distributed) transactions to the ''[[atomic commitment]]'' protocol according to the local (to the database) [[partial order]] induced by the local conflict graph (serializability graph) for the respective transactions, then ''Global CO'' and ''Global serializability'' are guaranteed. A database's CO compliance can be achieved effectively with any local [[Serializability#View serializability and conflict serializability|conflict serializability]] based concurrency control mechanism, with neither affecting any transaction's execution process or scheduling, nor aborting it. Also the database's autonomy is not violated. The only low overhead incurred is detecting conflicts (e.g., as with locking, but with no data-access blocking; if not already detected for other purposes), and ordering votes and local transactions' commits according to the conflicts.

[[Image:CO-ScheduleClasses.jpg|thumb|350px| '''Schedule classes containment:''' An arrow from class A to class B indicates that class A strictly contains B; a lack of a directed path between classes means that the classes are incomparable.

A property is '''inherently blocking''', if it can be enforced only by blocking transaction’s data access operations until certain events occur in other transactions. ([[#Raz1992|Raz 1992]])]]

In case of incompatible partial orders of two or more databases (no global partial order can [[Embedding|embed]] the respective local partial orders together), a global cycle (spans two databases or more) in the global conflict graph is generated. This, together with CO, results in a cycle of blocked votes, and a ''voting-[[deadlock]]'' occurs for the databases on that cycle (however, allowed concurrent voting in each database, typically for almost all the outstanding votes, continue to execute). In this case the atomic commitment protocol fails to collect all the votes needed for the blocked transactions on that global cycle, and consequently the protocol aborts some transaction with a missing vote. This breaks the global cycle, the voting-deadlock is resolved, and the related blocked votes are free to be executed. Breaking the global cycle in the global conflict graph ensures that both global CO and global serializability are maintained. Thus, in case of incompatible local (partial) commitment orders no action is needed since the atomic commitment protocol resolves it automatically by aborting a transaction that is a cause for the incompatibility. Furthermore, also global deadlocks due to locking (global cycles in the ''augmented conflict graph'' with at least one data access blocking) result in voting deadlocks and are resolved automatically by the same mechanism.

''Local CO'' is a necessary condition for guaranteeing ''Global serializability,'' if the databases involved do not share any concurrency control information beyond (unmodified) atomic commitment protocol messages, i.e., if the databases are ''autonomous'' in the context of concurrency control. This means that every global serializability solution for autonomous databases must comply with CO. Otherwise global serializability may be violated (and thus, is likely to be violated very quickly in a high-performance environment).

The CO solution [[Scalability|scales up]] with network size and the number of databases without performance penalty when it utilizes [[Two-phase commit protocol#Common architecture|common distributed atomic commitment architecture]].

==Distributed serializability and CO==

===Distributed CO===

A distinguishing characteristic of the CO solution to distributed serializability from other techniques is the fact that it requires no conflict information distributed (e.g., local precedence relations, locks, [[Timestamp-based concurrency control|timestamps]], tickets), which makes it uniquely effective. It utilizes (unmodified) atomic commitment protocol messages (which are already used) instead.

A common way to achieve distributed serializability in a [[Distributed system|(distributed) system]] is by a [[distributed lock manager]] (DLM). DLMs, which communicate lock (non-materialized conflict) information in a distributed environment, typically suffer from computer and communication [[Latency (engineering)|latency]], which reduces the performance of the system. CO allows to achieve distributed serializability under very general conditions, without a distributed lock manager, exhibiting the benefits already explored above for multidatabase environments; in particular: reliability, high performance, scalability, possibility of using ''optimistic concurrency control'' when desired, no conflict information related communications over the network (which have incurred overhead and delays), and automatic distributed deadlock resolution.

All ''distributed transactional systems'' rely on some atomic commitment protocol to coordinate atomicity (whether to commit or abort) among processes in a [[distributed transaction]]. Also, typically ''recoverable data'' (i.e., data under transactions' control, e.g., database data; not to be confused with the ''recoverability'' property of a schedule) are directly accessed by a single ''transactional data manager'' component (also referred to as a ''resource manager'') that handles local sub-transactions (the distributed transaction's portion in a single location, e.g., network node), even if these data are accessed indirectly by other entities in the distributed system during a transaction (i.e., indirect access requires a direct access through a local sub-transaction). Thus recoverable data in a distributed transactional system are typically partitioned among transactional data managers. In such system these transactional data managers typically comprise the participants in the system's atomic commitment protocol. If each participant complies with CO (e.g., by using SS2PL, or COCOs, or a combination; see above), then the entire distributed system provides CO (by the theorems above; each participant can be considered a separate transactional object), and thus (distributed) serializability. Furthermore: When CO is utilized together with an atomic commitment protocol also ''distributed deadlocks'' (i.e., deadlocks that span two or more data managers) caused by data-access locking are resolved automatically. Thus the following corollary is concluded:

*'''The CO Based Distributed Serializability Theorem'''

:Let a ''distributed transactional system'' (e.g., a [[distributed database]] system) comprise ''transactional data managers'' (also called ''resource managers'') that manage all the system's ''recoverable data''. The data managers meet three conditions:
# '''Data partition:''' Recoverable data are partitioned among the data managers, i.e., each recoverable datum (data item) is controlled by a single data manager (e.g., as common in a [[Shared nothing architecture]]; even copies of a same datum under different data managers are physically distinct, ''replicated'').
# '''Participants in atomic commitment protocol:''' These data managers are the participants in the system's atomic commitment protocol for coordinating distributed transactions' atomicity.
# '''CO compliance:''' Each such data manager is CO compliant (or some CO variant compliant; see below).
:Then
# The entire distributed system guarantees (distributed CO and) ''serializability'', and
# Data-access based ''distributed deadlocks'' (deadlocks involving two or more data managers with at least one non-materialized conflict) are resolved automatically.

:Furthermore: The data managers being CO compliant is a ''necessary condition'' for (distributed) serializability in a system meeting conditions 1, 2 above, when the data managers are ''autonomous'', i.e., do not share concurrency control information beyond unmodified messages of atomic commitment protocol.

This theorem also means that when SS2PL (or any other CO variant) is used locally in each transactional data manager, and each data manager has exclusive control of its data, no distributed lock manager (which is often utilized to enforce distributed SS2PL) is needed for distributed SS2PL and serializability. It is relevant to a wide range of distributed transactional applications, which can be easily designed to meet the theorem's conditions.

===Distributed optimistic CO (DOCO)===

For implementing Distributed Optimistic CO (DOCO) the generic local CO algorithm is utilized in all the atomic commitment protocol participants in the system with no data access blocking and thus with no local deadlocks. The previous theorem has the following corollary:

*'''The Distributed optimistic CO (DOCO) Theorem'''

:If DOCO is utilized, then:
:# No local deadlocks occur, and
:# Global (voting) deadlocks are resolved automatically (and all are serializability related (with non-blocking conflicts) rather than locking related (with blocking and possibly also non-blocking conflicts)).

:Thus, no deadlock handling is needed.

===Examples===

====Distributed SS2PL====

A distributed database system that utilizes [[Two-phase locking#Strong strict two-phase locking|SS2PL]] resides on two remote nodes, A and B. The database system has two ''transactional data managers'' (''resource managers''), one on each node, and the database data are partitioned between the two data managers in a way that each has an exclusive control of its own (local to the node) portion of data: Each handles its own data and locks without any knowledge on the other manager's. For each distributed transaction such data managers need to execute the available atomic commitment protocol.

Two distributed transactions, <math>T_{1}</math> and <math>T_{2}</math>, are running concurrently, and both access data x and y. x is under the exclusive control of the data manager on A (B's manager cannot access x), and y under that on B.

:<math>T_{1}</math> reads x on A and writes y on B, i.e., <math>T_{1} = R_{1A}(x)</math> <math>W_{1B}(y)</math> when using notation common for concurrency control.
:<math>T_{2}</math> reads y on B and writes x on A, i.e., <math>T_{2} = R_{2B}(y)</math> <math>W_{2A}(x)</math>

The respective ''local sub-transactions'' on A and B (the portions of <math>T_{1}</math> and <math>T_{2}</math> on each of the nodes) are the following:

:{| class="wikitable" style="text-align:center;"
|+Local sub-transactions
|-
! Transaction \ Node !! A  !! B
|-
! <math>T_{1}</math>
|  <math>T_{1A}=R_{1A}(x)</math> || <math>T_{1B}=W_{1B}(y)</math>
|-
! <math>T_{2}</math>
| <math>T_{2A}=W_{2A}(x)</math> || <math>T_{2B}=R_{2B}(y)</math>
|}

The database system's [[Schedule (computer science)|schedule]] at a certain point in time is the following:

:<math>R_{1A}(x)</math> <math>R_{2B}(y)</math>
:(also <math>R_{2B}(y)</math> <math>R_{1A}(x)</math> is possible)

<math>T_{1}</math> holds a read-lock on x and <math>T_{2}</math> holds read-locks on y. Thus <math>W_{1B}(y)</math> and <math>W_{2A}(x)</math> are blocked by the [[Two-phase locking#Data-access locks|lock compatibility]] rules of SS2PL and cannot be executed. This is a distributed deadlock situation, which is also a voting-deadlock (see below) with a distributed (global) cycle of length 2 (number of edges, conflicts; 2 is the most frequent length). The local sub-transactions are in the following states:

:<math>T_{1A}</math> is ''ready'' (execution has ended) and ''voted'' (in atomic commitment)
:<math>T_{1B}</math> is ''running'' and blocked (a non-materialized conflict situation; no vote on it can occur)
:<math>T_{2B}</math> is ''ready'' and ''voted''
:<math>T_{2A}</math> is ''running'' and blocked (a non-materialized conflict; no vote).

Since the atomic commitment protocol cannot receive votes for blocked sub-transactions (a voting-deadlock), it will eventually abort some transaction with a missing vote(s) by [[Timeout (computing)|timeout]], either <math>T_{1}</math>, or <math>T_{2}</math>, (or both, if the timeouts fall very close). This will resolve the global deadlock. The remaining transaction will complete running, be voted on, and committed. An aborted transaction is immediately ''restarted'' and re-executed.

'''Comments:'''
# The data partition (x on A; y on B) is important since without it, for example, x can be accessed directly from B. If a transaction <math>T_{3}</math> is running on B concurrently with <math>T_{1}</math> and <math>T_{2}</math> and directly writes x, then, without a distributed lock manager the read-lock for x held by <math>T_{1}</math> on A is not visible on B and cannot block the write of <math>T_{3}</math> (or signal a materialized conflict for a non-blocking CO variant; see below). Thus serializability can be violated.
# Due to data partition, x cannot be accessed directly from B. However, functionality is not limited, and a transaction running on B still can issue a write or read request of x (not common). This request is communicated to the transaction's local sub-transaction on A (which is generated, if does not exist already) which issues this request to the local data manager on A.

====Variations====

In the scenario above both conflicts are ''non-materialized'', and the global voting-deadlock is reflected as a cycle in the global ''wait-for graph'' (but not in the global ''conflict graph''; see [[Commitment ordering#Exact characterization of voting-deadlocks by global cycles|Exact characterization of voting-deadlocks by global cycles]] above). However the database system can utilize any CO variant with exactly the same conflicts and voting-deadlock situation, and same resolution. Conflicts can be either ''materialized'' or ''non-materialized'', depending on CO variant used. For example, if [[Commitment ordering#Strict CO (SCO)|SCO]] (below) is used by the distributed database system instead of SS2PL, then the two conflicts in the example are ''materialized'', all local sub-transactions are in ''ready'' states, and vote blocking occurs in the two transactions, one on each node, because of the CO voting rule applied independently on both A and B: due to conflicts <math>T_{2A}=W_{2A}(x)</math> is not voted on before <math>T_{1A}=R_{1A}(x)</math> ends, and <math>T_{1B}=W_{1B}(y)</math> is not voted on before <math>T_{2B}=R_{2B}(y)</math> ends, which is a voting-deadlock. Now the ''conflict graph'' has the global cycle (all conflicts are materialized), and again it is resolved by the atomic commitment protocol, and distributed serializability is maintained. Unlikely for a distributed database system, but possible in principle (and occurs in a multi-database), A can employ SS2PL while B employs SCO. In this case the global cycle is neither in the wait-for graph nor in the serializability graph, but still in the ''augmented conflict graph'' (the union of the two). The various combinations are summarized in the following table:

{| class="wikitable" style="text-align:center;"
|+Voting-deadlock situations
|-
!Case!! Node<br>A  !! Node<br>B !!Possible schedule!!Materialized<br>conflicts<br>on cycle!!Non-<br>materialized<br>conflicts!!<math>T_{1A}</math> =<br><math>R_{1A}(x)</math>!!<math>T_{1B}</math> =<br><math>W_{1B}(y)</math>!!<math>T_{2A}</math> =<br><math>W_{2A}(x)</math>!!<math>T_{2B}</math> =<br><math>R_{2B}(y)</math>
|-
! 1
|SS2PL||SS2PL||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math>|| 0 || 2 ||Ready<br>Voted||Running<br>(Blocked)||Running<br>(Blocked)||Ready<br>Voted
|-
! 2
|SS2PL|| SCO ||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{1B}(y)</math>|| 1 || 1 ||Ready<br>Voted ||Ready<br>Vote blocked||Running<br>(Blocked)||Ready<br>Voted
|-
! 3
|SCO||SS2PL|| <math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{2A}(x)</math> || 1 || 1 ||Ready<br>Voted||Running<br>(Blocked)||Ready<br>Vote blocked||Ready<br>Voted
|-
! 4
|SCO||SCO||<math>R_{1A}(x)</math> <math>R_{2B}(y)</math> <math>W_{1B}(y)</math> <math>W_{2A}(x)</math>|| 2 || 0 ||Ready<br>Voted||Ready<br>Vote blocked ||Ready<br>Vote blocked||Ready<br>Voted
|}

:'''Comments:'''
# Conflicts and thus cycles in the ''augmented conflict graph'' are determined by the transactions and their initial scheduling only, independently of the concurrency control utilized. With any variant of CO, any ''global cycle'' (i.e., spans two databases or more) causes a ''voting deadlock''. Different CO variants may differ on whether a certain conflict is ''materialized'' or ''non-materialized''.
# Some limited operation order changes in the schedules above are possible, constrained by the orders inside the transactions, but such changes do not change the rest of the table.
# As noted above, only case 4 describes a cycle in the (regular) conflict graph which affects serializability. Cases 1-3 describe cycles of locking based global deadlocks (at least one lock blocking exists). All cycle types are equally resolved by the atomic commitment protocol. Case 1 is the common Distributed SS2PL, utilized since the 1980s. However, no research article, except the CO articles, is known to notice this automatic locking global deadlock resolution as of 2009. Such global deadlocks typically have been dealt with by dedicated mechanisms.
# Case 4 above is also an example for a typical voting-deadlock when [[Commitment ordering#Distributed optimistic CO (DOCO)|Distributed optimistic CO (DOCO)]] is used (i.e., Case 4 is unchanged when Optimistic CO (OCO; see below) replaces SCO on both A and B): No data-access blocking occurs, and only materialized conflicts exist.

====Hypothetical Multi Single-Threaded Core (MuSiC) environment====

'''Comment:''' While the examples above describe real, recommended utilization of CO, this example is hypothetical, for demonstration only.

Certain experimental distributed memory-resident databases advocate multi single-threaded core (MuSiC) transactional environments. "Single-threaded" refers to transaction [[Thread (computer science)|threads]] only, and to ''serial'' execution of transactions. The purpose is possible orders of magnitude gain in performance (e.g., [[Michael Stonebraker#H-Store and VoltDB|H-Store]]<ref name=Stone08>Robert Kallman, Hideaki Kimura, Jonathan Natkins, Andrew Pavlo, Alex Rasin, [[Stanley Zdonik]], Evan Jones, Yang Zhang, Samuel Madden, [[Michael Stonebraker]], John Hugg, Daniel Abadi (2008): [http://portal.acm.org/citation.cfm?id=1454211  "H-Store: A High-Performance, Distributed Main Memory Transaction Processing System"], ''Proceedings of the 2008 VLDB'', pages 1496 - 1499, Auckland, New-Zealand, August 2008.</ref> and [[VoltDB]]) relatively to conventional transaction execution in multiple threads on a same core. In what described below MuSiC is independent of the way the cores are distributed. They may reside in one [[integrated circuit]] (chip), or in many chips, possibly distributed geographically in many computers. In such an environment, if recoverable (transactional) data are partitioned among threads (cores), and it is implemented in the conventional way for distributed CO, as described in previous sections, then DOCO and Strictness exist automatically. However, downsides exist with this straightforward implementation of such environment, and its practicality as a general-purpose solution is questionable. On the other hand, tremendous performance gain can be achieved in applications that can bypass these downsides in most situations.

'''Comment:''' The MuSiC straightforward implementation described here (which uses, for example, as usual in distributed CO, voting (and transaction thread) blocking in atomic commitment protocol when needed) is for demonstration only, and has '''no connection''' to the implementation in H-Store or any other project.

In a MuSiC environment local schedules are ''serial''. Thus both local Optimistic CO (OCO; see below) and the ''Global CO enforcement vote ordering strategy'' condition for the atomic commitment protocol are met automatically. This results in both distributed CO compliance (and thus distributed serializability) and automatic global (voting) deadlock resolution.

Furthermore, also local ''Strictness'' follows automatically in a serial schedule. By Theorem 5.2 in ([[#Raz1992|Raz 1992]]; page  307), when the CO vote ordering strategy is applied, also Global Strictness is guaranteed. Note that ''serial'' locally is the only mode that allows strictness and "optimistic" (no data access blocking) together.

The following is concluded:

* '''The MuSiC Theorem'''
:In MuSiC environments, if recoverable (transactional) data are partitioned among cores (threads), then both
:#''OCO'' (and implied ''Serializability''; i.e., DOCO and Distributed serializability)
:#''Strictness'' (allowing effective recovery; 1 and 2 implying Strict CO—see SCO below) and
:#(voting) ''deadlock resolution''
:automatically exist globally with unbounded scalability in number of cores used.

:'''Comment:''' However, two major downsides, which need special handling, may exist:
#Local sub-transactions of a global transaction are blocked until commit, which makes the respective cores idle. This reduces core utilization substantially, even if scheduling of the local sub-transactions attempts to execute all of them in time proximity, almost together. It can be overcome by detaching execution from commit (with some atomic commitment protocol) for global transactions, at the cost of possible cascading aborts.
#increasing the number of cores for a given amount of recoverable data (database size) decreases the average amount of (partitioned) data per core. This may make some cores idle, while others very busy, depending on data utilization distribution. Also a local (to a core) transaction may become global (multi-core) to reach its needed data, with additional incurred overhead. Thus, as the number of cores increases, the amount and type of data assigned to each core should be balanced according to data usage, so a core is neither overwhelmed to become a bottleneck, nor becoming idle too frequently and underutilized in a busy system. Another consideration is putting in a same core partition all the data that are usually accessed by a same transaction (if possible), to maximize the number of local transactions (and minimize the number of global, distributed transactions). This may be achieved by occasional data re-partition among cores based on load balancing (data access balancing) and patterns of data usage by transactions. Another way to considerably mitigate this downside is by proper physical data replication among some core partitions in a way that read-only global transactions are possibly (depending on usage patterns) completely avoided, and replication changes are synchronized by a dedicated commit mechanism.

==CO variants: Interesting special cases and generalizations==

Special case schedule property classes (e.g., SS2PL and SCO below) are strictly contained in the CO class. The generalizing classes (ECO and MVCO) strictly contain the CO class (i.e., include also schedules that are not CO compliant). The generalizing variants also guarantee global serializability without distributing local concurrency control information (each database has the ''generalized autonomy'' property: it uses only local information), while relaxing CO constraints and utilizing additional (local) information for better concurrency and performance: ECO uses knowledge about transactions being local (i.e., confined to a single database), and MVCO uses availability of data versions values. Like CO, both generalizing variants are ''non-blocking'', do not interfere with any transaction's operation scheduling, and can be seamlessly combined with any relevant concurrency control mechanism.

The term '''CO variant''' refers in general to CO, ECO, MVCO, or a combination of each of them with any relevant concurrency control mechanism or property (including Multi-version based ECO, MVECO). No other interesting generalizing variants (which guarantee global serializability with no local concurrency control information distribution) are known, but may be discovered.

===Strong strict two phase locking (SS2PL)===
{{main|Two-phase locking}}

'''Strong Strict Two Phase Locking''' (SS2PL;  also referred to as ''Rigorousness'' or ''Rigorous scheduling'') means that both read and write locks of a transaction are released only after the transaction has ended (either committed or aborted). The set of SS2PL schedules is a [[proper subset]] of the set of CO schedules.
This property is widely utilized in database systems, and since it implies CO, databases that use it and participate in global transactions generate together a serializable global schedule (when using any atomic commitment protocol, which is needed for atomicity in a multi-database environment). No database modification or addition is needed in this case to participate in a CO distributed solution: The set of undecided transactions to be aborted before committing in the [[Commitment ordering#The algorithm|local generic CO algorithm]] above is empty because of the locks, and hence such an algorithm is unnecessary in this case. A transaction can be voted on by a database system immediately after entering a "ready" state, i.e., completing running its task locally. Its locks are released by the database system only after it is decided by the atomic commitment protocol, and thus the condition in the ''Global CO enforcing theorem'' above is kept automatically. Interestingly, if a local timeout mechanism is used by a database system to resolve (local) SS2PL deadlocks, then aborting blocked transactions breaks not only potential local cycles in the global conflict graph (real cycles in the augmented conflict graph), but also database system's potential global cycles as a side effect, if the [[atomic commitment]] protocol's abort mechanism is relatively slow. Such independent aborts by several entities typically may result in unnecessary aborts for more than one transaction per global cycle. The situation is different for a local ''wait-for graph'' based mechanisms: Such cannot identify global cycles, and the atomic commitment protocol will break the global cycle, if the resulting voting deadlock is not resolved earlier in another database.

Local SS2PL together with atomic commitment implying global serializability can also be deduced directly: All transactions, including distributed, obey the [[Two-phase locking|2PL]] (SS2PL) rules. The atomic commitment protocol mechanism is not needed here for consensus on commit, but rather for the end of phase-two synchronization point. Probably for this reason, without considering the atomic commitment voting mechanism, automatic global deadlock resolution has not been noticed before CO.

===Strict CO (SCO)===

[[Image:SCO-VS-SS2PL.jpg|thumb|450px|'''Read-write conflict: SCO Vs. SS2PL'''. Duration of transaction T2 is longer with SS2PL than with SCO.

SS2PL delays write operation w2[x] of T2 until T1 commits, due to a lock on x by T1 following read operation r1[x]. If t time units are needed for transaction T2 after starting write operation w2[x] in order to reach ready state, than T2 commits t time units after T1 commits. However, SCO does not block w2[x], and T2 can commit immediately after T1 commits. ([[#Raz1991c|Raz 1991c]])]]

'''Strict Commitment Ordering''' (SCO; ([[#Raz1991c|Raz 1991c]])) is the intersection of [[Schedule (computer science)#Strict|strictness]] (a special case of recoverability) and CO, and provides an upper bound for a schedule's concurrency when both properties exist. It can be implemented using blocking mechanisms (locking) similar to those used for the popular SS2PL with similar overheads.

Unlike SS2PL, SCO does not block on a read-write conflict but possibly blocks on commit instead. SCO and SS2PL have identical blocking behavior for the other two conflict types: write-read, and write-write. As a result, SCO has shorter average blocking periods, and more concurrency (e.g., performance simulations of a single database for the most significant variant of ''[[locks with ordered sharing]],'' which is identical to SCO, clearly show this, with approximately 100% gain for some transaction loads; also for identical transaction loads SCO can reach higher transaction rates than SS2PL before ''lock [[Thrashing (computer science)|thrashing]]'' occurs). More concurrency means that with given computing resources more transactions are completed in time unit (higher transaction rate, [[throughput]]), and the average duration of a transaction is shorter (faster completion; see chart). The advantage of SCO is especially significant during lock contention.

*'''The SCO Vs. SS2PL Performance Theorem'''
:SCO provides shorter average transaction completion time than SS2PL, if read-write conflicts exist. SCO and SS2PL are identical otherwise (have identical blocking behavior with write-read and write-write conflicts).

SCO is as practical as SS2PL since as SS2PL it provides besides serializability also strictness, which is widely utilized as a basis for efficient recovery of databases from failure. An SS2PL mechanism can be converted to an SCO one for better performance in a straightforward way without changing recovery methods. A description of a SCO implementation can be found in (Perrizo and Tatarinov 1998).<ref>{{cite conference | first1 = William | last1 = Perrizo | first2 = Igor | last2 = Tatarinov | title =  A Semi-Optimistic Database Scheduler Based on Commit Ordering | citeseerx = 10.1.1.53.7318 | conference = 1998 Int'l Conference on Computer Applications in Industry and Engineering | pages = 75–79 | location = Las Vegas | date = November 11, 1998 }}</ref> See also  ''[[The History of Commitment Ordering#Semi-optimistic database scheduler|Semi-optimistic database scheduler]]''.

SS2PL is a proper subset of SCO (which is another explanation why SCO is less constraining and provides more concurrency than SS2PL).

===Optimistic CO (OCO)===

For implementing '''Optimistic commitment ordering''' (OCO) the generic local CO algorithm is utilized without data access blocking, and thus without local deadlocks. OCO without transaction or operation scheduling constraints covers the entire CO class, and is not a special case of the CO class, but rather a useful CO variant and mechanism characterization.

===Extended CO (ECO)===

====General characterization of ECO====

'''Extended Commitment Ordering''' (ECO; ([[#Raz1993a|Raz 1993a]])) generalizes CO. When local transactions (transactions confined to a single database) can be distinguished from global (distributed) transactions (transactions that span two databases or more), commitment order is applied to global transactions only. Thus, for a local (to a database) schedule to have the ECO property, the chronological (partial) order of commit events of global transactions only (unimportant for local transactions) is consistent with their order on the respective local conflict graph.

*'''Definition: extended commitment ordering'''

:Let <math>T_{1}, T_{2}</math> be two committed ''global'' transactions in a schedule, such that a ''directed path'' of unaborted transactions exists in the ''conflict graph'' ([[precedence graph]]) from <math>T_{1}</math> to <math>T_{2}</math> (<math>T_{1}</math> precedes <math>T_{2}</math>, possibly [[transitive relation|transitively]], indirectly). The schedule has the '''Extended commitment ordering''' (ECO) property, if for every two such transactions <math>T_{1}</math> commits before <math>T_{2}</math> commits.

A distributed algorithm to guarantee global ECO exists. As for CO, the algorithm needs only (unmodified) atomic commitment protocol messages. In order to guarantee global serializability, each database needs to guarantee also the conflict serializability of its own transactions by any (local) concurrency control mechanism.

* '''The ECO and Global Serializability Theorem'''

#(Local, which implies global) ECO together with local conflict serializability, is a sufficient condition to guarantee global conflict serializability.
#When no concurrency control information beyond atomic commitment messages is shared outside a database (autonomy), and local transactions can be identified, it is also a necessary condition.

:See a necessity proof in ([[#Raz1993a|Raz 1993a]]).

This condition (ECO with local serializability) is weaker than CO, and allows more concurrency at the cost of a little more complicated local algorithm (however, no practical overhead difference with CO exists).

When all the transactions are assumed to be global (e.g., if no information is available about transactions being local), ECO reduces to CO.

====The ECO algorithm====

Before a global transaction is committed, a generic local (to a database) ECO algorithm aborts a minimal set of undecided transactions (neither committed, nor aborted; either local transactions, or global that run locally), that can cause later a cycle in the conflict graph. This set of aborted transactions (not unique, contrary to CO) can be optimized, if each transaction is assigned with a weight (that can be determined by transaction's importance and by the computing resources already invested in the running transaction; optimization can be carried out, for example, by a reduction from the ''[[Max flow in networks]]'' problem ([[#Raz1993a|Raz 1993a]])). Like for CO such a set is time dependent, and becomes empty eventually. Practically, almost in all needed implementations a transaction should be committed only when the set is empty (and no set optimization is applicable). The local (to the database) concurrency control mechanism (separate from the ECO algorithm) ensures that local cycles are eliminated (unlike with CO, which implies serializability by itself; however, practically also for CO a local concurrency mechanism is utilized, at least to ensure Recoverability). Local transactions can be always committed concurrently (even if a precedence relation exists, unlike CO). When the overall transactions' local partial order (which is determined by the local conflict graph, now only with possible temporary local cycles, since cycles are eliminated by a local serializability mechanism) allows, also global transactions can be voted on to be committed concurrently (when all their transitively (indirect) preceding (via conflict) ''global'' transactions are committed, while transitively preceding local transactions can be at any state. This in analogy to the distributed CO algorithm's stronger concurrent voting condition, where all the transitively preceding transactions need to be committed).

The condition for guaranteeing ''Global ECO'' can be summarized similarly to CO:

*'''The Global ECO Enforcing Vote ordering strategy Theorem'''

:Let <math>T_{1}, T_{2}</math> be undecided (neither committed nor aborted) ''global transactions'' in a database system that ensures serializability locally, such that a ''directed path'' of unaborted transactions exists in the ''local conflict graph'' (that of the database itself) from <math>T_{1}</math> to <math>T_{2}</math>. Then, having <math>T_{1}</math> ended (either committed or aborted) before <math>T_{2}</math> is voted on to be committed, in every such database system in a multidatabase environment, is a [[necessary and sufficient condition]] for guaranteeing Global ECO (the condition guarantees Global ECO, which may be violated without it).

Global ECO (all global cycles in the global conflict graph are eliminated by atomic commitment) together with Local serializability (i.e., each database system maintains serializability locally; all local cycles are eliminated) imply Global serializability (all cycles are eliminated). This means that if each database system in a multidatabase environment provides local serializability (by ''any'' mechanism) and enforces the ''vote ordering strategy'' in the theorem above (a generalization of CO's vote ordering strategy), then ''Global serializability'' is guaranteed (no local CO is needed anymore).

Similarly to CO as well, the ECO ''voting-deadlock'' situation can be summarized as follows:

*'''The ECO Voting-Deadlock Theorem'''

:Let a multidatabase environment comprise database systems that enforce, each, both ''Global ECO'' (using the condition in the theorem above) and ''local conflict serializability'' (which eliminates local cycles in the global conflict graph). Then, a ''voting-deadlock'' occurs if and only if a ''global cycle'' (spans two or more databases) exists in the ''Global augmented conflict graph'' (also blocking by a data-access lock is represented by an edge). If the cycle does not break by any abort, then all the ''global transactions'' on it are involved with the respective voting-deadlock, and eventually each has its vote blocked (either directly, or indirectly by a data-access lock). If a local transaction resides on the cycle, it may be in any unaborted state (running, ready, or committed; unlike CO no local commit blocking is needed).

As with CO this means that also global deadlocks due to data-access locking (with at least one lock blocking) are voting deadlocks, and are automatically resolved by atomic commitment.

===Multi-version CO (MVCO)===

'''Multi-version Commitment Ordering''' (MVCO; ([[#Raz1993b|Raz 1993b]])) is a generalization of CO for databases with [[Multiversion concurrency control|multi-version resources]]. With such resources ''read-only transactions'' do not block or being blocked for better performance. Utilizing such resources is a common way nowadays to increase concurrency and performance by generating a new version of a database object each time the object is written, and allowing transactions' read operations of several last relevant versions (of each object). MVCO implies ''One-copy-serializability'' (1SER or 1SR) which is the generalization of [[serializability]] for multi-version resources. Like CO, MVCO is non-blocking, and can be combined with any relevant multi-version concurrency control mechanism without interfering with it. In the introduced underlying theory for MVCO conflicts are generalized for different versions of a same resource (differently from earlier multi-version theories). For different versions conflict chronological order is replaced by version order, and possibly reversed, while keeping the usual definitions for conflicting operations. Results for the regular and augmented conflict graphs remain unchanged, and similarly to CO a distributed MVCO enforcing algorithm exists, now for a mixed environment with both single-version and multi-version resources (now single-version is a special case of multi-version). As for CO, the MVCO algorithm needs only (unmodified) [[atomic commitment]] protocol messages with no additional communication overhead. Locking-based global deadlocks translate to voting deadlocks and are resolved automatically. In analogy to CO the following holds:

*'''The MVCO and Global one-copy-serializability Theorem'''

#MVCO compliance of every ''autonomous'' database system (or transactional object) in a mixed multidatabase environment of single-version and multi-version databases is a ''necessary condition'' for guaranteeing Global one-copy-serializability (1SER).
#MVCO compliance of every database system is a ''sufficient condition'' for guaranteeing Global 1SER.
#Locking-based global deadlocks are resolved automatically.

:'''Comment''': Now a CO compliant single-version database system is automatically also MVCO compliant.

MVCO can be further generalized to employ the generalization of ECO (MVECO).

====Example: CO based snapshot isolation (COSI)====

'''CO based snapshot isolation''' (COSI) is the intersection of ''[[Snapshot isolation]]'' (SI) with MVCO. SI is a [[multiversion concurrency control]] method widely utilized due to good performance and similarity to serializability (1SER) in several aspects. The theory in (Raz 1993b) for MVCO described above is utilized later in (Fekete et al. 2005) and other articles on SI, e.g., (Cahill et al. 2008);<ref name=Cahill08>Michael J. Cahill, Uwe Röhm, Alan D. Fekete (2008): [http://portal.acm.org/citation.cfm?id=1376690  "Serializable isolation for snapshot databases"], ''Proceedings of the 2008 ACM SIGMOD international conference on Management of data'', pp. 729-738, Vancouver, Canada, June 2008, ISBN 978-1-60558-102-6 (SIGMOD 2008 best paper award</ref> see also [[Snapshot isolation#Making Snapshot Isolation Serializable|Making snapshot isolation serializable]] and the references there), for analyzing conflicts in SI in order to make it serializable. The method presented in (Cahill et al. 2008), ''Serializable snapshot isolation'' (SerializableSI), a low overhead modification of SI, provides good performance results versus SI, with only small penalty for enforcing serializability. A different method, by combining SI with MVCO (COSI), makes SI serializable as well, with a relatively low overhead, similarly to combining the generic CO algorithm with single-version mechanisms. Furthermore, the resulting combination, COSI, being MVCO compliant, allows COSI compliant database systems to inter-operate and transparently participate in a CO solution for distributed/global serializability (see below). Besides overheads also protocols' behaviors need to be compared quantitatively. On one hand, all serializable SI schedules can be made MVCO by COSI (by possible commit delays when needed) without aborting transactions. On the other hand, SerializableSI is known to unnecessarily abort and restart certain percentages of transactions also in serializable SI schedules.

===CO and its variants are transparently interoperable for global serializability===

With CO and its variants (e.g., SS2PL, SCO, OCO, ECO, and MVCO above) global serializability is achieved via ''atomic commitment'' protocol based distributed algorithms. For CO and all its variants atomic commitment protocol is the instrument to eliminate global cycles (cycles that span two or more databases) in the ''global augmented'' (and thus also regular) ''conflict graph'' (implicitly; no global data structure implementation is needed). In cases of either incompatible local commitment orders in two or more databases (when no global [[partial order]] can [[Embedding|embed]] the respective local partial orders together), or a data-access locking related voting deadlock, both implying a global cycle in the global augmented conflict graph and missing votes, the atomic commitment protocol breaks such cycle by aborting an undecided transaction on it (see [[commitment ordering#The distributed CO algorithm|The distributed CO algorithm]] above). Differences between the various variants exist at the local level only (within the participating database systems). Each local CO instance of any variant has the same role, to determine the position of every global transaction (a transaction that spans two or more databases) within the local commitment order, i.e., to determine when it is the transaction's turn to be voted on locally in the atomic commitment protocol. Thus, all the CO variants exhibit the same behavior in regard to atomic commitment. This means that they are all interoperable via atomic commitment (using the same software interfaces, typically provided as [[Service (systems architecture)|service]]s, some already [[international standard|standardized]] for atomic commitment, primarily for the [[two phase commit]] protocol, e.g., [[X/Open XA]]) and transparently can be utilized together in any distributed environment (while each CO variant instance is possibly associated with any relevant local concurrency control mechanism type).

In summary, any single global transaction can participate simultaneously in databases that may employ each any, possibly different, CO variant (while concurrently running processes in each such database, and running concurrently with local and other global transactions in each such database). The atomic commitment protocol is indifferent to CO, and does not distinguish between the various CO variants. Any ''global cycle'' generated in the augmented global conflict graph may span databases of different CO variants, and generate (if not broken by any local abort) a voting deadlock that is resolved by atomic commitment exactly the same way as in a single CO variant environment. ''local cycles'' (now possibly with mixed materialized and non-materialized conflicts, both serializability and data-access-locking deadlock related, e.g., SCO) are resolved locally (each by its respective variant instance's own local mechanisms).

'''Vote ordering''' (VO or Generalized CO (GCO); [[#Raz2009|Raz 2009]]), the union of CO and all its above variants, is a useful concept and global serializability technique. To comply with VO, local serializability (in it most general form, commutativity based, and including multi-versioning) and the ''vote order strategy'' (voting by local precedence order) are needed.

Combining results for CO and its variants, the following is concluded:

*'''The CO Variants Interoperability Theorem'''
#In a multi-database environment, where each database system (transactional object) is compliant with some CO variant property (VO compliant), any global transaction can participate simultaneously in databases of possibly different CO variants, and Global serializability is guaranteed (''sufficient condition'' for Global serializability; and Global one-copy-serializability (1SER), for a case when a multi-version database exists).
#If only local (to a database system) concurrency control information is utilized by every database system (each has the ''generalized autonomy'' property, a generalization of ''autonomy''), then compliance of each with some (any) CO variant property (VO compliance) is a ''necessary condition'' for guaranteeing Global serializability (and Global 1SER; otherwise they may be violated).
#Furthermore, in such environment data-access-locking related global deadlocks are resolved automatically (each such deadlock is generated by a global cycle in the ''augmented conflict graph'' (i.e., a ''voting deadlock''; see above), involving at least one data-access lock (non-materialized conflict) and two database systems; thus, not a cycle in the regular conflict graph and does not affect serializability).

==References==

*{{citation|first=Yoav|last=Raz|url=http://www.vldb.org/conf/1992/P292.PDF|title=The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment|work=Proceedings of the Eighteenth International Conference on Very Large Data Bases|pages=292–312|place=Vancouver, Canada|date=August 1992}} (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990)
*{{citation|first=Yoav|last=Raz|title=Serializability by Commitment Ordering|work=Information Processing Letters|volume=51|number=5|pages=257–264|date=September 1994|doi=10.1016/0020-0190(94)90005-1}}
*{{citation|first=Yoav|last=Raz|url=http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering|title=Theory of Commitment Ordering: Summary|date=June 2009|accessdate=November 11, 2011}}
*{{citation|first=Yoav|last=Raz|url=http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf|title=On the Significance of Commitment Ordering|publisher=Digital Equipment Corporation|date=November 1990}}
*<cite id=Raz1991a>Yoav Raz (1991a): US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,899 (ECO)] [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=2&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,900 (CO)]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=1&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,701,480 (MVCO)] </cite>
*<cite id=Raz1991b>Yoav Raz (1991b): "The Commitment Order Coordinator (COCO) of a Resource Manager, or Architecture for Distributed Commitment Ordering Based Concurrency Control", DEC-TR 843, Digital Equipment Corporation, December 1991. </cite>
*<cite id=Raz1991c>Yoav Raz (1991c): "Locking Based Strict Commitment Ordering, or How to improve Concurrency in Locking Based Resource Managers", DEC-TR 844, December 1991. </cite>
*<cite id=Raz1993a>Yoav Raz (1993a): [http://portal.acm.org/citation.cfm?id=153858 "Extended Commitment Ordering or Guaranteeing Global Serializability by Applying Commitment Order Selectivity to Global Transactions."] ''Proceedings of the Twelfth ACM Symposium on Principles of Database Systems'' (PODS), Washington, DC, pp. 83-96, May 1993. (also DEC-TR 842, November 1991) </cite>
*<cite id=Raz1993b>Yoav Raz (1993b): [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=281924  "Commitment Ordering Based Distributed Concurrency Control for Bridging Single and Multi Version Resources."] ''Proceedings of the Third IEEE International Workshop on Research Issues on Data Engineering: Interoperability in Multidatabase Systems'' (RIDE-IMS), Vienna, Austria, pp. 189-198, April 1993. (also DEC-TR 853, July 1992)  </cite>

==Footnotes==
{{reflist}}

==External links==
*[http://sites.google.com/site/yoavraz2/the_principle_of_co Yoav Raz's Commitment ordering page]

{{DEFAULTSORT:Commitment Ordering}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]
[[Category:Distributed algorithms]]
<=====doc_Id=====>:202
<=====title=====>:
Operational historian
<=====text=====>:
{{Refimprove|date=October 2013}}
{{Use British English|date=March 2014}}
'''Operational historian''' refers to a database software application that logs or historizes time-based process data.<ref>{{cite web|title=A Practical Guide to Process Data Historians and Process Information Systems|url=http://www.tappi.org/Downloads/unsorted/UNTITLED---PCEI99339pdf.aspx|publisher=TAPPI|accessdate=14 September 2012|author=R. H. (Rick) Meeker, Jr.|format=PDF|date=13 January 1999}}</ref> Historian software is used to record trends and historical information about industrial processes for future reference. It captures plant management information about production status, performance monitoring, quality assurance, tracking and genealogy, and product delivery with enhanced data capture, data compression, and data presentation capabilities.<ref name=GlobalspecArticle  >{{cite web |url= http://www.globalspec.com/learnmore/industrial_engineering_software/industrial_controls_software/trending_historian_software |title= Globalspec Historian Article| accessdate=12 Jul 2012}}</ref>

Operational historians are like enterprise historians but differ in that they are used by engineers on the plant floor rather than by business processes.{{citation needed|date=February 2015}} They are typically cheaper, lighter in weight, and easier to use and reconfigure than enterprise historians. Having an operational historian enables "at the source" analysis of the historical data that is not typically possible with enterprise historians.
Typically, these applications offer two layers of data access: through a dedicated SDK(Standard Development Kit)(sometimes in two different flavours: full administration API(Application Programming Interface) and high-speed read/write API), as well as user front-end tools (for instance, administration panels, engineering consoles or portal-like web clients).

Because these applications are designed to fulfil specific operation time requirements, their marketing materials often indicate that these are real-time database systems.<ref name="example">[http://software.schneider-electric.com/products/wonderware/production-information-management/historian/ Wonderware Historian - Example of naming the operational historian the real-time database]</ref> However, since such performance measurements are often executed for atomic operations (especially write operations), not necessarily whole transactions, not all of the operational historians must be in fact real-time databases.

Usual challenges the operational historians must address are as follows:
* data collection from real-time external systems,
* storage and archiving of very large volumes of data,
* tag organisation (typically [[Time series database|time series]], where a single sample contains the information about the time stamp, the value and the sample quality),
* basic data limit monitoring (alarms) and user prompts (messages),
* performance of read and write operations.

== Data access ==
As opposed to enterprise historians, the data access layer in the operational historian is designed to offer sophisticated data fetching modes without complex information analysis facilities. The following settings are typically available for data access operations:
* Data scope (single point, history based on time range, history based on sample count),
* Request modes (raw data, last-known value, aggregation, interpolation),
* Sampling (single point, all points without sampling, all points with interval sampling),
* Data omission (based on the sample quality, based on the sample value, based on the count).

Even though the operational historians are rarely [[relational database management system]]s, they often offer [[SQL]]-based interfaces to query the database. In most of such implementations, the dialect does not follow the SQL standard in order to provide syntax for specifying data access operations parameters.

== Notable software ==
=== Commercial ===
* [[ABB]] Decathlon History
* [[Aspen Technology]] InfoPlus.21 <ref>{{Citation | url = http://www.aspentech.com/aspenONE_MES_Brochure.pdf | format = PDF | publisher = Aspen Technology, Inc. | title = aspenONE MES Brochure | page = 2 | year = 2014 | accessdate = 30 July 2014 }}</ref>
* [[ENEA_AB|Enea]]'s Polyhedra Historian, a module of [[Polyhedra DBMS]]<ref>{{Citation | url = http://developer.polyhedra.com/polyhedra-features/historian | publisher = Enea AB |  title = Handling time-series data in Polyhedra IMDB | date = 11 May 2012 | accessdate = 30 July 2014 }}</ref>
* [[GE Intelligent Platforms]] Proficy Historian<ref>{{Citation | url = http://www.ge-ip.com/account/prepsend/file/Proficy_Historian_5-5.pdf | format = PDF | publisher = GE Intelligent Platforms, Inc. | title = Datasheet: Proficy Historian 5.5 | year = 2013 | accessdate = 30 July 2014 }}</ref>
* [[Honeywell]] Uniformance PHD<ref>{{Citation | url = http://www.honeywellprocess.com/library/marketing/notes/uniformance-phd-pin.pdf | publisher = Honeywell International Inc. | title = Uniformance PHD Product Information Note | format = PDF | year = 2013 | accessdate = 30 July 2014 }}</ref>
* [[Iconics ]] [http://www.iconics.com/Home/Products/Historians/Hyper-Historian.aspx Hyper Historian]
* [[Inductive Automation]] [[Ignition SCADA#SQL Bridge| SQL Bridge]] module of [[Ignition SCADA]]<ref>{{Citation | url = http://inductiveautomation.com/scada-software/scada-modules/sqlbridge | publisher = Inductive Automation | title = High-Powered Data Acquisition | year = 2014 | accessdate = 30 July 2014 }}</ref>
* [[National Instruments]] Citadel, used in [[LabVIEW]] DSC and other products<ref>{{Citation | url = http://www.ni.com/white-paper/6579/en/ | publisher = National Instruments Corp. |  title = Logging Data with National Instruments Citadel | date = 19 July 2012 | accessdate = 30 July 2014 }}</ref>
* [[OSIsoft]] - PI System
* [[Schneider Electric]] InStep Software [http://www.instepsoftware.com/instep-software-products/edna-enterprise-data-historian eDNA Real-Time Historian]
* [[Schneider Electric]] Wonderware Historian<ref>{{Citation | url =  http://global.wonderware.com/EN/PDF%20Library/Datasheet_Wonderware_Historian.pdf | publisher = Invensys Systems, Inc. | title = Wonderware Historian Software Datasheet | format = PDF | year = 2013 | accessdate = 30 July 2014 }}</ref>
* [[Yokogawa]] Exaquantum Historian<ref>{{Citation | url = http://www.yokogawa.com/eu/pims/pdf/BU%20GMSCS0102-02E%20Exaquantum%20Bulletin%2072ppi.pdf | publisher = Yokogawa Marex Limited |  title = Exaquantum delivers Production Excellence | format = PDF | year = 2013 | accessdate = 30 July 2014 }}</ref>
* [[Jaaji Technologies]] inSis Historian<ref>{{Citation | url = http://www.jaajitech.com/Infoview | publisher = Jaaji Software Technologies Private Limited |  title = Find, View and Analyze your process data from everywhere | year = 2014 | accessdate = 30 July 2014 }}</ref>

==See also==
* [[Time series database]]
* [[Relational database management system]]

== References ==
{{reflist}}

[[Category:Data management]]
<=====doc_Id=====>:205
<=====title=====>:
Bitmap index
<=====text=====>:
A '''bitmap index''' is a special kind of [[Index (database)|database index]] that uses [[Bit array|bitmap]]s.

Bitmap indexes have traditionally been considered to work well for ''low-cardinality columns'', which have a modest number of distinct values, either absolutely, or relative to the number of records that contain the data. The extreme case of low cardinality is Boolean data (e.g., does a resident in a city have internet access?), which has two values, True and False. Bitmap indexes use [[bit array]]s (commonly called bitmaps) and answer queries by performing [[bitwise operation|bitwise logical operation]]s on these bitmaps. Bitmap indexes have a significant space and performance advantage over other structures for query of such data. Their drawback is they are less efficient than the traditional [[B-tree]] indexes for columns whose data is frequently updated: consequently, they are more often employed in read-only systems that are specialized for fast query - e.g., data warehouses, and generally unsuitable for [[online transaction processing]] applications.

Some researchers argue that bitmap indexes are also useful for moderate or even high-cardinality data (e.g., unique-valued data) which is accessed in a read-only manner, and queries access multiple bitmap-indexed columns using the AND, OR or XOR operators extensively.<ref name="sharma">[http://www.oracle.com/technetwork/articles/sharma-indexes-093638.html Bitmap Index vs. B-tree Index: Which and When?], Vivek Sharma, Oracle Technical Network.</ref>

Bitmap indexes are also useful in [[data warehousing]] applications for joining a large [[fact table]] to smaller [[dimension table]]s such as those arranged in a [[star schema]].

Bitmap based representation can also be used for representing a data structure which is labeled and directed attributed multigraph, used for queries in [[graph databases]].<code>[http://www.researchgate.net/publication/236593640_Efficient_graph_management_based_on_bitmap_indices Efficient graph management based on bitmap indices]</code> article shows how bitmap index representation can be used to manage large dataset(billions of data points) and answer queries related to graph efficiently.

==Example==
Continuing the internet access example, a bitmap index may be logically viewed as follows:
{| class="wikitable" style="text-align:center; float:left"
|-
!rowspan=2| Identifier
!rowspan=2| HasInternet
!colspan=2| Bitmaps
|-
!Y !! N
|-
|1 || Yes || 1|| 0
|-
|2 || No || 0 || 1
|-
|3 || No || 0 || 1
|-
|4 || Unspecified || 0 || 0
|-
|5 || Yes || 1 || 0
|}

On the left, [[Identifier]] refers to the unique number assigned to each resident, HasInternet is the data to be indexed, the content of the bitmap index is shown as two columns under the heading ''bitmaps''. Each column in the left illustration is a ''bitmap'' in the bitmap index. In this case, there are two such bitmaps, one for "has internet" ''Yes'' and one for "has internet" ''No''. It is easy to see that each bit in bitmap ''Y'' shows whether a particular row refers to a person who has internet access. This is the simplest form of bitmap index. Most columns will have more distinct values. For example, the sales amount is likely to have a much larger number of distinct values. Variations on the bitmap index can effectively index this data as well. We briefly review three such variations.

Note: Many of the references cited here are reviewed at ([[#JohnWu2007|John Wu (2007)]]).<ref>{{cite web|ref=JohnWu2007|author=John Wu |year=2007 |title=Annotated References on Bitmap Index |url=http://www.cs.umn.edu/~kewu/annotated.html}}</ref> For those who might be interested in experimenting with some of the ideas mentioned here, many of them are implemented in open source software such as FastBit,<ref>[http://codeforge.lbl.gov/projects/fastbit/ FastBit]</ref> the Lemur Bitmap Index C++ Library,<ref>[https://code.google.com/p/lemurbitmapindex/ Lemur Bitmap Index C++ Library]</ref> the Roaring Bitmap Java library,<ref>[http://roaringbitmap.org/ Roaring bitmaps]</ref>  the [[Apache Hive]] Data Warehouse system and [[LucidDB]].

{{Clear}}

==Compression==
Software can [[data compression|compress]] each bitmap in a bitmap index to save spaces.  There has been considerable amount of work on this subject.<ref>{{cite book |author=T. Johnson |editor1=Malcolm P. Atkinson |editor2=[[Maria Orłowska|Maria E. Orlowska]] |editor3=Patrick Valduriez |editor4=Stanley B. Zdonik |editor5=Michael L. Brodie | title = VLDB'99, Proceedings of 25th International Conference on Very Large Data Bases, September 7–10, 1999, Edinburgh, Scotland, UK | publisher = Morgan Kaufmann | year = 1999 | isbn = 1-55860-615-7 | chapter =Performance Measurements of Compressed Bitmap Indices | pages=278–89 | url=http://www.vldb.org/conf/1999/P29.pdf }}</ref><ref>{{cite web |vauthors=Wu K, Otoo E, Shoshani A | title=On the performance of bitmap indices for high cardinality attributes | date=March 5, 2004 | url=http://www.osti.gov/energycitations/servlets/purl/822860-LOzkmz/native/822860.pdf }}</ref>
Though there are exceptions such as Roaring bitmaps,<ref name=roaring>{{Cite journal | last1 = Chambi | first1 = S. | last2 = Lemire | first2 = D. | last3 = Kaser | first3 = O. | last4 = Godin | first4 = R.  | title = Better bitmap performance with Roaring bitmaps | doi = 10.1002/spe.2325 | journal = Software: Practice & Experience | volume = 46 | pages = 5 | year = 2016 | pmid =  | pmc = }}</ref> Bitmap compression algorithms typically employ [[run-length encoding]], such as the Byte-aligned Bitmap Code,<ref>{{US Patent|5363098|Byte aligned data compression}}</ref> the Word-Aligned Hybrid code,<ref>{{US Patent|6831575|Word aligned bitmap compression method, data structure, and apparatus}}</ref> the Partitioned Word-Aligned Hybrid (PWAH) compression,<ref>{{cite conference |url=http://dl.acm.org/citation.cfm?doid=1989323.1989419 |title=A memory efficient reachability data structure through bit vector compression | last1=van Schaik | first1=Sebastiaan |last2=de Moor |first2=Oege |year=2011 |publisher=ACM |booktitle=Proceedings of the 2011 international conference on Management of data |pages=913–924 |location=Athens, Greece |doi=10.1145/1989323.1989419 |conference=SIGMOD '11 |isbn=978-1-4503-0661-4 }}</ref> the Position List Word Aligned Hybrid,<ref name="doi_10.1145/1739041.1739071">{{cite book | chapter = Position list word aligned hybrid: optimizing space and performance for compressed bitmaps |vauthors=Deliège F, Pedersen TB |editor1=Ioana Manolescu |editor2=Stefano Spaccapietra |editor3=Jens Teubner |editor4=Masaru Kitsuregawa |editor5=Alain Leger |editor6=Felix Naumann |editor7=Anastasia Ailamaki |editor8=Fatma Ozcan | title = EDBT '10, Proceedings of the 13th International Conference on Extending Database Technology | publisher = ACM | location = New York, NY, USA | year = 2010 | pages = 228–39 | isbn = 978-1-60558-945-9 | doi = 10.1145/1739041.1739071 | url = http://alpha.uhasselt.be/icdt/edbticdt2010proc/edbt/papers/p0228-Deliege.pdf }}</ref> the Compressed Adaptive Index (COMPAX),<ref name="autogenerated1382">{{cite journal|author1=F. Fusco |author2=M. Stoecklin |author3=M. Vlachos |title=NET-FLi: on-the-fly compression, archiving and indexing of streaming network traffic |date=September 2010 | volume = 3 | issue = 1–2 | pages = 1382–93 | journal=Proc. VLDB Endow | url=http://www.comp.nus.edu.sg/~vldb2010/proceedings/files/papers/I01.pdf }}</ref> Enhanced Word-Aligned Hybrid (EWAH) <ref name=ewah>{{Cite journal | last1 = Lemire | first1 = D. | last2 = Kaser | first2 = O. | last3 = Aouiche | first3 = K. | title = Sorting improves word-aligned bitmap indexes | doi = 10.1016/j.datak.2009.08.006 | journal = Data & Knowledge Engineering | volume = 69 | pages = 3 | year = 2010 | pmid =  | pmc = }}</ref> and the COmpressed 'N' Composable Integer SEt.<ref>[http://ricerca.mat.uniroma3.it/users/colanton/concise.html Concise: Compressed 'n' Composable Integer Set] {{webarchive |url=https://web.archive.org/web/20110528033714/http://ricerca.mat.uniroma3.it/users/colanton/concise.html |date=May 28, 2011 }}</ref><ref name="doi_10.1016/j.ipl.2010.05.018" /> These compression methods require very little effort to compress and decompress. More importantly, bitmaps compressed with BBC, WAH, COMPAX, PLWAH, EWAH and CONCISE can directly participate in [[bitwise operation]]s without decompression. This gives them considerable advantages over generic compression techniques such as [[LZ77]]. BBC compression and its derivatives are used in a commercial [[database management system]]. BBC is effective in both reducing index sizes and maintaining [[database query|query]] performance. BBC encodes the bitmaps in [[bytes]], while WAH encodes in words, better matching current [[CPU]]s. "On both synthetic data and real application data, the new word aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC."<ref>{{cite book |vauthors=Wu K, Otoo EJ, Shoshani A |editor1=Henrique Paques |editor2=Ling Liu |editor3=David Grossman | chapter =A Performance comparison of bitmap indexes | year=2001 | title = CIKM '01 Proceedings of the tenth international conference on Information and knowledge management | publisher = ACM | location = New York, NY, USA | pages = 559–61 | isbn = 1-58113-436-3 | doi = 10.1145/502585.502689 | url = http://crd.lbl.gov/~kewu/ps/LBNL-48975.pdf }}</ref> PLWAH bitmaps were reported to take 50% of the storage space consumed by WAH bitmaps and offer up to 20% faster performance on [[logical operation]]s.<ref name="doi_10.1145/1739041.1739071" /> Similar considerations can be done for CONCISE <ref name="doi_10.1016/j.ipl.2010.05.018">{{cite journal |vauthors=Colantonio A, Di Pietro R | title=Concise: Compressed 'n' Composable Integer Set | journal = Information Processing Letters | volume = 110 | issue = 16 | date = 31 July 2010 | doi = 10.1016/j.ipl.2010.05.018 | url = http://ricerca.mat.uniroma3.it/users/colanton/docs/concise.pdf |pages=644–50 }}</ref> and Enhanced Word-Aligned Hybrid.<ref name="ewah"/>

The performance of schemes such as BBC, WAH, PLWAH, EWAH, COMPAX and CONCISE is dependent on the order of the rows. A simple lexicographical sort can divide the index size by 9 and make indexes several times faster.<ref>{{cite journal|author1=D. Lemire |author2=O. Kaser |author3=K. Aouiche |title=Sorting improves word-aligned bitmap indexes |journal=Data & Knowledge Engineering | volume=69 | issue=1 |date=January 2010 |arxiv=0901.3751 | doi = 10.1016/j.datak.2009.08.006|pages=3–28 }}</ref> The larger the table, the more important it is to sort the rows. Reshuffling techniques have also been proposed to achieve the same results of sorting when indexing streaming data.<ref name="autogenerated1382"/>

==Encoding==
Basic bitmap indexes use one bitmap for each distinct value. It is possible to reduce the number of bitmaps used by using a different encoding method.<ref name="autogenerated355">{{cite book |chapter=Bitmap index design and evaluation |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1998 | title = Proceedings of the 1998 ACM SIGMOD international conference on Management of data (SIGMOD '98) |editor1=Ashutosh Tiwary |editor2=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 355–6 | doi=10.1145/276304.276336 | url = http://www.comp.nus.edu.sg/~chancy/sigmod98.pdf }}</ref><ref>{{cite book |chapter=An efficient bitmap encoding scheme for selection queries |author1=C.-Y. Chan  |author2=Y. E. Ioannidis  |lastauthoramp=yes | year=1999 | title = Proceedings of the 1999 ACM SIGMOD international conference on Management of data (SIGMOD '99) | publisher = ACM | location = New York, NY, USA | pages = 215–26 | doi = 10.1145/304182.304201 | url = http://www.ist.temple.edu/~vucetic/cis616spring2005/papers/P4%20p215-chan.pdf }}</ref> For example, it is possible to encode C distinct values using log(C) bitmaps with binary encoding.<ref>{{cite journal |author1=P. E. O'Neil  |author2=D. Quass |lastauthoramp=yes | chapter = Improved Query Performance with Variant Indexes | title = Proceedings of the 1997 ACM SIGMOD international conference on Management of data (SIGMOD '97) | year = 1997 |editor1=Joan M. Peckman |editor2=Sudha Ram |editor3=Michael Franklin | publisher = ACM | location = New York, NY, USA | pages = 38–49| doi=10.1145/253260.253268 }}</ref>

This reduces the number of bitmaps, further saving space, but to answer any query, most of the bitmaps have to be accessed. This makes it potentially not as effective as scanning a vertical projection of the base data, also known as a [[materialized view]] or projection index. Finding the optimal encoding method that balances (arbitrary) query performance, index size and index maintenance remains a challenge.

Without considering compression, Chan and Ioannidis analyzed a class of multi-component encoding methods and came to the conclusion that two-component encoding sits at the kink of the performance vs. index size curve and therefore represents the best trade-off between index size and query performance.<ref name="autogenerated355"/>

==Binning==
For high-cardinality columns, it is useful to bin the values, where each bin covers multiple values and build the bitmaps to represent the values in each bin. This approach reduces the number of bitmaps used regardless of encoding method.<ref>{{cite book |chapter = Space efficient bitmap indexing| title= Proceedings of the ninth international conference on Information and knowledge management (CIKM '00) | year=2000 | author =N. Koudas | publisher = ACM | location = New York, NY, USA | pages = 194–201 | doi=10.1145/354756.354819 }}</ref> However, binned indexes can only answer some queries without examining the base data. For example, if a bin covers the range from 0.1 to 0.2, then when the user asks for all values less than 0.15, all rows that fall in the bin are possible hits and have to be checked to verify whether they are actually less than 0.15. The process of checking the base data is known as the candidate check. In most cases, the time used by the candidate check is significantly longer than the time needed to work with the bitmap index. Therefore, binned indexes exhibit irregular performance. They can be very fast for some queries, but much slower if the query does not exactly match a bin.

==History==
The concept of bitmap index was first introduced by Professor Israel Spiegler and Rafi Maayan in their research "Storage and Retrieval Considerations of Binary Data Bases", published in 1985.<ref>{{cite journal | title = Storage and retrieval considerations of binary data bases | author1 = Spiegler I | author2 = Maayan R | journal = Information Processing and Management: an International Journal | volume = 21 | issue = 3 | year = 1985 | doi = 10.1016/0306-4573(85)90108-6 | pages = 233–54   }}</ref> The first commercial database product to implement a bitmap index was Computer Corporation of America's [[Model 204]]. [[Patrick O'Neil]] published a paper about this implementation in 1987.<ref name="model204">{{cite conference | last = O'Neil | first = Patrick | title = Model 204 Architecture and Performance | booktitle = Proceedings of the 2nd International Workshop on High Performance Transaction Systems | pages = 40–59 | publisher = Springer-Verlag | year = 1987 | location = London, UK | editor = Dieter Gawlick |editor2=Mark N. Haynie |editor3=Andreas Reuter (Eds.) }}</ref> This implementation is a hybrid between the basic bitmap index (without compression) and the list of Row Identifiers (RID-list). Overall, the index is organized as a [[B+tree]]. When the column cardinality is low, each leaf node of the B-tree would contain long list of RIDs. In this case, it requires less space to represent the RID-lists as bitmaps. Since each bitmap represents one distinct value, this is the basic bitmap index. As the column cardinality increases, each bitmap becomes sparse and it may take more disk space to store the bitmaps than to store the same content as RID-lists. In this case, it switches to use the RID-lists, which makes it a [[B+tree]] index.<ref>{{cite conference | title=Bit-sliced index arithmetic | booktitle= Proceedings of the 2001 ACM SIGMOD international conference on Management of data (SIGMOD '01) | year = 2001 |author1=D. Rinfret, P. O'Neil  |author2=E. O'Neil  |lastauthoramp=yes | editor = Timos Sellis | publisher = ACM | location = New York, NY, USA | pages = 47–57 | doi = 10.1145/375663.375669 }}</ref><ref>{{cite conference |author1=E. O'Neil |author2=P. O'Neil |author3=K. Wu | title = Bitmap Index Design Choices and Their Performance Implications | booktitle = 11th International Database Engineering and Applications Symposium (IDEAS 2007) | year = 2007 | pages = 72–84 | url=http://crd.lbl.gov/~kewu/ps/LBNL-62756.pdf | isbn = 0-7695-2947-X | doi = 10.1109/IDEAS.2007.19 }}</ref>

==In-memory bitmaps==
One of the strongest reasons for using bitmap indexes is that the intermediate results produced from them are also bitmaps and can be efficiently reused in further operations to answer more complex queries. Many programming languages support this as a bit array data structure. For example, Java has the <code>[http://download.oracle.com/javase/6/docs/api/java/util/BitSet.html BitSet]</code> class.

Some database systems that do not offer persistent bitmap indexes use bitmaps internally to speed up query processing. For example, [[PostgreSQL]] versions 8.1 and later implement a "bitmap index scan" optimization to speed up arbitrarily complex [[logical operation]]s between available indexes on a single table.

For tables with many columns, the total number of distinct indexes to satisfy all possible queries (with equality filtering conditions on either of the fields) grows very fast, being defined by this formula:

:<math> \mathbf{C}_n^\left [ \frac{n}{2} \right ] \equiv \frac{n!}{\left(n-\left [ \frac{n}{2} \right ]\right)! \left [ \frac{n}{2} \right ]!}</math>.<ref>{{cite web|author=Alex Bolenok|date=2009-05-09|title=Creating indexes|url=http://explainextended.com/2009/05/09/creating-indexes/}}</ref><ref>{{cite web|author=Egor Timoshenko|title=On minimal collections of indexes|url=http://explainextended.com/files/index-en.pdf }}</ref>

A bitmap index scan combines expressions on different indexes, thus requiring only one index per column to support all possible queries on a table.

Applying this access strategy to B-tree indexes can also combine range queries on multiple columns. In this approach, a temporary in-memory bitmap is created with one [[bit]] for each row in the table (1 [[MiB]] can thus store over 8 million entries). Next, the results from each index are combined into the bitmap using [[bitwise operation]]s. After all conditions are evaluated, the bitmap contains a "1" for rows that matched the expression. Finally, the bitmap is traversed and matching rows are retrieved. In addition to efficiently combining indexes, this also improves [[locality of reference]] of table accesses, because all rows are fetched sequentially from the main table.<ref>{{cite web |author=Tom Lane |date=2005-12-26 |title=Re: Bitmap indexes etc. |publisher=PostgreSQL mailing lists |url=http://archives.postgresql.org/pgsql-performance/2005-12/msg00623.php |accessdate=2007-04-06 }}</ref> The internal bitmap is discarded after the query. If there are too many rows in the table to use 1 bit per row, a "lossy" bitmap is created instead, with a single bit per disk page. In this case, the bitmap is just used to determine which pages to fetch; the filter criteria are then applied to all rows in matching pages.

==References==
;Notes
{{Reflist|30em}}

;Bibliography
*{{Cite journal|last=O'Connell|first=S.|year=2005|title=Advanced Databases Course Notes|location=[[Southampton]]|publisher=[[University of Southampton]]|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}
*{{Cite journal|last1=O'Neil|first1=P.|last2=O'Neil|first2=E.|year=2001|title=Database Principles, Programming, and Performance|location=[[San Francisco]]|publisher=[[Morgan Kaufmann Publishers]]|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}
*{{Cite journal|last1=Zaker|first1=M.|last2=Phon-Amnuaisuk|first2=S.|last3=Haw|first3=S.C.|year=2008|issue=2|volume=2|title=An Adequate Design for Large Data Warehouse Systems: Bitmap index versus B-tree index|journal=[[International Journal of Computers and Communications]]|url=http://www.universitypress.org.uk/journals/cc/cc-21.pdf|accessdate=2010-01-07|postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}

{{DEFAULTSORT:Bitmap Index}}
[[Category:Bit data structures|Index]]
[[Category:Data management]]
[[Category:Database index techniques]]
<=====doc_Id=====>:208
<=====title=====>:
Technical data management system
<=====text=====>:
{{Orphan|date=February 2009}}
A '''Technical Data Management System''' (TDMS) is essentially a [[Document management system]] (DMS) pertaining to the management of technical and [[engineering drawing]]s and documents. Often the data are contained in 'records' of various forms, such as on paper, microfilms or digital media. Hence technical [[data management]] is also concerned with record management involving technical data. Proper Technical Document [[Management system|Management Systems]] are essential for executions within large organisations with large scale projects involving engineering. For example, TDMS is a vital function for the successful management of Integrated Steel Plants (ISP), Automobile factories, Aero-space facilities, Infrastructure companies, City Corporations, Research Organisations, etc. In such organisations, Technical Archives or Technical Documentation Centres are created as central facilities for effective management of technical data and records.
[[File:Information processing system (english).svg|alt= A simplified example of information flow within a Technical Data Management System|thumb|A simplified example of information flow within a Technical Data Management System]]
TDMS functions are similar to that of conventional archive functions in concepts, except that the archived materials in this case are essentially engineering drawings, survey maps, [[Specification|technical specifications]], plant and equipment data sheets, feasibility reports, project reports, operation and maintenance manuals, standards, etc.

Document registration, indexing, repository management, reprography, etc. are parts of TDMS.  Various kinds of sophisticated technologies such as document scanners, microfilming and digitization camera units, wide format printers, digital plotters, software, etc. are available now, making TDMS functions an easier process than previous times.

== Crucial Constituents of a Technical Data Management System ==
Technical data refers to both scientific and technical information recorded and presented in any form or manner (excluding financial and management information).<ref>{{Cite web|url = http://www.businessdictionary.com/definition/technical-data.html|title = What is technical data? Definition and meaning|date = 2015-11-03|accessdate = 2015-11-03|website = BusinessDictionary.com|publisher = WebFinance, Inc|last = |first = }}</ref> A Technical Data Management System is created within an organisation for archiving and sharing information such as [[technical specifications]], datasheets and drawings. Similar to other types of data management system, a Technical Data Management System consists of the 4 crucial constituents mentioned below.

=== Data planning ===
Data plans (long-term or short-term) are constructed as the first essential step of a proper and complete TDMS. It is created to ultimately help with the 3 other constituents, Data Acquisition, Data Management and Data sharing. A proper data plan should not exceed 2 pages and should address the following basics:<ref>{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-planning|title = Data planning|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}</ref>
* Types of data (samples, experiment results, reports, drawings, etc.) and [[Metadata]] (Data that summarizes and describes other data. In this case, it refers to details such as sample sizes, experiment conditions and procedures, dates of reports, explanations of drawings, etc.)<ref>{{Cite web|url = http://whatis.techtarget.com/definition/metadata|title = metadata|date = July 2014|accessdate = 2015-11-03|website = WhatIs.com|publisher = Search engine optimization (SEO)|last = Rouse|first = Margaret}}</ref> 
* Means of researches and collections of data (field works, experiments in production lines, etc.)
* Costs of researches
* Policies for access, sharing (re-use within the organisation and re-distribution to the public)
* Proposals for archiving data and maintaining access to it

=== Data Acquisition ===
Raw Data is collected from Primary Sites of the organisations through the use of modern Technologies.<ref name=":0">{{Cite web|url = http://sine.ni.com/cs/app/doc/p/id/cs-13019#|title = By using powerful default components, TDM, NI DataFinder, and DIAdem, and without using a database, we considerably reduced our creation and maintenance costs.|date = 2015-11-03|accessdate = 2015-11-03|website = National Instruments|publisher = a-solution GmbH|last = Finkl|first = Karl}}</ref> Please reference the table below for examples.<ref name=":0" />
{| class="wikitable"
!Organisations
!Raw Data
!Primary Sites
!Technologies
|-
|Integrated steel plants, Automobile factories
|Feasibility reports, Equipment datasheets, etc.
|Test rigs and Controls
|Transiting software to digitize data and Input software for recording report results and details on datasheets
|-
|Aero-space facilities
|Engineering drawings, Operation manuals, maintenance logs, etc.
|Engineering labs
|Scanners for engineering drawings, Input software for maintenance logs
|-
|City corporations
|Survey maps, Population reports, etc.
|City to be mapped and City that involves the research
|Digital cameras for survey maps, Input software for statistics of population
|}
The data collected is then transferred to Technical Data Centres for Data Management.

=== Data Management ===
After Data Acquisition, data is sorted out, whilst useful data is archived, unwanted data is disposed. When managing and archiving data, the features below of the data are considered.<ref>{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-management|title = Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}</ref>
* Names, labels, values and descriptions for variables and records. (In the case of TDMS, one example is names of equipments on an equipment datasheet)
* Derived data from the original data, with code, algorithm or command file used to create them. (In the case of TDMS, one example is an expectation report derived from the analysis of an equipment datasheet)
* [[Metadata]] associates with the data being archived

=== Data Sharing ===
Archived and managed data are accessible to rightful entities. A proper and complete TDMS should share data to a suitable extent, under suitable security, in order to achieve optimal usage of data within the organisation. It aims for easy access when reused by other researchers and hence it enhances other research processes. Data is often referred in other tests and [[Specification (technical standard)|technical specifications]], where new analysis is generated, managed and archived again. As a result, data is flowing within the organisation under effective management through the use of TDMS.<ref>{{Cite web|url = https://www.libraries.psu.edu/psul/pubcur/what_is_dm.html#data-sharing|title = Data Sharing|date = 2015-11-03|accessdate = 2015-11-03|website = Data Curation|publisher = Penn State University Libraries|last = |first = }}</ref>

== Advantages and disadvantages of usage of Technical Data Management Systems ==
There are strengths and weakness when using Technical Data Management Systems (TDMS) to archive data. Some of the advantages and disadvantages are listed below.<ref>{{Cite web|url = https://razorleaf.com/solutions/technologies/product-data-management/|title = Product Data Management / Technical Data Management (PDM/TDM)|date = 2015-11-03|accessdate = 2015-11-03|website = Razorleaf Solutions|publisher = Razorleaf Corporation|last = |first = }}</ref><ref>{{Cite web|url = http://arxiv.org/ftp/arxiv/papers/1008/1008.1321.pdf|title = Contributions of PDM Systems in Organiza- tional Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Mechanical Engineering Informatics and Virtual Product Development Division (MIVP),  Vienna University of Technology|last1 = Ahmed|first1 = Zeeshan|last2 = Gerhard|first2 = Detlef}}</ref><ref>{{Cite web|url = http://www.flosim.com/calcium.aspx|title = Calcium - technical data management|date = 2015-11-03|accessdate = 2015-11-03|website = Flow Simulation|publisher = Flow Simulation Ltd.|last = |first = }}</ref>

=== Advantages ===

==== 1. Faster and easier data management ====

Since TDMS is integrated into the organisation's systems, whenever workers develop data files (SolidWorks, AutoCAD, Microsoft Word, etc.), they can also archive and manage data, linking what they need to their current work, at the same time they can also update the archives with useful data. This speeds up working processes and makes them more efficient.

==== 2. Increased security ====

All data files are centralized, hence internal and external data leakages are less likely to happen, and the data flow is more closely monitored. As a result, data in the organisation is more secured.

==== 3. Increased collaboration within the organisation ====

Since the data files are centralized and the data flow within the organisation increases, researchers and workers within the organisation are able to work on joint projects. More complex tasks can be performed for higher yields.

==== 4. Compatible to various formats of data ====

TDMS is compatible to many formats of data, from basic data like Microsoft Words to complex data like voice data. This enhances the quality of the management of data archived.

=== Disadvantages ===

==== 1. Higher financial costs ====

Implementing TDMS into the organisation's systems involves monetary costs. Maintenance costs certain amount of human resources and money as well. These resources involve opportunity costs as they can be utilized in other aspects.

==== 2. Lower stability ====

Since TDMS manages and centralizes all the data the organisation processes, it links the working processes within the whole organisation together. It also increases the vulnerability of the organisation data network. If TDMS is not stable enough or when it is exposed to hacker and virus attacks, the organisation's data flow might shut down completely, affecting the work in an organisation-wide scale and leading to a lower stability as results.

== Comparison between Traditional Data Management Approaches and Technical Data Management Systems ==
Test engineers and researchers are facing great challenges in turning complex test results and simulation data into usable information for higher yields of firms. These challenges are listed below.<ref>{{Cite web|url = http://www.ni.com/white-paper/7389/en/|title = From Raw Data to Engineering Results: The NI Technical Data Management Solution|date = 2015-10-13|accessdate = 2015-11-03|website = |publisher = National Instruments|last = |first = }}</ref>
* Increase in complication of designs
* Reduced in time and budgets available
* Higher quality is demanded
[[File:Logo oracle.jpg|alt= A company logo for Oracle|thumb|A company logo for Oracle]]

=== Traditional Data Management Approaches ===
Many organisations are still applying the conventional file management systems, due to the difficulty in building a proper and complete archives for data management.

The first approach is the simple file-folder system. This costs the problem of ineffectiveness as workers and researchers have to manually go through numerous layers of systems and files for the target data. Moreover, the target data may contain files with different formats and these files may not be stored in the same machine. These files are also easily lost if renamed or moved to another location.

The second approach is conventional databases such as Oracle. These databases are capable of enabling easy search and access of data. However, a great drawback is that huge effort for preparing and modeling the data is required. For large-scale projects, huge monetary costs are induced, and extra IT human resources must be employed for constant handling, expanding and maintaining the inflexible system, which is custom for specific tasks, instead of all tasks. In the long-term, it is not cost-effective.

=== Technical Data Management Systems(TDMS) ===
TDMS is developed based on 3 principles, flexible and organized file storage, self-scaling hybrid data index, and an interactive post-processing environment. The system in practical, mainly consists of 3 components, data files with essential and relevant [[Metadata]], data finders for organizing and managing data regardless of files formats, and, a software of searching, analyzing and reporting. With [[Metadata]] attached to original data files, the data finder can identify different related data files during searches, even if they are in different file formats. TDMS hence allows researchers to search for data like browsing the Internet. Last but not least, it can adapt to changes and update itself according to the changes, unlike databases.

== Comparison between Strong Information Systems and Weak Information Systems ==
Complex organizations may need large amounts of technical information, which can be distributed among several independent archives. Existing approaches span from “no integration” to “strong integration”, that is based on a common database or product model. The so-called “Weak Information Systems” (WIS)<ref>{{cite conference |url=http://www.marcolazzari.it/publications/weak-information-systems-for-technical-data-management-preprint.pdf |title=Weak information systems for technical data management |first= |last1=Salvaneschi |first1=Paolo |last2=Lazzari |first2=Marco |year=1997 |conference=Worldwide ECCE Symposium on computers in the practice of building and civil engineering |location=Lahti, Finland |pages=310–314 |access-date=2015-11-29 }}</ref> lie somewhere in the middle. Their basic concept is to add to the pre-existing information a new layer of multiple partial models of products and processes, so that it is possible to reuse existing databases, to reduce the development from scratch, and to provide evolutionary paths relevant for the development of the WIS. Each partial model may include specific knowledge and it acts as a way to structure and access the information according to a specific user view.
The comparison between strong and weak information systems may be summarized as follows:
{| class="wikitable"
!Strong information systems
!Weak information systems
|-
|Common data model
|Multiple specific integration models
|-
|Database oriented architecture
|Integration of multiple data sources by adding integration layers
|-
|One shot design
|Growing process
|-
|Redesign of legacy systems
|Integration of legacy systems
|}
The architecture of a weak information  system is composed of:
* information sources (databases, computational programs, ...);
* the integration layer.
The integration layer comprises the following sub-layers:
* abstraction layer (information models);
* communication layer between models and information sources;
* communication layer between models and humans (human-computer interface).

== Technical Data Management Systems in terms of regulations in different countries ==
In some countries, such as in the US, record and document management are considered very vital functions, and much stress is given in the management of Technical Archives. Records and documents coming under the public domain are governed by appropriate laws.<ref>{{Cite web|url = http://apps.americanbar.org/lpm/lpt/articles/tch01093.shtml|title = Document Management in the Digital Law Office|date = January 2009|accessdate = 2015-11-03|website = Law Practice Today|publisher = American Bar Association|last1 = Best|last2 = Foster|first1 = Steven J.|first2 = Debbie}}</ref> However, this has not been so in many underdeveloped and [[Developing country|developing nations]]. For example, India enacted the ' Public Records Act'<ref>{{Cite web|url = http://nationalarchives.nic.in/writereaddata/html_en_files/html/public_records93.html|title = THE PUBLIC RECORDS ACT, 1993 (India)|date = 1993-12-22|accessdate = 2015-11-03|website = |publisher = Government of India|last = MOHANPURIA|first = K.L.}}</ref> in 1993. However, many in the country are not aware of the existence of such a law or its importance.

== Applications and Examples of Technical Data Management Systems ==
Technical Data Management Systems (TDMS) are widely applied across the globe, in different sectors. Some of the examples are listed below.
* Voith Hydro tests models of the power plant turbines, including 4 main program parts, engine characteristics values, oscillation and cavitation, and transfer data from 1 program part to the next one using TDMS.<ref name=":0" />
* Danburykline created a knowledge and data platform, SOROS, which is following the wiki based approach. It aims to represent data in accessible and simple forms.<ref>{{Cite web|url = http://danburykline.co.uk/DKWP/?page_id=967|title = Knowledge & Technical Data Management|date = 2015-11-03|accessdate = 2015-11-03|website = |publisher = Danburykline|last = |first = }}</ref>
* Berghof develops and provides a TDMS to simplify and manage data for development of firms including automobile firms. This TDMS enables reserve of data, centralization of data volumes on an online server. It is also compatible to Windows PC and many other systems.<ref>{{Cite web|url = http://www.berghof.com/en/products/test-engineering/technical-data-management/|title = Data availability|date = 2015-11-03|accessdate = 2015-11-03|website = Test engineering Technical data management|publisher = Berghof|last = |first = }}</ref>
* This journal proposes the use of [[Cloud database|Cloud]] TDMS in third world countries for higher education purposes. Republic of Sudan is the model in this journal. Some of the solutions mentioned include online course delivery and online assignments and tests for greater class participation. Weaknesses mentioned include high financial costs and the fact that underdeveloped countries have not enough infrastructure to support such proposal.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7315ijdms02.pdf|title = Cloud Computing Architecture for higher education in the third world countries (Republic of the Sudan as model)|last = Adrees|first = Mohmed Sirelkhtem|date = June 2015|journal = International Journal of Database Management Systems ( IJDMS )|doi = 10.5121/ijdms.2015.7302|pmid = |access-date = 2015-11-03|volume = 7|last3 = Sheta|last2 = Omer|first2 = Majzoob Kamal Aldein|first3 = Osama E.|issue = 3}}</ref>
* This journal is about [[text simplification]]. The purpose of this text simplification project in the journal is to simplify high level knowledge in English, so that students in high level studies who do not have sufficient English foundations can learn about these knowledge and data more easily. The method to do so suggested by the journal is to introduce a TDMS that can transform complicated English words into easier words. A problem with this project is that the Internet is flooded with useless information and it is very difficult to sort out useful information for simplification.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7415ijdms01.pdf|title = Software feasibility study to transform complex scientific written knowledge to a clear, rationale and simple language|last = Khandelwal|first = Manoj|date = August 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7401|pmid = |access-date = 2015-11-03|last2 = Jafarabad|first2 = Mohammad|issue = 4|volume = 7}}</ref>
* This journal mentions about [[River basin|River Basin]] Information System (RBIS), which monitors data of different parts of a river basin, in order to identify which parts of the basin are gauging. Data is dynamic and lots of information has to be taken, which is impossible to do it manually. RBIS can help with this but one current weakness is that there are only 2 synoptic stations working (Kara and Niamtougou), whilst the rest are out of order.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms02.pdf|title = An information for integrated land and water resources management in the Kara River Basin (Togo and Benin)|last = BADJANA|first = Hèou Maléki|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7102|pmid = |access-date = 2015-11-03|last2 = ZANDER|first2 = Franziska|issue = 1|volume = 7|last3 = KRALISCH|first3 = Sven|last4 = HELMSCHROT|first4 = Jörg|last5 = FLÜGEL|first5 = Wolfgang-Albert}}</ref>

== Data mining ==
Data mining is an important criteria in constructing a technical Data Management System. For example, in building a E-commence platform, TDMS is needed to search and display information about the products.<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms01.pdf|title = Web-mining on Indonesia E-commerce site: Lazada and Rakuten|last = Simanjuntak|first = Humasak|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7101|pmid = |access-date = 2015-11-03|last2 = Sibarani|first2 = Novitasari|issue = 1|volume = 7|last3 = inaga|first3 = Bambang|last4 = Hutabarat|first4 = Novalina}}</ref> It indicates that it is essential to gather information from other systems, to archive and manage it properly, and finally, to share it to users.

== See also ==
[[Data management system|Data Management System]]

[[Data mining]]

[[Database]]

[[Information Systems Research]]

== Further reading ==
http://airccse.org/journal/ijdms/papers/7115ijdms03.pdf<ref>{{Cite journal|url = http://airccse.org/journal/ijdms/papers/7115ijdms03.pdf|title = Mining closed sequential patterns in large sequence databases|last = Raju|first = V. Purushothama|date = February 2015|journal = International Journal of Database Management System (IJDMS)|doi = 10.5121/ijdms.2015.7103|pmid = |access-date = 2015-11-03|last2 = Varma|first2 = G.P. Saradhi|issue = 1|volume = 7}}</ref>

https://seer.lcc.ufmg.br/index.php/jidm<ref>{{Cite journal|url = https://seer.lcc.ufmg.br/index.php/jidm|title = JOURNAL OF INFORMATION AND DATA MANAGEMENT|date = February 2015|journal = JOURNAL OF INFORMATION AND DATA MANAGEMENT|doi = |pmid = |access-date = 2015-11-03|volume = 6|issue = 1|editor-last = Traina Junior|editor-first = Caetano|editor2-last = Cordeiro|editor2-first = Robson L. F.|editor3-last = Amo|editor3-first = Sandra de|display-editors = 3 |editor4-last = Davis|editor4-first = Clodoveu|issn = 2178-7107}}</ref>

== External links ==
* http://airccse.org/journal/ijdms/Editorialboard.html

==References==

<references>
</references>

<nowiki/>

[[Category:Data management]]
[[Category:Document management systems]]
[[Category:Systems engineering]]
<=====doc_Id=====>:211
<=====title=====>:
Category:Data centers
<=====text=====>:
{{Commons category|Data centers}}
{{catmain|Data center}}

[[Category:Data management|Centers]]
[[Category:Servers (computing)]]
[[Category:Computers]]
<=====doc_Id=====>:214
<=====title=====>:
Global serializability
<=====text=====>:
{{Technical|date=January 2017}}
In [[concurrency control]] of ''[[database]]s'', ''[[transaction processing]]'' (''transaction management''), and other transactional [[Distributed computing|distributed applications]], '''Global serializability''' (or '''Modular serializability''') is a property of a ''global schedule'' of [[Database transaction|transactions]]. A global schedule is the unified [[schedule (computer science)|schedule]] of all the individual database (and other [[transactional object]]) schedules in a multidatabase environment (e.g., [[federated database]]). Complying with global serializability means that the global schedule is ''[[serializable (databases)|serializable]]'', has the ''[[serializability]]'' property, while each component database (module) has a serializable schedule as well. In other words, a collection of serializable components provides overall system serializability, which is usually incorrect. A need in correctness across databases in multidatabase systems makes global serializability a major goal for ''[[global concurrency control]]'' (or ''modular concurrency control''). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], and small, portable, powerful computing devices (e.g., [[smartphone]]s), as well as increase in [[systems management]] sophistication, the need for atomic distributed transactions and thus effective global serializability techniques, to ensure correctness in and among distributed transactional applications, seems to increase.

In a [[federated database system]] or any other more loosely defined multidatabase system, which are typically distributed in a communication network, transactions span multiple (and possibly [[Distributed database|distributed]]) databases. Enforcing global serializability in such system, where different databases may use different types of [[concurrency control]], is problematic. Even if every local schedule of a single database is serializable, the global schedule of a whole system is not necessarily serializable. The massive communication exchanges of conflict information needed between databases to reach [[Serializability#View and conflict serializability|conflict serializability]] globally would lead to unacceptable performance, primarily due to computer and communication [[latency (engineering)|latency]]. Achieving global serializability effectively over different types of concurrency control has been [[Open problem|open]] for several years. ''[[Commitment ordering]]'' (or Commit ordering; CO), a serializability technique publicly introduced in 1991 by [[Yoav Raz]] from [[Digital Equipment Corporation]] (DEC), provides an effective general solution for global ([[Serializability#View and conflict serializability|conflict]]) serializability across any collection of database systems and other [[transactional object]]s, with possibly different concurrency control mechanisms. CO does not need the distribution of conflict information, but rather utilizes the already needed (unmodified) [[atomic commitment]] protocol messages without any further communication between databases. It also allows [[Optimistic concurrency control|optimistic]] (non-blocking) implementations. CO generalizes ''[[Two-phase locking|Strong strict two phase locking]]'' (SS2PL), which in conjunction with the ''[[Two-phase commit protocol|Two-phase commit]]'' (2PC) protocol is the [[de facto standard]] for achieving global serializability across (SS2PL based) database systems. As a result, CO compliant database systems (with any, different concurrency control types) can transparently join existing SS2PL based solutions for global serializability. The same applies also to all other multiple (transactional) object systems that use atomic transactions and need global serializability for correctness (see examples above; nowadays such need is not smaller than with database systems, the origin of atomic transactions).

The most significant aspects of CO that make it a uniquely effective general solution for global serializability are the following:
#Seamless, low overhead integration with any concurrency control mechanism, with neither changing any transaction's operation scheduling or blocking it, nor adding any new operation.
#[[Heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] (e.g., [[database management system]]s) with different (any) concurrency control mechanisms, without interfering with the mechanisms' operations.
#[[Modularity]]: Transactional objects can be added and removed transparently.
#[[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|Autonomy]] of transactional objects: No need of conflict or equivalent information distribution (e.g., local precedence relations, locks, timestamps, or tickets; no object needs other object's information).
#[[Scalability]]: With "normal" global transactions, [[computer network]] size and number of transactional objects can increase unboundedly with no impact on performance, and
#Automatic global deadlock resolution.

All these aspects, except the first two, are also possessed by the popular [[Two-phase locking|SS2PL]], which is a (constrained, blocking) special case of CO and inherits many of CO's qualities.

==The global serializability problem==

===Problem statement===

The difficulties described above translate into the following problem:
:Find an efficient (high-performance and [[fault tolerant]]) method to enforce ''Global serializability'' (global conflict serializability) in a heterogeneous distributed environment of multiple autonomous database systems. The database systems may employ different [[concurrency control]] methods. No limitation should be imposed on the operations of either local transactions (confined to a single database system) or [[distributed transaction|global transactions]] (span two or more database systems).

===Quotations===
Lack of an appropriate solution for the global serializability problem has driven researchers to look for alternatives to [[serializability]] as a correctness criterion in a multidatabase environment (e.g., see ''[[Global serializability#Relaxing global serializability|Relaxing global serializability]]'' below), and the problem has been characterized as difficult and ''[[open problem|open]]''. The following two quotations demonstrate the mindset about it by the end of the year 1991, with similar quotations in numerous other articles:

*"Without knowledge about local as well as global transactions, it is highly unlikely that efficient global concurrency control can be provided... Additional complications occur when different component DBMSs [Database Management Systems] and the FDBMSs [Federated Database Management Systems] support different concurrency mechanisms... It is unlikely that a theoretically elegant solution that provides conflict serializability without sacrificing performance (i.e., concurrency and/or response time) and [[availability]] exists."<ref>Amit Sheth, James Larson (1990): [http://www.informatik.uni-trier.de/~ley/db/journals/csur/ShethL90.html  "Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases"], ''ACM Computing Surveys'', Vol. 22, No 3, pp. 183-236, September 1990 (quotation from page 227)</ref>

[[Commitment ordering]],<ref name=Raz1992/><ref name=Raz1994/> publicly introduced in May 1991 (see below), provides an efficient [[Elegance|elegant]] general solution, from both practical<ref name=Raz1990/><ref name=Raz1991/> and [[Theory|theoretical]]<ref name=Raz2009/> points of view, to the global serializability problem across database systems with possibly different concurrency control mechanisms. It provides conflict serializability with no negative effect on availability, and with no worse performance than the [[de facto standard]] for global serializability, CO's special case [[Two-phase locking#Strong strict two-phase locking|Strong strict two-phase locking]] (SS2PL). It requires knowledge about neither local nor global transactions.

*"Transaction management in a heterogeneous, distributed database system is a difficult  issue. The main problem is that each of the local database management systems may be using a different type of concurrency control scheme. Integrating this is a challenging problem, made worse if we wish to preserve the local autonomy of each of the local databases, and allow local and global transactions to execute in parallel. One simple solution is to restrict global transactions to retrieve-only access. However, the issue of reliable transaction management in the general case, where global and local transactions are allowed to both read and write data, is [[open problem|still open]]."<ref>[[Abraham Silberschatz]], [[Michael Stonebraker]], and [[Jeffrey Ullman]] (1991): [http://www.informatik.uni-trier.de/~ley/db/journals/cacm/SilberschatzSU91.html  "Database Systems: Achievements and Opportunities"], ''Communications of the ACM'', Vol. 34, No. 10, pp. 110-120, October 1991 (quotation from page 120)</ref>

The commitment ordering solution comprises effective integration of autonomous database management systems with possibly different concurrency control mechanisms. This while local and global transactions execute in parallel without restricting any read or write operation in either local or global transactions, and without compromising the systems' autonomy.

Even in later years, after the public introduction of the Commitment ordering general solution in 1991, the problem still has been considered by many unsolvable:

*"We present a transaction model for multidatabase systems with autonomous component systems, coined heterogeneous 3-level transactions. It has become evident that in such a system the requirements of guaranteeing full [[ACID]] properties and full local autonomy can not be reconciled..."<ref>Peter Muth (1997): [http://portal.acm.org/citation.cfm?id=264226  "Application Specific Transaction Management in Multidatabase Systems"], ''Distributed and Parallel Databases'', Volume 5, Issue 4, pp. 357 - 403, October 1997, {{ISSN|0926-8782}} (quotation from the article's Abstract)</ref>

The quotation above is from a 1997 article proposing a relaxed global serializability solution (see ''[[Global serializability#Relaxing global serializability|Relaxing global serializability]]'' below), and referencing [[Commitment ordering]] (CO) articles. The CO solution supports effectively both full [[ACID]] properties and full local autonomy, as well as meeting the other requirements posed above in the ''[[Global serializability#Problem statement|Problem statement]]'' section, and apparently has been misunderstood.

Similar thinking we see also in the following quotation from a 1998 article:

*"The concept of serializability has been the traditionally accepted correctness criterion in database systems. However in multidatabase systems (MDBSs), ensuring global serializability is a difficult task. The difficulty arises due to the heterogeneity of the concurrency control protocols used by the participating local database management systems (DBMSs), and the desire to preserve the autonomy of the local DBMSs. In general, solutions to the global serializability problem result in executions with a low degree of concurrency. The alternative, relaxed serializability, may result in data inconsistency."<ref name=Shar1998>Sharad Mehrotra, Rajeev Rastogi, Henry Korth, [[Abraham Silberschatz]] (1998):
[http://portal.acm.org/citation.cfm?id=277629 "Ensuring Consistency in Multidatabases by Preserving Two-Level Serializability"], ''ACM Transactions on Database Systems'' (TODS), Vol. 23, No. 2, pp. 199-230, June 1998 (quotation from the article's Abstract)</ref>

Also the above quoted article proposes a relaxed global serializability solution, while referencing the CO work. The CO solution for global serializability both bridges between different concurrency control protocols with no substantial concurrency reduction (and typically minor, if at all), and maintains the autonomy of local DBMSs. Evidently also here CO has been misunderstood. This misunderstanding continues to 2010 in a textbook by some of the same authors, where the same relaxed global serializability technique, ''Two level serializability'', is emphasized and described in detail, and CO is not mentioned at all.<ref name=Silber2010>[[Abraham Silberschatz|Avi Silberschatz]], Henry F Korth, S. Sudarshan (2010): [http://highered.mcgraw-hill.com/sites/0073523321/  ''Database System Concepts''], 6th Edition, McGraw-Hill, ISBN 0-07-295886-3</ref>

On the other hand, the following quotation on CO appears in a 2009 book:<ref name=Bern2009>[[Phil Bernstein|Philip A. Bernstein]], Eric Newcomer (2009): [http://www.elsevierdirect.com/product.jsp?isbn=9781558606234 ''Principles of Transaction Processing'', 2nd Edition],  Morgan Kaufmann (Elsevier), June 2009, ISBN 978-1-55860-623-4 (quotation from page 145)</ref>

*"Not all concurrency control algorithms use locks... Three other techniques are timestamp ordering, serialization graph testing, and commit ordering. '''Timestamp ordering''' assigns each transaction a timestamp and ensures that conflicting operations execute in timestamp order. '''Serialization graph testing''' tracks conflicts and ensures that the serialization graph is acyclic. '''Commit ordering''' ensures that conflicting operations are consistent with the relative order in which their transactions commit, which can enable interoperability of systems using different concurrency control mechanisms."

:'''Comments:'''
#Beyond the common locking based algorithm SS2PL, which is a CO variant itself, also additional variants of CO that use locks exist, (see below). However, generic, or "pure" CO does not use locks.
#Since CO mechanisms order the commit events according to conflicts that already have occurred, it is better to describe CO as "'''Commit ordering''' ensures that the relative order in which transactions commit is consistent with the order of their respective conflicting operations."

The characteristics and properties of the CO solution are discussed below.

===Proposed solutions===
Several solutions, some partial, have been proposed for the global serializability problem. Among them:

* ''Global [[serializability#Testing conflict serializability|conflict graph]]'' (serializability graph, [[precedence graph]]) ''checking''
* ''Distributed [[Two phase locking]]'' (Distributed 2PL)
* ''Distributed [[Timestamp-based concurrency control|Timestamp ordering]]''
* ''Tickets'' (local logical timestamps which define local total orders, and are propagated to determine global partial order of transactions)
* ''Commitment ordering''

===Technology perspective===
The problem of global serializability has been a quite intensively researched subject in the late 1980s and early 1990s. ''Commitment ordering'' (CO) has provided an effective general solution to the problem, insight into it, and understanding about possible generalizations of ''[[serializability#Common mechanism - SS2PL|strong strict two phase locking]]'' (SS2PL), which practically and almost exclusively has been utilized (in conjunction with the ''[[Two-phase commit protocol]]'' (2PC) ) since the 1980s to achieve global serializability across databases. An important side-benefit of CO is the automatic ''global deadlock'' resolution that it provides (this is applicable also to distributed SS2PL; though global deadlocks have been an important research subject for SS2PL, automatic resolution has been overlooked, except in the CO articles, until today (2009)). At that time quite many commercial database system types existed, many non-relational, and databases were relatively very small. Multi database systems were considered a key for database scalability by database systems interoperability, and global serializability was urgently needed. Since then the tremendous progress in computing power, storage, and communication networks, resulted in [[Order of magnitude|orders of magnitude]] increases in both centralized databases' sizes, transaction rates, and remote access to database capabilities, as well as blurring the boundaries between centralized computing and distributed one over fast, low-latency local networks (e.g., [[Infiniband]]). These, together with progress in database vendors' distributed solutions (primarily the popular SS2PL with 2PC based, a [[de facto standard]] that allows interoperability among different vendors' (SS2PL-based) databases; both SS2PL and 2PC technologies have gained substantial expertise and efficiency), [[workflow]] management systems, and [[database replication]] technology, in most cases have provided satisfactory and sometimes better [[information technology]] solutions without multi database atomic [[distributed transaction]]s over databases with different concurrency control (bypassing the problem above). As a result, the sense of urgency that existed with the problem at that period, and in general with high-performance distributed atomic transactions over databases with different concurrency control  types, has reduced. However, the need in concurrent distributed atomic transactions as a fundamental element of reliability exists in distributed systems also beyond database systems, and so the need in global serializability as a fundamental correctness criterion for such transactional systems (see also [[Serializability#Distributed serializability|Distributed serializability]] in [[Serializability]]). With the proliferation of the [[Internet]], [[Cloud computing]], [[Grid computing]], small, portable, powerful computing devices (e.g., [[smartphone]]s), and sophisticated [[systems management]] the need for effective global serializability techniques to ensure correctness in and among distributed transactional applications seems to increase, and thus also the need in Commitment ordering (including the popular for databases special case SS2PL; SS2PL, though, does not meet the requirements of many other transactional objects).

==The commitment ordering solution==
{{POV-section|Commitment ordering|date=November 2011}}
{{main|Commitment ordering}}
{{main|The History of Commitment Ordering}}

Commitment ordering<ref name=Raz1992>[[Yoav Raz]] (1992): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html "The Principle of Commitment Ordering, or Guaranteeing Serializability in a Heterogeneous Environment of Multiple Autonomous Resource Managers Using Atomic Commitment"] {{webarchive |url=https://web.archive.org/web/20070523182950/http://www.informatik.uni-trier.de/~ley/db/conf/vldb/Raz92.html |date=May 23, 2007 }}, ''Proc. of the Eighteenth Int. Conf. on Very Large Data Bases'' (VLDB), pp. 292-312, Vancouver, Canada, August 1992. (also DEC-TR 841, [[Digital Equipment Corporation]], November 1990) 
</ref><ref name=Raz1994>Yoav Raz (1994): [http://linkinghub.elsevier.com/retrieve/pii/0020019094900051 "Serializability by Commitment Ordering"], ''Information Processing Letters'', [http://www.informatik.uni-trier.de/~ley/db/journals/ipl/ipl51.html#Raz94 Volume 51, Number 5],  pp. 257-264, September 1994. (Received August 1991)</ref> (or Commit ordering; CO) is the only high-performance, [[fault tolerant]], [[Serializability#View and conflict serializability|conflict serializability]] providing solution that has been proposed as a fully distributed (no central computing component or data-structure are needed), general mechanism that can be combined seamlessly with any local (to a database) [[concurrency control]] mechanism (see [[Commitment ordering#Summary|technical summary]]). Since the CO property of a schedule is a [[necessary condition]] for global serializability of [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|''autonomous databases'']] (in the context of concurrency control), it provides the only general solution for autonomous databases (i.e., if autonomous databases do not comply with CO, then global serializability may be violated). Seemingly by sheer luck, the CO solution possesses many attractive properties: 
 
#does not interfere with any transaction's operation, particularly neither block, restrict nor delay any data-access operation (read or write) for either local or [[distributed transaction|global]] transactions (and thus does not cause any extra aborts); thus allows seamless integration with any concurrency control mechanism.
#allows [[Optimistic concurrency control|optimistic]] implementations (''non-blocking'', i.e., non data access blocking).
#allows [[heterogeneity]]: Global serializability is achieved across multiple [[transactional objects]] with different (any) concurrency control mechanisms, without interfering with the mechanisms' operations.
#allows [[modularity]]: Transactional objects can be added and removed transparently.
#allows full [[ACID]] transaction support.
#maintains each database's [[Commitment ordering#CO is a necessary condition for global serializability across autonomous database systems|autonomy]], and does not need any concurrency control information distribution (e.g., local precedence relations, locks, timestamps, or tickets).
#does not need any knowledge about the transactions.
#requires no communication overhead since it only uses already needed, unmodified ''[[atomic commitment]]'' protocol messages (any such protocol; using [[fault tolerant]] atomic commitment protocols and database systems makes the CO solution fault tolerant).
#automatically resolves global [[deadlock]]s due to [[lock (computer science)|locking]].
#[[Scalability|scales up]] effectively with [[computer network]] size and number of databases, almost without any negative impact on performance, since each global transaction is typically confined to certain relatively small numbers of databases and network nodes.
#requires no additional, artificial transaction access operations (e.g., "take [[Timestamp-based concurrency control|timestamp]]" or "take ticket"), which typically result in  additional, artificial conflicts that reduce concurrency.
#requires low overhead.

The only overhead incurred by the CO solution is locally detecting conflicts (which is already done by any known serializability mechanism, both pessimistic and optimistic) and locally ordering in each database system both the (local) commits of local transactions and the voting for atomic commitment of global transactions. Such overhead is low. The net effect of CO may be some delays of commit events (but never more delay than SS2PL, and on the average less). This makes CO instrumental for global concurrency control of multidatabase systems (e.g., [[federated database system]]s). The underlying ''Theory of Commitment ordering'',<ref name=Raz2009>Yoav Raz (2009): [http://sites.google.com/site/yoavraz2/home/theory-of-commitment-ordering Theory of Commitment Ordering - Summary] GoogleSites - Site of Yoav Raz. Retrieved 1 Feb, 2011.</ref> part of [[Serializability]] theory, is both sound and [[Scientific method#Hypothesis development|elegant]] (and even [[Mathematical beauty|"mathematically beautiful"]]; referring to structure and dynamics of conflicts, graph cycles, and deadlocks), with interesting implications for transactional [[Distributed computing|distributed applications]].

All the qualities of CO in the list above, except the first three, are also possessed by SS2PL, which is a special case of CO, but blocking and constraining. This partially explains the popularity of SS2PL as a solution (practically, the only solution, for many years) for achieving global serializability. However, property 9 above, automatic resolution of global deadlocks, has not been noticed for SS2PL in the database research literature until today (2009; except in the CO publications). This, since the phenomenon of voting-deadlocks in such environments and their automatic resolution by the [[atomic commitment]] protocol has been overlooked.

Most existing database systems, including all major commercial database systems, are ''[[serializability#Common mechanism - SS2PL|strong strict two phase locking (SS2PL)]]'' based and already CO compliant. Thus they can participate in a [[commitment ordering#Summary|CO based solution for global serializability in multidatabase environments]] without any modification (except for the popular ''[[Multiversion concurrency control|multiversioning]]'', where additional CO aspects should be considered). Achieving global serializability across SS2PL based databases using atomic commitment (primarily using ''[[two phase commit]], 2PC'') has been employed for many years (i.e., using the same CO solution for a specific special case; however, no reference is known prior to CO, that notices this special case's automatic global deadlock resolution by the atomic commitment protocol's [[commitment ordering#Exact characterization of voting-deadlocks by global cycles|augmented-conflict-graph]] global cycle elimination process). Virtually all existing distributed transaction processing environments and supporting products rely on SS2PL and provide 2PC. As a matter of fact SS2PL together with 2PC have become a [[de facto standard]]. This solution is a homogeneous concurrency control one, suboptimal (when both Serializability and [[Schedule (computer science)#Strict|Strictness]] are needed; see [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering]]; SCO) but still quite effective in most cases, sometimes at the cost of increased computing power needed relatively to the optimum. (However, for better performance [[Serializability#Relaxing serializability|relaxed serializability]] is used whenever applications allow). It allows inter-operation among SS2PL-compliant different database system types, i.e., allows heterogeneity in aspects other than concurrency control. SS2PL is a very constraining schedule property, and "takes over" when combined with any other property. For example, when combined with any [[Concurrency control#Concurrency control mechanisms|optimistic property]], the result is not optimistic anymore, but rather characteristically SS2PL. On the other hand, CO does not change data-access scheduling patterns at all, and ''any'' combined property's characteristics remain unchanged. Since also CO uses atomic commitment (e.g., 2PC) for achieving global serializability, as SS2PL does, any CO compliant database system or transactional object can transparently join existing SS2PL based environments, use 2PC, and maintain global serializability without any environment change. This makes CO a straightforward, natural generalization of SS2PL for any conflict serializability based database system, for all practical purposes.

Commitment ordering has been quite widely known inside the ''[[transaction processing]]'' and ''[[database]]s'' communities at ''[[Digital Equipment Corporation]]'' (DEC) since 1990. It has been under ''company confidentiality'' due to [[patent]]ing<ref name=Raz1990>Yoav Raz (1990): [http://yoavraz.googlepages.com/DEC-CO-MEMO-90-11-16.pdf  ''On the Significance of Commitment Ordering''] - Call for patenting, Memorandum, [[Digital Equipment Corporation]], November 1990.</ref>
<ref name=Raz1991>
Yoav Raz: US patents [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=3&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,899]  [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=2&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,504,900]   [http://patft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=1&f=G&l=50&co1=AND&d=PTXT&s1=%22commitment+ordering%22.TI.&OS=TTL/ 5,701,480]</ref> processes. CO was disclosed outside of DEC by lectures and technical reports' distribution to database researches in May 1991, immediately after its first patent filing. It has been misunderstood by many database researchers years after its introduction, which is evident by the quotes above from articles in 1997-1998 referencing Commitment ordering articles. On the other hand, CO has been utilized extensively as a solution for global serializability in works on [[Transactional processes]],
<ref>Heiko Schuldt, Hans-Jörg Schek, and Gustavo Alonso (1999): [http://portal.acm.org/citation.cfm?id=853907 "Transactional Coordination Agents for Composite Systems"], In ''Proceedings of the 3rd International Database Engineering and Applications Symposium'' (IDEAS’99), IEEE Computer Society Press, Montrteal, Canada, pp. 321–331.</ref>
<ref>Klaus Haller, Heiko Schuldt, Can Türker (2005): [http://portal.acm.org/citation.cfm?doid=1099554.1099563 "Decentralized coordination of transactional processes in peer-to-peer environments",] ''Proceedings of the 2005 ACM CIKM, International Conference on Information and Knowledge Management'', pp. 28-35, Bremen, Germany, October 31 - November 5, 2005, ISBN 1-59593-140-6</ref> and more recently in the related '''Re:GRIDiT''', 
<ref>Laura Cristiana Voicu, Heiko Schuldt, Fuat Akal, Yuri Breitbart, Hans Jörg Schek (2009): [http://dbis.cs.unibas.ch/publications/2009/grid2009/dbis_publication_view  "Re:GRIDiT – Coordinating Distributed Update Transactions on Replicated Data in the Grid"], ''10th IEEE/ACM International Conference on Grid Computing (Grid 2009)'', Banff, Canada, 2009/10.</ref>
<ref>Laura Cristiana Voicu and Heiko Schuldt (2009): [http://dbis.cs.unibas.ch/publications/2009/clouddb09/dbis_publication_view  "How Replicated Data Management in the Cloud can benefit from a Data Grid Protocol — the Re:GRIDiT Approach"], ''Proceedings of the 1st International Workshop on Cloud Data Management (CloudDB 2009)'', Hong Kong, China, 2009/11.</ref>
which is an approach for transaction management in the converging [[Grid computing]] and [[Cloud computing]]. 
See more in ''[[The History of Commitment Ordering]]''.

==Relaxing global serializability==
Some techniques have been developed for '''relaxed global serializability''' (i.e., they do not guarantee global serializability; see also ''[[Serializability#Relaxing serializability|Relaxing serializability]]''). Among them (with several publications each):

* ''Quasi serializability''<ref name=Du1989>Weimin Du and Ahmed K. Elmagarmid (1989): [http://www.informatik.uni-trier.de/~ley/db/conf/vldb/DuE89.html  "Quasi Serializability: a Correctness Criterion for Global Concurrency Control in InterBase"], ''Proceedings of the Fifteenth International Conference on Very Large Data Bases'' (VLDB), August 22–25, 1989, Amsterdam, The Netherlands, pp. 347-355, Morgan Kaufmann, ISBN 1-55860-101-5</ref>
* ''Two-level serializability''<ref name=Shar1998 />

While local (to a database system) relaxed serializability methods compromise ''serializability'' for performance gain (and are utilized only when the application can tolerate possible resulting inaccuracies, or its integrity is unharmed), it is unclear that various proposed ''relaxed global serializability'' methods which compromise ''global serializability'', provide any performance gain over ''commitment ordering'' which guarantees global serializability. Typically, the declared intention of such methods has not been performance gain over effective global serializability methods (which apparently have been unknown to the inventors), but rather correctness criteria alternatives due to lack of a known effective global serializability method. Oddly, some of them were introduced years after CO had been introduced, and some even quote CO without realizing that it provides an effective global serializability solution, and thus without providing any performance comparison with CO to justify them as alternatives to global serializability for some applications (e.g., ''Two-level serializability''<ref name=Shar1998 />). ''Two-level serializability'' is even presented as a major global concurrency control method in a 2010 edition of a text-book on databases<ref name=Silber2010/> (authored by two of the original authors of Two-level serializability, where one of them, [[Abraham Silberschatz|Avi Silberschatz]], is also an author of the original ''[[The History of Commitment Ordering#AESO is modified to Strong recoverability (CO)|Strong recoverability]]'' articles). This book neither mentions CO nor references it, and strangely, apparently does not consider CO a valid ''Global serializability'' solution.

Another common reason nowadays for Global serializability relaxation is the requirement of [[availability]] of [[internet]] products and [[Internet service provider|services]]. This requirement is typically answered by large scale data [[Replication (computer science)|replication]]. The straightforward solution for synchronizing replicas' updates of a same database object is including all these updates in a single atomic [[distributed transaction]]. However, with many replicas such a transaction is very large, and may span several [[computer]]s and [[computer network|networks]] that some of them are likely to be unavailable. Thus such a transaction is likely to end with abort and miss its purpose.<ref name=Gray1996>{{cite conference
 | author = [[Jim Gray (computer scientist)|Gray, J.]]
 |author2=Helland, P. |author3=[[Patrick O'Neil|O’Neil, P.]] |author4=[[Dennis Shasha|Shasha, D.]]
 | year = 1996
 | title = The dangers of replication and a solution
 | conference = Proceedings of the 1996 [[ACM SIGMOD International Conference on Management of Data]]
 | pages = 173–182
 | url = ftp://ftp.research.microsoft.com/pub/tr/tr-96-17.pdf
 | doi = 10.1145/233269.233330
 }}</ref>
Consequently, [[Optimistic replication]] (Lazy replication) is often utilized (e.g., in many products and services by [[Google]], [[Amazon.com|Amazon]], [[Yahoo]], and alike), while Global serializability is relaxed and compromised for [[Eventual consistency]]. In this case relaxation is done only for applications that are not expected to be harmed by it.

Classes of schedules defined by ''relaxed global serializability'' properties either contain the global serializability class, or are incomparable with it. What differentiates techniques for ''relaxed global conflict serializability'' (RGCSR) properties from those of ''relaxed conflict serializability'' (RCSR) properties that are not RGCSR is typically the different way ''global cycles'' (span two or more databases) in the ''global conflict graph'' are handled. No distinction between global and local cycles exists for RCSR properties that are not RGCSR. RCSR contains RGCSR. Typically RGCSR techniques eliminate local cycles, i.e., provide ''local serializability'' (which can be achieved effectively by regular, known [[concurrency control]] methods), however, obviously they do not eliminate all global cycles (which would achieve global serializability).

==References==
{{reflist|33em}}

{{DEFAULTSORT:Global Serializability}}
[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]
<=====doc_Id=====>:217
<=====title=====>:
Social information architecture
<=====text=====>:
{{orphan|date=April 2011}}
'''Social information architecture''' is a sub-domain of [[information architecture]] which deals with the social aspects of conceptualizing, modeling and organizing information. Social Information Architecture, also known as Social iA <ref>[http://sweetinformationarchitecture.net/social-information-architecture/%20 Sweet Information Architecture]</ref> has become more relevant because of the rise of [[Social Media]] and [[Web 2.0]] in recent times.

== Approach ==
There are different approaches to the explanation of Social iA. 

===A) The architecture model (internal space)===

Architects designing a physical community space, have to consider how the architecture will shape social interactions. A long hallway of offices creates an utterly different dynamic than desks with arranged in an open space. One might foster individuality, privacy, propriety; the other: collaboration, distraction, communalism.

Still, physical spaces can be flexibly repurposed and worked around if the inhabitants desire a social dynamic not instantly afforded by the space. Office doors can be left open to invite easier interaction. Partitions can be raised between adjacent desks to limit distraction and increase privacy.

That’s physical architecture. The information architectures of online communities are far more deterministic and far less flexible. They literally define the social architecture by pre-specifying in immutable computer code what information you have access to, who you can talk to, where you can go. In the online world, information architecture = social architecture.<ref>http://www.steinbock.org/</ref>

===B) The social dialogue and information model (external space)===

All  major brands use information architecture to market their products online, it is then commonly wrapped under the umbrella phrase 'digital strategy'. Information architecture used for strategic purposes encompasses brand [[SEO]], strategic placement of virals, social media presence etc. 

Charities, news outlets and social dialogue forums can make a much more specific use of the same tools for positive and  important social purposes. Social Information Architecture is perceived as the socially conscious wing of  commercial information architecture <ref>http://www.sweetinformationarchitecture.net</ref> and function to exchange information and ideas between people and groups. 

Social iA can pick up on conflicting issues that are treated with misunderstanding between  cultures and leaves individuals and societies vulnerable to exploitation and manipulation. Since the net has such a far reach it is obvious to use it for meaningful and coordinated social dialogue. 

Example of such issues are faith, environment,  politics, climate change, war, injustice and other social challenges. Information architecture can  help create frameworks in which sharing information brings people together, inspires and encourages them to participate in a forward thinking and unfragmented way. One of its core activities is to spread messages that bring people from opposite sites of social  and cultural spectrums together and to confront uncomfortable subject head on.

== How does social information architecture work? ==
Social iA utilizes a variety of [[Web2.0]] applications to filter relevant or valuable information and weave them in appropriate information repository or provide feedback to interesting channels. Social iA makes strategic use of Search Engines, Social Media, Google Algorithms, as well as websites, video & news channels. It ‘reads’ or 'listens' to social conversations and [[search engine]] queries and engages with the net actively to gather clues about the world’s pulse on the internet. It assesses data, social & political trends, and respond with targeted campaigns to give people ideas, as well as help people with making sense of information.

== Principals ==
Dan Brown in his paper 8 Principals of Social Information Architecture <ref>[http://socialinformationarchitecture.org.uk/paper/8principal_infoarchi.pdf Eight Principles of Information Architecture], Dan Brown. Published in the Bulletin of the American Society for Information Science and Technology – August/September 2010 – Volume 36, Number 6</ref> enlists the following principals:<br />
1. The principle of objects: Treat content as a living, breathing thing,
with a lifecycle, behaviors and attributes.<br />
2. The principle of choices: Create pages that offer meaningful choices to users, keeping the range of choices available focused on a particular task. <br />
3. The principle of disclosure: Show only enough information to help
people understand what kinds of information they’ll find as they dig
deeper. <br />
4. The principle of exemplars: Describe the contents of categories by
showing examples of the contents.<br />
5. The principle of front doors: Assume at least half of the website’s
visitors will come through some page other than the home page.<br />
6. The principle of multiple classification: Offer users several different classification schemes to browse the site’s content.<br />
7. The principle of focused navigation: Don’t mix apples and oranges
in your navigation scheme.<br />
8. The principle of growth: Assume the content you have today is a
small fraction of the content you will have tomorrow.

== What can social information architecture achieve? ==
Social information architecture has many potentials in terms of fostering social connections and how information is shared in social spaces on the web.

== References==
{{Reflist}}

== See also ==
Wodtke, Christina and Govella, Austin '''Information Architecture: Blueprints for the Web''' (2009) Second Edition, Published by New Riders

{{Semantic Web}}

[[Category:Information architects|*Information architecture]]
[[Category:World Wide Web]]
[[Category:Data management]]
[[Category:Information science]]
[[Category:Information technology]]
[[Category:Digital technology]]
[[Category:New media]]
<=====doc_Id=====>:220
<=====title=====>:
Customer data management
<=====text=====>:
'''Customer data management (CDM)''' is the ways in which businesses keep track of their customer information and survey their [[customer base]] in order to obtain feedback. CDM embraces a range of software or [[cloud computing]] applications designed to give large organizations rapid and efficient access to customer data. Surveys and data can be centrally located and widely accessible within a company, as opposed to being warehoused in separate departments. CDM encompasses the collection, analysis, organizing, reporting and sharing of customer information throughout an organization. Businesses need a thorough understanding of their customers’ needs if they are to retain and increase their customer base. Efficient CDM solutions provide companies with the ability to deal instantly with customer issues and obtain immediate feedback. As a result, [[customer retention]] and [[customer satisfaction]] can show dramatic improvement. According to a recent study by [[Aberdeen Group]] inc.: "Above-average and best-in-class companies... attain greater than 20% annual improvement in retention rates, revenues, data accuracy and partner/customer satisfaction rates."<ref>Smalltree, Hannah (2006) [http://searchcrm.techtarget.com/news/1212337/Best-practices-in-managing-customer-data]</ref>

== Customer data management and cloud computing ==

Cloud computing offers an attractive choice for CDM in many companies due to its accessibility and [[Cost efficiency|cost-effectiveness]]. Businesses can decide who, within their company, should have the ability to create, adjust, analyze or share customer information. In December 2010, 52% of [[Information technology|Information Technology]] (IT) professionals worldwide were deploying, or planning to deploy, cloud computing;<ref>Cisco.com (December 2010) [http://newsroom.cisco.com/dlls/2010/prod_120810.html]</ref> this percentage is far higher in many countries.

== Uses for management ==

'''Customer data management'''
* should provide a cost-effective, user-friendly solution for [[marketing]], research, sales, [[human resources]] and IT departments
* enables companies to create and email online surveys, reports and newsletters
* encompasses and simplifies [[customer relationship management]] (CRM) and [[Customer feedback management services|customer feedback management]] (CFM)

== Background ==

Customer data management, as a term, was coined in the 1990s, pre-dating the alternative term [[enterprise feedback management]] (EFM). Customer data management (CDM) was introduced as a software solution that would replace earlier disc-based or paper-based surveys and [[spreadsheet]] data. Initially, CDM solutions were marketed to businesses as software, specific to one company, and often to one department within that company. This was superseded by [[application service  provider]]s (ASPs) where software was hosted for [[end user]] organizations, thus avoiding the necessity for IT professionals to deploy and support software. However, ASPs with their single-tenancy architecture were, in turn, superseded by [[software as a service]] (SaaS), engineered for multi-tenancy. By 2007 SaaS applications, giving businesses on-demand access to their customer information, were rapidly gaining popularity compared with ASPs. Cloud computing now includes SaaS and many prominent CDM providers offer cloud-based applications to their clients.

In recent years, there has been a push away from the term EFM, with many of those working in this area advocating the slightly updated use of CDM. The return to the term CDM is largely based on the greater need for clarity around the solutions offered by companies, and on the desire to retire terminology veering on techno-jargon that customers may have a hard time understanding.<ref>InSiteSystems.com (December, 2010) [http://www.insitesystems.com/systems/blogs/the-problem-with-efm.html]</ref>

== References ==
<!--- See [[Wikipedia:Footnotes]] on how to create references using <ref></ref> tags which will then appear here automatically -->
{{Reflist}}

<!--- Categories --->
[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]
<=====doc_Id=====>:223
<=====title=====>:
Super column
<=====text=====>:
[[Image:SuperColumn (data store).png|300px|thumb|The super column consists of a (unique) super column name, and a number of columns.]]
A '''super column''' is a [[tuple]] (a pair) with a binary super column name and a value that maps it to many columns.<ref>{{cite web
| accessdate = 2011-03-18
| author = Arin Sarkissian
| date = 2009-09-01
| location = http://arin.me/post/40054651676/wtf-is-a-supercolumn-cassandra-data-model
| publisher = Arin Sarkissian
| title = WTF is a SuperColumn? An Intro to the Cassandra Data Model
| quote = A SuperColumn is a tuple with a binary name & a value which is a map containing an unbounded number of Columns – keyed by the Column‘s name.
| url = }}</ref> They consist of a key-value pairs, where the values are columns. Theoretically speaking, super columns are ([[Sorting algorithm|sorted]]) [[associative array]] of columns.<ref>{{cite web
| accessdate = 2011-03-18
| location = http://wiki.apache.org/cassandra/DataModel
| publisher = Apache Cassandra
| title = Cassandra wiki: Data Model: Super columns
| url = http://wiki.apache.org/cassandra/DataModel}}</ref> Similar to a regular [[column family]] where a row is a sorted map of column names and column values, a row in a super column family is a sorted map of super column names that maps to column names and column values. 

A super column is part of a [[keyspace (data model)]] together with other super columns and column families, and columns.

==Code example==
Written in the [[JSON]]-like syntax, a super column definition can be like this:

<source lang="SQL">
 {
   "mccv": {
     "Tags": {
       "cassandra": {
         "incubator": {"url": "http://incubator.apache.org/cassandra/"},
         "jira": {"url": "http://issues.apache.org/jira/browse/CASSANDRA"}
       },
       "thrift": {
         "jira": {"url": "http://issues.apache.org/jira/browse/THRIFT"}
       }
     }
   }
 }
</source>

==See also==
* [[Column (data store)]]
* [[Keyspace (NoSQL)]]

==References==
{{reflist}}

==External links==
* [http://wiki.apache.org/cassandra/DataModel The Apache Cassandra data model]

<!--Interwikies-->
[[Category:Data_management]]
<!--Categories-->


{{database-stub}}
<=====doc_Id=====>:226
<=====title=====>:
PureXML
<=====text=====>:
{{Lowercase title}}
'''pureXML''' is the native [[XML]] storage feature in the [[IBM DB2]] data server.  pureXML provides [[query language]]s, storage technologies, indexing technologies, and other features to support XML data.  The word ''pure'' in pureXML was chosen to indicate that DB2 natively stores and natively processes XML data in its inherent hierarchical structure, as opposed to treating XML data as plain text or converting it into a relational format.<ref>http://www.ibm.com/developerworks/blogs/page/datastudioteam?entry=purexml_and_purequery_what_s</ref>

== Technical information ==
DB2 includes two distinct storage mechanisms: one for efficiently managing traditional SQL data types, and another for managing XML data.  The underlying storage mechanism is transparent to users and applications; they simply use SQL (including SQL with XML extensions or [[SQL/XML]]) or [[XQuery]] to work with the data.

XML data is stored in columns of DB2 tables that have the XML data type.  XML data is stored in a parsed format that reflects the hierarchical nature of the original XML data.  As such, pureXML uses trees and nodes as its model for storing and processing XML data.  If you instruct DB2 to validate XML data against an XML schema prior to storage, DB2 annotates all nodes in the XML hierarchy with information about the schema types; otherwise, it will annotate the nodes with default type information.  Upon storage, DB2 preserves the internal structure of XML data, converting its tag names and other information into integer values. Doing so helps conserve disk space and also improves the performance of queries that use navigational expressions. However, users aren't aware of this internal representation.  Finally, DB2 automatically splits XML nodes across multiple database pages, as needed.

XML schemas specify which XML elements are valid, in what order these elements should appear in XML data, which XML data types are associated with each element, and so on.  pureXML allows you to validate the cells in a column of XML data against no schema, one schema, or multiple schemas.  pureXML also provides tools to support evolving XML schemas.

IBM has enhanced its [[programming language]] interfaces to support access to its XML data. These enhancements span [[Java (programming language)|Java]] ([[JDBC]]), [[C (programming language)|C]] (embedded SQL and call-level interface), [[COBOL]] (embedded SQL), [[PHP]], and [[Microsoft]]'s [[.NET Framework]] (through the DB2.NET provider).

== History ==
pureXML was first included in the DB2 9 for [[Linux]], [[Unix]], and [[Microsoft Windows]] release, which was codenamed Viper, in June 2006.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | title = IBM News room - 2006-06-08 IBM Transforms Database Market With Introduction of DB2 - United States | archiveurl= https://web.archive.org/web/20121011235127/http://www-03.ibm.com/press/us/en/pressrelease/19781.wss | archivedate= 2012-10-11 }}</ref>  It was available on DB2 9 for [[z/OS]] in March 2007.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | title = IBM News room - 2007-03-06 IBM Unveils DB2 Viper for the Mainframe - United States | archiveurl= https://web.archive.org/web/20121011235143/http://www-03.ibm.com/press/us/en/pressrelease/21189.wss | archivedate= 2012-10-11 }}</ref>  In October 2007, IBM released DB2 9.5 with improved XML data transaction performance and improved storage savings.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | title = IBM News room - 2007-10-15 IBM Extends Data Server Technology Lead With Introduction of DB2 &quot;Viper 2&quot; - United States | archiveurl= https://web.archive.org/web/20121011235149/http://www-03.ibm.com/press/us/en/pressrelease/22455.wss | archivedate= 2012-10-11 }}</ref> In June 2009, IBM released DB2 9.7 with XML supported for database-partitioned, range-partitioned, and multi-dimensionally clustered tables as well as compression of XML data and indices.<ref>{{cite web| url= http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | title = IBM News room - 2009-04-22 IBM Database Software Improves Operational Efficiency and Cuts Storage Costs by Up to 75% - United States | archiveurl= https://web.archive.org/web/20121121014600/http://www-03.ibm.com/press/us/en/pressrelease/27279.wss | archivedate= 2012-11-21 }}</ref>

== Competition ==
{{See also|XML database}}
DB2 is a hybrid data server—it offers data management for traditional relational data, as well as providing native XML data management.  Other vendors that offer data management for both relational data and native XML storage include [[Oracle Corporation|Oracle]] with its [[Oracle Database|11g]] product and Microsoft with its [[Microsoft SQL Server|SQL Server]] product.

pureXML also competes with native XML databases like [[BaseX (database)|BaseX]], [[eXist]], [[MarkLogic]] or [[Sedna (database)|Sedna]].

== User groups ==
The International DB2 Users Group (IDUG) is an independent, not-for-profit association of IT professionals who use IBM DB2.  IDUG provides education, technical resources, peer networking opportunities, online resources and other programs for DB2 users.

== Books ==
IBM International Technical Support Organization (ITSO) has published the following books, which are available in print or as free e-books:
* [http://www.redbooks.ibm.com/abstracts/sg247298.html?Open DB2 9: pureXML Overview and Fast Start]
* [http://www.redbooks.ibm.com/abstracts/sg247315.html?Open DB2 9 pureXML Guide]

The following books are also available for purchase:
* [http://www.amazon.com/DB2-pureXML-Cookbook-Master-Hybrid/dp/0138150478/ DB2 pureXML Cookbook: Master the Power of IBM Hybrid Data Server]

== Education and training ==
The following pureXML classroom and online courses are available from IBM Education:
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&courseCode=CG130 Query and Manage XML Data with DB2 9].  IBM course CG130.  Classroom.  Duration: 4 days.
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_description&courseCode=CG100 Query XML Data with DB2 9].  IBM course CG100.  Classroom.  Duration: 2 days (first 2 days of CG130).
* Managing XML Data in DB2 9.  IBM course CG160.  Classroom.  Duration: 2 days (last 2 days of CG130).
* [http://www-304.ibm.com/jct03001c/services/learning/ites.wss/us/en?pageType=course_search&sortBy=5&searchType=1&sortDirection=9&includeNotScheduled=15&rowStart=0&rowsToReturn=20&maxSearchResults=200&searchString=CT140&language=en&country=us DB2 pureXML].  IBM Course CT140.  Self-paced study plus Live Virtual Classroom.

== See also ==
* [[IBM DB2]]
* [[XML database]]

== References ==
{{Reflist}}

== External links ==
* {{official website|http://www.ibm.com/software/data/db2/xml}}
* [http://www.ibm.com/developerworks/wikis/display/db2xml/Home pureXML Wiki]
* [http://www.ibm.com/developerworks/forums/forum.jspa?forumID=1423 pureXML Forum]
* [http://www.ibm.com/developerworks/blogs/page/purexml pureXML Team Blog]
* [http://www.nativexmldatabase.com Native XML Database Blog]
* [http://blog.4loeser.net Blog with pureXML Topics]

=== Online communities ===
Online communities allow pureXML users to network with fellow professionals.
* [http://www.linkedin.com/groups?gid=129185 pureXML Group on LinkedIn]

{{IBM DB2 product family}}

[[Category:XML software]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:IBM DB2]]
[[Category:IBM software]]
[[Category:XML databases]]
<=====doc_Id=====>:229
<=====title=====>:
Data deduplication
<=====text=====>:
{{multiple issues|
{{original research|date=February 2011}}
{{More footnotes|date=September 2009}}
}}

In [[computing]], '''data deduplication''' is a specialized [[data compression]] technique for eliminating duplicate copies of repeating data. Related and somewhat synonymous terms are '''intelligent (data) compression''' and '''[[single-instance storage|single-instance (data) storage]]'''. This technique is used to improve storage utilization and can also be applied to network data transfers to reduce the number of bytes that must be sent. In the deduplication process, unique chunks of data, or byte patterns, are identified and stored during a process of analysis. As the analysis continues, other chunks are compared to the stored copy and whenever a match occurs, the redundant chunk is replaced with a small reference that points to the stored chunk. Given that the same byte pattern may occur dozens, hundreds, or even thousands of times (the match frequency is dependent on the chunk size), the amount of data that must be stored or transferred can be greatly reduced.<ref>"[http://www.druva.com/blog/2009/01/09/understanding-data-deduplication/ Understanding Data Deduplication]" Druva, 2009. Retrieved 2013-2-13</ref>

This type of deduplication is different from that performed by standard file-compression tools, such as [[LZ77 and LZ78]]. Whereas these tools identify short repeated substrings inside individual files, the intent of storage-based data deduplication is to inspect large volumes of data and identify large sections – such as entire files or large sections of files – that are identical, in order to store only one copy of it. This copy may be additionally compressed by single-file compression techniques. For example, a typical email system might contain 100 instances of the same 1 MB ([[megabyte]]) file attachment. Each time the [[email]] platform is backed up, all 100 instances of the attachment are saved, requiring 100 MB storage space. With data deduplication, only one instance of the attachment is actually stored; the subsequent instances are referenced back to the saved copy for deduplication ratio of roughly 100 to 1.

==Benefits==
* Storage-based data deduplication reduces the amount of storage needed for a given set of files. It is most effective in applications where many copies of very similar or even identical data are stored on a single disk—a surprisingly common scenario. In the case of data backups, which routinely are performed to protect against data loss, most data in a given backup remain unchanged from the previous backup. Common backup systems try to exploit this by omitting (or [[hard link]]ing) files that haven't changed or storing [[Data differencing|differences]] between files.  Neither approach captures all redundancies, however. Hard-linking does not help with large files that have only changed in small ways, such as an email database;  differences only find redundancies in adjacent versions of a single file (consider a section that was deleted and later added in again, or a logo image included in many documents).
* Network data deduplication is used to reduce the number of bytes that must be transferred between endpoints, which can reduce the amount of bandwidth required. See [[WAN optimization]] for more information.
* Virtual servers benefit from deduplication because it allows nominally separate system files for each virtual server to be coalesced into a single storage space. At the same time, if a given server customizes a file, deduplication will not change the files on the other servers—something that alternatives like hard links or shared disks do not offer.  Backing up or making duplicate copies of virtual environments is similarly improved.

==Deduplication overview==
Deduplication may occur "in-line", as data is flowing, or "post-process" after it has been written.

===Post-process deduplication===
With post-process deduplication, new data is first stored on the storage device and then a process at a later time will [[analysis|analyze]] the data looking for duplication. The benefit is that there is no need to wait for the hash calculations and lookup to be completed before storing the data, thereby ensuring that store performance is not degraded. Implementations offering policy-based operation can give users the ability to defer optimization on "active" files, or to process files based on type and location. One potential drawback is that duplicate data may be unnecessarily stored for a short time, which can be problematic if the system is nearing full capacity.

===In-line deduplication===
Alternatively, deduplication hash calculations can be done in real-time as data enters the target device. If the storage system identifies a block which it has already stored, only a reference to the existing block is stored, rather than the whole new block.

The advantage of in-line deduplication over post-process deduplication is that it requires less storage, since duplicate data is never stored.  On the negative side, it is frequently argued{{by whom|date=August 2016}} that because hash calculations and lookups take so long, [[data ingestion]] can be slower, thereby reducing the backup throughput of the device.  However, certain vendors with in-line deduplication have demonstrated equipment with similar performance to their post-process deduplication counterparts{{according to whom|date=April 2015}}.

Data coming in is stored into "lining space" before it hits real storage blocks. On SSD disks lining space is provided using [[Non-volatile random-access memory|NVRAM]] which is not cost efficient{{according to whom|date=August 2016}}.

Post-process and in-line deduplication methods are often heavily debated.<ref>{{cite web|url=http://www.backupcentral.com/content/view/134/47/ |title=In-line or post-process de-duplication? (updated 6-08) |publisher=Backup Central |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091206035054/http://www.backupcentral.com:80/content/view/134/47 |archivedate=2009-12-06 |df= }}</ref><ref>{{cite web|url=http://searchdatabackup.techtarget.com/tip/0,289483,sid187_gci1315295,00.html |title=Inline vs. post-processing deduplication appliances |publisher=Searchdatabackup.techtarget.com |date= |accessdate=2009-10-16}}</ref>

===Data formats===
[[SNIA Dictionary]] identifies two methods:
* content-agnostic data deduplication - a data deduplication method that does not require awareness of specific application data formats. 
* content-aware data deduplication - a data deduplication method that leverages knowledge of specific application data formats.

===Source versus target deduplication===
Another way to classify data deduplication methods is according to where they occur. Deduplication occurring close to where data is created, is often referred to{{according to whom|date=August 2016}} as "source deduplication". When it occurs near where the data is stored, it is commonly called "target deduplication".

* Source deduplication ensures that data on the data source is deduplicated.  This generally takes place directly within a file system.<ref>{{cite web|url=http://www.microsoft.com/windowsserver2008/en/us/WSS08/SIS.aspx |title=Windows Server 2008: Windows Storage Server 2008 |publisher=Microsoft.com |date= |accessdate=2009-10-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20091004073508/http://www.microsoft.com:80/windowsserver2008/en/us/WSS08/SIS.aspx |archivedate=2009-10-04 |df= }}</ref><ref>{{cite web|url=http://www.netapp.com/us/products/platform-os/dedupe.html |title=Products - Platform OS |publisher=NetApp |date= |accessdate=2009-10-16}}</ref>  The file system will periodically scan new files creating hashes and compare them to hashes of existing files.   When files with same hashes are found then the file copy is removed and the new file points to the old file.  Unlike [[hard links]] however, duplicated files are considered to be separate entities and if one of the duplicated files is later modified, then using a system called [[copy-on-write]] a copy of that file or changed block is created.  The deduplication process is transparent to the users and backup applications.  Backing up a deduplicated file system will often cause duplication to occur resulting in the backups being bigger than the source data.
* Target deduplication is the process of removing duplicates when the data was not generated at that location.  Example of this would be a server connected to a SAN/NAS, The SAN/NAS would be a target for the server (Target deduplication).  The server is not aware of any deduplication, the server is also the point of data generation.

A second example would be backup. If you have a backup system with deduplication. Generally this will be a backup store such as a data repository or a [[virtual tape library]].

===Deduplication methods===
One of the most common forms of data deduplication implementations works by comparing chunks of data to detect duplicates. For that to happen, each chunk of data is assigned an identification, calculated by the software, typically using cryptographic hash functions. In many implementations, the assumption is made that if the identification is identical, the data is identical, even though this cannot be true in all cases due to the [[pigeonhole principle]]; other implementations do not assume that two blocks of data with the same identifier are identical, but actually verify that data with the same identification is identical.<ref>An example of an implementation that checks for identity rather than assuming it is described in [http://appft1.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-bool.html&r=1&f=G&l=50&co1=AND&d=PG01&s1=shnelvar&OS=shnelvar&RS=shnelvar "US Patent application # 20090307251"].</ref> If the software either assumes that a given identification already exists in the deduplication namespace or actually verifies the identity of the two blocks of data, depending on the implementation, then it will replace that duplicate chunk with a link.

Once the data has been deduplicated, upon read back of the file, wherever a link is found, the system simply replaces that link with the referenced data chunk. The deduplication process is intended to be transparent to end users and applications.

Commercial deduplication implementations differ by their chunking methods and architectures.

* Chunking.  In some systems, chunks are defined by physical layer constraints (e.g. 4KB block size in [[Write Anywhere File Layout|WAFL]]). In some systems only complete files are compared, which is called [[single-instance storage]] or SIS. The most intelligent (but CPU intensive) method to chunking is generally considered to be sliding-block. In sliding block, a window is passed along the file stream to seek out more naturally occurring internal file boundaries.
* Client backup deduplication. This is the process where the deduplication hash calculations are initially created on the source (client) machines.  Files that have identical hashes to files already in the target device are not sent, the target device just creates appropriate internal links to reference the duplicated data.  The benefit of this is that it avoids data being unnecessarily sent across the network thereby reducing traffic load.
* Primary storage and secondary storage. By definition, primary storage systems are designed for optimal performance, rather than lowest possible cost.  The design criteria for these systems is to increase performance, at the expense of other considerations.  Moreover, primary storage systems are much less tolerant of any operation that can negatively impact performance.  Also by definition, secondary storage systems contain primarily duplicate, or secondary copies of data.  These copies of data are typically not used for actual production operations and as a result are more tolerant of some performance degradation, in exchange for increased efficiency.

To date, data deduplication has predominantly been used with secondary storage systems.  The reasons for this are two-fold.  First, data deduplication requires overhead to discover and remove the duplicate data.  In primary storage systems, this overhead may impact performance.  The second reason why deduplication is applied to secondary data, is that secondary data tends to have more duplicate data.  Backup application in particular commonly generate significant portions of duplicate data over time.

Data deduplication has been deployed successfully with primary storage in some cases where the system design does not require significant overhead, or impact performance.

==Drawbacks and concerns==
Whenever data is transformed, concerns arise about potential loss of data.  By definition, data deduplication systems store data differently from how it was written.  As a result, users are concerned with the integrity of their data.  The various methods of deduplicating data all employ slightly different techniques.  However, the integrity of the data will ultimately depend upon the design of the deduplicating system, and the quality used to implement the algorithms.  As the technology has matured over the past decade, the integrity of most of the major products has been well proven .{{citation needed|date=November 2012}}

One method for deduplicating data relies on the use of [[cryptographic hash function]]s to identify duplicate segments of data. If two different pieces of information generate the same hash value, this is known as a [[collision (computer science)|collision]].  The probability of a collision depends upon the hash function used, and although the probabilities are small, they are always non zero. Thus, the concern arises that [[data corruption]] can occur if a [[hash collision]] occurs, and additional means of verification are not used to verify whether there is a difference in data, or not. Both in-line and post-process architectures may offer bit-for-bit validation of original data for guaranteed data integrity.<ref>{{citation |url=http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ |title=Data Deduplication - Why, When, Where and How |publisher=Evaluator Group |accessdate=2011-07-05}}</ref> The hash functions used include standards such as [[SHA-1]], [[SHA-256]] and others.

The computational resource intensity of the process can be a drawback of data deduplication.  However, this is rarely an issue for stand-alone devices or appliances, as the computation is completely offloaded from other systems.  This can be an issue when the deduplication is embedded within devices providing other services. To improve performance, many systems utilize both weak and strong hashes.  Weak hashes are much faster to calculate but there is a greater risk of a hash collision.  Systems that utilize weak hashes will subsequently calculate a strong hash and will use it as the determining factor to whether it is actually the same data or not. Note that the system overhead associated with calculating and looking up hash values is primarily a function of the deduplication workflow. The reconstitution of files does not require this processing and any incremental performance penalty associated with re-assembly of data chunks is unlikely to impact application performance.

Another area of concern with deduplication is the related effect on [[Snapshot (computer storage)|snapshots]], [[backup]], and [[archival]], especially where deduplication is applied against primary storage (for example inside a [[Network-attached storage|NAS]] filer).{{elucidate|date=December 2011}} Reading files out of a storage device causes full reconstitution of the files (also known as rehydration), so any secondary copy of the data set is likely to be larger than the primary copy. In terms of snapshots, if a file is snapshotted prior to deduplication, the post-deduplication snapshot will preserve the entire original file. This means that although storage capacity for primary file copies will shrink, capacity required for snapshots may expand dramatically.

Another concern is the effect of compression and encryption. Although deduplication is a version of compression, it works in tension with traditional compression. Deduplication achieves better efficiency against smaller data chunks, whereas compression achieves better efficiency against larger chunks. The goal of encryption is to eliminate any discernible patterns in the data. Thus encrypted data cannot be deduplicated, even though the underlying data may be redundant. Deduplication ultimately reduces redundancy.  If this was not expected and planned for, this may ruin the underlying reliability of the system.  (Compare this, for example, to the [[LOCKSS]] storage architecture that achieves reliability through multiple copies of data.)

Scaling has also been a challenge for deduplication systems because ideally, the scope of deduplication needs to be shared across storage devices. If there are multiple disk backup devices in an infrastructure with discrete deduplication, then space efficiency is adversely affected. A deduplication shared across devices preserves space efficiency, but is technically challenging from a reliability and performance perspective.{{citation needed|date=December 2011}}

Although not a shortcoming of data deduplication, there have been data breaches{{citation needed|date=August 2016}} when insufficient security and access validation procedures are used with large repositories of deduplicated data.  In some systems, as typical with cloud storage{{citation needed|date=August 2016}}, an attacker can retrieve data owned by others by knowing or guessing the hash value of the desired data.<ref>{{cite journal |title=A Cloud You Can Trust |publisher=[[IEEE]] |work=[[IEEE Spectrum]] |accessdate=2011-12-21 |url=http://spectrum.ieee.org/computing/networks/a-cloud-you-can-trust |author1=CHRISTIAN CACHIN |author2=MATTHIAS SCHUNTER |date=December 2011}}</ref>

==See also==
* [[Capacity optimization]]
* [[Cloud storage]]
* [[Single-instance storage]]
* [[Content-addressable storage]]
* [[Delta encoding]]
* [[Linked data]]
* [[Pointer (computer programming)|Pointer]]
* [[Record linkage]]
* [[Identity resolution]]
* [[Convergent encryption]]

==References==
{{Reflist|30em}}

==External links==
* Biggar, Heidi(2007.12.11). [http://wayback.archive.org/web/20120325005645/http://www.infostor.com/webcast/display_webcast.cfm?ID=540 WebCast: The Data Deduplication Effect]
* Fellows, Russ(Evaluator Group, Inc.) [http://www.evaluatorgroup.com/document/data-de-duplication-%E2%80%93why-when-where-and-how-infostor-article-by-russ-fellows/ Data Deduplication, why when where and how?]
* [http://wayback.archive.org/web/20120328022229/http://www.tacoma.washington.edu/tech/docs/research/gradresearch/MSpiz.pdf Using Latent Semantic Indexing for Data Deduplication].
* [http://www.forbes.com/2009/08/08/exagrid-storage-data-technology-cio-network-tape.html A Better Way to Store Data].
* [http://www.eweek.com/c/a/Database/What-Is-the-Difference-Between-Data-Deduplication-File-Deduplication-and-Data-Compression What Is the Difference Between Data Deduplication, File Deduplication, and Data Compression?] - Database from eWeek
* [http://www.snia.org/forums/dmf/programs/data_protect_init/ddsrsig/ SNIA DDSR SIG] * * [http://wayback.archive.org/web/20120322084240/http://www.snia.org/forums/dmf/knowledge/white_papers_and_reports/Understanding_Data_Deduplication_Ratios-20080718.pdf Understanding Data Deduplication Ratios]
* [http://public.dhe.ibm.com/common/ssi/ecm/en/tsu12345usen/TSU12345USEN.PDF Data Footprint Reduction Technology Whitepaper]
* [http://www.itnext.in/content/doing-more-less.html Doing More with Less by Jatinder Singh]
* [http://www.sersc.org/journals/IJSIA/vol7_no5_2013/38.pdf Byte Index Chunking Algorithm for Data Deduplication]

{{DEFAULTSORT:Data Deduplication}}
[[Category:Data management]]
[[Category:Data compression]]
<=====doc_Id=====>:232
<=====title=====>:
Contrast set learning
<=====text=====>:
'''Contrast set learning''' is a form of [[association rule learning]] that seeks to identify meaningful differences between separate groups by reverse-engineering the key predictors that identify for each particular group. For example, given a set of attributes for a pool of students (labeled by degree type), a contrast set learner would identify the ''contrasting'' features between students seeking bachelor's degrees and those working toward PhD degrees.

== Overview ==

A common practice in [[data mining]] is to [[Statistical classification|classify]], to look at the attributes of an object or situation and make a guess at what category the observed item belongs to. As new evidence is examined (typically by feeding a ''training set'' to a learning [[algorithm]]), these guesses are reﬁned and improved. Contrast set learning works in the opposite direction. While classiﬁers read a collection of data and collect information that is used to place new data into a series of discrete categories, contrast set learning takes the category that an item belongs to and attempts to reverse engineer the statistical evidence that identifies an item as a member of a class. That is, contrast set learners seek rules associating attribute values with changes to the class distribution.<ref name="bay01">{{cite journal
 |author1=Stephen Bay |author2=Michael Pazzani | year = 2001
 | title = Detecting group differences: Mining contrast sets
 | journal = Data Mining and Knowledge Discovery
 | volume = 5
 | issue= 3
 | pages= 213–246
 | url= http://wotan.liu.edu/docis/lib/musl/rclis/dbl/dmiknd/(2001)5%253A3%253C213%253ADGDMCS%253E/www.isle.org%252F~sbay%252Fpapers%252Fstucco.dmkd.pdf
 }}
</ref> They seek to identify the key predictors that contrast one classification from another.

For example, an aerospace engineer might record data on test launches of a new rocket. Measurements would be taken at regular intervals throughout the launch, noting factors such as the trajectory of the rocket, operating temperatures, external pressures, and so on. If the rocket launch fails after a number of successful tests, the engineer could use contrast set learning to distinguish between the successful and failed tests. A contrast set learner will produce a set of association rules that, when applied, will indicate the key predictors of each failed tests versus the successful ones (the temperature was too high, the wind pressure was too high, etc.).

Contrast set learning is a form of [[association rule learning]].<ref name="webb03">{{cite conference
 |author1=GI Webb |author2=S. Butler |author3=D. Newlands | year = 2003
 | title = On Detecting Differences Between Groups
 | conference = KDD'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
| url= http://portal.acm.org/citation.cfm?id=956781
 }}
</ref> Association rule learners typically offer rules linking attributes commonly occurring together in a training set (for instance, people who are enrolled in four-year programs and take a full course load tend to also live near campus). Instead of ﬁnding rules that describe the current situation, contrast set learners seek rules that differ meaningfully in their distribution across groups (and thus, can be used as predictors for those groups).<ref name="bay99">{{cite conference
 |author1=Stephen Bay |author2=Michael Pazzani | year = 1999
 | title = Detecting change in categorical data: mining contrast sets
 | conference = KDD '99 Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining
 }}
</ref> For example, a contrast set learner could ask, “What are the key identifiers of a person with a bachelor's degree or a person with a PhD, and how do people with PhD's and bachelor’s degrees differ?”

Standard [[Classification in machine learning|classifier]] algorithms, such as [[C4.5]], have no concept of class importance (that is, they do not know if a class is "good" or "bad"). Such learners cannot bias or filter their predictions towards certain desired classes. As the goal of contrast set learning is to discover meaningful differences between groups, it is useful to be able to target the learned rules towards certain classifications. Several contrast set learners, such as MINWAL<ref name="cai98">{{cite conference
 |author1=C.H. Cai |author2=A.W.C. Fu |author3=C.H. Cheng |author4=W.W. Kwong | year = 1998
 | title = Mining association rules with weighted items
 | conference = Proceedings of International Database Engineering and Applications Symposium (IDEAS 98)
 | url= http://appsrv.cse.cuhk.edu.hk/~kdd/assoc_rule/paper_chcai.pdf
 }}
</ref> or the family of TAR algorithms,<ref name="hu03"/><ref name="burlet07">{{cite conference
 |author1=K. Gundy-Burlet |author2=J. Schumann |author3=T. Barrett |author4=T. Menzies | year = 2007
 | title = Parametric analysis of ANTARES re-entry guidance algorithms using advanced test generation and data analysis
 | conference = In 9th International Symposium on Artiﬁcial Intelligence, Robotics and Automation in Space
 }}
</ref><ref name="gay10">{{cite journal
 |author1=Gregory Gay |author2=Tim Menzies |author3=Misty Davies |author4=Karen Gundy-Burlet | year = 2010
 | title = Automatically Finding the Control Variables for Complex System Behavior
 | journal = Automated Software Engineering
 | volume = 17
 | issue= 4
 | url= http://www.greggay.com/pdf/10tar3.pdf 
 }}
</ref> assign weights to each class in order to focus the learned theories toward outcomes that are of interest to a particular audience. Thus, contrast set learning can be though of as a form of weighted class learning.<ref name="menzies03">{{cite journal
 |author1=T. Menzies |author2=Y. Hu | year = 2003
 | title = Data Mining for Very Busy People
 | journal = IEEE Computer
 | volume = 36
 | issue= 11
 | pages= 22–29
 | url= http://menzies.us/pdf/03tar2.pdf
 | doi=10.1109/mc.2003.1244531
 }}
</ref>

=== Example: Supermarket Purchases ===

The differences between standard classification, association rule learning, and contrast set learning can be illustrated with a simple supermarket metaphor. In the following small dataset, each row is a supermarket transaction and each "1" indicates that the item was purchased (a "0" indicates that the item was not purchased):

{| class="wikitable"
|-
! ''Hamburger'' !! ''Potatoes'' !! ''Foie Gras'' !! ''Onions'' !! ''Champagne'' !! ''Purpose of Purchases''
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 0 || 0 || 1 || 0 || 1 || Anniversary
|-
| 1 || 1 || 0 || 1 || 0 || Cookout
|-
| 1 || 1 || 0 || 0 || 1 || Frat Party
|}

Given this data,
* Association rule learning may discover that customers that buy onions and potatoes together are likely to also purchase hamburger meat.
* Classification may discover that customers that bought onions, potatoes, and hamburger meats were purchasing items for a cookout.
* Contrast set learning may discover that the major difference between customers shopping for a cookout and those shopping for an anniversary dinner are that customers acquiring items for a cookout purchase onions, potatoes, and hamburger meat (and ''do not purchase'' foie gras or champagne).

== Treatment learning ==

<!--[[File:TreatmentLearningExample.png|thumb|200px|Example of a treatment produced based on data collected while riding a bicycle. This treatment states that an optimal riding speed can be obtained while the hill slope is constrained to between −10 and 0% and the cadence is between 1.4 and 2.5. {{deletable image-caption|date=December 2011}}]]-->

Treatment learning is a form of weighted contrast-set learning that takes a single ''desirable'' group and contrasts it against the remaining ''undesirable'' groups (the level of desirability is represented by weighted classes).<ref name="hu03">{{cite book
 | author = Y. Hu
 | year = 2003
 | title = Treatment learning: Implementation and application
 | type= Master's thesis
 | publisher=Department of Electrical Engineering, University of British Columbia
 }}
</ref> The resulting "treatment" suggests a set of rules that, when applied, will lead to the desired outcome.

Treatment learning differs from standard contrast set learning through the following constraints:
* Rather than seeking the differences between all groups, treatment learning specifies a particular group to focus on, applies a weight to this desired grouping, and lumps the remaining groups into one "undesired" category.
* Treatment learning has a stated focus on minimal theories. In practice, treatment are limited to a maximum of four constraints (i.e., rather than stating all of the reasons that a rocket differs from a skateboard, a treatment learner will state one to four major differences that predict for rockets at a high level of statistical significance).

This focus on simplicity is an important goal for treatment learners. Treatment learning seeks the ''smallest'' change that has the ''greatest'' impact on the class distribution.<ref name="menzies03"/>

Conceptually, treatment learners explore all possible subsets of the range of values for all attributes. Such a search is often infeasible in practice, so treatment learning often focuses instead on quickly pruning and ignoring attribute ranges that, when applied, lead to a class distribution where the desired class is in the minority.<ref name="gay10"/>

=== Example: Boston housing data ===

The following example demonstrates the output of the treatment learner TAR3 on a dataset of housing data from the city of [[Boston]] (a nontrivial public dataset with over 500 examples). In this dataset, a number of factors are collected for each house, and each house is classified according to its quality (low, medium-low, medium-high, and high). The ''desired'' class is set to "high", and all other classes are lumped together as undesirable.

The output of the treatment learner is as follows:

<code>
 Baseline class distribution:
 low: 29%
 medlow: 29%
 medhigh: 21%
 high: 21%

 Suggested Treatment: [PTRATIO=[12.6..16), RM=[6.7..9.78)]

 New class distribution:
 low: 0%
 medlow: 0%
 medhigh: 3%
 high: 97%
</code>

With no applied treatments (rules), the desired class represents only 21% of the class distribution. However, if one filters the data set for houses with 6.7 to 9.78 rooms and a neighborhood parent-teacher ratio of 12.6 to 16, then 97% of the remaining examples fall into the desired class (high-quality houses).

== Algorithms ==

There are a number of algorithms that perform contrast set learning. The following subsections describe two examples.

=== STUCCO ===

The STUCCO contrast set learner<ref name="bay01"/><ref name="bay99"/> treats the task of learning from contrast sets as a [[Tree traversal|tree search]] problem where the root node of the tree is an empty contrast set. Children are added by specializing the set with additional items picked through a canonical ordering of attributes (to avoid visiting the same nodes twice). Children are formed by appending terms that follow all existing terms in a given ordering. The formed tree is searched in a breadth-first manner. Given the nodes at each level, the dataset is scanned and the support is counted for each group. Each node is then examined to determine if it is significant and large, if it should be pruned, and if new children should be generated. After all significant contrast sets are located, a post-processor selects a subset to show to the user - the low order, simpler results are shown first, followed by the higher order results which are "surprising and significantly different.<ref name="bay99"/>"

The support calculation comes from testing a null hypothesis that the contrast set support is equal across all groups (i.e., that contrast set support is ''independent of group membership''). The support count for each group is a frequency value that can be analyzed in a contingency table where each row represents the truth value of the contrast set and each column variable indicates the group membership frequency. If there is a difference in proportions between the contrast set frequencies and those of the null hypothesis, the algorithm must then determine if the differences in proportions represent a relation between variables or if it can be attributed to random causes. This can be determined through a [[Chi-squared test|chi-square test]] comparing the observed frequency count to the expected count.

Nodes are pruned from the tree when all specializations of the node can never lead to a significant and large contrast set. The decision to prune is based on:
* The minimum deviation size: The maximum difference between the support of any two groups bust be greater than a user-specified threshold.
* Expected cell frequencies: The expected cell frequencies of a contingency table can only decrease as the contrast set is specialized. When these frequencies are too small, the validity of the chi-square test is violated.
* <math>\chi^2</math> bounds: An upper bound is kept on the distribution of a statistic calculated when the null hypothesis is true. Nodes are pruned when it is no longer possible to meet this cutoff.

=== TAR3 ===

The TAR3<ref name="burlet07"/><ref name="schumann09">{{cite conference
 |author1=J. Schumann |author2=K. Gundy-Burlet |author3=C. Pasareanu |author4=T. Menzies |author5=A. Barrett | year = 2009
 | title = Software V&V support by parametric analysis of large software simulation systems
 | conference = Proceedings of the 2009 IEEE Aerospace Conference
 }}
</ref> weighted contrast set learner is based on two fundamental concepts - the '''lift''' and '''support''' of a rule set.

The lift of a set of rules is the change that some decision makes to a set of examples after imposing that decision (i.e., how the class distribution shifts in response to the imposition of a rule). TAR3 seeks the smallest set of rules which induces the biggest changes in the sum of the weights attached to each class multiplied by the frequency at which each class occurs. The lift is calculated by dividing the score of the set in which the set of rules is imposed by the score of the baseline set (i.e., no rules are applied). Note that by reversing the lift scoring function, the TAR3 learner can also select for the remaining classes and reject the target class.

It is problematic to rely on the lift of a rule set alone. Incorrect or misleading data noise, if correlated with failing examples, may result in an overfitted rule set. Such an overfitted model may have a large lift score, but it does not accurately reﬂect the prevailing conditions within the dataset. To avoid overfitting, TAR3 utilizes a support threshold and rejects all rules that fall on the wrong side of this threshold. Given a target class, the support threshold is a user-supplied value (usually 0.2) which is compared to the ratio of the frequency of the target class when the rule set has been applied to the frequency of that class in the overall dataset. TAR3 rejects all sets of rules with support lower than this threshold.

By requiring both a high lift and a high support value, TAR3 not only returns ideal rule sets, but also favors smaller sets of rules. The fewer rules adopted, the more evidence that will exist supporting those rules.

The TAR3 algorithm only builds sets of rules from attribute value ranges with a high heuristic value. The algorithm determines which ranges to use by ﬁrst determining the lift score of each attribute’s value ranges. These individual scores are then sorted and converted into a cumulative probability distribution. TAR3 randomly selects values from this distribution, meaning that low-scoring ranges are unlikely to be selected. To build a candidate rule set, several ranges are selected and combined. These candidate rule sets are then scored and sorted. If no improvement is seen after a user-defined number of rounds, the algorithm terminates and returns the top-scoring rule sets.

== References ==
{{Reflist}}

{{DEFAULTSORT:Contrast Set Learning}}
[[Category:Data management]]
[[Category:Data mining]]
<=====doc_Id=====>:235
<=====title=====>:
Machine-readable data
<=====text=====>:
'''Machine-readable data''' is [[data]] (or [[metadata]]) which is in a format that can be understood by a [[computer]].

There are two types; human-readable data that is [[markup language|marked up]] so that it can also be read by machines (examples; [[microformat]]s, [[RDFa]], [[HTML]]) or data file formats intended principally for processing by machines ([[Resource Description Framework|RDF]], [[XML]], [[JSON]]).

''Machine readable'' is not synonymous with ''digitally accessible''.  A digitally accessible document may be online, making it easier for a human to access it via a computer, but unless the relevant data is available in a machine readable format, it will be much harder to use the computer to extract, transform and process that data.<ref>{{cite web
 | url=https://www.data.gov/developers/blog/primer-machine-readability-online-documents-and-data
 | title=A Primer on Machine Readability for Online Documents and Data
 | work=Data.gov
 | date=2012-09-24
 | accessdate=2015-02-27 }}
</ref>

For purposes of implementation of the [[Government Performance and Results Act]] (GPRA) Modernization Act, the [[Office of Management and Budget]] (OMB) defines "machine readable" as follows:  "Format in a standard computer language (not English text) that can be read automatically by a web browser or computer system. (e.g.; xml). Traditional word processing documents and portable document format (PDF) files are easily read by humans but typically are difficult for machines to interpret. Other formats such as extensible markup language (XML), (JSON), or spreadsheets with header columns that can be exported as comma separated values (CSV) are machine readable formats. As HTML is a structural markup language, discreetly labeling parts of the document, computers are able to gather document components to assemble Tables of Content, outlines, literature search bibliographies, etc. It is possible to make traditional word processing documents and other formats machine readable but the documents must include enhanced structural elements."<ref>[http://www.whitehouse.gov/sites/default/files/omb/assets/a11_current_year/s200.pdf OMB Circular A-11, Part 6], Preparation and Submission of Strategic Plans, Annual Performance Plans, and Annual Program Performance Reports</ref>

==References==
{{reflist}}

==See also==

* [[Open data]]
* [[Linked data]]
* [[Machine-Readable Documents]]
* [[Human-readable medium]]
* [http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10 Section 10] of the [[Government Performance and Results Act|GPRA]] Modernization Act (GPRAMA), which requires U.S. federal agencies to publish their strategic and performance plans and reports in machine-readable format, like [[Strategy Markup Language]] (StratML)
* President Obama's [http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml Executive Order] Making Open and Machine Readable the New Default for Government Information
* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards
[[Category:Data management]]


{{Comp-stub}}
<=====doc_Id=====>:238
<=====title=====>:
Electronic lab notebook
<=====text=====>:
An '''electronic lab notebook''' (also known as electronic laboratory notebook, or ELN) is a [[computer program]] designed to replace paper [[lab notebook|laboratory notebook]]s.  Lab notebooks in general are used by [[scientist]]s, [[engineer]]s, and [[technician]]s to document [[research]], [[experiment]]s, and procedures performed in a laboratory.  A lab notebook is often maintained to be a [[legal document]] and may be used in a [[court of law]] as [[evidence (law)|evidence]].  Similar to an [[inventor's notebook]], the lab notebook is also often referred to in [[patent]] prosecution and [[intellectual property]] [[litigation]].

Electronic lab notebooks are a fairly new technology and offer many benefits to the user as well as organizations. For example: electronic lab notebooks are easier to search upon, simplify data copying and backups, and support collaboration amongst many users.<ref>{{cite conference |
title = A Collaborative Electronic Notebook |
first = James | 
last = Myers |author2=Elena Mendoza |author3=Bonnie Hoopes |
journal = Proceedings of the IASTED International Conference on Internet and Multimedia Systems and Applications |
year = 2001 
}}</ref>  
ELNs can have fine-grained access controls, and can be more secure than their paper counterparts.<ref>{{
cite conference | 
last= Myers | 
first = James | 
year = 2003 | 
journal = Proceedings of the 2003 International Symposium On Collaborative Technologies and Systems | 
title = Collaborative Electronic Notebooks as Electronic Records:Design Issues for the Secure Electronic Laboratory Notebook (ELN) | 
url = http://collaboratory.emsl.pnl.gov/resources/publications/papers/seceln(final1)1-22Nov.pdf
}}</ref>  They also allow the direct incorporation of data from instruments, replacing the practice of printing out data to be stapled into a paper notebook.<ref>{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541–543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}</ref>

==Types==
ELNs can be divided into two categories:

* "Specific ELNs" contain features designed to work with specific applications, scientific instrumentation or data types.
* "[[Cross-disciplinary]] ELNs" or "Generic ELNs" are designed to support access to all data and information that needs to be recorded in a lab notebook.

Solutions range from specialized programs designed from the ground up for use as an ELN, to modifications or direct use of more general programs.  Examples of using more general software include using [[OpenWetWare]], a [[MediaWiki]] install (running the same software that Wikipedia uses), as an ELN, or the use of general note taking software such as OneNote as an ELN.<ref>{{Cite journal | last1 = Perkel | first1 = J. M. | title = Coding your way out of a problem | journal = Nature Methods | volume = 8 | issue = 7 | pages = 541–543 | year = 2011 | pmid = 21716280 | doi = 10.1038/nmeth.1631}}.</ref>

ELN's come in many different forms. They can be standalone programs, use a client-server model, or be entirely web-based. Some use a lab-notebook approach, others resemble a blog.

A good many variations on the "ELN" acronym have appeared.<ref>{{Cite web|url=http://cerf-notebook.com/articles/eln-glossary/|title=Lab Notebook (ELN) Glossary - CERF|date=2016-02-16|language=en-US|access-date=2016-08-20}}</ref> Differences between systems with different names are often subtle, with considerable functional overlap between them. Examples include "ERN" (Electronic Research Notebook), "ERMS" (Electronic Resource (or Research or Records) Management System (or Software) and SDMS (Scientific Data (or Document) Management System (or Software). Ultimately, these types of systems all strive to do the same thing: Capture, record, centralize and protect scientific data in a way that is highly searchable, historically accurate, and legally stringent, and which also promotes secure collaboration, greater efficiency, reduced mistakes and lowered total research costs.

==Objectives==
A good electronic laboratory notebook should offer a secure environment to protect the integrity of both data and process, whilst also affording the flexibility to adopt new processes or changes to existing processes without recourse to further software development. The package architecture should be a modular design, so as to offer the benefit of minimizing validation costs of any subsequent changes that you may wish to make in the future as your needs change.

A good electronic laboratory notebook should be an "out of the box" solution that, as standard, has fully configurable forms to comply with the requirements of regulated analytical groups through to a sophisticated ELN for inclusion of structures, spectra, chromatograms, pictures, text, etc. where a preconfigured form is less appropriate. All data within the system may be stored in a database (e.g. MySQL, MS-SQL, Oracle) and be fully searchable. The system should enable data to be collected, stored and retrieved through any combination of forms or ELN that best meets the requirements of the user.

The application should enable secure forms to be generated that accept laboratory data input via PCs and/or laptops / palmtops, and should be directly linked to electronic devices such as laboratory balances, pH meters, etc.  Networked or wireless communications should be accommodated for by the package which will allow data to be interrogated, tabulated, checked, approved, stored and archived to comply with the latest regulatory guidance and legislation.  A system should also include a scheduling option for routine procedures such as equipment qualification and study related timelines. It should include configurable qualification requirements to automatically verify that instruments have been cleaned and calibrated within a specified time period, that reagents have been quality-checked and have not expired, and that workers are trained and authorized to use the equipment and perform the procedures.

==Regulatory and legal aspects==
The laboratory accreditation criteria found in the [[ISO 17025]] standard needs to be considered for the protection and computer backup of electronic records. These criteria can be found specifically in clause 4.13.1.4 of the standard.<ref>"ISO/IEC 17025:2005 - General Requirements for the Competence of Testing and Calibration Laboratories." ISO - International Organization for Standardization. Web. 16 Nov. 2011. <http://www.iso.org/iso/Catalogue_detail?csnumber=39883>.</ref>

Electronic lab notebooks used for development or research in regulated industries, such as medical devices or pharmaceuticals, are expected to comply with FDA regulations related to software validation.  The purpose of the regulations is to ensure the integrity of the entries in terms of time, authorship, and content.  Unlike ELNs for patent protection, FDA is not concerned with patent interference proceedings, but is concerned with avoidance of falsification.  Typical provisions related to software validation are included in the medical device regulations at 21 CFR 820 (et seq.)<ref>United States. Food and Drug Administration. Department of Health and Human Resources. 1 Food and Drugs - Subchapter H Medical Devices - Part 820 System RegCode of Federal Regulations - Title 2ulation. FDA.gov, 7 Oct. 1996. Web. <http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=820>.</ref> and [[Title 21 CFR Part 11]].<ref>United States. Food and Drug Administration. Department of Health and Human Resources. Code of Federal Regulations - Title 21 Part 11 Electronic Records; Electronic Signatures. FDA.gov. Authority: 21 U.S.C. 321-393; 42 U.S.C. 262., 20 Mar. 1997. Web. 16 Nov. 2011. <http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfcfr/cfrsearch.cfm?cfrpart=11>.</ref>  Essentially, the requirements are that the software has been designed and implemented to be suitable for its intended purposes.  Evidence to show that this is the case is often provided by a Software Requirements Specification (SRS) setting forth the intended uses and the needs that the ELN will meet; one or more testing protocols that, when followed, demonstrate that the ELN meets the requirements of the specification and that the requirements are satisfied under worst-case conditions.  Security, audit trails, prevention of unauthorized changes without substantial collusion of otherwise independent personnel (i.e., those having no interest in the content of the ELN such as independent quality unit personnel) and similar tests are fundamental.  Finally, one or more reports demonstrating the results of the testing in accordance with the predefined protocols are required prior to release of the ELN software for use.  If the reports show that the software failed to satisfy any of the SRS requirements, then corrective and preventive action ("CAPA") must be undertaken and documented.  Such CAPA may extend to minor software revisions, or changes in architecture or major revisions.  CAPA activities need to be documented as well.

Aside from the requirements to follow such steps for regulated industry, such an approach is generally a good practice in terms of development and release of any software to assure its quality and fitness for use.  There are standards related to software development and testing that can be applied (see ref.).

==See also==
* [[List of ELN software packages]]	
* [[Data management]]
* [[Laboratory informatics]]
* [[Scientific management]]

==References==
{{reflist}}

== Further reading ==
* {{Cite journal 
| last1 = Taylor | first1 = K. T. 
| title = The status of electronic laboratory notebooks for chemistry and biology 
| journal = Current opinion in drug discovery & development 
| volume = 9 
| issue = 3 
| pages = 348–353 
| year = 2006 
| pmid = 16729731
}}
* {{Cite journal | last1 = Rubacha | first1 = M. | last2 = Rattan | first2 = A. K. | last3 = Hosselet | first3 = S. C. | doi = 10.1016/j.jala.2009.01.002 | title = A Review of Electronic Laboratory Notebooks Available in the Market Today | journal = Journal of Laboratory Automation | volume = 16 | issue = 1 | pages = 90–98 | year = 2011 | pmid =  21609689| pmc = }}

{{DEFAULTSORT:Electronic Lab Notebook}}
[[Category:Electronic lab notebook]]
[[Category:Research]]
[[Category:Science software]]
[[Category:Scientific documents]]
[[Category:Notebooks]]
[[Category:Electronic documents]]
[[Category:Data management]]
[[Category:Content management systems]]
[[Category:Data management software]]
<=====doc_Id=====>:241
<=====title=====>:
National Information Governance Board for Health and Social Care
<=====text=====>:
The '''National Information Governance Board for Health and Social Care''' (NIGB) was established in the [[United Kingdom]] under section 157<ref>[http://www.legislation.gov.uk/ukpga/2008/14/section/157 Section 157 of the Health and Social Care Act 2008]</ref> of the Health and Social Care Act 2008, with effect from October 2008, with a range of advisory functions relating to [[information governance]].  From January 2009, the NIGB also gained functions under section 251<ref>[http://www.legislation.gov.uk/ukpga/2006/41/section/251 Section 251 of the NHS Act 2006]</ref> of the NHS Act 2006 which had previously been held by the [[Patient Information Advisory Group]] (PIAG) until its abolition. These functions were to advise the [[Secretary of State for Health]] on the use of powers to set aside the common law duty of confidentiality in [[England]] where identifiable patient information is needed and where consent is not practicable. From 1 April 2013, the NIGB's functions for monitoring and improving information governance practice have transferred to the [[Care Quality Commission]], which established a National Information Governance Committee to oversee this work. Functions relating to section 251 of the [[National Health Service Act 2006|NHS Act 2006]] (access to people’s personal and confidential information for research purposes) were transferred to the [[Health Research Authority]]'s Confidentiality Advisory Group.<ref>[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ Webarchive page of the National Information Governance Board for Health and Social Care]</ref>

==Terms of reference==

The key functions of the NIGB (excerpted from the legislation) were:
<ol type="a">
<li> to monitor the practice followed by relevant bodies in relation to the processing of relevant information;</li>
<li> to keep the [[Secretary of State for Health]], and such bodies as the [[Secretary of State for Health]] may designate by direction, informed about the practice being followed by relevant bodies in relation to the processing of relevant information;</li>
<li> to publish guidance on the practice to be followed in relation to the processing of relevant information;</li>
<li> to advise the [[Secretary of State for Health]] on particular matters relating to the processing of relevant information by any person; and</li>
<li> to advise persons who process relevant information on such matters relating to the processing of relevant information by them as the [[Secretary of State for Health]] may from time to time designate by direction.</li>
</ol>

The definition of “relevant information” in the legislation covers patient information, any other information obtained or generated in the course of the provision of the health service, and any information obtained or generated in the course of the exercise by a local social services authority in [[England]] of its adult social services functions.

==Ethics and Confidentiality Committee==

Some areas of NIGB functions (d) and (e) above had been delegated to the NIGB’s Ethics and Confidentiality Committee (ECC).  These functions primarily related to applications to use identifiable patient information without consent, in specific circumstances within the bounds of section 251 of the NHS Act 2006.  These applications, which had been considered by PIAG before the NIGB, passed on to the [[Health Research Authority]]'s Confidentiality Advisory Group (CAG) on 1 April 2013.

==Care Record Development Board==

The NIGB had also replaced the Care Record Development Board (CRDB),<ref>[http://www.connectingforhealth.nhs.uk/crdb Care Record Development Board archive page hosted by NHS Connecting for Health]</ref> which had closed in September 2007. The NIGB had subsequently maintained the NHS Care Record Guarantee which was originally developed by the CRDB and developed a companion Social Care Record Guarantee.

==Members==

The NIGB had consisted of a Chair, a number of Public Members appointed by the NHS Appointments Commission, and a number of Representative Members appointed by the [[Secretary of State for Health]] from a range of stakeholder organisations.  Representatives of several other stakeholder organisations had served as Corresponding Advisers to the NIGB but had not typically attended meetings.  Regular observers at meetings had included representatives from the [[Information Commissioner's Office]] and the devolved UK administrations.

The ECC had consisted of a Chair and a number of Members, all of whom had been appointed by the NIGB with advice from an NHS Appointments Commission approved independent assessor.  The ECC Chair and two ECC Members had also been NIGB Members.

Between 1 June 2011 and 31 March 2013 Dame [[Fiona Caldicott]]<ref>[http://www.connectingforhealth.nhs.uk/newsroom/news/nigbchair Appointment of Fiona Caldicott as new NIGB Chair - press release on NHS Connecting for Health website, June 2011]</ref> had been Chair of the NIGB, succeeding [[Harry Cayton]] who had chaired the NIGB since its inception.

==Geography==

Members of the NIGB and ECC had been widely distributed nationally but had attended meetings at the NIGB office.  Since September 2011, this had been based at [[Skipton House]], London SE1.  The NIGB’s staff team had been predominantly based at this office.

==Abolition==

As a result of the [[Health and Social Care Act 2012]] the NIGB was abolished with effect from 1 April 2013. The functions delegated to the ECC with respect to research transferred to the [[Health Research Authority]].<ref>[http://www.hra.nhs.uk/news/2012/12/17/further-update-on-transfer-of-s251-function-from-nigb-to-hra/ Transfer of s251 function from NIGB to HRA]</ref> The [[NHS Commissioning Board]] is now responsible for providing advice and guidance to NHS bodies. Other functions were transferred to the National Information Governance Committee hosted by the [[Care Quality Commission]].

==References==

{{Reflist}}

==External links==
*[http://webarchive.nationalarchives.gov.uk/20130513181011/http://www.nigb.nhs.uk/ NIGB website (archived)]
*[http://www.hra.nhs.uk/ NHS Health Research Authority]
*[http://www.cqc.org.uk/ Care Quality Commission]
*[http://www.nres.nhs.uk National Research Ethics Service]
*[http://www.commissioningboard.nhs.uk/ NHS Commissioning Board]


[[Category:Data management]]
[[Category:Medical privacy]]
[[Category:2008 establishments in the United Kingdom]]
[[Category:Organizations established in 2008]]
[[Category:Governance in the United Kingdom]]
[[Category:Social care in the United Kingdom]]
<=====doc_Id=====>:244
<=====title=====>:
Data grid
<=====text=====>:
[[File:High Level View Data Grid V1.jpg|200px|right|High Level View Data Grid Topology]]

A '''data grid''' is an [[architecture]] or set of services that gives individuals or groups of users the ability to access, modify and transfer extremely large amounts of geographically distributed [[data]] for research purposes.<ref>Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. Data Grid tools: enabling science on big distributed data</ref> Data grids make this possible through a host of [[middleware]] [[Application software|applications]] and [[Service (systems architecture)|services]] that pull together data and [[Resource (computer science)|resources]] from multiple [[administrative domain]]s and then present it to users upon request. The data in a data grid can be located at a single site or multiple sites where each site can be its own administrative domain governed by a set of security restrictions as to who may access the data.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.37</ref> Likewise, multiple [[replica]]s of the data may be distributed throughout the grid outside their original administrative domain and the security restrictions placed on the original data for who may access it must be equally applied to the replicas.<ref>Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids. p.15</ref> Specifically developed data grid middleware is what handles the integration between users and the data they request by controlling access while making it available as efficiently as possible. The diagram to the right depicts a high level view of a data grid.

==Middleware==
Middleware provides all the services and applications necessary for efficient management of [[dataset]]s and [[Computer file|files]] within the data grid while providing users quick access to the datasets and files.<ref>Padala, Pradeep. A survey of data middleware for Grid systems p.1</ref> There are a number of concepts and tools that must be available to make a data grid operationally viable. However, at the same time not all data grids require the same capabilities and services because of differences in access requirements, security and location of resources in comparison to users. In any case, most data grids will have similar middleware services that provide for a universal [[namespace|name space]], data transport service, data access service, data replication and resource management service. When taken together, they are key to the data grids functional capabilities.

===Universal namespace===
Since sources of data within the data grid will consist of data from multiple separate systems and [[Computer network|networks]] using different file [[naming convention]]s, it would be difficult for a user to locate data within the data grid and know they retrieved what they needed based solely on existing physical file names (PFNs). A universal or unified name space makes it possible to create logical file names (LFNs) that can be referenced within the data grid that map to PFNs.<ref>Padala, Pradeep. A survey of data middleware for Grid systems</ref> When an LFN is requested or queried, all matching PFNs are returned to include possible replicas of the requested data. The end user can then choose from the returned results the most appropriate replica to use. This service is usually provided as part of a management system known as a [[Storage Resource Broker]] (SRB).<ref>Arcot, Rajasekar; Wan, Michael; Moore, Reagan; Schroeder, Wayne; Kremenek. Storage resource broker – managing distributed data in a grid</ref> Information about the locations of files and mappings between the LFNs and PFNs may be stored in a [[metadata]] or replica catalogue.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.11</ref> The replica catalogue would contain information about LFNs that map to multiple replica PFNs.

===Data transport service===
Another middleware service is that of providing for data transport or data transfer. Data transport will encompass multiple functions that are not just limited to the transfer of bits, to include such items as fault tolerance and data access.<ref>Coetzee, Serena. Reference model for a data grid approach to address data in a dynamic SDI p.16</ref> Fault tolerance can be achieved in a data grid by providing mechanisms that ensures data transfer will resume after each interruption until all requested data is received.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.21</ref> There are multiple possible methods that might be used to include starting the entire transmission over from the beginning of the data to resuming from where the transfer was interrupted. As an example, [[GridFTP]] provides for fault tolerance by sending data from the last acknowledged byte without starting the entire transfer from the beginning.

The data transport service also provides for the low-level access and connections between [[Host (network)|hosts]] for file transfer.<ref>Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.</ref> The data transport service may use any number of modes to implement the transfer to include parallel data transfer where two or more data streams are used over the same [[Channel (communications)|channel]] or striped data transfer where two or more steams access different blocks of the file for simultaneous transfer to also using the underlying built-in capabilities of the network hardware or specifically developed [[Protocol (object-oriented programming)|protocols]] to support faster transfer speeds.<ref>Izmailov, Rauf; Ganguly, Samrat; Tu, Nan. Fast parallel file replication in data grid p.2</ref> The data transport service might optionally include a [[network overlay]] function to facilitate the routing and transfer of data as well as file [[I/O]] functions that allow users to see remote files as if they were local to their system. The data transport service hides the complexity of access and transfer between the different systems to the user so it appears as one unified data source.

===Data access service===
Data access services work hand in hand with the data transfer service to provide security, access controls and management of any data transfers within the data grid.<ref>Raman, Vijayshankar; Narang, Inderpal; Crone, chris; Hass, Laura; Malaika, Susan. Services for data access and data processing on grids</ref> Security services provide mechanisms for authentication of users to ensure they are properly identified. Common forms of security for authentication can include the use of passwords or [[Kerberos (protocol)]]. Authorization services are the mechanisms that control what the user is able to access after being identified through authentication. Common forms of authorization mechanisms can be as simple as file permissions. However, need for more stringent controlled access to data is done using [[Access Control List]]s (ACLs), [[Role-Based Access Control]] (RBAC) and Tasked-Based Authorization Controls (TBAC).<ref>Thomas, R. K. and Sandhu R. S. Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management</ref> These types of controls can be used to provide granular access to files to include limits on access times, duration of access to granular controls that determine which files can be read or written to. The final data access service that might be present to protect the confidentiality of the data transport is encryption.<ref>Sreelatha, Malempati. Grid based approach for data confidentiality. p.1</ref> The most common form of encryption for this task has been the use of [[Transport Layer Security|SSL]] while in transport. While all of these access services operate within the data grid, access services within the various administrative domains that host the datasets will still stay in place to enforce access rules. The data grid access services must be in step with the administrative domains access services for this to work.

===Data replication service===
To meet the needs for scalability, fast access and user collaboration, most data grids support replication of datasets to points within the distributed storage architecture.<ref>Chervenak, Ann; Schuler, Robert; Kesselman, Carl; Koranda, Scott; Moe, Brian. Wide area data replication for scientific collaborations</ref> The use of replicas allows multiple users faster access to datasets and the preservation of bandwidth since replicas can often be placed strategically close to or within sites where users need them. However, replication of datasets and creation of replicas is bound by the availability of storage within sites and bandwidth between sites. The replication and creation of replica datasets is controlled by a replica management system. The replica management system determines user needs for replicas based on input requests and creates them based on availability of storage and bandwidth.<ref>Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments</ref> All replicas are then cataloged or added to a directory based on the data grid as to their location for query by users. In order to perform the tasks undertaken by the replica management system, it needs to be able to manage the underlying storage infrastructure. The data management system will also ensure the timely updates of changes to replicas are propagated to all nodes.

====Replication update strategy====
There are a number of ways the replication management system can handle the updates of replicas. The updates may be designed around a centralized model where a single master replica updates all others, or a decentralized model, where all peers update each other.<ref>Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments</ref> The topology of node placement may also influence the updates of replicas. If a hierarchy topology is used then updates would flow in a tree like structure through specific paths. In a flat topology it is entirely a matter of the peer relationships between nodes as to how updates take place. In a hybrid topology consisting of both flat and hierarchy topologies updates may take place through specific paths and between peers.

====Replication placement strategy====
There are a number of ways the replication management system can handle the creation and placement of replicas to best serve the user community. If the storage architecture supports replica placement with sufficient site storage, then it becomes a matter of the needs of the users who access the datasets and a strategy for placement of replicas.<ref>Padala, Pradeep. A survey of data middleware for Grid systems</ref> There have been numerous strategies proposed and tested on how to best manage replica placement of datasets within the data grid to meet user requirements. There is not one universal strategy that fits every requirement the best. It is a matter of the type of data grid and user community requirements for access that will determine the best strategy to use. Replicas can even be created where the files are encrypted for confidentiality that would be useful in a research project dealing with medical files.<ref>Kranthi, G. and Rekha, D. Shashi. Protected data objects replication in data grid p.40</ref> The following section contains several strategies for replica placement.

=====Dynamic replication=====
Dynamic replication is an approach to placement of replicas based on popularity of the data.<ref>Belalem, Ghalem and Meroufel, Bakhta. Management and placement of replicas in a hierarchical data grid</ref> The method has been designed around a hierarchical replication model. The data management system keeps track of available storage on all nodes. It also keeps track of requests (hits) for which data clients (users) in a site are requesting. When the number of hits for a specific dataset exceeds the replication threshold it triggers the creation of a replica on the server that directly services the user’s client. If the direct servicing server known as a father does not have sufficient space, then the father’s father in the hierarchy is then the target to receive a replica and so on up the chain until it is exhausted. The data management system algorithm also allows for the dynamic deletion of replicas that have a null access value or a value lower than the frequency of the data to be stored to free up space. This improves system performance in terms of response time, number of replicas and helps load balance across the data grid. This method can also use dynamic algorithms that determine whether the cost of creating the replica is truly worth the expected gains given the location.<ref>Lamehamedi, Houda; Szymanski, Boleslaw; Shentu, Zujun; Deelman, Ewa. Data replication strategies in grid environments</ref>

=====Adaptive replication=====
This method of replication like the one for dynamic replication has been designed around a hierarchical replication model found in most data grids. It works on a similar algorithm to dynamic replication with file access requests being a prime factor in determining which files should be replicated. A key difference, however, is the number and frequency of replica creations is keyed to a dynamic threshold that is computed based on request arrival rates from clients over a period of time.<ref>Shorfuzzaman, Mohammad; Graham, Peter; Eskicioglu, Rasit. Adaptive replica placement in hierarchical data grids</ref> If the number of requests on average exceeds the previous threshold and shows an upward trend, and storage utilization rates indicate capacity to create more replicas, more replicas may be created. As with dynamic replication, the removal of replicas that have a lower threshold that were not created in the current replication interval can be removed to make space for the new replicas.

=====Fair-share replication=====
Like the adaptive and dynamic replication methods before, fair-share replication is based on a hierarchical replication model. Also, like the two before, the popularity of files play a key role in determining which files will be replicated. The difference with this method is the placement of the replicas is based on access load and storage load of candidate servers.<ref>Rasool, Qaisar; Li, Jianzhong; Oreku, George S.; Munir, Ehsan Ullah. Fair-share replication in data grid</ref> A candidate server may have sufficient storage space but be servicing many clients for access to stored files. Placing a replicate on this candidate could degrade performance for all clients accessing this candidate server. Therefore, placement of replicas with this method is done by evaluating each candidate node for access load to find a suitable node for the placement of the replica. If all candidate nodes are equivalently rated for access load, none or less accessed than the other, then the candidate node with the lowest storage load will be chosen to host the replicas. Similar methods to the other described replication methods are used to remove unused or lower requested replicates if needed. Replicas that are removed might be moved to a parent node for later reuse should they become popular again.

=====Other replication=====
The above three replica strategies are but three of many possible replication strategies that may be used to place replicas within the data grid where they will improve performance and access. Below are some others that have been proposed and tested along with the previously described replication strategies.<ref>Ranganathan, Kavitha and Foster, Ian. Identifying dynamic replication strategies for a high performance data grid</ref> 
* '''Static''' – uses a fixed replica set of nodes with no dynamic changes to the files being replicated.
* '''Best Client''' – Each node records number of requests per file received during a preset time interval; if the request number exceeds the set threshold for a file a replica is created on the best client, one that requested the file the most; stale replicas are removed based on another algorithm. 
* '''Cascading''' – Is used in a hierarchical node structure where requests per file received during a preset time interval is compared against a threshold. If the threshold is exceeded a replica is created at the first tier down from the root, if the threshold is exceeded again a replica is added to the next tier down and so on like a waterfall effect until a replica is placed at the client itself.
* '''Plain Caching''' – If the client requests a file it is stored as a copy on the client.
* '''Caching plus Cascading''' – Combines two strategies of caching and cascading.
* '''Fast Spread''' – Also used in a hierarchical node structure this strategy automatically populates all nodes in the path of the client that requests a file.

===Tasks scheduling and resource allocation===
Such characteristics of the data grid systems as large scale and heterogeneity require specific methods of tasks scheduling and resource allocation. To resolve the problem, majority of systems use extended classic methods of scheduling.<ref>Epimakhov, Igor; Hameurlain, Abdelkader ; Dillon, Tharam; Morvan, Franck. Resource Scheduling Methods for Query Optimization in Data Grid Systems</ref> Others invite fundamentally different methods based on incentives for autonomous nodes, like virtual money or reputation of a node.
Another specificity of data grids, dynamics, consists in the continuous process of connecting and disconnecting of nodes and local load imbalance during an execution of tasks. That can make obsolete or non-optimal results of initial resource allocation for a task. As a result, much of the data grids utilize execution-time adaptation techniques that permit the systems to reflect to the dynamic changes: balance the load, replace disconnecting nodes, use the profit of newly connected nodes, recover a task execution after faults.

===Resource management system (RMS)===
The resource management system represents the core functionality of the data grid. It is the heart of the system that manages all actions related to storage resources. In some data grids it may be necessary to create a federated RMS architecture because of different administrative policies and a diversity of possibilities found within the data grid in place of using a single RMS. In such a case the RMSs in the federation will employ an architecture that allows for interoperability based on an agreed upon set of protocols for actions related to storage resources.<ref>Krauter, Klaus; Buyya, Rajkumar; Maheswaran, Muthucumaru. A taxonomy and survey of grid resource management systems for distributed computing</ref>

====RMS functional capabilities====
* Fulfillment of user and application requests for data resources based on type of request and policies; RMS will be able to support multiple policies and multiple requests concurrently
* Scheduling, timing and creation of replicas
* Policy and security enforcement within the data grid resources to include authentication, authorization and access 
* Support systems with different administrative policies to inter-operate while preserving site autonomy
* Support quality of service (QoS) when requested if feature available
* Enforce system fault tolerance and stability requirements
* Manage resources, i.e. disk storage, network bandwidth and any other resources that interact directly or as part of the data grid 
* Manage trusts concerning resources in administrative domains, some domains may place additional restrictions on how they participate requiring adaptation of the RMS or federation.
* Supports adaptability, extensibility, and scalability in relation to the data grid.

==Topology==
[[File:Data Grid Multiple Topologies 1.jpg|right|Possible Data Grid Topologies]]
Data grids have been designed with multiple topologies in mind to meet the needs of the scientific community. On the right are four diagrams of various topologies that have been used in data grids.<ref>Zhu, Lichun. Metadata management in grid database federation</ref> Each topology has a specific purpose in mind for where it will be best utilized. Each of these topologies is further explained below.

'''Federation topology''' is the choice for institutions that wish to share data from already existing systems. It allows each institution control over their data. When an institution with proper authorization requests data from another institution it is up to the institution receiving the request to determine if the data will go to the requesting institution. The federation can be loosely integrated between institutions, tightly integrated or a combination of both.

'''Monadic topology''' has a central repository that all collected data is fed into. The central repository then responds to all queries for data. There are no replicas in this topology as compared to others. Data is only accessed from the central repository which could be by way of a web portal. One project that uses this data grid topology is the [[Network for Earthquake Engineering Simulation| Network for Earthquake Engineering Simulation (NEES)]] in the United States.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.16</ref> This works well when all access to the data is local or within a single region with high speed connectivity.

'''Hierarchical topology''' lends itself to collaboration where there is a single source for the data and it needs to be distributed to multiple locations around the world. One such project that will benefit from this topology would be [[CERN]] that runs the [[Large Hadron Collider]] that generates enormous amounts of data. This data is located at one source and needs to be distributed around the world to organizations that are collaborating in the project.

'''Hybrid Topology''' is simply a configuration that contains an architecture consisting of any combination of the previous mentioned topologies. It is used mostly in situations where researchers working on projects want to share their results to further research by making it readily available for collaboration.

==History==
The need for data grids was first recognized by the [[scientific community]] concerning [[climate modeling]], where [[terabyte]] and [[petabyte]] sized [[data set]]s were becoming the norm for transport between sites.<ref>Allcock, Bill; Foster,Ian; Nefedova, Veronika; Chervenak, Ann; Deelman, Ewa; Kesselman, Carl. High-performance remote access to climate simulation data: A challenge problem for data grid technologies.</ref> More recent research requirements for data grids have been driven by the [[Large Hadron Collider]] (LHC) at [[CERN]], the [[LIGO|Laser Interferometer Gravitational Wave Observatory (LIGO)]], and the [[Sloan Digital Sky Survey|Sloan Digital Sky Survey (SDSS)]]. These examples of scientific instruments produce large amounts of data that need to be accessible by large groups of geographically dispersed researchers.<ref>Allcock, Bill; Chervenak, Ann; Foster, Ian; et al. p.571</ref><ref>Tierney, Brian L. Data grids and data grid performance issues. p.7</ref> Other uses for data grids involve governments, hospitals, schools and businesses where efforts are taking place to improve services and reduce costs by providing access to dispersed and separate data systems through the use of data grids.<ref>Thibodeau, P. Governments plan data grid projects</ref>
 
From its earliest beginnings, the concept of a Data Grid to support the scientific community was thought of as a specialized extension of the “grid” which itself was first envisioned as a way to link super computers into meta-computers.<ref>Heingartner, douglas. The grid: the next-gen internet</ref> However, that was short lived and the grid evolved into meaning the ability to connect computers anywhere on the web to get access to any desired files and resources, similar to the way electricity is delivered over a grid by simply plugging in a device. The device gets electricity through its connection and the connection is not limited to a specific outlet. From this the data grid was proposed as an integrating architecture that would be capable of delivering resources for distributed computations. It would also be able to service numerous to thousands of queries at the same time while delivering gigabytes to terabytes of data for each query. The data grid would include its own management infrastructure capable of managing all aspects of the data grids performance and operation across multiple wide area networks while working within the existing framework known as the web.<ref>Heingartner, douglas. The grid: the next-gen internet</ref> 
 
The data grid has also been defined more recently in terms of usability; what must a data grid be able to do in order for it to be useful to the scientific community. Proponents of this theory arrived at several criteria.<ref>Venugopal, Srikumar; Buyya, Rajkumar; Ramamohanarao, Kotagiri. A taxonomy of data grids for distributed data sharing - management and processing p.1</ref> One, users should be able to search and discover applicable resources within the data grid from amongst its many datasets. Two, users should be able to locate datasets within the data grid that are most suitable for their requirement from amongst numerous replicas. Three, users should be able to transfer and move large datasets between points in a short amount of time. Four, the data grid should provide a means to manage multiple copies of datasets within the data grid. And finally, the data grid should provide security with user access controls within the data grid, i.e. which users are allowed to access which data.

The data grid is an evolving technology that continues to change and grow to meet the needs of an expanding community. One of the earliest programs begun to make data grids a reality was funded by the [[DARPA|Defense Advanced Research Projects Agency (DARPA)]] in 1997 at the [[University of Chicago]].<ref>Globus. About the globus toolkit</ref> This research spawned by DARPA has continued down the path to creating open source tools that make data grids possible. As new requirements for data grids emerge projects like the [[Globus Toolkit]] will emerge or expand to meet the gap. Data grids along with the "Grid" will continue to evolve.

== Notes ==
{{Reflist}}

== References ==
*{{cite journal
|last1= Allcock
|first1= Bill |last2= Chervenak |first2= Ann |last3= Foster |first3= Ian |last4= Kesselman |first4= Carl |last5= Livny |first5= Miron 
|year= 2005
|title= Data Grid tools: enabling science on big distributed data
|journal= Journal of Physics: Conference Series
|volume= 16
|pages= 571–575
|publisher= Institute of Physics Publishing
|doi= 10.1088/1742-6596/16/1/079
|url= http://iopscience.iop.org/1742-6596/16/1/079
|accessdate= April 15, 2012}}

*{{cite journal
|last1=Allcock
|first1=Bill |last2=Foster |first2=Ian |last3=Nefedova  |first3= Veronika  l|last4= Chervenak |first4= Ann |last5= Deelman |first5= Ewa |last6= Kesselman |first6= Carl |last7= Lee |first7= Jason |last8= Sim |first8= Alex |last9= Shoshani |first9= Arie |last10= Drach |first10=Bob |last11= Williams |first11= Dean    
|title= High-performance remote access to climate simulation data: A challenge problem for data grid technologies
|work =
|publisher =  [[ACM Press]]
|year = 2001
|citeseerx = 10.1.1.64.6603
|format =
|doi =
|accessdate = <!-- April 20, 2012 --> }}

*{{cite web
 |last1=Arcot 
 |first1=Rajasekar 
 |last2=Wan 
 |first2=Michael 
 |last3=Moore 
 |first3=Reagan 
 |last4=Schroeder 
 |first4=Wayne 
 |last5=Kremenek 
 |first5=George 
 |title=Storage resource broker – managing distributed data in a grid 
 |work= 
 |publisher= 
 |date= 
 |url=http://www.npaci.edu/DICE/Pubs/CSI-paper-sent.doc 
 |format= 
 |doi= 
 |accessdate=April 28, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20060507193028/http://www.npaci.edu:80/DICE/Pubs/CSI-paper-sent.doc 
 |archivedate=May 7, 2006 
 |df= 
}}

*{{cite journal
|last1= Belalem
|first1= Ghalem |last2= Meroufel |first2= Bakhta 
|year= 2011
|title= Management and placement of replicas in a hierarchical data grid 
|journal= International Journal of Distributed and Parallel Systems (IJDPS)
|volume= 2
|issue= 6
|pages= 23–30
|location =
|publisher=
|doi= 10.5121/ijdps.2011.2603
|url= http://www.scribd.com/doc/75105419/Management-and-Placement-of-Replicas-in-a-Hierarchical-Data-Grid
|accessdate= April 28, 2012}}

*{{cite journal
|last1= Chervenak
|first1= A.|last2= Foster |first2= I. |last3= Kesselman |first3= C.|last4= Salisbury |first4= C. |last5= Tuecke |first5= S.
|year= 2001
|title= The data grid: towards an architecture for the distributed management and analysis of large scientific datasets 
|journal= Journal of Network and Computer Applications
|volume= 23
|issue= 
|pages= 187–200
|location =
|publisher=
|doi= 10.1006/jnca.2000.0110
|url= http://www.globus.org/alliance/publications/papers/JNCApaper.pdf
|accessdate= April 11, 2012}}

*{{cite web
|last1= Chervenak
|first1= Ann |last2= Schuler |first2= Robert |last3= Kesselman | first3= Carl |last4= Koranda |first4= Scott |last5= Moe |first5= Brian
|title= Wide area data replication for scientific collaborations
|work= |publisher = [[IEEE]]
|date = November 14, 2005
|url= http://www.globus.org/alliance/publications/papers/chervenakGrid2005.pdf
|format=
|doi=
| accessdate = April 25, 2012 }}

*{{cite journal
 |last1=Coetzee 
 |first1=Serena 
 |year=2012 
 |title=Reference model for a data grid approach to address data in a dynamic SDI 
 |journal=Geoinformatica 
 |volume=16 
 |issue=1 
 |pages=111–129 
 |location= 
 |publisher= 
 |doi=10.1007/s10707-011-0129-4 
 |url=http://web.up.ac.za/sitefiles/file/48/16053/Coetzee_2011_ReferenceModelForDataGridApproach(2).pdf 
 |accessdate=April 28, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

*{{cite conference
|last1= Epimakhov
|first1= Igor 
|last2= Hameurlain
|first2= Abdelkader 
|last3= Dillon
|first3= Tharam 
|last4= Morvan
|first4= Franck 
|title= Resource Scheduling Methods for Query Optimization in Data Grid Systems 
|booktitle = Advances in Databases and Information Systems. 15th International Conference, ADBIS 2011
|pages = 185–199
|publisher = Springer Berlin Heidelberg
|year = 2011
|location = Vienna, Austria
|url = http://link.springer.com/chapter/10.1007%2F978-3-642-23737-9_14
|doi = 10.1007/978-3-642-23737-9_14
|id = 
|accessdate = September 20, 2011 }}

*{{cite web
|last1= Globus
|first1= 
|title= About the globus toolkit
|work=
|publisher= [[Globus Alliance|Globus]]
|year= 2012
|url= http://www.globus.org/toolkit/about.html
|doi=
|accessdate = May 27, 2012 }}

*{{cite news
 |last1=Heingartner 
 |first1=Douglas 
 |title=The Grid: The Next-Gen Internet 
 |work=Wired 
 |publisher= 
 |date=March 8, 2001 
 |url=http://www.wired.com/science/discoveries/news/2001/03/42230 
 |format= 
 |doi= 
 |accessdate=May 13, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20120504035536/http://www.wired.com:80/science/discoveries/news/2001/03/42230 
 |archivedate=May 4, 2012 
 |df= 
}}

*{{cite web
 |last1=Izmailov 
 |first1=Rauf 
 |last2=Ganguly 
 |first2=Samrat 
 |last3=Tu 
 |first3=Nan 
 |title=Fast parallel file replication in data grid 
 |work= 
 |publisher= 
 |year=2004 
 |url=http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf 
 |format= 
 |doi= 
 |accessdate=May 10, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20120421081052/http://www.cs.huji.ac.il/labs/danss/p2p/resources/fast-parallel-file-replication-on-data-grid.pdf 
 |archivedate=April 21, 2012 
 |df= 
}}

*{{cite journal
 |last1=Kranthi 
 |first1=G. Aruna 
 |last2=Rekha 
 |first2=D. Shashi 
 |year=2012 
 |title=Protected data objects replication in data grid 
 |journal=International Journal of Network Security & Its Applications (IJNSA) 
 |volume=4 
 |issue=1 
 |pages=29–41 
 |location= 
 |publisher= 
 |doi=10.5121/ijnsa.2012.4103 
 |url=http://journaldatabase.org/articles/protected_data_objects_replication.html 
 |issn=0975-2307 
 |accessdate=April 1, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20131008004646/http://journaldatabase.org/articles/protected_data_objects_replication.html 
 |archivedate=October 8, 2013 
 |df= 
}}

*{{cite journal
|last1= Krauter
|first1= Klaus |last2= Buyya |first2= Rajkumar |last3= Maheswaran |first3= Muthucumaru
|year= 2002
|title= A taxonomy and survey of grid resource management systems for distributed computing
|journal= Software Practice and Experience (SPE)
|volume= 32
|issue= 2
|pages= 135–164 
|location = 
|publisher= 
|doi=10.1002/spe.432
|citeseerx = 10.1.1.38.2122
|accessdate= <!-- April 17, 2012 -->}}


*{{cite conference
|last1= Lamehamedi
|first1= Houda |last2= Szymanski |first2= Boleslaw |last3= Shentu |first3= Zujun |last4= Deelman |first4= Ewa  
|title = Data replication strategies in grid environments
|booktitle = Fifth International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP’02)
|pages = 378–383
|publisher = Press
|year = 2002
|location =
|citeseerx = 10.1.1.11.5473
|doi =
|accessdate = <!-- April 5, 2012 --> }}

*{{cite journal
|last1= Padala
|first1= Pradeep
|title= A survey of data middleware for Grid systems
|work =
|publisher = 
|date = 
|citeseerx = 10.1.1.114.1901
|format =
|doi =
|accessdate = <!-- April 28, 2012 --> }}

*{{cite web
|last1= Raman 
|first1= Vijayshankar |last2= Narang |first2= Inderpal |last3= Crone |first3= Chris |last4= Hass |first4= Laura |last5= Malaika |first5= Susan 
|title= Services for data access and data processing on grids
|work =
|publisher = 
|date = February 9, 2003
|url = http://www.ogf.org/documents/GFD.14.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite conference
|last1= Ranganathan
|first1= Kavitha | last2= Foster |first2= Ian
|title= Identifying dynamic replication strategies for a high performance data grid
|booktitle= In Proc. of the International Grid Computing Workshop
|pages= 75–86
|publisher = 
|year= 2001
|location=
|citeseerx = 10.1.1.20.6836
|format=
|doi= 10.1007/3-540-45644-9_8
|accessdate = <!-- May 15, 2012 --> }}

*{{cite journal
|last1= Rasool
|first1= Qaisar |last2= Li |first2= Jianzhong |last3= Oreku| first3= George S.|last4= Munir |first4= Ehsan Ullah
|year= 2008
|title= Fair-share replication in data grid
|journal= Information Technology Journal
|volume= 7
|issue= 5
|pages= 776–782
|publisher=
|doi= 10.3923/itj.2008.776.782
|url= http://scialert.net/abstract/?doi=itj.2008.776.782
|accessdate= April 27, 2012 }}

*{{cite journal
|last1= Shorfuzzaman
|first1= Mohammad |last2= Graham |first2= Peter |last3= Eskicioglu |first3= Rasit 
|year= 2010
|title= Adaptive replica placement in hierarchical data grids
|journal= Journal of Physics: Conference Series
|volume= 256
|issue= 1
|pages= 1–18
|location =
|publisher= [[IOP Publishing Ltd]]
|doi= 10.1088/1742-6596/256/1/012020
|url= http://iopscience.iop.org/1742-6596/256/1/012020
|accessdate= April 15, 2012}}

*{{cite journal
|last1= Sreelatha
|first1= Malempati
|year= 2011
|title= Grid based approach for data confidentiality 
|journal= International Journal of Computer Applications
|volume= 25
|issue= 9
|pages= 1–5
|location =
|publisher=
|doi= 10.5120/3063-4186
|issn = 0975-8887
|url= http://www.ijcaonline.org/volume25/number9/pxc3874186.pdf
|accessdate= April 28, 2012}}

*{{cite journal
|last1= Thibodeau
|first1=P.   
|title= Governments plan data grid projects
|journal= Computerworld
|volume= 39
|issue= 42
|pages= 14
|location= United States
|publisher= Computerworld
|date = May 30, 2005
|url = http://www.computerworld.com/s/article/102119/Governments_Plan_Data_Grid_Projects
|format =
|doi=
|issn= 0010-4841
|accessdate = April 28, 2012 }}

*{{cite web
|last1= Thomas
|first1= R. K. |last2= Sandhu |first2= R. S. 
|title= Task-based authorization controls (tbac): a family of models for active and enterprise-oriented authorization management
|work =
|publisher = 
|year = 1997
|url = http://profsandhu.com/confrnc/ifip/i97tbac.pdf
|format =
|doi =
|accessdate = April 28, 2012 }}

*{{cite web
|last1=Tierney
|first1=Brian L.   
|title= Data grids and data grid performance issues
|work =
|publisher = 
|year = 2000
|url = http://www-didc.lbl.gov/presentations/CSC2000-tierney.pdf
|format =
|doi =
|accessdate = April 28, 2012 }}

*{{cite journal
|last1= Venugopal
|first1= Srikumar |last2= Buyya |first2= Rajkumar |last3= Ramamohanarao |first3= Kotagiri
|year= 2006
|title= A taxonomy of data grids for distributed data sharing, management and processing
|journal= ACM Computing Surveys
|volume= 38
|issue= 1
|pages= 1–60 
|location = New York
|publisher= [[Association for Computing Machinery]]
|doi=
|url= http://www.cloudbus.org/reports/DataGridTaxonomy.pdf
|accessdate= April 10, 2012}}

*{{cite web
 |last1=Zhu 
 |first1=Lichun 
 |title=Metadata management in grid database federation 
 |work= 
 |publisher= 
 |date= 
 |url=http://cs.uwindsor.ca/richard/cs510/lichun_zhu_survey.pdf 
 |format= 
 |doi= 
 |accessdate=May 15, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

==Further reading==

*{{cite web
|last1= Allcock
|first1= W.
|authorlink = W. Allcock
|title= Gridftp: protocol extensions to ftp for the grid
|work=
|publisher = [[Argonne National Laboratory]]
|date = April 2003
|url = http://www.globus.org/alliance/publications/papers/GFD-R.0201.pdf
|format = 
|doi =
| accessdate = April 20, 2012 }}

*{{cite web
|last1= Allcock
|first1= W.|last2= Bresnahan |first2= J. |last3= Kettimuthu |first3= R.|last4= Link |first4= M.|last5= Dumitrescu |first5= C.|last6= Raicu |first6= I.|last7= Foster |first7= I.
|title= The globus striped gridftp framework and server
|work=
|publisher= [[ACM Press]]
|date= November 2005
|url= http://www.globus.org/alliance/publications/papers/gridftp_final.pdf
|format=
|doi=
|accessdate = April 20, 2012 }}

*{{cite journal
|last1= Foster
|first1= Ian |last2= Kesselman |first2= Carl |last3= Tuecke |first3= Steven
|year= 2001
|title= The anatomy of the grid enabling scalable virtual organizations
|journal= [[International Journal of High Performance Computing Applications]]
|volume= 15
|issue= 3
|pages= 200–222 
|location = Thousand Oaks
|publisher= [[Sage Publications]]
|doi=10.1177/109434200101500302
|url= http://www.globus.org/alliance/publications/papers/anatomy.pdf
|accessdate= April 10, 2012}}

*{{cite web
 |last1=Foster 
 |first1=Ian 
 |last2=Kesselman 
 |first2=Carl 
 |last3=Nick 
 |first3=Jeffrey M. 
 |last4=Tuecke 
 |first4=Steven 
 |title=The physiology of the grid: an open grid services architecture for distributed systems integration 
 |work= 
 |publisher= 
 |date=June 22, 2002 
 |url=http://forge.gridforum.org/sf/go/doc13483?nav=1 
 |format= 
 |doi= 
 |accessdate=May 10, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20080322035911/http://forge.gridforum.org:80/sf/go/doc13483?nav=1 
 |archivedate=March 22, 2008 
 |df= 
}}

*{{cite journal
|last1= Hancock
|first1= B.
|year= 2009
|title= A simple data grid using the inferno operating system
|journal= Library Hi Tech
|volume= 27
|issue= 3
|pages= 382–392 
|location =
|publisher= [[Emerald Group Publishing Limited]]
|doi= 10.1108/07378830910988513
|url= }}<!--|accessdate= April 10, 2012-->

*{{cite web
 |last1=Hoschek 
 |first1=W. 
 |last2=McCance 
 |first2=G. 
 |title=Grid enabled relational database middleware 
 |work= 
 |publisher=[[Global Grid Forum]] 
 |date=October 10, 2001 
 |url=http://ppewww.ph.gla.ac.uk/preprints/2001/11/GGF3Rome2001.pdf 
 |format= 
 |doi= 
 |accessdate=April 22, 2012 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20060128234459/http://ppewww.ph.gla.ac.uk:80/preprints/2001/11/GGF3Rome2001.pdf 
 |archivedate=January 28, 2006 
 |df= 
}}

*{{cite web
|last1= Kunszt
|first1= Peter Z.|last2= Guy |first2= Leanne P.
|title= The open grid services architecture and data grids
|work =
|publisher = 
|date = July 7, 2002
|url = http://www.computing.surrey.ac.uk/courses/csm23/Papers/data_grid.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite web
|last1= Moore
|first1= Reagan W. 
|title= Evolution of data grid concepts
|work =
|publisher = 
|date = 
|url = http://www.nesc.ac.uk/events/GGF10-DA/programme/papers/06-Moore-Grid-evolution.pdf
|format =
|doi =
|accessdate = May 10, 2012 }}

*{{cite conference
|last1= Rajkumar
|first1= Kettimuthu |last2= Allcock |first2= William |last3= Liming |first3= Lee |last4= Navarro |first4= John-Paul |last5= Foster |first5= Ian
| title = GridCopy moving data fast on the grid
| booktitle = International parallel and distributed processing symposium (IPDPS 2007)
| pages = 1–6
| publisher = IEEE International
| date = March 30, 2007
| location =  Long Beach
| url = http://www.globus.org/alliance/publications/papers/GridCopy.pdf
| doi =
| id =
| accessdate = April 29, 2012 }}

*{{cite journal
|last1= Thenmozhi
|first1= N. |last2= Madheswaran |first2= M. 
|year= 2011
|title= Content based data transfer mechanism for efficient bulk data transfer in grid computing environment
|journal= International Journal of Grid Computing & Applications (IJGCA)
|volume= 2
|issue= 4
|pages= 49–62
|location =
|publisher=
|doi= 10.5121/ijgca.2011.2405
|issn= 2229-3949
|url= http://www.scribd.com/doc/78611092/Content-Based-Data-Transfer-Mechanism-for-Efficient-Bulk-Data-Transfer-in-Grid-Computing-Environment
|accessdate= April 28, 2012}}

*{{cite journal
 |last1=Tu 
 |first1=Manghui 
 |last2=Li 
 |first2=Peng 
 |last3=I-Ling 
 |first3=Yen 
 |last4=Thuraisingham 
 |first4=Bhavani 
 |last5=Khan 
 |first5=Latifur 
 |year=2010 
 |title=Secure data objects replication in data grid 
 |journal=IEEE Transactions on Dependable and Secure Computing 
 |volume=7 
 |issue=1 
 |pages=50–64 
 |publisher=[[IEEE]] 
 |doi=10.1109/tdsc.2008.19 
 |url=http://www.utdallas.edu/~lkhan/papers/Secure_Data_Objects_Replication_in_Data_Grid.pdf 
 |accessdate=April 26, 2012 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

[[Category:Data management]]
<=====doc_Id=====>:247
<=====title=====>:
Disaster recovery plan
<=====text=====>:
{{merge to|Business continuity planning|date=June 2015}}
A '''disaster recovery plan''' (DRP) is a documented process or set of procedures to recover and protect a business [[Information_technology|IT]] infrastructure in the event of a [[disaster]].<ref name="5 tips">{{cite web |url=http://www.smallbusinesscomputing.com/News/ITManagement/5-tips-to-build-an-effective-disaster-recovery-plan.html |title=5 Tips to Build an Effective Disaster Recovery Plan |publisher=Small Business Computing |date=14 June 2012 |accessdate=9 August 2012 |first1=Bill |last1=Abram}}</ref>  Such a plan, ordinarily documented in written form, specifies  procedures an organization is to follow in the event of a disaster. It is "a comprehensive statement of consistent actions to be taken before, during and after a disaster."<ref name="DR journal">{{cite web |url=http://www.drj.com/new2dr/w2_002.htm |title=Disaster Recovery Planning Process |first1=Geoffrey H. |last1=Wold |work=Disaster Recovery Journal |series=Adapted from Volume 5 #1 | publisher=Disaster Recovery World |year=1997 |accessdate=8 August 2012}}</ref> The disaster could be [[Natural disaster|natural]], [[Environmental disaster|environmental]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, an act of a terrorist) or unintentional (that is, accidental, such as the breakage of a man-made dam).

Given organizations' increasing dependency on [[information technology]] to run their operations, a disaster recovery plan, sometimes erroneously called a [[Continuity of Operations]] Plan (COOP), is increasingly associated with the recovery of information technology data, assets, and facilities.

==Objectives==

Organizations cannot always avoid disasters, but with careful planning the effects of a disaster can be minimized. The objective of a disaster recovery plan is to minimize downtime and data loss.<ref>[http://www.comp-soln.com/DRP_whitepaper.pdf ''An Overview of the Disaster Recovery Planning Process - From Start to Finish.''] Comprehensive Consulting Solutions Inc.( "Disaster Recovey Planning, An Overview: White Paper." )March 1999. Retrieved 8 August 2012.</ref> The primary objective is to protect the organization in the event that all or part of its operations and/or computer services are rendered unusable. The plan minimizes the disruption of operations and ensures that some level of organizational stability and an orderly recovery after a disaster will prevail.<ref name="DR journal" />  Minimizing downtime and data loss is measured in terms of two concepts: the [[recovery time objective]] (RTO) and the [[recovery point objective]] (RPO).
 
The recovery time objective is the time within which a business process must be restored, after a [[disaster|major incident]] (MI) has occurred, in order to avoid unacceptable consequences associated with a break in [[business continuity]]. The recovery point objective (RPO) is the age of files that must be recovered from backup storage for normal operations to resume if a computer, system, or network goes down as a result of a MI. The RPO is expressed backwards in time (that is, into the past) starting from the instant at which the MI occurs, and can be specified in seconds, minutes, hours, or days.<ref>[http://whatis.techtarget.com/definition/recovery-point-objective-RPO ''Definition: Recovery point objective (RPO).''] Retrieved 10 August 2012.</ref> The recovery point objective (RPO) is thus the maximum acceptable amount of data loss measured in time. It is the age of the files or data in backup storage required to resume normal operations after the MI.<ref>{{cite web |url=http://www.techopedia.com/definition/1032/recovery-point-objective-rpo |title=Recovery Point Objective (RPO): Definition - What does Recovery Point Objective (RPO) mean? |work=Techopedia |publisher=Janalta Interactive Inc. |year=2012 |accessdate=10 August 2012 }}</ref>

[[File:Schematic ITSC and RTO, RPO, MI.jpg|frame|left|A DR plan illustrating the chronology of the '''{{color|#bd00e0|RPO}}''' and the '''{{color|#ff7f7c|RTO}}''' with respect to the '''{{color|#fe0000|MI}}'''.]]
{{clear}}

==Relationship to the Business Continuity Plan==

According to the SANS institute, the [[Business continuity planning|Business Continuity Plan]] (BCP) is a comprehensive organizational plan that includes the disaster recovery plan. The Institute further states that a Business Continuity Plan (BCP) consists of the five component plans:<ref name="The Disaster Recovery Plan.">[http://www.sans.org/reading_room/whitepapers/recovery/disaster-recovery-plan_1164 ''The Disaster Recovery Plan.''] Chad Bahan. GSEC Practical Assignment version 1.4b. SANS Institute InfoSec Reading Room. June 2003. Retrieved 24 August 2012.</ref>

* Business Resumption Plan
* Occupant Emergency Plan
* Continuity of Operations Plan
* Incident Management Plan
* Disaster Recovery Plan

The Institute states that the first three plans (Business Resumption, Occupant Emergency, and Continuity of Operations Plans) do not deal with the IT infrastructure. They further state that the Incident Management Plan (IMP) does deal with the IT infrastructure, but since it establishes structure and procedures to address cyber attacks against an organization’s IT systems, it generally does not represent an agent for activating the Disaster Recovery Plan, leaving The Disaster Recovery Plan as the only BCP component of interest to IT.<ref name="The Disaster Recovery Plan."/>

[[Disaster Recovery Institute]] International states that disaster recovery is the area of business continuity that deals with ''technology'' recovery as opposed to the recovery of business operations.<ref>https://www.drii.org/glossary.php</ref>

==Benefits==

Like every insurance plan, there are benefits that can be obtained from the drafting of a disaster recovery plan. Some of these benefits are:<ref name="DR journal" />

* Providing a sense of security
* Minimizing risk of delays
* Guaranteeing the reliability of standby systems
* Providing a standard for testing the plan
* Minimizing decision-making during a disaster
* Reducing potential legal liabilities
* Lowering unnecessarily stressful work environment

==Types of plans==

There is no one right type of disaster recovery plan,<ref name=MSU>{{cite web |url=http://www.drp.msu.edu/documentation/stepbystepguide.htm |publisher=Michigan State University |title=Disaster Recovery Planning - Step by Step Guide |accessdate=9 May 2014 }}</ref> nor is there a one-size-fits-all disaster recovery plan.<ref name="5 tips" /><ref name=MSU /> However, there are three basic strategies that feature in all disaster recovery plans: (1) preventive measures, (2) detective measures, and (3) corrective measures.<ref>{{cite web |url=http://emailarchivingandremotebackup.com/backup-disaster-recovery.html |title=Backup Disaster Recovery |publisher=Email Archiving and Remote Backup |year=2010 |accessdate=9 May 2014}}</ref> Preventive measures will try to prevent a disaster from occurring. These measures seek to identify and reduce risks. They are designed to mitigate or prevent an event from happening. These measures may include keeping data backed up and off site, using surge protectors, installing generators and conducting routine inspections. Detective measures are taken to discover the presence of any unwanted events within the IT infrastructure.  Their aim is to uncover new potential threats. They may detect or uncover unwanted events. These measures include installing fire alarms, using up-to-date antivirus software, holding employee training sessions, and installing server and [[network monitoring]] software.  Corrective measures are aimed to restore a system after a disaster or otherwise unwanted event takes place. These measures focus on fixing or restoring the systems after a disaster. Corrective measures may include keeping critical documents in the Disaster Recovery Plan or securing proper [[insurance policy|insurance policies]], after a "lessons learned" brainstorming session.<ref name="5 tips" /><ref>{{cite web|url=http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |title=Disaster Recovery & Business Continuity Plans |publisher=Stone Crossing Solutions |date=2012 |accessdate=9 August 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20120823045007/http://www.stonecrossingsolutions.com/technical-solutions/disaster-recovery/ |archivedate=23 August 2012 |df= }}</ref>

A disaster recovery plan must answer at least three basic questions: (1) what is its objective and purpose, (2) who will be the people or teams who will be responsible in case any disruptions happen, and (3) what will these people do (the procedures to be followed) when the disaster strikes.<ref>{{cite web|url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |title=Disaster Recovery – Benefits of Getting Disaster Planning Software and Template and Contracting with Companies Offering Data Disaster Recovery Plans, Solutions and Services: Why Would You Need a Disaster Recovery Plan? |publisher=Continuity Compliance |date=7 June 2011 |accessdate=14 August 2012 |archivedate=9 May 2014 |archiveurl=http://www.webcitation.org/6PQaoed5G?url=http://www.continuitycompliance.org/disaster-recovery-planning-on-virtual-and-cloud-platforms-survey-results-now-available/ |deadurl=yes |df= }}</ref>

==Types of disasters==
[[Image:SH-60B helicopter flies over Sendai.jpg|thumb|right|200px|The tsunami that affected Japan in 2011, a type of natural disaster]]
[[Image:UA Flight 175 hits WTC south tower 9-11 edit.jpeg|thumb|right|200px|September 11, 2001, in New York City, a type of man-made disaster: it caused pollution, loss of lives, property damage, and considerable [[data loss]]]]

Disasters can be [[Natural disaster|natural]] or [[Anthropogenic hazard|man-made]]. Man-made disasters could be intentional (for example, sabotage or an act of [[terrorism]]) or unintentional (that is, accidental, such as the breakage of a man-made dam).  Disasters may encompass more than weather. They may involve Internet threats or take on other man-made manifestations such as theft.<ref name="5 tips" />

===Natural disaster===
{{Main article|Natural disaster}}
A natural disaster is a major adverse event resulting from the earth's natural hazards. Examples of natural disasters are [[flood]]s, [[tsunami]]s, [[tornado]]es, [[hurricane|hurricanes/cyclones]], [[volcanic eruption]]s, [[earthquake]]s, [[heat wave]]s, and [[landslide]]s.  Other types of disasters include the more [[End time|cosmic]] scenario of an [[Impact event|asteroid hitting the Earth]].

===Man-made disasters===
{{Main article|Man-made disasters}}
Man-made disasters are the consequence of technological or human hazards. Examples include [[stampede]]s, [[fire|urban fires]], [[industrial accident]]s, [[oil spill]]s, [[nuclear explosion]]s/[[nuclear radiation]] and acts of [[war]].  Other types of man-made disasters include the more cosmic scenarios of catastrophic [[global warming]], [[nuclear war]], and [[bioterrorism]].

The following table categorizes some disasters and notes first response initiatives. Note that whereas the sources of a disaster may be natural (for example, heavy rains) or man-made (for example, a broken dam), the results may be similar (flooding).<ref>[http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc ''Business Continuity Planning (BCP): Sample Plan For Nonprofit Organizations.''] {{wayback|url=http://www.nten.org/sites/nten/files/Sample%20Disaster%20Recovery%20Plan.doc |date=20100602065521 }} Pages 11-12. Retrieved 8 August 2012.</ref>

{| class="wikitable"
! rowspan="16" | Natural
! colspan="3"  | Disaster
|- bgcolor="#CCCCCC"
!Example|| Profile || First Response
|-
|[[Avalanche]]||The sudden, drastic flow of snow down a slope, occurring when either natural triggers, such as loading from new snow or rain, or artificial triggers, such as explosives or backcountry skiers, overload the snowpack||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Blizzard]]||A severe snowstorm characterized by very strong winds and low temperatures||Power off all equipment; listen to blizzard advisories; Evacuate area, if unsafe; Assess damage
|-
|[[Earthquake]]||The shaking of the earth’s crust, caused by underground volcanic forces of breaking and shifting rock beneath the earth’s surface||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Fire|Fire (wild)]]||Fires that originate in uninhabited areas and which pose the risk to spread to inhabited areas||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories
|-
|[[Flood]]||Flash flooding: Small creeks, gullies, dry streambeds, ravines, culverts or even low-lying areas flood quickly||Monitor flood advisories; Determine flood potential to facilities; Pre-stage emergency power generating equipment; Assess damage
|-
|[[Freezing Rain]]||Rain occurring when outside surface temperature is below freezing||Monitor weather advisories; Notify employees of business closure; home; Arrange for snow and ice removal
|-
|[[Heat wave]]||A prolonged period of excessively hot weather relative to the usual weather pattern of an area and relative to normal temperatures for the season||Listen to weather advisories; Power-off all servers after a graceful shutdown if there is imminent potential of power failure; Shut down main electric circuit usually located in the basement or the first floor
|-
|[[Hurricane]]||Heavy rains and high winds||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage
|-
|[[Landslide]]||Geological phenomenon which includes a range of ground movement, such as rock falls, deep failure of slopes and shallow debris flows||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Lightning strike]]||An electrical discharge caused by lightning, typically during thunderstorms||Power off all equipment; listen to hurricane advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Do not use telephones, in the event of severe lightning; Assess damage
|-
|[[Limnic eruption]]||The sudden eruption of carbon dioxide from deep lake water||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
|[[Tornado]]||Violent rotating columns of air which descent from severe thunderstorm cloud systems||Monitor tornado advisories; Power off equipment; Shut off utilities (power and gas); Assess damage once storm passes
|-
|[[Tsunami]]||A series of water waves caused by the displacement of a large volume of a body of water, typically an ocean or a large lake, usually caused by earthquakes, volcanic eruptions, underwater explosions, landslides, glacier calvings, meteorite impacts and other disturbances above or below water||Power off all equipment; listen to tsunami advisories; Evacuate area, if flooding is possible; Check gas, water and electrical lines for damage; Assess damage
|-
|[[Volcanic eruption]]||The release of hot magma, volcanic ash and/or gases from a volcano||Shut off utilities; Evacuate building if necessary; Determine impact on the equipment and facilities and any disruption
|-
! rowspan="6" | Man-made
|[[Bioterrorism]]||The intentional release or dissemination of biological agents as a means of coercion||Get information immediately from your [[Public Health]] officials via the news media as to the right course of action; If you think you have been exposed, quickly remove your clothing and wash off your skin; Also put on a [[HEPA]] to help prevent inhalation of the agent<ref>[http://answers.webmd.com/answers/1176206/what-should-i-do-if-there ''What should I do if there has been a bioterrorism attack?.''] Edmond A. Hooker. WebMD. 9 October 2007. Retrieved 18 September 2012.</ref> 
|-
|[[Civil unrest]]||A disturbance caused by a group of people that may include [[sit-in]]s and other forms of obstructions, riots, sabotage and other forms of crime, and which is intended to be a demonstration to the public and the government, but can escalate into general chaos||Contact local police or law enforcement<ref>[http://www.usfa.fema.gov/downloads/pdf/publications/fa-142.pdf ''Report of the Joint Fire/Police Task Force on Civil Unrest (FA-142): Recommendations for Organization and Operations During Civil Disturbance.''] Page 55. FEMA. Retrieved 21 October 2012.</ref><ref>[http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf ''Business Continuity Planning: Developing a Strategy to Minimize Risk and Maintain Operations.''] {{wayback|url=http://www.xanaboo.com/BCP%20-%20Developing%20a%20Strategy%20to%20Minimize%20Risk%20and%20Maintain%20Operations.pdf |date=20140327234742 }} Adam Booher. Retrieved 19 September 2012.</ref> 
|-
|[[Fire|Fire (urban)]]||Even with strict building fire codes, people still perish needlessly in fires||Attempt to suppress fire in early stages; Evacuate personnel on alarm, as necessary; Notify fire department; Shut off utilities; Monitor weather advisories
|-
|[[Hazardous material|Hazardous material spills]]||The escape of solids, liquids, or gases that can harm people, other living organisms, property or the environment, from their intended controlled environment such as a container.||Leave the area and call the local fire department for help.<ref>[http://www.tnema.org/public/hazmat.html ''Hazardous Materials.''] {{wayback|url=http://www.tnema.org/public/hazmat.html |date=20121011150052 }} Tennessee Emergency Management Office. Retrieved 7 September 2012.</ref> If anyone was affected by the spill, call the your local Emergency Medical Services line<ref>[http://www.atsdr.cdc.gov/MHMI/index.asp ''Managing Hazardous Materials Incidents (MHMIs).''] Center for Disease Control. Retrieved 7 September 2012.</ref>
|-
||[[Nuclear and radiation accidents|Nuclear and Radiation Accidents]]||An event involving significant release of radioactivity to the environment or a reactor core meltdown and which leads to major undesirable consequences to people, the environment, or the facility||Recognize that a CBRN incident has or may occur. Gather, assess and disseminate all available information to first responders. Establish an overview of the affected area. Provide and obtain regular updates to and from first responders.<ref>[http://www.nato.int/docu/cep/cep-cbrn-response-e.pdf ''Guidelines for First Response to a CBRN Incident.''] Project on Minimum Standards and Non-Binding Guidelines for First Responders Regarding Planning, Training, Procedure and Equipment for Chemical, Biological, Radiological and Nuclear (CBRN) Incidents.] NATO. Emergency Management. Retrieved 21 October 2012.</ref>
|-
|[[Power Failure]]||Caused by summer or winter storms, lightning or construction equipment digging in the wrong location||Wait 5–10 minutes; Power-off all Servers after a graceful shutdown; Do not use telephones, in the event of severe lightning; Shut down main electric circuit usually located in the basement or the first floor
|-
|}

In the realm of information technology per se, disasters may also be the result of a computer security exploit. Some of these are: [[computer virus]]es, [[cyberattack]]s, [[denial-of-service attack]]s, [[hacker (computer security)|hacking]], and [[malware]] exploits. These are ordinarily attended to by [[information security]] experts.

==Planning methodology==

According to Geoffrey H. Wold of the Disaster Recovery Journal, the entire process involved in developing a Disaster Recovery Plan consists of 10 steps:<ref name="DR journal" />

===Obtaining top management commitment===
For a disaster recovery plan to be successful, the central responsibility for the plan must reside on [[Management#Top-level managers|top management]]. Management is responsible for coordinating the disaster recovery plan and ensuring its effectiveness within the organization. It is also responsible for allocating adequate time and resources required in the development of an effective plan. Resources that management must allocate include both financial considerations and the effort of all personnel involved.

===Establishing a planning committee===
A [[plan]]ning [[committee]] is appointed to oversee the development and implementation of the plan. The planning committee includes representatives from all functional areas of the organization. Key committee members customarily include the operations manager and the data processing manager. The committee also defines the scope of the plan.

===Performing a risk assessment===
The planning committee prepares a [[Probabilistic risk assessment|risk analysis]] and a [[business impact analysis]] (BIA) that includes a range of possible disasters, including natural, technical and human threats. Each functional area of the organization is analyzed to determine the potential consequence and impact associated with several disaster scenarios. The risk assessment process also evaluates the safety of critical documents and vital records. Traditionally, fire has posed the greatest threat to an organization. Intentional human destruction, however, should also be considered. A thorough plan provides for the “worst case” situation: destruction of the main building. It is important to assess the impacts and consequences resulting from loss of information and services. The planning committee also analyzes the costs related to minimizing the potential exposures.

===Establishing priorities for processing and operations===
At this point, the critical needs of each department within the organization are evaluated in order to prioritize them. Establishing [[Wiktionary:priority|priorities]] is important because no organization possesses infinite resources and criteria must be set as to where to allocate resources first. Some of the areas often reviewed during the prioritization process are functional operations, key personnel and their functions, information flow, processing systems used, services provided, existing documentation, historical records, and the department's policies and procedures.

Processing and operations are analyzed to determine the maximum amount of time that the department and organization can operate without each critical system. This will later get mapped into the [[Recovery Time Objective]]. A critical system is defined as that which is part of a system or procedure necessary to continue operations should a department, computer center, main facility or a combination of these be destroyed or become inaccessible. A method used to determine the critical needs of a department is to document all the functions performed by each department. Once the primary functions have been identified, the operations and processes are then ranked in order of priority: essential, important and non-essential.

===Determining recovery strategies===
During this phase, the most practical alternatives for processing in case of a disaster are researched and evaluated. All aspects of the organization are considered, including [[Building|physical facilities]], [[computer hardware]] and [[software]], [[communications link]]s, [[data file]]s and [[database]]s, [[customer service]]s provided, user operations, the overall [[management information system]]s (MIS) structure, [[end-user]] systems, and any other processing operations.

Alternatives, dependent upon the evaluation of the computer function, may include: [[hot site]]s, [[warm site]]s, [[cold site]]s, [[reciprocal agreement (disaster preparedness)|reciprocal agreements]], the provision of more than one data center, the installation and deployment of multiple computer system, duplication of service center, [[consortium]] arrangements, lease of equipment, and any combinations of the above.

Written [[Contract|agreements]] for the specific recovery alternatives selected are prepared, specifying contract duration, termination conditions, [[system testing]], [[cost]], any special security procedures, procedure for the notification of system changes, hours of operation, the specific hardware and other equipment required for processing, personnel requirements, definition of the circumstances constituting an [[emergency]], process to negotiate service extensions, guarantee of [[Computer compatibility|compatibility]], [[availability]], non-mainframe resource requirements, priorities, and other contractual issues.

===Collecting data===
In this phase, data collection takes place. Among the recommended data gathering materials and documentation often included are
various lists (employee backup position listing, critical telephone numbers list, master call list, master vendor list, notification checklist), inventories (communications equipment, documentation, office equipment, forms, [[insurance policy|insurance policies]], workgroup and data center computer hardware, [[microcomputer]] hardware and software, [[office supplies|office supply]], off-site storage location equipment, telephones, etc.), distribution register, software and data files backup/retention schedules, temporary location specifications, any other such other lists, materials, inventories and documentation. Pre-formatted forms are often used to facilitate the data gathering process.

===Organizing and documenting a written plan===
Next, an outline of the plan’s contents is prepared to guide the development of the detailed procedures. Top management reviews and approves the proposed plan. The outline can ultimately be used for the [[table of contents]] after final revision. Other four benefits of this approach are that (1) it helps to organize the detailed procedures, (2) identifies all major steps before the actual writing process begins, (3) identifies redundant procedures that only need to be written once, and (4) provides a [[plan|road map]] for developing the procedures.

It is often considered [[best practice]] to develop a standard format for the disaster recovery plan so as to facilitate the writing of detailed procedures and the documentation of other information to be included in the plan later. This helps ensure that the disaster plan follows a consistent format and allows for its ongoing future maintenance. [[Standardization]] is also important if more than one person is involved in writing the procedures.

It is during this phase that the actual written plan is developed in its entirety, including all detailed procedures to be used before, during, and after a disaster. The procedures include methods for maintaining and updating the plan to reflect any significant internal, external or systems changes. The procedures allow for a regular review of the plan by key personnel within the organization. The disaster recovery plan is structured using a team approach. Specific responsibilities are assigned to the appropriate team for each functional area of the organization. Teams responsible for administrative functions, [[building|facilities]], [[logistics]], user support, [[backup|computer backup]], restoration and other important areas in the organization are identified.

The structure of the contingency organization may not be the same as the existing organization chart. The contingency organization is usually structured with teams responsible for major functional areas such as administrative functions, facilities, logistics, user support, computer backup, restoration, and any other important area.

The [[management team]] is especially important because it coordinates the recovery process. The team assesses the disaster, activates the recovery plan, and contacts team managers. The management team also oversees, documents and monitors the recovery process. It is helpful when management team members are the final decision-makers in setting priorities, policies and procedures. Each team has specific responsibilities that are completed to ensure successful execution of the plan. The teams have an assigned manager and an alternate in case the team manager is not available. Other team members may also have specific assignments where possible.

===Developing testing criteria and procedures===
Best practices dictate that DR plans be thoroughly tested and evaluated on a regular basis (at least annually). Thorough DR plans include documentation with the procedures for testing the plan. The tests will provide the organization with the assurance that all necessary steps are included in the plan. Other reasons for testing include:
* Determining the feasibility and compatibility of backup facilities and procedures.
* Identifying areas in the plan that need modification.
* Providing training to the team managers and team members.
* Demonstrating the ability of the organization to recover.
* Providing motivation for maintaining and updating the disaster recovery plan.

===Testing the plan===
After testing procedures have been completed, an initial "[[Dry run (testing)|dry run]]" of the plan is performed by conducting a structured walk-through test. The test will provide additional information regarding any further steps that may need to be included, changes in procedures that are not effective, and other appropriate adjustments. These may not become evident unless an actual dry-run test is performed. The plan is subsequently updated to correct any problems identified during the test. Initially, testing of the plan is done in sections and after normal business hours to minimize disruptions to the overall operations of the organization. As the plan is further polished, future tests occur during normal business hours.

Types of tests include: checklist tests, simulation tests, parallel tests, and full interruption tests.

===Obtaining plan approval===
Once the disaster recovery plan has been written and tested, the plan is then submitted to management for approval. It is top management’s ultimate responsibility that the organization has a documented and tested plan. Management is responsible for (1) establishing the policies, procedures and responsibilities for comprehensive [[contingency plan]]ning, and (2) reviewing and approving the contingency plan annually, documenting such reviews in writing.

Organizations that receive information processing from [[service bureau]]s will, in addition, also need to (1) evaluate the adequacy of contingency plans for its service bureau, and (2)ensure that its contingency plan is compatible with its service bureau’s plan.

==Caveats/controversies==

Due to its high cost, disaster recovery plans are not without critics. [[Cormac Foster]] has identified five "common mistakes" organizations often make related to disaster recovery planning:<ref>[https://web.archive.org/web/20130116112225/http://content.dell.com/us/en/enterprise/d/large-business/mistakes-that-kill-disaster.aspx ''Five Mistakes That Can Kill a Disaster Recovery Plan. In archive.org''] Cormac Foster. Dell Corporation. 25 October 2010. Retrieved 8 August 2012.</ref>

===Lack of buy-in===
One factor is the perception by executive management that DR planning is "just another fake earthquake drill" or CEOs that fail to make DR planning and preparation a priority, are often significant contributors to the failure of a DR plan.

===Incomplete RTOs and RPOs===
Another critical point is failure to include each and every important business process or a block of data. "Every item in your DR plan requires a Recovery Time Objective (RTO) defining maximum process downtime or a Recovery Point Objective (RPO) noting an acceptable restore point. Anything less creates ripples that can extend the disaster's impact." As an example, "payroll, accounting and the weekly customer newsletter may not be mission-critical in the first 24 hours, but left alone for several days, they can become more important than any of your initial problems."

===Systems myopia===
A third point of failure involves focusing only on DR without considering the larger business continuity needs: "Data and systems restoration after a disaster are essential, but every business process in your organization will need IT support, and that support requires planning and resources." As an example, corporate office space lost to a disaster can result in an instant pool of teleworkers which, in turn, can overload a company's [[VPN]] overnight, overwork the IT support staff at the blink of an eye and cause serious bottlenecks and monopolies with the dial-in PBX system.

===Lax security===
When there is a disaster, an organization's data and business processes become vulnerable. As such, security can be more important than the raw speed involved in a disaster recovery plan's RTO. The most critical consideration then becomes securing the new data pipelines: from new VPNs to the connection from offsite backup services. Another security concern includes documenting every step of the recovery process—something that is especially important in highly regulated industries, government agencies, or in disasters requiring post-mortem forensics. Locking down or remotely wiping lost handheld devices is also an area that may require addressing.

===Outdated plans===
Another important aspect that is often overlooked involves the frequency with which DR Plans are updated. Yearly updates are recommended but some industries or organizations require more frequent updates because business processes evolve or because of quicker data growth. To stay relevant, disaster recovery plans should be an integral part of all [[business analysis]] processes, and should be revisited at every major corporate acquisition, at every new product launch and at every new system development milestone.

==See also==
* [[Disaster recovery]]
* [[Business continuity planning]]
* [[Federal Emergency Management Agency]]
* [[Backup rotation scheme]]
* [[Seven tiers of disaster recovery]]

==References==
{{reflist|2}}

{{DEFAULTSORT:Disaster recovery plan}}
[[Category:Disaster recovery]]
[[Category:Data management]]
[[Category:Backup]]
[[Category:IT risk management]]
[[Category:Planning]]
<=====doc_Id=====>:250
<=====title=====>:
Single customer view
<=====text=====>:
A '''Single Customer View''' is an aggregated, consistent and holistic representation of the [[data]] known by an organisation about its customers<ref>[http://www.experian.co.uk/assets/about-us/white-papers/single-customer-view-whitepaper.pdf Exploiting the Single Customer  View to Maximise the Value of Customer Relationships]</ref><ref>[http://www.marketingweek.co.uk/driving-value-from-the-single-customer-view/3015497.article Driving value from the single customer view]</ref> that can be viewed in one place, such as a single page.<ref>[https://spotlessdata.com/blog/data-driven-marketing Data-driven marketing]</ref> The advantage to an organisation of attaining this unified view comes from the ability it gives to analyse past behaviour in order to better target and personalise future customer interactions.<ref>[http://www.atominsight.com/about-us/blog/single-customer-view-essential Why a single customer view is essential]</ref> A single customer view is also considered especially relevant where organisations engage with customers through [[multichannel marketing]], since customers expect those interactions to reflect a consistent understanding of their history and preferences.<ref>[http://econsultancy.com/uk/blog/9612-the-impact-of-a-single-customer-view-on-consumer-behaviour-infographic The impact of a single customer view on consumer behaviour: infographic]</ref> However, some commentators have challenged the idea that a single view of customers across an entire organisation is either natural or meaningful, proposing that the priority should instead be consistency between the multiple views that arise in different contexts.

Where representations of a customer are held in more than one [[data set]], achieving a single customer view can be difficult: firstly because customer identity must be traceable between the records held in those systems, and secondly because anomalies or discrepancies in the customer data must be [[data cleansing|data cleansed]].<ref>[http://www.atominsight.com/about-us/blog/single-customer-view-hard Why building a single customer view isn’t as easy as you might think]</ref> As such, the acquisition by an organisation of a single customer view is one potential outcome of successful [[master data management]]. Since 31 December, 2010, maintaining a single customer view has become mandatory for [[United Kingdom]] banks and other deposit takers due to new rules introduced by the [[Financial Services Compensation Scheme]].<ref>[https://www.fscs.org.uk/industry/single-customer-view/ Single Customer View]</ref>

==References==
{{reflist}}

[[Category:Identity management]]
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]
<=====doc_Id=====>:253
<=====title=====>:
Consistency (database systems)
<=====text=====>:
{{merge from|Data consistency|date=November 2014}}{{About||Consistency in distributed systems as defined in the CAP Theorem|CAP theorem}}

'''Consistency''' in [[database systems]] refers to the requirement that any given [[database transaction]] must change affected data only in allowed ways. Any data written to the database must be valid according to all defined rules, including [[Integrity constraints|constraints]], [[Cascading rollback|cascades]], [[Database trigger|triggers]], and any combination thereof.  This does not guarantee correctness of the transaction in all ways the application programmer might have wanted (that is the responsibility of application-level code) but merely that any programming errors cannot result in the violation of any defined rules.

==As an ACID guarantee==
Consistency is one of the four guarantees that define [[ACID]] [[database transaction|transactions]]; however, significant ambiguity exists about the nature of this guarantee. It is defined variously as:
* The guarantee that any transactions started in the future necessarily see the effects of other transactions committed in the past<ref name="CAP Theorem Paper">http://webpages.cs.luc.edu/~pld/353/gilbert_lynch_brewer_proof.pdf "Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web Services"</ref><ref name="Ports et al">{{cite journal | url=http://drkp.net/papers/txcache-osdi10.pdf | title=Transactional Consistency and Automatic Management in an Application Data Cache |author1=Ports, D.R.K |author2=Clements, A.T |author3=Zhang, I |author4=Madden, S |author5=Liskov, B. | journal=MIT CSAIL}}</ref>
* The guarantee that [[Relational database#Constraints|database constraints]] are not violated, particularly once a transaction commits<ref name="Haerder & Reuter">{{cite journal | url=http://www.minet.uni-jena.de/dbis/lehre/ws2005/dbs1/HaerderReuter83.pdf | title=Principles of Transaction-Oriented Database Recovery |author1=Haerder, T |author2=Reuter, A. | journal=Computing Surveys |date=December 1983  | volume=15 | issue=4 | pages=287–317}}</ref><ref>{{cite web|url=http://databases.about.com/od/specificproducts/a/acid.htm|title=The ACID Model|author=Mike Chapple|work=About}}</ref><ref>{{cite web|url=http://msdn.microsoft.com/en-us/library/aa480356.aspx|title=ACID properties|publisher=}}</ref><ref>{{cite web|url=http://www.techopedia.com/definition/23949/atomicity-consistency-isolation-durability-acid|title=What is ACID in Databases? - Definition from Techopedia|author=Cory Janssen|work=Techopedia.com}}</ref>
* The guarantee that operations in transactions are performed accurately, correctly, and with validity, with respect to application semantics<ref>{{cite web|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=27614|title=ISO/IEC 10026-1:1998 - Information technology -- Open Systems Interconnection -- Distributed Transaction Processing -- Part 1: OSI TP Model|publisher=}}</ref>

As these various definitions are not mutually exclusive, it is possible to design a system that guarantees "consistency" in every sense of the word, as most [[relational database management system]]s in common use today arguably do.

==As a CAP trade-off==

The [[CAP theorem]] is based on three trade-offs, one of which is "atomic consistency" (shortened to "consistency" for the acronym), about which the authors note, "Discussing atomic consistency is somewhat different than talking about an ACID database, as database consistency refers to transactions, while atomic consistency refers only to a property of a single request/response operation sequence. And it has a different meaning than the Atomic in ACID, as it subsumes the database notions of both Atomic and Consistent."<ref name="CAP Theorem Paper" />

==See also==
* [[Consistency model]]
* [[CAP theorem]]
* [[Eventual consistency]]

==References==
{{reflist}}

{{DEFAULTSORT:Consistency (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:256
<=====title=====>:
PL/Perl
<=====text=====>:
'''PL/Perl (Procedural Language/Perl)''' is a procedural language supported by the [[PostgreSQL]] [[RDBMS]].

PL/Perl, as an [[imperative programming language]], allows more control than the [[relational algebra]] of [[SQL]].
Programs created in the PL/Perl language are called functions and can use most of the features that the [[Perl|Perl programming language]] provides, including common flow control structures and syntax that has incorporated [[regular expressions]] directly.
These functions can be evaluated as part of a SQL statement, or in response to a [[Database trigger|trigger]] or [[Constraint (database)|rule]].

The design goals of PL/Perl were to create a loadable procedural language that:

* can be used to create functions and trigger procedures,
* adds control structures to the SQL language,
* can perform complex computations,
* can be defined to be either [http://www.postgresql.org/docs/current/static/plperl-trusted.html trusted or untrusted] by the server,
* is easy to use.

PL/Perl is one of many "PL" languages available for PostgreSQL
[[PL/pgSQL]]
[http://gborg.postgresql.org/project/pljava/projdisplay.php PL/Java], 
[http://plphp.commandprompt.com/ plPHP], 
[http://www.postgresql.org/docs/current/interactive/plpython.html PL/Python], 
[http://www.joeconway.com/plr/ PL/R], 
[http://raa.ruby-lang.org/list.rhtml?name=pl-ruby PL/Ruby], 
[http://plsh.projects.postgresql.org/ PL/sh], 
and [http://www.postgresql.org/docs/current/interactive/pltcl.html PL/Tcl].

==References==
* [http://www.postgresql.org/docs/current/static/plperl.html PostgreSQL PL/Perl documentation]

{{DEFAULTSORT:PL Perl}}
[[Category:Data management]]
[[Category:PostgreSQL]]
[[Category:Data-centric programming languages]]
<=====doc_Id=====>:259
<=====title=====>:
EU Open Data Portal
<=====text=====>:
{{Infobox Website
| logo = |250px
| url = {{URL|http://data.europa.eu/euodp/}}
| commercial = No
| type = [[public services|Public service]] [[Web portal|portal]] and <br />institutional information
| registration = Not required
| language = 24 official languages of the EU 
| owner = {{Flag|European Union}}
| content license = Open
| author = [[Publications Office (European Union)|EU Publications Office]]
| launch date = December 2012
}}

The '''EU Open Data Portal''' is the single point of access to a wide range of data held by EU institutions, agencies and other bodies. The portal is a key element of EU open data strategy.

== Legal basis and launch date ==

Launched in December 2012 in beta mode, the portal was formally established by Commission Decision of 12 December 2011 (2011/833/EU) on the reuse of Commission documents to promote accessibility and reuse.<ref name="r1">{{cite news|url=http://eur-lex.europa.eu/legal-content/en/TXT/?uri=CELEX:32011D0833|title=Commission Decision of 12 December 2011 (2011/833/EU)}}</ref> 

While the operational management of the portal is the task of the [[Publications Office of the European Union]], implementation of EU open data policy is the responsibility of the [[Directorate General for Communications Networks, Content and Technology]] of the European Commission.

== Features ==

The portal allows anyone to easily search, explore, link, download and reuse the data for commercial or non-commercial purposes, through a catalogue of common metadata. Through this catalogue, users access data stored on the websites of the EU institutions, agencies and other bodies.

Semantic technologies offer new functionalities. The metadata catalogue can be searched via an interactive search engine (Data tab) and through [[SPARQL]] queries (Linked data tab). There is also a showcase of visualisation applications from various EU institutions, agencies and other bodies.

Users can suggest data they would like the portal to be linked to, give feedback on the quality of data obtainable and share information with other users about how they have used it.

The interface is in 24 EU official languages, while most [[metadata]] are currently available in a limited number of languages (English, French and German). Some of the metadata (e.g. names of the data providers, geographical coverage) are in 24 languages following the translation of [[controlled vocabulary]] lists that are used by the portal.<ref name="r3">{{cite news|url=http://publications.europa.eu/mdr/authority/index.html|title=EU controlled vocabularies}}</ref>

== Terms of reuse ==

Most data accessible via the EU Open Data Portal are covered by the Europa Legal Notice <ref name="r6">{{cite news|url=http://ec.europa.eu/geninfo/legal_notices_en.htm|title=Europa Legal Notice}}</ref> and can be reused free of charge, for commercial and non-commercial purposes, provided that the source is acknowledged. Specific conditions on reuse, related mostly to the protection of third-party intellectual property rights, apply for a very limited amount of data.

== Data available ==

The portal contains a very wide variety of high-value open data across EU policy domains, as also more recently identified by the G8 Open Data Charter. These include the economy, employment, science, environment and education. The number of data providers — which include [[Eurostat]], the [[European Environment Agency]] and the [[Joint Research Centre]] — continues to grow.

So far, around 56 EU institutions, bodies or departments (e.g. Eurostat, the European Environment Agency, the Joint Research Centre and other European Commission Directorates General and EU Agencies) have made datasets available, making a total of over 7,800.

In addition to giving access to datasets, the portal also is an easy entry point to a whole range of visualisation applications using EU data. The applications are displayed as much for their information value as for giving examples of what applications can be made using the data.

== Architecture of the portal ==

The portal is built using open source solutions such as the [[Drupal]] content management system and [[CKAN]], the data catalogue software developed by the [[Open Knowledge Foundation]]. It uses Virtuoso as an [[Resource Description Framework|RDF]] database and has a [[SPARQL]] endpoint.

Its metadata catalogue is built on the basis of international standards such as [[Dublin Core]], the data catalogue vocabulary DCAT and the asset description metadata schema [[ADMS]].<ref name="r5">{{cite news|url=http://ec.europa.eu/digital-agenda/en/open-data-portals|title=Open Data Portals in Europe}}</ref>

==See also==

*[[Open data]]
*[[Institutions of the European Union]] 
*[[Agencies of the European Union]]
*[[Bodies of the European Union]]
*[[European Data Portal]]

==References==
{{reflist}}

==External links==
* [http://ec.europa.eu/europe2020/index_en.htm Europe 2020 – Official EU Site]
* [http://ec.europa.eu/digital-agenda/ Digital Agenda for Europe]
* [http://ec.europa.eu/digital-agenda/en/open-data-0 Open Data section of above site]
* [https://joinup.ec.europa.eu/community/ods/description Joinup community on EU open data]
* [http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:52011DC0882 Communication ‘Open data — An engine for innovation, growth and transparent governance’]
* [https://ec.europa.eu/digital-agenda/en/legislative-measures Legal rules on public services information]
* [http://okfn.org/ Open Knowledge Foundation]
* [http://dublincore.org/ Dublin Core]
* [http://latc-project.eu/ Publication and usage of linked data on the Web]
* [http://datacatalogs.org/group/eu-official Data catalogues]
* [http://5stardata.info/ Open data classification by [[Tim Berners Lee]]] 
* [http://opendatachallenge.org/ Open Data Challenge (now over)]

[[Category:European Commission]]
[[Category:Open data]]
[[Category:Transparency (behavior)]]
[[Category:Open government]]
[[Category:Semantic Web]]
[[Category:Data management]]
[[Category:Creative Commons]]
<=====doc_Id=====>:262
<=====title=====>:
Approximate inference
<=====text=====>:
'''Approximate inference''' methods make it possible to learn realistic models from [[big data]] by trading off computation time for accuracy, when exact learning and [[inference]] are [[computationally intractable]].

==Major methods classes ==

*[[Variational Bayesian method]]s 
*[[Expectation propagation]]
*[[Markov random field]]s 
*[[Bayesian network]]s
**[[Variational message passing]]
*loopy and generalized [[belief propagation]] 
<ref>{{cite journal|url=http://academic.research.microsoft.com/Paper/14666.aspx|title=Approximate Inference and Constrained Optimization|journal=Uncertainty in Artificial Intelligence - UAI|pages=313–320|year=2003}}</ref><ref>{{cite web|url=http://mlg.eng.cam.ac.uk/zoubin/approx.html|title=Approximate Inference|accessdate=2013-07-15}}</ref>

==See also==
*[[Statistical inference]]
*[[fuzzy logic]]
*[[data mining]]

==References==
{{reflist}}

==External links==
*{{cite web|url=http://videolectures.net/mlss09uk_minka_ai/|title=Machine Learning Summer School (MLSS), Cambridge 2009, Approximate Inference|author= Tom Minka, Microsoft Research|date=Nov 2, 2009|type=video lecture}}

[[Category:Data management]]
<=====doc_Id=====>:265
<=====title=====>:
System of record
<=====text=====>:
A '''system of record''' (SOR) or '''Source System of Record''' (SSoR) is a [[Data Management|data management]] term for an [[information]] storage system (commonly implemented on a [[computer system]]) that is the authoritative data source for a given [[data element]] or piece of information. The need to identify systems of record can become acute in organizations where [[management information system]]s have been built by taking output data from multiple source systems, re-processing this data, and then re-presenting the result for a new business use.

In these cases, multiple information systems may disagree about the same piece of information. These disagreements may stem from semantic differences, differences in opinion, use of different sources, differences in the timing of the [[Extract, transform, load|ETL]] extracts that create the data they report against, or may simply be the result of bugs. 

The [[integrity]] and [[validity]] of any data set is open to question when there is no [[tracing (software)|traceable]] connection to a good source, such as a known System of Record. Where the integrity of the data is vital, if there is an agreed system of record, the data element must either be linked to, or extracted directly from it. In other cases, the provenance and estimated data quality should be documented. 

The "system of record" approach is a good fit for environments where both:
* there is a single authority over all data consumers, and 
* all consumers have similar needs 
In diverse environments, one instead needs to support the presence of multiple opinions. Consumers may accept different authorities or may differ on what constitutes an authoritative source -- researchers may prefer carefully vetted data, while tactical military systems may require the most recent credible report.

==See also==
* [[Single Source of Truth]] practice of using one source for a particular data element
* [[Privacy Act of 1974]] United States law including requirement for agencies to publish System Of Records Notices (SORN) in the [[Federal Register]] to identify the system and describe the use of individuals data.
* [[Master Data Management]] defining the handling of master data
* [[Systems of Engagement]] — more decentralized systems that incorporate technologies which encourage peer interactions

==References==
* {{cite web |title=The System of Record in the Global Data Warehouse | url=http://www.information-management.com/issues/20030501/6645-1.html  |publisher = Information Management |accessdate=2007-12-18 |author=[[Bill Inmon]]  |date=May 2003 |work= }}
* {{cite web 
| title = The Golden Copy 
| url = http://adam.goucher.ca/?p=72
| last = Goucher
| first = Adam
| date = 2006-04-26
| accessdate = 2013-04-30}}
* {{cite web
| title = The Move from Systems of Record to Systems Of Engagement
| url = http://www.forbes.com/sites/joshbersin/2012/08/16/the-move-from-systems-of-record-to-systems-of-engagement/
| last = Bersin
| first = Josh
| date = 2012-08-16
| accessdate = 2013-04-30}}


[[Category:Information systems]]
[[Category:Data management]]


{{compu-stub}}
<=====doc_Id=====>:268
<=====title=====>:
Address space
<=====text=====>:
{{About|a concept used universally in computing|addressing specifically the main memory|Memory address}}
{{refimprove|date=December 2011}}

In [[computing]], an '''address space''' defines a range of discrete addresses, each of which may correspond to a [[network host]], [[peripheral device]], [[disk sector]], a [[computer data storage|memory]] cell or other logical or physical entity.

For software programs to save and retrieve stored data, each unit of data must have an address where it can be individually located or else the program will be unable to find and manipulate the data. The number of address spaces available will depend on the underlying address structure and these will usually be limited by the computer architecture being used.

Address spaces are created by combining enough uniquely identified qualifiers to make an address unambiguous (within a particular address space). For a person's physical address, the ''address space'' would be a combination of locations, such as a neighborhood, town, city, or country. Some elements of an address space may be the same– but if any element in the address is different than addresses in said space will reference different entities. An example could be that there are multiple buildings at the same address of "32 Main Street" but in different towns, demonstrating that different towns have different, although similarly arranged, [[street address]] spaces.

An address space usually provides (or allows) a partitioning to several regions according to the [[mathematical structure]] it has. In the case of [[total order]], as for [[memory address]]es, these are simply [[interval (mathematics)|chunks]]. Some nested domains hierarchy appears in the case of [[arborescence (graph theory)|directed ordered tree]] as for the [[Domain Name System]] or a [[directory structure]]; this is similar to the hierarchical design of [[postal address]]es. In the [[Internet]], for example, the [[Internet Assigned Numbers Authority]] (IANA) allocates ranges of [[IP address]]es to various registries in order to enable them to each manage their parts of the global Internet address space.<ref>{{cite web|url=http://www.iana.org/assignments/ipv4-address-space/ |title= IPv4 Address Space Registry |date=March 11, 2009 |publisher=Internet Assigned Numbers Authority (IANA) |accessdate= September 1, 2011}}</ref>

==Examples==
Uses of addresses include, but are not limited to the following:
* [[Memory address]]es for [[main memory]], [[memory-mapped I/O]], as well as for [[virtual memory]];
* Device addresses on an [[expansion bus]];
* [[disk sector|Sector]] addressing for [[disk drive]]s;
* [[File name]]s on a particular [[volume (computing)|volume]];
* Various kinds of network host addresses in [[computer network]]s;
* [[Uniform resource locator]]s in the Internet.

== Address mapping and translation ==
[[File:CNFTL9.JPG|thumb|Illustration of translation from logical block addressing to physical geometry]]
Another common feature of address spaces are [[map (mathematics)|mappings and translations]], often forming numerous layers. This usually means that some higher-level address must be translated to lower-level ones in some way. 
For example, [[file system]] on a [[logical disk]] operates [[one-dimensional array|linear]] sector numbers, which have to be translated to ''absolute'' [[Logical block addressing|LBA]] sector addresses, in simple cases, via [[addition]] of the partition's first sector address. Then, for a disk drive connected via [[Parallel ATA]], each of them must be converted to ''logical'' (means fake) [[cylinder-head-sector]] address due to the interface historical shortcomings. It is converted back to LBA by the disk [[controller (computing)|controller]] and then, finally, to ''physical'' [[cylinder (disk drive)|cylinder]], [[disk head|head]] and [[track (disk drive)|sector]] numbers.

The [[Domain Name System]] maps its names to (and from) network-specific addresses (usually IP addresses), which in turn may be mapped to [[link layer]] network addresses via [[Address Resolution Protocol]]. Also, [[network address translation]] may occur on the edge of ''different'' IP spaces, such as a [[local area network]] and the Internet.

[[File:Virtual address space and physical address space relationship.svg|thumb|Virtual address space and physical address space relationship]]
An iconic example of virtual-to-physical address translation is [[virtual memory]], where different [[Page (computer memory)|pages]] of [[virtual address space]] map either to [[paging|page file]] or to main memory [[physical address]] space. It is possible that several numerically different virtual addresses all refer to one physical address and hence to the same physical byte of [[Random access memory|RAM]]. It is also possible that a single virtual address maps to zero, one, [[CPU cache#Homonym and synonym problems|or more than one]] physical address.

== See also ==
* [[Linear address space]]
* [[Name space]]
* [[Virtualization]]

== References ==
{{Reflist}}

[[Category:Computing terminology]]
[[Category:Data management]]
[[Category:Computer architecture]]
<=====doc_Id=====>:271
<=====title=====>:
Content repository
<=====text=====>:
A '''content  repository''' or '''content store''' is a database of digital content with an associated set of data management, search and access methods allowing application-independent access to the content, rather like a digital library, but with the ability to store and modify content in addition to searching and retrieving. The content repository acts as the storage engine for a larger application such as a [[Content Management System]] or a [[Document Management System]], which adds a [[user interface]] on top of the repository's [[application programming interface]].<ref>[http://openacs.org/doc/acs-content-repository/design.html Content Repository Design], [http://openacs.org/doc/acs-content-repository/ ACS Content Repository], [http://openacs.org/ OpenACS.org].</ref>

==Advantages provided by repositories==

*Common rules for data access allow many applications to work with the same content without interrupting the data.
*They give out signals when changes happen, letting other applications using the repository know that something has been modified, which enables collaborative data management.
*Developers can deal with data using programs that are more compatible with the desktop programming environment.
*The data model is scriptable when users use a content repository.

== Content repository features ==
A content repository may provide functionality such as:
* Add/edit/delete content
* Hierarchy and sort order management
* Query / search
* Versioning
* Access control
* Import / export
* Locking
* Life-cycle management
* Retention and holding / records management

== Examples ==

* [[Apache Jackrabbit]]
* ModeShape

==Applications==
*[[Content management]]
*[[Document management system|Document management]]
*[[Digital asset management]]
*[[Records management]]
*[[Revision control]]
*[[Social collaboration]]
*[[Web content management system|Web content management]]

== Standards and specification ==
*[[Content repository API for Java]]
*[[WebDAV]]
*[[Content Management Interoperability Services]]

== See also ==
* [[Information repository]]
* [[Content (media)]]

== References ==
{{reflist}}

==External links==
* [http://db-engines.com/en/ranking/content+store DB-Engines Ranking of Content Stores] by popularity, updated monthly

[[Category:Data management]]
[[Category:Content management systems]]
<=====doc_Id=====>:274
<=====title=====>:
ISO/IEC JTC 1/SC 32
<=====text=====>:
'''ISO/IEC JTC 1/SC 32 Data management and interchange''' is a [[standardization]] subcommittee of the Joint Technical Committee [[ISO/IEC JTC1|ISO/IEC JTC 1]] of the [[International Organization for Standardization]] (ISO) and the [[International Electrotechnical Commission]] (IEC), which develops and facilitates standards within the field of data management and interchange. The international [[Secretariat (administrative office)|secretariat]] of ISO/IEC JTC 1/SC 32 is the [[American National Standards Institute]] (ANSI) located in the United States.<ref name=countries>{{cite web| title=ISO/IEC JTC 1/SC 32 - Data management and interchange| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee_participation.htm?commid=45342| author=ISO| accessdate=2013-10-03}}</ref>

==History==
ISO/IEC JTC 1/SC 32 was formed in 1997, as a combination of the following three ISO/IEC JTC 1 subgroups: ISO/IEC JTC 1/SC 21/WG 3, Database; ISO/IEC JTC 1/SC 14, Data elements; and ISO/IEC JTC 1/SC 30, Open-edi. The new subcommittee was established with the intention of developing, and facilitating the development of, standards for data management within local and distributed information system environments.<ref name=briefing>{{cite news| title=Information technology: ISO/IEC JTC 1/SC 32, Data Management and Interchange| type=Briefings| author1=Cannan, Stephen| author2=Melton, Jim| journal=ISO Bulletin| date=January 2000| url=http://jtc1sc32.org/doc/N0601-0650/32N0607.pdf| pages=3–4| volume=31| issue=1}}</ref> ISO/IEC JTC 1/SC 32 was originally made up of five working groups (WGs), though ISO/IEC JTC 1/SC 32/WG 5, Database access and interchange, was disbanded in March 2002.<ref>{{cite report| type=Business Plan Draft| title=Draft Business Plan for ISO/IEC JTC 1/SC32, Data Management and Interchange| author=Mann, Douglas| accessdate=2013-10-04| url=http://jtc1sc32.org/doc/N0751-0800/32N0783.pdf| date=2002-04-04|page=4}}</ref> The four other original working groups of the subcommittee are currently active, although the title of ISO/IEC JTC 1/SC 32/WG 1 was changed from Open-edi to its current title, e-Business.<ref name=briefing/>

==Scope==
The scope of ISO/IEC JTC 1/SC 32 is “Standards for data management within and among local and distributed information systems environments. SC 32 provides enabling technologies to promote harmonization of data management facilities across sector-specific areas. Specifically, SC32 standards include:”<ref name=business2012>{{cite report| type=Business Plan| url=http://jtc1info.org/wp-content/uploads/2013/03/SC-32-Business-Plan-2012.pdf| accessdate=2013-10-03| author=Melton, Jim| date=2012-10-02| title=Business Plan for JTC1/SC32: 2012-2013}}</ref>
* Reference models and frameworks for the coordination of existing and emerging standards
* Definition of data domains, data types, and data structures, and their associated semantics
* Languages, services, and protocols for persistent storage, concurrent access and concurrent update, and interchange of data
* Methods, languages, services, and protocols to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce

==Structure==
ISO/IEC JTC 1/SC 32 is made up of four active working groups, each of which carries out specific tasks in standards development within the field of data management and interchange. As a response to changing standardization needs, working groups of ISO/IEC JTC 1/SC 32 can be disbanded if their area of work is no longer applicable, or established if new working areas arise. The focus of each working group is described in the group’s terms of reference. Active working groups of ISO/IEC JTC 1/SC 32 are:<ref name=business2012/><ref>{{cite web| title=ISO/IEC JTC 1/SC 32 Data management and interchange| author=ISO| accessdate=2013-10-03| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342}}</ref>
{| class="wikitable" width="60%"
! width="20%" | Working Group
! width="40%" | Working Area
|-
| ISO/IEC JTC 1/SC 32/WG 1 || [[Electronic business|e-Business]]
|-
|ISO/IEC JTC 1/SC 32/WG 2	|| [[Metadata]]
|-
|ISO/IEC JTC 1/SC 32/WG 3	|| [[Database#Database languages|Database languages]]
|-
|ISO/IEC JTC 1/SC 32/WG 4 || [[SQL]] Multimedia and application packages
|-
|}

==Collaborations==
ISO/IEC JTC 1/SC 32 works in close collaboration with a number of other organizations or subcommittees, both internal and external to ISO or IEC, in order to avoid conflicting or duplicative work. Organizations internal to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:<ref name=business2012/><ref>{{cite web| url=http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342| title=ISO/IEC JTC 1/SC 32 Data management and interchange| accessdate=2013-10-03| author=ISO}}</ref><ref>{{cite web| title=SC32 Liaison Organizations| accessdate=2013-10-03| author=ISO/IEC JTC 1/SC 32| url=http://jtc1sc32.org/}}</ref>
* [[ISO/IEC JTC 1/SC 7]], Software and systems engineering
* [[ISO/IEC JTC 1/SC 25]], Interconnection of information technology equipment
* [[ISO/IEC JTC 1/SC 38]], Cloud Computing and Distributed Platforms
* ISO/TC 12, Quantities and units
* [[ISO/TC 37]], Terminology and other language and content resources 
* ISO/TC 37/SC 2, Terminographical and lexicographical working methods
* ISO/TC 37/SC 3, Systems to manage terminology, knowledge and content
* ISO/TC 37/SC 4, Language resource management
* ISO/TC 46/SC 4, Technical interoperability
* ISO/TC 46/SC 11, Archives/records management
* ISO/TC 68/SC 2, Financial Services, security
* ISO/TC 127, Earth-moving machinery
* ISO/TC 154, Processes, data elements and documents in commerce, industry and administration
* ISO/TC 184, Automation systems and integration
* [[ISO TC 184/SC 4|ISO/TC 184/SC 4]], Industrial data
* ISO/TC 204, Intelligent transport systems
* [[ISO/TC 211]], Geographic information/Geomatics
* [[ISO/TC 215]], Health informatics
* ISO/TC 232, Learning services outside formal education

Some organizations external to ISO or IEC that collaborate with or are in liaison to ISO/IEC JTC 1/SC 32 include:
* [[Confédération Internationale des Sociétés d´Auteurs et Compositeurs|International Confederation of Societies of Authors and Composers]] (CISAC)
* [[Dublin Core Metadata Initiative]] (DCMI)
* [[EUROSTAT]]
* [[International Telecommunications Satellite Organization]] (ITSO)
* [[International Telecommunication Union|ITU]]
* [[Infoterm]]
* [[Object Management Group]] (OMG)
* [[Society for Worldwide Interbank Financial Telecommunication]] (SWIFT)
* [[UN/CEFACT]]
* [[United Nations Economic Commission for Europe]] (UNECE)
* [[World Meteorological Organization]] (WMO)
* [[W3C]]

==Member countries==
Countries pay a fee to ISO to be members of subcommittees.<ref>{{cite manual| url=http://www.iso.org/iso/iso_membership_manual_2012.pdf| pages=-18| chapter=III. What Help Can I Get from the ISO Central Secretariat?| title=ISO Membership Manual| author=ISO| date=June 2012| accessdate=2013-07-12| publisher=ISO}}</ref>

The 14 "P" (participating) members of ISO/IEC JTC 1/SC 32 are: Canada, China, Czech Republic, Côte d'Ivoire, Egypt, Finland, Germany, India, Japan, Republic of Korea, Portugal, Russian Federation, United Kingdom, and United States.<ref name=countries/>

The 22 "O" (observing) members of ISO/IEC JTC 1/SC 32 are: Australia, Austria, Belgium, Bosnia and Herzegovina, France, Ghana, Hungary, Iceland, Indonesia, Islamic Republic of Iran, Ireland, Italy, Kazakhstan, Luxembourg, Netherlands, Norway, Poland, Romania, Serbia, Spain, Switzerland, and Turkey.

==Published standards==
ISO/IEC JTC 1/SC 32 standards are meant to structure, organize, and register metadata and other information resources associated with sharing and interoperability, including electronic commerce.<ref name=briefing/> ISO/IEC JTC 1/SC 32 currently has 74 published standards within the field of data management and interchange, including:<ref>{{cite web| title=ISO/IEC JTC 1/SC 32| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_tc_browse.htm?commid=45342&published=on| accessdate=2013-10-03| author=ISO}}</ref><ref>{{cite web| publisher=ISO| url=http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html| title=Freely Available Standards| accessdate=2013-09-26}}</ref>
{| class="wikitable sortable" width="100%"
! data-sort-type="number" width="14%" | ISO/IEC Standard
! width="29%" | Title
! width="6%" | Status
! width="49%" | Description
! width= "2%" | WG
|-
|data-sort-value="14662"|ISO/IEC 14662 [http://standards.iso.org/ittf/licence.html free] || Information technology – Open-edi reference model || Published (2010) || Specifies the framework for coordinating the integration of existing International Standards and the development of future International Standards for the interworking of Open-edi parties through Open-edi<ref>{{cite journal| title=ISO/IEC 14662| date=2010-02-15| author=ISO| edition=3| page=1}}</ref><ref>{{cite web| title=ISO/IEC 14662:2010| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55290| date=2010-02-02| author=ISO}}</ref> || 1
|-
|data-sort-value="15944"|ISO/IEC 15944-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Business Operational View – Part 1: Operational aspects of Open-edi for implementation || Published (2011) || Allows constraints, including legal requirements, commercial and/or international trade and contract terms, public policy, and laws and regulations, to be defined and integrated into Open-edi through the business operational view (BOV)<ref>{{cite journal| title=ISO/IEC 15944-1| date=2011-08-01| author=ISO| edition=2| page=1}}</ref><ref>{{cite web| title=ISO/IEC 15944-1:2011| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=55289| date=2011-07-21| author=ISO}}</ref> || 1
|-
|data-sort-value="11179"|[[ISO/IEC 11179]]-3 [http://standards.iso.org/ittf/licence.html free] || Information technology – [[metadata registry|Metadata registries]] (MDR) – Part 3: Registry metamodel and basic attributes || Published (2013) || Specifies the structure of a metadata registry in the form of a conceptual data model and specifies basic attributes which are required to describe metadata items<ref>{{cite journal| page=1| date=2003-02-15| title=ISO/IEC 11179-3| author=ISO| edition=2}}</ref><ref>{{cite web| title=ISO/IEC 11179-3:2013| accessdate=2013-10-03| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=50340| date=2013-02-12| author=ISO}}</ref> || 2
|-
|data-sort-value="20943"| ISO/IEC TR 20943-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Procedures for achieving metadata registry content consistency – Part 1: Data elements || Published (2003) || “Describes a set of procedures for the consistent registration of data elements and their attributes in a registry.”<ref>{{cite journal| title=ISO/IEC TR 20943-1| author=ISO| page=1| date=2003-08-01| edition=1}}</ref><ref>{{cite web| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=34343| date=2012-12-19| title=ISO/IEC TR 20943-1:2003| author=ISO}}</ref> || 2
|-
|data-sort-value="20944"| ISO/IEC 20944-1 || Information technology – Metadata Registries Interoperability and Bindings (MDR-IB) – Part 1: Framework, common vocabulary, and common provisions for conformance || Published (2013) || Contains the overview, framework, common vocabulary, and common provisions for conformance for the ISO/IEC 20944 series, which provides the bindings and their interoperability for MDRs<ref>{{cite web| author=ISO| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=51914| date=2013-01-08| accessdate=2013-10-04| title=ISO/IEC 20944-1:2013}}</ref> || 2
|-
|data-sort-value="19502"|ISO/IEC 19502 || Information technology – [[Meta-Object Facility|Meta Object Facility (MOF)]] || Published (2005) || Defines a metamodel using MOF, and a set of interfaces using Open Distributed Processing (ODP) that can be used to define and manipulate a set of interoperable metamodels and their corresponding models<ref>{{cite web| title=ISO/IEC 19502:2005| accessdate=2013-10-04| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32621| date=2011-03-17| author=ISO}}</ref> || 2
|-
|data-sort-value="19773"|ISO/IEC 19773 || Information technology – Metadata Registries (MDR) modules || Published (2011) || Specifies small modules of data to be used or reused in applications<ref>{{cite web| url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=41769| date=2011-09-01| accessdate=2013-10-04| author=ISO| title=ISO/IEC 19773:2011}}</ref> || 2
|-
|data-sort-value="09075"|ISO/IEC 9075-1 [http://standards.iso.org/ittf/licence.html free] || Information technology – Database languages – [[SQL#Standardization|SQL]] – Part 1: Framework (SQL/Framework) || Published (2011) || Defines the conceptual framework to specify the grammar of SQL and the result of processing statements in that language by an SQL-implementation<ref>{{cite journal| title=ISO/IEC 9075-1| page=1| date=2008-07-15| edition=3| author=ISO}}</ref><ref>{{cite web| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53681| date=2013-02-04| accessdate=2013-10-04| author=ISO| title=ISO/IEC 9075-1:2011}}</ref> || 3
|-
|data-sort-value="13249"|ISO/IEC 13249-3 || Information technology – Database languages – SQL multimedia and application packages – Part 3: Spatial || Published (2011) || “Defines spatial user-defined types, routines, and schemas for generic spatial data handling.”<ref>{{cite web| date=2011-08-22| url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=53698| accessdate=2013-10-04| author=ISO| title=ISO/IEC 13249-3:2011}}</ref> || 4
|-
|}

==See also==
* [[ISO/IEC JTC1]]
* [[List of International Organization for Standardization standards|List of ISO standards]]
* [[American National Standards Institute]]
* [[International Organization for Standardization]]
* [[International Electrotechnical Commission]]

==References==
{{Reflist|2}}

==External links==
* [http://www.iso.org/iso/home/standards_development/list_of_iso_technical_committees/iso_technical_committee.htm?commid=45342 ISO/IEC JTC 1/SC 32 page at ISO]

{{DEFAULTSORT:ISO IEC JTC1 SC32}}
[[Category:ISO/IEC JTC1 subcommittees|#032]]
[[Category:Standards organizations]]
[[Category:Data management]]
[[Category:Data interchange standards]]
<=====doc_Id=====>:277
<=====title=====>:
Datafication
<=====text=====>:
'''Datafication''' is a modern technological trend turning many aspects of our life into computerised data <ref name="CukierMayer-Schoenberger2013">{{cite journal | last = Cukier | first =Kenneth | last2 = Mayer-Schoenberger | first2 = Viktor  | title =The Rise of Big Data | journal =Foreign Affairs | issue =May/June | pages = 28–40. | date =2013  | url = http://www.foreignaffairs.com/articles/139104/kenneth-neil-cukier-and-viktor-mayer-schoenberger/the-rise-of-big-data | accessdate = 24 January 2014}}</ref> and transforming this information into new forms of value. 
<ref name="SchuttOneil2014">
{{cite book
 | last = O'Neil | first =Cathy
 | last2 =Schutt
 | first2 = Rachel
| title =Doing Data Science
 | publisher =O’Reilly Media
 | date =2013
 | pages =406
  | isbn =978-1-4493-5865-5
 }}
</ref>
Examples of datafication as applied to social and communication media are how [[Twitter]] datafies stray thoughts or datafication of [[Human resource management|HR]] by [[LinkedIn]] and others.  Alternative examples are diverse and include aspects of the built environment, and design via engineering and or other tools that tie data to formal, functional or other physical media outcomes of which [[Formsolver]]<ref>https://www.formsolver.com</ref> is an example.

[[File:Shape optimization for buildings by formsolver.jpg|thumbnail|right|Example: Datafication of the skin and form of a building to assist engineers, designers and architects determine the performance of particular building geometries. Example provided courtesy of Formsolver.com]]
[[File:Emerging Shape Optimization Families for buildings by formsolver.jpg|thumbnail|right|Example: Shape families resulting from differing goals when data is used for the purposes of shape optimization. Example provided courtesy of Formsolver.com]]

==See also==
* [[Big data]]

==References==
{{Reflist}}

[[Category:Information science]]
[[Category:Technology forecasting]]
[[Category:Data management]]
[[Category:Big data]]
[[Category:Information society]]


{{Tech-stub}}
<=====doc_Id=====>:280
<=====title=====>:
Vinelink.com
<=====text=====>:
'''''Vinelink.com''''' (VINE) is a national website in the [[United States]] that allows victims of crime, and the general public, to track the movements of prisoners held by the various [[US states|states]] and [[Territories of the United States|territories]]. The first four letters in the websites name, "vine", are an acronym for "Victim Information and Notification Everyday". Vinelink.com displays information, based on the information provided by the various states' departments of correction and other law enforcement agencies, on whether an inmate is in custody, has been released, has been granted parole or probation, or has escaped from custody. In some cases, the website will reveal whether a defendant has been granted parole or probation, but then subsequently violated conditions of their release and become a fugitive.<ref>{{cite web | url=http://www.correct.state.ak.us/probation-parole/vine|title=Automated Victim Notification System (VINE)|publisher=[[Alaska Department of Corrections]]|accessdate=2014-05-20}}</ref> Information provided on Vinelink.com represents [[metadata]], in that the website lists a defendants custody status; but does not list what the individual is charged with, their criminal history, or the amount of their bail, if applicable.

[[Internet]] users accessing the Vinelink.com website choose from a map of states and provinces within the United States where they wish to perform a search for an inmate. The user may then search for an individual using the inmate or parolees name, or by entering the inmates specific department of corrections inmate number, if known. When the inmates custody status changes, users who have registered to be notified of such changes will be notified via email, phone or both.<ref>{{cite web | url=http://www.doj.state.or.us/victims/pages/vine.aspx|title=Victim Information and Notification Everyday (VINE)|publisher=[[Oregon Department of Justice]]|date=2013-02-03| accessdate=2014-05-20}}</ref> This information is currently released upon request, without the website requesting reasons for the users search or requiring payment, as [[public records]] available to the [[general public]].

Inmate information is available for most states, and for [[Puerto Rico]], on the website. The states of [[Arizona]], [[Georgia (U.S. state)|Georgia]], [[Massachusetts]], [[Montana]], [[New Hampshire]] and [[West Virginia]] provide very limited information on the site. The states of [[Kansas]], [[Maine]] and [[South Dakota]] do not participate in the VINE system.<ref>{{cite web | url=http://www.theledger.com/article/20130203/news/130209793| title=Who to Call: VINElink|publisher=[[The Ledger]]|date=2010| accessdate=2014-05-20}}</ref> The website does not provide data on prisoners detained by the [[United States federal government]].

==References==
==External links==
* [http://www.vinelink.com official website] 

==See also==
Visit [http://www.bop.gov/inmateloc/ the official Federal Bureau of Prisons website] to search for inmates being held by the [[United States Federal Bureau of Prisons]]

[[Category:Data management]]
[[Category:Government services web portals in the United States]]
[[Category:Law enforcement websites]]
[[Category:Metadata]]
[[Category:Public records]]


{{website-stub}}
{{Crime-stub}}
<=====doc_Id=====>:283
<=====title=====>:
Data security
<=====text=====>:
{{Refimprove|date=February 2012}}
'''Data security''' means protecting data, such as a database, from destructive forces and from the unwanted actions of unauthorized users.<ref>Summers, G. (2004). Data and databases.  In: Koehne, H Developing Databases with Access: Nelson Australia Pty Limited. p4-5.</ref>

== Data security technologies ==

=== Disk encryption ===

[[Disk encryption]] refers to encryption technology that encrypts data on a [[hard disk drive]]. Disk encryption typically takes form in either software (see [[disk encryption software]]) or hardware (see [[disk encryption hardware]]). Disk encryption is often referred to as [[on-the-fly encryption]] (OTFE) or transparent encryption.

=== Software versus hardware-based mechanisms for protecting data ===

Software-based security solutions encrypt the data to protect it from theft. However, a malicious program or a hacker could corrupt the data in order to make it unrecoverable, making the system unusable. Hardware-based security solutions can prevent read and write access to data and hence offer very strong protection against tampering and unauthorized access.

Hardware based security or assisted [[computer security]] offers an alternative to software-only computer security. [[Security token]]s such as those using [[PKCS#11]] may be more secure due to the physical access required in order to be compromised. Access is enabled only when the token is connected and correct [[Personal identification number|PIN]] is entered (see [[two-factor authentication]]). However, dongles can be used by anyone who can gain physical access to it. Newer technologies in hardware-based security solves this problem offering full proof security for data.

Working of hardware-based security: A hardware device allows a user to log in, log out and set different privilege levels by doing manual actions. The device uses biometric technology to prevent malicious users from logging in, logging out, and changing privilege levels. The current state of a user of the device is read by controllers in [[peripheral devices]] such as hard disks. Illegal access by a malicious user or a malicious program is interrupted based on the current state of a user by hard disk and DVD controllers making illegal access to data impossible. Hardware-based access control is more secure than protection provided by the operating systems as operating systems are vulnerable to malicious attacks by [[Computer virus|viruses]] and hackers. The data on hard disks can be corrupted after a malicious access is obtained. With hardware-based protection, software cannot manipulate the user privilege levels. It is impossible for a [[Hacker (computer security)|hacker]] or a malicious program to gain access to secure data protected by hardware or perform unauthorized privileged operations. This assumption is broken only if the hardware itself is malicious or contains a backdoor.<ref>{{Citation| last1 = Waksman  | first1 = Adam | last2 = Sethumadhavan | first2 = Simha | title = Silencing Hardware Backdoors | volume = | pages =  | periodical = Proceedings of the IEEE Symposium on Security and Privacy | location = Oakland, California  | url = http://www.cs.columbia.edu/~simha/preprint_oakland11.pdf | year = 2011  | issn =  | doi =  | isbn = }}</ref> The hardware protects the operating system image and file system privileges from being tampered. Therefore, a completely secure system can be created using a combination of hardware-based security and secure system administration policies.

=== Backups ===

[[Backup]]s are used to ensure data which is lost can be recovered from another source. It is considered essential to keep a backup of any data in most industries and the process is recommended for any files of importance to a user.

===Data masking===
{{main|Data masking}}
[[Data masking]] of structured data is the process of obscuring (masking) specific data within a database table or cell to ensure that data security is maintained and sensitive information is not exposed to unauthorized personnel.<ref>{{cite web|title=What is Data Obfuscation|url=http://www.dataobfuscation.com.au|accessdate=1 March 2016}}</ref>  This may include masking the data from users (for example so banking customer representatives can only see the last 4 digits of a customers national identity number), developers (who need real production data to test new software releases but should not be able to see sensitive financial data), outsourcing vendors, etc.
<ref>{{Cite web
 |url = http://searchsecurity.techtarget.com/definition/data-masking
 |title = data masking
 |accessdate = 29 July 2016
}}</ref>

===Data erasure===
[[Data erasure]] is a method of software-based overwriting that completely destroys all electronic data residing on a hard drive or other digital media to ensure that no sensitive data is leaked when an asset is retired or reused...

== International laws and standards ==

=== International laws ===

In the [[United Kingdom|UK]], the [[Data Protection Act 1998|Data Protection Act]] is used to ensure that personal data is accessible to those whom it concerns, and provides redress to individuals if there are inaccuracies.<ref>{{Cite web
 |url = https://ico.org.uk/for-organisations/guide-to-data-protection/principle-1-fair-and-lawful/
 |title = data protection act
 |accessdate = 29 July 2016
}}</ref> This is particularly important to ensure individuals are treated fairly, for example for credit checking purposes. The Data Protection Act states that only individuals and companies with legitimate and lawful reasons can process personal information and cannot be shared. [[Data Privacy Day]] is an international [[holiday]] started by the [[Council of Europe]] that occurs every January 28.<ref name=dataprivacyday>{{cite web|url=http://googleblog.blogspot.com/2008/01/celebrating-data-privacy.html|title=Celebrating data privacy |author=[[Peter Fleischer]], [[Jane Horvath]], [[Shuman Ghosemajumder]]|publisher=[[Google Blog]] |accessdate=12 August 2011 |year=2008}}</ref>

=== International standards ===

The international standards ISO/IEC 27001:2013 and ISO/IEC 27002:2013 covers data security under the topic of [[information security]], and one of its cardinal principles is that all stored information, i.e. data, should be owned so that it is clear whose responsibility it is to protect and control access to that data.

The [[Trusted Computing Group]] is an organization that helps standardize computing security technologies.

The [[PCI DSS|Payment Card Industry Data Security Standard]] is a proprietary international information security standard for organizations that handle cardholder information for the major [[Debit card|debit]], [[Credit card|credit]], prepaid, [[e-purse]], [[Cash machine|ATM]] and POS cards.<ref>{{cite web|title=PCI DSS Definition|url=http://www.pcmag.com/encyclopedia/term/59104/pci-dss|accessdate=1 March 2016}}</ref>

== Industry and software ==
There are several data security software available to be used by consumers and one of the most used data security software with a U.S issued patent is [[Folder lock|Folder Lock]].

==See also==
* [[Copy Protection]]
* [[Data-centric security]]
* [[Data erasure]]
* [[Data masking]]
* [[Data recovery]]
* [[Digital inheritance]]
* [[Disk encryption]]
** [[Comparison of disk encryption software]]
* [[Identity Based Security]]
* [[Information security]]
* [[IT network assurance]]
* [[Pre-boot authentication]]
* [[Privacy engineering]]
* [[Secure USB drive]]
* [[Security Breach Notification Laws]]
* [[Single sign-on]]
* [[Smart card]]
* [[Trusted Computing Group]]

== Notes and references ==
{{reflist}}

==External links==
{{Commons category}}

{{Data}}
{{Privacy}}
{{Portal bar|Computer security|Information technology}}

[[Category:Data security| ]]
[[Category:Data management]]
<=====doc_Id=====>:286
<=====title=====>:
Query Rewriting
<=====text=====>:
{{Orphan|date=August 2014}}

'''Query Rewriting''' is a technique used in mediation based [[data integration]] systems for translating the queries formulated over the mediated schema to a query over the various sources by making use of the view definitions.<ref name="refone">{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270–294}}</ref> Mediation based data integration system hides from the end user the underlying heterogeneity of the various data providing sources linked to it by providing a uniform query interface in the form of a mediated schema. This schema is also referred to as the global schema whereas the schema of the various data sources is collectively referred to as the local schema. The local schema and the mediated schema are mapped to each other using view definitions. The queries formulated on the mediated schema cannot be directly used to query the sources. Therefore query rewriting translates such a query formulated over the global schema to a query over the various data sources. Examples include bucket algorithm, Minicon algorithm, inverse rules algorithm.<ref name="refone"/> This rewritten query is then evaluated to obtain the query response making use of the data obtained by querying the data sources.

==See also==
* [[Data integration]]
* [[Schema Matching]]
* [[Data Virtualization]]

==References==
<references/>

{{DEFAULTSORT:Query Rewriting}}
[[Category:Data management]]
<=====doc_Id=====>:289
<=====title=====>:
Government Performance Management
<=====text=====>:
{{Refimprove|date=October 2014}}
'''Government Performance Management''' (GPM) consists of a set of processes that help government organizations optimize their business performance. It provides a framework for organizing, automating, and analyzing business methodologies, metrics, processes and systems that drive business performance.<ref>{{cite web|url=http://www.information-management.com/bissues/20070301/2600312-1.html|title=Performance Management for Government|author=Michael Owellen|date=28 February 2007|work=BI Review Magazine|accessdate=22 October 2014}}</ref> Some commentators{{who|date=October 2014}} see GPM as the next generation of [[business intelligence]] (BI) for governments. GPM helps governments to make use of their financial, human, material, and other resources.  In the past, owners have sought to drive strategy down and across their organizations; they have struggled to transform strategies into actionable metrics and they have grappled with meaningful analysis to expose the cause-and-effect relationships that, if understood, could give profitable insight to their operational decision-makers.  GPM software and methods allow a systematic, integrated approach that links government strategy to core processes and activities. "Running by the numbers" now means something: planning, budgeting, analysis, and reporting can give the measurements that empower management decisions.<ref>{{cite web|url=http://www.encyclopedia.com/doc/1O12-performancemanagement.html |accessdate=February 7, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20090112223214/http://www.encyclopedia.com/doc/1O12-performancemanagement.html |archivedate=January 12, 2009 }}</ref>

== Performance Management (PM) Market ==
According to [[Gartner]]{{citation needed|date=October 2014}}, the Enterprise Performance Management (EPM) suite market continues to experience strong momentum, growing 19% during 2007. This is slightly in advance of their earlier market sizing and forecast analysis, which anticipated 2007 revenue to be $1.836 million, representing an 18% year-over-year growth. In the latest forecast, Gartner believe that the market for EPM will be more than $3 billion by 2011, representing a 14.4% compound annual growth rate. Several factors contributed to the continued significant growth in EPM revenue during 2007:
* Many organizations replaced difficult-to-maintain, inflexible, or outmoded spreadsheets and homegrown financial applications.
* Continued growth in large enterprises was fueled by desires to achieve greater transparency and adherence to governance and compliance legislation.
* Increased demand for applications that support strategic plans and operational activities drove new momentum in the deployment of scorecards.
* There was increased demand from mid-size enterprises, representing one of the largest untapped and dynamic areas of the business application software sector.
* Advertising and PR from increasingly large vendors and system integrators are raising the EPM profile and generating greater demand.

Gartner also expects the Business Intelligence software market to reach $3 billion in 2009. "Companies around the world have purchased more than US $40 billion worth of enterprise applications, including ERP, CRM and HR, during the past few years," said Colleen Graham, principal research analyst at Gartner. "This has generated significant volumes of data in support of the operational processes they automate. By investing in BI, companies can further leverage their enterprise application investments and turn the torrent of data into meaningful insight to better measure performance, respond more quickly to market changes and opportunities and comply with an increasingly complex regulatory environment."<ref>{{cite web|url=http://www.gartner.com/press_releases/asset_144782_11.html|title=Gartner News Room|publisher=Gartner.com|accessdate=22 October 2014}}</ref>

== ITWorx Government Performance Management (GPM) ==

[[ITWorx]] GPM is a bilingual, [[Microsoft]]-based framework that gives governments the capability to cascade, share, track, and update strategies and plans organization-wide. It creates detailed views of multi-source [[Key Performance Indicator]]s (KPIs) using customized [[balance scorecard]]s, dashboards, strategy maps, statistical charts, and reports, as well as provides ad hoc analytical and reporting tools.

== ITWorx GPM Features ==
{{Merge to|section=yes|ITWorx|date=October 2014}}
{{advert|date=October 2014}}
ITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> provides a top-down approach in recording government strategy. The strategy is cascaded and shared across government bodies to define objectives and balancing targets.

It also links strategy to execution. Operational plans are recorded, linked to strategies, assigned a time-range for implementation, broken down to initiatives and business activities, approved, and then propagated to all levels.

ITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> defines a time-range for implementing an initiative, its owners, cost drivers, budgets, KPIs, and targets; and links initiatives to strategic objectives. Business activities, contributing in strategy execution, are defined including their KPIs and targets. KPIs can be entered and configured manually, calculated using other KPIs, or extracted from external data sources.

ITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> enables the definition of government-specific business rules such as KPI calculation formulas; it also enables administration of system settings such as the configuration of the organization structure and definition of approval [[workflows]] for each organization unit. Furthermore, administrators can manage user roles and groups as well as archive plans and approvals.

ITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> calculates measurement formulas, compares actual values against targets, and performs analysis. Color-coded KPIs are represented through strategic and customized scorecards, dashboards, strategy maps, and statistical charts and graphs, in addition to ad hoc analytical and reporting tools.

The solution enables communication throughout the decision-making process by allowing users to post comments and discuss topics regarding a strategy, KPI, or report. Keeping a documented record of why and when decisions are made, ITWorx GPM retains the history of contributions.

ITWorx GPM,<ref>[http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx]  {{webarchive |url=https://web.archive.org/web/20110311185821/http://www.itworx.com/Solutions/MicrosoftGovernmentPerformance/Pages/default.aspx |date=March 11, 2011 }}</ref> provides a mechanism for policy-makers and strategy implementers to facilitate the strategic management process without compromising data. Government frontline officials are provided with a feedback channel to submit change requests and propositions to approved strategic plans, targets, and actual data while securing the validity and consistency of data.

Frontline officials can monitor performance though consolidated views while detailed views are provided for department and executive levels. Based on privileges, users can view rolled-up KPIs and drill-down for [[root cause analysis]] or corrective actions.

==References==
{{Reflist}}

==External links==
*[http://gpm.itworx.com Gpm.itworx.com]
*[http://www.microsoft.com/downloads/details.aspx?displaylang=en&FamilyID=efdc60d3-2622-44f5-aa5d-2b79d10c93ab  Microsoft.com]
*[http://www.pcmag-mideast.com/gitex/tag/itworx/ Pcmag-mideast.com]
*[http://www.itp.net/578068-itworx-releases-new-gpm-suite Itp.net]

[[Category:Business intelligence]]
[[Category:Data management]]
<=====doc_Id=====>:292
<=====title=====>:
Systems of Engagement
<=====text=====>:
The concept of '''Systems of Engagement''' is attributed to [[Geoffrey Moore]], a business author of such books as Crossing the Chasm.<ref>Crossing the Chasm: Marketing and Selling High-tech Products to Mainstream Customers (1991, revised 1999 and 2014) – ISBN 0-06-051712-3</ref> In his paper for AIIM.org entitled: "Systems of Engagement and the Future of Enterprise IT" Moore states:
“Amidst the texting and Twittering and Facebooking of a generation of digital natives, the fundamentals of next-generation communication and collaboration are being worked out. For them, it is clear, there is no going back. So at minimum, if you expect these folks to be your customers, your employees, and your citizens (and, frankly, where else could you look?), then you need to apply THEIR expectations to the next generation of enterprise IT systems....Systems of Engagement … will overlay and complement our deep investments in systems of record.”<ref>Moore, Geoffrey (2011). "Systems of Engagement and the Future of Enterprise IT". http://www.aiim.org/futurehistory</ref>{{cite news |last=Moore |first=Geoffrey |date=2011 |title=Systems of Engagement and the Future of Enterprise IT: A Sea Change in Enterprise IT |url=http://www.aiim.org/futurehistory |accessdate=October 7, 2014}}
Since then Systems of Engagement has been adopted by organizations such as [http://blogs.forrester.com/category/systems_of_engagement Forrester Research], [[Hewlett-Packard|HP]], [[IBM]], [[AIIM]], and [http://www.avoka.com/blog/2013/07/deliver-a-system-of-engagement/ Avoka]. Forrester defines Systems of Engagement as follows: "Systems of engagement are different from the traditional systems of record that log transactions and keep the financial accounting in order: They focus on people, not processes....These new systems harness a perfect storm of mobile, social, cloud, and big data innovation to deliver apps and smart products directly in the context of the daily lives and real-time workflows of customers, partners, and employees.”<ref>http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement</ref> {{cite news |last=Schadler |first=Ted |date=February 14, 2012 |title=A Billion Smartphones Require New Systems Of Engagement |url=http://blogs.forrester.com/ted_schadler/12-02-14-a_billion_smartphones_require_new_systems_of_engagement |accessdate=October 7, 2014 }}

==See also==
* [[System of record]] — conventional enterprise systems designed to contain the authoritative data source for a given piece of information.
==References==
{{Reflist}}


[[Category:Information systems]]
[[Category:Data management]]


{{compu-stub}}
<=====doc_Id=====>:295
<=====title=====>:
Category:Extract, transform, load tools
<=====text=====>:
'''Extract, transform, load tools''' are software packages that facilitate the performing of [[Extract, transform, load|ETL]] tasks.

[[Category:Data management]]
[[Category:Data warehousing products]]
<=====doc_Id=====>:298
<=====title=====>:
Relational data stream management system
<=====text=====>:
A '''relational data stream management system (RDSMS)''' is a distributed, in-memory [[data stream management system]] (DSMS) that is designed to use standards-compliant [[SQL]] queries to process unstructured and structured data streams in real-time. Unlike [[SQL]] queries executed in a traditional [[RDBMS]], which return a result and exit, SQL queries executed in a RDSMS do not exit, generating results continuously as new data become available. Continuous SQL queries in a RDSMS use the [[SQL]] Window function to analyze, join and aggregate data streams over fixed or sliding windows. Windows can be specified as time-based or row-based.

== RDSMS SQL Query Examples ==

Continuous [[SQL]] queries in a RDSMS conform to the [[ANSI]] [[SQL]] standards. The most common RDSMS SQL query is performed with the declarative <code>SELECT</code> statement. A continuous SQL <code>SELECT</code> operates on data across one or more data streams, with optional keywords and clauses that include <code>FROM</code> with an optional <code>JOIN</code> subclause to specify the rules for joining multiple data streams, the <code>WHERE</code> clause and comparison predicate to restrict the records returned by the query, <code>GROUP BY</code> to project streams with common values into a smaller set, <code>HAVING</code> to filter records resulting from a <code>GROUP BY</code>, and <code>ORDER BY</code> to sort the results.

The following is an example of a continuous data stream aggregation using a <code>SELECT</code> query that aggregates a sensor stream from a weather monitoring station. The <code>SELECT</code>query aggregates the minimum, maximum and average temperature values over a one-second time period, returning a continuous stream of aggregated results at one second intervals.
 
<source lang="sql">
SELECT STREAM
    FLOOR(WEATHERSTREAM.ROWTIME to SECOND) AS FLOOR_SECOND,
    MIN(TEMP) AS MIN_TEMP,
    MAX(TEMP) AS MAX_TEMP,
    AVG(TEMP) AS AVG_TEMP
FROM WEATHERSTREAM
GROUP BY FLOOR(WEATHERSTREAM.ROWTIME TO SECOND);
</source>

RDSMS SQL queries also operate on data streams over time or row-based windows. The following example shows a second continuous SQL query using the <code>WINDOW</code> clause with a one-second duration. The <code>WINDOW</code> clause changes the behavior of the query, to output a result for each new record as it arrives. Hence the output is a stream of incrementally updated results with zero result latency.

<source lang="sql">
SELECT STREAM
    ROWTIME,
    MIN(TEMP) OVER W1 AS WMIN_TEMP,
    MAX(TEMP) OVER W1 AS WMAX_TEMP,
    AVG(TEMP) OVER W1 AS WAVG_TEMP
FROM WEATHERSTREAM
WINDOW W1 AS ( RANGE INTERVAL '1' SECOND PRECEDING );
</source>

== See also ==
* [[SQL]]
* [[NoSQL]]
* [[NewSQL]]

== External links ==
* [http://www.sqlstream.com/stream-processing/ Stream processing with SQL]
* [http://researcher.watson.ibm.com/researcher/view_group.php?id=2531 IBM System S]
* [http://www.mcjones.org/System_R/SQL_Reunion_95/sqlr95.html ''1995 SQL Reunion: People, Projects, and Politics'', by Paul McJones (ed.)]: transcript of a reunion meeting devoted to the personal history of relational databases, SQL System R.

[[Category:Data management]]
[[Category:Relational model]]
<=====doc_Id=====>:301
<=====title=====>:
Data lineage
<=====text=====>:
{{peacock|date=May 2015}}

'''Data lineage''' is defined as a data life cycle that includes the data's origins and where it moves over time.<ref>http://www.techopedia.com/definition/28040/data-lineage</ref> It describes what happens to data as it goes through diverse processes. It helps provide visibility into the analytics pipeline and simplifies tracing errors back to their sources. It also enables replaying specific portions or inputs of the dataflow for step-wise debugging or regenerating lost output. In fact, database systems have used such information, called data provenance, to address similar validation and debugging challenges already.<ref name="DeSoumyarupa">De, Soumyarupa. (2012). Newt : an architecture for lineage based replay and debugging in DISC systems. UC San Diego: b7355202. Retrieved from: https://escholarship.org/uc/item/3170p7zn</ref>

'''Data Lineage''' provides a visual representation to discover the data flow/movement from its source to destination via various changes and hops on its way in the enterprise environment. 
''Data lineage'' represents: how the data hops between various data points, how the data gets transformed along the way, how the representation and parameters change, and how the data splits or converges after each hop. Easier representation of the ''Data Lineage'' can be shown with dots and lines, where dot represents a data container for data point(s) and lines connecting them represents the transformation(s) the data point under goes, between the data containers.

<!-- Deleted image removed: [[File:DataLineage Dots lines.png]] -->

Representation of ''Data Lineage'' broadly depends on scope of the ''[[Meta-data management|Metadata Management]]'' and reference point of interest. ''Data Lineage'' provides sources of the data and intermediate data flow hops from the reference point with '''Backward data lineage''', leads to the final destination's data points and its intermediate data flows with '''Forward data lineage'''.  These views can be combined with '''End to End Lineage''' for a reference point that provides complete audit trail of that data point of interest from source(s) to its final destination(s). As the data points or hops increases, the complexity of such representation becomes incomprehensible. Thus, the best feature of the data lineage view would be to be able to simplify the view by temporarily ''Masking'' unwanted peripheral data points. Tools that have the '''masking''' feature enables scalability of the view and enhances analysis with best user experience for both Technical and business users alike.

'''Scope of the data lineage''' determines the volume of metadata required to represent its data lineage. Usually, [[Data governance|Data Governance]], and [[Data management|Data Management]] determines the scope of the data lineage based on their [[regulation]]s, ''enterprise data management strategy'', ''data impact'', ''reporting attributes'', and ''critical [[data element]]s'' of the organization.

''Data Lineage'' provides the audit trail of the data points at the lowest granular level,but presentation of the lineage may be done at various zoom levels to simplify the vast information, similar to the ''analytic web maps''. ''Data Lineage'' can be visualized at various levels based on the granularity of the view. At a very high level ''data lineage'' provides what systems the data interacts before it reaches destination. As the granularity increases it goes up to the data point level where it can provide the details of the data point and its historical behavior, attribute properties, and trends and ''[[Data quality|Data Quality]]'' of the data passed through that specific data point in the ''data lineage''.

''[[Data governance|Data Governance]]'' plays a key role in metadata management for guidelines, strategies, policies, implementation. ''[[Data quality|Data Quality]]'', and ''[[Master data management|Master Data Management]]'' helps in enriching the data lineage with more business value. Even though the final representation of ''Data lineage'' is provided in one interface but the way the metadata is harvested and exposed to the data lineage '''[[Graphical user interface|User Interface (UI)]]''' could be entirely different. Thus, ''Data lineage'' can be broadly divided into three categories based on the way metadata is harvested:Data lineage involving ''software packages for structured data'', ''Programming Languages'', and ''Big Data''.

''Data lineage'' expects to view at least the technical metadata involving the data points and its various transformations. Along with technical data, ''Data Lineage'' may enrich the metadata with their corresponding Data Quality results,Reference Data values, [[Data model|Data Models]], [[Glossary of business and management terms|Business Vocabulary]], [[Data steward|People]], [[Program management|Programs]], and [[Enterprise system|Systems]] linked to the data points and transformations. Masking feature in the data lineage visualization allows the tools to incorporate all the enrichments that matter for the specific use case.  
Metadata normalization may be done in data lineage to represent disparate systems into one common view.

'''Data provenance''' documents the inputs, entities, systems, and processes that influence data of interest, in effect providing a historical record of the data and its origins. The generated evidence supports essential forensic activities such as data-dependency analysis, error/compromise detection and recovery, and auditing and compliance analysis. "'''Lineage''' is a simple type of '''why provenance'''."<ref name="DeSoumyarupa"/>

==Case for Data Lineage==
The world of [[big data]] is changing dramatically right before our eyes. Statistics say that Ninety percent (90%) of the world’s data has been created in the last two years alone.<ref>http://newstex.com/2014/07/12/thedataexplosionin2014minutebyminuteinfographic/</ref> This explosion of data has resulted in the ever-growing number of systems and automation at all levels in all sizes of organizations.

Today, distributed systems like Google [[Map Reduce]],<ref>Jeffrey Dean and Sanjay Ghemawat. Mapreduce: simplified data processing on
large clusters. Commun. ACM, 51(1):107–113, January 2008.</ref> Microsoft Dryad,<ref>Michael Isard, Mihai Budiu, Yuan Yu, Andrew Birrell, and Dennis Fetterly.
Dryad: distributed data-parallel programs from sequential building blocks. In Proceedings of the 2nd ACM SIGOPS/EuroSys European Conference onComputer
Systems 2007, EuroSys ’07, pages 59–72, New York, NY, USA, 2007. ACM.</ref> Apache Hadoop <ref>Apache Hadoop. http://hadoop.apache.org.</ref>(an open-source project) and Google Pregel<ref>Grzegorz Malewicz, Matthew H. Austern, Aart J.C Bik, James C. Dehnert, Ilan Horn, Naty Leiser, and Grzegorz Czajkowski. Pregel: a system for largescale graph processing. In Proceedings of the 2010 international conference on Managementof data, SIGMOD ’10, pages 135–146, New York, NY, USA, 2010. ACM.</ref> provide such platforms for businesses and users. However, even with these systems, [[big data]] analytics can take several hours, days or weeks to run, simply due to the data volumes involved. For example, a ratings prediction algorithm for the Netflix Prize challenge took nearly 20 hours to execute on 50 cores, and a large-scale image processing task to estimate geographic information took 3 days to complete using 400 cores.<ref>Shimin Chen and Steven W. Schlosser. Map-reduce meets wider varieties of
applications. Technical report, Intel Research, 2008.</ref> "The Large Synoptic Survey Telescope is expected to generate terabytes of data every night and eventually store more than 50 petabytes, while in the bioinformatics sector, the largest genome 12 sequencing houses in the world now store petabytes of data apiece."<ref>The data deluge in genomics. https://www-304.ibm.com/connections/blogs/ibmhealthcare/entry/data overload in genomics3?lang=de, 2010.</ref>
Due to the humongous size of the [[big data]], there could be features in the data that are not considered in the machine learning algorithm, possibly even outliers. It is very difficult for a data scientist to trace an unknown or an unanticipated result.

===Big Data Debugging===

[[Big data]] analytics is the process of examining large data sets to uncover hidden patterns, unknown correlations, market trends, customer preferences and other useful business information. They apply machine learning algorithms etc. to the data which transform the data. Due to the humongous size of the data, there could be unknown features in the data, possibly even outliers. It is pretty difficult for a data scientist to actually debug an unexpected result.

The massive scale and unstructured nature of data, the complexity of these analytics pipelines, and long runtimes pose significant manageability and debugging challenges. Even a single error in these analytics can be extremely difficult to identify and remove. While one may debug them by re-running the entire analytics through a debugger for step-wise debugging, this can be expensive due to the amount of time and resources needed. Auditing and data validation are other major problems due to the growing ease of access to relevant data sources for use in experiments, sharing of data between scientific communities and use of third-party data in business enterprises.<ref>Yogesh L. Simmhan, Beth Plale, and Dennis Gannon. A survey of data prove-
nance in e-science. SIGMOD Rec., 34(3):31–36, September 2005.</ref><ref name="IanFosterJensVockler">Ian Foster, Jens Vockler, Michael Wilde, and Yong Zhao. Chimera: A Virtual Data System for Representing, Querying, and Automating Data Derivation. In 14th International Conference on Scientific and Statistical Database Management, July 2002.</ref><ref name="Benjamim&Luiz">Benjamin H. Sigelman, Luiz Andr Barroso, Mike Burrows, Pat Stephenson, Manoj Plakal, Donald Beaver, Saul Jaspan, and Chandan Shanbhag. Dapper, a large-scale distributed systems tracing infrastructure. Technical report, Google Inc, 2010.</ref><ref name="PeterBuneman">Peter Buneman, Sanjeev Khanna, and Wang Chiew Tan. Data provenance: Some basic issues. In Proceedings of the 20th Conference on Foundations of SoftwareTechnology and Theoretical Computer Science, FST TCS 2000, pages 87–93, London, UK, UK, 2000. Springer-Verlag</ref> These problems will only become larger and more acute as these systems and data continue to grow. As such, more cost-efficient ways of analyzing [[data-intensive computing|data intensive scalable computing]] (DISC) are crucial to their continued effective use.

===Challenges in [[Big Data]] Debugging===

====Massive Scale====
According to an EMC/IDC study:<ref>http://www.emc.com/about/news/press/2012/20121211-01.htm</ref>
* 2.8ZB of data were created and replicated in 2012,
* the digital universe will double every two years between now and 2020, and
* there will be approximately 5.2TB of data for every man, woman and child on earth in 2020.
Working with this scale of data has become very challenging.

====Unstructured Data====
The phrase [[unstructured data]] usually refers to information that doesn't reside in a traditional row-column database. Unstructured data files often include text and multimedia content. Examples include e-mail messages, word processing documents, videos, photos, audio files, presentations, webpages and many other kinds of business documents. Note that while these sorts of files may have an internal structure, they are still considered "unstructured" because the data they contain doesn't fit neatly in a database.
Experts estimate that 80 to 90 percent of the data in any organization is unstructured. And the amount of unstructured data in enterprises is growing significantly often many times faster than structured databases are growing. "[[Big data]] can include both structured and unstructured data, but IDC estimates that 90 percent of [[big data]] is unstructured data."<ref>Webopedia http://www.webopedia.com/TERM/U/unstructured_data.html</ref>

====Long Runtime====
In today’s hyper competitive business environment, companies not only have to find and analyze the relevant data they need, they must find it quickly. The challenge is going through the sheer volumes of data and accessing the level of detail needed, all at a high speed. The challenge only grows as the degree of granularity increases. One possible solution is hardware. Some vendors are using increased memory and powerful parallel processing to crunch large volumes of data extremely quickly. Another method is putting data in-memory but using a grid computing approach, where many machines are used to solve a problem. Both approaches allow organizations to explore huge data volumes. Even this level of sophisticated hardware and software, few of the image processing tasks in large scale take a few days to few weeks.<ref>SAS. http://www.sas.com/resources/asset/five-big-data-challenges-article.pdf</ref> Debugging of the data processing is extremely hard due to long run times.

====Complex Platform====
[[Big Data]] platforms have a very complicated structure. Data is distributed among several machines. Typically the jobs are mapped into several machines and results are later combined by reduce operations. Debugging of a [[big data]] pipeline becomes very challenging because of the very nature of the system. It will not be an easy task for the data scientist to figure out which machine's data has the outliers and unknown features causing a particular algorithm to give unexpected results.

====Proposed Solution====
Data provenance or data lineage can be used to make the debugging of [[big data]] pipeline easier. This necessitates the collection of data about data transformations. The below section will explain data provenance in more detail.

==Data Provenance==
Data Provenance provides a historical record of the data and its origins. The provenance of data which is generated by complex transformations such as workflows is of considerable value to scientists. From it, one can ascertain the quality of the data based on its ancestral data and derivations, track back sources of errors, allow automated re-enactment of derivations to update a data, and provide attribution of data sources. Provenance is also essential to the business domain where it can be used to drill down to the source of data in a data warehouse, track the creation of intellectual property, and provide an audit trail for regulatory purposes.

The use of data provenance is proposed in distributed systems to trace records through a dataflow, replay the dataflow on a subset of its original inputs and debug data flows. To do so, one needs to keep track of the set of inputs to each operator, which were used to derive each of its outputs. Although there are several forms of provenance, such as copy-provenance and how-provenance,<ref name="PeterBuneman" /><ref>Robert Ikeda and Jennifer Widom. Data lineage: A survey. Technical report, Stanford University, 2009.</ref> the information we need is a simple form of '''why-provenance, or lineage''', as defined by Cui et al.<ref name="YCui">Y. Cui and J. Widom. Lineage tracing for general data warehouse transformations. VLDB Journal, 12(1), 2003.</ref>

==Lineage Capture==
Intuitively, for an operator T producing output o, lineage consists of triplets of form {I, T, o}, where I is the set of inputs to T used to derive o. Capturing lineage for each operator T in a dataflow enables users to ask questions such as “Which outputs were produced by an input i on operator T ?” and “Which inputs produced output o in operator T ?”<ref name="DeSoumyarupa"/> A query that finds the inputs deriving an output is called a backward tracing query, while one that finds the outputs produced by an input is called a forward tracing query.<ref name="RobertIkedaHyunjung">Robert Ikeda, Hyunjung Park, and Jennifer Widom. Provenance for generalized map and reduce workflows. In Proc. of CIDR, January 2011.</ref> Backward tracing is useful for debugging, while forward tracing is useful for tracking error propagation.<ref name="RobertIkedaHyunjung" /> Tracing queries also form the basis for replaying an original dataflow.<ref name="IanFosterJensVockler" /><ref name="YCui" /><ref name="RobertIkedaHyunjung" /> However, to efficiently use lineage in a DISC system, we need to be able to capture lineage at multiple levels (or granularities) of operators and data, capture accurate lineage for DISC processing constructs and be able to trace through multiple dataflow stages efficiently.

DISC system consists of several levels of operators and data, and different use cases of lineage can dictate the level at which lineage needs to be captured.  Lineage can be captured at the level of the job, using files and giving lineage tuples of form {IF i, M RJob, OF i }, lineage can also be captured at the level of each task, using records and giving, for example, lineage tuples of form {(k rr, v rr ), map, (k m, v m )}. The first form of lineage is called coarse-grain lineage, while the second form is called fine-grain lineage. Integrating lineage across different granularities enables users to ask questions such as “Which file read by a MapReduce job produced this particular output record?” and can be useful in debugging across different operator and data granularities within a dataflow.<ref name="DeSoumyarupa" />
[[File:Map Reduce Job -1.png|thumb|center|500px|Map Reduce Job showing containment relationships]]

To capture end-to-end lineage in a DISC system, we use the Ibis model,<ref>C. Olston and A. Das Sarma. Ibis: A provenance manager for multi-layer
systems. In Proc. of CIDR, January 2011.</ref> which introduces the notion of containment hierarchies for operators and data. Specifically, Ibis proposes that an operator can be contained within another and such a relationship between two operators is called '''operator containment'''. "Operator containment implies that the contained (or child) operator performs a part of the logical operation of the containing (or parent) operator."<ref name="DeSoumyarupa" /> For example, a MapReduce task is contained in a job. Similar containment relationships exist for data as well, called data containment. Data containment implies that the contained data is a subset of the containing data (superset).
[[File:Containment Hierarchy.png|thumb|center|500px|Containment Hierarchy]]

==Prescriptive Data Lineage==

The concept of '''Prescriptive Data Lineage''' combines both the logical model (entity) of how that data should flow with the actual lineage for that instance.<ref>http://info.hortonworks.com/rs/549-QAL-086/images/Hadoop-Governance-White-Paper.pdf</ref>

Data lineage and provenance typically refers to the way or the steps a dataset came to its current state Data lineage, as well as all copies or derivatives. However, simply looking back at only audit or log correlations to determine lineage from a forensic point of view is flawed for certain data management cases.  For instance, it is impossible to determine with certainty if the route a data workflow took was correct or in compliance without the logic model.

Only by combining the a logical model with atomic forensic events can proper activities be validated:
#Authorized copies, joins, or CTAS operations
#Mapping of processing to the systems that those process are run on
#Ad-Hoc versus established processing sequences

Many certified compliance reports require provenance of data flow as well as the end state data for a specific instance. With these types of situations, any deviation from the prescribed path need to be accounted for and potentially remediated.<ref>[https://www.sec.gov/info/smallbus/secg/bd-small-entity-compliance-guide.htm SEC Small Entity Compliance Guide]</ref>   This is marks a shift in thinking from purely a look back model  to a framework which is better suited to capture compliance workflows.

==Active vs Lazy Lineage==
Lazy lineage collection typically captures only coarse-grain lineage at run time. These systems incur low capture overheads due to the small amount of lineage they capture. However, to answer fine-grain tracing queries, they must replay the data flow on all (or a large part) of its input and collect fine-grain lineage during the replay. This approach is suitable for forensic systems, where a user wants to debug an observed bad output.

Active collection systems capture entire lineage of the data flow at run time. The kind of lineage they capture may be coarse-grain or fine-grain, but they do
not require any further computations on the data flow after its execution. Active fine-grain lineage collection systems incur higher capture overheads than lazy collection systems. However, they enable sophisticated replay and debugging.<ref name="DeSoumyarupa" />

==Actors==
An actor is an entity that transforms data; it may be a Dryad vertex, individual map and reduce operators, a MapReduce job, or an entire dataflow pipeline. Actors act as black-boxes and the inputs and outputs of an actor are tapped to capture lineage in the form of associations, where an association is a triplet {i, T, o} that relates an input i with an output o for an actor T . The instrumentation thus captures lineage in a dataflow one actor at a time, piecing it into a set of associations for each actor. The system developer needs to capture the data an actor reads (from other actors) and the data an actor writes (to other actors). For example, a developer can treat the Hadoop Job Tracker as an actor by recording the set of files read and written by each job.
<ref name="mainPaper">Dionysios Logothetis, Soumyarupa De, and Kenneth Yocum. 2013. Scalable lineage capture for debugging DISC analytics. In Proceedings of the 4th annual Symposium on Cloud Computing (SOCC '13). ACM, New York, NY, USA, , Article 17 , 15 pages.</ref>

==Associations==
Association is a combination of the inputs, outputs and the operation itself. The operation is represented in terms of a black box also known as the actor. The associations describe the transformations that are applied on the data. The associations are stored in the association tables. Each unique actor is represented by its own association table. An association itself looks like {i, T, o} where i is the set of inputs to the actor T and o is set of outputs given produced by the actor. Associations are the basic units of Data Lineage. Individual associations are later clubbed together to construct the entire history of transformations that were applied to the data.<ref name="DeSoumyarupa"/>

==Architecture==
[[Big data]] systems scale horizontally i.e. increase capacity by adding new hardware or software entities into the distributed system. The distributed system acts as a single entity in the logical level even though it comprises multiple hardware and software entities. The system should continue to maintain this property after horizontal scaling. An important advantage of horizontal scalability is that it can provide the ability to increase capacity on the fly. The biggest plus point is that horizontal scaling can be done using commodity hardware.

The horizontal scaling feature of [[Big Data]] systems should be taken into account while creating the architecture of lineage store. This is essential because the lineage store itself should also be able to scale in parallel with the [[Big data]] system. The number of associations and amount of storage required to store lineage will increase with the increase in size and capacity of the system. The architecture of [[Big data]] systems makes the use of a single lineage store not appropriate and impossible to scale. The immediate solution to this problem is to distribute the lineage store itself.<ref name="DeSoumyarupa"/>

The best case scenario is to use a local lineage store for every machine in the distributed system network. This allows the lineage store also to scale horizontally. In this design, the lineage of data transformations applied to the data on a particular machine is stored on the local lineage store of that specific machine. The lineage store typically stores association tables. Each actor is represented by its own association table. The rows are the associations themselves and columns represent inputs and outputs. This design solves 2 problems. It allows horizontal scaling of the lineage store. If a single centralized lineage store was used, then this information had to be carried over the network, which would cause additional network latency. The network latency is also avoided by the use of a distributed lineage store.<ref name="mainPaper"/>

[[File:Selection 065.png|thumb|center|500px|Architecture of Lineage Systems]]

==Data flow Reconstruction==
The information stored in terms of associations needs to be combined by some means to get the data flow of a particular job. In a distributed system a job is broken down into multiple tasks. One or more instances run a particular task. The results produced on these individual machines are later combined together to finish the job. Tasks running on different machines perform multiple transformations on the data in the machine. All the transformations applied to the data on a machines is stored in the local lineage store of that machines. This information needs to be combined together to get the lineage of the entire job. The lineage of the entire job should help the data scientist understand the data flow of the job and he/she can use the data flow to debug the [[big data]] pipeline. The data flow is reconstructed in 3 stages.

===Association tables===
The first stage of the data flow reconstruction is the computation of the association tables. The association tables exists for each actor in each local lineage store. The entire association table for an actor can be computed by combining these individual association tables. This is generally done using a series of equality joins based on the actors themselves. In few scenarios the tables might also be joined using inputs as the key. Indexes can also be used to improve the efficiency of a join.The joined tables need to be stored on a single instance or a machine to further continue processing. There are multiple schemes that are used to pick a machine where a join would be computed. The easiest one being the one with minimum CPU load. Space constraints should also be kept in mind while picking the instance where join would happen.

===Association Graph===
The second step in data flow reconstruction is computing an association graph from the lineage information. The graph represents the steps in the data flow. The actors act as vertices and the associations act as edges. Each actor T is linked to its upstream and downstream actors in the data flow. An upstream actor of T is one that produced the input of T, while a downstream actor is one that consumes the output of T . Containment relationships are always considered while creating the links. The graph consists of three types of links or edges.

====Explicitly specified links====
The simplest link is an explicitly specified link between two actors. These links are explicitly specified in the code of a machine learning algorithm. When an actor is aware of its exact upstream or downstream actor, it can communicate this information to lineage API. This information is later used to link these actors during the tracing query. For example, in the [[MapReduce]] architecture, each map instance knows the exact record reader instance whose output it consumes.<ref name="DeSoumyarupa"/>

====Logically inferred links====
Developers can attach data flow [[archetypes]] to each logical actor. A data flow archetype explains how the children types of an actor type arrange themselves in a data flow. With the help of this information, one can infer a link between each actor of a source type and a destination type. For example, in the [[MapReduce]] architecture, the map actor type is the source for reduce, and vice versa. The system infers this from the data flow archetypes and duly links map instances with reduce instances. However, there may be several [[MapReduce]] jobs in the data flow, and linking all map instances with all reduce instances can create false links. To prevent this, such links are restricted to actor instances contained within a common actor instance of a containing (or parent) actor type. Thus, map and reduce instances are only linked to each other if they belong to the same job.<ref name="DeSoumyarupa"/>

====Implicit links through data set sharing====
In distributed systems, sometimes there are implicit links, which are not specified during execution. For example, an implicit link exists between an actor that wrote to a file and another actor that read from it. Such links connect actors which use a common data set for execution. The dataset is the output of the first actor and is the input of the actor following it.<ref name="DeSoumyarupa"/>

===Topological Sorting===
The final step in the data flow reconstruction is the [[Topological sorting]] of the association graph. The directed graph created in the previous step is topologically sorted to obtain the order in which the actors have modified the data. This inherit order of the actors defines the data flow of the big data pipeline or task.

==Tracing & Replay==
This is the most crucial step in [[Big Data]] debugging. The  captured lineage is combined and processed to obtain the data flow of the pipeline. The data flow helps the data  scientist or a developer to look deeply into the actors and their transformations. This step allows the data scientist to figure out the part of the algorithm that is generating the unexpected output. A [[big data]] pipeline can go wrong in 2 broad ways. The first is a presence of a suspicious actor in the data-flow. The second being the existence of outliers in the data.

The first case can be debugged by tracing the data-flow. By using lineage and data-flow information together a data scientist can figure out how the inputs are converted into outputs. During the process actors that behave unexpectedly can be caught. Either these actors can be removed from the data flow or they can be augmented by new actors to change the data-flow. The improved data-flow can be replayed to test the validity of it. Debugging faulty actors include recursively performing coarse-grain replay on actors in the data-flow,<ref>Wenchao Zhou, Qiong Fei, Arjun Narayan, Andreas Haeberlen, Boon Thau Loo, and Micah Sherr. Secure network provenance. In Proceedings of 23rd ACM Symposium on Operating System Principles (SOSP), December 2011.</ref> which can be expensive in resources for long dataflows. Another approach is to manually inspect lineage logs to find anomalies,<ref name="Benjamim&Luiz" /><ref>Rodrigo Fonseca, George Porter, Randy H. Katz, Scott Shenker, and Ion Stoica. X-trace: A pervasive network tracing framework. In In Proceedings of NSDI’07, 2007.</ref> which can be tedious and time-consuming across several stages of a data-flow. Furthermore, these approaches work only when the data scientist can discover bad outputs. To debug analytics without known bad outputs, the data scientist need to analyze the data-flow for suspicious behavior in general. However, often, a user may not know the expected normal behavior and cannot specify predicates. This section describes a debugging methodology for retrospectively analyzing lineage to identify faulty actors in a multi-stage data-flow. We believe that sudden changes in an actor’s behavior, such as its average selectivity, processing rate or output size, is characteristic of an anomaly. Lineage can reflect such changes in actor behavior over time and across different actor instances. Thus, mining lineage to identify such changes can be useful in debugging faulty actors in a data-flow.
[[File:Tracing Anomalous Actors.png|thumb|center|400px|Tracing Anomalous Actors]]

The second problem i.e. the existence of outliers can also be identified by running the data-flow step wise and looking at the transformed outputs. The data scientist finds a subset of outputs that are not in accordance to the rest of outputs. The inputs which are causing these bad outputs are the outliers in the data. This problem can be solved by removing the set of outliers from the data and replaying the entire data-flow. It can also be solved by modifying the machine learning algorithm by adding, removing or moving actors in the data-flow. The changes in the data-flow are successful if the replayed data-flow does not produce bad outputs.
[[File:Tracing Outliers in the data.png|thumb|center|400px|Tracing Outliers in the data]]

==Challenges==
Even though use data lineage is a novel way of debugging of [[big data]] pipelines, the process is not simple. The challenges are scalability of lineage store, fault tolerance of the lineage store, accurate capture of lineage for black box operators and many others. These challenges must be considered carefully and trade offs between them need to be evaluated to make a realistic design for data lineage capture.

===Scalability===
DISC systems are primarily batch processing systems designed for high throughput. They execute several jobs per analytics, with several tasks per job. The overall number of operators executing at any time in a cluster can range from hundreds to thousands depending on the cluster size. Lineage capture for
these systems must be able scale to both large volumes of data and numerous operators to avoid being a bottleneck for the DISC analytics.

===Fault tolerance===
Lineage capture systems must also be fault tolerant to avoid rerunning data flows to capture lineage. At the same time, they must also accommodate failures in the DISC system. To do so, they must be able to identify a failed DISC task and avoid storing duplicate copies of lineage between the partial lineage generated by the failed task and duplicate lineage produced by the restarted task. A lineage system should also be able to gracefully handle multiple instances of local lineage systems going down. This can achieved by storing replicas of lineage associations in multiple machines. The replica can act like a backup in the event of the real copy being lost.

===Black-box operators===
Lineage systems for DISC dataflows must be able to capture accurate lineage across black-box operators to enable fine-grain debugging. Current approaches to this include Prober, which seeks to find the minimal set of inputs that can produce a specified output for a black-box operator by replaying the data-flow several times to deduce the minimal set,<ref>Anish Das Sarma, Alpa Jain, and Philip Bohannon. PROBER: Ad-Hoc Debugging of Extraction and Integration Pipelines. Technical report, Yahoo, April 2010.</ref> and dynamic slicing, as used by Zhang et al.<ref>Mingwu Zhang, Xiangyu Zhang, Xiang Zhang, and Sunil Prabhakar. Tracing lineage beyond relational operators. In Proc. Conference on Very Large Data Bases (VLDB), September 2007.</ref> to capture lineage for [[NoSQL]] operators through binary rewriting to compute dynamic slices. Although producing highly accurate lineage, such techniques can incur significant time overheads for capture or tracing, and it may be preferable to instead trade some accuracy for better performance. Thus, there is a need for a lineage collection system for DISC dataflows that can capture lineage from arbitrary operators with reasonable accuracy, and without significant overheads in capture or tracing.

===Efficient tracing===
Tracing is essential for debugging, during which, a user can issue multiple tracing queries. Thus, it is important that tracing has fast turnaround times. Ikeda et al.<ref name="RobertIkedaHyunjung" /> can perform efficient backward tracing queries for MapReduce dataflows, but are not generic to different DISC systems and do not perform efficient forward queries. Lipstick,<ref>Yael Amsterdamer, Susan B. Davidson, Daniel Deutch, Tova Milo, and Julia Stoyanovich. Putting lipstick on a pig: Enabling database-style workflow provenance. In Proc. of VLDB, August 2011.</ref> a lineage system for Pig,<ref>Christopher Olston, Benjamin Reed, Utkarsh Srivastava, Ravi Kumar, and Andrew Tomkins. Pig latin: A not-so-foreign language for data processing. In Proc. of ACM SIGMOD, Vancouver, Canada, June 2008.</ref> while able to perform both backward and forward tracing, is specific to Pig and SQL operators and can only perform coarse-grain tracing for black-box operators. Thus, there is a need for a lineage system that enables efficient forward and backward tracing for generic DISC systems and dataflows with black-box operators.

===Sophisticated replay===
Replaying only specific inputs or portions of a data-flow is crucial for efficient debugging and simulating what-if scenarios. Ikeda et al. present a methodology for lineage-based refresh, which selectively replays updated inputs to recompute affected outputs.<ref>Robert Ikeda, Semih Salihoglu, and Jennifer Widom. Provenance-based refresh in data-oriented workflows. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM ’11, pages 1659–1668, New York, NY, USA, 2011. ACM.</ref> This is useful during debugging for re-computing outputs when a bad input has been fixed. However, sometimes a user may want to remove the bad input and replay the lineage of outputs previously affected by the error to produce error-free outputs. We call this exclusive replay. Another use of replay in debugging involves replaying bad inputs for step-wise debugging (called selective replay). Current approaches to using lineage in DISC systems do not address these. Thus, there is a need for a lineage system that can perform both exclusive and selective replays to address different debugging needs.

===Anomaly detection===
One of the primary debugging concerns in DISC systems is identifying faulty operators. In long dataflows with several hundreds of operators or tasks, manual inspection can be tedious and prohibitive. Even if lineage is used to narrow the subset of operators to examine, the lineage of a single output can still span several operators. There is a need for an inexpensive automated debugging system, which can substantially narrow the set of potentially faulty operators, with reasonable accuracy, to minimize the amount of manual examination required.

==See also==
<!-- please do not list specific implementations here -->
* [[Provenance]]
* [[Big Data]]
* [[Topological Sorting]]
* [[Debugging]]
* [[NoSQL]]
* [[Scalability]]
* [[Directed acyclic graph]]

==References==
{{Reflist|33em}}

[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Big data]]
<=====doc_Id=====>:304
<=====title=====>:
Open Compute Project
<=====text=====>:
{{Infobox Organization
|name           = Open Compute Project
|image          = OpenCompute logo.jpg
|mcaption       = 
|formation      = 2011
|type           = Industry trade group
|purpose        = Sharing designs of [[data center]] products
|headquarters   = 
|membership     = 
|website        = {{URL|opencompute.org}}
|remarks        =
}}
[[File:Open Compute Server Front.jpg|thumb|Open Compute V2 Server]]
[[File:Open Compute 1U Drive Tray Bent.jpg|thumb|Open Compute V2 Drive Tray,<br />2nd lower tray extended]]
The '''Open Compute Project''' ('''OCP''') is an organization that shares designs of [[data center]] products among companies, including [[Facebook]], [[Intel]], [[Nokia]], [[Google]], [[Apple Inc.|Apple]], [[Microsoft]], [[Seagate Technology]], [[Dell]], [[Rackspace]], [[Ericsson]],  [[Cisco]], [[Juniper Networks]], [[Goldman Sachs]], [[Fidelity Investments|Fidelity]], [[Lenovo]] and [[Bank of America]].<ref>{{cite web|url=http://www.wired.com/2015/03/facebook-got-even-apple-back-open-source-hardware/|title=How Facebook Changed the Basic Tech That Runs the Internet|date=11 Apr 2015}}</ref><ref>{{Cite web|url=http://www.opencompute.org/about/ocp-incubation-committee/|title=Incubation Committee|website=Open Compute|access-date=2016-08-19}}</ref>

The Open Compute Project's mission is to design and enable the delivery of the most efficient server, storage and data center hardware designs for scalable computing. "We believe that openly sharing ideas, specifications and other intellectual property is the key to maximizing innovation and reducing operational complexity in the scalable computing space."<ref>{{cite web|url=http://www.opencompute.org/about/mission-and-principles/|title=Mission and Principles|website = Open Compute|accessdate = 2016-05-13}}</ref><br />
All Facebook Data Centers are 100% OCP: Prineville Data Center, Forest City Data Center, Altoona Data Center, Luleå Data Center (Sweden).
Facebook Data Centers under construction: Fort Worth Data Center, Clonee Data Center (Ireland).<ref>{{cite web|url=http://uk.businessinsider.com/facebook-eu-data-center-open-compute-project-2016-1|first=Matt|last=Weinberger|title=Facebook's newest data center is going to make some big tech companies very nervous|website=Open Compute|date=January 25, 2016|accessdate = 2016-05-16}}</ref>

==Details==
The initiative was announced in April 2011 by Jonathan Heiliger<ref>{{cite news|last1 = Heiliger|first1 = Jonathan|title = Why I Started the Open Compute Project|url = http://www.vertexventures.com/2015/06/why-i-started-the-open-compute-project/|accessdate = 18 June 2015|date = 2015-06-15}}</ref> at [[Facebook]] to openly share designs of [[data center]] products.<ref>{{cite web |url= http://www.datacenterknowledge.com/archives/2011/04/14/will-open-compute-alter-the-data-center-market/ |title=Will Open Compute Alter the Data Center Market? |date=April 14, 2011 |first= Rich|last= Miller |work= Data Center Knowledge |accessdate= July 9, 2013 }}</ref>
The effort came out of a redesign of [[Facebook]]'s data center in [[Prineville, Oregon]].<ref>{{Cite web |url= http://www.facebook.com/notes/facebook-engineering/building-efficient-data-centers-with-the-open-compute-project/10150144039563920 |title= Building Efficient Data Centers with the Open Compute Project |first= Jonathan|last= Heiliger |date= April 7, 2011 |work= Facebook Engineering's notes |accessdate= July 9, 2013 }}</ref>
After two years, with regards to a more module server design, it was admitted that "the new design is still a long way from live data centers".<ref>{{Cite news |title= Facebook Shatters the Computer Server Into Tiny Pieces |date= January 16, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/01/facebook-server-pieces/ |accessdate= July 9, 2013 }}</ref>
However, some aspects published were used in the Prineville center to improve the energy efficiency, as measured by the [[power usage effectiveness]] index defined by [[The Green Grid]].<ref name="Stanford">{{Cite web |title= Facebook's Open Compute Project |work= Stanford EE Computer Systems Colloquium |date= February 15, 2012  |url= http://www.stanford.edu/class/ee380/Abstracts/120215.html |first= Amir|last= Michael |publisher= [[Stanford University]]}}  ([http://ee380.stanford.edu/cgi-bin/videologger.php?target=120215-ee380-300.asx video archive])</ref>

The Open Compute Project Foundation is a 501(c)(6) non-profit incorporated in the state of Delaware. Corey Bell serves as the Foundation's CEO. Currently there are 7 members who serve on board of directors which is made up of two individual members and five organizational members.  Jason Taylor ([[Facebook]]) is the Foundation's president and chairman. Frank Frankovsky (formerly of Facebook and past president and chairman) and  [[Andy Bechtolsheim]] are the two individual members.  In addition to Jason Taylor who represents [[Facebook]], other organizations on the Open Compute board of directors include [[Intel]] (Jason Waxman), [[Goldman Sachs]] (Don Duet), [[Rackspace]] (Mark Roenick), and [[Microsoft]] (Bill Laing).<ref>{{Cite web|title = Organization and Board|url = http://www.opencompute.org/about/organization-and-board/|website = Open Compute|accessdate = 2015-09-12}}</ref>

On March 11, 2015 [[Apple Inc.|Apple]], [[Cisco]] and [[Juniper Networks]] joined the project.<ref>{{Cite web |title= Open Compute: Apple, Cisco Join While HP Expands |first= Charles|last= Babcock |date= March 11, 2015 |url=http://www.informationweek.com/cloud/infrastructure-as-a-service/open-compute-apple-cisco-join-while-hp-expands/d/d-id/1319421  |accessdate= March 11, 2015 }}</ref>

On November 16, 2015 [[Nokia]] joined the project.<ref>{{Cite web |title= Nokia Networks joins Open Compute Project to advance its AirFrame Data Center Solution|date= November 16, 2015 |url=http://company.nokia.com/en/news/press-releases/2015/11/16/nokia-networks-joins-open-compute-project-to-advance-its-airframe-data-center-solution}}</ref>

On February 23, 2016 [[Lenovo]] joined the project.<ref>{{Cite web |title= Lenovo joins Open Compute Project |date= February 23, 2016 |url=http://news.lenovo.com/blog/lenovo-joins-open-compute-projects.htm }}</ref>

On March 9, 2016 [[Google]] joined the project.<ref>{{Cite web |title= Google joins the Open Compute Project |date= March 9, 2016 |url=http://techcrunch.com/2016/03/09/google-joins-the-open-compute-project/ }}</ref>

Components of the Open Compute Project include:

* Server compute nodes included one for [[Intel]] processors and one for [[Advanced Micro Devices|AMD]] processors. In 2013, [[Calxeda]] contributed a design with [[ARM architecture]] processors.<ref>{{Cite web |title= ARM Server Motherboard Design for Open Vault Chassis Hardware v0.3 MB-draco-hesperides-0.3 |first= Tom|last= Schnell |date= January 16, 2013 |url=http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_ARM_Server_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}</ref><br />Several generations of server designs have been deployed. So far being: Freedom (Intel), Spitfire (AMD), Windmill (Intel E5-2600), Watermark (AMD), Winterfell (Intel E5-2600 v2) and Leopard (Intel E5-2600 v3)<ref>{{Cite web |title=Guide to Facebook’s Open Source Data Center Hardware
|author=Data Center Knowledge|date=April 28, 2016|url=http://www.datacenterknowledge.com/archives/2016/04/28/guide-to-facebooks-open-source-data-center-hardware/|accessdate=May 13, 2016}}</ref><ref>{{Cite web |title=Facebook rolls out new web and database server designs|first=The|last=Register|date=January 17, 2013|url=http://www.theregister.co.uk/2013/01/17/open_compute_facebook_servers/|accessdate=May 13, 2016}}</ref>

* Open Vault storage building blocks offer high disk densities, with 30 drives in a 2U Open Rack chassis designed for easy [[disk drive]] replacement. The 3.5 inch disks are stored in two drawers, five across and three deep in each drawer, with connections via [[serial attached SCSI]].<ref>{{Cite web |title= Open Vault Storage Hardware V0.7 OR-draco-bueana-0.7 |author= Mike Yan and Jon Ehlen |date= January 16, 2013 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Open_Vault_Storage_Specification_v0.7.pdf |accessdate= July 9, 2013 }}</ref> This storage is also called Knox, there is also a cold storage variant where the disks power down if not used to save energy consumption.<ref>{{Cite web |title=Under the hood: Facebook’s cold storage system|date=May 4, 2015|url=https://code.facebook.com/posts/1433093613662262/-under-the-hood-facebook-s-cold-storage-system-/|accessdate=May 13, 2016}}</ref> Another design concept was contributed by Hyve Solutions, a division of [[Synnex]] in 2012.<ref>{{Cite web |title= Hyve Solutions Contributes Storage Design Concept to OCP Community |work= News release |date= January 17, 2013 |url= http://ir.synnex.com/releasedetail.cfm?ReleaseID=733922 |accessdate= July 9, 2013 }}</ref><ref>{{Cite web |title= Torpedo Design Concept Storage Server for Open Rack Hardware v0.3 ST-draco-chimera-0.3 |first= Conor|last= Malone |date= January 15, 2012 |url= http://www.opencompute.org/wp/wp-content/uploads/2013/01/Open_Compute_Project_Storage_Server_for_Open_Rack_Specification_v0.3.pdf  |accessdate= July 9, 2013 }}</ref><br />At the OCP Summit 2016 Facebook together with Taiwanese ODM Wistron's spin-off Wiwynn introduced Lightning, a flexible NVMe JBOF (just a bunch of flash), based on the existing Open Vault (Knox) design.<ref>{{Cite web |title=Introducing Lightning: A flexible NVMe JBOF|first=Chris|last=Petersen|date=March 9, 2016|url=https://code.facebook.com/posts/989638804458007/introducing-lightning-a-flexible-nvme-jbof/|accessdate= May 13, 2016}}</ref><ref>{{Cite web |title=Wiwynn Showcases All-Flash Storage Product with Leading-edge NVMe Technology|date=March 9, 2016|url=http://www.wiwynn.com/english/company/newsinfo/23|accessdate= May 13, 2016}}</ref>
* Mechanical mounting system: Open racks have the same outside width (600&nbsp;mm) and depth as standard [[19-inch rack]]s, but are designed to mount wider chassis with a 537&nbsp;mm width (about 21 inches). This allows more equipment to fit in the same volume and improves air flow. Compute chassis sizes are defined in multiples of an OpenU, which is 48&nbsp;mm, slightly larger than the typical [[rack unit]].
* Data center designs for energy efficiency, include 277 VAC power distribution that eliminates one transformer stage in typical data centers. A single voltage (12.5 VDC) power supply designed to work with 277 VAC input and 48 VDC battery backup.<ref name="Stanford" />
* On May 8, 2013, an effort to define an open [[network switch]] was announced.<ref>{{Cite web |title= Up next for the Open Compute Project: The Network |date= May 8, 2013 |author= Jay Hauser for Frank Frankovsky |work= Open Compute blog |url= http://www.opencompute.org/blog/up-next-for-the-open-compute-project-the-network/ |accessdate= June 20, 2014 }}</ref> The plan was to allow Facebook to load its own [[operating system]] software onto the switch. Press reports predicted that more expensive and higher-performance switches would continue to be popular, while less expensive products treated more like a [[commodity]] (using the [[buzzword]] "top-of-rack") might adopt the proposal.<ref>{{Cite news |title= Can Open Compute change network switching? |first= David|last= Chernicoff |work= ZDNet |date= May 9, 2013 |url= http://www.zdnet.com/can-open-compute-change-network-switching-7000015141/ |accessdate= July 9, 2013 }}</ref><br />A similar project for a custom switch for the [[Google platform]] had been rumored, and evolved to use the [[OpenFlow]] protocol.<ref>{{Cite news |title= Facebook Rattles Networking World With ‘Open Source’ Gear |date= May 8, 2013 |first= Cade|last= Metz |work= Wired |url= http://www.wired.com/wiredenterprise/2013/05/facebook_networking/ |accessdate= July 9, 2013 }}</ref><ref>{{Cite news |title= Going With the Flow: Google’s Secret Switch to the Next Wave of Networking |date= April 17, 2012 |first= Steven|last= Levy |work= Wired |url= http://www.wired.com/wiredenterprise/2012/04/going-with-the-flow-google/ |accessdate= July 9, 2013 }}</ref><br />The first switch Open Sourced by Facebook was designed together with Taiwanese ODM Accton using Broadcom Trident II chip and is called Wedge, the Linux OS that it runs is called FBOSS.<ref>{{cite web|url=https://code.facebook.com/posts/681382905244727/introducing-wedge-and-fboss-the-next-steps-toward-a-disaggregated-network/|title=Introducing "Wedge" and "FBOSS," the next steps toward a disaggregated network|website =Meet the engineers who code Facebook|date=June 18, 2014|accessdate = 2016-05-13}}</ref><ref>{{cite web|url=https://code.facebook.com/posts/843620439027582/facebook-open-switching-system-fboss-and-wedge-in-the-open/|title=Facebook Open Switching System ("FBOSS") and Wedge in the open|website=Meet the engineers who code Facebook|date=March 10, 2015|accessdate = 2016-05-13}}</ref> Later switch contributions include "6-pack" and Wedge-100, based on Broadcom Tomahawk chips.<ref>{{cite web|url=https://code.facebook.com/posts/203733993317833/opening-designs-for-6-pack-and-wedge-100/|title=Opening designs for 6-pack and Wedge 100|website=Meet the engineers who code Facebook|date=March 9, 2016|accessdate = 2016-05-13}}</ref> Similar switch hardware designs have been contributed by: Edge-Core Networks Corporation (Accton spin-off), Mellanox Technologies, Interface Masters Technologies, Agema Systems.<ref>{{cite web|url=http://www.opencompute.org/wiki/Networking/SpecsAndDesigns|title=Accepted or shared hardware specifications|website=Open Compute|accessdate = 2016-05-13}}</ref> Capable of running ONIE compatible Operating Systems such as Cumulus Linux, Big Switch or Pica8.<ref>{{cite web|url=http://www.opencompute.org/wiki/Networking/ONIE/NOS_Status|title=Current Network Operating System (NOS) List|website=Open Compute|accessdate = 2016-05-13}}</ref>

== Providers ==
The promoted vendors include:<ref>[http://www.opencompute.org/about/open-compute-project-solution-providers/ open compute project solution providers]</ref>
* [[AMAX Information Technologies]]
* Circle B
* [[Itochu Techno-Solutions]] (CTC)
* [[Hewlett Packard Enterprise]]
* Hyperscale IT
* [[Synnex|Hyve Solutions]]
* [[Penguin Computing]]
* [[Nokia]]
* [[Quanta Computer]]
* Racklive
* Stack Velocity
* Wiwynn

== See also ==
* [[Novena (computing platform)]]
* [[Open-source computing hardware]]
* [[OpenPOWER Foundation]]
* [[Telecom Infra Project]]  - [[Facebook]] sister project focusing on [[Optical networking|Optical]] [[broadband networks]] and open [[cellular network|cellular networks]]

== References ==
{{reflist|33em}}

== External links ==
{{Commons category|Data Centers}}
* {{Official website|http://opencompute.org/}}
* [https://www.facebook.com/PrinevilleDataCenter/ Prineville Data Center]
* [https://www.facebook.com/ForestCityDataCenter/ Forest City Data Center]
* [https://www.facebook.com/AltoonaDataCenter/ Altoona Data Center]
* [https://www.facebook.com/LuleaDataCenter/ Luleå Data Center (Sweden)]
* [https://www.facebook.com/FortWorthDataCenter/ Fort Worth Data Center]
* [https://www.facebook.com/CloneeDataCenter/ Clonee Data Center (Ireland)]
* Videos
** {{youtube|2hTfzUmdAOw|HC23-T2: The Open Compute Project}}, Hot Chips 23, 2011 2.5 Hour Tutorial
** {{youtube|QtTF9pDQxPc|Facebook Open Compute Server}}, Facebook V1 Open Compute Server
** {{youtube|ckNzwqhDS60|Facebook V2 Windmill Server}}
** {{youtube|GbzQe3jO4hc|Hyve: Adapting Facebook's Servers for Your Data Center}}, Open Compute starts at 5:40

{{Facebook navbox|state=collapsed}}

[[Category:Open-source hardware]]
[[Category:Facebook]]
[[Category:2011 software]]
[[Category:Data centers]]
[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]
[[Category:Applications of distributed computing]]
[[Category:Cloud storage]]
[[Category:Computer networking]]
[[Category:Science and technology in the San Francisco Bay Area]]
<=====doc_Id=====>:307
<=====title=====>:
Data recovery hardware
<=====text=====>:
{{more footnotes|date=March 2015}}

'''Data recovery hardware''' was developed because [[data recovery software]] lacks the ability to deal with all lost or corrupted data files. Often the failures, such as media files with [[bad sectors]], [[firmware]] failures, PCB ([[Printed circuit board]]) failures, hard drive head failures, etc., cannot be fixed.

==Bad sectors==

The two types of bad sectors are "physical" and "logical" bad sectors, or "hard" and "soft" bad sectors.<ref>{{cite web
 | url = http://www.howtogeek.com/173463/bad-sectors-explained-why-hard-drives-get-bad-sectors-and-what-you-can-do-about-it/
 | title = Bad Sectors Explained: Why Hard Drives Get Bad Sectors and What You Can Do About It
 | date = October 9, 2013| accessdate = March 26, 2015
 | author = Chris Hoffman  | website = How-To Geek, LLC
}}</ref>

When a disk has physical bad sectors, software cannot effectively offer soft reset, hard reset, power reset, error handling, read algorithm auto exchange nor skip sectors. If a disk with bad physical sectors is connected to a [[Personal computer|PC]], the condition would potentially not be detected.

Soft bad sectors can potentially be fixed by either data recovery software or hardware, depending on the damaged condition. Some amount of bad sectors can be skipped using software, while a severely corrupted disk with a large area of bad sectors may potentially only be repaired.<ref>{{cite web
 | url = https://www.winxdvd.com/resource/repair-mp4-file-free.htm
 | title = How to Repair Corrupted MP4 Video File
 | date = July 3, 2015| accessdate = August 24, 2016
 | author = Estrella Garcia  | website = WinXDVD
}}</ref>  Bad sectors are areas on the hard drive that cannot be read. Even new hardrives sometimes contain bad sectors. Since manufacturers intensely compete to cram more space into disks, systems operate close to the limit of that generation of technology.<ref>{{Cite web|url=https://www.grc.com/sr/faq.htm|title=GRC {{!}} SpinRite 6.0 FAQ - Frequently Asked Questions|website=www.grc.com|access-date=2016-10-20}}</ref>

==Dead PCB==

When the drive has dead PCB ([[Printed circuit board]]), users need to:
* swap in a new PCB
* put one donor [[Integrated circuit|chip]] and write by chip reader with matching ROM ([[Read-only memory]]) content
* get the dead drive spinning

When the drive has physical head damage, users need to open the drive in [[cleanroom]] environment and find donor heads or other donor components to swap.

==Data recovery hardware types==
*Disk image;
*File extraction hardware;
*Firmware repair hardware;
*ROM chip reader;
*Head and Platter Swap Tools (See [[Hard disk drive platter]]);
*Spindle release hardware;
*Other hardware

== See also ==
{{Portal|Computer security|Computing}}

{{Div col||20em}}
* [[Data recovery]]
* [[Firmware]]
* [[Bad sector]]
* [[Disk image]]
* [[Printed circuit board]]
* [[Cleanroom]]
* [[List of data recovery software]]
* [[Comparison of file systems]]
* [[Computer forensics]]
* [[Continuous data protection]]
* [[Data archaeology]]
* [[Data loss]]
* [[Error detection and correction]]
* [[File carving]]
* [[Hidden file and hidden directory]]
* [[Knowledge extraction]]
* [[Undeletion]]
{{Div col end}}

==Further reading==
* Tanenbaum, A. & Woodhull, A. S. (1997). ''Operating Systems: Design And Implementation,'' 2nd ed. New York: Prentice Hall.
* {{dmoz|Computers/Hardware/Storage/Data_Recovery/|Data recovery}}
* {{cite web|url=https://www.grc.com/sr/faq.htm|title=GRC&nbsp;-&nbsp;SpinRite 6.0 FAQ - Frequently Asked Questions|publisher=}}

==References==
{{Reflist|30em}}

{{DEFAULTSORT:Data Recovery}}
[[Category:Data recovery|*]]
[[Category:Computer data]]
[[Category:Data management]]
[[Category:Transaction processing]]
[[Category:Hard disk software|*]]
[[Category:Backup|Recovery]]
<=====doc_Id=====>:310
<=====title=====>:
Data architect
<=====text=====>:
{{expert-subject|Computer science|talk=Copyright issues and need for expert attention}}
A ''data architect'' is a practitioner of [[data architecture]], an information technology discipline concerned with designing, creating, deploying and managing an organization's data architecture. Data architects define how the data will be stored, consumed, integrated and managed by different data entities and IT systems, as well as any applications using or processing that data in some way.<ref>{{cite web|title=Definition of Data Architect|url=http://stage.web.techopedia.com/definition/29452/data-architect|website=Techopedia}}</ref>  It is closely allied with [[business architecture]] and is considered to be one of the four domains of [[enterprise architecture]].

==Role==
According to the Data Management Body of Knowledge,<ref>{{cite web|title=Data Management Body of Knowledge|url=http://www.dama.org/content/body-knowledge|publisher=Data Management Association}}</ref> the data architect “provides a standard common business vocabulary, expresses strategic data requirements, outlines high level integrated designs to meet these requirements, and aligns with enterprise strategy and related business architecture.”

According to the Open Group Architecture Framework (TOGAF), a data architect is expected to set data architecture principles, create models of data that enable the implementation of the intended business architecture, create diagrams showing key data entities, and create an inventory of the data needed to implement the architecture vision.<ref name=TOGAF>{{cite book|title=The Open Group Architectural Framework (TOGAF 9.1)|publisher=The Open Group|location=Chapter 10 - Data Architecture|url=http://pubs.opengroup.org/architecture/togaf9-doc/arch/chap10.html|accessdate=1 March 2015}}</ref>

==Responsibilities==
# Organizes data at the macro level (i.e. which subject areas are managed in which goldensources) 
# Organizes data at the micro level, data models, for a new application. 
# Provides a logical data model as a standard for the goldensource and for consuming applications to inherit. 
# Provides a logical data model with elements and business rules needed for the creation of DQ rules.

==Skills==
Bob Lambert, Director of Data Architecture at consulting firm CapTech, describes the necessary skills of a Data Architect as follows:<ref>{{cite web|last1=Lambert|first1=Bob|title=Skills of a Data Architect|url=http://www.captechconsulting.com/blog/bob-lambert/skills-the-data-architect|website=Captech}}</ref>

* Foundation in systems development: the data architect should understand the system development life cycle; software project management approaches; and requirements, design, and test techniques. The data architect is asked to conceptualize and influence application and interface projects, and therefore must understand what advice to give and where to plug in to steer toward desirable outcomes.
* Depth in data modeling and database design: This is the core skill of the data architect, and the most requested in data architect job descriptions. The effective data architect is sound across all phases of data modeling, from conceptualization to database optimization. In his experience this skill extends to SQL development and perhaps database administration.
* Breadth in established and emerging data technologies: In addition to depth in established data management and reporting technologies, the data architect is either experienced or conversant in emerging tools like columnar and NoSQL databases, predictive analytics, data visualization, and unstructured data. While not necessarily deep in all of these technologies, the data architect hopefully is experienced in one or more, and must understand them sufficiently to guide the organization in understanding and adopting them.
* Ability to conceive and portray the big data picture: When the data architect initiates, evaluates, and influences projects he or she does so from the perspective of the entire organization. The data architect maps the systems and interfaces used to manage data, sets standards for data management, analyzes current state and conceives desired future state, and conceives projects needed to close the gap between current state and future goals.
* Ability to astutely operate in the organization: Well respected and influential, Able to emphasize methodology, modeling, and governance, Technologically and politically neutral, Articulate, persuasive, and a good salesperson, and Enthusiastic

==References==
{{reflist}}

==See also==
* [[Data Architecture]]
* [[Information Architect]]
* [[Enterprise Architecture]]

[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Data security]]
<=====doc_Id=====>:313
<=====title=====>:
Data based decision making
<=====text=====>:
{{more footnotes|date=May 2015}}

'''Data based decision making''' or data driven decision making refers to educator’s  ongoing process of collecting and analyzing different types of data, including demographic, student achievement test, satisfaction, process data to guide decisions towards improvement of educational process. DDDM becomes more important in education since federal and state test-based  accountability policies. [[No Child Left Behind Act]] opens broader opportunities and incentives in using [[data]] by educational organizations by requiring schools and districts to analyze additional components of data, as well as pressing them to increase student test scores. Information makes schools accountable for year by year improvement various student groups. DDDM helps to recognize the problem and who is affected by the problem; therefore, DDDM can find a solution of the problem

==Purpose==

The purpose of DDDM is to help educators, schools, districts, and states to use information they have to actionable knowledge to improve student outcomes. DDDM requires high-quality data and possibly technical assistance; otherwise, data can misinform and lead to unreliable inferences. [[Data management]] techniques can improve teaching and learning in schools. Test scores are used by many principals to identify “bubble kids”, students whose results are just below proficiency level in reading and mathematics.<ref name=r1>{{cite journal|last1=Mandinach|first1=Ellen|title=A perfect time for data use|journal=Educational Psychologist|date=April 23, 2012|volume=47|page=2|doi=10.1080/00461520.2012.667064}}</ref>

==Types of data used in education==

There are 4 major types of data used in education: demographics data, perceptions data, student learning data, and school processes data.<ref name=Bernhardt>{{cite book|last1=Bernhardt|first1=Victoria|title=Data analysis for continuous school improvement|date=2013|publisher=Routledge|location=711 Third Avenue, New York, 10017|isbn=978-1-59667-252-9|pages=27–80}}</ref>

1. Demographics data in educational organizations answers the question, "Who are we?". Demographics show the current context of the school and shows the trends. Trends help to predict and plan for the future, along with seeing measures where leaders work towards continuous school improvement. Thorough demographic data explains the structure of school, system, and the leadership. In education demographic data to the next items: number of students in the school, number of students with special needs, number of English learners, age or grade of students in cohorts, socio-economical status of students, attendance rates, [[ethnicity]]/[[race (human classification)|race]]/[[religious beliefs]], graduation rates, dropout rates, experience information of teachers, information about parents of students.<ref name="Bernhardt"/>

2. Perception data tells us what students, staff, and parents think about a school and answers the question, "How do we do business?". School culture, climate, and organizational processes are assessed by perception data. Perception data includes values, beliefs, perceptions, opinions, observations. Perception data is collected mostly questionnaires. Perception data can be differentiate by two groups: 1- staff, 2 - students and parents. Staff are being asked if any changes in instruction or [[curriculum]] need to take place. Student and parent are questioned to report their interests, how difficult it take them to learn, how are they taught and treated.<ref name="Bernhardt"/>

3. Student learning data answers two questions: How are our students doing? and Where are we now? Student learning data requires information from all subject areas, disaggregated by demographic groups, by teachers, by grade level, by cohorts over time, and individual student growth. This type of data helps to address additional help to students who are not proficient, deepening into what they know and what they don't know to become proficient. Student learning data connects with [[curriculum]], [[Teaching|instruction]], and [[Educational assessment|assessment]] in order to improve outcomes. Student learning data can clearly state the effectiveness of a single educator or the entire school. SLD can be gathered by looking at diagnostic tests, formative assessments, performance assessments, standardized tests, non-referenced tests, summative assessments, teacher-assigned tests, and others.<ref name="Bernhardt"/>

4. School processes refer to actions of administrators and teachers to achieve the purpose of the school. Teachers' habits, customs, knowledge, and professionalism are the things leading towards progress inside organizations. School processes data tell us what works, what doesn't, the results of educational process, and answers the question, "What are our processes are?". School processes produce school and class results. There are 4 major types of school processes: 1. instructional processes, 2. Organizational processes, 3. Administrative processes, 4. Continuous school improvement processes.   
<ref name="Bernhardt"/>

==Use in educational organizations==

[[The U.S. Department of Education]] and the [[Institute of Education Sciences]] require to use data and DDDM in past decades to run educational organizations. Hard evidence and the use of data are emphasized to inform decisions. The data in educational organizations means more than analyzing test scores. Educational data movement is considered as a sociotechnical revolution. Educational data systems involve technologies and evidence to explain districts', schools', classrooms' tendencies. DDDM is used to explain complexity of education, support collaboration, creating new designs of teaching. Student performance is central in DDDM. NCLB provided boost in the collection and use of educational information.<ref name=Piety>{{cite book|last1=Piety|first1=Philip|title=Assessing the educational data movement|date=2013|publisher=Teachers college press|location=New york|isbn=978-0-8077-5426-9|pages=1–20}}</ref>

For example, in a rural area educators tried to understand why a particular subset of students were struggling academically. Data analysts collected students performance data, medical records, behavioral data, attendance, and other data less qualitative information. After not finding direct correlation between collected data and student outcomes they decided to include transportation data into the research. As result, educators found that students who had longer way from houses to the school were struggling the most. According to the finding administrators modified transportation arrangements to make the way shorter for students as well as installing Internet access in buses so students could concentrate on doing homework. DDDM in this particular case helped to improve student results.<ref name="r1"/>

==Effects on schools==

Effective schools showing outstanding gains in academic measures report that the wide and wise use of data has a positive effect on student achievement and progress. DDDM is suggested to be a main tool to move educational organizations towards school improvement and [[educator effectiveness]]. Data can be used to measure growth over time, program evaluation along with identifying root causes of problems connected to education. Involving school teachers in data inquiry causes more collaborative work from staff. Data provides increasing communication and knowledge which has a positive effect on altering educator attitudes towards groups inside schools which are underperforming 
<ref>{{cite journal|last1=Wayman|first1=Jeffrey|title=Involving teachers in data driven decision making:Using computer data systems to support teacher inquiry and reflection|journal=Journal of education for students placed at risk|date=2005|pages=296–300}}</ref>

==Notes==
{{Reflist}}

==General references==
* {{cite journal|last1=Spillane|first1=James P.|title=Data in Practice: Conceptualizing the Data-Based Decision-Making Phenomena|journal=American Journal of Education|date=2012|volume=118|issue=2|pages=113–141|jstor=10.1086/663283|doi=10.1086/663283}}
* {{cite journal|last1=Reeves|first1=Patricia L.|last2=Burt|first2=Walter L.|title=Challenges in Data-based Decision-making: Voices from Principals|journal=Educational Horizons|date=2006|volume=85|issue=1|pages=65–71|jstor=42925967}}

[[Category:Data management]]
[[Category:Standards-based education]]
<=====doc_Id=====>:316
<=====title=====>:
Cleo (company)
<=====text=====>:
{{Infobox company
| logo             = [[File:Cleo (company) logo, 2014.png]]
| name             = Cleo
| type             = [[Privately held company]]
| foundation       =  1976
| location_city    = [[Loves Park, Illinois]]
| location_country = [[United States of America]]
| key_people       =
Mahesh Rajasekharan <small>([[Chief executive officer|CEO]])</small><br/>Sumit Garg <small>(President)</small>
| num_employees    = 200+
| industry         = [[Managed file transfer]], data integration, [[network management]] and secure file sharing
| homepage         = {{url|http://cleo.com}}
}}

'''Cleo''' is an [[enterprise software]] company that provides [[electronic data interchange]] (EDI), and application-to-application (A2A), [[business-to-business]] (B2B), and [[big data]] integration services to organizations with [[managed file transfer]] needs. The company, formerly known as Cleo Communications, was founded in 1976. Cleo was acquired by investment firm Globe Equity Partners in 2012. Mahesh Rajasekharan is Cleo's [[CEO]], and Sumit Garg serves as Cleo's president.<ref>{{cite web|author=Alex Gary |url=http://www.rrstar.com/x1364621329/Private-equity-firm-acquires-Loves-Park-company |title=Private equity firm acquires Loves Park company - Blogs - Rockford Register Star |publisher=Rrstar.com |date= |accessdate=2014-06-05}}</ref>

== Business ==
Cleo originally began as a division of Phone 1 Inc., a voice data gathering systems manufacturer, and built data concentrators and [[terminal emulator]]s — multi-bus computers, modems, and terminals to interface with [[IBM]] mainframes via [[Binary Synchronous Communications|bisynchronous communications]]. The company then began developing [[mainframe]] middleware in the 1980s, and with the rise of the [[Personal computer|PC]], moved into B2B data communications and [[file transfer]] software.<ref>{{cite web|url=https://books.google.com/books?id=JNJWAAAAMAAJ&q=cleo+%22phone+1%22&dq=cleo+%22phone+1%22&hl=en&sa=X&ei=6I65VLiABc6yogTf0IKABA&ved=0CEcQ6AEwBTge |title=Kelly/Grimes IBM PC compatible computer directory - Brian W. Kelly, Dennis J. Grimes - Google Books |publisher=Books.google.com |date=2008-01-28 |accessdate=2015-04-02}}</ref>

Cleo's portfolio features big data, extreme file transfer, [[data transformation]], person-to-person collaboration, and file sharing solutions,<ref>http://www.channelworld.in/interviews/high-speed-data-transfer-is-more-critical-than-ever%3A-mahesh-rajasekharan%2C-cleo</ref> and its product line includes software for secure file transfer, exchange, and [[Cloud collaboration|collaboration]]; secure email, text, and voice messaging; and others.<ref>[http://investing.businessweek.com/research/stocks/private/snapshot.asp?privcapId=204651549 Cleo Communications, Inc.: Private Company Information - Businessweek<!-- Bot generated title -->]</ref>  Cleo products use the [[AS2]] specification and other protocols for connectivity and community management.<ref>{{cite web|author=|url=http://www.filetransferconsulting.com/forresters-managed-file-transfer-good-bad-ugly/ |title=Forrester’s "Managed File Transfer Solutions" – Good, Bad and Ugly |publisher=Filetransferconsulting.com |date=2011-07-14 |accessdate=2014-06-05}}</ref> Cleo VersaLex is the engine behind its software offerings, which include Cleo LexiCom,<ref>[http://www.itjungle.com/fhs/fhs030408-story10.html Four Hundred Stuff-Cleo Updates B2B Communications Software<!-- Bot generated title -->]</ref> Cleo VLTrader,<ref>[http://www.itjungle.com/fhs/fhs021610-story08.html Four Hundred Stuff-Stonebranch Taps Cleo for B2B Expertise<!-- Bot generated title -->]</ref> and Cleo Harmony, which supports the streamlining of [[data integration]].<ref>[http://webcache.googleusercontent.com/search?q=cache:http://lerablog.org/technology/software/ftp-tools-to-help-with-large-file-transfers/ Best FTP Tools for Large File Transfers<!-- Bot generated title -->]{{dead link|date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> The company also developed the Cleo Unify and Cleo Trust secure [[file sharing]] and [[email]] messaging solutions that work independently or in conjunction with Cleo's data integration platform.<ref>{{cite web|url=http://www.rrstar.com/article/20150209/News/150209528 |title=Cleo releases new software - Rockford Register Star |publisher=rrstar.com |date=2015-02-09 |accessdate=2015-02-19}}</ref> In 2015, Cleo introduced the Cleo Jetsonic high-speed data transfer software solution.<ref>{{cite web|url=http://www.rrstar.com/article/20150708/NEWS/150709580/-1/json |title=Cleo announces new data solution - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-09}}</ref>

The City of [[Atlanta]] adopted Cleo's [[fax]] technology, Cleo Streem, in 2006 to accommodate its communication needs,<ref>http://citycouncil.atlantaga.gov/2013/images/proposed/13R3556.pdf</ref> and the [[U.S. Department of Veterans Affairs]] did the same in 2013 when in need of [[FIPS 140-2]]-compliant technology to protect information.<ref>[http://www.va.gov/TRM/ToolPage.asp?tid=6568 One-VA Technical Reference Model<!-- Bot generated title -->]</ref> Cleo also serves U.S. transportation [[logistics]] company MercuryGate International<ref>{{cite web|url=http://www.rrstar.com/article/20140820/ENTERTAINMENTLIFE/140829886/10487/BUSINESS |title=Loves Park business selected to provide online services - Rockford Register Star |publisher=rrstar.com |date=2014-08-20 |accessdate=2015-02-19}}</ref> as a customer and partners with [[Hortonworks]]<ref>[http://hortonworks.com/blog/secure-reliable-hadoop-data-transfer-option-cleo-mft/ Secure, reliable Hadoop data transfer with Cleo MFT - Hortonworks<!-- Bot generated title -->]</ref> for big data integration and [[Tech Data]] for software distribution.<ref>[http://logistics.cioreview.com/news/cleo-s-data-transfer-solutions-now-available-on-the-tech-data-online-store-nid-2119-cid-33.html Cleo's Data Transfer Solutions Now Available on the Tech Data Online Store<!-- Bot generated title -->]</ref> Cleo software also powers the architecture for several major supply chain companies, such as [[JDA Software]] and [[SAP SE|SAP]].<ref>{{cite web|last=Grackin |first=Ann |url=http://searchmanufacturingerp.techtarget.com/tip/Smart-sensors-bring-the-supply-chain-to-life |title=Smart sensors bring the supply chain to life |publisher=Searchmanufacturingerp.techtarget.com |date= |accessdate=2015-07-08}}</ref>

In 2009, Cleo was added to the [[Gartner]] [[Magic Quadrant]] for managed file transfer.<ref>http://www.servicecatalog.dts.ca.gov/services/sft/docs/MFT_Quad_2009_axway_3183.pdf</ref>

== Expansion ==
In June 2014, Cleo opened an office in [[Chicago]] for members of its support and engineering teams.<ref>[http://rockrivertimes.com/2014/07/16/cleo-continues-to-grow-expands-operations-into-chicago-office/ Cleo continues to grow, expands operations into Chicago office | The Rock River Times<!-- Bot generated title -->]</ref> The company in 2014 hired Jorge Rodriguez as senior vice president of product development<ref>{{cite web|author=|url=http://www.marketwatch.com/story/jorge-rodriguez-joins-cleo-as-senior-vice-president-of-product-development-2014-02-04 |title=Jorge Rodriguez Joins Cleo as Senior Vice President of Product Development |publisher=MarketWatch |date=2014-02-04 |accessdate=2015-02-19}}</ref> and John Thielens as vice president of technology.<ref>{{cite web|author=|url=http://www.marketwatch.com/story/john-thielens-joins-cleo-as-vice-president-of-technology-2014-01-31 |title=John Thielens Joins Cleo as Vice President of Technology |publisher=MarketWatch |date=2014-01-31 |accessdate=2015-02-19}}</ref> And in 2015, Cleo hired Dave Brunswick as vice president of solutions for North America.<ref>{{cite web|url=http://www.rrstar.com/article/20150705/NEWS/150709917 |title=Cleo announces new hire - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-07-08}}</ref> Cleo also opened its Center of Innovation product development facility in [[Bengaluru, India]], in 2015.<ref>http://www.deccanherald.com/content/500091/cleo-bengaluru-centre-plans-co.html</ref>

In 2016, Cleo acquired [[Extol International|EXTOL International]], a [[Pottsville, Pennsylvania|Pottsville, Pa.]]-based business and EDI integration and data transformation company for an undisclosed amount. The Pottsville office will operate under the Cleo name.<ref>http://www.lvb.com/article/20160406/LVB01/160409929/pottsville-tech-firm-acquired-by-illinois-company</ref>

== Certification ==
Cleo regularly submits its products to Drummond Group's interoperability software testing for AS2,<ref>[http://www.supplychainbrain.com/content/technology-solutions/supplier-relationship-mgmt/single-article-page/article/cleos-versalex-wins-drummond-certification-for-as2-interoperability/ Cleo's VersaLex Wins Drummond Certification for AS2 Interoperability<!-- Bot generated title -->]</ref> AS3<ref>{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/341-as3-secure-messaging-products-are-drummond-certified-in-1q14-interoperability-test-event |title=AS3 Secure Messaging Products are Drummond Certified™ in 1Q14 Interoperability Test Event |publisher=Drummond Group |date=2014-02-19 |accessdate=2014-06-05}}</ref> and ebMS 2.0.<ref>{{cite web|url=http://www.drummondgroup.com/index.php/newsevents/press-releases/339-newest-ebms-20-secure-messaging-products-are-drummond-certified |title=Newest ebMS 2.0 Secure Messaging Products are Drummond Certified™ |publisher=Drummond Group |date=2013-09-09 |accessdate=2014-06-05}}</ref>

== Awards ==
Cleo has been given a [[Xerox]] partner of the year award for each of the past five years. The Cleo Streem solution integrates with Xerox multi-function products, providing customers with comprehensive solutions for network fax and interactive messaging needs.<ref>{{cite web|url=http://www.rrstar.com/article/20150330/NEWS/150339927/10447/NEWS |title=Cleo Wins Xerox Partner of the Year Award - News - Rockford Register Star |publisher=rrstar.com |date= |accessdate=2015-04-02}}</ref>

== References ==
{{Reflist|3}}

[[Category:EDI software companies]]
[[Category:Software companies based in Illinois]]
[[Category:Network management]]
[[Category:Managed file transfer]]
[[Category:File transfer protocols]]
[[Category:Data management]]
<=====doc_Id=====>:319
<=====title=====>:
Embedded analytics
<=====text=====>:
'''Embedded analytics''' is the technology designed to make [[data analysis]] and [[business intelligence]] more accessible by all kinds of application or user.

==Definition==

According to Gartner analysts Kurt Schlegel, traditional [[business intelligence]] were suffering in 2008 a lack of integration between the data and the business users.<ref>{{cite web
| last = Kelly
| first = Jeff
| title = Gartner Business Intelligence Summit: Embed BI within business processes
| publisher = TechTarget
| accessdate = August 2015
| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes
}}</ref> This technology intention is to be more pervasive by real-time autonomy and self-service of data visualization or customization, meanwhile decision makers, business users or even customers are doing their own daily workflow and tasks.

==History==

First mentions of the concept were made by Howard Dresner, consultant, author, former Gartner analyst and inventor of the term "business intelligence".<ref>{{cite web
| last = Kelly
| first = Jeff
| title = Gartner Business Intelligence Summit: Embed BI within business processes
| publisher = TechTarget
| accessdate= August 2015
| url = http://searchbusinessanalytics.techtarget.com/news/1507180/Gartner-Business-Intelligence-Summit-Embed-BI-within-business-processes
}}</ref> Consolidation of [[business intelligence]] "doesn't mean the BI market has reached maturity" <ref>{{cite web
| last = Dresner
| first = Howard 
| title = Howard Dresner predicts the future of business intelligence
| publisher = TechTarget
| accessdate= August 2015
| url = http://searchbusinessanalytics.techtarget.com/podcast/Howard-Dresner-predicts-the-future-of-business-intelligence
}}</ref> said Howard Dresner while he was working for Hyperion Solutions, a company that Oracle bought in 2007. Oracle started then to use the term "embedded analytics" at their press release for Oracle® Rapid Planning on 2009.<ref>{{cite web
| title = Oracle Announces Oracle® Rapid Planning
| publisher = Oracle
| accessdate= August 2015
| url = http://www.oracle.com/us/corporate/press/040402
}}</ref> Gartner Group, a company for which Howard Dresner has been working,  finally added the term to their IT Glossary on November 5, 2012. 
<ref>{{cite web
| title = Gartner IT Glossary: Embedded Analytics 
| publisher = Gartner
| accessdate= August 2015
| url = http://www.gartner.com/it-glossary/embedded-analytics 
}}</ref>
. It was clear this was a mainstream technology when Dresner Advisory Services published the 2014 Embedded Business Intelligence Market Study as part of the Wisdom of Crowds® Series of Research, including 24 vendors.<ref>{{cite web
| title = 2014 Embedded Business Intelligence Market Study Now Available From Dresner Advisory Services 
| publisher = Market Wired
| accessdate= August 2015
| url = http://www.marketwired.com/press-release/2014-embedded-business-intelligence-market-study-now-available-from-dresner-advisory-1962227.htm
}}</ref>

==Tools==

{{colbegin|2}}

* [[Actuate Corporation|Actuate]]
* [[Dundas Data Visualization]]
* [[GoodData]]
* [[IBM]]
* [[icCube]]
* [[Logi Analytics]]
* [[Pentaho]]
* [[Qlik]]
* [[SAP_SE|SAP]]
* [[SAS_(software)|SAS]]
* [[ServiceNow]]
* [[Tableau Software|Tableau]]
* [[ThoughtSpot]]
* [[TIBCO]]
* [[Sisense]]

{{colend}}

==References==
{{Reflist}}

[[Category:Types of analytics]]
[[Category:Big data|analytics]]
[[Category:Business intelligence]]
[[Category:Data management]]
<=====doc_Id=====>:322
<=====title=====>:
Data availability
<=====text=====>:
{{cleanup reorganize|date=June 2015}}
Data availability<ref>http://searchstorage.techtarget.com/definition/data-availability</ref> is a term used by computer storage manufacturers and storage service providers (SSPs) to describe products and services that ensure that data continues to be available at a required level of performance in situations ranging from normal through "disastrous."

Anytime a server loses power, for example, it has to reboot, recover data and repair corrupted data. The time it takes to recover, known as the mean time to recover (MTR), could be minutes, hours or days.<ref>http://blog.schneider-electric.com/datacenter/2012/10/12/understanding-data-center-reliability-availability-and-the-cost-of-downtime/</ref>

==Data Center Standards==
The two organizations in the United States that publish data center standards are the [[Telecommunications Industry Association]] (TIA) and the [[Uptime Institute]].

===TIA - Data Center Standards===
See wiki entry on [[TIA-942]].

===Uptime Institute - Data Center Tier Standards===
'''Tier I Requirements'''<ref>http://www.firstcomm.com/overview-of-data-center-availability-tiers/</ref>
* Single non-redundant distribution path serving the IT equipment
*  Non-redundant capacity components
* Basic site infrastructure with expected availability of 99.671%

'''Tier II Requirements'''
* Meets or exceeds all Tier I requirements
* Redundant site infrastructure capacity components with expected availability of 99.741%

'''Tier III Requirements'''
* Meets or exceeds all Tier I and Tier II requirements
* Multiple independent distribution paths serving the IT equipment
* All IT equipment must be dual-powered and fully compatible with the topology of a site’s architecture
* Concurrently maintainable site infrastructure with expected availability of 99.982%

'''Tier IV Requirements'''
* Meets or exceeds all Tier I, Tier II and Tier III requirements
* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems
* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%

The Uptime Institute’s tier system allows for the following minutes of downtime annually:
* Tier I (99.671% minimum uptime) (1729 minutes maximum annual downtime)
* Tier II (99.741% minimum uptime) (1361 minutes maximum annual downtime)
* Tier III (99.982% minimum uptime) (95 minutes maximum annual downtime)
* Tier IV (99.995% minimum uptime) (26 minutes maximum annual downtime)

==See also==
* Data Center Tiers [[Data center#Data center tiers|Data center tiers]]

==References==
{{reflist}}

[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]
<=====doc_Id=====>:325
<=====title=====>:
Skyline operator
<=====text=====>:
The '''Skyline operator''' is used in a query and performs a filtering of results from a database so that it keeps only those objects that are not worse than any other.

This operator is an extension to [[SQL]] proposed by Börzsönyi et al.<ref name=borzsony2001skyline>{{cite journal|last1=Borzsonyi|first1=Stephan|last2=Kossmann|first2=Donald|last3=Stocker|first3=Konrad|title=The Skyline Operator|journal=Proceedings 17th International Conference on Data Engineering|date=2001|pages=421–430|doi=10.1109/ICDE.2001.914855}}</ref>  A classic example of application of the Skyline operator involves selecting a hotel for a holiday. The user wants the hotel to be both cheap and close to the beach. However, hotels that are close to the beach may also be expensive. In this case, the Skyline operator would only present those hotels that are not worse than any other hotel in both price and distance to the beach.

== Proposed syntax ==

To give an example in SQL: Börzsönyi et al.<ref name=borzsony2001skyline/> proposed the following syntax for the Skyline operator:

<source lang="sql">
SELECT ... FROM ... WHERE ...
GROUP BY ... HAVING ...
SKYLINE OF [DISTINCT] d1 [MIN | MAX | DIFF],
                 ..., dm [MIN | MAX | DIFF]
ORDER BY ...
</source>
where d<sub>1</sub>, ... d<sub>m</sub> denote the dimensions of the Skyline and MIN, MAX and DIFF specify whether the value in that dimension should be minimised, maximised or simply be different.

== Implementation ==
The Skyline operator can be implemented directly in SQL using current SQL constructs, however this has been shown to be very slow.<ref name=borzsony2001skyline/> Other algorithms have been proposed that make use of divide and conquer, indices,<ref name=borzsony2001skyline/> [[MapReduce]]<ref>{{cite journal|last1=Mullesgaard|first1=Kasper|last2=Pedersen|first2=Jens Laurits|last3=Lu|first3=Hua|last4=Zhou|first4=Yongluan|title=Efficient Skyline Computation in MapReduce|journal=Proc. 17th International Conference on Extending Database Technology (EDBT)|date=2014|pages=37–48|url=http://www.openproceedings.eu/2014/conf/edbt/MullesgaardPLZ14.pdf}}</ref> and [[General-purpose computing on graphics processing units|general-purpose computing on graphics cards]].<ref>{{cite journal|last1=Bøgh|first1=Kenneth S|last2=Assent|first2=Ira|last3=Magnani|first3=Matteo|title=Efficient GPU-based skyline computation|journal=Proceedings of the Ninth International Workshop on Data Management on New Hardware|date=2013|pages=5:1–5:6|doi=10.1145/2485278.2485283}}</ref> Skyline queries on data streams (i.e. continuous skyline queries) have been studied in the context of parallel query processing on multicores, owing to their wide diffusion in real-time decision making problems and data streaming analytics.<ref>{{cite journal|last1=De Matteis|first1=Tiziano|last2=Di Girolamo|first2=Salvatore|last3=Mencagli|first3=Gabriele|title=Continuous skyline queries on multicore architectures|journal=Concurrency and Computation: Practice and Experience|date=25 August 2016|volume=28|issue=12|pages=3503–3522|doi=10.1002/cpe.3866}}</ref>

==References==
<references />

[[Category:Data management]]
[[Category:Query languages]]
[[Category:Relational database management systems]]
[[Category:SQL]]


{{database-software-stub}}
<=====doc_Id=====>:328
<=====title=====>:
Couchbase Server
<=====text=====>:
{{Infobox software
| name                   = Couchbase Server
| logo                   = [[File:CouchbaseLogo.svg|224px]]
| screenshot             = Couchbase Server Screenshot.jpg
| caption                =
| developer              = [[Couchbase, Inc.]]
| released               = {{Start date|2010|08}}
| latest release version = 4.5
| latest release date    = {{release date|2016|06|22}}
| status                 = active
| programming language   = [[C++]], [[Erlang (programming language)|Erlang]], [[C (programming language)|C]],<ref>{{cite web |author= Damien Katz|url=http://damienkatz.net/2013/01/the_unreasonable_effectiveness_of_c.html |title=The Unreasonable Effectiveness of C |date= January 8, 2013 |accessdate= September 30, 2016 }}</ref> [[Go (programming language)|Go]]
| operating system       = [[Cross-platform]]
| genre                  = [[Multi-model database]] / [[Key-value database|Distributed Key-Value]] / [[Document-oriented database]]
| license                = [[Apache License]] (Open Source edition), [[Proprietary software|Proprietary]] (Free Community edition and Paid Enterprise edition)
| website                = {{URL|http://www.couchbase.com/}}
| frequently updated     = yes
}}

'''Couchbase Server''', originally known as '''Membase''', is an [[open-source]], distributed ([[shared-nothing architecture]]) [[Multi-model database|multi-model]] [[NoSQL]] [[document-oriented database]] software package that is optimized for interactive applications. These applications may serve many [[concurrent user]]s by creating, storing, retrieving, aggregating, manipulating and presenting data. In support of these kinds of application needs, Couchbase Server is designed to provide easy-to-scale key-value or JSON document access with low latency and high sustained throughput. It is designed to be [[Cluster (computing)|clustered]] from a single machine to very large-scale deployments spanning many machines.
A version originally called '''Couchbase Lite''' was later marketed as Couchbase Mobile combined with other software.

Couchbase Server provided client protocol compatibility with [[memcached]],<ref>{{cite web|url=http://code.google.com/p/memcached/wiki/NewProtocols |title=NewProtocols - memcached - Klingon - Memcached - Google Project Hosting |publisher=Code.google.com |date=2011-08-22 |accessdate=2013-06-04}}</ref> but added disk [[Persistence (computer science)|persistence]], [[data replication]], live cluster reconfiguration, rebalancing and [[multitenancy]] with [[Partition (database)|data partitioning]].

==Product history==
Membase was developed by several leaders of the [[memcached]] project, who had founded a company, NorthScale, to develop a [[key-value store]] with the simplicity, speed, and scalability of memcached, but also the storage, persistence and querying capabilities of a database. The original membase source code was contributed by NorthScale, and project co-sponsors [[Zynga]] and [[Naver Corporation]] (then known as NHN) to a new project on membase.org in June 2010.<ref>{{Cite book |title= Professional NoSQL |author= Shashank Tiwari  |publisher= John Wiley & Sons |pages= 15–16 |isbn= 9781118167809 }}</ref>

On February 8, 2011, the Membase project founders and Membase, Inc. announced a merger with CouchOne (a company with many of the principal players behind [[CouchDB]]) with an associated project merger. The merged company was called [[Couchbase, Inc.]] In January 2012, Couchbase released Couchbase Server 1.8. 
In September, 2012, [[Orbitz]] said it had changed some of its systems to use Couchbase.<ref>{{cite web |url= http://gigaom.com/cloud/balancing-oracle-and-open-source-at-orbitz/ |title= Balancing Oracle and open source at Orbitz |publisher=[[GigaOM]] |date= September 21, 2012 |accessdate= September 19, 2016 }}</ref>
On December 2012, 
Couchbase Server 2.0 (announced in July 2011) was released and included a new [[JSON]] document store, indexing and querying, incremental [[MapReduce]] and [[Replication (computing)|replication]] across [[data center]]s.<ref name="zd2">{{cite web |url= http://www.zdnet.com/couchbase-2-0-released-implements-json-document-store-7000008649/ |title= Couchbase 2.0 released; implements JSON document store |publisher= [[ZDNet]] |author=  Andrew Brust |date= December 12, 2012}}</ref><ref>{{Cite web |title= Couchbase goes 2.0, pushes SQL for NoSQL |author= Derrick Harris |date= July 29, 2011 |work= GigaOm |url= https://gigaom.com/2011/07/29/couchbase-2-0-unql-sql-nosql/ |accessdate= September 19, 2016 }}</ref>

==Architecture==
Every Couchbase node consists of a data service, index service, query service, and cluster manager component. Starting with the 4.0 release, the three services can be distributed to run on separate nodes of the cluster if needed.
In the parlance of Eric Brewer’s [[CAP theorem]], Couchbase is normally a CP type system meaning it provides [[Consistency (database systems)|consistency]] and [[Network partitioning|partition tolerance]], or it can be set up as an AP system with multiple clusters.

===Cluster manager===
The cluster manager supervises the configuration and behavior of all the servers in a Couchbase cluster. It configures and supervises inter-node behavior like managing replication streams and re-balancing operations. It also provides metric aggregation and consensus functions for the cluster, and a [[REST]]ful cluster management interface. The cluster manager uses the [[Erlang (programming language)|Erlang programming language]] and the [[Open Telecom Platform]].

====Replication and fail-over====
[[Data replication]] within the nodes of a cluster can be controlled with several parameters.
In December 2012, replication was also supported between different [[data center]]s.<ref name="zd2" />

===Data manager===
The data manager stores and retries documents in response to data operations from applications.
It asynchronously writes data to disk after acknowledging to the client.  In version 1.7 and later, applications can optionally ensure data is written to more than one server or to disk before acknowledging a write to the client.
Parameters define item ages that affect when data is persisted, and how max memory and migration from main-memory to disk is handled.
It supports working sets greater than a memory quota per "node" or "bucket".
External systems can subscribe to filtered data streams, supporting, for example, [[full text search]] indexing, [[data analytics]] or archiving.<ref>{{Cite web |url= http://blog.couchbase.com/want-know-what-your-memcached-servers-are-doing-tap-them |title= Want to know what your memcached servers are doing? Tap them |author= Trond Norbye |work= Couchbase blog |date= March 15, 2010}}</ref>

====Data format====
A document is the most basic unit of data manipulation in Couchbase Server. Documents are stored in JSON document format with no predefined schemas.

====Object-managed cache====
Couchbase Server includes a built-in multi-threaded object-managed cache that implements memcached compatible APIs such as get, set, delete, append, prepend etc.

====Storage engine ====
Couchbase Server has a tail-append storage design that is immune to data corruption, [[OOM killer]]s or sudden loss of power.  Data is written to the data file in an append-only manner, which enables Couchbase to do mostly sequential writes for update, and provide an optimized access patterns for disk I/O.

=== Performance ===
A performance benchmark done by [[Altoros]] in 2012, compared Couchbase Server with other technologies.<ref>{{cite web |url= http://www.couchbase.com/nosql-resources/presentations/benchmarking-couchbase%5B2%5D.html |title= Benchmarking Couchbase |author= Frank Weigel |publisher=Couchbase |date= October 30, 2012 |accessdate= September 30, 2016 }}</ref>
[[Cisco Systems]] published a benchmark that measured the latency and throughput of Couchbase Server with a mixed workload in 2012.<ref>{{cite web |url= http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |title=Cisco and Solarflare Achieve Dramatic Latency Reduction for Interactive Web Applications with Couchbase, a NoSQL Database |publisher=[[Cisco Systems]] |date= June 18, 2012 |archivedate=  August 13, 2012 |archiveurl= https://web.archive.org/web/20120813162214/http://www.cisco.com/en/US/prod/collateral/switches/ps9441/ps9670/white_paper_c11-708169.pdf |accessdate= October 7, 2016 }}</ref>

== Licensing and support ==
Couchbase Server is a packaged version of Couchbase's [[open source software]] technology and is available in a community edition without recent bug fixes with Apache 2.0 license.<ref>{{cite web |url= http://developer.couchbase.com/open-source-projects |title=Couchbase Open Source Projects |work= Couchbase web site |accessdate= October 7, 2016 }}</ref> and an edition for commercial use.<ref>{{cite web|url=http://www.couchbase.com/couchbase-server/editions|title=Couchbase Server Editions|publisher= Couchbase }}</ref> 
Couchbase Server builds are available for Ubuntu, Debian, Red Hat, SUSE, Oracle Linux, [[Microsoft Windows]] and Mac OS X operating systems.

Couchbase has supported software developers' kits for the programming languages [[.NET Framework|.Net]], [[PHP]], [[Ruby (programming language)|Ruby]], [[Python (programming language)|Python]], [[C (programming language)|C]], [[Node.js]], [[Java (programming language)|Java]], and [[Go (programming language)|Go]].

==N1QL==
A [[query language]] called the non-first normal form query language, N1QL (pronounced nickel), is used for manipulating the JSON data in Couchbase, just like SQL manipulates data in RDBMS. It has SELECT, INSERT, UPDATE, DELETE, MERGE statements to operate on JSON data.
It was announced in March 2015 as "SQL for documents".<ref>{{Cite web |title= Ssssh!  don’t tell anyone but Couchbase is a serious contender: Couchbase Live Europe 2015 |author= Andrew Slater |date= March 24, 2015 |accessdate= September 19, 2016 }}</ref>

The N1QL [[data model]] is [[Database normalization#Non-first normal form .28NF.C2.B2 or N1NF.29|non-first normal form]] (N1NF) with support for nested attributes and domain-oriented [[Database normalization|normalization]].  The N1QL data model is also a proper superset and generalization of the [[relational model]].

===Example===
<source lang="json">
{
  "email":"testme@gmail.com",
  "friends":[
            {"name":"rick"},
            {"name":"cate"}
           ]
}
</source>

;Like Query: {{code|2=sql|SELECT * FROM `bucket` WHERE LIKE "%@gmail.com";}}

;Array Query: {{code|2=sql|1=SELECT * FROM `bucket` WHERE ANY x IN friends SATISFIES x.name = "cate" END;}}

==Bibliography==
* {{cite book |last=Brown |first=MC |editor-first=|editor-last=|title=Getting Started with Couchbase Server (1st edition) |publisher=O'Reilly Media |date=June 22, 2012 |page=88 | isbn=978-1449331061}}
* {{citation
| first1    = David
| last1     = Ostrovsky
| first2   = Mohammed
| last2   = Haji
| first3  = Yaniv
| last3   = Rodenski
| date      = November 26, 2015
| title     = Pro Couchbase Server 2nd ed.
| edition   = 2nd
| publisher = [[Apress]]
| page     = 349
| isbn      = 978-1484211861
}}
* {{citation
| first1    = Henry
| last1     = Potsangbam
| date      = November 23, 2015
| title     = Learning Couchbase
| edition   = 1st
| publisher = [[Packt]]
| page     = 202
| isbn      = 978-1785288593
}}
* {{citation
| first1    = Deepak
| last1     = Vohra
| date      = August 3, 2015
| title     = Pro Couchbase Development: A NoSQL Platform for the Enterprise
| edition   = 1st
| publisher = [[Apress]]
| page     = 331
| isbn      = 978-1484214350
}}

==References==
{{Reflist}}

==External links==
*{{Official website}}

[[Category:Free database management systems]]
[[Category:Distributed computing architecture]]
[[Category:NoSQL]]
[[Category:Cross-platform software]]
[[Category:Structured storage]]
[[Category:Client-server database management systems]]
[[Category:Database-related software for Linux]]
[[Category:Applications of distributed computing]]
[[Category:Databases]]
[[Category:Data management]]
[[Category:Distributed data stores]]
<=====doc_Id=====>:331
<=====title=====>:
Category:Data management software
<=====text=====>:
[[Software]], typically proprietary products or open-source projects, with a primary purpose of [[data management]].  [[:Category:Database management systems by license|Database management system software]] could be considered a related category, though those will typically exist for the purpose of managing a [[database]] in a particular structure (i.e. relational, object-oriented). 
[[Category:Data management|Software]]
[[Category:Application software]]
<=====doc_Id=====>:334
<=====title=====>:
Data philanthropy
<=====text=====>:
{{Orphan|date=March 2016}}

'''Data philanthropy''' describes a form of collaboration in which private sector companies share data for public benefit.<ref name="Pawelke">Pawelke, A. and Tatevossian, A. (2013, May 8) [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data philanthropy: where are we now?] United Nations Global Pulse.</ref> There are multiple uses of data philanthropy being explored from humanitarian, corporate, human rights, and academic use. Since the introduction of this term the [[United Nations Global Pulse]] has began pushing for a global “data philanthropy movement.”<ref name= "Coren">Coren, M. (2011, December 9) [http://www.fastcoexist.com/1678963/data-philanthropy-open-data-for-world-changing-solutions Data Philanthropy: Open data for world-changing solutions] Fast Company.</ref>

== Definition==
A large amount of data collected from the Internet comes from [[user-generated content]]. This includes blogs, posts on social networks, and information submitted in forms. Besides user-generated data, corporations are also currently [[data mining]] data from consumers in order to understand customers, identify new markets, and make investment decisions. Kirkpatrick the Director at United Nations Global Pulse labels this data “massive passive data” or “data exhaust.”<ref name="Kirkpatrick">Kirkpatrick, R. (2011, September 20). [http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business/ Data philanthropy is good for business] Forbes.</ref> Data philanthropy is the idea that something positive can come from this overload of data. Data philanthropy is defined as the private sector sharing this data in ways that the public can benefit.<ref name="Pawelke" /> The term philanthropy helps to emphasis that [[data sharing]] is a positive act and that the shared data is a public good.<ref name="Kirkpatrick" />

== Challenges ==
A challenge that comes with sharing data is the [[Internet privacy]] of the user whose data is being used. Mathematical techniques ([[differential privacy]] and space time boxes) have been introduced in order to make personal data accessible, while providing the users providing such data with anonymity. But even if these algorithms work there is always the possibility and fear of re-identification.<ref name="Pawelke" />
 
The other challenge is convincing corporations to share their data. The big data corporations collect provides them with market competitiveness. They are able to infer meaning regarding [[consumer behavior]]. The fear is that by sharing all their information, they may lose their competitive edge.<ref name="Pawelke" />

== Sharing strategies ==
The goal of data philanthropy is to create a global data commons where companies, governments, and individuals can contribute anonymous, aggregated datasets.<ref name="Coren" /> The United Nations Global Pulse offers four different tactics that companies can use to share their data that preserve consumer anonymity.  These include:<ref name="Pawelke" />
# Share aggregated and derived data sets for analysis under nondisclosure agreements (NDA)
# Allow researchers to analyze data within the private company’s own network, under NDA
# Real-Time Data Commons: data pooled and aggregated between multiple companies of the same industry to protect competitiveness
# Public/Private Alerting Network: companies mine data behind their own firewalls and share indicators

By providing these four tactics United Nations Global Pulse hopes to provide initiative and options for companies to share their data with the public.

== Digital disease detection ==
Data philanthropy has led to advancements in the field of health and wellness. By using data gathered from social media, cell phones, and other communication modes health researchers have been able to track the spread of diseases.<ref name="Schmidt">Schmidt, C. (2012). [http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3261963/ Trending Now: Using Social Media to Predict and Track Disease Outbreaks.] Environ Health Perspect, 120(1), A30–a33-A30–a33.</ref>

In the United States [[HealthMap]], a freely available website and mobile app software is using data philanthropy related tactics to track the outbreak of diseases. HealthMap analyzes data from publicly available media sources such as news websites, government alerts, and social media sites like Twitter for outbreaks of various illnesses around the world.<ref name="Schmidt" /><ref name="Reddy">Reddy, E. (2015, July 14). [https://blog.twitter.com/2015/twitter-data-public-health Using Twitter data to study the world's health] Twitter.</ref>  The creators of HealthMap have another website, Flu Near You, which allows users to report their own health status on a weekly basis. Traditional flu surveillance can take up to 2 weeks to confirm outbreaks.<ref name= "Schmidt" /> Doctors must wait for virological test to confirm the outbreak before reporting it to the Centers for Disease Control. This form of data philanthropy allows for up to date information regarding various health concerns by using publicly available information gathered from news outlets, government alerts, and social media sites. It is the data gathered on social media sites, where users are not aware their data is being mined that leads to HealthMap and Flue Near You being considered data philanthropy.<ref name="Schmidt" /> 
 
The [[Centers for Disease Control and Prevention]] collaborated with Google and launched [[Google Flu Trends]] in 2008, a website that tracks flu-related searches and user location to track the spread of the flu. Users can visit the website to compare the amount of flu-related search activity against the reported numbers of flu outbreaks on a graphic map. The difficulty with this method of tracking is that Google searched are sometimes performed due to curiosity rather than because an individual is suffering from the flu. According to Ashley Fowlkes, an epidemiologist in the CDC Influenza division, “the Google Flu Trends system tries to account for that type of media bias by modeling search terms over time to see which ones remain stable.”<ref name="Schmidt" /> Google Flu Trends is not longer publishing current flu estimates on the public website. Visitors to the site can still view and download previous estimates. Current data can be shared with verified researchers.<ref name="O'Connor'">O'Connor, F. (2015, August 20). [http://www.pcworld.com/article/2974153/websites/google-flu-trends-calls-out-sick-indefinitely.html Google Flu Trends calls out sick, indefinitely] PC World.</ref>
 
A study by Harvard School of Public Health (HSPH) released in the October 12, 2012 issues of the journal Science discussed how phone data helped curb the spread of malaria in Kenya. The researchers mapped phone calls and texts made by 14,816,521 Kenyan mobile phone subscribers.<ref name= "Datz">Datz, T. (2012, October 11). [http://www.hsph.harvard.edu/news/press-releases/cell-phone-data-malaria/ Using cell phone data to curb the spread of malaria.] Harvard Chan.</ref> When individuals left their primary living location the destination and length of journey was calculated. This data was then compared to a 2009 malaria prevalence map to estimate the disease’s commonness in each location.  Combining all this information the researchers can estimate the probability of an individual carrying malaria and map the movement of the disease. This research, a result of data philanthropy, can be used to track the spread of similar diseases.<ref name="Datz" />

==Application in various fields==
Through data philanthropy ‘[[big data]]’ corporations such as [[social networking sites]], telecommunication companies, [[search engines]] amongst others, collect and make user generated information available to a data sharing system. This also permits institutions to give back to a beneficial cause. With the onset of [[technological]] advancements, sharing data on a global scale and an in-depth analysis of these data structures could alter the reaction towards certain occurrences, be it [[natural disaster]]s, [[epidemics]], worldwide [[economic]] problems and many other events. Some analyst have argued<ref name="Forbes">[http://www.forbes.com/sites/oreillymedia/2011/09/20/data-philanthropy-is-good-for-business Data Philanthropy is Good for Business], by Robert Kirkpatrick, Forbes, 2011-09-20</ref> that this aggregated Information is beneficial for the common good and can lead to developments in [[research]] and [[data]] production in a range of varied fields.<ref name="Forbes"/>

===Humanitarian aid===
Calling patterns of [[mobile phone]] users can determine the [[socioeconomic]] standings of the populace which can be used to deduce “its access to housing, education, healthcare, and basic services such as water and electricity”.<ref name="Forbes"/> Researchers from Columbia University and Karolinska Institute utilize information from [[mobile phone]] providers, in order to assist in the dispersal of resources by deducing the movement of those displaced by natural disasters. Big data can also provide information on looming disasters and can assist relief organizations in rapid response and locating displaced individuals. By analyzing certain patterns within this ‘big data’, could successively transform the response to destructive occurrences like natural disasters, [[outbreak]]s of diseases and global economic distress, by employing real-time information to achieve a comprehension of the welfare of individuals. Corporations utilize digital services, such as human sensor systems to detect and solve impending problems within [[communities]]. This is a strategy implemented by the private sector in order to protect its citizens by anonymously dispersing customer information to the public sector, whilst also ensuring the protection of their privacy.<ref name="Forbes"/>

===Impoverished areas===
[[Poverty]] still remains a worldwide issue with over 2.5 billion people<ref name="Smart Data Collective">[http://www.smartdatacollective.com/rick-delgado/200566/lifting-how-big-data-can-help-eliminate-poverty Lifting Up: How Big Data Can Help Eliminate Poverty], by Rick Delgado, Smart Data Collection , 2014-05-23</ref> currently impoverished. Accumulating accurate data has been a complex issue but developments in [[technology]] and utilising 'big data',<ref name="Smart Data Collective" /> is one solution for improving this situation. Statistics indicate the widespread use of mobile phones, even within impoverished communities. This availability could prove vital in gathering data on populations living in poverty. Additional data can be collected through [[Internet access]], social media, utility payments and [[governmental]] statistics. Data-driven activities can lead to the cumulation of ‘big data’, which in turn can assist international non-governmental organization in documenting and evaluating the needs of underprivileged populations. Through data philanthropy, [[NGO]]’s can distribute information whilst cooperating with governments and private companies.<ref name="Smart Data Collective" />

===Corporate===
Data philanthropy incorporates aspects of social philanthropy by permitting  [[corporations]] to create profound impacts through the act of giving back by dispersing proprietary datasets.<ref name="Irevolution">[http://irevolution.net/2012/06/04/big-data-philanthropy-for-humanitarian-response/Big Data Philanthropy for Humanitarian Response], by Irevolution, 2012-07-04</ref> The [[public sector]], is faced with an unequal and limited access to the frequency of data and they also produce, collect and preserve information, which has proven to be an essential asset. Company’s track and analyze users online activities, so as to gain more insight into their needs in relation to new products and services.<ref>[https://hbr.org/2014/07/sharing-data-is-a-form-of-corporate-philanthropy/Sharing Data Is a Form of Corporate Philanthropy], by Matt Stempeck,Harvard Business Review 2014-07-24</ref>
These companies view the welfare of the population as a vital key to the expansion and progression of businesses by using their data to place a spotlight on the plight of global citizens.<ref name="Forbes" />[[Expert]]s in the private sector contend the importance of merging various data streams such as retail, mobile phone and social media data to create necessary solutions to handle global issues. Despite the inevitable risk of sharing private information, it works in a beneficial manner and serves the interest of the public.<ref>[https://hbr.org/2013/03/a-new-type-of-philanthropy-don&cm_sp=Article-_-Links-_-Top%20of%20Page%20Recirculation A New Type of Philanthropy: Donating Data], by Robert Kirkpatrick,Harvard Business Review 2013-03-21</ref> The digital revolution causes an extensive production of ‘big data’ that is user-generated and available on the web. Corporations accumulate information on customer preferences through the digital services they utilize and products they purchase, in order to gain a clear insight on their clientele and future market opportunities.<ref name="Forbes" /> However the rights of individuals concerning privacy and ownership of data are a controversial issue as governments and other institutions can use this collective data for other unethical purposes. Companies monitor and probe consumer online activities in order to better comprehend and develop tailored needs for their clientele and in turn increase their profits.<ref name="Jim Fruchterman">[https://hbr.org/2013/03/big-data-means-more-than-big-p Big Data Means More Than Big Profits], by Jim Fruchterman, Harvard Business Review, 2013-03-19</ref>

===Academia===
Data philanthropy plays an important role in [[academia]]. Researchers encounter countless obstacles whilst attempting to access data. This data is available to a limited number of researchers with sole access to restricted resources who are authorized to utilize this information; like social media streams enabling them to produce more [[knowledge]] and develop new studies. For example, Twitter markets access to its real-time APIs at exorbitant prices, which often surpasses the budgets of most researchers. 'Data Grants’<ref name="Jim Fruchterman" /> is a trial program created by Twitter that provides a selective number of academics and researchers with access to real-time databases in order to garner more knowledge. They apply to gain entry into vast data downloads, on specific topics.<ref name="Jim Fruchterman" />

===Human rights===
Data philanthropy aids the human rights movement, by assisting in the dispersal of evidence for truth commissions and war crimes tribunals. Proponents of human rights accumulate data on abuse occurring within states, which is then used for scientific analysis and propels awareness and action. For example, non-profit organizations compile data from Human Rights monitors in war zones in order to assist the UN High Commissioner for Human Rights. It uncovers inconsistencies in the number of casualties of war, which in turn leads to international attention and exerts influence on discussions relating to global policy.<ref name="Jim Fruchterman" />

==See also==
* [[Big Data]]

==References==
<references />

== External links ==
* [http://www.unglobalpulse.org/data-philanthropy-where-are-we-now Data Philanthropy, where are we now?] in UN Global Pulse blog by Adreas Pawelke and Anoush Rima Tatevossian (2013-05-08).

[[Category:Big data| ]]
[[Category:Data management]]
<=====doc_Id=====>:337
<=====title=====>:
NoSQL
<=====text=====>:
{{Redirect|Structured storage|the Microsoft technology also known as structured storage|COM Structured Storage}}
A '''NoSQL''' (originally referring to "non SQL", "non relational" or "not only SQL")<ref>http://nosql-database.org/ "NoSQL DEFINITION: Next Generation Databases mostly addressing some of the points: being non-relational, distributed, open-source and horizontally scalable"</ref> database provides a mechanism for [[Computer data storage|storage]] and [[data retrieval|retrieval]] of data which is modeled in means other than the tabular relations used in [[relational database]]s. Such databases have existed since the late 1960s, but did not obtain the "NoSQL" moniker until a surge of popularity in the early twenty-first century,{{r|leavitt}} triggered by the needs of [[Web 2.0]] companies such as [[Facebook]], [[Google]], and [[Amazon.com]].<ref>{{cite conference |title=History Repeats Itself: Sensible and NonsenSQL Aspects of the NoSQL Hoopla |first=C. |last=Mohan |conference=Proc. 16th Int'l Conf. on Extending Database Technology |year=2013 |url=http://openproceedings.eu/2013/conf/edbt/Mohan13.pdf}}</ref><ref>http://www.eventbrite.com/e/nosql-meetup-tickets-341739151 "Dynamo clones and BigTables"</ref><ref>http://www.wired.com/2012/01/amazon-dynamodb/ "Amazon helped start the “NoSQL” movement."</ref> NoSQL databases are increasingly used in [[big data]] and [[real-time web]] applications.<ref>{{cite web|url= http://db-engines.com/en/blog_post/23 |title= RDBMS dominate the database market, but NoSQL systems are catching up |publisher= DB-Engines.com |date= 21 Nov 2013 |accessdate= 24 Nov 2013 }}</ref>   NoSQL systems are also sometimes called "Not only SQL" to emphasize that they may support [[SQL]]-like query languages.<ref>{{cite web|url=http://searchdatamanagement.techtarget.com/definition/NoSQL-Not-Only-SQL|title=NoSQL (Not Only SQL)|quote=NoSQL database, also called Not Only SQL}}</ref><ref>{{cite web | url = http://martinfowler.com/bliki/NosqlDefinition.html | title = NosqlDefinition | first = Martin | last = Fowler | authorlink = Martin Fowler | quote = many advocates of NoSQL say that it does not mean a "no" to SQL, rather it means Not Only SQL }}</ref>

Motivations for this approach include: simplicity of design, simpler [[Horizontal scaling#Horizontal and vertical scaling|"horizontal" scaling]] to [[cluster computing|clusters]] of machines  (which is a problem for relational databases),<ref name="leavitt">{{cite journal |first=Neal |last=Leavitt |title=Will NoSQL Databases Live Up to Their Promise? |journal=[[IEEE Computer]] |year=2010 |url=http://www.leavcom.com/pdf/NoSQL.pdf}}</ref> and finer control over availability. The data structures used by NoSQL databases (e.g. key-value, wide column, graph, or document) are different from those used by default in relational databases, making some operations faster in NoSQL. The particular suitability of a given NoSQL database depends on the problem it must solve.  Sometimes the data structures used by NoSQL databases are also viewed as "more flexible" than relational database tables.<ref>http://www.allthingsdistributed.com/2012/01/amazon-dynamodb.html "Customers like SimpleDB’s table interface and its flexible data model. Not having to update their schemas when their systems evolve makes life much easier"</ref>

Many NoSQL stores compromise [[consistency (database systems)|consistency]] (in the sense of the [[CAP theorem]]) in favor of availability, partition tolerance, and speed. Barriers to the greater adoption of NoSQL stores include the use of low-level query languages (instead of SQL, for instance the lack of ability to perform ad-hoc JOINs across tables), lack of standardized interfaces, and huge previous investments in existing relational databases.<ref>{{cite web
| url         = http://www.journalofcloudcomputing.com/content/pdf/2192-113X-2-22.pdf
| title       = Data management in cloud environments: NoSQL and NewSQL data stores
| first1 = K. | last1 = Grolinger | first2 = W. A. | last2 = Higashino | first3 = A. | last3 = Tiwari | first4 = M. A. M. | last4 = Capretz
| date = 2013
| publisher   = Aira, Springer
| accessdate  = 8 Jan 2014
}}
</ref>
Most NoSQL stores lack true [[ACID]] transactions, although a few databases, such as [[MarkLogic]], [[Aerospike database|Aerospike]], FairCom [[c-treeACE]], Google [[Spanner (database)|Spanner]] (though technically a [[NewSQL]] database), Symas [[Lightning Memory-Mapped Database|LMDB]], and [[OrientDB]] have made them central to their designs. (See [[#ACID and JOIN Support|ACID and JOIN Support]].)

Instead, most NoSQL databases offer a concept of "eventual consistency" in which database changes are propagated to all nodes "eventually" (typically within milliseconds) so queries for data might not return updated data immediately or might result in reading data that is not accurate, a problem known as stale reads.<ref>https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads</ref>  Additionally, some NoSQL systems may exhibit lost writes and other forms of [[data loss]].<ref>Martin Zapletal: Large volume data analysis on the Typesafe Reactive Platform, ScalaDays 2015, [http://www.slideshare.net/MartinZapletal/zapletal-martinlargevolumedataanalytics Slides]</ref> Fortunately, some NoSQL systems provide concepts such as [[write-ahead logging]] to avoid data loss.<ref>http://www.dummies.com/how-to/content/10-nosql-misconceptions.html "NoSQL databases lose data" section</ref> For [[distributed transaction processing]] across multiple databases, data consistency is an even bigger challenge that is difficult for both NoSQL and relational databases. Even current relational databases "do not allow referential integrity constraints to span databases."<ref>https://iggyfernandez.wordpress.com/2013/07/28/no-to-sql-and-no-to-nosql/</ref> There are few systems that maintain both [[ACID]] transactions and [[X/Open XA]] standards for distributed transaction processing.

== History ==
The term ''NoSQL'' was used by Carlo Strozzi in 1998 to name his lightweight, [[Strozzi NoSQL (RDBMS)|Strozzi NoSQL open-source relational database]] that did not expose the standard [[SQL|Structured Query Language]] (SQL) interface, but was still relational.<ref name=":0">{{cite web
| url         = http://publications.lib.chalmers.se/records/fulltext/123839.pdf
| title       = Investigating storage solutions for large data: A comparison of well performing and scalable data storage solutions for real time extraction and batch insertion of data
| first       = Adam
| last        = Lith
| first2 = Jakob | last2 = Mattson
| date        = 2010
| publisher   = Department of Computer Science and Engineering, Chalmers University of Technology
| location    = Göteborg
| page        = 70
| accessdate  = 12 May 2011
| quote       = Carlo Strozzi first used the term NoSQL in 1998 as a name for his open source relational database that did not offer a SQL interface[...]
}}
</ref>  His NoSQL RDBMS is distinct from the circa-2009 general concept of NoSQL databases.  Strozzi suggests that, because the current NoSQL movement "departs from the relational model altogether, it should therefore have been called more appropriately 'NoREL'",<ref>{{cite web|url=http://www.strozzi.it/cgi-bin/CSA/tw7/I/en_US/nosql/Home%20Page |title=NoSQL Relational Database Management System: Home Page |publisher=Strozzi.it |date=2 October 2007 |accessdate=29 March 2010}}</ref> referring to 'No Relational'.

Johan Oskarsson of [[Last.fm]] reintroduced the term ''NoSQL'' in early 2009 when he organized an event to discuss "open source [[distributed database|distributed, non relational databases]]".<ref>{{cite web|url= http://blog.sym-link.com/2009/05/12/nosql_2009.html |title= NoSQL 2009 |publisher= Blog.sym-link.com |date= 12 May 2009 |accessdate= 29 March 2010 }}</ref> The name attempted to label the emergence of an increasing number of non-relational, distributed data stores, including open source clones of Google's BigTable/MapReduce and Amazon's Dynamo. Most of the early NoSQL systems did not attempt to provide [[ACID|atomicity, consistency, isolation and durability]] guarantees, contrary to the prevailing practice among relational database systems.<ref>{{cite web|url= http://databases.about.com/od/specificproducts/a/acid.htm |title= The ACID Model| first = Mike | last = Chapple }}</ref>

Based on 2014 revenue, the NoSQL market leaders are [[MarkLogic]], [[MongoDB]], and [[Datastax]].<ref>{{cite web|accessdate=2015-11-17|url=http://wikibon.com/hadoop-nosql-software-and-services-market-forecast-2013-2017/|title=Hadoop-NoSQL-rankings}}</ref> Based on 2015 popularity rankings, the most popular NoSQL databases are [[MongoDB]], [[Apache Cassandra]], and [[Redis]].<ref>{{cite web|accessdate=2015-07-31|url=http://db-engines.com/en/ranking|title=DB-Engines Ranking}}</ref>

== Types and examples of NoSQL databases ==
There have been various approaches to classify NoSQL databases, each with different categories and subcategories, some of which overlap. What follows is a basic classification by data model, with examples:
* '''[[Column (data store)|Column]]''': [[Accumulo]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[HBase]], [[Vertica]], [[SAP HANA]]
* '''[[Document-oriented database|Document]]''': [[Apache CouchDB]], [[Clusterpoint]], [[Couchbase]], [[DocumentDB]], [[HyperDex]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[OrientDB]], [[Qizx]], [[RethinkDB]]
* '''[[Key-value store|Key-value]]''': [[Aerospike database|Aerospike]], [[Couchbase]], [[Dynamo (storage system)|Dynamo]], FairCom [[c-treeACE]], [[FoundationDB]], [[HyperDex]], [[MemcacheDB]], [[MUMPS]], [[Oracle NoSQL Database]], [[OrientDB]], [[Redis]], [[Riak]], [[Berkeley DB]]
* '''[[Graph database|Graph]]''': [[AllegroGraph]], ArangoDB, [[InfiniteGraph]], [[Apache Giraph]], [[MarkLogic]], [[Neo4J]], [[OrientDB]], [[Virtuoso Universal Server|Virtuoso]], [[Stardog]]
* '''[[Multi-model database|Multi-model]]''': Alchemy Database, ArangoDB, CortexDB, [[Couchbase]], [[FoundationDB]], [[MarkLogic]], [[OrientDB]]

A more detailed classification is the following, based on one from Stephen Yen:<ref>{{cite web|url=https://dl.dropboxusercontent.com/u/2075876/nosql-steve-yen.pdf|format=PDF|title=NoSQL is a Horseless Carriage|last=Yen|first=Stephen|publisher=NorthScale|accessdate=2014-06-26}}.</ref>

{| style="text-align: left;" class="wikitable sortable"
|-
! Type !! Examples of this type
|-
| Key-Value Cache || [[Oracle Coherence|Coherence]], [[IBM WebSphere eXtreme Scale|eXtreme Scale]], [[GigaSpaces]],  GemFire, [[Hazelcast]], [[Infinispan]], JBoss Cache, [[Memcached]], Repcached, [[Terracotta, Inc.|Terracotta]], [[Velocity (memory cache)|Velocity]]
|-
| Key-Value Store || Flare, Keyspace, RAMCloud, SchemaFree, [[Hyperdex]], [[Aerospike database|Aerospike]]
|-
| Key-Value Store (Eventually-Consistent) || DovetailDB, [[Oracle NoSQL Database]], [[Dynamo (storage system)|Dynamo]], [[Riak]], Dynomite, MotionDb, [[Voldemort (distributed data store)|Voldemort]], SubRecord
|-
| Key-Value Store (Ordered) || Actord, [[FoundationDB]], Lightcloud, [[Lightning Memory-Mapped Database|LMDB]], Luxio, [[MemcacheDB]],  NMDB, Scalaris, TokyoTyrant
|-
| Data-Structures Server || [[Redis]]
|-
| Tuple Store || [[Jini|Apache River]], Coord, [[GigaSpaces]]
|-
| Object Database || DB4O, [[Objectivity/DB]], [[Perst]], Shoal, [[Zope Object Database|ZopeDB]]
|-
| Document Store || [[Clusterpoint]], [[Couchbase]], [[CouchDB]], [[DocumentDB]], [[Lotus Notes|IBM Domino]], [[MarkLogic]], [[MongoDB]], [[Qizx]], [[RethinkDB]], [[XML database|XML-databases]]
|-
| [[Wide column store|Wide Column Store]] || [[BigTable]], [[Apache Cassandra|Cassandra]], [[Druid (open-source data store)|Druid]], [[Apache HBase|HBase]], [[Hypertable]], KAI, KDI, OpenNeptune, Qbase
|}

[[Correlation database]]s are model-independent, and instead of row-based or column-based storage, use value-based storage.

=== Key-value store ===
{{main|Key-value database}}
Key-value (KV) stores use the [[associative array]] (also known as a map or dictionary) as their fundamental data model. In this model, data is represented as a collection of key-value pairs, such that each possible key appears at most once in the collection.<ref>{{cite web
| accessdate =1 January 2012
| publisher = Stackexchange
| location = http://dba.stackexchange.com/questions/607/what-is-a-key-value-store-database
| title = Key Value stores and the NoSQL movement
| author = Sandy
| date = 14 January 2011
| url = http://dba.stackexchange.com/a/619
| quote = Key-value stores allow the application developer to store schema-less data. This data usually consists of a string that represents the key, and the actual data that is considered the value in the "key-value" relationship. The data itself is usually some kind of primitive of the programming language (a string, an integer, or an array) or an object that is being marshaled by the programming language's bindings to the key-value store. This structure replaces the need for a fixed data model and allows proper formatting.}}</ref><ref>{{cite web
| accessdate =1 January 2012
| publisher = Marc Seeger
| location = http://blog.marc-seeger.de/2009/09/21/key-value-stores-a-practical-overview/
| title = Key-Value Stores: a practical overview
| first = Marc | last = Seeger
| date = 21 September 2009
| url = http://blog.marc-seeger.de/assets/papers/Ultra_Large_Sites_SS09-Seeger_Key_Value_Stores.pdf
| quote = Key-value stores provide a high-performance alternative to relational database systems with respect to storing and accessing data. This paper provides a short overview of some of the currently available key-value stores and their interface to the Ruby programming language.}}</ref>

The key-value model is one of the simplest non-trivial data models, and richer data models are often implemented as an extension of it. The key-value model can be extended to a discretely ordered model that maintains keys in [[Lexicographical order|lexicographic order]]. This extension is computationally powerful, in that it can efficiently retrieve selective key ''ranges''.<ref>{{cite web
| accessdate =8 May 2014
| publisher = Ilya Katsov
| title = NoSQL Data Modeling Techniques 
| first = Ilya | last = Katsov
| date = 1 March 2012
| url = http://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/}}</ref>

Key-value stores can use [[consistency model]]s ranging from [[eventual consistency]] to [[serializability]]. Some databases support ordering of keys. There are various hardware implementations, and some users maintain data in memory (RAM), while others employ [[solid-state drive]]s or [[hard disk drive|rotating disks]].

Examples include [[Oracle NoSQL Database]], [[Redis]], and [[dbm]].

=== Document store ===
{{main|Document-oriented database|XML database}}
The central concept of a document store is the notion of a "document". While each document-oriented database implementation differs on the details of this definition, in general, they all assume that documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include XML, [[YAML]], and [[JSON]] as well as binary forms like [[BSON]].  Documents are addressed in the database via a unique ''key'' that represents that document. One of the other defining characteristics of a document-oriented database is that in addition to the key lookup performed by a key-value store, the database offers an API or query language that retrieves documents based on their contents.

Different implementations offer different ways of organizing and/or grouping documents:
* Collections
* Tags
* Non-visible metadata
* Directory hierarchies

Compared to relational databases, for example, collections could be considered analogous to tables and documents analogous to records. But they are different: every record in a table has the same sequence of fields, while documents in a collection may have fields that are completely different.

=== Graph ===
{{main|Graph database}}

This kind of database is designed for data whose relations are well represented as a [[graph (discrete mathematics)|graph]] consisting of elements interconnected with a finite number of relations between them. The type of data could be social relations, public transport links, road maps or network topologies.

; Graph databases and their query language
{| style="text-align: left;" class="wikitable sortable"
 ! Name !! Language(s) !! Notes
 |-
 | [[AllegroGraph]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store
 |-
 | [[DEX (Graph database)|DEX/Sparksee]] || [[C++]], [[Java (programming language)|Java]], [[.NET Framework|.NET]], [[Python (programming language)|Python]] || [[Graph database]]
 |-
 | [[FlockDB]] || [[Scala (programming language)|Scala]] || [[Graph database]]
 |-
 | [[IBM DB2]] || [[SPARQL]] || [[Resource Description Framework|RDF]] triple store added in DB2 10
 |-
 | [[InfiniteGraph]] || [[Java (programming language)|Java]] || [[Graph database]]
 |-
 | [[MarkLogic]] || [[Java (programming language)|Java]], [[JavaScript]], [[SPARQL]], [[XQuery]] || Multi-model [[Document-oriented database|document database]] and [[Resource Description Framework|RDF]] triple store
 |-
 | [[Neo4j]] || [[Cypher Query Language|Cypher]] || [[Graph database]]
 |-
 | [[Ontotext|OWLIM]] || [[Java (programming language)|Java]], [[SPARQL|SPARQL 1.1]]|| [[Resource Description Framework|RDF]] triple store
 |-
 |-
 | [[Oracle Database|Oracle]] || [[SPARQL|SPARQL 1.1]] || [[Resource Description Framework|RDF]] triple store added in 11g
 |-
 | [[OrientDB]] || [[Java (programming language)|Java]], SQL || Multi-model [[Document-oriented database|document]] and [[graph database]]
 |-
 | [[sqrrl|Sqrrl Enterprise]] || [[Java (programming language)|Java]] || [[Graph database]]
 |-
 | [[Virtuoso Universal Server|OpenLink Virtuoso]] || [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]] || [[Middleware]] and [[database engine]] hybrid
 |-
 | [[Stardog]] || [[Java (programming language)|Java]], [[SPARQL]] || [[Graph database]]
|}

=== Object database ===
{{main|Object database}}
* [[db4o]]
* [[Gemstone (database)|GemStone/S]]
* [[InterSystems Caché]]
* [[JADE (programming language)|JADE]]
* [[ObjectDatabase++]]
* [[ObjectDB]]
* [[Objectivity/DB]]
* [[ObjectStore]]
* [[Odaba|ODABA]]
* [[Perst]]
* [[Virtuoso Universal Server|OpenLink Virtuoso]]
* [[Versant Object Database]]
* [[ZODB]]

=== Tabular ===
* [[Apache Accumulo]]
* [[BigTable]]
* [[HBase|Apache Hbase]]
* [[Hypertable]]
* [[Mnesia]]
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Tuple store ===
* [[Apache River]]
* [[GigaSpaces]]
* [[Tarantool]]
* [[TIBCO Software|TIBCO]] ActiveSpaces
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Triple/quad store (RDF) database ===
{{main|Triplestore|Named graph}}
* [[AllegroGraph]]
* [[Jena (framework)|Apache JENA]] (It is a framework, not a database)
* [[MarkLogic]]
* [[Ontotext|Ontotext-OWLIM]]
* [[Oracle NoSQL Database|Oracle NoSQL database]]
* [[Virtuoso Universal Server]]
* [[Stardog]]

=== Hosted ===
* [[Amazon DynamoDB]]
* [[Amazon SimpleDB]]
* [[Appengine|Datastore on Google Appengine]]
* [[Clusterpoint|Clusterpoint database]]
* [[Cloudant|Cloudant Data Layer (CouchDB)]]
* [[Freebase (database)|Freebase]]
* [[Microsoft Azure#Table Service|Microsoft Azure Tables]]<ref>http://azure.microsoft.com/en-gb/services/storage/tables/</ref>
* [[DocumentDB|Microsoft Azure DocumentDB]]<ref>http://azure.microsoft.com/en-gb/services/documentdb/</ref>
* [[Virtuoso Universal Server|OpenLink Virtuoso]]

=== Multivalue databases ===
* D3 [[Pick database]]
* [[Extensible Storage Engine]] (ESE/NT)
* [[InfinityDB]]
* [[InterSystems Caché]]
* jBASE [[Pick database]]
* [[Northgate Information Solutions]] Reality, the original Pick/MV Database
* [[OpenQM]]
* Revelation Software's [[OpenInsight]]
* [[Rocket U2]]

=== Multimodel database ===

* [[Couchbase]]
* [[FoundationDB]]
* [[MarkLogic]]
* [[OrientDB]]

== Performance ==
Ben Scofield rated different categories of NoSQL databases as follows:<ref>{{cite web|url=http://www.slideshare.net/bscofield/nosql-codemash-2010|title=NoSQL - Death to Relational Databases(?)|last=Scofield|first=Ben |date=2010-01-14|accessdate=2014-06-26}}</ref>

{| style="text-align: left;" class="wikitable sortable"
|-
! Data Model !! Performance !! Scalability !! Flexibility !! Complexity !! Functionality
|-
| Key–Value Store ||  high || high || high || none || variable (none)
|-
| Column-Oriented Store || high || high || moderate || low || minimal
|-
| Document-Oriented Store || high || variable (high) || high || low || variable (low)
|-
| Graph Database || variable || variable || high || high || [[graph theory]]
|-
| Relational Database || variable || variable || low || moderate || [[relational algebra]]
|}

Performance and scalability comparisons are sometimes done with the [[YCSB]] benchmark.

{{see also|Comparison of structured storage software}}

== Handling relational data ==
Since most NoSQL databases lack ability for joins in queries, the [[database schema]] generally needs to be designed differently. There are three main techniques for handling relational data in a NoSQL database. (See table Join and ACID Support for NoSQL databases that support joins.)

=== Multiple queries ===
Instead of retrieving all the data with one query, it's common to do several queries to get the desired data. NoSQL queries are often faster than traditional SQL queries so the cost of having to do additional queries may be acceptable. If an excessive number of queries would be necessary, one of the other two approaches is more appropriate.

=== Caching/replication/non-normalized data ===
Instead of only storing foreign keys, it's common to store actual foreign values along with the model's data. For example, each blog comment might include the username in addition to a user id, thus providing easy access to the username without requiring another lookup. When a username changes however, this will now need to be changed in many places in the database. Thus this approach works better when reads are much more common than writes.<ref name="DataModeling-Couchbase.com_December_5_2014c">{{cite web |url=http://www.couchbase.com/sites/default/files/uploads/all/whitepapers/Couchbase_Whitepaper_Transitioning_Relational_to_NoSQL.pdf |title=Making the Shift from Relational to NoSQL
 |newspaper=Couchbase.com |accessdate= December 5, 2014}}</ref>

=== Nesting data ===
With document databases like MongoDB it's common to put more data in a smaller number of collections. For example, in a blogging application, one might choose to store comments within the blog post document so that with a single retrieval one gets all the comments. Thus in this approach a single document contains all the data you need for a specific task.

== ACID and JOIN Support ==

If a database is marked as supporting [[ACID]] or [[Join (SQL)|joins]], then the documentation for the database makes that claim. The degree to which the capability is fully supported in a manner similar to most SQL databases or the degree to which it meets the needs of a specific application is left up to the reader to assess.
{| class="wikitable"
|-
! Database !! ACID !! Joins
|-
| [[Aerospike (company)|Aerospike]] || {{Yes}} || {{No}}
|-
| ArangoDB || {{Yes}} || {{Yes}}
|-
| [[CouchDB]] || {{Yes}} || {{Yes}}
|-
| [[c-treeACE]] || {{Yes}} || {{Yes}}
|-
| [[HyperDex]] || {{Yes}}{{refn|name=HyperDexAcid|group=nb|HyperDex currently offers ACID support via its Warp extension, which is a commercial add-on.}} || {{Yes}}
|-
| [[InfinityDB]] || {{Yes}} || {{No}}
|-
| [[Lightning Memory-Mapped Database|LMDB]] || {{Yes}} || {{No}}
|-
| [[MarkLogic]] || {{Yes}} || {{Yes}}{{refn|name=MarkLogicJoins|group=nb|Joins do not necessarily apply to document databases, but MarkLogic can do joins using semantics.<ref>http://www.gennet.com/big-data/cant-joins-marklogic-just-matter-semantics/</ref>}}
|-
| [[OrientDB]] || {{Yes}} || {{Yes}}

|}

{{reflist|group=nb}}

== See also ==
<!-- please do not list specific implementations here -->
* [[CAP theorem]]
* [[Comparison of object database management systems]]
* [[Comparison of structured storage software]]
* [[Correlation database]]
* [[Distributed cache]]
* [[Faceted search]]
* [[MultiValue]] database
* [[Multi-model database]]
* [[Triplestore]]
* [[Schema-agnostic databases]]

== References ==
{{Reflist|33em}}

== Further reading ==
*{{cite book
 | first1 = Pramod | last1 = Sadalage | first2 = Martin | last2 = Fowler | authorlink2 = Martin Fowler
 | date = 2012
 | title = NoSQL Distilled: A Brief Guide to the Emerging World of Polyglot Persistence
 | publisher = Addison-Wesley
 | isbn = 0-321-82662-0
}}
*{{cite book
 | first1 = Dan | last1 = McCreary | first2 = Ann | last2 =Kelly
 | date = 2013
 | title = Making Sense of NoSQL: A guide for managers and the rest of us
 | isbn = 9781617291074
}}
*{{cite book
 | first1 = Lena | last1 = Wiese 
 | date = 2015
 | title =  Advanced Data Management for SQL, NoSQL, Cloud and Distributed Databases 
 | publisher = DeGruyter/Oldenbourg
 | isbn = 978-3-11-044140-6
}}
* {{cite web| first = Christof | last = Strauch | date = 2012|title=NoSQL Databases|url=http://www.christof-strauch.de/nosqldbs.pdf}}
* {{cite journal| last1 = Moniruzzaman | first1 = A. B. | last2 = Hossain | first2 = S. A. | date = 2013|title=NoSQL Database: New Era of Databases for Big data Analytics - Classification, Characteristics and Comparison|arxiv=1307.0191}}
* {{cite journal| first = Kai | last = Orend | date = 2013|title=Analysis and Classification of NoSQL Databases and Evaluation of their Ability to Replace an Object-relational Persistence Layer|citeseerx = 10.1.1.184.483 }}
* {{cite web| first1 = Ganesh | last1 = Krishnan | first2 = Sarang | last2 = Kulkarni | first3 = Dharmesh Kirit | last3 = Dadbhawala | title=Method and system for versioned sharing, consolidating and reporting information|url=https://www.google.com/patents/US7383272?pg=PA1&dq=ganesh+krishnan&hl=en&sa=X}}

== External links ==
* {{cite web|url=http://www.christof-strauch.de/nosqldbs.pdf|title=NoSQL whitepaper| first = Christoph | last = Strauch|publisher=Hochschule der Medien|location = Stuttgart}}
* {{cite web|url=http://nosql-database.org/|title=NoSQL database List| first = Stefan | last = Edlich}}
* {{cite web|year=2010|url=http://www.infoq.com/articles/graph-nosql-neo4j|title=Graph Databases, NOSQL and Neo4j| first = Peter | last = Neubauer}}
* {{cite web|year=2012|url=http://www.networkworld.com/article/2160905/tech-primers/a-vendor-independent-comparison-of-nosql-databases--cassandra--hbase--mongodb--riak.html|title=A vendor-independent comparison of NoSQL databases: Cassandra, HBase, MongoDB, Riak| first = Sergey | last = Bushik|publisher=NetworkWorld}}
* {{cite web|year=2014|url=http://www.odbms.org/category/downloads/nosql-data-stores/nosql-data-stores-articles/|title=NoSQL Data Stores – Articles, Papers, Presentations|first = Roberto V. | last = Zicari|website=odbms.org}}
{{Use dmy dates|date=February 2012}}
{{Databases}}



[[Category:NoSQL| ]]
[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Structured storage]]
<=====doc_Id=====>:340
<=====title=====>:
SQL injection
<=====text=====>:
{{Use mdy dates|date=February 2012}}
[[File:KD SQLIA Classification 2010.png|thumb|alt=Classification of SQL injection attack vectors in 2010|A classification of SQL injection attacking vector as of 2010.]]

'''SQL injection ''' is a [[code injection]] technique, used  to [[Attack (computing)|attack]] data-driven applications, in which nefarious [[SQL]] statements are inserted into an entry field for execution (e.g. to dump the database contents to the attacker).<ref>{{cite web | url = http://technet.microsoft.com/en-us/library/ms161953%28v=SQL.105%29.aspx | title = SQL Injection | accessdate = 2013-08-04 | author = Microsoft | quote = SQL injection is an attack in which malicious code is inserted into strings that are later passed to an instance of SQL Server for parsing and execution. Any procedure that constructs SQL statements should be reviewed for injection vulnerabilities because SQL Server will execute all syntactically valid queries that it receives. Even parameterized data can be manipulated by a skilled and determined attacker.}}</ref> SQL injection must exploit a [[security vulnerability]] in an application's software, for example, when user input is either incorrectly filtered for [[string literal]] [[escape sequence|escape characters]] embedded in SQL statements or user input is not [[Strongly-typed programming language|strongly typed]] and unexpectedly executed. SQL injection is mostly known as an attack [[Vector (malware)|vector]] for websites but can be used to attack any type of SQL database.

SQL injection attacks allow attackers to spoof identity, tamper with existing data, cause repudiation issues such as voiding transactions or changing balances, allow the complete disclosure of all data on the system, destroy the data or make it otherwise unavailable, and become administrators of the database server.

In a 2012 study, it was observed that the average web application received 4 attack campaigns per month, and retailers received twice as many attacks as other industries.<ref>{{cite web | url = http://www.imperva.com/docs/HII_Web_Application_Attack_Report_Ed4.pdf | title = Imperva Web Application Attack Report | accessdate = 2013-08-04 | author = Imperva | date = July 2012 | format = PDF | quote = Retailers suffer 2x as many SQL injection attacks as other industries. / While most web applications receive 4 or more web attack campaigns per month, some websites are constantly under attack. / One observed website was under attack 176 out of 180 days, or 98% of the time.}}</ref>

==History==

The first public discussions of SQL injection started appearing around 1998;<ref>{{cite web |title= How Was SQL Injection Discovered? The researcher once known as Rain Forrest Puppy explains how he discovered the first SQL injection more than 15 years ago. |author= Sean Michael Kerner  |date= November 25, 2013 |url= http://www.esecurityplanet.com/network-security/how-was-sql-injection-discovered.html }}</ref> for example, a 1998 article in [[Phrack Magazine]].<ref>{{cite journal |title= NT Web Technology Vulnerabilities |author= Jeff Forristal (signing as rain.forest.puppy) |journal= [[Phrack Magazine]] |volume= 8 |issue= 54 (article 8) |date= Dec 25, 1998 |url= http://www.phrack.com/issues.html?issue=54&id=8#article }}</ref>

==Form==
SQL injection (SQLI) is considered one of the top 10 web application vulnerabilities of 2007 and 2010 by the [[OWASP|Open Web Application Security Project]].<ref>{{cite web|url=https://www.owasp.org/index.php/Category:OWASP_Top_Ten_Project |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2011-06-03}}</ref> In 2013, SQLI was rated the number one attack on the OWASP top ten.<ref>{{cite web|url=https://www.owasp.org/index.php/Top_10_2013-Top_10 |title=Category:OWASP Top Ten Project |publisher=OWASP |accessdate=2013-08-13}}</ref> There are four main sub-classes of SQL injection:
* Classic SQLI
* Blind or Inference SQL injection
* [[Database management system]]-specific SQLI
* Compounded SQLI

:* SQL injection + insufficient authentication<ref>{{cite web|url=http://www.xiom.com/whid-2007-60 |title=WHID 2007-60: The blog of a Cambridge University security team hacked |publisher=Xiom |accessdate=2011-06-03}}</ref>
:* SQL injection + [[DDoS]] attacks<ref>{{cite web|url=http://www.xiom.com/content/whid-2009-1-gaza-conflict-cyber-war |title=WHID 2009-1: Gaza conflict cyber war |publisher=Xiom |accessdate=2011-06-03}}</ref>
:* SQL injection + [[DNS hijacking]]<ref>[http://www.xiom.com/whid-list/DNS%20Hijacking ] {{webarchive |url=https://web.archive.org/web/20090618125914/http://www.xiom.com/whid-list/DNS%20Hijacking |date=June 18, 2009 }}</ref>
:* SQL injection + [[Cross-site scripting|XSS]]<ref>{{cite web|url=http://www.darkreading.com/security/management/showArticle.jhtml?articleID=211201482 |title=Third Wave of Web Attacks Not the Last |publisher=Dark Reading |accessdate=2012-07-29}}</ref>

The [[Storm Worm]] is one representation of Compounded SQLI.<ref>{{cite web|last=Danchev |first=Dancho |url=http://ddanchev.blogspot.com/2007/01/social-engineering-and-malware.html|title=Mind Streams of Information Security Knowledge: Social Engineering and Malware |publisher=Ddanchev.blogspot.com |date=2007-01-23 |accessdate=2011-06-03}}</ref>

This classification represents the state of SQLI, respecting its evolution until 2010—further refinement is underway.<ref>{{cite web|last=Deltchev|first=Krassen|title=New Web 2.0 Attacks|url=http://www.nds.ruhr-uni-bochum.de/teaching/theses/Web20/|work=B.Sc. Thesis|publisher=Ruhr-University Bochum|accessdate=February 18, 2010}}</ref>

==Technical implementations==

===Incorrectly filtered escape characters===
This form of SQL injection occurs when user input is not filtered for [[escape character]]s and is then passed into an SQL statement. This results in the potential manipulation of the statements performed on the database by the end-user of the application.

The following line of code illustrates this vulnerability:

 statement = "<source lang="sql" enclose="none">SELECT * FROM users WHERE name = '</source>" + userName + "<source lang="sql" enclose="none">';</source>"

This SQL code is designed to pull up the records of the specified username from its table of users. However, if the "userName" variable is crafted in a specific way by a malicious user, the SQL statement may do more than the code author intended. For example, setting the "userName" variable as:

<pre>' OR '1'='1</pre>
or using comments to even block the rest of the query (there are three types of SQL comments<ref>{{citation |title= IBM Informix Guide to SQL: Syntax. Overview of SQL Syntax &gt; How to Enter SQL Comments |publisher= IBM |url= http://publib.boulder.ibm.com/infocenter/idshelp/v10/index.jsp?topic=/com.ibm.sqls.doc/sqls36.htm }}</ref>). All three lines have a space at the end:
<pre>' OR '1'='1' --
' OR '1'='1' ({
' OR '1'='1' /* </pre>
.
renders one of the following SQL statements by the parent language:

<source lang="sql">SELECT * FROM users WHERE name = '' OR '1'='1';</source>
<source lang="sql">SELECT * FROM users WHERE name = '' OR '1'='1' -- ';</source>

If this code were to be used in an authentication procedure then this example could be used to force the selection of every data field (*) from ''all'' users rather than from one specific user name as the coder intended,  because the evaluation of '1'='1' is always true ([[short-circuit evaluation]]).

The following value of "userName" in the statement below would cause the deletion of the "users" table as well as the selection of all data from the "userinfo" table (in essence revealing the information of every user), using an [[API]] that allows multiple statements:

 a';<source lang="sql" enclose="none">DROP TABLE users; SELECT * FROM userinfo WHERE 't' = 't'</source>

This input renders the final SQL statement as follows and specified:

<source lang="sql">SELECT * FROM users WHERE name = 'a';DROP TABLE users; SELECT * FROM userinfo WHERE 't' = 't';</source>

While most SQL server implementations allow multiple statements to be executed with one call in this way, some SQL APIs such as [[PHP]]'s <code>mysql_query()</code> function do not allow this for security reasons. This prevents attackers from injecting entirely separate queries, but doesn't stop them from modifying queries.

===Incorrect type handling===
This form of SQL injection occurs when a '''user-supplied''' field is not [[strongly typed]] or is not checked for [[data type|type]] constraints. This could take place when a numeric field is to be used in a SQL statement, but the programmer makes no checks to validate that the user supplied input is numeric. For example:
 statement := "<source lang="sql" enclose="none">SELECT * FROM userinfo WHERE id = </source>" + a_variable + ";"

It is clear from this statement that the author intended a_variable to be a number correlating to the "id" field. However, if it is in fact a [[String (computer science)|string]] then the [[end-user]] may manipulate the statement as they choose, thereby bypassing the need for escape characters. For example, setting a_variable to

<pre>1;DROP TABLE users</pre>

will drop (delete) the "users" table from the database, since the SQL becomes:

<source lang="sql">SELECT * FROM userinfo WHERE id=1; DROP TABLE users;</source>

===Blind SQL injection===
Blind SQL Injection is used when a web application is vulnerable to an SQL injection but the results of the injection are not visible to the attacker. The page with the vulnerability may not be one that displays data but will display differently depending on the results of a logical statement injected into the legitimate SQL statement called for that page.
This type of attack has traditionally been considered time-intensive because a new statement needed to be crafted for each bit recovered, and depending on its structure, the attack may consist of many unsuccessful requests. Recent advancements have allowed each request to recover multiple bits, with no unsuccessful requests, allowing for more consistent and efficient extraction. <ref>{{cite web | url = http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html | title = Extracting Multiple Bits Per Request From Full-blind SQL Injection Vulnerabilities | publisher = Hack All The Things | accessdate = July 8, 2016 |archiveurl = https://web.archive.org/web/20160708190141/http://howto.hackallthethings.com/2016/07/extracting-multiple-bits-per-request.html |archivedate = July 8, 2016}}</ref> There are several tools that can automate these attacks once the location of the vulnerability and the target information has been established.<ref>{{cite web | url = http://www.justinclarke.com/archives/2006/03/sqlbrute.html | title = Using SQLBrute to brute force data from a blind SQL injection point | publisher = Justin Clarke | accessdate = October 18, 2008 |archiveurl = http://web.archive.org/web/20080614203711/http://www.justinclarke.com/archives/2006/03/sqlbrute.html <!-- Bot retrieved archive --> |archivedate = June 14, 2008}}</ref>

====Conditional responses====
One type of blind SQL injection forces the database to evaluate a logical statement on an ordinary application screen. As an example, a book review website uses a [[query string]] to determine which book review to display. So the [[URL]] <code><nowiki>http://books.example.com/showReview.php?ID=5</nowiki></code> would cause the server to run the query
<source lang="sql">SELECT * FROM bookreviews WHERE ID = 'Value(ID)';</source>
from which it would populate the review page with data from the review with [[Identifier|ID]] 5, stored in the [[Table (database)|table]] bookreviews. The query happens completely on the server; the user does not know the names of the database, table, or fields, nor does the user know the query string. The user only sees that the above URL returns a book review. A [[Hacker (computer security)|hacker]] can load the URLs <code><source lang="sql" enclose="none">http://books.example.com/showReview.php?ID=5 OR 1=1</source></code> and <code><source lang="sql" enclose="none">http://books.example.com/showReview.php?ID=5 AND 1=2</source></code>, which may result in queries
<source lang="sql">SELECT * FROM bookreviews WHERE ID = '5' OR '1'='1';
SELECT * FROM bookreviews WHERE ID = '5' AND '1'='2';</source>
respectively. If the original review loads with the "1=1" URL and a blank or error page is returned from the "1=2" URL, and the returned page has not been created to alert the user the input is invalid, or in other words, has been caught by an input test script, the site is likely vulnerable to a SQL injection attack as the query will likely have passed through successfully in both cases. The hacker may proceed with this query string designed to reveal the version number of [[MySQL]] running on the server: <code><source lang="mysql" enclose="none">http://books.example.com/showReview.php?ID=5 AND substring(@@version, 1, INSTR(@@version, '.') - 1)=4</source></code>, which would show the book review on a server running MySQL 4 and a blank or error page otherwise. The hacker can continue to use code within query strings to glean more information from the server until another avenue of attack is discovered or his or her goals are achieved.<ref>{{cite web|url=http://forum.intern0t.org/web-hacking-war-games/818-blind-sql-injection.html|title=Blind SQL Injection tutorial|author=macd3v|accessdate=6 December 2012}}</ref><ref>{{cite web|title=TDSS botnet: full disclosure|url=http://nobunkum.ru/analytics/en-tdss-botnet|accessdate=6 December 2012|author=Andrey Rassokhin|author2=Dmitry Oleksyuk }}</ref>

===Second order SQL injection===
Second order SQL injection occurs when submitted values contain malicious commands that are stored rather than executed immediately.  In some cases, the application may correctly encode an SQL statement and store it as valid SQL.  Then, another part of that application without controls to protect against SQL injection might execute that stored SQL statement.  This attack requires more knowledge of how submitted values are later used.  Automated web application security scanners would not easily detect this type of SQL injection and may need to be manually instructed where to check for evidence that it is being attempted.

==Mitigation==
An SQL injection is a well known attack and easily prevented by simple measures. After an apparent SQL injection attack on [[TalkTalk Group|Talktalk]] in 2015, the BBC reported that security experts were stunned that such a large company would be vulnerable to it.<ref>{{Cite web|title = Questions for TalkTalk - BBC News|url = http://www.bbc.com/news/technology-34636308|website = BBC News|accessdate = 2015-10-26|language = en}}</ref>

===Parameterized statements===
{{Main article|Prepared statement}}
With most development platforms, parameterized statements that work with parameters can be used  (sometimes called placeholders or [[bind variable]]s) instead of embedding user input in the statement. A placeholder can only store a value of the given type and not an arbitrary SQL fragment. Hence the SQL injection would simply be treated as a strange (and probably invalid) parameter value.

In many cases, the SQL statement is fixed, and each parameter is a [[Scalar (computing)|scalar]], not a [[Table (database)|table]]. The user input is then assigned (bound) to a parameter.<ref>{{cite web|title=SQL Injection Prevention Cheat Sheet|url=https://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet|publisher=Open Web Application Security Project|accessdate=3 March 2012}}</ref>

====Enforcement at the coding level====
Using [[object-relational mapping]] libraries avoids the need to write SQL code. The ORM library in effect will generate parameterized SQL statements from object-oriented code.

===Escaping===
A straightforward, though error-prone way to prevent injections is to escape characters that have a special meaning in SQL. The manual for an SQL DBMS explains which characters have a special meaning, which allows creating a comprehensive [[Blacklist (computing)|blacklist]] of characters that need translation. For instance, every occurrence of a single quote (<code>'</code>) in a parameter must be replaced by two single quotes (<code><nowiki>''</nowiki></code>) to form a valid SQL string literal. For example, in [[PHP]] it is usual to escape parameters using the function <code>mysqli_real_escape_string();</code> before sending the SQL query:
<source lang="php">
$mysqli = new mysqli('hostname', 'db_username', 'db_password', 'db_name');
$query = sprintf("SELECT * FROM `Users` WHERE UserName='%s' AND Password='%s'",
                  $mysqli->real_escape_string($username),
                  $mysqli->real_escape_string($password));
$mysqli->query($query);
</source>

This function prepends backslashes to the following characters: \x00, \n, \r, \, ', " and \x1a.
This function is normally used to make data safe before sending a query to [[MySQL]].<ref>{{cite web|url=http://in2.php.net/manual/en/mysqli.real-escape-string.php|title=mysqli->real_escape_string - PHP Manual|publisher=PHP.net}}</ref><br /> There are other functions for many database types in PHP such as pg_escape_string() for [[PostgreSQL]]. The function <code>addslashes(string $str)</code> works for escaping characters, and is used especially for querying on databases that do not have escaping functions in PHP.  It returns a string with backslashes before characters that need to be quoted in database queries, etc. These characters are single quote ('), double quote ("), backslash (\) and NUL (the NULL byte).<ref>{{cite web|url=http://pl2.php.net/manual/en/function.addslashes.php|title=Addslashes - PHP Manual|publisher=PHP.net}}</ref><br />
Routinely passing escaped strings to SQL is error prone because it is easy to forget to escape a given string. Creating a transparent layer to secure the input can reduce this error-proneness, if not entirely eliminate it.<ref>{{cite web|url=http://www.xarg.org/2010/11/transparent-query-layer-for-mysql/|title=Transparent query layer for MySQL|publisher=Robert Eisele|date=November 8, 2010}}</ref>

===Pattern check===
Integer, float or boolean,string parameters can be checked if their value is valid representation for the given type. Strings that must follow some strict pattern (date, UUID, alphanumeric only, etc.) can be checked if they match this pattern.

===Database permissions===
Limiting the permissions on the database logon used by the web application to only what is needed may help reduce the effectiveness of any SQL injection attacks that exploit any bugs in the web application.

For example, on [[Microsoft SQL Server]], a database logon could be restricted from selecting on some of the system tables which would limit exploits that try to insert JavaScript into all the text columns in the database.
<source lang="tsql">
deny select on sys.sysobjects to webdatabaselogon;
deny select on sys.objects to webdatabaselogon;
deny select on sys.tables to webdatabaselogon;
deny select on sys.views to webdatabaselogon;
deny select on sys.packages to webdatabaselogon;
</source>

==Examples==
* In February 2002, Jeremiah Jacks discovered that Guess.com was vulnerable to an SQL injection attack, permitting anyone able to construct a properly-crafted URL to pull down 200,000+ names, credit card numbers and expiration dates in the site's customer database.<ref>{{cite web|url=http://www.securityfocus.com/news/346|title=Guesswork Plagues Web Hole Reporting|publisher=[[SecurityFocus]]|date=March 6, 2002}}</ref>
* On November 1, 2005, a teenaged hacker used SQL injection to break into the site of a [[Taiwan]]ese information security magazine from the Tech Target group and steal customers' information.<ref>{{cite web|url=http://www.xiom.com/whid-2005-46|title=WHID 2005-46: Teen uses SQL injection to break to a security magazine web site|publisher=[[Web Application Security Consortium]]|date=November 1, 2005|accessdate=December 1, 2009}}</ref>
* On January 13, 2006, [[Russia]]n computer criminals broke into a [[Government of Rhode Island|Rhode Island government]] website and allegedly stole credit card data from individuals who have done business online with state agencies.<ref>{{cite web|url=http://www.xiom.com/whid-2006-3|title=WHID 2006-3: Russian hackers broke into a RI GOV website|publisher=[[Web Application Security Consortium]]|date=January 13, 2006|accessdate=May 16, 2008}}</ref>
* On March 29, 2006, a hacker discovered an SQL injection flaw in an official [[Government of India|Indian government]]'s [[Tourism in India|tourism]] site.<ref>{{cite web|url=http://www.xiom.com/whid-2006-27|title=WHID 2006-27: SQL Injection in incredibleindia.org|publisher=[[Web Application Security Consortium]]|date=March 29, 2006|accessdate=March 12, 2010}}</ref>
* On June 29, 2007, a computer criminal defaced the [[Microsoft]] UK website using SQL injection.<ref>{{cite web|url=http://www.cgisecurity.net/2007/06/hacker-defaces.html|title=Hacker Defaces Microsoft U.K. Web Page|publisher=cgisecurity.net|author=Robert|date=June 29, 2007|accessdate=May 16, 2008}}</ref><ref>{{cite web|url=http://rcpmag.com/news/article.aspx?editorialsid=8762|title=Hacker Defaces Microsoft UK Web Page|publisher=Redmond Channel Partner Online|author=Keith Ward|date=June 29, 2007|accessdate=May 16, 2008}}</ref> UK website ''[[The Register]]'' quoted a Microsoft [[spokesperson]] acknowledging the problem.
* On September 19, 2007 and January 26, 2009 the Turkish hacker group "m0sted" used SQL injection to exploit Microsoft's SQL Server to hack web servers belonging to [[McAlester Army Ammunition Plant]] and the [[United States Army Corps of Engineers|US Army Corps of Engineers]] respectively.<ref>{{cite web|url=http://www.informationweek.com/architecture/anti-us-hackers-infiltrate-army-servers/d/d-id/1079964|publisher=[[Information Week]]|title=Anti-U.S. Hackers Infiltrate Army Servers|date=May 29, 2009|accessdate=December 17, 2016}}</ref>
* In January 2008, tens of thousands of PCs were infected by an automated SQL injection attack that exploited a vulnerability in application code that uses [[Microsoft SQL Server]] as the database store.<ref name="chinesefarm" />
* In July 2008, [[Kaspersky Lab|Kaspersky]]'s [[Malaysia]]n site was hacked by the "m0sted" hacker group using SQL injection.
* On April 13, 2008, the [[Sex offender registries in the United States|Sexual and Violent Offender Registry]] of [[Oklahoma]] shut down its website for "[[routine maintenance]]" after being informed that 10,597 [[Social Security number]]s belonging to [[sex offender]]s had been downloaded via an SQL injection attack<ref>{{cite web|url=http://thedailywtf.com/Articles/Oklahoma-Leaks-Tens-of-Thousands-of-Social-Security-Numbers,-Other-Sensitive-Data.aspx|publisher=[[The Daily WTF]]|title=Oklahoma Leaks Tens of Thousands of Social Security Numbers, Other Sensitive Data|author=Alex Papadimoulis|date=April 15, 2008|accessdate=May 16, 2008}}</ref>
* In May 2008, a [[server farm]] inside [[China]] used automated queries to [[Google Search|Google's search engine]] to identify [[Microsoft SQL Server|SQL server]] websites which were vulnerable to the attack of an automated SQL injection tool.<ref name="chinesefarm">{{cite web | url = http://www.pcworld.com/businesscenter/article/146048/mass_sql_injection_attack_targets_chinese_web_sites.html | title = Mass SQL Injection Attack Targets Chinese Web Sites | author = Sumner Lemon, IDG News Service | publisher = [[PC World (magazine)|PCWorld]] | date = May 19, 2008 | accessdate = May 27, 2008 }}</ref><ref name="attackspecifics">{{cite web | url = http://www.bloombit.com/Articles/2008/05/ASCII-Encoded-Binary-String-Automated-SQL-Injection.aspx | title = ASCII Encoded/Binary String Automated SQL Injection Attack |author=Michael Zino| date = May 1, 2008 }}</ref>
* In 2008, at least April through August, a sweep of attacks began exploiting the SQL injection vulnerabilities of Microsoft's [[Internet Information Services|IIS web server]] and [[Microsoft SQL Server|SQL Server database server]]. The attack does not require guessing the name of a table or column, and corrupts all text columns in all tables in a single request.<ref name="broad_inject_specifics">{{cite web | url = http://hackademix.net/2008/04/26/mass-attack-faq/ | title = Mass Attack FAQ |author=Giorgio Maone| date = April 26, 2008 }}</ref>  A HTML string that references a [[malware]] [[JavaScript]] file is appended to each value. When that database value is later displayed to a website visitor, the script attempts several approaches at gaining control over a visitor's system. The number of exploited web pages is estimated at 500,000.<ref name="broad_inject_numbers">{{cite web | url = http://www.computerworld.com/article/2535473/security0/huge-web-hack-attack-infects-500-000-pages.html | title = Huge Web hack attack infects 500,000 pages |author=Gregg Keizer| date = April 25, 2008 |accessdate=October 16, 2015}}</ref>
* On August 17, 2009, the [[United States Department of Justice]] charged an American citizen, [[Albert Gonzalez]], and two unnamed Russians with the theft of 130 million credit card numbers using an SQL injection attack. In reportedly "the biggest case of [[identity theft]] in American history", the man stole cards from a number of corporate victims after researching their [[Payment processor|payment processing system]]s. Among the companies hit were credit card processor [[Heartland Payment Systems]], convenience store chain [[7-Eleven|7&#8209;Eleven]], and supermarket chain [[Hannaford Brothers]].<ref>{{cite news |url=http://news.bbc.co.uk/2/hi/americas/8206305.stm |title=US man 'stole 130m card numbers' |publisher=BBC |date=August 17, 2009 |accessdate=August 17, 2009}}</ref>
* In December 2009, an attacker breached a [[RockYou]] plaintext database containing the [[Encryption|unencrypted]] usernames and passwords of about 32&nbsp;million users using an SQL injection attack.<ref>{{cite news | url=http://www.nytimes.com/external/readwriteweb/2009/12/16/16readwriteweb-rockyou-hacker-30-of-sites-store-plain-text-13200.html | title = RockYou Hacker - 30% of Sites Store Plain Text Passwords | work=New York Times | first=Jolie | last=O'Dell | date=December 16, 2009 | accessdate=May 23, 2010}}</ref>
*On July 2010, a South American security researcher who goes by the [[User (computing)|handle]] "Ch&nbsp;Russo" obtained sensitive user information from popular [[BitTorrent]] site [[The Pirate Bay]]. He gained access to the site's administrative control panel and exploited a SQL injection vulnerability that enabled him to collect user account information, including [[IP address]]es, [[MD5]] [[Cryptographic hash function|password hashes]] and records of which torrents individual users have uploaded.<ref>{{cite news |url=http://krebsonsecurity.com/2010/07/pirate-bay-hack-exposes-user-booty/ | title= The pirate bay attack | date=July 7, 2010 }}</ref>
*From July 24 to 26, 2010, attackers from [[Japan]] and [[China]] used an SQL injection to gain access to customers' credit card data from Neo Beat, an [[Osaka]]-based company that runs a large online supermarket site. The attack also affected seven business partners including supermarket chains Izumiya Co, Maruetsu Inc, and Ryukyu Jusco Co. The theft of data affected a reported 12,191 customers. As of August 14, 2010 it was reported that there have been more than 300 cases of credit card information being used by third parties to purchase goods and services in China.
* On September 19 during the [[Swedish general election, 2010|2010 Swedish general election]] a voter attempted a code injection by hand writing SQL commands as part of a [[Write-in candidate|write&#8209;in]] vote.<ref>{{cite web|url=http://alicebobandmallory.com/articles/2010/09/23/did-little-bobby-tables-migrate-to-sweden |title=Did Little Bobby Tables migrate to Sweden? |publisher=Alicebobandmallory.com |accessdate=2011-06-03}}</ref>
* On November 8, 2010 the British [[Royal Navy]] website was compromised by a Romanian hacker named TinKode using SQL injection.<ref>[http://www.bbc.co.uk/news/technology-11711478 Royal Navy website attacked by Romanian hacker] ''BBC News'', 8-11-10, Accessed November 2010</ref><ref>{{cite web|url=http://news.sky.com/skynews/Home/World-News/Stuxnet-Worm-Virus-Targeted-At-Irans-Nuclear-Plant-Is-In-Hands-Of-Bad-Guys-Sky-News-Sources-Say/Article/201011415827544 |title=Super Virus A Target For Cyber Terrorists
|author=Sam Kiley |date=November 25, 2010 |accessdate=November 25, 2010}}</ref>
* On February 5, 2011 [[HBGary]], a technology security firm, was broken into by [[LulzSec]] using a SQL injection in their CMS-driven website<ref>{{cite web|url=http://www.par-anoia.net/We_Are_Anonymous_Inside_the_Hacker_World_of_LulzSe.pdf|title=We Are Anonymous: Inside the Hacker World of LulzSec|publisher=Little, Brown and Company}}</ref>
* On March 27, 2011, [http://www.mysql.com mysql.com], the official homepage for [[MySQL]], was compromised by a hacker using SQL blind injection<ref>{{cite web|url=http://blog.sucuri.net/2011/03/mysql-com-compromised.html|title=MySQL.com compromised|publisher=[[sucuri]]}}</ref>
* On April 11, 2011, [[Barracuda Networks]] was compromised using an SQL injection flaw. [[Email address]]es and usernames of employees were among the information obtained.<ref>{{cite web|url=http://www.networkworld.com/news/2011/041211-hacker-breaks-into-barracuda-networks.html?hpg1=bn |title=Hacker breaks into Barracuda Networks database}}</ref>
*Over a period of 4&nbsp;hours on April 27, 2011, an automated SQL injection attack occurred on [[Broadband Reports]] website that was able to extract 8% of the username/password pairs: 8,000 random accounts of the 9,000 active and 90,000 old or inactive accounts.<ref name="DSLReports">{{cite web|url=http://www.dslreports.com/forum/r25793356- |title=site user password intrusion info |publisher=Dslreports.com |accessdate=2011-06-03}}</ref><ref name="Cnet News">{{cite news|url=http://news.cnet.com/8301-27080_3-20058471-245.html|title=DSLReports says member information stolen|publisher=Cnet News|date=2011-04-28|accessdate=2011-04-29}}</ref><ref name="The Tech Herald">{{cite news|url=http://www.thetechherald.com/article.php/201117/7127/DSLReports-com-breach-exposed-more-than-100-000-accounts|title=DSLReports.com breach exposed more than 100,000 accounts|publisher=The Tech Herald|date=2011-04-29|accessdate=2011-04-29}}</ref>
*On June 1, 2011, "[[hacktivist]]s" of the group [[LulzSec]] were accused of using SQLI to steal [[coupon]]s, download keys, and passwords that were stored in plaintext on [[Sony]]'s website, accessing the personal information of a million users.<ref>{{citation |title= LulzSec hacks Sony Pictures, reveals 1m passwords unguarded | date= June 2, 2011 |work= electronista.com |url= http://www.electronista.com/articles/11/06/02/lulz.security.hits.sony.again.in.security.message/ }}</ref><ref>{{citation |title= LulzSec Hacker Arrested, Group Leaks Sony Database|author=Ridge Shan | date= June 6, 2011 |work= The Epoch Times |url=http://www.theepochtimes.com/n2/technology/lulzsec-member-arrested-group-leaks-sony-database-57296.html}}</ref>
* In June 2011, [[PBS]] was hacked, mostly likely through use of SQL injection; the full process used by hackers to execute SQL injections was described in this [http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html Imperva] blog.<ref name="PBS Breached - How Hackers Probably Did It">{{cite news|url=http://blog.imperva.com/2011/05/pbs-breached-how-hackers-probably-did-it.html|title=Imperva.com: PBS Hacked - How Hackers Probably Did It|accessdate=2011-07-01}}</ref>
* In May 2012, the website for ''[[Wurm Online]]'', a [[massively multiplayer online game]], was shut down from an SQL injection while the site was being updated.<ref>{{cite web|url=http://wurmonline.tumblr.com/post/22835329693/wurm-online-restructuring |title=Wurm Online is Restructuring |date=May 11, 2012}}</ref>
* [[2012 Yahoo! Voices hack|In July 2012]] a hacker group was reported to have stolen 450,000 login credentials from [[Yahoo!]]. The logins were stored in [[plain text]] and were allegedly taken from a Yahoo [[subdomain]], [[Yahoo! Voices]]. The group breached Yahoo's security by using a "[[Set operations (SQL)#UNION operator|union]]-based SQL injection technique".<ref>Chenda Ngak. [http://www.cbsnews.com/8301-501465_162-57470956-501465/yahoo-reportedly-hacked-is-your-account-safe/ "Yahoo reportedly hacked: Is your account safe?"], CBS News. July 12, 2012. Retrieved July 16, 2012.</ref><ref>http://www.zdnet.com/450000-user-passwords-leaked-in-yahoo-breach-7000000772/</ref>
* On October 1, 2012, a hacker group called "Team GhostShell" published the personal records of students, faculty, employees, and alumni from 53 universities including [[Harvard]], [[Princeton University|Princeton]], [[Stanford]], [[Cornell]], [[Johns Hopkins University|Johns Hopkins]], and the [[University of Zurich]] on [[Pastebin|pastebin.com]]. The hackers claimed that they were trying to "raise awareness towards the changes made in today’s education", bemoaning changing education laws in Europe and increases in [[College tuition in the United States|tuition in the United States]].<ref>{{cite news|last=Perlroth|first=Nicole|title=Hackers Breach 53 Universities and Dump Thousands of Personal Records Online|url=http://bits.blogs.nytimes.com/2012/10/03/hackers-breach-53-universities-dump-thousands-of-personal-records-online/|newspaper=New York Times|date=3 October 2012}}</ref>
* In February 2013, a group of Maldivian hackers, hacked the website "UN-Maldives" using SQL Injection.
* On June 27, 2013, hacker group "[[RedHack]]" breached Istanbul Administration Site.<ref>{{Cite news | title=RedHack Breaches Istanbul Administration Site, Hackers Claim to Have Erased Debts | url=http://news.softpedia.com/news/RedHack-Breaches-Istanbul-Administration-Site-Hackers-Claim-to-Have-Erased-Debts-364000.shtml}}</ref>  They claimed that, they’ve been able to erase people's debts to water, gas, Internet, electricity, and telephone companies. Additionally, they published admin user name and password for other citizens to log in and clear their debts early morning. They announced the news from Twitter.<ref>{{Cite news | title=Redhack tweet about their achievement | url=http://twitter.com/RedHack_EN/statuses/350461821456613376 }}</ref>
* On November 4, 2013, hacktivist group "RaptorSwag" allegedly compromised 71 Chinese government databases using an SQL injection attack on the Chinese Chamber of International Commerce. The leaked data was posted publicly in cooperation with [[Anonymous (group)|Anonymous]].<ref>http://news.softpedia.com/news/Hackers-Leak-Data-Allegedly-Stolen-from-Chinese-Chamber-of-Commerce-Website-396936.shtml</ref>
* On February 2, 2014, AVS TV had 40,000 accounts leaked by a hacking group called @deletesec <ref>http://www.maurihackers.info/2014/02/40000-avs-tv-accounts-leaked.html</ref>
* On February 21, 2014, United Nations Internet Governance Forum had 3,215 account details leaked.<ref>http://www.batblue.com/united-nations-internet-governance-forum-breached/</ref>
* On February 21, 2014, Hackers of a group called @deletesec hacked Spirol International after allegedly threatening to have the hackers arrested for reporting the security vulnerability. 70,000 user details were exposed over this conflict.<ref>http://news.softpedia.com/news/Details-of-70-000-Users-Leaked-by-Hackers-From-Systems-of-SPIROL-International-428669.shtml</ref>
* On March 7, 2014, officials at Johns Hopkins University publicly announced that their Biomedical Engineering Servers had become victim to an SQL injection attack carried out by an Anonymous hacker named "Hooky" and aligned with hacktivist group "RaptorSwag". The hackers compromised personal details of 878 students and staff, posting a [http://pastebin.com/UG4fYnby press release] and the leaked data on the internet.<ref>http://articles.baltimoresun.com/2014-03-07/news/bs-md-hopkins-servers-hacked-20140306_1_engineering-students-identity-theft-server</ref>
* In August 2014, [[Milwaukee]]-based computer security company Hold Security disclosed that it uncovered [[2014 Russian hacker password theft|a theft of confidential information]] from nearly 420,000 websites through SQL injections.<ref>Damon Poeter. [http://www.pcmag.com/article2/0,2817,2462057,00.asp 'Close-Knit' Russian Hacker Gang Hoards 1.2 Billion ID Creds], ''PC Magazine'', August 5, 2014</ref> ''[[The New York Times]]'' confirmed this finding by hiring a security expert to check the claim.<ref>Nicole Perlroth. [http://www.nytimes.com/2014/08/06/technology/russian-gang-said-to-amass-more-than-a-billion-stolen-internet-credentials.html?_r=0 Russian Gang Amasses Over a Billion Internet Passwords], ''The New York Times'', August 5, 2014.</ref>
* In October 2015, an SQL injection attack was used to steal the personal details of 156,959 customers from British telecommunications company [[TalkTalk Group|Talk Talk's]] servers, exploiting a vulnerability in a legacy web portal<ref>https://ico.org.uk/about-the-ico/news-and-events/news-and-blogs/2016/10/talktalk-gets-record-400-000-fine-for-failing-to-prevent-october-2015-attack/</ref>

==In popular culture==
* Unauthorized login to web sites by means of SQL injection forms the basis of one of the subplots in [[J.K. Rowling]]'s novel ''[[The Casual Vacancy]]'', published in 2012.
* An ''[[xkcd]]'' cartoon involved a character "Robert'); DROP TABLE students;--" named to carry out a SQL injection. As a result of this cartoon, SQL injection is sometimes informally referred to as 'Bobby Tables'.<ref>{{cite web|last=Munroe|first=Randall|title=XKCD: Exploits Of A Mom|url=http://xkcd.com/327/|accessdate=26 February 2013}}</ref><ref>{{cite web|title=Bobby Tables: A guide to preventing SQL injection|url=http://bobby-tables.com/|accessdate=6 October 2013}}</ref>
* In 2014, an individual in Poland legally renamed his business to ''<nowiki>Dariusz Jakubowski x'; DROP TABLE users; SELECT '1</nowiki>'' in an attempt to disrupt operation of spammers’ [[Web scraping|harvesting bots]].<ref>{{cite web|title=Jego firma ma w nazwie SQL injection. Nie zazdrościmy tym, którzy będą go fakturowali ;)|website=Niebezpiecznik|language=pl|date=11 September 2014|url=http://niebezpiecznik.pl/post/jego-firma-ma-w-nazwie-sql-injection-nie-zazdroscimy-tym-ktorzy-beda-go-fakturowali/|accessdate=26 September 2014}}</ref>
* The 2015 game [[Hacknet]] has a hacking program called SQL_MemCorrupt. It is described as injecting a table entry that causes a corruption error in a SQL database, then queries said table, causing a SQL database crash and core dump.

==See also==
{{Portal|Software Testing}}
* [[Code injection]]
* [[Cross-site scripting]]
* [[Metasploit Project]]
* [[OWASP]] Open Web Application Security Project
* [[SGML entity]]
* [[Uncontrolled format string]]
* [[w3af]]
* [[Web application security]]

==References==
{{Reflist|30em}}

==External links==
* [http://www.techyfreaks.com/2012/05/manual-sql-injection-tutorial.html Manual Sql Injection Tutorial] By The Ajay Devgan
* [http://www.websec.ca/kb/sql_injection SQL Injection Knowledge Base], by Websec.
* [http://www.sqlinjectionwiki.com/ SQL Injection Wiki]
* [http://projects.webappsec.org/SQL-Injection WASC Threat Classification - SQL Injection Entry], by the Web Application Security Consortium.
* [https://docs.google.com/leaf?id=0BykNNUTb95yzYTRjMjNjMWEtODBmNS00YzgwLTlmMGYtNWZmODI2MTNmZWYw&sort=name&layout=list&num=50 Why SQL Injection Won't Go Away], by Stuart Thomas.
* [http://www.owasp.org/index.php/SQL_Injection_Prevention_Cheat_Sheet SQL Injection Prevention Cheat Sheet], by OWASP.
* [http://sqlmap.org/ sqlmap: automatic SQL injection and database takeover tool]
* [http://go.microsoft.com/?linkid=9707610 SDL Quick security references on SQL injection] by Bala Neerumalla.
* [http://arstechnica.com/information-technology/2016/10/how-security-flaws-work-sql-injection/ How security flaws work: SQL injection]
* [https://www.netsparker.com/blog/web-security/sql-injection-cheat-sheet/ SQL Injection Cheat Sheet] by Netsparker

[[Category:Data management]]
[[Category:Injection exploits]]
[[Category:SQL]]
[[Category:Articles with example SQL code]]
[[Category:Computer security exploits]]
<=====doc_Id=====>:343
<=====title=====>:
Secure Electronic Delivery
<=====text=====>:
'''Secure Electronic Delivery'''  (SED) is a service created in 2003 and provided by the  [[British Library#Document Supply Service|British Library Document Supply Service]] (BLDSS). Its purpose is to enable faster delivery of digital materials as [[Encryption|encrypted]], copyright-compliant [[Portable Document Format| PDF Document]]s, to a personal e-mail address. These documents are supplied from the British Library via its On Demand service.<ref>{{cite web |url=http://www.bl.uk/sed   |title=Secure Electronic Delivery  |author=  |publisher=[[British Library]] |date=  |accessdate= }}</ref><ref>{{cite web |url=http://www.bl.uk/reshelp/atyourdesk/docsupply/help/receiving/deliveryoptions/electronic/sed/sedhelpsheetfinal.pdf  |title= Secure Electronic Delivery – Technical Helpsheet  |author=  |publisher=[[British Library]] |date=  |accessdate= }}</ref> When the British Library supplies articles electronically, it sends them securely in order to ensure its usage is permitted (research purposes) and copyright law is observed.

==Methods==
As the [[publishing | publishing industry]], authors and creators become highly protective of their assets and [[intellectual property]], they impose strict rules on delivery methods to prevent [[copyright infringement]]. Nowadays, [[Digital rights management|DRM]]-enabled secure delivery appears to be the most widely used solution to address issues faced by libraries in supplying ebooks and digital materials to their users.<ref>{{cite news  | title=Secure E-mail Delivery Poised to Take Off  |url=https://books.google.com.mx/books?id=PWHbLjAQ57gC&pg=PA38&lpg=PA38&dq=document+secure+delivery+technology&source=bl&ots=YgfE16c0Yy&sig=qd0R1j9LtZI6_hJi6zqHCGOBzfQ&hl=en&sa=X&redir_esc=y#v=onepage&q=document%20secure%20delivery%20technology&f=false   |date=23 August 1999 |author=Dominique Deckmyn  |newspaper=[[Computerworld]] }}</ref><ref>{{cite web  |title= Practical problems for libraries distributing ebooks & secure electronic delivery |url=http://www.locklizard.com/libraries-secure-electronic-delivery/   |publisher=Locklizard Limited |date=  |accessdate= }}</ref>  SED, one of these solutions, is using [[Adobe LiveCycle]] Digital Rights Management (LCDRM) as an encryption method to deliver documents.<ref>{{cite web |url=http://www.lancaster.ac.uk/library/using-the-library/interlending-and-document-supply/secure-electronic-delivery/ |title=British Library On Demand Electronic Delivery  |author=  |publisher=[[Lancaster University]] Library  |date=  |accessdate= }}</ref>

==Advantages==
SED offers convenience, quality and speed as documents are delivered upon request at any location and on any device. Requested articles are scanned for high quality reproduction, opened anywhere on any machine, including mobile devices.<ref>{{cite web |url= http://www.brad.ac.uk/library/media/library/interlibraryloans/sed.pdf |title=SED – Secure Electronic Delivery  |author=  |publisher=[[University of Bradford]] |date=  |accessdate= }}</ref>

== Restrictions==
The following are restrictions hold in a SED service implementation:
*  The digital material is accessible only for 14 days via a link sent to a personal message.
* Due to copyright reasons,<ref>{{cite journal |last=Eiblum |first= Paula   |last2= Ardito |first2= Stephanie    |date= September 1999 |title= Document Delivery & Copyright: Librarians Take the Fifth |url= |journal=Online (magazine) |publisher= |volume=23 |issue=5 |pages=74–77  |doi= |access-date=7 May 2016}}</ref>  the material can be opened only once, saved for 14 days and does not allow a copy-paste action.
* Upon display, the material must be printed from the same device and reprinted only once.
* The On Demand encryption technology works best on the default Safari browser although other browsers may accommodate it.

==See also==
* [[Digital rights management]]
* [[Digital asset management]]

==References==
{{Reflist}}

==External links==
[http://www.bl.uk/sed SED Web page]

{{DEFAULTSORT:Secure Electronic Delivery}}
[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Document management systems]]
[[Category:Data management]]
[[Category:Secure communication]]
<=====doc_Id=====>:346
<=====title=====>:
Database normalization
<=====text=====>:
'''Database normalization''', or simply '''normalization''', is the process of organizing the [[column (database)|columns]] (attributes) and [[table (database)|tables]] (relations) of a [[relational database]] to reduce [[data redundancy]] and improve data integrity.

Normalization involves arranging attributes in tables based on [[Dependency theory (database theory)|dependencies]] between attributes, ensuring that the dependencies are properly enforced by database integrity constraints. Normalization is accomplished through applying some formal rules either by a process of synthesis or decomposition. Synthesis creates a normalized database design based on a known set of dependencies. Decomposition takes an existing (insufficiently normalized) database design and improves it based on the known set of dependencies.

[[Edgar F. Codd]], the inventor of the [[relational model]] (RM), introduced the concept of normalization and what we now know as the [[First normal form]] (1NF) in 1970.<ref name="Codd1970">{{cite journal|first=E. F.|last=Codd|authorlink=E.F. Codd|title=A Relational Model of Data for Large Shared Data Banks|journal=[[Communications of the ACM]]|volume=13|issue=6|date=June 1970|pages=377–387|url=http://www.acm.org/classics/nov95/toc.html | doi = 10.1145/362384.362685}}</ref> Codd went on to define the [[Second normal form]] (2NF) and [[Third normal form]] (3NF) in 1971,<ref name="Codd, E.F 1971">Codd, E.F. "Further Normalization of the Data Base Relational Model". (Presented at Courant Computer Science Symposia Series 6, "Data Base Systems", New York City, May 24–25, 1971.) IBM Research Report RJ909 (August 31, 1971). Republished in Randall J. Rustin (ed.), ''Data Base Systems: Courant Computer Science Symposia Series 6''. Prentice-Hall, 1972.</ref> and Codd and [[Raymond F. Boyce]] defined the Boyce-Codd Normal Form ([[Boyce–Codd normal form|BCNF]]) in 1974.<ref name="CoddBCNF">Codd, E. F. "Recent Investigations into Relational Data Base Systems". IBM Research Report RJ1385 (April 23, 1974). Republished in ''Proc. 1974 Congress'' (Stockholm, Sweden, 1974). , N.Y.: North-Holland (1974).</ref> Informally, a relational database table is often described as "normalized" if it meets Third Normal Form.<ref name="DateIntroDBSys">C.J. Date.  ''An Introduction to Database Systems''. Addison-Wesley (1999), p. 290</ref>  Most 3NF tables are free of insertion, update, and deletion anomalies.


==Objectives==
A basic objective of the [[first normal form]] defined by Codd in 1970 was to permit data to be queried and manipulated using a "universal data sub-language" grounded in [[first-order logic]].<ref>"The adoption of a relational model of data ... permits the development of a universal data sub-language based on an applied predicate calculus. A first-order predicate calculus suffices if the collection of relations is in first normal form. Such a language would provide a yardstick of linguistic power for all other proposed data languages, and would itself be a strong candidate for embedding (with appropriate syntactic modification) in a variety of host Ianguages (programming, command- or problem-oriented)."  Codd, [http://www.acm.org/classics/nov95/toc.html "A Relational Model of Data for Large Shared Data Banks"], p. 381</ref> ([[SQL]] is an example of such a data sub-language, albeit one that Codd regarded as seriously flawed.)<ref>Codd, E.F.  Chapter 23, "Serious Flaws in SQL", in ''The Relational Model for Database Management: Version 2''. Addison-Wesley (1990), pp. 371–389</ref>

The objectives of normalization beyond 1NF (First Normal Form) were stated as follows by Codd:

{{Quotation|
# To free the collection of relations from undesirable insertion, update and deletion dependencies;
# To reduce the need for restructuring the collection of relations, as new types of data are introduced, and thus increase the life span of application programs;
# To make the relational model more informative to users;
# To make the collection of relations neutral to the query statistics, where these statistics are liable to change as time goes by.
|E.F. Codd|"Further Normalization of the Data Base Relational Model"<ref>Codd, E.F. "Further Normalization of the Data Base Relational Model", p. 34</ref>}}

The sections below give details of each of these objectives.

===Free the database of modification anomalies===
[[File:Update anomaly.svg|280px|thumb|right|An '''update anomaly'''. Employee 519 is shown as having different addresses on different records.]]
[[File:Insertion anomaly.svg|280px|thumb|right|An '''insertion anomaly'''. Until the new faculty member, Dr. Newsome, is assigned to teach at least one course, his details cannot be recorded.]]
[[File:Deletion anomaly.svg|280px|thumb|right|A '''deletion anomaly'''. All information about Dr. Giddens is lost if he temporarily ceases to be assigned to any courses.]]
When an attempt is made to modify (update, insert into, or delete from) a table, undesired side-effects may arise in tables that have not been sufficiently normalized. An insufficiently normalized table might have one or more of the following characteristics:

* The same information can be expressed  on multiple rows; therefore updates to the table may result in logical inconsistencies. For example, each record in an "Employees' Skills" table might contain an Employee ID, Employee Address, and Skill; thus a change of address for a particular employee will potentially need to be applied to multiple records (one for each skill). If the update is not carried through successfully—if, that is, the employee's address is updated on some records but not others—then the table is left in an inconsistent state. Specifically, the table provides conflicting answers to the question of what this particular employee's address is. This phenomenon is known as an '''update anomaly'''.
* There are circumstances in which certain facts cannot be recorded at all. For example, each record in a "Faculty and Their Courses" table might contain a Faculty ID, Faculty Name, Faculty Hire Date, and Course Code—thus we can record the details of any faculty member who teaches at least one course, but we cannot record the details of a newly hired faculty member who has not yet been assigned to teach any courses except by setting the Course Code to null. This phenomenon is known as an '''insertion anomaly'''.
* Under certain circumstances, deletion of data representing certain facts necessitates deletion of data representing completely different facts. The "Faculty and Their Courses" table described in the previous example suffers from this type of anomaly, for if a faculty member temporarily ceases to be assigned to any courses, we must delete the last of the records on which that faculty member appears, effectively also deleting the faculty member, unless we set the Course Code to null in the record itself.  This phenomenon is known as a '''deletion anomaly'''.

===Minimize redesign when extending the database structure===
When a fully normalized database structure is extended to allow it to accommodate new types of data, the pre-existing aspects of the database structure can remain largely or entirely unchanged. As a result, applications interacting with the database are minimally affected.

Normalized tables, and the relationship between one normalized table and another, mirror real-world concepts and their interrelationships.

===Example===
Querying and manipulating the data within a data structure that is not normalized, such as the following non-1NF representation of customers, credit card transactions, involves more complexity than is really necessary:

{| class="wikitable"
! Customer !! Cust. ID !! Transactions
|-
| Jones || 1
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12890
| 14-Oct-2003
| &minus;87
|-
| 12904
| 15-Oct-2003
| &minus;50
|}
|-
| Wilkins || 2
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12898
| 14-Oct-2003
| &minus;21
|}
|-
| Stevens || 3
|| 
{| class="wikitable"
! Tr. ID !! Date !! Amount
|-
| 12907
| 15-Oct-2003
| &minus;18
|-
| 14920
| 20-Nov-2003
| &minus;70
|-
| 15003
| 27-Nov-2003
| &minus;60
|}
|}
<br>
To each customer corresponds a ''repeating group'' of transactions.  The automated evaluation of any query relating to customers' transactions therefore would broadly involve two stages:
# Unpacking one or more customers' groups of transactions allowing the individual transactions in a group to be examined, and
# Deriving a query result based on the results of the first stage

For example, in order to find out the monetary sum of all transactions that occurred in October 2003 for all customers, the system would have to know that it must first unpack the ''Transactions'' group of each customer, then sum the ''Amounts'' of all transactions thus obtained where the ''Date'' of the transaction falls in October 2003.

One of Codd's important insights was that this structural complexity could always be removed completely, leading to much greater power and flexibility in the way queries could be formulated (by [[user (computing)|users]] and [[application software|applications]]) and evaluated (by the [[database management system|DBMS]]).  The normalized equivalent of the structure above would look like this:

{| class="wikitable"
|-
! Customer !! Cust. ID
|-
| Jones || 1
|-
| Wilkins || 2
|-
| Stevens || 3
|}

{| class="wikitable"
|-
! Cust. ID !! Tr. ID !! Date !! Amount
|-
| 1 || 12890 || 14-Oct-2003 || &minus;87
|-
| 1 || 12904 || 15-Oct-2003 || &minus;50
|-
| 2 || 12898 || 14-Oct-2003 || &minus;21
|-
| 3 || 12907 || 15-Oct-2003 || &minus;18
|-
| 3 || 14920 || 20-Nov-2003 || &minus;70
|-
| 3 || 15003 || 27-Nov-2003 || &minus;60
|}

In the modified structure, the keys are {Customer} and {Cust. ID} in the first table, {Cust. ID, Tr ID} in the second table.

Now each row represents an individual credit card transaction, and the DBMS can obtain the answer of interest, simply by finding all rows with a Date falling in October, and summing their Amounts.  The data structure places all of the values on an equal footing, exposing each to the DBMS directly, so each can potentially participate directly in queries; whereas in the previous situation some values were embedded in lower-level structures that had to be handled specially.  Accordingly, the normalized design lends itself to general-purpose query processing, whereas the unnormalized design does not. The normalized version also allows the user to change the customer name in one place and guards against errors that arise if the customer name is misspelled on some records.

==List of Normal Forms==
* UNF - "[[Denormalization|Unnormalized]] Form"
* [[First normal form|1NF - First Normal Form]]
* [[Second normal form|2NF - Second Normal Form]]
* [[Third normal form|3NF - Third Normal Form]]
* [[Elementary Key Normal Form|EKNF - Elementary Key Normal Form]]
* [[Boyce–Codd normal form|BCNF - Boyce–Codd Normal Form]]
* [[Fourth normal form|4NF - Fourth Normal Form]]
* [http://researcher.watson.ibm.com/researcher/files/us-fagin/icdt12.pdf ETNF - Essential Tuple Normal Form]
* [[Fifth normal form|5NF - Fifth Normal Form]]
* [[Sixth normal form|6NF - Sixth Normal Form]]
* [[Domain/key normal form|DKNF - Domain/Key Normal Form]]

==See also==
*[[Refactoring]]

==Notes and references==
{{reflist|2}}
{{refbegin}}
{{refend}}

==Further reading==
* Date, C. J. (1999), ''[http://www.aw-bc.com/catalog/academic/product/0,1144,0321197844,00.html  An Introduction to Database Systems]'' (8th ed.). Addison-Wesley Longman. ISBN 0-321-19784-4.
* Kent, W. (1983) ''[http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]'', Communications of the ACM, vol. 26, pp.&nbsp;120–125
* H.-J. Schek, P. Pistor Data Structures for an Integrated Data Base Management and Information Retrieval System

==External links==
* [http://databases.about.com/od/specificproducts/a/normalization.htm Database Normalization Basics] by Mike Chapple (About.com)
* [http://www.databasejournal.com/sqletc/article.php/1428511 Database Normalization Intro], [http://www.databasejournal.com/sqletc/article.php/26861_1474411_1 Part 2]
* [http://mikehillyer.com/articles/an-introduction-to-database-normalization/ An Introduction to Database Normalization] by Mike Hillyer.
* [http://phlonx.com/resources/nf3/ A tutorial on the first 3 normal forms] by Fred Coulson
* [http://www.dbnormalization.com/ DB Normalization Examples]
* [http://support.microsoft.com/kb/283878 Description of the database normalization basics] by Microsoft
* [http://www.barrywise.com/2008/01/database-normalization-and-design-techniques/ Database Normalization and Design Techniques] by Barry Wise, recommended reading for the Harvard MIS.
* [http://www.bkent.net/Doc/simple5.htm A Simple Guide to Five Normal Forms in Relational Database Theory]
* [http://beginnersbook.com/2015/05/normalization-in-dbms/ Normalization in DBMS by Chaitanya (beginnersbook.com)]

{{Database normalization}}
{{Database}}
{{Databases}}

{{DEFAULTSORT:Database Normalization}}
[[Category:Database normalization| ]]
[[Category:Database constraints]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:Relational algebra]]
<=====doc_Id=====>:349
<=====title=====>:
Digital obsolescence
<=====text=====>:
{{Outdated|date=January 2015}}[[File:VCF 2010 Domesday tray open.jpg|thumb|300px|A Domesday Project machine with its modified [[Laserdisc]]. The Domesday Project was published in 1986.]]
'''Digital obsolescence''' is a situation where a digital resource is no longer readable because of its archaic format: the physical media, the reader (required to read the media), the hardware, or the software that runs on it is no longer available.<ref name='national-archives'>{{cite web | last = | first = | authorlink = | title =Managing Digital Obsolescence Risks | work = | publisher =The National Archives | date =April 2009 | url = http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf| format = pdf| doi = | accessdate = | archiveurl =http://webarchive.nationalarchives.gov.uk/+/http://www.nationalarchives.gov.uk/documents/information-management/siro-guidance-on-the-risk-of-digital-obsolescence.pdf | archivedate = 28 Jun 2011}}</ref> 

A prime example of this is the [[BBC Domesday Project]] from the 1980s, although its data was eventually recovered after a significant amount of effort. [[Cornell University]] Library’s [http://www.icpsr.umich.edu/dpm/dpm-eng/eng_index.html digital preservation tutorial] (now hosted by [[ICPSR]]) has a timeline of obsolete media formats, called the [http://www.icpsr.umich.edu/dpm/dpm-eng/oldmedia/index.html “Chamber of Horrors”], that shows how rapidly new technologies are created and cast aside.

==Introduction==
The rapid evolution and proliferation of different kinds of [[computer hardware]], modes of digital encoding, [[operating systems]] and general or specialized [[software]] ensures that digital obsolescence will become a problem in the future.<ref>Rothenberg, J. (1998). [http://www.clir.org/pubs/reports/rothenberg/introduction.html#longevity Avoiding Technological Quicksand: Finding a Viable Technical Foundation for Digital Preservation]</ref> Many versions of word-processing programs, data-storage media, standards for encoding images and films are considered "standards" for some time, but in the end are always replaced by new versions of the software or completely new hardware. Files meant to be read or edited with a certain program (for example [[Microsoft Word]]) will be unreadable in other programs, and as operating systems and hardware move on, even old versions of programs developed by the same company become impossible to use on the new platform (for instance, older versions of [[Microsoft Works]], before Works 4.5, cannot be run under [[Windows 2000]] or later). 

Early attention was brought to the challenges of preserving [[machine-readable data]] by the work of [[Charles M Dollar]] in the 1970s, but it was only during the 1990s that libraries and archives came to appreciate the significance of the problem<ref>Hedstrom, M. (1995). [http://www.uky.edu/%7Ekiernan/DL/hedstrom.html Digital Preservation: A Time Bomb for Digital Libraries]</ref> and has been discussed among professionals in those branches, though so far without any obvious solutions other than continual forward-migration of files and information to the latest data-storage standards. File formats should be widespread, backward compatible, often upgraded, and, ideally, open format. In 2002, the National Initiative for a Networked Cultural Heritage cited<ref>National Initiative for a Networked Cultural Heritage. (2002). [http://www.nyu.edu/its/humanities/ninchguide/V/ NINCH Guide to Good Practice in the Digital Representation and Management of Cultural Heritage Materials]</ref> the following as “de facto” formats that are unlikely to be rendered obsolete in the near future:  uncompressed [[TIFF]]  and [[ASCII]] and [[Rich Text Format|RTF]] (for text).

In order to prevent this from happening, it is important that an institution regularly evaluate and explore its current technologies and evaluate its long term business model.<ref name='national-archives'/>

== Types ==
Digital objects are vulnerable to three types of obsolescence:<ref>{{cite web|publisher=National Archives of Australia|title=Obsolescence – a key challenge in the digital age|url=http://www.naa.gov.au/records-management/agency/preserve/e-preservation/obsolescence.aspx|accessdate=17 March 2014}}</ref>
# '''Physical media''': the physical carrier of the digital file becomes obsolete; e.g. 8 inch floppy disks, which are no longer commercially available. 
# '''Hardware''': the hardware needed to access the digital file becomes obsolete; e.g. floppy disk drive, which computers are no longer manufactured with.
# '''Software''': the software needed to access the digital file becomes obsolete; e.g. [[WordStar]], a word processor popular in the 1980s which used a [[Open data|closed data]] format and is no longer readily available.

== Strategies ==
Any organization that has digital records should assess its records to identify any potential risks for file format obsolescence. The Library of Congress maintains [http://www.digitalpreservation.gov/formats/intro/intro.shtml Sustainability of Digital Formats], which includes technical details about many different format types. The UK National Archives maintains an online registry of file formats called [http://www.nationalarchives.gov.uk/PRONOM/Default.aspx PRONOM].

In its 2014 agenda, the National Digital Stewardship Alliance recommended developing File Format Action Plans: "it is important to shift from more abstract considerations about file format obsolescence to develop actionable strategies for monitoring and mining information about the heterogeneous digital files the organizations are managing."<ref>{{cite web|publisher=National Digital Stewardship Alliance|title=National Agenda for Digital Stewardship 2014|url=http://www.digitalpreservation.gov/ndsa/documents/2014NationalAgenda.pdf|accessdate=17 March 2014|date=2014}}</ref> 

File Format Action Plans are documents internal to an organization which list the type of digital files in its holdings and assess what actions should be taken to ensure its ongoing accessibility.<ref>{{cite web|last=Owens|first=Trevor|title=File Format Action Plans in Theory and Practice|url=http://blogs.loc.gov/digitalpreservation/2014/01/file-format-action-plans-in-theory-and-practice/|accessdate=17 March 2014|date=6 January 2014}}</ref>  Examples include the [http://fclaweb.fcla.edu/node/795 Florida Digital Archive Action Plan] and University of Michigan's [http://deepblue.lib.umich.edu/static/about/deepbluepreservation.html Deep Blue Preservation and Format Support Policy].

== Copyright issues ==
Untangling [[copyright]] issues also presented a significant challenge for projects attempting to overcome the obsolescence issues related to the BBC Domesday Project. In addition to copyright surrounding the many contributions made by the estimated 1 million people who took part in the project, there are also copyright issues that relate to the technologies employed. It is likely that the Domesday Project will not be completely free of copyright restrictions until at least 2090, unless copyright laws are revised for earlier [[Copyright term|expiration]] of software into [[Public domain software|public domain]].<ref>{{Cite web |url=http://www2.si.umich.edu/CAMILEON/reports/IPRreport.doc|title= The CAMiLEON Project: Legal issues arising from the work aiming to preserve elements of the interactive multimedia work entitled "The BBC Domesday Project."|first= Andrew|last= Charlesworth|date= 5 November 2002|publisher= Information Law and Technology Unit, University of Hull|location= Kingston upon Hull|format= Microsoft Word|accessdate= 23 March 2011}}</ref>

==Intentional obsolescence==
In some cases, obsolete technologies are used in a deliberate attempt to avoid data intrusion in a strategy known as "[[security through obsolescence]]".<ref>{{cite news | url=http://www.linux.com/articles/23313 | title=Security through obsolescence | first=Robin | last=Miller | publisher=Linux.com | date=2002-06-06 | accessdate=2008-07-18}}</ref>

==See also==
* [[Obsolescence]]
* [[Digital preservation]]
* [[Digital Dark Age]]
* [[CAMiLEON]]
* [[Emulation (computing)]]
* [[M-DISC]]

==References==
{{reflist|30em}}

==External links==
* [http://www.digitalpreservation.gov/formats/ The Library of Congress, Sustainability of Digital Formats]
* [[Wired Magazine]]: [http://wired-vig.wired.com/wired/archive/6.09/saved.html What death can't destroy and how to digitize it]
* [https://www.icpsr.umich.edu/icpsrweb/content/datamanagement/preservation/ Digital Preservation at ICPSR]

{{DigitalPreservation}}
{{DEFAULTSORT:Digital Obsolescence}}
[[Category:Data management]]
[[Category:Digital libraries]]
[[Category:Digital preservation]]
[[Category:Future problems]]
[[Category:Obsolescence]]
[[Category:Records management]]
<=====doc_Id=====>:352
<=====title=====>:
Physical data model
<=====text=====>:
{{Refimprove|date=April 2008}}
[[File:Physical Data Model Options.jpg|thumb|320px|Physical Data Model Options.<ref name="WH05">[http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF FEA Consolidated Reference Model Document]. whitehouse.gov May 2005. p.91.  {{webarchive |url=https://web.archive.org/web/20100705040628/http://georgewbush-whitehouse.archives.gov/omb/egov/documents/CRM.PDF |date=July 5, 2010 }}</ref>]]

A '''physical data model''' (or '''[[database design]]''') is a representation of a data design as implemented, or intended to be implemented, in a [[database management system]].  In the [[Project lifecycle | lifecycle of a project]] it typically derives from a [[logical data model]], though it may be [[reverse-engineer]]ed from a given [[database]] implementation.  A complete physical data model will include all the [[database artifact]]s required to create [[relationships between table]]s or to achieve performance goals, such as [[index (database)|index]]es, constraint definitions, linking tables, [[partitioned table]]s or [[cluster (computing)|cluster]]s.  Analysts can usually use a  physical data model to calculate storage estimates; it may include specific storage allocation details for a given database system.

{{As of | 2012}} seven main databases dominate the commercial marketplace: [[Informix Dynamic Server|Informix]], [[Oracle Database|Oracle]], [[PostgreSQL|Postgres]], [[Microsoft SQL Server|SQL Server]], [[Sybase]], [[IBM DB2|DB2]] and [[MySQL]]. Other RDBMS systems tend either to be legacy databases or used within academia such as universities or further education colleges. Physical data models for each implementation would differ significantly, not least due to underlying [[operating system | operating-system]] requirements that may sit underneath them.  For example: SQL Server runs only on [[Microsoft Windows]] operating-systems, while Oracle and MySQL can run on Solaris, Linux and other UNIX-based operating-systems as well as on Windows. This means that the disk requirements, security requirements and many other aspects of a physical data model will be influenced by the RDBMS that a [[database administrator]] (or an organization) chooses to use.

==Physical schema==
''Physical schema'' is a term used in [[data management]] to describe how [[data]] is to be represented and stored (files, indices, ''et al.'') in [[secondary storage]] using a particular [[database management system]] (DBMS) (e.g., Oracle RDBMS, Sybase SQL Server, etc.).

In the [[ANSI-SPARC Architecture|ANSI/SPARC Architecture]] [[three schema approach]], the ''internal schema'' is the view of data that involved data management technology.  This is as opposed to an ''external schema'' that reflects an individual's view of the data, or the ''[[conceptual schema]]'' that is the integration of a set of external schemas.

Subsequently{{Citation needed|date=June 2012}} the internal schema was recognized to have two parts:

The [[logical schema]] was the way data were represented to conform to the constraints of a particular approach to database management.  At that time the choices were hierarchical and network.  Describing the logical schema, however, still did not describe how physically data would be stored on disk drives.  That is the domain of the ''physical schema''.  Now logical schemas describe data in terms of relational ''tables and columns'', object-oriented ''classes'', and [[XML]] ''tags''.

A single set of tables, for example, can be implemented in numerous ways, up to and including an architecture where table rows are maintained on computers in different countries.

==See also==
*[[Database schema]]
*[[Logical schema]]

==References==
{{Reflist}}

{{DEFAULTSORT:Physical Data Model}}
[[Category:Data modeling]]
[[Category:Data management]]
[http://www.whitehouse.gov/sites/default/files/omb/assets/fea_docs/FEA_CRM_v23_Final_Oct_2007_Revised.pdf FEA Consolidated Reference Model Document] (whitehouse.gov) Oct 2007.

[[ja:スキーマ (データベース)]]
<=====doc_Id=====>:355
<=====title=====>:
StoredIQ
<=====text=====>:
{{Underlinked|date=July 2016}}

<!-- Don't mess with this line! --><!-- Write your article below this line -->
'''StoredIQ''' was a company founded for [[information lifecycle management]] (ILM) of unstructured data. Founded in 2001 as Deepfile<ref>{{cite news|title=Deepfile Comes to the Surface|url=http://www.networkcomputing.com/storage/deepfile-comes-surface/865316998|publisher=Network Computing}}</ref> in [[Austin, Texas]] by Jeff Erramouspe, Jeff Bone, Russell Turpin, Rudy Rouhana, Laura Arbilla and Brett Funderburg.<ref>{{cite news|title=Enterprise file management made easy|url=http://www.networkworld.com/article/2332452/wireless/deepfile.html|publisher=Network World}}</ref> The company changed its name in 2005 to StoredIQ<ref>{{cite news|title=Deep file Becomes StoredIQ|url=http://www.networkcomputing.com/storage/deepfile-becomes-storediq/1788209585|publisher=Network Computing}}</ref> and continued to operate successfully for over a decade until it was acquired in 2012 by IBM.<ref>{{cite web|title=IBM Extends ILG Suite and Big Data Governance with StoredIQ Acquisition|url=http://public.dhe.ibm.com/software/data/sw-library/ecm-programs/Parity_Research_StoredIQ_Whitepaper.pdf|website=IBM}}</ref> It now serves as a platform for IBM's information life cycle governance, [[big data]] governance and [[enterprise content management]] technologies.<ref>{{cite web|title=StoredIQ is now an IBM Company|url=https://www-01.ibm.com/software/info/storediq/|website=IBM}}</ref>

StoredIQ was awarded five patents by the USPTO. The first, originally filed in 2003, enabled unstructured data in file systems to be manipulated in a similar way to information stored in databases.<ref>{{cite web|title=Method and apparatus for managing file systems and file-based data storage|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}</ref> Subsequent patents only added to StoredIQ's market dominance by building upon the patented actionable file system with further enhancements specific to Enterprise Policy Management  and expanding the reach of StoredIQ's management capability all the way to individual desktops.<ref>{{cite web|title=Patents by Assignee Storediq, Inc.|url=http://patents.justia.com/assignee/storediq-inc|website=JUSTIA Patents}}</ref>

In 2008 StoredIQ was recognized as "Best in Compliance" by Network Products Guide.<ref>{{cite web|title=StoredIQ Wins Network Products Guide Award For Best In Compliance|url=http://www.datastorageconnection.com/doc/storediq-network-products-best-in-compliance-0001|publisher=Data Storage Connection}}</ref> At the same time, StoredIQ was being recognized as a "Top 5 Provider" by the prestigious Socha-Gelbmann eDiscovery survey.<ref>{{cite web|title=StoredIQ Recognized With "Top 5 Provider" Rating In Socha-Gelbmann eDiscovery Survey|url=http://www.datastorageconnection.com/doc/torediq-ediscovery-survey-storage-0001|publisher=Data Storage Connection}}</ref> This incredible breath of information governance capability is what originally drew the attention of [[EMC Corporation]], StoredIQ's first potential acquirer. Initially a strategic investor in StoredIQ, many experts{{Who|date=August 2016}} predicted an inevitable acquisition. However, the company shunned their first suitor; leaving EMC to acquire a competitor.<ref>{{cite web|title=EMC Acquires Kazeon, Stiffs StoredIQ|url=http://www.informationweek.com/software/information-management/emc-acquires-kazeon-stiffs-storediq/d/d-id/1082836?|publisher=Information Week}}</ref>

The company published a whitepaper titled ''The Truth About Big Data''. This promotion combined with StoredIQ's patented, technology led to [[IBM]] selecting StoredIQ as the basis for some products.<ref>{{cite news|last1=Butta|first1=Tom|title=The Truth Behind IBM’s Plans to Acquire Big Data Company, StoredIQ|url=http://www.huffingtonpost.com/entry/ibm-storediq_b_2377339|publisher=Huffington Post|date=2012-12-31}}</ref>

==References==
{{reflist}}
<!-- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. -->



[[Category:Companies established in 2001]]
[[Category:Companies based in Austin, Texas]]
[[Category:Information technology management]]
[[Category:Data management]]
[[Category:Data warehousing]]
<=====doc_Id=====>:358
<=====title=====>:
Object storage
<=====text=====>:
'''Object storage''' (also known as '''object-based storage'''<ref>{{cite journal|last=Mesnier|first=Mike|author2=Gregory R. Ganger |author3=Erik Riedel |title=Object-Based Storage|journal=IEEE Communications Magazine|date=August 2003|pages=84–90|url=http://www.storagevisions.com/White%20Papers/MesnierIEEE03.pdf|accessdate=27 October 2013|doi=10.1109/mcom.2003.1222722 }}</ref>) is a storage architecture that manages data as objects, as opposed to other storage architectures like [[file systems]] which manage data as a file hierarchy and [[block storage]] which manages data as blocks within sectors and tracks.<ref>{{cite web|last=Porter De Leon|first=Yadin|author2=Tony Piscopo|title=Object Storage versus Block Storage: Understanding the Technology Differences|url=http://www.druva.com/blog/object-storage-versus-block-storage-understanding-technology-differences/|publisher=Druva.com|accessdate=19 January 2015}}</ref> Each object typically includes the data itself, a variable amount of [[metadata]], and a globally unique identifier. Object storage can be implemented at multiple levels, including the device level (object storage device), the system level, and the interface level. In each case, object storage seeks to enable capabilities not addressed by other storage architectures, like interfaces that can be directly programmable by the application, a namespace that can span multiple instances of physical hardware, and data management functions like data replication and data distribution at object-level granularity.

Object storage systems allow relatively inexpensive, scalable and self-healing retention of massive amounts of [[unstructured data]]. Object storage is used for diverse purposes such as storing photos on [[Facebook]], songs on [[Spotify]], or files in online collaboration services, such as [[Dropbox (service)|Dropbox]].<ref>{{cite web|authors=Chandrasekaran, Arun, Dayley, Alan|title=Critical Capabilities for Object Storage|publisher=Gartner Research|date=11 February 2014|url=http://www.gartner.com/technology/reprints.do?id=1-1R78PJ9&ct=140226&st=sb}}</ref>

==History==

===Origins===
In 1995, new research by Garth Gibson, ''et al.'' on [[Network Attached Secure Disks]] first promoted the concept of splitting less common operations, like namespace manipulations, from common operations, like reads and writes, to optimize the performance and scale of both.<ref name="NASD">{{cite web|title=File Server Scaling with Network-Attached Secure Disks|url=http://www.pdl.cmu.edu/ftp/NASD/Sigmetrics97.pdf|publisher=Proceedings of the ACM International Conference on Measurement and Modeling of Computer Systems (Sigmetrics ‘97)|accessdate=27 October 2013|author=Garth A. Gibson |author2=Nagle D. |author3=Amiri K. |author4=Chan F. |author5=Feinberg E. |author6=Gobioff H. |author7=Lee C. |author8=Ozceri B. |author9=Riedel E. |author10=Rochberg D. |author11=Zelenka J.}}</ref>  In the same year, 1995, a Belgium company - FilePool - was established to build the basis for archiving functions by using those and own concepts. Object storage was proposed  at [[Carnegie Mellon University|Carnegie Mellon University's]] Parallel Data Lab as a research project in 1996 .<ref>{{cite web|last1=Factor|first1=Michael|last2=Meth|first2=K.|last3=Naor|first3=D.|last4=Rodeh|first4=O.|last5=Satran |first5=J.|title=Object Storage: The Future Building Block for Storage Systems|url=http://webhdd.ru/library/files/PositionOSD.pdf|publisher=IBM Haifa Research Labs|accessdate=26 September 2013}}</ref>   Another key concept was abstracting the writes and reads of data to more flexible data containers (objects). Fine grained access control through object storage architecture<ref>{{cite web|title=Security for Network Attached Storage Devices (CMU-CS-97-185)|url=http://repository.cmu.edu/cgi/viewcontent.cgi?article=1147&context=pdl|publisher=Parallel Data Laboratory|accessdate=7 November 2013|author=Gobioff, Howard|author2=Gibson, Garth A. |author3= Tygar, Doug |date=1 October 1997}}</ref>  was further described by one of the NASD team, Howard Gobioff, who later was one of the inventors of the [[Google File System]].<ref>{{cite web|title=The Google File System|url=http://research.google.com/archive/gfs-sosp2003.pdf|publisher=Google|accessdate=7 November 2013|author=Sanjay Ghemawat |author2=Howard Gobioff |author3=Shun-Tak Leung|date=October 2003}}</ref>  Other related work includes the [[Coda (file system)|Coda]] filesystem project at [[Carnegie Mellon]], which started in 1987, and spawned the [[Lustre (file system)|Lustre file system]].<ref name="Lustre">{{cite web|last=Braam|first=Peter|title=Lustre: The intergalactic ﬁle system|url=http://ols.fedoraproject.org/OLS/Reprints-2002/braam-reprint.pdf|accessdate=17 September 2013}}</ref> There is also the OceanStore project at UC Berkeley,<ref>{{cite web|title=OceanStore|url=http://oceanstore.cs.berkeley.edu/|accessdate=18 September 2013}}</ref> which started in 1999.<ref>{{cite journal|last1=Kubiatowicz|first1=John|last2=Bindel|first2=D.|last3=Chen|first3=Y.|last4=Czerwinski|first4=S.|last5=Eaton|first5=P.|last6=Geels|first6=D.|last7=Gummadi|first7=R.|last8=Rhea|first8=S.|last9=Weatherspoon|first9=H.|last10=Weimer |first10=W.|last11=Wells|first11=C.|last12=Zhao|first12=B.|title=OceanStore: An Architecture for Global-Scale Persistent Storage|journal=Proceedings of the Ninth international Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS 2000)|date=November 2000|url=http://oceanstore.cs.berkeley.edu/publications/papers/pdf/asplos00.pdf|accessdate=18 September 2013}}</ref>

One of the earliest and best-known object storage products, EMC's Centera, debuted in 2002.<ref>{{cite news|title=EMC Unveils Low-Cost Data-Storage Product|url=http://articles.latimes.com/2002/apr/30/business/fi-techbriefs30.3|accessdate=17 September 2013|newspaper=LA Times|date=April 30, 2002}}</ref> [[Content-addressable storage|Centera's technology]]  has been developed at Filepool and the company had been acquired  by EMC² in 2002.

===Development===
Overall industry investment in object storage technology has been sustained for over a decade. From 1999 to 2013, there has been at least $300 million of venture financing related to object storage, including vendors like Amplidata, Bycast, Cleversafe, Cloudian, Nirvanix, and Scality.<ref>{{cite web|last=Leung|first=Leo|title=After 10 years, object storage investment continues and begins to bear significant fruit|url=http://blog.oxygencloud.com/2013/09/16/after-10-years-object-storage-investment-continues-and-begins-to-bear-significant-fruit/|accessdate=17 September 2013|date=16 September 2013}}</ref> This doesn't include millions of dollars of private engineering from systems vendors like DataDirect Networks (WOS), [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&tab14=0 Dell EMC Elastic Cloud Storage], Centera, [[Atmos]], HDS (HCP), HP ([[HP OpenStack]]), IBM, NetApp (StorageGRID), Redhat GlusterFS and [http://www.keepertech.com Keeper Technology] ([http://www.keepertech.com/products/keepersafe/ keeperSAFE]), cloud services vendors like Amazon ([[AWS S3]]), Microsoft ([[Microsoft Azure]]) and Google ([[Google Cloud Storage]]), or the many man years of open source development at [[Lustre (file system)|Lustre]], OpenStack ([[OpenStack#Object Storage .28Swift.29|Swift]]), MogileFS, [[Ceph (file system)|Ceph]], [[Skylable SX (object storage)|Skylable SX]] and OpenIO.<ref name="Mellor">{{cite web|last=Mellor|first=Chris (Dec. 2, 2015)|title=Openio's objective is opening up object storage space|url=http://www.theregister.co.uk/2015/12/02/openio_object_storage_upstart/}}</ref><ref name="Nicolas">{{cite web|last=Nicolas|first=Philippe (Oct. 2, 2015)|title=OpenIO, ready to take off|url=http://filestorage.blogspot.fr/2015/10/openio-ready-to-take-off.html/}}</ref><ref name="Raffo">{{cite web|last=Raffo|first=Dave (May 20, 2016)|title=OpenIO joins object storage cloud scrum|url=http://searchcloudstorage.techtarget.com/news/450296765/OpenIO-joins-object-storage-cloud-scrum/}}</ref><ref name="Maleval">{{cite web|last=Maleval|first=Jean-Jacques (Apr. 25, 2016)|title=Start-Up Profile: OpenIO|url=http://www.storagenewsletter.com/rubriques/start-ups/start-up-profile-openio/}}</ref>

A great article written by Philippe Nicolas illustrating products' timeline was published in July 2016 on The Register with all players, pioneers, mergers and acquisitions and of course genesis with CAS included.<ref>{{cite web|last=Nicolas|first=Philippe (July 15, 2016)|title=The History Boys: Object storage ... from the beginning|url=http://www.theregister.co.uk/2016/07/15/the_history_boys_cas_and_object_storage_map/}}</ref>

==Architecture==
[[File:High level object storage architecture.png|thumb]]

===Abstraction of storage===
One of the design principles of object storage is to abstract some of the lower layers of storage away from the administrators and applications. Thus, data is exposed and managed as objects instead of files or blocks. Objects contain additional descriptive properties which can be used for better indexing or management. Administrators do not have to perform lower level storage functions like constructing and managing [[Logical unit number|logical volumes]] to utilize disk capacity or setting [[RAID]] levels to deal with disk failure.

Object storage also allows the addressing and identification of individual objects by more than just file name and file path. Object storage adds a unique identifier within a bucket, or across the entire system, to support much larger namespaces and eliminate name collisions.

=== Inclusion of rich custom metadata within the object ===
Object storage explicitly separates file metadata from data to support additional capabilities:
As opposed to fixed metadata in file systems (filename, creation date, type, etc.), object storage provides for full function, custom, object-level metadata in order to:
* Capture application-specific or user-specific information for better indexing purposes
* Support data management policies (e.g. a policy to drive object movement from one storage tier to another)
* Centralize management of storage across many individual nodes and clusters
* Optimize metadata storage (e.g. encapsulated, database or key value storage) and caching/indexing (when authoritative metadata is encapsulated with the metadata inside the object) independently from the data storage (e.g. unstructured binary storage)

Additionally, in some object-based file system implementations:
* The file system clients only contact metadata servers once when the file is opened and then get content directly via object storage servers (vs. block-based file systems which would require constant metadata access)
* Data objects can be configured on a per-file basis to allow adaptive stripe width, even across multiple object storage servers, supporting optimizations in bandwidth and I/O

'''Object-based storage devices''' ('''OSD''') as well as some software implementations (e.g., Caringo Swarm) manage metadata and data at the storage device level:
* Instead of providing a block-oriented interface that reads and writes fixed sized blocks of data, data is organized into flexible-sized data containers, called objects
* Each object has both data (an uninterpreted sequence of bytes) and metadata (an extensible set of attributes describing the object); physically encapsulating both together benefits recoverability.
* The command interface includes commands to create and delete objects, write bytes and read bytes to and from individual objects, and to set and get attributes on objects
* Security mechanisms provide per-object and per-command access control

===Programmatic data management===
Object storage provides programmatic interfaces to allow applications to manipulate data. At the base level, this includes [[CRUD]] functions for basic read, write and delete operations. Some object storage implementations go further, supporting additional functionality like object versioning, object replication, and movement of objects between different tiers and types of storage. Most API implementations are [[Representational state transfer|ReST]]-based, allowing the use of many standard [[HTTP]] calls.

==Implementation==

===Object-based storage devices===
Object storage at the protocol and device layer was proposed 20 years ago and approved for the [[SCSI]] command set nearly 10 years ago as "Object-based Storage Device Commands" (OSD),<ref>{{cite web|last=Riedel|first=Erik|title=Object Storage and Applications|url=https://www.usenix.org/legacy/event/lsf07/tech/riedel.pdf|accessdate=3 November 2013|author2=Sami Iren |date=February 2007}}</ref> but has not been productized until the development of the Seagate Kinetic Open Storage platform.<ref>{{cite web|title=The Seagate Kinetic Open Storage Vision|url=http://www.seagate.com/tech-insights/kinetic-vision-how-seagate-new-developer-tools-meets-the-needs-of-cloud-storage-platforms-master-ti/|publisher=Seagate|accessdate=3 November 2013}}</ref><ref>{{cite news|last=Gallagher|first=Sean|title=Seagate introduces a new drive interface: Ethernet|url=http://arstechnica.com/information-technology/2013/10/seagate-introduces-a-new-drive-interface-ethernet/|accessdate=3 November 2013|newspaper=Arstechnica.com|date=27 October 2013}}</ref>  The [[SCSI]] command set for Object Storage Devices was developed by a working group of the [[Storage Networking Industry Association]] (SNIA) for the T10 committee of the [[International Committee for Information Technology Standards]] (INCITS).<ref>{{cite web|last=Corbet|first=Jonathan|title=Linux and object storage devices|url=https://lwn.net/Articles/305740/|accessdate=8 November 2013|newspaper=LWN.net|date=4 November 2008}}</ref>  T10 is responsible for all SCSI standards.

===Object-based file systems===
Some distributed file systems use an object-based architecture, where file metadata is stored in metadata servers and file data is stored in object storage servers. File system client software interacts with the distinct servers, and abstracts them to present a full file system to users and applications. [[IBM General Parallel File System|IBM Spectrum Scale (also known as GPFS)]], [http://www.emc.com/en-us/storage/ecs/index.htm#collapse=&tab14=0 Dell EMC Elastic Cloud Storage], [[Ceph (software)|Ceph]], [[XtreemFS]], and [[Lustre (file system)|Lustre]] are examples of this type of object storage.

===Archive storage===
Some early incarnations of object storage were used for archiving, as implementations were optimized for data services like immutability, not performance. [[Content-addressable storage|EMC Centera]] and Hitachi HCP (formerly known as HCAP) are two commonly cited object storage products for archiving. Another example is Quantum Lattus Object Storage Platform.

===Cloud storage===
The vast majority of cloud storage available in the market leverages an object storage architecture. Two notable examples are [[AWS S3|Amazon Web Services S3]], which debuted in 2005, and [[Rackspace]] Files (whose code was released as [[OpenStack#Swift|OpenStack Swift]]). Other major cloud storage services include Microsoft Azure, Google Cloud Storage, Alibaba Cloud OSS, Oracle Elastic Storage Service and DreamHost based on Ceph.

==="Captive" object storage===
Some large internet companies developed their own software when object storage products were not commercially available or use cases were very specific. Facebook famously invented their own object storage software, code-named Haystack, to address their particular massive scale photo management needs efficiently.<ref name="haystack">{{cite web|last=Vajgel|first=Peter|title=Needle in a haystack: efficient storage of billions of photos|url=https://www.facebook.com/note.php?note_id=76191543919|accessdate=17 September 2013}}</ref>

===Hybrid storage===
A few object storage systems, such as [[Ceph (software)|Ceph]], [[GlusterFS]], [[Cloudian]],<ref name="Primesberger">{{cite web|last=Primesberger|first=Chris (27 October 2016)|title=Cloudian Raises $41 Million VC for Hybrid Cloud Object Storage|url=http://www.eweek.com/storage/cloudian-raises-41-million-vc-for-hybrid-cloud-object-storage.html}}</ref> and [[Scality]] support Unified File and Object (UFO) storage, allowing some clients to store objects on a storage system while simultaneously other clients store files on the same storage system. While "hybrid storage" is not a widely accepted term for this concept, interoperable interfaces to the same set of data is becoming available in some object storage products.

===Virtual object storage===
In addition to object storage systems that own the managed files, some systems provide an object abstraction on top of one or more traditional filesystem based solutions. These solutions do not own the underlaying raw storage, but instead actively mirror the filesystem changes and replicate them in their own object catalog, alongside any metadata that can be automatically extracted from the files. Users can then contribute additional metadata through the virtual object storage APIs. A global namespace and replication capabilities both inside and across filesystems are typically supported.

Notable examples in this category are [[Nirvana (software)|Nirvana]], and its open-source cousin iRODS.

Most products in this category have recently extended their capabilities to support other Object Store solutions as well.

===Object storage systems===
More general purpose object storage systems came to market around 2008. Lured by the incredible growth of "captive" storage systems within web applications like Yahoo Mail and the early success of cloud storage, object storage systems promised the scale and capabilities of cloud storage, with the ability to deploy the system within an enterprise, or at an aspiring cloud storage service provider. Notable examples of object storage systems include [[EMC Atmos]], [[OpenStack#Object Storage (Swift)|OpenStack Swift]], [[Scality|Scality RING]], Caringo Swarm<ref>{{cite web|last=Nicolas|first=Philippe (Sept. 21, 2009)|title=Caringo FileFly, back to the future|url=http://continuousdataprotection.blogspot.fr/2015/09/caringo-filefly-back-to-future.html}}</ref> (formerly CAStor), [[Cloudian]],<ref name="Primesberger"/> and OpenIO.<ref name="Mellor"/>

==Market adoption==
[[File:Titan supercomputer at the Oak Ridge National Laboratory.jpg|thumb|The Titan supercomputer at Oak Ridge National Laboratory]]
One of the first object storage products, Lustre, is used in 70% of the Top 100 supercomputers and ~50% of the [[Top 500]].<ref>{{cite web|last=Dilger|first=Andreas|title=Lustre Future Development|url=http://storageconference.org/2012/Presentations/M04.Dilger.pdf|publisher=IEEE MSST|accessdate=27 October 2013}}</ref> As of June 16, 2013, this includes 7 of the top 10, including the current fastest system on the list - China's Tianhe-2 and the second fastest, the [[Titan (supercomputer)|Titan supercomputer]] at [[Oak Ridge National Laboratory]] (pictured on the right).<ref>{{cite web|title=Datadirect Networks to build world's fastest storage system for Titan, the world's most powerful supercomputer|url=http://www.multivu.com/mnr/60497-datadirect-networks-titan-supercomputer-storage-system-ornl|accessdate=27 October 2013}}</ref>

Object storage systems had good adoption in the early 2000s as an archive platform, particularly in the wake of compliance laws like [[Sarbanes-Oxley]]. After five years in the market, EMC's Centera product claimed over 3,500 customers and 150 [[petabytes]] shipped by 2007.<ref>{{cite web|title=EMC Marks Five Years of EMC Centera Innovation and Market Leadership|url=http://www.emc.com/about/news/press/us/2007/04182007-5028.htm|publisher=EMC|accessdate=3 November 2013|date=18 April 2007}}</ref> Hitachi's HCP product also claims many [[petabyte]]-scale customers.<ref>{{cite web|title=Hitachi Content Platform Supports Multiple Petabytes, Billions of Objects|url=http://www.techvalidate.com/portals/hitachi-content-platform-customers-with-more-than-1pb-of-data-stored|publisher=Techvalidate.com|accessdate=19 September 2013}}</ref> Newer object storage systems have also gotten some traction, particularly around very large custom applications like eBay's auction site, where EMC Atmos is used to manage over 500 million objects a day.<ref>{{cite news|last=Robb|first=Drew|title=EMC World Continues Focus on Big Data, Cloud and Flash|url=http://www.infostor.com/backup-and_recovery/cloud-storage/emc-world-continues-focus-on-big-data-cloud-and-flash-.html|accessdate=19 September 2013|newspaper=Infostor|date=11 May 2011}}</ref> As of March 3, 2014, EMC claims to have sold over 1.5 exabytes of Atmos storage.<ref>{{cite web|last=Hamilton|first=George|title=In it for the Long Run: EMC's Object Storage Leadership|url=http://www.rethinkstorage.com/in-it-for-the-long-run-emcs-object-storage-leadership#.UyEzj9yllFI|accessdate=15 March 2014}}</ref> On July 1, 2014, [[Los Alamos National Lab]] chose the [[Scality|Scality RING]] as the basis for a 500 petabyte storage environment, which would be among the largest ever.<ref>{{cite news|last1=Mellor|first1=Chris|title=Los Alamos National Laboratory likes it, puts Scality's RING on it|url=http://www.theregister.co.uk/2014/07/01/scalitys_ring_goes_faster/|accessdate=26 January 2015|publisher=The Register|date=1 July 2014}}</ref>

"Captive" object storage systems like Facebook's Haystack have scaled impressively. In April 2009, Haystack was managing 60 billion photos and 1.5 petabytes of storage, adding 220 million photos and 25 terabytes a week.<ref name="haystack" /><ref>{{cite web|last=Nicolas|first=Philippe (Sept. 13, 2009)|title=Haystack chez Facebook|url=http://filestorage.blogspot.com/2009/09/haystack-chez-facebook.html}}</ref> Facebook more recently stated that they were adding 350 million photos a day and were storing 240 billion photos.<ref>{{cite news|last=Miller|first=Rich|title=Facebook Builds Exabyte Data Centers for Cold Storage|url=http://www.datacenterknowledge.com/archives/2013/01/18/facebook-builds-new-data-centers-for-cold-storage/|accessdate=6 November 2013|newspaper=Datacenterknowledge.com|date=13 January 2013}}</ref> This could equal as much as 357 petabytes.<ref>{{cite web|last=Leung|first=Leo|title=How much data does x store?|url=http://techexpectations.org/2014/05/17/how-much-data-does-x-store/|publisher=Techexpectations.org|accessdate=23 May 2014|date=17 May 2014}}</ref>

Cloud storage has become pervasive as many new web and mobile applications choose it as a common way to store [[binary data]].<ref>{{cite web|last=Leung|first=Leo|title=Object storage already dominates our days (we just didn’t notice)|url=http://blog.oxygencloud.com/2012/01/11/object-storage-already-dominates/|accessdate=27 October 2013|date=January 11, 2012}}</ref>  As the storage backend to many popular applications like [[Smugmug]] and [[Dropbox (service)|Dropbox]], AWS S3 has grown to massive scale, citing over 2 trillion objects stored in April 2013.<ref>{{cite news|last=Harris|first=Derrick|title=Amazon S3 goes exponential, now stores 2 trillion objects|url=http://gigaom.com/2013/04/18/amazon-s3-goes-exponential-now-stores-2-trillion-objects/|accessdate=17 September 2013|newspaper=Gigaom|date=18 April 2013}}</ref> Two months later, Microsoft claimed that they stored even more objects in Azure at 8.5 trillion.<ref>{{cite news|last=Wilhelm|first=Alex|title=Microsoft: Azure powers 299M Skype users, 50M Office Web Apps users, stores 8.5T objects|url=http://thenextweb.com/microsoft/2013/06/27/microsoft-our-cloud-powers-hundreds-of-millions/|accessdate=18 September 2013|newspaper=thenextweb.com|date=27 June 2013}}</ref> By April 2014, Azure claimed over 20 trillion objects stored.<ref>{{cite news|last1=Nelson|first1=Fritz|title=Microsoft Azure's 44 New Enhancements, 20 Trillion Objects|url=http://www.tomsitpro.com/articles/microsoft-azure-paas-iaas-cloud-computing,1-1841.html|accessdate=3 September 2014|publisher=Tom's IT Pro|date=4 April 2014}}</ref> Windows Azure Storage manages Blobs (user files), Tables (structured storage), and Queues (message delivery) and counts them all as objects.<ref>{{cite web|last=Calder|first=Brad|title=Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency|url=http://sigops.org/sosp/sosp11/current/2011-Cascais/printable/11-calder.pdf|publisher=Microsoft|accessdate=6 November 2013|location=23rd ACM Symposium on Operating Systems Principles (SOSP)}}</ref>

==Market analysis==
[[International Data Corporation|IDC]] has begun to assess the object-based storage market annually using its MarketScape methodology. IDC describes the MarketScape as: "...a quantitative and qualitative assessment of the characteristics that assess a vendor's current and future success in the said market or market segment and provide a measure of their ascendancy to become a Leader or maintain a leadership. IDC MarketScape assessments are particularly helpful in emerging markets that are often fragmented, have several players, and lack clear leaders."<ref>{{cite web|last1=Nadkarni|first1=Ashish|title=IDC MarketScape: Worldwide Object-Based Storage 2013 Vendor Assessment|url=http://www.idc.com/getdoc.jsp?containerId=244081|website=http://www.idc.com|publisher=IDC|accessdate=26 January 2015}}</ref>

In 2013, IDC rated [[Cleversafe]], [[Scality]], [[DataDirect Networks]], [[Amplidata]], and [[EMC Corporation|EMC]] as leaders.<ref>{{cite news|last1=Mellor|first1=Chris|title=IDC's explicit snapshot: Everyone who's anyone in object storage: In 3D|url=http://www.theregister.co.uk/2013/11/27/idcs_objectscape_pretty_as_a_picture/|accessdate=26 January 2015|publisher=The Register|date=27 November 2013}}</ref> In 2014, it rated [[Scality]], [[Cleversafe]], [[DataDirect Networks]], [[Hitachi Data Systems]], [[Amplidata]], [[EMC Corporation|EMC]], and [[Cloudian]]<ref>{{cite web|last=Nicolas|first=Philippe (Sept. 14, 2015)|title=Cloudian shakes the object storage market|url=http://filestorage.blogspot.fr/2015/09/cloudian-shakes-object-storage-market.html}}</ref><ref>{{cite web|last=Mellor|first=Chris (June. 21, 2016)|title=Cloudian clobbers car drivers with targeted ads|url=http://www.theregister.co.uk/2016/06/21/cloudian_could_clobber_car_drives_with_targeted_ads/}}</ref><ref>{{cite web|last=Nicolas|first=Philippe (June. 22, 2016)|title=Cloudian is the real S3 leader|url=http://filestorage.blogspot.fr/2016/06/cloudian-is-real-s3-leader.html}}</ref> as leaders.<ref>{{cite news|last1=Mellor|first1=Chris|title=IDC: Who's HOT and who's NOT (in object storage) in 2014|url=http://www.theregister.co.uk/2015/01/06/idc_shows_emcs_object_presence_shrinking/|accessdate=26 January 2015|publisher=The Register|date=6 January 2015}}</ref><ref>{{cite web|last=Mellor|first=Chris (Nov. 24, 2015)|title=We pick storage brains: Has object storage endgame started?|url=http://www.channelregister.co.uk/2015/11/24/object_storage_endgame/}}</ref><ref>{{cite web|last=Nicolas|first=Philippe (Oct. 19, 2015)|title=Red alert for Object Storage vendors|url=http://filestorage.blogspot.com/2015/10/red-alert-for-object-storage-vendors.html}}</ref>

==Standards==

===Object-based storage device standards===

====OSD version 1====
In the first version of the OSD standard,<ref>{{cite web|title=INCITS 400-2004|url=http://www.techstreet.com/cgi-bin/detail?product_id=1204555|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013}}</ref> objects are specified with a 64-bit partition ID and a 64-bit object ID. Partitions are created and deleted within an OSD, and objects are created and deleted within partitions. There are no fixed sizes associated with partitions or objects; they are allowed to grow subject to physical size limitations of the device or logical quota constraints on a partition.

An extensible set of attributes describe objects. Some attributes are implemented directly by the OSD, such as the number of bytes in an object and the modify time of an object. There is a special policy tag attribute that is part of the security mechanism. Other attributes are uninterpreted by the OSD. These are set on objects by the higher-level storage systems that use the OSD for persistent storage. For example, attributes might be used to classify objects, or to capture relationships among different objects stored on different OSDs.

A list command returns a list of identifiers for objects within a partition, optionally filtered by matches against their attribute values. A list command can also return selected attributes of the listed objects.

Read and write commands can be combined, or piggy-backed, with commands to get and set attributes. This ability reduces the number of times a high-level storage system has to cross the interface to the OSD, which can improve overall efficiency.

====OSD version 2====
A second generation of the SCSI command set, "Object-Based Storage Devices - 2" (OSD-2) added support for snapshots, collections of objects, and improved error handling.<ref>{{cite web|title=INCITS 458-2011|url=http://www.techstreet.com/products/1801667|publisher=InterNational Committee for Information Technology Standards|accessdate=8 November 2013|date=15 March 2011}}</ref>

A [[snapshot (computer storage)|snapshot]] is a point in time copy of all the objects in a partition into a new partition. The OSD can implement a space-efficient copy using [[copy-on-write]] techniques so that the two partitions share objects that are unchanged between the snapshots, or the OSD might physically copy the data to the new partition. The standard defines clones, which are writeable, and snapshots, which are read-only.

A collection is a special kind of object that contains the identifiers of other objects. There are operations to add and delete from collections, and there are operations to get or set attributes for all the objects in a collection. Collections are also used for error reporting.  If an object becomes damaged by the occurrence of a media defect (i.e., a bad spot on the disk) or by a software error within the OSD implementation, its identifier is put into a special error collection. The higher-level storage system that uses the OSD can query this collection and take corrective action as necessary.

==Differences between Key-Value and Object Stores==
{{Disputed|date=December 2015}}
Let’s first clarify what a key/value store and an object store are. Using the traditional block storage interface, one has a series of fixed size blocks which are numbered starting at 0. Data must be that exact fixed size and can be stored in a particular block which is identified by its logical block number (LBN). Later, one can retrieve that block of data by specifying its unique LBN.

With a key/value store, data is identified by a key rather than a LBN. A key might be "cat" or "olive" or "42". It can be an arbitrary sequence of bytes of arbitrary length. Data (called a value in this parlance) does not need to be a fixed size and also can be an arbitrary sequence of bytes of arbitrary length. One stores data by presenting the key and data (value) to the data store and can later retrieve the data by presenting the key. You’ve seen this concept before in programming languages. Python calls them dictionaries, Perl calls them hashes, Java and C++ call them maps, etc. Several data stores also implement key/value stores such as Memcached, Redis and CouchDB.

Object stores are similar to key/value stores except that the key must be a positive integer like a LBN. However, unlike a LBN, the key can be any positive integer; it does not have to map to an existing logical block number. In practice, it is usually limited to 64 bits. More like a key/value store than the traditional block storage interface, data is not limited to a fixed size block but may be an arbitrary size. Object stores also allow one to associate a limited set of attributes with each piece of data. The key, value and set of attributes is referred to as an object. To add more confusion, sometimes key/value stores are loosely referred to as object stores but technically there is a difference.<ref>http://blog.gigaspaces.com/were-flash-keyvalue-and-object-stores-made-for-each-other-guest-post-by-johann-george-sandisk/</ref>

==See also==
*[[Cloud storage]]
*[[Clustered file system]]
*[[Object access method]]

==References==
{{Reflist|2}}
<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->
*
*
*
*

==External links==
*[http://docs.aws.amazon.com/AmazonS3/latest/API/Welcome.html AWS S3 API Documentation]
*[https://developers.google.com/storage/ Google Cloud Storage API Documentation]
*[http://docs.openstack.org/developer/swift/ Openstack Swift API Documentation]
*[https://developers.seagate.com/display/KV/Kinetic+Open+Storage+Documentation+Wiki Seagate Kinetic Open Storage Documentation]
*[http://msdn.microsoft.com/en-us/library/windowsazure/dd179355.aspx Windows Azure Storage API Documentation]
*[https://nkolayofis.com a Saas solution in Turkey]
*[https://quictransfer.com a Cloud storage]


[[Category:Data management]]
[[Category:Data management software]]
[[Category:Computer file systems]]
[[Category:Computer data storage]]
[[Category:Network file systems]]
[[Category:Cloud storage]]
<=====doc_Id=====>:361
<=====title=====>:
Data
<=====text=====>:
{{about||data in computer science|Data (computing)|other uses}}
{{pp-move-indef}}
[[File:Data types - en.svg|thumb|right|200px|Some of the different types of data.]]
'''Data''' ({{IPAc-en|ˈ|d|eɪ|t|ə}} {{respell|DAY|tə}}, {{IPAc-en|ˈ|d|æ|t|ə}} {{respell|DA|tə}}, or {{IPAc-en|ˈ|d|ɑː|t|ə}} {{respell|DAH|tə}})<ref>The pronunciation {{IPAc-en|ˈ|d|eɪ|t|ə}} {{respell|DAY|tə}} is widespread throughout most varieties of English. The pronunciation {{IPAc-en|ˈ|d|æ|t|ə}} {{respell|DA|tə}} is chiefly [[Hiberno-English|Irish]] and [[American English|North American]]. The pronunciation {{IPAc-en|ˈ|d|ɑː|t|ə}} {{respell|DAH|tə}} is chiefly [[Australian English|Australian]], [[New Zealand English|New Zealand]] and [[South African English|South African]]. Each pronunciation may be realized differently depending on the dialect/language of the speaker.</ref> is a [[set (mathematics)|set]] of values of [[Qualitative data|qualitative]] or [[Quantitative data|quantitative]] variables. An example of qualitative data would be an [[anthropologist]]'s handwritten notes about her interviews with people of an Indigenous tribe. Pieces of data are individual pieces of [[information]]. While the concept of data is commonly associated with [[scientific research]], data is collected by a huge range of organizations and institutions, including businesses (e.g., sales data, revenue, profits, [[stock price]]), governments (e.g., [[crime rate]]s, [[unemployment rate]]s, [[literacy]] rates) and non-governmental organizations (e.g., censuses of the number of [[homelessness|homeless people]] by non-profit organizations).

Data is [[measurement|measured]], [[data reporting|collected and reported]], and [[data analysis|analyzed]], whereupon it can be [[data visualization|visualized]] using graphs, images or other analysis tools. Data as a general [[concept]] refers to the fact that some existing [[information]] or [[knowledge]] is ''[[Knowledge representation and reasoning|represented]]'' or ''[[code]]d'' in some form suitable for better usage or [[data processing|processing]]. ''[[Raw data]]'' ("unprocessed data") is a collection of [[number]]s or [[character (computing)|characters]] before it has been "cleaned" and corrected by researchers. Raw data needs to be corrected to remove [[outlier]]s or obvious instrument or data entry errors (e.g., a thermometer reading from an outdoor Arctic location recording a tropical temperature).  Data processing commonly occurs by stages, and the "processed data" from one stage may be considered the "raw data" of the next stage. [[Field work|Field data]] is raw data that is collected in an uncontrolled "[[in situ]]" environment. [[Experimental data]] is data that is generated within the context of a scientific investigation by observation and recording. Data has been described as the new [[Petroleum|oil]] of the [[digital economy]].<ref>[https://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Data Is the New Oil of the Digital Economy]</ref><ref>[https://spotlessdata.com/blog/data-new-oil Data is the new Oil]</ref>

== Etymology and terminology ==
The first English use of the word "data" is from the 1640s. Using the word "data" to mean "transmittable and storable computer information" was first done in 1946. The expression "data processing" was first used in 1954.<ref name="eol">http://www.etymonline.com/index.php?term=data</ref>

The [[Data (word)|Latin word ''data'']] is the plural of ''datum'', "(thing) given," neuter past participle of ''dare'' "to give".<ref name="eol"/>  Data may be used as a plural noun in this sense, with some writers in the 2010s using ''datum'' in the singular and ''data'' for plural. In the 2010s, though, in non-specialist, everyday writing, "data" is most commonly used in the singular, as a [[mass noun]] (like "information", "sand" or "rain").<ref>{{cite web|last=Hickey |first=Walt |url=http://fivethirtyeight.com/datalab/elitist-superfluous-or-popular-we-polled-americans-on-the-oxford-comma/ |title=Elitist, Superfluous, Or Popular? We Polled Americans on the Oxford Comma |publisher=FiveThirtyEight |date=2014-06-17 |accessdate=2015-05-04}}</ref>

== Meaning ==
Data, [[information]], [[knowledge]] and [[wisdom]] are closely related concepts, but each has its own role in relation to the other, and each term has its own meaning. Data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. <ref>{{cite web|title=Joint Publication 2-0, Joint Intelligence|url=http://www.dtic.mil/doctrine/new_pubs/jp2_0.pdf|work=Defense Technical Information Center (DTIC)|publisher=Department of Defense|accessdate=February 22, 2013|pages=GL-11|date=22 June 2007}}</ref> [[Knowledge]] is derived from extensive amounts of experience dealing with information on a subject. For example, the height of [[Mount Everest]] is generally considered data. The height can be recorded precisely with an [[altimeter]] and entered into a database. This data may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to make a decision about the best method to climb it. Using an understanding based on experience climbing mountains to advise persons on the way to reach Mount Everest's peak may be seen as "knowledge". Some complement the series "data", "information" and "knowledge" with "wisdom", which would mean the status of a person in possession of a certain "knowledge" who also knows under which circumstances is good to use it.

Data is the least abstract concept, information the next least, and knowledge the most abstract.<ref>{{cite web|author=Akash Mitra|year=2011|title=Classifying data for successful modeling|url=http://www.dwbiconcepts.com/data-warehousing/12-data-modelling/101-classifying-data-for-successful-modeling.html}}</ref> Data becomes information by interpretation; e.g., the height of Mount Everest is generally considered "data", a book on Mount Everest geological characteristics may be considered "information", and a climber's guidebook containing practical information on the best way to reach Mount Everest's peak may be considered "knowledge". "Information" bears a diversity of meanings that ranges from everyday usage to technical use. Generally speaking, the concept of information is closely related to notions of constraint, communication, control, data, form, instruction, knowledge, meaning, mental stimulus, pattern, perception, and representation.<!--given by nupur seth--> Beynon-Davies uses the concept of a [[sign]] to differentiate between data and information; data is a series of symbols, while information occurs when the symbols are used to refer to something.<ref>{{cite book|author=P. Beynon-Davies|year=2002|title=Information Systems: An introduction to  informatics in organisations|publisher=[[Palgrave Macmillan]] |location=Basingstoke, UK|isbn=0-333-96390-3}}</ref><ref>{{cite book|author=P. Beynon-Davies|year=2009|title=Business information systems|publisher=Palgrave |location=Basingstoke, UK|isbn=978-0-230-20368-6}}</ref>

Before the development of computing devices and machines, only people could collect data and impose patterns on it. Since the development of computing devices and machines, these devices can also collect data. In the 2010s, computers are widely used in many fields to collect data and sort or process it, in disciplines ranging from [[marketing]], analysis of [[social services]] usage by citizens to scientific research. These patterns in data are seen as information which can be used to enhance knowledge. These patterns may be interpreted as "[[truth]]" (though "truth" can be a subjective concept), and may be authorized as aesthetic and ethical criteria in some disciplines or cultures. Events that leave behind perceivable physical or virtual remains can be traced back through data. Marks are no longer considered data once the link between the mark and observation is broken.<ref>{{cite book|author=Sharon Daniel|title=The Database: An Aesthetics of Dignity}}</ref>

Mechanical computing devices are classified according to the means by which they represent data. An [[analog computer]] represents a datum as a voltage, distance, position, or other physical quantity. A [[Computer|digital computer]] represents a piece of data as a sequence of symbols drawn from a fixed [[alphabet]]. The most common digital computers use a binary alphabet, that is, an alphabet of two characters, typically denoted "0" and "1". More familiar representations, such as numbers or letters, are then constructed from the binary alphabet. Some special forms of data are distinguished. A [[computer program]] is a collection of data, which can be interpreted as instructions. Most computer languages make a distinction between programs and the other data on which programs operate, but in some languages, notably [[Lisp (programming language)|Lisp]] and similar languages, programs are essentially indistinguishable from other data. It is also useful to distinguish [[metadata]], that is, a description of other data. A similar yet earlier term for metadata is "ancillary data."  The prototypical example of metadata is the library catalog, which is a description of the contents of books.

== In other fields ==
Though data is also increasingly used in other fields, it has been suggested that the highly interpretive nature of them might be at odds with the ethos of data as "given". Peter Checkland introduced the term ''capta'' (from the Latin ''capered'', “to take”) to distinguish between an immense number of possible data and a sub-set of them, to which attention is oriented.<ref>{{cite book | author = P. Checkland and S. Holwell | title = Information, Systems, and Information Systems: Making Sense of the Field. | year = 1998 | publisher = John Wiley & Sons | location = Chichester, West Sussex | isbn = 0-471-95820-4 | pages = 86–89  }}</ref> [[Johanna Drucker]] has argued that since the humanities affirm knowledge production as "situated, partial, and constitutive," using ''data'' may introduce assumptions that are counterproductive, for example that phenomena are discrete or are observer-independent.<ref>{{cite web
 |author=Johanna Drucker
 |year=2011
 |title=Humanities Approaches to Graphical Display
 |url=http://www.digitalhumanities.org/dhq/vol/5/1/000091/000091.html
}}</ref> The term ''capta'', which emphasizes the act of observation as constitutive, is offered as an alternative to ''data'' for visual representations in the humanities.

== See also ==
{{div col|5}}
* [[Biological data]]
* [[Data acquisition]]
* [[Data analysis]]
* [[Data cable]]
* [[Dark data]]
* [[Data domain]]
* [[Data element]]
* [[Data farming]]
* [[Data governance]]
* [[Data integrity]]
* [[Data maintenance]]
* [[Data management]]
* [[Data mining]]
* [[Data modeling]]
* [[Data visualization]]
* [[Computer data processing]]
* [[Data publication]]
* [[Information privacy|Data protection]]
* [[Data remanence]]
* [[Data set]]
* [[Data warehouse]]
* [[Database]]
* [[Datasheet]]
* [[Environmental data rescue]]
* [[Fieldwork]]
* [[Metadata]]
* [[Open data]]
* [[Scientific data archiving]]
* [[Statistics]]
* [[Computer memory]]
* [[Data structure]]
* [[Raw Data]]
* [[Secondary Data]]
{{div col end}}

== References ==
{{FOLDOC}}
{{Reflist}}

== External links ==
{{Wiktionary}}
* [http://purl.org/nxg/note/singular-data Data is a singular noun] (a detailed assessment)

{{Statistics}}

[[Category:Computer data| ]]
[[Category:Data| ]]
[[Category:Data management]]
<=====doc_Id=====>:364
<=====title=====>:
Rasdaman
<=====text=====>:
{{Infobox software
| name = rasdaman
| logo = [[Image:Rasdaman logo.png|frame|center|x250px|alt=rasdaman logo (used with permission of copyright holder)|rasdaman logo (used with permission of copyright holder)]]
| developer = rasdaman GmbH
| latest_release_version = rasdaman v9.2.1
| latest_release_date = {{release date |2016|02|17}}
| status = Active
| operating_system = most [[Unix-like]] operating systems
| programming language = [[C++]]<ref>{{cite web |url=https://www.openhub.net/p/rasdaman |title=The rasdaman Open Source Project on Open Hub |work=Open Hub |publisher=Black Duck Software |accessdate=2016-08-01}}</ref>
| genre = [[Array DBMS]]
| license = [[GNU General Public License|GPL v3]]/[[GNU Lesser General Public License|LGPL v3]] or [[Proprietary software|proprietary]]<ref>{{cite web|url=http://rasdaman.org/wiki/License |title=Rasdaman License |publisher=rasdaman.org |date= |accessdate=2016-08-01}}</ref>
| website = {{URL|http://rasdaman.org}}, {{URL|http://rasdaman.com}}
}}

'''Rasdaman''' ("raster data manager") is an [[Array DBMS]], that is: a [[Database Management System]] which adds capabilities for storage and retrieval of massive multi-dimensional [[array data structure|arrays]], such as sensor, image, and statistics data. A frequently used synonym to arrays is raster data, such as in 2-D [[raster graphics]]; this actually has motivated the name ''rasdaman''. However, rasdaman has no limitation in the number of dimensions - it can serve, for example, 1-D measurement data, 2-D satellite imagery, 3-D x/y/t image time series and x/y/z exploration data, 4-D ocean and climate data, and even beyond spatio-temporal dimensions.

== History ==

In 1989, [[Peter Baumann (computer scientist)|Peter Baumann]] started a research on database support for images, then at [[Fraunhofer Society|Fraunhofer Computer Graphics Institute]]. Following an in-depth investigation on raster data formalizations in imaging, in particular the AFATL Image Algebra, he established a database model for multi-dimensional arrays, including a data model and declarative query language.<ref>Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/journals/vldb/vldb3.html#Baumann94 On the Management of Multidimensional Discrete Data]. VLDB Journal 4(3)1994, Special Issue on Spatial Database Systems, pp. 401 - 444</ref>

At [[Technical University Munich|TU Munich]], in the EU funded basic research project ''RasDaMan'', a first prototype was established, on top of the O2 [[Object-oriented database|object-oriented DBMS]], and tested in Earth and Life science applications.<ref name="cordis.europa.eu/">http://cordis.europa.eu/result/rcn/20754_en.html</ref> Over further EU funded projects, this system was completed and extended to support relational DBMSs.
A dedicated research spin-off, rasdaman GmbH,<ref name="Rasdaman.com">http://www.rasdaman.com</ref> was established to give commercial support in addition to the research which subsequently has been continued at [[Jacobs University Bremen|Jacobs University]].<ref name="Rasdaman.com/Archive">http://www.rasdaman.com/News/archive.php</ref> Since then, both entities collaborate on the further development and use of the rasdaman technology.

== Concepts ==

=== Data model ===

Based on an array algebra<ref>Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/ngits/ngits99.html#Baumann99 A Database Array Algebra for Spatio-Temporal Data and Beyond]. Proc. NGITS’99, LNCS 1649, Springer 1999, pp.76-93</ref> specifically developed for database purposes, rasdaman adds a new attribute type, array, to the relational model. As this array definition is parametrized it constitutes a [[Second-order logic|second-order]] construct or [[Template (C++)|template]]); this fact is reflected by the second-order functionals in the algebra and query language.

For historical reasons, [[Table (database)|tables]] are called ''collections'', as initial design emphasized an embedding into the object-oriented database standard, [[ODMG]]. Anticipating a full integration with SQL, rasdaman collections represent a binary relation with the first attribute being an [[object identifier]] and the second being the array. This allows to establish [[Foreign key|foreign key references]] between arrays and regular [[Tuple|relational tuples]].

=== Raster Query Language ===

The rasdaman query language, rasql, embeds itself into standard SQL and its set-oriented processing.
On the new attribute type, multi-dimensional arrays, a set of extra operations is provided which all are based on a minimal set of algebraically defined core operators, an ''array constructor'' (which establishes a new array and fills it with values) and an ''array condenser'' (which, similarly to SQL aggregates, derives scalar summary information from an array). The query language is declarative (and, hence, optimizable) and safe in evaluation - that is: every query is guaranteed to return after a finite number of processing steps.

The rasql query guide<ref>n.n.: [http://rasdaman.org/browser/manuals_and_examples/manuals/doc-guides/ql-guide.pdf Rasdaman Query Language Guide]</ref> provides details, here some examples may illustrate its use:

* "From all 4-D x/y/z/t climate simulation data cubes, a cutout which contains all in x, a y extract between 100 and 200, all available along z, and a slice at position 42 (effectively resulting in a 3-D x/y/z cube)":
<source lang="sql">
select c[ *:*, 100:200, *:*, 42 ] 
from   ClimateSimulations as c 
</source>

* "In all Landsat satellite images, suppress all non-green areas":
<source lang="sql">
select img * (img.green > 130)
from   LandsatArchive as img
</source>

Note: this is a ''very'' naive phrasing of vegetation search; in practice one would use the [[NDVI]] formula, use null values for cloud masking, and several more techniques.

* "All MRI images where, in some region defined by the bit masks, intensity exceeds a threshold of 250":
<source lang="sql">
select img
from   MRI as img, Masks as m
where  some_cells( img > 250 and m )
</source>

* "A 2-D x/y slice from all 4-D climate simulation data cubes, each one encoded in PNG format": 
<source lang="sql">
select png( c[ *:*, *:*, 100, 42 ] )
from   ClimateSimulations as c 
</source>

== Architecture ==

=== Storage management ===

[[Image:Sample tiling of an array for storage in rasdaman.png|frame|x110px|alt=Sample rasdaman tiling|Sample array tiling in rasdaman]]

Raster objects are maintained in a standard relational database, based on the partitioning of an raster object into ''tiles''.<ref>
Furtado, P., Baumann, P.: [http://www.informatik.uni-trier.de/~ley/db/conf/icde/icde99.html#FurtadoB99 Storage of Multidimensional Arrays based on Arbitrary Tiling]. Proc. ICDE'99, March 23–26, 1999, Sydney, Australia, pp. 328-336</ref> Aside from a regular subdivision, any user or system generated partitioning is possible. As tiles form the unit of disk access, it is of critical importance that the tiling pattern is adjusted to the query access patterns; several tiling strategies assist in establishing a well-performing tiling. A geo index is employed to quickly determine the tiles affected by a query. Optionally, tiles are compressed using one of various choices, including lossless and lossy (wavelet) algorithms; independently from that, query results can be comressed for transfer to the client. Both tiling strategy and compression comprise database tuning parameters.

Tiles and tile index are stored as [[Binary large object|BLOBs]] in a relational database which also holds the data dictionary needed by rasdaman’s dynamic type system. Adaptors are available for several relational systems, among them open-source [[Postgresql|PostgreSQL]].
For arrays larger than disk space, hierarchical storage management (HSM) support has been developed.

=== Query processing ===

Queries are parsed, optimised, and executed in the rasdaman server. The parser receives the query string and generates the operation tree. Further, it applies algebraic optimisation rules to the query tree where applicable; of the 150 algebraic rewriting rules, 110 are actually optimising while the other 40 serve to transform the query into canonical form. Parsing and optimization together take less than a millisecond on a laptop.

Execution follows a ''tile streaming'' paradigm: whenever possible, array tiles addressed by a query are fetched sequentially, and each tile is discarded after processing. This leads to an architecture scalable to data volumes exceeding server main memory by orders of magnitude.
   
Query execution is parallelised. First, rasdaman offers inter-query parallelism: A dispatcher schedules requests into a pool of server processes on a per-transaction basis. Intra-query parallelism transparently distributes query subtrees across available cores, GPUs, or cloud nodes.

=== Client APIs ===

The primary interface to rasdaman is the query language. Embeddings into C++ and Java APIs allow invocation of queries, as well as client-side convenience functions for array handling. Arrays per se are delivered in the main memory format of the client language and processor architecture, ready for further processing. Data format codecs allow to retrieve arrays in common raster formats, such as [[Comma-separated values|CSV]], [[Portable Network Graphics|PNG]], and [[Netcdf|NetCDF]].

A Web design toolkit, raswct, is provided which allows to establish Web query frontends easily, including graphical widgets for parametrized query handling, such as sliders for thresholds in queries.

=== Geo Web Services ===

A [[Java (programming language)|Java]] servlet, ''petascope'', running as a rasdaman client offers Web service interfaces specifically for geo data access, processing and filtering. 
The following [[Open Geospatial Consortium|OGC]] standards are supported: [[Web Map Service|WMS]], [[Web Coverage Service|WCS]], [[Web Coverage Processing Service|WCPS]], and [[Web Processing Service|WPS]].

For [[Web Coverage Service|WCS]] and [[Web Coverage Processing Service|WCPS]], rasdaman is the [[reference implementation]].

== Status and license model ==

Today, rasdaman is a fully-fledged implementation offering select / insert / update / delete array query functionality. It is being used in both research and commercial installations.

In a collaboration of the original code owner, rasdaman GmbH<ref name="Rasdaman.com"/> and [[Jacobs University]], a code split was performed in 2008 - 2009 resulting in ''rasdaman community'',<ref>http://www.rasdaman.org</ref> an [[open source]] branch, and ''rasdaman enterprise'', the commercial branch. Since then, ''rasdaman community'' is being maintained by Jacobs University whereas ''rasdaman enterprise'' remains proprietary to rasdaman GmbH.
The difference between both variants mainly consists of performance boosters (such as specific optimization techniques) intended to support particularly large databases, user numbers, and complex queries; Details are available on the ''rasdaman community'' website.<ref>[http://rasdaman.eecs.jacobs-university.de/trac/rasdaman/wiki/License rasdaman license model]</ref>

The ''rasdaman community'' license releases the server in [[GPL]] and all client parts in [[LGPL]], thereby allowing to use the system in any kind of license environment.

== Impact and Use ==

Being the first Array DBMS shipped (first prototype available in 1996), rasdaman has shaped this recent database research domain. Concepts of the data and query model (declarativeness, sometimes choice of operators) find themselves in more recent approaches.

In 2008, the [[Open Geospatial Consortium]] released the [[Web Coverage Processing Service]] standard which defines a raster query language based on the concept of a [[Coverage data|coverage]]. Operator semantics<ref>Baumann, P.: [http://www.springerlink.com/openurl.asp?genre=article&id=doi:10.1007/s10707-009-0087-2 The OGC Web Coverage Processing Service (WCPS) Standard]. Geoinformatica, 14(4)2010, pp. 447-479</ref> is influenced by the rasdaman array algebra.

EarthLook<ref>http://standards.rasdaman.org/</ref> is a showcase for [[Open Geospatial Consortium|OGC]] [[Coverage data|coverage]] standards in action, offering 1-D through 4-D use cases of raster data access and ad-hoc processing. EarthLook is built on rasdaman.

A sample large project in which rasdaman is being used for large-scale services in all [[Earth sciences]] is EarthServer:<ref>http://www.earthserver.eu</ref> six services with a volume of at least 100 Terabyte each are being set up for integrated data / metadata retrieval and distributed query processing.

==References==
{{Reflist}}

{{DEFAULTSORT:rasdaman}}
[[Category:Free database management systems]]
[[Category:Proprietary database management systems]]
[[Category:NoSQL]]
[[Category:Data management]]
[[Category:Query languages]]
<=====doc_Id=====>:367
<=====title=====>:
Critical data studies
<=====text=====>:
{{notability|date=December 2016}}
{{Orphan|date=December 2016}}

'''Critical data studies''' is the systematic study of data and its criticisms.<ref>Dalton, Craig, and Jim Thatcher, 2014.</ref> The field was named by scholars [[Craig Dalton]] and [[Jim Thatcher]] in their 2015 article titled "What does a critical data studies look like, and why do we care?" Interest has developed in this domain as a response to the emergence and reliance on '[[big data]]' in contemporary society.<ref>''Ibid.''</ref> Some of the other key scholars in this discipline include [[Rob Kitchen]] and [[Tracey P. Lauriault]].<ref>Kitchin, Rob, and Tracey P. Lauriault, 2014</ref><ref>Kitchin, Rob, 2014</ref> Scholars have attempted to make sense of data through different theoretical frameworks, some of which include analyzing data technically, ethically, politically/economically, temporally/spatially, and philosophically.<ref>Kitchin, Rob, 2014</ref> Some of the key academic journals related to critical data studies include the ''[[Journal of Big Data]]'' and ''[[Big Data and Society]]''.

==Why is a critical approach to data needed?==

In their article in which they coin the term 'critical data studies,' Dalton and Thatcher also provide several justifications as to why data studies is a discipline worthy of a critical approach.<ref>Dalton, Craig, and Jim Thatcher, 2014.</ref> Firstly, 'big data' is an important aspect of twenty-first century society, and the analysis of 'big data' allows for a deeper understanding of what is happening and for what reasons.<ref>Dalton, Craig, and Jim Thatcher, 2014.</ref> Furthermore, big data as a technological tool and the information that it yields are not neutral, according to Dalton and Thatcher,<ref>''Ibid.''</ref> making it worthy of critical analysis in order to identify and address its biases. Building off this idea, another justification for a critical approach is that the relationship between big data and society is an important one, and therefore worthy of study.<ref>''Ibid.''</ref>  Dalton and Thatcher stress how the relationship is not an example of [[technological determinism]], but rather how big data can shape the lives of individuals. Big data technology can cause significant changes in society's structure and in the everyday lives of people,<ref>''Ibid.''</ref> and being a product of society, big data technology is worthy of sociological investigation.<ref>''Ibid.''</ref> Moreover, data sets are almost never completely raw, that is to say without any influences. Dalton and Thatcher describe how data are shaped by the vision or goals of a research team, and during the data collection process, certain things are quantified, stored, sorted and even discarded by the research team.<ref>''Ibid.''</ref> A critical approach is thus necessary in order to understand and reveal the intent behind the information being presented. Dalton and Thatcher also argue how data alone cannot speak for itself; in order to possess any concrete meaning, data must be accompanied by theoretical insight or be accompanied by alternative quantitative or qualitative research measures.<ref>''Ibid.''</ref> Dalton and Thatcher argue that if one were to only think of data in terms of its exploitative power, there is no possibility of using data for revolutionary, liberatory purposes.<ref>''Ibid.''</ref> Finally, Dalton and Thatcher propose that a critical approach in studying data allows for 'big data' to be combined with older, 'small data,' and thus create more thorough research, opening up more opportunities, questions and topics to be explored.<ref>''Ibid.''</ref>

==Issues and Concerns for Critical Data Scholars==

The use of data in modern society brings about new ways of understanding and measuring the world, but also brings with it certain concerns or issues.<ref>Kitchin, Rob, 2014</ref> Data scholars attempt to bring some of these issues to light in their quest to be critical of data. Rob Kitchin identifies both technical and organizational issues of data, as well as some normative and ethical questions.<ref>''Ibid.''</ref> Technical and organization issues concerning data range from the scope of datasets, access to the data, the quality of the data, the integration of the data, the application of analytics and ecological fallacies, as well as the skills and organizational capabilities of the research team.<ref>''Ibid.''</ref> Some of the normative and ethical concerns addressed by Kitchin include surveillance through one's data (dataveillance), the privacy of one's data, the ownership of one's data, the security of one's data, anticipatory or corporate governance, and finally profiling individuals by their data.<ref>''Ibid.''</ref> All of these concerns must be taken into account by scholars of data in their objective to be critical.

==References==
{{reflist|24em}}

==Sources==
* Dalton, Craig, and Jim Thatcher. "What does a critical data studies look like, and why do we care? Seven points for a critical approach to ‘big data’." ''Society and Space open site'' (2014). Retrieved October 23, 2016.
* Elkins, James R. "The Critical Thinking Movement: Alternating Currents in One Teacher's Thinking". ''myweb.wvnet.edu''(1999). Retrieved 29 November 2016.
* Kitchin, Rob. ''The data revolution: Big data, open data, data infrastructures and their consequences.'' Sage, 2014. Retrieved October 23, 2016.
* Kitchin, Rob, and Tracey P. Lauriault. "Towards critical data studies: Charting and unpacking data assemblages and their work." (2014). Retrieved October 23, 2016.

[[Category:Data management]]
<=====doc_Id=====>:370
<=====title=====>:
Data storage device
<=====text=====>:
[[File:PersonalStorageDevices.agr.jpg|thumb|Many different consumer electronic devices can store data.]]
[[File:EdisonPhonograph.jpg|thumb|Edison cylinder phonograph ca. 1899. The phonograph cylinder is a storage medium. The phonograph may be considered a storage device.]]
[[File:Reel-to-reel recorder tc-630.jpg|thumb|On a reel-to-reel tape recorder (Sony TC-630), the recorder is data storage equipment and the magnetic tape is a data storage medium.]]
[[File:RNA-comparedto-DNA thymineAndUracilCorrected.png|thumb|upright|[[RNA]] might be the oldest [[data]] storage medium.<ref>{{cite journal|title=The RNA World|journal=[[Nature (journal)|Nature]]|first=Walter|last=Gilbert|authorlink=Walter Gilbert|date=Feb 1986|pages=618|volume=319|doi=10.1038/319618a0|issue=6055|bibcode=1986Natur.319..618G}}</ref>]]

A '''data storage device''' is a device for [[recording]] (storing) [[information]] (data). Recording can be done using virtually any form of [[energy]], spanning from manual muscle power in [[handwriting]], to acoustic vibrations in [[phonograph]]ic recording, to electromagnetic energy modulating [[magnetic tape]] and [[optical disc]]s.

A storage device may hold information, process information, or both. A device that only holds information is a recording [[Medium (communication)|medium]]. Devices that process information (data storage equipment) may either access a separate portable (removable) recording medium or a permanent component to store and retrieve data.

Electronic data storage requires electrical power to store and retrieve that data. Most storage devices that do not require [[Visual perception|vision]] and a brain to read data fall into this category.  Electromagnetic data may be stored in either an  analog [[data]] or [[digital data]] format on a variety of media. This type of data is considered to be [[Machine-readable medium|electronically encoded]] data, whether it is electronically stored in a  [[semiconductor]] [[Computer data storage|device]], for it is certain that a semiconductor device was used to record it on its medium. Most electronically processed data storage media (including some forms of [[computer data storage]]) are considered permanent (non-volatile) storage, that is, the data will remain stored when power is removed from the device. In contrast, most electronically stored information within most types of semiconductor (computer chips) [[microcircuit]]s are [[volatile memory]], for it vanishes if power is removed.

Except for [[barcode]]s, [[optical character recognition]] (OCR), and [[magnetic ink character recognition]] (MICR) data, electronic data storage is easier to revise and may be more cost effective than alternative methods due to smaller physical space requirements and the ease of replacing (rewriting) data on the same medium.<ref>{{Cite web|url=https://www.seas.gwu.edu/~shmuel/WORK/Differences/Chapter%203%20-%20Sources.pdf|title=The Difference between Electronic and Paper Documents|last=Rotenstreich|first=Shmuel|website=Seas.GWU.edu|publisher=The George Washington University|access-date=12 April 2016}}</ref>

==Global capacity, digitization, and trends==
In a recent study in [[Science (journal)|''Science'']] it was estimated that the world's technological capacity to store information in analog and digital devices grew from less than three (optimally compressed) [[exabyte]]s in 1986, to 295 (optimally compressed) [[exabyte]]s in 2007,<ref name="HilbertLopez2011">{{cite journal | last1 = Hilbert | first1 = Martin | last2 = López | first2 = Priscila | year = 2011 | title = The World's Technological Capacity to Store, Communicate, and Compute Information | journal = [[Science (journal)|Science]] | volume = 332 | issue = 6025| pages = 60–65 | doi=10.1126/science.1200970 | pmid=21310967}}; free access to the article through here: martinhilbert.net/WorldInfoCapacity.html</ref> and doubles roughly every three years.<ref name="Hilbertvideo2011">[http://ideas.economist.com/video/giant-sifting-sound-0 "video animation on The World’s Technological Capacity to Store, Communicate, and Compute Information from 1986 to 2010]</ref>

It is estimated that the year 2002 marked the beginning of the digital age for information storage, the year that marked the date when human kind started to store more information digitally than on analog storage devices.<ref name="HilbertLopez2011" />

==See also==
{{colbegin||22em}}
* [[Archival science]]
* [[Blank media tax]]
* [[Computer data storage]]
* [[Content format]]
* [[Data transmission]]
* [[Digital Data Storage|Digital Data Storage (DDS)]]
* [[Digital preservation]]
* [[Disk drive performance characteristics]]
* [[Format war]]
* [[Flip-flop (electronics)]]
* [[IOPS]]
* [[Library]]
* [[Media controls]]
* [[Medium format (film)]]
* [[Memristor]]
* [[Nanodot]]
* [[Nonlinear medium]] ([[random access]])
* [[Recording format]]
* [[Semiconductor memory]]
* [[Telecommunication]]
{{colend}}

==References==
{{Reflist|30em}}

==Further reading==
* {{cite journal |last = Bennett |first=John C. | title = 'JISC/NPO Studies on the Preservation of Electronic Materials: A Framework of Data Types and Formats, and Issues Affecting the Long Term Preservation of Digital Material | publisher = British Library Research and Innovation Report 50 | year = 1997 | url = http://www.ukoln.ac.uk/services/papers/bl/jisc-npo50/bennet.html }}
* [http://www.zetta.net/history-of-computer-storage/ History of Computer Storage from 1928 to 2013]
* [http://www.remosoftware.com/info/history-of-storage-from-cave-paintings-to-electrons/ History of Storage from Cave Paintings to Electrons]
* [[Plant-based digital data storage]]

==External links==
* [http://ns1758.ca/winch/winchest.html Historical Notes about the Cost of Hard Drive Storage Sp]
* @[http://ns1758.ca/winch/winchest.html ace]
* [https://www.securedatarecovery.com/infographics/the-evolution-of-data-storage The Evolution of Data Storage]

{{Magnetic storage media}}
{{Optical storage media}}
{{Paper data storage media}}
{{Primary storage technologies}}

{{Authority control}}

[[Category:Computer storage devices]]
[[Category:Data management]]
[[Category:Film and video technology]]
[[Category:Media technology]]
[[Category:Recording]]
[[Category:Sound production technology]]
[[Category:Storage media]]
<=====doc_Id=====>:373
<=====title=====>:
Cut, copy, and paste
<=====text=====>:
{{Redirect|Copy & Paste|the album|Hurricane Venus}}
{{other uses|Cut and paste (disambiguation)}}
{{Refimprove|date=August 2008}}
In [[human–computer interaction]], '''cut''', '''copy''' and '''paste''' are related [[Command (computing)|commands]] that offer a [[user interface|user-interface]] [[interprocess communication]] technique for transferring [[data (computing)|data]]. The '''cut''' command removes the [[Selection (user interface)|selected data]] from its original position, while the '''copy''' command creates a duplicate; in both cases the selected data is kept in a temporary storage tool called the [[Clipboard (software)|clipboard]]. The data in the clipboard is later inserted in the position where the '''paste''' command is issued.

The command names are an [[interface metaphor]] based on the physical procedure used in [[manuscript]] editing to create a [[page layout]].

This [[interaction technique]] has close associations with related techniques in [[graphical user interface]]s that use [[pointing device]]s such as a [[computer mouse]] (by [[drag and drop]], for example).

The capability to replicate information with ease, changing it between contexts and applications, involves [[privacy]] concerns because of the risks of disclosure when handling [[Information sensitivity|sensitive information]]. Terms like ''cloning'', ''copy forward'', ''carry forward'', or ''re-use'' refer to the dissemination of such information through documents, and may be subject to regulation by [[administrative body|administrative bodies]].<ref name="Laubach">{{cite web|url=http://hcca-info.org/portals/0/pdfs/resources/conference_handouts/regional_conference/2012/seattle/laubachwakefieldprint2.pdf|title=Cloning and Other Compliance Risks in Electronic Medical Records|last1=Laubach|first1=Lori|last2=Wakefield|first2=Catherine|date=June 8, 2012|publisher=[[Moss Adams LLP]], [[MultiCare]]|accessdate=April 23, 2014}}</ref>

==History==

===Origins===
The term "''cut and paste''" comes from the traditional practice in manuscript-editings whereby people would cut paragraphs from a page with [[scissors]] and [[Adhesive|paste]] them onto another page. This practice remained standard into the 1980s. Stationery stores formerly sold "editing scissors" with blades long enough to cut an 8½"-wide page. The advent of [[photocopier]]s made the practice easier and more flexible.

The act of copying/transferring text from one part of a computer-based document ("[[Data buffer|buffer]]") to a different location within the same or different computer-based document was a part of the earliest on-line computer editors. As soon as computer data entry moved from punch-cards to online files (in the mid/late 1960s) there were "commands" for accomplishing this operation. This mechanism was often used to transfer frequently-used commands or text snippets from additional buffers into the document, as was the case with the [[QED (text editor)|QED]] editor.<ref name="communications1967">{{citation|doi=10.1145/363848.363863|last1=Deutsch|first1=L. Peter|authorlink1=L. Peter Deutsch|last2=Lampson|first2=Butler W.|authorlink2=Butler Lampson|title=An online editor|journal=Communications of the ACM |volume=10|issue=12|year=1967|pages=793–799, 803|url=http://research.microsoft.com/en-us/um/people/blampson/04-OnlineEditor/04-OnlineEditor.htm<!-- http://portal.acm.org/citation.cfm?id=363848.363863&coll=ACM&dl=ACM&CFID=15669714&CFTOKEN=68334085 -->}}, p. 793.</ref>

===Early methods===
The earliest editors, since they were designed for [[teleprinter]] terminals, provided [[computer keyboard|keyboard]] commands to delineate contiguous regions of text, remove such regions, or move them to some other location in the file.  Since moving a region of text required first removing it from its initial location and then inserting it into its new location various schemes had to be invented to allow for this multi-step process to be specified by the user.

Often this was done by the provision of a 'move' command, but some text editors required that the text be first put into some temporary location for later retrieval/placement. In 1983, the [[Apple Lisa]] became the first text editing system to call that temporary location "the clipboard".

Earlier control schemes such as [[NLS (computer system)|NLS]] used a [[Linguistic typology#Subject.E2.80.93verb.E2.80.93object positioning|verb-object command structure]], where the command name was provided first and the object to be copied or moved was second. The inversion from [[Subject–verb–object|verb-object]] to [[Subject–object–verb|object-verb]] on which copy and paste are based, where the user selects the object to be operated before initiating the operation, was an innovation crucial for the success of the desktop metaphor as it allowed copy and move operations based on [[direct manipulation]].<ref>{{cite paper|title=Metaphors create theories for users|author=Kuhn, Werner|journal=Spatial Information Theory A Theoretical Basis for GIS|pages=366–376|year=1993|publisher=Springer}}</ref>

===Popularization===
Inspired by early line and character editors that broke a move or copy operation into two steps—between which the user could invoke a preparatory action such as navigation—[[Lawrence G. Tesler]] (Larry Tesler) proposed the names "cut" and "copy" for the first step and "paste" for the second step. Beginning in 1974, he and colleagues at [[Xerox PARC|Xerox Corporation Palo Alto Research Center (PARC)]] implemented several text editors that used cut/copy-and-paste commands to move/copy text.<ref>{{cite web|url=http://www.designinginteractions.com/ |title=Bill Moggridge, Designing Interactions, MIT Press 2007, pp. 63–68 |publisher=Designinginteractions.com |date= |accessdate=2011-11-25}}</ref>

[[Apple Computer]] widely popularized the computer-based cut/copy-and-paste paradigm through the [[Apple Lisa|Lisa]] (1983) and [[Apple Macintosh|Macintosh]] (1984) operating systems and applications. Apple mapped the functionalities to key combinations consisting of the [[Command key]] (a special [[modifier key]]) held down while typing the letters X (for cut), C (for copy), and V (for paste), choosing a handful of [[keyboard shortcuts]] to control basic editing operations. The keys involved all cluster together at the left end of the bottom row of the standard [[QWERTY]] keyboard, and each key is combined with a special [[modifier key]] to perform the desired operation:
* [[control-Z|Z]] to [[undo]]
* [[control-X|X]] to cut
* [[control-C|C]] to copy
* [[control-V|V]] to paste
The [[IBM Common User Access]] (CUA) standard also uses combinations of the [[Insert key|Insert]], [[Del key|Del]], [[Shift key|Shift]] and [[Control key]]s.  Early versions of [[Microsoft Windows|Windows]]{{Dubious|date=March 2014}} used the IBM standard. [[Microsoft]] later also adopted the Apple key combinations with the introduction of [[Microsoft Windows|Windows]]{{Dubious|date=January 2016}}, using the [[control key]] as [[modifier key]]. For users migrating to Windows from [[MS-DOS]] this was a big change as MS-DOS users used the "copy" and "move" commands.

Similar patterns of key combinations, later borrowed by others, remain widely available {{As of|2007|alt= today}} in most GUI text editors, word processors, and file system browsers.

== Cut and paste ==
Computer-based editing can involve very frequent use of cut-and-paste operations. Most software-suppliers provide several methods for performing such tasks, and this can involve (for example)  key combinations, pulldown menus, pop-up menus, or [[toolbar]] buttons.
# The user selects or "highlights" the text or file for moving by some method, typically by [[dragging]] over the text or file name with the pointing-device or holding down the [[Shift key]] while using the [[arrow keys]] to move the [[Cursor (computers)|text cursor]].
# The user performs a "cut" operation via key combination [[Control key|Ctrl]]+x ([[Command key|⌘]]+x for [[Macintosh]] users), menu, or other means.
# Visibly, "cut" text immediately disappears from its location.  "Cut" files typically change color to indicate that they will be moved.
# Conceptually, the text has now moved to a location often called the [[Clipboard (software)|clipboard]]. The clipboard typically remains invisible. On most systems only one clipboard location exists, hence another cut or copy operation overwrites the previously stored information. Many [[Unix|UNIX]] text-editors provide multiple clipboard entries, as do some Macintosh programs such as Clipboard Master,<ref>{{cite web |title=Clipboard Master |work=Clipboard Master 2.0 by In Phase Consulting, July 1994|url=http://forums.info-mac.org/viewtopic.php?f=243&t=14244&sid=739ce1119f88340c52dc2aed3c788fff |accessdate=14 September 2009}}</ref> and Windows [[clipboard manager|clipboard-manager]] programs such as the one in [[Microsoft Office]].
# The user selects a location for insertion by some method, typically by clicking at the desired insertion point.
# A ''paste'' operation takes place which visibly inserts the clipboard text at the insertion point. (The paste operation does not typically destroy the clipboard text: it remains available in the clipboard and the user can insert additional copies at other points).
Whereas cut-and-paste often takes place with a mouse-equivalent in Windows-like GUI environments, it may also occur entirely from the keyboard, especially in [[Unix|UNIX]] [[text editor]]s, such as [[Pico (text editor)|Pico]] or [[vi]]. Cutting and pasting without a mouse can involve a selection (for which Ctrl+x is pressed in most graphical systems) or the entire current line, but it may also involve text after the [[cursor (computers)|cursor]] until the end of the line and other more sophisticated operations.

When a software environment provides ''cut'' and ''paste'' functionality, a nondestructive operation called ''copy''  usually accompanies them; ''copy'' places a copy of the selected text in the clipboard without removing it from its original location.

The clipboard usually stays invisible, because the operations of cutting and pasting, while actually independent, usually take place in quick succession, and the user (usually) needs no assistance in understanding the operation or maintaining mental context. Some application programs provide a means of viewing, or sometimes even editing, the data on the clipboard.

== Copy and paste ==
The term "copy-and-paste" refers to the popular, simple method of reproducing [[Character (computing)|text]] or other [[data]] from a source to a destination. It differs from '''cut and paste''' in that the original source text or data does not get deleted or removed. The popularity of this method stems from its simplicity and the ease with which users can move data between various applications visually – without resorting to [[Disk storage|permanent storage]].

Once one has copied data into the [[clipboard]], one may '''paste''' the contents of the clipboard into a destination document.

The [[X Window System]] maintains an additional clipboard containing the most recently selected text; middle-clicking pastes the content of this "selection" clipboard into whatever the [[pointer (computing WIMP)|pointer]] is on at that time.

Most [[terminal emulator]]s and some other applications support the key combinations Ctrl-Insert to copy and Shift-Insert to paste. This is in accordance with the [[IBM Common User Access]] (CUA) standard.

== Find and go ==
The [[NeXTStep]] operating system extended the concept of having a single copy buffer by adding a second system-wide ''' Find buffer''' used for searching. The Find buffer is also available in [[OSX|Mac OS X]].

Text can be placed in the Find buffer by either using the Find panel or by selecting text and hitting {{key press|⌘E}}.

The text can then be searched with '''Find Next''' {{key press|⌘G}} and '''Find Previous''' {{key press|⌘D}}.

The functionality comes in handy when for example editing [[source code]]. To find the occurrence of a variable or function name elsewhere in the file, simply select the name by double clicking, hit {{key press|⌘E}} and then jump to the next or previous occurrence with {{key press|⌘G}} / {{key press|⌘D}}.

Note that this does ''not'' destroy your copy buffer as with other [[User interface|UIs]] like [[Windows]] or the [[X Window System]].

Together with copy and paste this can be used for quick and easy replacement of repeated text:
* select the text that you want to replace (i.e. by double clicking)
* put the text in the Find buffer with {{key press|⌘E}}
* overwrite the selected text with your replacement text
* select the replacement text (try {{key press| ⎇⇧←}} to avoid lifting your hands from the keyboard)
* copy the replacement text {{key press|⌘C}}
* find the next or previous occurrence {{key press|⌘G}} / {{key press|⌘D}}
* paste the replacement text {{key press|⌘V}}
* repeat the last two steps as often as needed
or in short:
* select {{key press|⌘ E}}  {{key press|replstr}}  {{key press| ⎇⇧←}}  {{key press|⌘C}}  {{key press|⌘G}}{{key press|⌘V}} {{key press|⌘G}}{{key press|⌘V}} ...
While this might sound a bit complicated at first, it is often ''much'' faster than using the find panel, especial when only a few occurrences shall be replaced or when only some of the occurrences shall be replaced. When a text shall not be replaced, simply hit {{key press|⌘G}} again to skip to the next occurrence.

The find buffer is system wide. That is, if you enter a text in the find panel (or with {{key press|⌘E}}) in one application and then switch to another application you can immediately start searching without having to enter the search text again.

== Common keyboard shortcuts ==
{| class="wikitable"
|-
! &nbsp;
! Cut
! Copy
! Paste
|-
! Apple
| Command+X
| Command-C
| Command-V
|-
! Windows/GNOME/KDE
| Control-X / Shift-Delete
| Control-C / Control-Insert
| Control-V / Shift-Insert
|-
! GNOME/KDE terminal emulators
| <!-- cut -->
| Shift-Control-C / Control-Insert
| Shift-Control-V / Shift-Control-Insert (Shift-Insert for pasting selected text)
|-
! BeOS
| Alt-X
| Alt-C
| Alt-V
|-
! Common User Access
| Shift+Delete
| Control+Insert
| Shift+Insert
|-
! Emacs
| Control-W (to mark)<br />Control-K (to end of line)
| [[Meta key|meta]]-W (to mark)
| Control-Y
|-
! vi
| d (delete)
| y (yank)
| p (put)
|-
! X Window System
| <!-- cut -->
| click-and-drag to highlight
| middle mouse button
|}

== Copy and paste automation ==
Copying data one by one from one application to another, such as from [[Microsoft Excel|Excel]] to a [[Form (HTML)|web form]], might involve a lot of manual work. Copy and paste can be automated with the help of a [[Computer program|program]] that would iterate through the values list and paste them to the active [[Window (computing)|application window]]. Such programs might come in the form of [[Macro (computer science)|macros]] or dedicated programs which involve more or less scripting. Alternatively, applications supporting [[simultaneous editing]] may be used to copy or move collections of items.

== Additional differences between moving and copying ==<!-- This section is linked from [[Spreadsheet]] -->
In a spreadsheet, moving (cut and paste) need not equate to copying (copy and paste) and then deleting the original: when moving, references to the moved cells may move accordingly.

[[Windows Explorer]] also differentiates moving from merely copy-and-delete: a "cut" file will not actually disappear until pasted elsewhere and cannot be pasted more than once. The icon fades to show the transient "cut" state until it is pasted somewhere. Cutting a second file while the first one is cut will release the first from the "cut" state and leave it unchanged. Shift+Delete cannot be used to cut files; instead it deletes them without using the Recycle bin.

== Multiple clipboards ==
Several editors allow copying text into or pasting text from specific clipboards, typically using a special keystroke-sequence to specify a particular clipboard-number.

[[Clipboard manager]]s can be very convenient productivity-enhancers by providing many more features than system-native clipboards. Thousands of clips from the clip history are available for future pasting, and can be searched, edited, or deleted. Favorite clips that a user frequently pastes (for example, the current date, or the various fields of a user's contact info) can be kept standing ready to be pasted with a few clicks or keystrokes.

Similarly, a '''kill ring''' provides a [[LIFO (computing)|LIFO]] [[stack (data structure)|stack]] used for cut-and-paste operations as a type of clipboard capable of storing multiple pieces of data.<ref>{{cite web|url=http://www.ai.sri.com/~gkb/general.html#kill-ring |title=GKB (Generic Knowledge Base) Editor user's manual |work=[[Artificial Intelligence Center]] |publisher=[[SRI International]] |accessdate=2011-11-25}}</ref>
For example, the [[GNU Emacs]] text editor provides a kill ring.<ref>{{cite web|url=https://www.gnu.org/software/emacs/manual/html_mono/emacs.html#Kill-Ring |title=GNU Emacs manual |publisher=Gnu.org |date= |accessdate=2011-11-25}}</ref>
Each time a user performs a cut or copy operation, the system adds the affected text to the ring. The user can then access the contents of a specific (relatively numbered) buffer in the ring when performing a subsequent paste-operation. One can also give kill-buffers individual names, thus providing another form of multiple-clipboard functionality.

==Use in healthcare==
Concerns have been raised over the use of copy and paste functions in healthcare documentation and [[electronic health records]]. There is potential for the introduction of [[medical error|errors]], [[information overload]], and [[fraud]].<ref name="Laubach" /><ref>{{cite web|url=http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_050621.pdf|title=Appropriate Use of the Copy and Paste Functionality in Electronic Health Records|date=March 17, 2014|publisher=[[American Health Information Management Association]]|accessdate=April 23, 2014}}</ref>

==Use in software development==
[[Copy and paste programming]] is an [[antipattern]] arising from the blind pasting of pre-existing code into another [[source code]] file.

== See also ==
* [[Clipboard (software)|Clipboard]]
* [[Control key]]
* [[Cut and paste job]]
* [[Drag and drop]]
* [[Photomontage]]
* [[Publishing Interchange Language]]
* [[Simultaneous editing]]
* [[X Window selection]]

== References ==
{{Reflist}}

== External links ==
* [http://tronche.com/gui/x/icccm/sec-2.html 2. Peer-to-Peer Communication by Means of Selections] in the [[ICCCM]]

[[Category:User interface techniques]]
[[Category:Data management]]
[[Category:Clipboard (computing)]]
<=====doc_Id=====>:376
<=====title=====>:
Rubrik
<=====text=====>:
'''Rubrik''' is a privately held cloud data management company headquartered in [[Palo Alto, California|Palo Alto, CA]]. Rubrik offers a data management platform for enterprises in private, public, and hybrid [[Cloud computing|cloud environments]].<ref>{{Cite web|url=http://social.techcrunch.com/2016/08/16/rubrik-snares-61-million-series-c-led-by-khosla-ventures/|title=Rubrik snares $61 million Series C led by Khosla Ventures|last=Miller|first=Ron|website=TechCrunch|access-date=2017-01-23}}</ref>

{{Infobox company
| name = Rubrik
| type = Private
| logo = Rubrik logo.png
| industry = Cloud Data Management
| founder =  Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap
| hq_location_city = Palo Alto, CA
| hq_location_country = United States
| products = Rubrik Cloud Data Management platform
| website = {{URL|www.rubrik.com/}}
}}

== History ==
Rubrik was founded in 2014 by Bipul Sinha, Arvind Jain, Soham Mazumdar, and Arvind Nithrakashyap.<ref>{{Cite news|url=http://www.forbes.com/sites/benkepes/2015/03/24/with-an-a-grade-founding-team-and-a-grade-investors-rubrik-launches/#219bf1ea2b6e|title=With An A-Grade Founding Team And A-Grade Investors, Rubrik Launches|last=Kepes|first=Ben|newspaper=Forbes|access-date=2017-01-23}}</ref> It raised $10 million in March 2015,<ref>{{Cite news|url=http://blogs.wsj.com/venturecapital/2015/03/24/rubrik-emerges-with-10-million-for-software-defined-backup/|title=Rubrik Emerges With $10 Million for Software-Defined Backup|last=Gage|first=Deborah|newspaper=WSJ|language=en-US|access-date=2017-01-23}}</ref> followed by a $41 million round that same May. In August 2016, Rubrik raised an additional $61 million in funding led by [[Khosla Ventures]].<ref>{{Cite news|url=http://www.businessinsider.com/rubrik-raises-61-million-khosla-ventures-2016-8|title=This investor turned founder scoffs at funding slowdown: 'real businesses' can still get money|newspaper=Business Insider|language=en|access-date=2017-01-23}}</ref> Sinha, who is Founding Investor at [[Nutanix]] and a partner at [[Lightspeed Venture Partners]], currently serves as CEO.

In 2015, Rubrik launched with its r300 series appliances, called "Briks", in two configurations for VMware environments. The appliance includes a console from which users can manage and monitor their data.<ref>{{Cite web|url=http://cormachogan.com/2015/05/26/a-closer-look-at-rubrik/|title=A closer look at Rubrik|date=2015-05-26|website=CormacHogan.com|access-date=2017-01-23}}</ref> In April 2016, it launched its r528 Brik, which is [[FIPS 140-2]] Level 2 certified.<ref>{{Cite web|url=http://www.thepaypers.com/default/rubrik-r528-provides-storage-encryption-and-security/764067-0|title=Rubrik r528 provides storage encryption and security|website=www.thepaypers.com|access-date=2017-01-23}}</ref>

In April 2016, the company released their Cloud Data Management platform with Rubrik Firefly, which extended capabilities to physical [[SQL]] and [[Linux]] environments, and with Rubrik Edge, a software appliance that extended protection to remote and branch offices. The release also included a software upgrade to utilize [[Erasure code|erasure coding]].<ref>{{Cite web|url=http://www.theregister.co.uk/2016/08/16/rubriks_extra_funding_as_firefly_data_management_flies_out_of_the_coop/|title=Rubrik's extra funding as Firefly extended data management flies out of the coop|last=18:13|first=16 Aug 2016 at|last2=tweet_btn()|first2=Chris Mellor|access-date=2017-01-23}}</ref><ref>{{Cite web|url=http://blog.mwpreston.net/2016/08/16/rubrik-firefly-released/|title=Rubrik Firefly - Now with physical, edge, and moar cloud!|date=2016-08-16|website=mwpreston.net|access-date=2017-01-23}}</ref>

== Awards ==
Rubrik was a winner of the 2015 Virtualization Review Editor's Choice Awards.<ref>{{Cite web|url=https://virtualizationreview.com/articles/2015/12/01/editors-choice-awards.aspx|title=The 2015 Virtualization Review Editor's Choice Awards -|last=Ward|first=By Keith|last2=01/04/2016|website=Virtualization Review|access-date=2017-01-23}}</ref> In 2016, Rubrik was named a [[Gartner]] Cool Vendor in Storage Technologies.<ref>{{Cite web|url=https://www.gartner.com/doc/3290518/cool-vendors-storage-technologies-|title=Cool Vendors in Storage Technologies, 2016|website=www.gartner.com|access-date=2017-01-23}}</ref> Rubrik won “Best security or data protection project” for Best of VMworld Europe User Awards 2016.<ref>{{Cite news|url=http://www.techtarget.com/press-release/techtargets-searchservervirtualization-com-announces-best-vmworld-2016-award-winners/|title=TechTarget’s SearchServerVirtualization.com Announces “Best of VMworld” 2016 Award Winners - TechTarget|newspaper=TechTarget|language=en-US|access-date=2017-01-23}}</ref> In October 2016, Rubrik was selected as one of 25 “Next Billion-Dollar Startups” by [[Forbes]].<ref>{{Cite news|url=http://www.forbes.com/sites/amyfeldman/2016/10/19/next-billion-dollar-startups-2016/#1221eca0554e|title=Next Billion-Dollar Startups 2016|last=Feldman|first=Amy|newspaper=Forbes|access-date=2017-01-23}}</ref>

== References ==
{{Reflist}}

[[Category:Data management]]
[[Category:Cloud computing providers]]
[[Category:Companies based in California]]
<=====doc_Id=====>:379
<=====title=====>:
Compound document
<=====text=====>:
{{about|compound documents in general|the W3C standard|Compound Document Format}}
{{refimprove|date=November 2015}}
In [[computing]], a '''compound document''' is a document type typically produced using [[word processor|word processing]] software, and is a regular text document intermingled with non-text elements such as [[spreadsheet]]s, [[picture]]s, [[digital video]]s, [[digital audio]], and other [[multimedia]] features. It can also be used to collect several documents into one.

Compound document [[technology|technologies]] are commonly utilized on top of a [[software componentry]] framework, but the idea of software componentry includes several other concepts apart from compound documents, and software components alone do not enable compound documents. Well-known technologies for compound documents include:

*[[ActiveX Document]]s
*[[Bonobo (computing)|Bonobo]] by [[Ximian]] (primarily used by [[GNOME]])
*[[KPart]]s in [[KDE]]
*[[Multipurpose Internet Mail Extensions]]
*[[Object linking and embedding]] (OLE) by [[Microsoft]]; see [[Compound File Binary Format]]
*[[Open Document Architecture]] from [[ITU-T]] (not used)
*[[OpenDoc]] by [[Apple Computer]] (now defunct)
*[http://sourceforge.net/projects/verdantium Verdantium]
*[[XML]] and [[Extensible Stylesheet Language|XSL]] are encapsulation formats used for compound documents of all kinds

The first public implementation was on the [[Xerox Star]] [[workstation]], released in 1981.<ref>http://www.digibarn.com/collections/systems/xerox-8010/index.html</ref>

==See also==
* [[COM Structured Storage]]
* [[Transclusion]]
* [[Electronic Notebook]]

==References==
{{Reflist}}

[[Category:Electronic documents]]
[[Category:Multimedia]]

{{Multimedia-software-stub}}
<=====doc_Id=====>:382
<=====title=====>:
Category:Computer storage media
<=====text=====>:
{{Cat main|Computer storage media}}
This category refers to [[digital media]] used in [[computer storage]] devices.  Examples of such media include (a) magnetic disks, cards, tapes, and drums, (b) punched cards and paper tapes, (c) optical disks, (d) barcodes and (e) magnetic ink characters.


{{Commons cat|Computer storage media}}

[[Category:Electronic documents]]
[[Category:Storage media]]
[[Category:Digital media]]
[[Category:Computer data storage| Media]]
<=====doc_Id=====>:385
<=====title=====>:
Information capture
<=====text=====>:
{{refimprove|date=June 2016}}
'''Information capture''' is the process of collecting paper [[documents]], [[form (document)|form]]s and e-documents, transforming them into accurate, retrievable, [[Digital data|digital]] information, and delivering the information into business applications and [[databases]] for immediate action.<ref>http://www.emc.com/collateral/advertorial/aiim-advertorial.pdf</ref>

==See also==
* [[Ibml]]

==References==
<references/>

[[Category:Electronic documents]]


{{database-stub}}
<=====doc_Id=====>:388
<=====title=====>:
Transaction document
<=====text=====>:
'''Transaction documents'''  refers to legally relevant [[documents]] that are either printed, inserted and mailed, or electronically presented.<ref>[http://www.outputlinks.com/html/news/acadami_xplor_best_practices_progam_canada_043008.shtml Transaction documents]<blockquote>"...The course focuses on the concepts, technologies, and best practices associated with automated transaction document production."</blockquote></ref> They consist of a mixture of fixed and variable data. 

These [[documents]] are usually created by organizations through their financial computing system and then delivered to other parties (such as clients) through the [[post office]] or through an [[electronic billing]] system. The printed transaction documents, once delivered to the [[post office]], conform to the [[mail box rule]]. 

Common examples of transaction documents are:
* bills
* [[bank statements]] (and credit card, financial services, etc.)
* insurance policies
* notices
* other legally relevant correspondence, etc.

[[Xplor international]] is a technical association that focuses on the best practices and technologies associated with these documents.

==References==
{{Reflist}}

[[Category:Electronic documents]]
[[Category:Contract law]]


{{law-stub}}
<=====doc_Id=====>:391
<=====title=====>:
SAFE-BioPharma Association
<=====text=====>:
{{primary sources|date=February 2010}}
'''SAFE-BioPharma Association''' is the non-profit association that created and manages the SAFE-BioPharma [[digital identity]] and [[digital signature]] standard for the global [[pharmaceutical]], [[biotech]] and [[healthcare]] industries. SAFE stands for "Signatures & Authentication For Everyone" (but originally stood for "Secure Access For Everyone"<ref>{{cite web|title=CSI page on SAFE describing original goal and name|url=http://www.csirochester.com/safe.htm}}</ref>).  It was originally created as an initiative of the [[Pharmaceutical Research and Manufacturers of America]] (PhRMA) association to encourage the use of a common digital identity and digital signature standard for the pharmaceutical industry, but is now an independent non-profit association offering such standards services to the government and the entire healthcare industry.

The SAFE-BioPharma industry standard is used to establish and manage digital identities and to issue and apply digital signatures. It mitigates legal, regulatory and business risk associated with business-to-business and business-to-regulator electronic transactions. It also facilitates interoperability by providing a secure, enforceable, and regulatory-compliant way to verify identities of parties involved in electronic transactions.

SAFE-BioPharma’s vision is to be a catalyst in transforming the biopharmaceutical and healthcare communities to a fully electronic business environment by 2012.

== Certificate authority ==
The SAFE-BioPharma digital identity and signature standard operates one of the nation’s leading [[public key infrastructure]] (PKI) certificate authority bridges. These PKI certificate bridges establish an infrastructure for the trusted exchange of confidential information and the reliable authentication of identities over the Internet. By providing a highly secure way to validate, trust, and manage identities of unknown participants in an Internet transaction, the SAFE-BioPharma Bridge [[certificate authority|Certificate Authority]] (SBCA) is essential to helping achieve the speed, efficiency and cost-savings inherent in use of the Internet for business transactions. This technology also improves interoperability across many different systems.

== Cross-certification ==
The SAFE-BioPharma Bridge Certificate Authority is cross-certified with the [http://www.cio.gov/fpkia/ Federal Bridge Public Key Infrastructure Architecture] (FPKIA), facilitating the ability of SAFE-BioPharma member companies that meet certain security, technical and operational criteria to leverage the identity credentials of any and all bridge members in the exchange of sensitive and confidential information. In essence, it allows officials in [[Health and Human Services]], the [[Food and Drug Administration]], [[United States Department of Defense|Department of Defense]], and other government agencies to trust the origins of electronic documents received from corporate managers, physicians, clinical researchers, etc. who are credentialed to digitally sign documents with SAFE-BioPharma digital signatures. Additionally, the identities are trusted for authentication access control for sites requiring strong authentication.

== Regulatory acceptance ==
SAFE-BioPharma has worked closely with the US Food and Drug Administration (FDA), the [[European Medicines Agency]] (EMA) and other global healthcare and regulatory agencies to ensure the digital signatures generated using SAFE-BioPharma certificates meet regulatory requirements and are accepted by these agencies when used on documents that are part of electronic submissions. It thus allows voluminous paper documents used for regulatory compliance to be digitally signed and submitted in electronic form. This improves accuracy, reduces costs, enables electronic search and retrieval and saves energy and natural resources.

== Member organizations ==
SAFE-BioPharma Association members include [[Abbott Laboratories]] (NYSE: ABT), [[Amgen]] (NASDAQ: AMGN), [[AstraZeneca]] (NYSE: AZN), [[Bristol-Myers Squibb]] (NYSE:BMY), [[GlaxoSmithKline]] (NYSE: GSK), [[Johnson & Johnson]] (NYSE: JNJ), [[Eli Lilly and Company|Eli Lilly]] (NYSE: LLY), [[Merck & Co.|Merck]] (NYSE:MRK), [[National Notary Association]], [[Pfizer]] (NYSE: PFE), Premier Inc.,  and [[Sanofi-Aventis]] (NYSE:SNY).

SAFE-BioPharma is a trademark of the SAFE-BioPharma Association. Any use of this trademark requires approval from the SAFE-BioPharma Association.

==See also==
* [[Electronic lab notebook]]
* [[Public key infrastructure|PKI]]
* [[Title 21 CFR Part 11]]
* [[Digital signature]]
* [[Electronic Signatures in Global and National Commerce Act]] (ESIGN, USA)
* [[European Medicines Agency]] (EMEA)
* [[Food and Drug Administration]] (FDA)
* [[Pharmaceutical company]]
* [[Japan Pharmaceutical Manufacturers Association]] (JPMA)
* [[Digital signature]]
* [[Data management]]

==External links==
* [http://www.safe-biopharma.org SAFE-BioPharma Organization Website]

==References==

{{Reflist}}

{{DEFAULTSORT:Safe-Biopharma Association}}
[[Category:Public-key cryptography]]
[[Category:Cryptography companies]]
[[Category:Electronic documents]]
[[Category:Non-profit organizations based in New Jersey]]
[[Category:Pharmaceutical industry trade groups]]
[[Category:International medical and health organizations]]
[[Category:Medical and health organizations based in the United States]]
<=====doc_Id=====>:394
<=====title=====>:
Email management
<=====text=====>:
'''Email management''' is a specific field of [[communications management]] for managing high volumes of inbound electronic mail received by organizations. Today, email management is an essential component of customer service management.  Customer service call centers currently employ email response management agents along with telephone support agents, and typically use software solutions to manage emails.<ref>"Communications Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp <http://mdg.mit.edu/email-lab-interests.asp>] on 15 Nov 2011</ref><ref>How to use e-mail to improve customer service. Inc.com, Guide E-mail Customer Service, Retrieved from web [http://www.inc.com/guides/cust_email/20909.html <http://www.inc.com/guides/cust_email/20909.html >] on 20 January 2012</ref>

==Background==
Email management evolved from [[database management]] and [[customer relationship management]] (CRM).  Database management began in the 1960s. IBM provided one of the earliest solutions and established standards for database management.  Prominent database management platforms include Oracle, SQL Server etc.<ref>"Database Management - History Of Database Management." Free Encyclopedia of Ecommerce. Net Industries, n.d. Retrieved from Web. [http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html  <http://ecommerce.hostip.info/pages/295/Database-Management-HISTORY-DATABASE-MANAGEMENT.html>]. on 19 Dec 2011.</ref>  Vern Watts, inventor of [[IBM Information Management System|IBM's Information Management System]] (IMS), and [[Larry Ellison]], founder of [[Oracle Corporation]], are pioneers in database management systems.<ref>Luongo, C. et al. (2008). The tale of Vern Watts. [Web Video]. Retrieved from [www.youtube.com/watch?v=x98hgieE08o  <www.youtube.com/watch?v=x98hgieE08o>]. on 19 December 2011</ref><ref>"Larry Ellison Biography." Academy of Achievement. American Academy of Achievement, 16 Feb 2010. Web. 19 Dec 2011. <http://www.achievement.org/autodoc/page/ell0bio-1>.</ref>

As database management solutions became more sophisticated in functionality, marketing and customer service departments of large organizations started using information about customers for [[database marketing]].  Customer service managers soon realized that they could extend database marketing to store and retrieve all customer communications to improve visibility with key clients.  This led to the development of CRM systems which managed communication with customers and prospective customers using various media, including phone, direct mail, web site, and email.<ref>"The history of CRM -- evolving beyond the customer database." CRM Software Guide. crm-software-guide.com, n.d. Retrieved from Web. [http://www.crm-software-guide.com/history-of-crm.htm <http://www.crm-software-guide.com/history-of-crm.htm>]. on 19 Dec 2011.</ref>  Pioneers in CRM include [[David Duffield]], creator of [[PeopleSoft]], and [[Thomas Siebel|Tom Siebel]], founder of [[Siebel Systems]].<ref>"PeopleSoft Inc." International Directory of Company Histories. 2000. In Retrieved Encyclopedia.com Retrieved from web [http://www.encyclopedia.com/doc/1G2-2843700094.html  <http://www.encyclopedia.com/doc/1G2-2843700094.html>] on 19 December 2011</ref><ref>Thomas Siebel 1952- Biography - Early life and education, Oracle, Siebel systems. ND. Reference for Business Encyclopedia of Business, 2nd ed. Retrieved from web [http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b < http://www.referenceforbusiness.com/biography/S-Z/Siebel-Thomas-1952.html#b >] on 20 January 2012</ref>

As email became one of the most prevalent business-to-customer communication media in the 1990s, customer service departments needed specialized systems of tools and trained staff to manage email communication with their customers and prospective customers.

==History==
In 1994, Information Cybernetics, a company in Cambridge, Massachusetts, developed tools for pattern analysis and categorization of emails and other electronic communication channels. The platform of tools was called [[EchoMail]].  The first company to adopt  EchoMail was [[AT&T]]. [[J. C. Penney|JC Penney]] adopted EchoMail in 1997.<ref>Callaway, Erin. "Return to Sender." PC Week Executive. 14 July 1997: 111, 114. Print.</ref><ref>O'Brien, J. A. (2002). Introduction to information systems. (10 ed., p. 370). McGraw-Hill Irwin. Retrieved from Web. [http://www.mcm.edu/~palafoxt/sixth.htm <http://www.mcm.edu/~palafoxt/sixth.htm>]. On 8 Dec 2011</ref><ref>"The EchoMail Digital Refinery." www.echomail.com. EchoMail, Inc., n.d. Retrieved from Web [http://echomail.com/technology-for-email-management-detailed/ <http://echomail.com/technology-for-email-management-detailed/>]. on 8 Dec 2011</ref>

Another early company that developed email management software systems was FortÈ Internet Software, which produced Adante.<ref>Pavita, H. (1997, June 24). Forte introduces adante 1.0 server software for managing internet-based customer service and communications. Business Wire, Retrieved from Web [http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024 <http://www.thefreelibrary.com/Forte introduces Adante 1.0 server software for managing..-a019535024>] on 8 Dec 2011</ref>  By late 1999, companies such as KANA Software, Inc., also emerged to support this effort.<ref>"Email Response System - Intelligent Message Handling :: KANA." www.kana.com. KANA Software, n.d. Retrieved from Web. [http://www.kana.com/customer-service/email-response-system.php  <http://www.kana.com/customer-service/email-response-system.php>]. on 8 Dec 2011</ref>  Eventually, companies such as Siebel CRM Systems, Inc., incorporated components of email management into their CRM systems.<ref>"Bookshelf v7.5: Overview of Siebel eMail Response." docs.oracle.com. ORACLE Corporation, 21 April 2003. Retrieved from Web. [http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html  <http://docs.oracle.com/cd/E05554_01/books/eMail/eMailOverview.html>]. on 8 Dec 2011</ref>

==Typical system components==
An email management system consists of various components to handle different phases of the email management process.<ref>"EMAIL Management." Media and Organizational Biomimetics Initiative. mdg.mit.edu, 25 Oct 2011. Retrieved from web  [http://mdg.mit.edu/email-lab-interests.asp <http://mdg.mit.edu/email-lab-interests.asp>] on 15 Nov 2011</ref>  These components include: 
*Email ticketing system - One of the key tasks performed by email [[management system]]s is to allocate reference numbers to all incoming [[email]]s. This process is known as ticketing. All subsequent emails relating to one matter can then be grouped under the same reference. This allows users to track their correspondence in a more time effective and productive way.
*Email receipt module - Receives emails, filters out spam and unwanted content to a separate queue (sometimes called [[email filtering]]), and assigns unique ticket numbers based on certain conditions.
*Bayesian spam filters - Statistical technique of filtering spam that most current email management systems utilize.
*Data enhancement module - Adds tags to each email for further processing and may include the ability to connect to remote databases and retrieve specific information about the email author and his/her transactions with the organization.
*Intelligent Analysis module - Reads the subject, message, and attachments, and any tags added by the data enhancement module, analyzing its content in an attempt to understand the subject matter of the email.  This module may store this 'intelligence' as additional tags.

==References==
{{reflist|30em}}

{{DEFAULTSORT:E-Mail Ticketing System}}
[[Category:Email]]
[[Category:Communication software]]
[[Category:Electronic documents]]
[[Category:Records management]]
<=====doc_Id=====>:397
<=====title=====>:
E-bible
<=====text=====>:
{{Orphan|date=December 2009}}
{{Refimprove|date=December 2009}}

Sometimes known as document bibles or transaction deal bibles, '''e-bibles''' are a means of storing, indexing and comprehensively searching large volumes of [[document]]s related to any corporate transaction. 

They are commonly used by [[Legal firm]]s to collate documents from a certain case in order to store or give to a client at the end of a project. e-bibles are a means of storing complex legal folders which were usually kept in hard copy.

In 2009, Proposals<ref>http://www.litig.org/index.php?option=com_content&task=category&sectionid=2&id=20&Itemid=33 </ref> were put in place in order to standardise the creation of e-bibles throughout the [[legal industry]].

There are few suppliers of COTS solutions, however Diskbuilder<ref>http://www.diskbuilder.co.uk</ref>  and Ideagen<ref>https://www.ideagenplc.com/</ref> (formerly Capgen) are notable exceptions.
== References ==
{{Reflist}}

[[Category:Legal documents]]
[[Category:Electronic documents]]
[[Category:Document management systems]]
<=====doc_Id=====>:400
<=====title=====>:
Quickstart guide
<=====text=====>:
A '''quickstart guide''' is a short, simple introductory guide to a piece of equipment for many consumer electronics products (e.g. [[television]]) or recently, [[automobile]]s, [[mobile phone]]s, computers connection. 

With the increase in complexity and functions with electronics products quickstart guides are created to get users quickly accustomed to the basic operations of the product. Complex or detailed operations are usually left in the full-length [[owner's manual]].

{{Electronics-stub}}

[[Category:Electronic documents]]
<=====doc_Id=====>:403
<=====title=====>:
Apache Wave
<=====text=====>:
{{Infobox software
|name                       = Apache Wave
|logo                       = Apache Wave logo.png
|screenshot                 = Google Wave.png
|caption                    = Google Wave, the previous incarnation of Apache Wave
|collapsible                = 
|author                     = [[Google]]
|developer                  = [[Apache Software Foundation]], Google
|released                   = {{start date|2009|5|27}}
|latest release version     =
|latest release date        = <!-- {{Start date and age|YYYY|MM|DD}} -->
|latest preview version     =
|latest preview date        = <!-- {{Start date and age|YYYY|MM|DD}} -->
|frequently updated         =
|programming language       = [[Java (programming language)|Java]]<ref>{{cite web |url=http://www.lextrait.com/Vincent/implementations.html |title=The Programming Languages Beacon, v10.0 |first=Vincent |last =Lextrait |date=January 2010 |accessdate=14 March 2010}}</ref>
|operating system           =
|platform                   = [[Web application]]
|size                       =
|language                   =
|status                     =
|genre                      = [[Collaborative real-time editor]]
|license                    = [[Apache License]]
|website                    = {{URL|incubator.apache.org/wave/}}
|repo                       = {{URL|https://git-wip-us.apache.org/repos/asf/incubator-wave.git}}
}}
'''Apache Wave''' is a software framework for [[Collaborative real-time editor|real-time collaborative editing]] online. [[Google]] originally developed it as '''Google Wave'''.<ref>{{cite web|url=http://wave.google.com/about.html
|title=Google Wave Overview|author=Google Inc.
|quote=[A] new web application for real-time communication and collaboration.
|year=2009|accessdate=May 2010| archiveurl= https://web.archive.org/web/20100427183005/http://wave.google.com/about.html| archivedate= 27 April 2010 <!--DASHBot-->| deadurl= no}}</ref>
It was announced at the [[Google I/O]] conference on May 27, 2009.<ref>[[TechCrunch]] (May 28, 2009):
[http://www.techcrunch.com/2009/05/28/google-wave-drips-with-ambition-can-it-fulfill-googles-grand-web-vision/ Google Wave Drips With Ambition.  A New Communication Platform For A New Web.]</ref><ref name="iokeynote">{{cite web
|url=https://www.youtube.com/watch?v=v_UyVmITiYQ
|title=I/O Conference Google Wave Keynote|author=Google Inc.}}</ref>

Wave is a [[web application|web-based]] [[computing platform]] and [[communications protocol]] designed to merge key features of [[Media (communication)|communications media]] such as [[email]], [[instant messaging]], [[wiki]]s, and [[Social networking service|social networking]].<ref name="aboutgw">{{cite web
|url=http://wave.google.com/help/wave/about.html|title=About Google Wave|author=Google Inc.}}</ref> Communications using the system can be [[Synchronization|synchronous]] or [[Asynchronous communication#Electronically mediated communication|asynchronous]]. Software extensions provide contextual [[spell checker|spelling and grammar checking]], [[machine translation|automated language translation]]<ref name="iokeynote" /> and other features.<ref name="gwdevblog">{{cite web|url=http://googlewavedev.blogspot.com/2009/05/introducing-google-wave-apis-what-can.html|title=Google Wave Developer Blog|publisher=Google}}</ref>

Initially released only to developers, a preview release of Google Wave was extended to 100,000 users in September 2009, each allowed to invite additional users. Google accepted most requests submitted starting November 29, 2009, soon after the September extended release of the technical preview. On May 19, 2010, it was released to the general public.<ref>Shankland, Stephen. (2010-05-19) [http://news.cnet.com/8301-30685_3-20005394-264.html Google Wave: Now open to the public | Deep Tech – CNET News]. News.cnet.com. Retrieved on 2010-12-14.</ref>

On August 4, 2010, Google announced the suspension of stand-alone Wave development and the intent of maintaining the web site at least for the remainder of the year,<ref>[http://googleblog.blogspot.com/2010/08/update-on-google-wave.html Official Google Blog: Update on Google Wave]. Googleblog.blogspot.com (2010-04-08). Retrieved on 2010-12-14.</ref> and on November 22, 2011, announced that existing Waves would become read-only in January 2012 and all Waves would be deleted in April 2012.<ref>{{cite web|url=http://googleblog.blogspot.com/2011/11/more-spring-cleaning-out-of-season.html |title=Official Blog: More spring cleaning out of season |publisher=Googleblog.blogspot.com |date=2011-11-22 |accessdate=2013-06-15}}</ref> Development was handed over to the [[Apache Software Foundation]] which started to develop a server-based product called '''Wave in a Box'''.<ref>Meyer, David. (2010-09-03) [http://www.zdnet.co.uk/news/application-development/2010/09/03/google-puts-open-source-wave-in-a-box-40089999/ Google puts open-source Wave in a 'box' | Application Development | ZDNet UK]. Zdnet.co.uk. Retrieved on 2010-12-14.</ref><ref>[http://www.idg.se/2.1085/1.355483/google-wave-inte-ute-ur-leken Google Wave inte ute ur leken]. IDG.se. Retrieved on 2010-12-14.</ref><ref>Murphy, David. (1970-01-01) [http://www.pcmag.com/article2/0,2817,2368730,00.asp Google Spins Wave Into 'Wave in a Box' for Third-Party Use | News & Opinion]. PCMag.com. Retrieved on 2010-12-14.</ref>

==History==
[[File:Googlewave.svg|thumb|upright|The original logo while owned by Google]]

===Origin of name===
The science fiction television series ''[[Firefly (TV series)|Firefly]]'' provided the inspiration for the project's name.<ref name=itnewsau>{{cite news
|first=Nate
|last=Cochrane
|url=http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx
|title=Opinion: Google's wave drowns the bling in Microsoft's Bing
|agency=IT News Australia
|date=2009-05-29
|accessdate=2009-06-03
| archiveurl= https://web.archive.org/web/20090603041903/http://www.itnews.com.au/News/104396,opinion-googles-wave-drowns-the-bling-in-microsofts-bing.aspx| archivedate= 3 June 2009 <!--DASHBot-->| deadurl= no}}</ref> In the series, a ''wave'' is an electronic communication, often consisting of a [[video call]] or video message.<ref name=itnewsau/>  During the developer preview, a number of references were made to the series, such as [[Lars Rasmussen (Software Developer)|Lars Rasmussen]] replying to a message with "shiny", a word used in the series to mean ''cool'' or ''good'', and the crash message of Wave being a popular quotation from the series: "Curse your sudden but inevitable betrayal!"<ref name="iokeynote" /><ref>Originally said by [[List of characters in the Firefly universe#Hoban Washburne|Wash]] at 6:36, in ''[[Serenity (Firefly episode)|Serenity]]''; [[Firefly (TV series)|Firefly]]: The Complete Series (Blu-ray), 2008, 20th Century Fox.</ref> Another common error message, "Everything's shiny, Cap'n. Not to fret!" is a quote from [[Kaylee Frye]] in the 2005 motion-picture ''Firefly'' continuation, ''[[Serenity (film)|Serenity]]'', and it is matched with a sign declaring that "This wave is experiencing some turbulence and might explode. If you don't want to explode..." which is another reference to the opening of the film.

During an event in [[Amsterdam]], [[Netherlands]],<ref name="nextweb">{{cite news | first = Ralf | last = Rottmann | url = http://thenextweb.com/appetite/2009/10/30/breaking-google-wave-opened-federation-today-host/ | title = Google Wave to be opened for federation today! | agency = The Next Web | date = October 30, 2009}}</ref> it became apparent that the 60-strong team that was currently working on Wave in [[Sydney, Australia]] use  [[Joss Whedon]]-related references to describe, among others, the sandbox version of Wave called ''[[Dollhouse (TV series)|Dollhouse]]'' after the TV-series by ''Firefly'' producer Joss Whedon, which was aired on Fox in the U.S. The development of external extensions is codenamed "Serenity", after the spaceship used in ''Firefly'' and ''Serenity''.

===Open source===
Google released most of the source code as [[open source software]],<ref name="iokeynote"/> allowing the public to develop its features through extensions.<ref name="iokeynote" />  Google allowed third parties to build their own Wave services (be it private or commercial) because it wanted the [[Google Wave Federation Protocol|Wave protocol]] to replace the [[e-mail]] protocol.<ref name="iokeynote"/><ref name="gwarchitecture" /><ref name="wpcsmodel" /> Initially, Google was the only Wave service provider, but it was hoped that other service providers would launch their own Wave services, possibly designing their own unique web-based clients as is common with many email service providers.  The possibility also existed for native Wave clients to be made, as demonstrated with their [[command-line interface|CLI]]-based console client.<ref name="osreleasenext">{{cite web|url=https://groups.google.com/group/wave-protocol/browse_thread/thread/618ff4e9ef477e80?pli=1|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}</ref>

Google released initial open-source components of Wave:<ref name="osrelease1">{{cite web|url=http://googlewavedev.blogspot.com/2009/07/google-wave-federation-protocol-and.html|title=Google Wave Federation Protocol and Open Source Updates|publisher=Google}}</ref>
# the [[operational transformation]] (OT) code,
# the underlying wave model, and
# a basic client/server prototype that uses the wave protocol

In addition, Google provided some detail about later phases of the open-source release:<ref name="osreleasenext" />
# wave model code that is a simplified version of Google's production code and is tied to the OT code; this code will evolve into the shared code base that Google will use and expects that others will too
# a testing and verification suite for people who want to do their own implementation (for example, for porting the code to other languages)

===Reception===
{{Wikinews|Google to discontinue social networking application Google Wave}}
During the initial launch of Google Wave, invitations were widely sought by users and were sold on auction sites.<ref>[http://mashable.com/2009/09/30/google-wave-invite/ Google Wave Invite Selling for $70 on eBay]</ref><!--  However, people were confused as to how to use it.<ref>[http://www.csmonitor.com/Innovation/Horizons/2009/1124/so-youve-got-google-wave-now-what Christian Science Monitor: So you've got Google wave, now what?]</ref> Google Wave was called an "over-hyped disappointment for the first generation of users"<ref>[http://www.linux-mag.com/id/7653 Linux Mag 2009 Review]</ref> with "dismal usability"<ref>[http://themilwaukeeseo.com/2009/12/14/the-google-wave-failure/ Google Wave Failure on Milwaukee SEO]</ref> that "humans may not be able to comprehend."<ref name="EngagetWave">[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]</ref> -->
Those who received invitations and decided to test Google Wave could not communicate with their contacts on their regular email accounts. The initial spread of Wave was very restricted.

===End of development of ''Google Wave''===
Google Wave initially received positive press coverage for its design<ref>[http://news.bbc.co.uk/1/hi/technology/8282687.stm B.B.C. report introducing Google Wave in September 2009]</ref> and potential uses.<ref name="EngagetWave">[http://www.engadget.com/2009/10/27/google-wave-to-have-its-own-app-store/ Google Wave to get its own App Store (Engadget)]</ref><ref>[http://asia.cnet.com/blogs/tokyo-shift/post.htm?id=63015591 CNET Predictions for 2010]</ref> On August 4, 2010, Google announced Wave would no longer be developed as a stand-alone product due to a lack of interest.<ref name="ZDNet on GW's death">[http://www.zdnet.com/blog/google/how-will-google-wave-be-reincarnated/2344 ZDNet on GW's death]</ref> Google's statement surprised many in the industry and user community.

Google later clarified the Wave service would be available until [[Google Docs]] was capable of accessing saved waves.<ref>{{cite web|url=http://www.google.com/support/wave/bin/answer.py?answer=1083134 |title=Status of Google Wave - Google Help |publisher=Google.com |date= |accessdate=2013-06-15}}</ref>

Response to the news of the end of development came from Wave users in the form of a website.<ref>[http://www.webpronews.com/topnews/2010/08/09/save-google-wave-site-forms '"Save Google Wave" Site Forms']</ref> Since their announcement in early August, the website has recorded over 49,000 supporter registrations urging Google Wave's continuation.<ref>[http://www.savegooglewave.com Save Google Wave!]. Retrieved on 2011-05-14.</ref>

In retrospect, the lack of success of Google Wave was attributed among other things to its complicated user interface resulting in a product that merged features of email, instant messengers and wikis but ultimately failed to do anything significantly better than the existing solutions.<ref>[http://arstechnica.com/software/news/2010/08/google-wave-why-we-didnt-use-it.ars Google Wave: why we didn't use it], [[Ars Technica]]</ref>

Chris Dawson of online technology magazine [[Zdnet]] discussed inconsistencies in the reasoning of Google in deciding to end support for Wave,<ref name="ZDNet on GW's death"/> mentioning its "deep involvement" in developing social media networks, to which many of Wave's capabilities are ideally suited. Perhaps Google Wave was ended to clear the stage for their new social network [[Google+]] that tried to compete with Facebook but has not gained the reach as a similar comprehensive social networking site.<ref>[http://www.utalkmarketing.com/pages/Article.aspx?ArticleID=21768&Title=Can_Google+_really_challenge_Facebook_and_be_an_asset_to_brands "Can Google+ really challenge Facebook and be an asset to brands?" utalkmarketing.com]</ref>

===Apache Wave===
Google Wave was accepted by the [[Apache Software Foundation]]'s Incubator program under the project name Apache Wave. The Google Wave Developer blog was updated with news of the change on December 6, 2010.<ref>North, Alex. (2010-12-06) [http://googlewavedev.blogspot.com/2010/12/introducing-apache-wave.html Google Wave Developer Blog: Introducing Apache Wave]. Googlewavedev.blogspot.com. Retrieved on 2010-12-14.</ref> A Wave Proposal page with details on the project's goals was created on the Apache Foundation's Incubator Wiki.<ref>[http://wiki.apache.org/incubator/WaveProposal WaveProposal – Incubator Wiki]. Wiki.apache.org (2010-11-24). Retrieved on 2010-12-14.</ref>

====Wave in a Box====
[[File:Wave in a Box logo.png|thumb|75px|The logo for Wave in a Box]]
Wave in a Box is the current server implementation of Apache Wave.  Currently, there are not any demo servers available.<ref name=demo_servers>{{cite web|title=Wave in a Box demo servers|url=http://incubator.apache.org/wave/demo-servers.html|publisher=Apache Software Foundation|accessdate=10 October 2012}}</ref>

==Features==
Google Wave was a new [[Internet]] [[communications]] platform. It was written in [[Java (programming language)|Java]] using [[OpenJDK]] and its web interface used the [[Google Web Toolkit]]. Google Wave works like previous messaging systems such as [[email]] and [[Usenet]], but instead of sending a message along with its entire thread of previous messages, or requiring all responses to be stored in each user's inbox for context, message documents (referred to as ''waves'') that contain complete threads of multimedia messages (blips) are perpetually stored on a central server. Waves are shared with collaborators who can be added or removed from the wave at any point during a wave's existence.

Waves, described by Google as "''equal parts conversation and document''", are hosted [[XML]] documents that allow seamless and low latency concurrent modifications.<ref name="ot">[http://www.waveprotocol.org/whitepapers/operational-transform Google Wave Operational Transformation – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.</ref> Any participant of a wave can reply anywhere within the message, edit any part of the wave, and add participants at any point in the process. Each edit/reply is a blip and users can reply to individual blips within waves. Recipients are notified of changes/replies in all waves in which they are active and, upon opening a wave, may review those changes in chronological order. In addition, waves are live. All replies/edits are visible in real-time, letter-by-letter, as they are typed by the other collaborators. Multiple participants may edit a single wave simultaneously in Google Wave. Thus, waves can function not only as e-mails and [[threaded discussion|threaded conversations]] but also as an [[instant messaging]] service when many participants are online at the same time. A wave may repeatedly shift roles between e-mail and instant messaging depending on the number of users editing it concurrently. The ability to show messages as they are typed can be disabled, similar to conventional instant messaging.<ref name="aboutgw"/>

The ability to modify a wave at any location lets users create collaborative documents, [[collaborative editing|edited]] in a manner akin to [[wiki]]s. Waves can easily link to other waves. In many respects, it is a more advanced forum.<ref>[http://variableghz.com/2009/10/google-wave-review/ Google Wave Review]. VariableGHz (2009-10-13). Retrieved on 2010-12-14.</ref> It can be read and known to exist by only one person, or by two or more and can also be public, available for reading ''and'' writing to everyone on the Wave.

The history of each wave is stored within it. Collaborators may use a playback feature to observe the order in which it was edited, blips that were added, and who was responsible for what in the wave.<ref name="aboutgw"/><ref name="gwdevblog"/> The history may also be searched by a user to view and/or modify specific changes, such as specific kinds of changes or messages from a single user.<ref name="iokeynote" />

==Extension programming interface==
{{anchor|Google Wave extensions}}
Google Wave is extensible through an [[application programming interface]] (API). It provides extensions in the form of ''Gadgets'' and ''Robots'', and is embeddable by dropping interactive windows into a given wave on external sites, such as [[blog]] sites.<ref name="iokeynote" /><ref name="codepage" />

The last version of robots API is 2.0.<ref name="robotsapiv2">{{cite web|url=http://googlewavedev.blogspot.com/2010/03/introducing-robots-api-v2-rise-of.html|title=Introducing Robots API v2: The Rise of Active Robots|publisher=Google}}</ref>

Google Wave also supports extension installers, which bundle back-end elements (robots and gadgets) and front-end user interface elements into an integrated package. Users may install extensions directly within the Wave client using an extension installer.

===Extensions===
Google Wave extensions are [[Plug-in (computing)|add-ins]] that may be installed on Google Wave to enhance its functionality. They may be [[Internet bot]]s (robots) to automate common tasks, or gadgets to extend or change user interaction features, e.g., posting blips on [[microblog]] feeds or providing RSVP recording mechanisms.<ref name="iokeynote" /><ref name="aboutgw" /><ref name="codepage">{{cite web|url=https://code.google.com/apis/wave/|title=Google Wave API – Google Code|publisher=Google}}</ref>

Over 150 Google Wave extensions have been developed either in the form of Gadgets or Robots.<ref>[http://wave-samples-gallery.appspot.com/ Google Wave Samples Gallery]. Wave-samples-gallery.appspot.com. Retrieved on 2010-12-14.</ref>

====Robots====
A robot is an automated participant on a wave. It can read the contents of a wave in which it participates, modify its contents, add or remove participants, and create new blips or new waves. Robots perform actions in response to events. For example, a robot might publish the contents of a wave to a public [[blog]] site and update the wave with user comments.

Robots may be added as participants to the Wave itself. In theory, a robot can be added anywhere a human participant can be involved.

====Gadgets====
Gadget extensions are applications that run within the wave, and to which all participants have access. Robots and Gadgets can be used together, but they generally serve different purposes. A gadget is an application users could participate with, many of which are built on Google’s [[OpenSocial]] platform. A good comparison would be iGoogle gadgets or Facebook applications.

The gadget is triggered based on the user action. They can be best described as applications installed on a mobile phone. For example, a wave might include a [[sudoku]] gadget that lets the wave participants compete to see who can solve the puzzle first.

Gadgets may be added to individual waves and all the participants share and interact with the gadget.

==Federation protocol==
{{Main article|Google Wave Federation Protocol}}
Google Wave provides [[Federation (information technology)|federation]] using an extension of [[XMPP|Extensible Messaging and Presence Protocol]] (XMPP), the [[open standard|open]] [[Google Wave Federation Protocol|Wave Federation Protocol]]. Being an open protocol, anyone can use it to build a custom Wave system and become a wave provider.<ref>{{cite web|url=http://www.waveprotocol.org/|title=Google Wave Federation Protocol|publisher=Google}}</ref>  The use of an open protocol is intended to parallel the openness and ease of adoption of the [[e-mail]] protocol and, like e-mail, allow communication regardless of provider. Google hoped that waves would replace e-mail as the dominant form of Internet communication.<ref name="iokeynote"/><ref name="gwarchitecture">[http://www.waveprotocol.org/whitepapers/google-wave-architecture Google Wave Federation Architecture – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.</ref><ref name="wpcsmodel">[http://www.waveprotocol.org/whitepapers/internal-client-server-protocol Google Wave Client-Server Protocol – Google Wave Federation Protocol]. Waveprotocol.org. Retrieved on 2010-12-14.</ref>  In this way, Google intended to be only one of many wave providers<ref name="iokeynote"/><ref name="gwarchitecture" /><ref name="wpcsmodel" />  and to also be used as a supplement to [[e-mail]], [[instant messaging]], [[FTP]], etc.

A key feature of the protocol is that waves are stored on the service provider's servers instead of being sent between users. Waves are federated; copies of waves and wavelets are distributed by the wave provider of the originating user to the providers of all other participants in a particular wave or wavelet so all participants have immediate access to up-to-date content. The originating wave server is responsible for hosting, processing, and concurrency control of waves.<ref name="gwarchitecture" /><ref name="wpcsmodel" />  The protocol allows private reply wavelets within parent waves, where other participants have no access or knowledge of them.<ref name="gwarchitecture" /><ref name="wpcsmodel" />

Security for the communications is provided via [[Transport Layer Security]] authentication, and encrypted connections and waves/wavelets are identified uniquely by a service provider's [[domain name]] and ID strings. User-data is not federated, that is, not shared with other wave providers.

===Adoption of Wave Protocol and Wave Federation Protocol===
Besides Apache Wave itself, there are other open-source variants of servers and clients with different percentage of Wave Federation and Wave Protocol support. Wave has been adopted for corporate applications by Novell for [[Novell Pulse]],<ref>[http://www.novell.com/products/pulse/ Novell Vibe cloud service]. Novell.com. Retrieved on 2010-12-14.</ref> or by [[SAP AG|SAP]] for Cloudave,<ref>Elliott, Timo. (2009-10-19) [http://www.cloudave.com/link/sap-gravity-prototype-business-collaboration-using-google-wave SAP's Gravity Prototype: Business Collaboration Using Google Wave]. Cloudave.com. Retrieved on 2010-12-14.</ref> and community projects such as PyOfWave or [[Kune (software)|Kune]].

====Compatible third-party servers====
The following servers are compatible with the Google Wave protocol:
* '''[[Kune (software)|Kune]]'''<ref>{{cite web|title=Kune Homepage|url=http://kune.ourproject.org|accessdate=22 April 2012}}</ref> is a free/open source platform for social networking, collaborative work and web publishing, focusing on work groups and organizations rather than in individuals. It provides lists, tasks, documents, galleries, etc., while using waves underneath. It focuses on [[free culture movement|free culture]] and [[social movements]] needs.
* '''[[Novell Vibe]]''', formerly known as Novell Pulse<ref>[http://www.novell.com/promo/vibe.html Novell Vibe]. Novell.com (2009-12-31). Retrieved on 2010-12-14.</ref>
* '''Rizzoma'''<ref>{{cite web|title=Rizzoma Homepage|url=http://rizzoma.com|accessdate=9 May 2012}}</ref> is a platform for collaborative work in real time. It allows communication within a certain context permitting a chat to instantly become a document where topics of a discussion organized into branches of mind-map diagram and minor details are collapsed to avoid distraction. The user is able to sign in using a Google or Facebook account and choose whether your topics are private or public.
* '''[[SAP StreamWork]]''' is a collaboration decision making service.<ref>Williams, Alex. (2010-05-17) [http://www.readwriteweb.com/cloud/2010/05/sap-streamworks-integrates-wit.php SAP StreamWork Integrates With Google Wave – ReadWriteCloud]. Readwriteweb.com. Retrieved on 2010-12-14.</ref><ref>[http://www.sapstreamwork.com/how-it-works/ How It Works | SAP® StreamWork™]. Sapstreamwork.com. Retrieved on 2010-12-14.</ref>

==See also==
{{Portal|Software}}
* [[Microsoft Sharepoint Workspace]]
* [[Real-time text]]
* [[Opera Unite]]

==References==
{{Reflist|colwidth=30em}}

==External links==
{{Commons category|Google Wave}}
* [http://incubator.apache.org/wave/ Apache Wave]
* [http://incubator.apache.org/wave/demo-servers.html Wave in a Box]
* [http://wave.google.com/ Google Wave]
* [https://code.google.com/apis/wave/ Google Wave API]
* [http://googlewavedev.blogspot.com/ Google Wave Developer Blog]
* [https://www.youtube.com/watch?v=v_UyVmITiYQ Full Video of the Developer Preview at Google IO on ]
* [https://www.youtube.com/watch?v=p6pgxLaDdQw Google Wave overview video]
* [http://www.waveprotocol.org/ Google Wave Federation Protocol]

{{Google Inc.}}
{{Apache}}

[[Category:Discontinued Google services|Wave]]
[[Category:Web applications]]
[[Category:Computing platforms]]
[[Category:Electronic documents]]
[[Category:Instant messaging]]
[[Category:Online chat]]
[[Category:Social information processing]]
[[Category:Groupware]]
[[Category:Wikis]]
[[Category:Internet protocols]]
[[Category:Internet Protocol based network software]]
[[Category:Self-organization]]
[[Category:Blogging]]
[[Category:Collaborative real-time editors]]
[[Category:2009 software]]
[[Category:2010 disestablishments]]
[[Category:Discontinued software]]
[[Category:Discontinued Google software]]
[[Category:Software using the Apache license]]
[[Category:Social networking services]]
[[Category:Apache Software Foundation|Wave]]
<=====doc_Id=====>:406
<=====title=====>:
ViXra
<=====text=====>:
{{lowercase title}}{{Infobox Website
|name           = viXra
|logo           = 
|screenshot     = 
|caption        = 
|url            = [http://viXra.org/ viXra.org]
|alexa          = 
|commercial     = No
|type           = Science
|language       = English
|registration   = 
|owner          = 
|author         = 
|launch date    = 
|current status = Online
|revenue        = 
|slogan         =
}}

'''viXra''' is an electronic [[e-print]] archive set up by independent physicist Philip Gibbs as an alternative to the dominant [[arXiv]] service operated by [[Cornell University]].

==Description==
Although dominated by physics and mathematics submissions, viXra aims to cover topics across the whole scientific community. It accepts submissions without requiring authors to have an academic affiliation and without any threshold for quality.<ref>"[http://blogs.nature.com/news/2009/07/whats_arxiv_spelled_backwards.html What’s arXiv spelled backwards? A new place to publish]". ''Nature News Blog''. 16 July 2009.</ref> The e-prints on viXra are grouped into seven broad categories: physics, mathematics, computational science, biology, chemistry, humanities, and other areas.<ref name="vixra">{{cite web |url=http://vixra.org/ |title=ViXra.org open e-print archive |work=viXra.org | accessdate=22 August 2011}}</ref> Anyone may post anything on viXra, though house rules do prohibit “vulgar, libellous [sic], plagiaristic or dangerously misleading” content.<ref>http://nautil.us/issue/41/selection/what-counts-as-science</ref>

Gibbs' original motivation for starting the archive was to cater for researchers who believed that their preprints had been unfairly rejected or reclassified by the arXiv moderators.<ref name="pw">{{cite journal |title=Fledgling site challenges arXiv server |work=[[Physics World]] |date=15 July 2009 |url= http://physicsworld.com/cws/article/news/39845}}</ref> As of 2013 it had already over 4000 preprints<ref>{{citation|title=A Good Year for viXra|first=Philip E.|last=Gibbs|journal=Prespacetime Journal|volume=4|issue=1|year=2013|pages=87–90|url=http://prespacetime.com/index.php/pst/article/view/482}}.</ref> and in October, 2016 the number had grown to 16,214.<ref>Official site (front page)</ref>

==References==
{{Reflist}}

==External links==
* [http://vixra.org/ Official website]

[[Category:Eprint archives]]
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Online archives]]


{{website-stub}}
<=====doc_Id=====>:409
<=====title=====>:
Lettrs
<=====text=====>:
{{lowercase title}}
{{Infobox software
| name         = lettrs
| logo         = [[File:Lettrs logo Square.jpg|200px]]
| released     = {{Start date|2012}}
| developer   = [[Drew Bartkiewicz]] (CEO)
| status             = Active
| operating system   = [[IOS|Apple iOS]], [[Android (operating system)|Android]]
| genre              = [[Social Networking]]
| website  =  {{URL|http://www.lettrs.com}}
}}

'''lettrs''' is a global [[mobile app|mobile application]] and [[social network]] that allows users to compose and send mobile messages privately or publicly.<ref name=crunch>{{cite web |url=https://www.crunchbase.com/organization/lett-rs |title= lettrs |author=<!--Staff writer(s); no by-line.--> |website=CrunchBase.com |publisher=AOL Inc |accessdate=26 January 2015}}</ref><ref name=bweek>{{cite web |url=http://www.businessweek.com/articles/2014-01-30/youve-sent-mail-a-letter-writing-app-forces-users-to-slow-down |title=You've Sent Mail: A Letter-Writing App Forces Users to Slow Down |last1=Leonard |first1=Devin |date=30 January 2014 |website=BusinessWeek.com |publisher=Bloomberg LP |accessdate=26 January 2015}}</ref><ref name=think>{{cite web |url=http://customerthink.com/lettrs_launches_platform_to_organize_and_deliver_the_world_s_letters_in_the_cloud/ |title=lettrs Launches Platform to Organize and Deliver the World’s Letters in the Cloud |author=<!--Staff writer(s); no by-line.--> |date=21 December 2012 |website=CustomerThink.com |publisher=Customer Think Corporation |accessdate=26 January 2015}}</ref> The lettrs app converts mobile voice, data and pictures to digital personal and public messages via its text and mobile delivery inventions.<ref name=mash1>{{cite web |url=http://mashable.com/2013/04/23/lettrs-mobile-app/ |title=Lettrs App Lets You Send Snail Mail From Your iPhone |last1=Petronzio |first1=Matt |date=23 April 2013 |website=Mashable.com |publisher=Mashable Inc |accessdate=26 January 2015}}</ref>

lettrs is headquartered in [[New York City]] and [[Drew Bartkiewicz]] is the company’s CEO and co-founder.<ref name=crunch></ref> In 2015, [[Mark Jung]] was named the company [[Chairman]].<ref name=trutower2>{{cite web |url=http://www.trutower.com/2015/05/06/lettrs-chat-app-chairman-woman-note-23902/ |title=lettrs Messaging App Announces New Chairman and Launch of "Woman Of Note" Collection |last1=Nay |first1=Josh Robert |date=6 May 2015 |website=trutower.com |publisher=TruTower |access-date=3 July 2015}}</ref><ref name=techco>{{cite web |url=http://tech.co/lettrs-women-of-note-2015-05 |title=lettrs: Bringing Hand Written Notes to the Digital World |last1=Schmidt |first1=Will |date=6 May 2015 |website=tech.co |publisher=TechCo |access-date=3 July 2015}}</ref> lettrs has a global user base in 174 companies,<ref name=mobile>{{cite web |url=http://www.adweek.com/socialtimes/messaging-app-lettrs-launches-socialstamps-advertisers-charities/554471?red=im |title=Messaging App Lettrs Launches SocialStamps for Advertisers, Charities |last1=Shaul |first1=Brandy |date=9 December 2014 |website=Social Times |publisher=Prometheus Global Media |accessdate=26 January 2015}}</ref> over 1 million downloads and has been featured in several media outlets, including [[The Wall Street Journal]], [[CBS]] and [[NPR]].<ref name=crunch/><ref name=postal>{{cite web |url=http://postalvision2020.com/postalvision-2020-3-0/speakers-3-0-conference/drew-bartkiewicz/ |title=Drew Bartkiewicz |author=<!--Staff writer(s); no by-line.--> |website=PostalVision2020.com |publisher=Ursa Major Associates, LLC |accessdate=26 January 2015}}</ref>

==History==
lettrs was established in 2012 by technology entrepreneur [[Drew Bartkiewicz]].<ref name=bweek/><ref name=think/><ref name=postal/> Bartkiewicz came up with the idea for the company in 2008<ref name=bweek/> after being inspired by his grandmother’s letter writing<ref name=yahoo>{{cite web |url=https://news.yahoo.com/lettrs-brings-snail-mail-back-future-230223676.html |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Yahoo News |publisher=Mashable Inc |accessdate=26 January 2015}}</ref><ref name=mash2>{{cite web |url=http://mashable.com/2012/06/19/lettrs/ |title=Lettrs Brings Snail Mail Back to The Future |last1=Van Paris |first1=Calin |date=19 June 2012 |website=Nashable |publisher=Mashable Inc |accessdate=26 January 2015}}</ref> and his own experiences during his service in the military.<ref name=bweek/><ref name=think/><ref name=parcel>{{cite web |url=http://postandparcel.info/54661/in-depth/innovation-in-depth/postalvision-sets-sights-on-congress-and-younger-americans/ |title=PostalVision sets sights on Congress and younger Americans |author=<!--Staff writer(s); no by-line.--> |date=27 March 2013 |website=PostandParcel.info |publisher=Post & Parcel |accessdate=26 January 2015}}</ref> lettrs was officially established the summer of 2012 with the help of his wife, Araceli Bartkiewicz, and children,<ref name=postal/> though it was not launched as a global platform from its [[Software release life cycle#Beta|beta]] phase until December 2012.<ref name=think/><ref name=nextweb>{{cite web |url=http://thenextweb.com/apps/2013/04/23/word-up-lettrs-launches-on-ios/ |title=TNW Pick of the Day: Lettrs turns your iPhone into a personal writing desk, transcriber and post office |last1=Sawers |first1=Paul |date=23 April 2013 |website=The Next Web |publisher=The Next Web, Inc |accessdate=26 January 2015}}</ref>

Bartkiewicz introduced the lettrs mobile application at the PostalVision 2020/3.0 conference in [[Washington, D.C.]] in 2013.<ref name=postal/><ref name=mash1/><ref name=nextweb/> The Android version was released in July 2014,<ref name=android>{{cite web |url=http://www.androidcentral.com/lettrs-app-comes-android-more-personal-messages |title=The lettrs app comes to Android for more personal messages |last1=Callaham |first1=John |date=14 June 2014 |website=AndroidCentral.com |publisher=Mobile Nations |accessdate=26 January 2015}}</ref> followed by a re-release of the iOS app in October.<ref name=apple>{{cite web |url=http://www.prweb.com/releases/2014/10/prweb12261924.htm |title=lettrs Raises $1.5M, Releases First Native iPad and Popular New iPhone App |author=<!--Staff writer(s); no by-line.--> |date=21 October 2014 |website=PRWeb.com |publisher=Vocus PRW Holdings, LLC |accessdate=26 January 2015}}</ref>

==Features==
lettrs provides a [[mobile phone|mobile]] platform for customers to create and deliver mobile letters in 80 translated languages with a selection of writing themes, proprietary “SocialStamps” and styles.<ref name=think/><ref name=mnn>{{cite web |url=http://www.mnn.com/lifestyle/responsible-living/blogs/miss-sending-letters-try-lettrs |title=Miss sending letters? Try lettrs! |last1=Vartan |first1=Starre |date=8 February 2013 |website=Mother Nature Network |publisher=MNN Holding Company, LLC |accessdate=26 January 2015}}</ref> It facilitates both private messaging and public posting of signed, translated and networked mobile-to-mobile letters.<ref name=crunch/>

The signature service of lettrs is the translation of letter messages in real time complete with original user signatures and selectable SocialStamps. The lettrs mobile network is able to translate an original digital letter in up to 80 languages. Users may also share open letters and the lettrs stamps across other major social networks.<ref name=bweek/><ref name=mash1/><ref name=yahoo/><ref name=parcel/>

In December 2014 the company introduced a feature named SocialStamps that allows users to add a customized stamp to a letter. At the feature’s launch, the company offered 47 different stamps with plans to issue new stamps monthly. As part of the release the lettrs 2014 Persons of Note stamps on the lettrs network featured [[Michelle Phan]], [[Narendra Modi]], [[Bob Woodruff]] of [[ABC News]] and [[Stanley A. McChrystal]].<ref name=mobile/><ref name=stamps>{{cite web |url=http://venturebeat.com/2014/12/09/lettrs-com-calls-postage-stamps-into-social-duty-for-its-old-style-letters/ |title=Lettrs calls postage stamps into social duty for its old-style letters |last1=Levine |first1=Barry |date=9 December 2014 |website=VentureBeat.com |publisher=VentureBeat |accessdate=26 January 2015}}</ref><ref name=marketer>{{cite web |url=http://www.mobilemarketer.com/cms/news/social-networks/19316.html |title=United Way of New York City leverages lettrs' SocialStamps for fundraising |last1=Samuely |first1=Alex |date=9 December 2014 |website=MobileMarketer.com |publisher=Napean LLC |accessdate=26 January 2015}}</ref>

Users can share letters and the SocialStamps via [[Facebook]] and [[Twitter]].<ref name=think/><ref name=nextweb/> lettrs also integrates with [[Google+]] and [[Instagram]] so that users may broaden the distribution of their letters beyond the mobile app.<ref name=bweek/> Users can also pen open public letters or petitions for supporting causes, persons, or brands.<ref name=think/><ref name=nextweb/>

lettrs conducted its first Hollywood movie integration in April 2015 with Relativity Media. The company released stamps featuring images from the movie ''[[Desert Dancer]]''.<ref name=trutower/> In May 2015, lettrs released the "Women of Note" stamp collection. It featured 12 notable women including [[Michelle Obama]], [[Queen Rania of Jordan]], [[Shakira]], [[Michelle Bachelet]], [[Laura Bush]], [[Sonia Gandhi]], [[Ellen DeGeneres]] and [[Angelina Jolie]].<ref name=trutower2/><ref name=techco/>

==Recognition and partnerships==
In 2014, Google selected lettrs as one of the Best Android Apps of the year.<ref name=time>{{cite web |url=http://time.com/3611709/best-android-google-play-apps-2014/ |title=Google Says These Are 2014's Best Android Apps |last1=Luckerson |first1=Victor |date=1 December 2014 |website=Time.com |publisher=Time Inc |accessdate=1 February 2015}}</ref><ref name=techco/>

lettrs has worked with the [[United Service Organizations|USO]], [[Aspen Institute]], and the [[United Way]].<ref name=trutower2/> In 2014, the company published the first digitally sourced book of letters, ''Poetguese''. The book contains a foreword by author [[Paulo Coelho]] with all proceeds donated to charity.<ref name=broadway>{{cite web |url=http://www.broadwayworld.com/bwwbooks/article/Lettrs-Announces-POETGUESE-20141008 |title=lettrs Announces Poetguese |author=<!--Staff writer(s); no by-line.--> |date=8 October 2014 |website=broadwayworld.com |publisher=Wisdom Digital Media |access-date=3 July 2015}}</ref>

lettrs established lettrs Foundation, an organization that partners with schools and non-profits to improve literacy through social networking, including partnerships with the United Way and Aspen Institute.<ref name=trutower>{{cite web |url=http://www.trutower.com/2014/07/14/lettrs-letter-writing-messaging-app-launch-on-android/ |title=lettrs Platform Launches on Android, Bringing Handwritten Letters Back to the Mainstream |last1=Nay |first1=Josh Robert |date=14 July 2014 |website=TruTower.com |publisher=TruTower |accessdate=1 February 2015}}</ref>

==References==
{{reflist}}

==External links==
*{{Official site|http://about.lettrs.com}}
*[http://www.lettrsfoundation.org lettrs Foundation]

[[Category:Mobile social software]]
[[Category:Postal system]]
[[Category:Postal services]]
[[Category:Internet terminology]]
[[Category:Electronic documents]]
<=====doc_Id=====>:412
<=====title=====>:
Category:Eprint archives
<=====text=====>:
An [[eprint]] archive contains electronic copies of [[Academic journal|research paper]]s (which can be closed or open).

{{cat see also|Open-access archives|Bibliographic databases and indexes}}

== External links ==
[http://roar.eprints.org/ Registry of Open Access Repositories] (ROAR)

{{DEFAULTSORT:Eprint archives}}
[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Lists of publications in science]]
[[Category:Online archives]]
[[Category:Digital libraries]]
[[Category:Full text scholarly online databases]]
<=====doc_Id=====>:415
<=====title=====>:
Comparison of e-book formats
<=====text=====>:
The following is a '''comparison of e-book formats''' used to create and publish [[e-book]]s.

The [[EPUB]] format is the most widely supported vendor-independent [[XML]]-based (as opposed to [[Portable Document Format|PDF]]) e-book format; that is, it is supported by the largest number of e-Readers, including [[Kindle Fire|Amazon Kindle Fire]] (but not standard Kindle).<ref name="kdp.amazon.com">{{cite web|url=https://kdp.amazon.com/help?topicId=A2GF0UFHIYG9VQ |title=Amazon Kindle Direct Publishing: Get help with self-publishing your book to Amazon's Kindle Store |publisher=Kdp.amazon.com |date= |accessdate=2015-08-31}}</ref> See table below for details. 
{{TOC right}}

==Format descriptions==
Formats available include, but are not limited to:

===Broadband eBooks (BBeB) ===
{{main article |BBeB}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Sony media
|-
| style="background:#ddd;"| ''Published as'':
| .lrf; .lrx
|}
The digital book format originally used by [[Sony Corporation]].  It is a proprietary format, but some reader software for general-purpose computers, particularly under [[GNU Project|GNU]]/Linux (for example, [[Calibre (software)|Calibre]]'s internal viewer<ref>{{Citation | title = About | url = http://calibre-ebook.com/about | publisher = Calibre}}</ref>), have the capability to read it.  The LRX file extension represents a [[Digital rights management|DRM]] encrypted eBook. More recently, Sony has converted its books from BBeB to EPUB and is now issuing new titles in EPUB.

===Comic Book Archive file ===
{{main article|Comic book archive}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| compressed images
|-
| style="background:#ddd;"| ''Published as'':
|.cbr (RAR); .cbz (ZIP); .cb7 (7z); .cbt (TAR); .cba (ACE)
|}

===Compiled HTML ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[Microsoft Compiled HTML Help]]
|-
| style="background:#ddd;"| ''Published as'':
| .chm
|}
CHM format is a proprietary format based on HTML. Multiple pages and embedded graphics are distributed along with [[metadata]] as a single compressed file. The indexing is both for keywords for full text search.

===DAISY – ANSI/NISO Z39.86  ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[DAISY Digital Talking Book|DAISY]]
|-
| style="background:#ddd;"| ''Published as'':
|
|}

The Digital Accessible Information SYstem (DAISY) is an [[XML]]-based open standard maintained by the DAISY Consortium for people with [[print disabilities]].  DAISY has wide international support with features for multimedia, navigation and synchronization. A subset of the DAISY format has been adopted by law in the United States as the National Instructional Material Accessibility Standard (NIMAS), and K-12 textbooks and instructional materials are now required to be provided to students with disabilities.

DAISY is already aligned with the EPUB technical standard, and is expected to fully converge with its forthcoming EPUB3 revision.<ref>{{cite web|url=http://www.daisy.org/z3986 |title=DAISY Standard &#124; DAISY Consortium |publisher=Daisy.org |date= |accessdate=2015-08-31}}</ref>

===DjVu ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| DjVu
|-
| style="background:#ddd;"| ''Published as'':
| .[[DjVu|djvu]]
|}
DjVu is a format specialized for storing scanned documents. It includes advanced compressors optimized for low-color images, such as text documents. Individual files may contain one or more pages. DjVu files cannot be re-flowed.

The contained page images are divided in separate layers (such as multi-color, low-resolution, background layer using [[lossy compression]], and few-colors, high-resolution, tightly compressed foreground layer), each compressed in the best available method. The format is designed to decompress very quickly, even faster than vector-based formats.

The advantage of DjVu is that it is possible to take a high-resolution scan (300–400 DPI), good enough for both on-[[screen reading]] and printing, and store it very efficiently. Several dozens of 300 DPI black-and-white scans can be stored in less than a megabyte.

===DOC===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Word
|-
| style="background:#ddd;"| ''Published as'':
| .[[Doc (computing)|DOC]]
|}

[[Doc (computing)|DOC]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format is that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.

===DOCX===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Word (XML)
|-
| style="background:#ddd;"| ''Published as'':
| .[[DOCX]]
|}

[[DOCX]] is a [[document]] file format that is directly supported by few ebook readers. Its advantages as an ebook format are that it can be easily converted to other ebook formats and it can be reflowed. It can be easily edited.

=== EPUB ===
{{Main article|EPUB}}
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IDPF/EPUB
|-
| style="background:#ddd;"| ''Published as'':
| .epub
|}
[[File:EPUB logo.svg|thumb|right|150px|The EPUB logo]]
The .epub or [[OEBPS]] format is a technical standard for e-books created by the [[International Digital Publishing Forum]] (IDPF).

The EPUB format has gained some popularity as a vendor-independent XML-based e-book format. The format can be read by the [[Kobo eReader]], [[BlackBerry]] devices, Apple's [[iBooks]] app running on [[Macintosh]] computers and [[IOS (Apple)|iOS]] devices, [[Google Play|Google Books]] app running on [[Android (operating system)|Android]] and iOS devices, Barnes & Noble [[Nook]], Amazon [[Kindle Fire]],<ref name="kdp.amazon.com"/> [[Sony Reader]], [[BeBook]], [[Cybook Gen3|Bookeen Cybook Gen3 (with firmware v2 and up)]], COOL-ER, [[Adobe Digital Editions]], [[Lexcycle Stanza]], BookGlutton, AZARDI, [[FBReader]], [[Aldiko]], [[CoolReader]], [[Mantano Reader]], [[Moon+ Reader]], the [[Mozilla Firefox]] [[Add-on (Mozilla)|add-on]] [[EPUBReader]], [[Okular]] and other reading apps.

[[Adobe Digital Editions]] uses .epub format for its e-books, with [[digital rights management]] (DRM) protection provided through their proprietary ADEPT mechanism. The ADEPT framework and scripts have been reverse-engineered to circumvent this DRM system.<ref>{{cite web|author= |url=http://i-u2665-cabbages.blogspot.com/2009/02/circumventing-adobe-adept-drm-for-epub.html |title=i♥cabbages: Circumventing Adobe ADEPT DRM for EPUB |publisher=I-u2665-cabbages.blogspot.com |date=2009-02-18 |accessdate=2015-08-31}}</ref>

===eReader ===
;Formerly Palm Digital Media/Peanut Press
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Palm Media
|-
| style="background:#ddd;"| ''Published as'':
| .[[PDB (Palm OS)|pdb]]
|}

eReader is a [[freeware]] program for viewing Palm Digital Media electronic books which use the pdb format used by many Palm applications. Versions are available for [[Android (operating system)|Android]], [[BlackBerry]], [[IOS (Apple)|iOS]], [[Palm OS]] (not webOS), [[Symbian]], [[Windows Mobile]] Pocket PC/Smartphone, and [[OS X]]. The reader shows text one page at a time, as paper books do. eReader supports embedded hyperlinks and images. Additionally, the [[Lexcycle Stanza|Stanza]] application for the [[iPhone]] and [[iPod touch]] can read both [[encryption|encrypted]] and unencrypted eReader files.

The program supports features like bookmarks and footnotes, enabling the user to mark any page with a bookmark and any part of the text with a footnote-like commentary. Footnotes can later be exported as a Memo document.

On July 20, 2009, [[Barnes & Noble]] made an announcement<ref>{{cite web|url=http://www.barnesandnobleinc.com/press_releases/2009_july_20_ebookstore.html |title=Barnes & Noble Booksellers |publisher=Barnesandnobleinc.com |date=2009-07-20 |accessdate=2015-08-31}}</ref> implying that eReader would be the company's preferred format to deliver e-books. Exactly three months later, in a press release by [[Adobe Systems|Adobe]], it was revealed Barnes & Noble would be joining forces with the software company to standardize the EPUB and PDF eBook formats.<ref>{{cite press release
 | title = Barnes & Noble adopts open EPUB eBook Format, PDF and Adobe Content Server | publisher = [[Adobe Systems]] | date = 2009-10-20 | url = https://www.adobe.com/aboutadobe/pressroom/pressreleases/200910/AdobeandBarnesNobleJoinForcestoStandardizeeBookTechnology.html | accessdate = 2013-05-06}}</ref><ref>{{Citation|last=Rothman |first=David |title=‘Barnes & Noble adopts open EPUB eBook Format, PDF and Adobe Content Server’ |publisher=TeleRead |date=2009-10-20 |url=http://www.teleread.com/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130506010320/http://www.teleread.com:80/ebooks/barnes-noble-adopts-open-epub-ebook-format-pdf-and-adobe-content-server/ |archivedate=2013-05-06 |deadurl=yes |df= }}</ref> Barnes & Noble e-books are now sold mostly in EPUB format.<ref>{{Citation | last = Bell | first = Ian | title = Barnes & Noble Adopts ePub Standard; Aligns With Adobe | publisher = [[Digital Trends]] | date = 2009-11-18 | url = http://www.digitaltrends.com/gadgets/barnes-aligns-with-adobe/ | accessdate = 2013-05-06}}</ref><ref>{{Citation|last=Meadows |first=Chris |title=Barnes & Noble quietly changes e-book format, neglects to tell consumers |publisher=TeleRead |date=2009-12-13 |url=http://www.teleread.com/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |accessdate=2013-05-06 |archiveurl=https://web.archive.org/web/20130130085503/http://www.teleread.com:80/drm/barnes-noble-quietly-changes-e-book-format-neglects-to-tell-consumers/ |archivedate=2013-01-30 |deadurl=yes |df= }}</ref><ref>{{Citation | last = James | first = Kendrick | title = Has Barnes & Noble Changed Its e-Book Format to ePUB? | publisher = [[GigaOM]] | date = 2009-12-14 | url = http://gigaom.com/2009/12/14/has-barnes-noble-changed-its-e-book-format-to-epub/ | accessdate = 2013-05-06}}</ref>

===FictionBook (Fb2) ===

{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| FictionBook
|-
| style="background:#ddd;"| ''Published as'':
| .[[FictionBook|fb2]]
|}

[[FictionBook]]<ref>[http://haali.cs.msu.ru/pocketpc/FictionBook_description.html]  {{webarchive |url=https://web.archive.org/web/20070703204958/http://haali.cs.msu.ru/pocketpc/FictionBook_description.html |date=July 3, 2007 }}</ref> is a popular [[XML]]-based e-book format, supported by free readers such as [[FBReader]], [[Okular]], [[CoolReader]], [[Bebook]] and [[STDU Viewer]].

The FictionBook format does not specify the appearance of a document; instead, it describes its structure and semantics. All the ebook metadata, such as the author name, title, and publisher, is also present in the ebook file. Hence the format is convenient for automatic processing, indexing, and ebook collection management. This also is convenient to store books in it for later automatic conversion into other formats.

===Founder Electronics ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Apabi Reader
|-
| style="background:#ddd;"| ''Published as'':
| .[[XEB|xeb]]; .ceb
|}
[[APABI]] is a format devised by [[Founder Electronics]]. It is a popular format for Chinese e-books. It can be read using the [[Apabi Reader]] software, and produced using [[Apabi Publisher]]. Both .xeb and .ceb files are encoded binary files. The [[iLiad (E-book Reader)|Iliad]] e-book device includes an Apabi 'viewer'.

===Hypertext Markup Language ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Hypertext
|-
| style="background:#ddd;"| ''Published as'':
| .htm; .html and typically auxiliary images, js and css
|}
[[HTML]] is the [[markup language]] used for most [[World Wide Web|web]] pages. E-books using HTML can be read using a [[Web browser]]. The specifications for the format are  available without charge from the [[W3C]].

HTML adds specially marked meta-elements to otherwise plain text encoded using [[character set]]s like [[ASCII]] or [[UTF-8]]. As such, suitably formatted files can be, and sometimes are, generated ''by hand'' using a ''[[text editor|plain text editor]]'' or ''[[Source code editor|programmer's editor]]''. Many ''HTML generator'' applications exist to ease this process and often require less intricate knowledge of the format details involved.

HTML on its own is not a particularly efficient format to store information in, requiring more storage space for a given work than many other formats. However, several e-Book formats including the Amazon Kindle, Open eBook, Compiled HTML,  Mobipocket and EPUB store each book chapter in HTML format, then use [[ZIP (file format)|ZIP]] compression to compress the HTML data, images, metadata and style sheets into a single, significantly smaller, file.

HTML files encompass a wide range of standards<ref>{{cite web|url=http://www.webstandards.org/learn/faq/ |title=Frequently Asked Questions (FAQ) - The Web Standards Project |publisher=Webstandards.org |date=2002-02-27 |accessdate=2015-08-31}}</ref> and displaying HTML files correctly can be complicated. Additionally many of the features supported, such as forms, are not relevant to e-books.

===iBook (Apple) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| iBook
|-
| style="background:#ddd;"| ''Published as'':
| .ibooks
|}
The .ibooks format is created with the free [[iBooks Author]] ebook layout software from [[Apple Inc.]]. This proprietary format is based on the [[EPUB]] standard, with some differences in the CSS tags used in an ibooks format file, thus making it incompatible with the EPUB specification. The End-User Licensing Agreement (EULA) that comes with iBooks Author states that "If you want to charge a fee for a work that includes files in the .ibooks format generated using iBooks Author, you may only sell or distribute such work through Apple". The "through Apple" will typically be in the Apple [[iBooks]] store. The EULA further states that "This restriction does not apply to the content of such works when distributed in a form that does not include files in the .ibooks format." Therefore, Apple has not included distribution restrictions in the iBooks Author EULA for ibooks format ebooks created in iBooks Author that are made available for free, and it does not prevent authors from re-purposing the content in other ebook formats to be sold outside the iBookstore. This software currently supports import and export functionally for three formats. ibook, Plain text and PDF. The iBooks Author 2.3 and later supports importing EPUB and export EPUB 3.0.<ref>{{Cite web|url=https://support.apple.com/en-us/HT204884|title=About ePubs created with iBooks Author|language=en-US|access-date=2016-09-25}}</ref>

===IEC 62448===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IEC 62448
|-
| style="background:#ddd;"| ''Published as'':
|
|}

IEC 62448 is an international standard created by [[International Electrotechnical Commission]] (IEC), Technical Committee 100, Technical Area 10 (Multimedia e-publishing and e-book).

The current version of IEC 62448 is an umbrella standard that contains as appendices two concrete formats, XMDF of Sharp and BBeB of Sony. However, BBeB has been discontinued by Sony and the version of XMDF that is in the specification is out of date. The IEC TA10 group is discussing the next steps, and has invited the IDPF organization which has standardized [[EPUB]] to be a liaison. It is possible that the current version of EPUB and/or the forthcoming EPUB3 revision may be added to IEC 62448.  Meanwhile, a number of Japanese companies have proposed that IEC standardize a proposed new Japanese-centric file format that is expected to unify DotBook of Voyager Japan and XMDF of Sharp.  This new format has not been publicly disclosed as of November 2010 but it is supposed to cover basic representations for the Japanese language.  Technically speaking, this revision is supposed to provide a Japanese minimum set, a Japanese extension set, and a stylesheet language. These issues were discussed in the TC100 meeting held  in October 2010 but no decisions were taken besides offering the liaison status to IDPF.

===INF (IBM) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| IBM & Open Source
|-
| style="background:#ddd;"| ''Published as'':
| .inf
|}
[[IBM]] created this e-book format and used it extensively for [[OS/2]] and other of its operating systems. The INF files were often digital versions of printed books that came with some bundles of OS/2 and other products. There were many other newsletters and monthly publications (e.g.: EDM/2) available in the INF format too.

The advantage of INF is that it is very compact and very fast. It also supports images, reflowed text, tables and various list formats. INF files get generated by compiling the markup text files — in the [[Information Presentation Facility]] (IPF) format — into binary files.

Originally only IBM created an INF viewer and compiler, but later open source viewers like NewView, DocView and others appeared. There is also an open source IPF compiler named WIPFC, created by the [[Open Watcom]] project.

===KF8 (Amazon Kindle) ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Kindle
|-
| style="background:#ddd;"| ''Published as'':
| .azw3; .azw; .kf8
|}
With the release of the [[Kindle Fire]] reader in late 2011, [[Amazon.com]] also released [[Kindle Format 8]], their newest file format, also known as .AZW3. The .azw3 file format supports a subset of [[HTML5]] and [[CSS3]] features, with some additional nonstandard features; the new data is stored within a container which can also be used to store a Mobi content document, allowing limited backwards compatibility.<ref>{{cite web|url=http://www.amazon.com/gp/feature.html?docId=1000729511|title=Kindle Format 8 Overview|publisher=Amazon.com|year=2012}}</ref><ref>{{cite web|url=http://musingsandmarvels.com/2012/03/06/the-new-kindle-format-8-kf8/|title=The New Kindle Format KF8|publisher=Musings and Marvels:Learning the ins and outs of the publishing industry|date=2012-03-06|accessdate=2012-03-16}}</ref><ref>{{cite web|url=http://www.amazon.com/gp/feature.html/ref=amb_link_357613502_6?ie=UTF8&docId=1000729901&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=right-4&pf_rd_r=0GN9VRRB0NJ08VXGFKWK&pf_rd_t=1401&pf_rd_p=1343256942&pf_rd_i=1000729511|title=HTML5 tags supported by KF8|publisher=Amazon.com|accessdate=2012-03-16}}</ref>

Older [[Amazon Kindle|Kindle]] e-readers use the proprietary format, AZW. It is based on the [[Mobipocket]] standard, with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]) and its own [[Digital rights management|DRM]] formatting. Because the ebooks bought on the Kindle are delivered over its wireless system called Whispernet, the user does not see the AZW files during the download process. The Kindle format is available on a variety of platforms, such as through the Kindle app for the various mobile device platforms.

===Microsoft LIT ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Microsoft Reader
|-
| style="background:#ddd;"| ''Published as'':
| .[[LIT (file format)|lit]]
|}
DRM-protected LIT files are only readable in the proprietary [[Microsoft Reader]] program, as the .LIT format, otherwise similar to Microsoft's [[Microsoft Compiled HTML Help|CHM]] format, includes [[Digital Rights Management]] features. Other third party readers, such as [[Lexcycle Stanza]], can read unprotected LIT files.

The Microsoft Reader uses patented [[ClearType]] display technology. In Reader navigation works with a keyboard, mouse, stylus, or through electronic bookmarks. The Catalog Library records reader books in a personalized "home page", and books are displayed with ClearType to improve readability.  A user can add annotations and notes to any page, create large-print e-books with a single command, or create free-form drawings on the reader pages.  A built-in dictionary allows the user to look up words.

In August 2011, Microsoft announced they were discontinuing both Microsoft Reader and the use of the .lit format for ebooks<ref>{{cite web|url=http://aazae.com/|title=Ebooks|work=Aazae}}</ref> at the end of August 2012, and ending sales of the format on November 8, 2011.<ref>"Microsoft is discontinuing Microsoft Reader effective August 30, 2012, which includes download access of the Microsoft Reader application from the Microsoft Reader website."[http://www.microsoft.com/reader/ Microsoft Reader] {{webarchive |url=https://web.archive.org/web/20050822035209/http://www.microsoft.com/reader/ |date=August 22, 2005 }}</ref>

===Mobipocket ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Mobipocket
|-
| style="background:#ddd;"| ''Published as'':
| [[PRC (Palm OS)|.prc]]; .mobi
|}
The [[Mobipocket]] e-book format is based on the [[Open eBook]] standard using [[XHTML]] and can include [[JavaScript]] and frames. It also supports native [[SQL]] queries to be used with embedded databases. There is a corresponding e-book reader.

The [[Mobipocket]] Reader has a home page library. Readers can add blank pages in any part of a book and add free-hand drawings. Annotations – highlights, bookmarks, corrections, notes, and drawings – can be applied, organized, and recalled from a single location. Images are converted to GIF format and have a maximum size of 64K,<ref>{{cite web|url=http://www.mobipocket.com/dev/article.asp?BaseFolder=creatorhome&File=image.htm |title=Mobipocket Developer Center - Importing Image files |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}</ref> sufficient for mobile phones with small screens, but rather restrictive for newer gadgets. [[Mobipocket]] Reader has electronic bookmarks,  and a built-in dictionary.

The reader has a full screen mode for reading and support for many [[Personal digital assistant|PDAs]], [[Personal digital assistant|Communicators]], and [[Smartphone]]s. [[Mobipocket]] products support most Windows, Symbian, BlackBerry and Palm operating systems, but not the Android platform. Using WINE, the reader works under Linux or Mac OS X. Third-party applications like [[Okular]] and [[FBReader]] can also be used under Linux or Mac OS X, but they work only with unencrypted files.

The Amazon Kindle's AZW format is basically just the Mobipocket format with a slightly different serial number scheme (it uses an [[asterisk]] instead of a [[dollar sign]]), and .prc publications can be read directly on the Kindle.  The Kindle AZW format also lacks some Mobipocket features such as JavaScript.<ref>{{cite web|url=http://www.mobileread.com/forums/showpost.php?p=1299906&postcount=2 |title=MobileRead Forums - View Single Post - Javascript in mobi ebooks? |publisher=Mobileread.com |date=2010-12-29 |accessdate=2015-08-31}}</ref>

[[Amazon.com|Amazon]] has developed an .epub to .mobi converter called KindleGen,<ref>{{cite web|url=http://www.mobipocket.com/dev/ |title=Mobipocket Developer Center |publisher=Mobipocket.com |date= |accessdate=2015-08-31}}</ref> and it supports IDPF 1.0 and IDPF 2.0 EPUB format.

===Multimedia eBooks ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Eveda
|-
| style="background:#ddd;"| ''Published as'':
| .exe or .html
|}
A [[multimedia ebook]] is [[media (communication)|media]] and [[book]] [[content (media and publishing)|content]] that utilizes a combination of different book [[content format]]s. The term can be used as a noun (a medium with multiple content formats) or as an adjective describing a medium as having multiple content formats.

The "multimedia ebook" term is used in contrast to media which only utilize traditional forms of printed or text books. Multimedia ebooks include a combination of [[Written language|text]], [[Audio file format|audio]], [[image]]s, [[video]], or [[interactive]] content formats.  Much like how a traditional book can contain images to help the text tell a story, a multimedia ebook can contain other elements not formerly possible to help tell the story.

With the advent of more widespread tablet-like computers, such as the [[smartphone]], some publishing houses are planning to make multimedia ebooks, such as Penguin.<ref>[http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ ] {{webarchive |url=https://web.archive.org/web/20100617170741/http://paidcontent.co.uk/article/419-first-look-how-penguin-will-reinvent-books-with-ipad/ |date=June 17, 2010 }}</ref>

===Newton eBook ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Newton eBook
|-
| style="background:#ddd;"| ''Published as'':
| .pkg
|}
Commonly known as an [[Apple Newton]] book; a single Newton package file can contain multiple books (for example, the three books of a trilogy might be packaged together). All systems running the Newton operating system (the most common include the Newton MessagePads, eMates, Siemens Secretary Stations, Motorola Marcos, Digital Ocean Seahorses and Tarpons) have built-in support for viewing Newton books. The Newton package format was released to the public by Newton, Inc. prior to that company's absorption into Apple Computer. The format is thus arguably open and various people have written readers for it (writing a Newton book converter has even been assigned as a university-level class project<ref>{{cite web|url=http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |accessdate=July 6, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20060904191234/http://metcs.bu.edu/~feneric/cs331/Archives/Project2002/ |archivedate=September 4, 2006 }}</ref>).

Newton books have no support for DRM or encryption. They do support internal links, potentially multiple tables of contents and indexes, embedded gray scale images, and even some scripting capability (for example, it's possible to make a book in which the reader can influence the outcome).<ref>{{cite web|url=http://tools.unna.org/wikiwikinewt/index.php/MakeNewtonEbooksIndex |title=WikiWikiNewt Undergoing Maintenance |publisher=Tools.unna.org |date= |accessdate=2015-08-31}}</ref> Newton books utilize [[Unicode]] and are thus available in numerous languages. An individual [[Newton book]] may actually contain multiple views representing the same content in different ways (such as for different screen resolutions).

===Open Electronic Package===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Open eBook
|-
| style="background:#ddd;"| ''Published as'':
| .opf
|}

[[Open eBook|OPF]] is an [[XML]]-based e-book format created by E-Book Systems; it has been superseded by the EPUB electronic publication standard.

===Portable Document Format ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Portable Document Format
|-
| style="background:#ddd;"| ''Published as'':
| .[[Portable Document Format|pdf]]
|}

Invented by [[Adobe Systems]], and first released in 1993, [[PDF]] became ISO 32000 in 2008. The format was developed to provide a platform-independent means of exchanging fixed-layout documents. Derived from [[PostScript]], but without language features like loops, PDF adds support for features such as compression, passwords, semantic structures and DRM. Because PDF documents can easily be viewed and printed by users on a variety of computer [[Platform (computing)|platforms]], they are very common on the [[World Wide Web]] and in document management systems worldwide. The current PDF specification, ISO 32000-1:2008, is available from ISO's website, and under special arrangement, without charge from Adobe.<ref>{{cite web|url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=PDF Reference and Adobe Extensions to the PDF Specification &#124; Adobe Developer Connection |publisher=Adobe.com |date=2007-01-29 |accessdate=2015-08-31}}</ref>

Because the format is designed to reproduce fixed-layout pages, re-flowing text to fit mobile device and e-book reader screens has traditionally been problematic. This limitation was addressed in 2001 with the release of PDF Reference 1.5 and "Tagged PDF",<ref>{{cite web|author= |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 |title=What is Tagged PDF? |publisher=Planet PDF |date= |accessdate=2015-08-31}}</ref> but 3rd party support for this feature was limited until the release of [[PDF/UA]] in 2012.

Many products support creating and reading PDF files, such as Adobe Acrobat, [[PDFCreator]] and [[OpenOffice.org]], and several programming libraries such as [[iText]] and [[Formatting Objects Processor|FOP]]. Third party viewers such as [[xpdf]] and [[Nitro PDF]] are also available. Mac OS X has built-in PDF support, both for creation as part of the printing system and for display using the built-in Preview application.

PDF files are supported by almost all modern e-book readers, tablets and smartphones. However, PDF reflow based on Tagged PDF, as opposed to re-flow based on the actual sequence of objects in the content-stream, is not yet commonly supported on mobile devices. Such Re-flow options as may exist are usually found under "view" options, and may be called "word-wrap".

===Plain text files ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| text
|-
| style="background:#ddd;"| ''Published as'':
| .txt
|}
The first e-books in history were in [[text file|plain text]] (.txt) format, supplied for free by the [[Project Gutenberg]] community, but the format itself existed before the e-book era. The plain text format doesn't support digital rights management (DRM) or formatting options (such as different fonts, graphics or colors), but it has excellent portability as it is the simplest e-book encoding possible as a plain text file contains only [[ASCII]] or [[Unicode]] text (text files with [[UTF-8]] or [[UTF-16]] encoding are also popular for languages other than English). Almost all operating systems can read ASCII text files (e.g. Unix, Macintosh, Microsoft Windows, DOS and other systems) and newer operating systems support Unicode text files as well. The only potential for portability problems of ASCII text files is that operating systems differ in their preferred line ending convention and their interpretation of values outside the ASCII range (their character encoding). Conversion of files from one to another line-ending convention is easy with free software. DOS and Windows uses CRLF, Unix and Apple's OS X use LF, Mac OS up to and including OS 9 uses CR. By convention, lines are often broken to fit into 80 characters, a legacy of older terminals and consoles. Alternately, each paragraph may be a single line.

The size in bytes of a text file is simply the number of characters, including spaces, and with a new line counting for 1 or 2. For example, the [[Bible]], which is approximately 800,000 words, is about 4 MB.<ref name="bible">{{cite web|url=http://www.gutenberg.org/ebooks/10 |accessdate=January 10, 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20081205071232/http://www.gutenberg.org/ebooks/10 |archivedate=December 5, 2008 }}</ref>

===Plucker ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Plucker
|-
| style="background:#ddd;"| ''Published as'':
|.pdb
|}
[[Plucker]] is an Open Source [[free software|free]] mobile and desktop e-book reader application with its own associated file format and software to automatically generate Plucker files from text, PDF, HTML, or other document format files, web sites or RSS feeds.  The format is public and well-documented. Free readers are available for all kinds of desktop computers and many PDAs.

===PostScript ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| PostScript
|-
| style="background:#ddd;"| ''Published as'':
| .[[PostScript|ps]]
|}
[[PostScript]] is a [[page description language]] used in the electronic and [[desktop publishing]] areas for defining the contents and layout of a printed page, which can be used by a rendering program to assemble and create the actual output [[Raster graphics|bitmap]]. Many office printers directly support interpreting PostScript and printing the result. As a result, the format also sees wide use in the [[Unix]] world.<!-- IE if you don't want to fool around with output filters, ghostscript, and whatnot, get a postscript printer. Most Unix programs with specialized ``print'' functions output ps anyway (pity the firefox print renderer sucks so much). Don't see a way to comment on that here so left in a comment. Would be nice if (the higher end) ebook readers would add a ps interpreter, though -->

===RTF===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| Rich Text Format
|-
| style="background:#ddd;"| ''Published as'':
| .[[Rich Text Format|rtf]]
|}

[[Rich Text Format]] is a [[document]] file format that is supported by many ebook readers. Its advantages as an ebook format is that it is widely supported, and it can be reflowed. It can be easily edited. It can be easily converted to other ebook formats, increasing its support.

===SSReader ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| SSReader
|-
| style="background:#ddd;"| ''Published as'':
| .pdg
|}
The digital book format used by a popular digital library company 超星数字图书馆<ref>[http://www.ssreader.com/downland_index.asp ]{{dead link|date=August 2015}}</ref> in China.  It is a proprietary raster image compression and binding format, with reading time OCR plug-in modules.  The company scanned a huge number of Chinese books in the China National Library and this becomes the major stock of their service.  The detailed format is not published.  There are also some other commercial e-book formats used in Chinese digital libraries.

===Text Encoding Initiative ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| [[TEI Lite]]
|-
| style="background:#ddd;"| ''Published as'':
| .xml{{Citation needed|date=August 2009}}
|}
[[TEI Lite]] is the most{{Citation needed|date=September 2010}} popular of the [[Text Encoding Initiative|TEI]]-based (and thus [[XML]]-based or [[Standard Generalized Markup Language|SGML]]-based) electronic text formats.

===TomeRaider ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| TomeRaider
|-
| style="background:#ddd;"| ''Published as'':
| .tr2; .tr3
|}

The [[TomeRaider]] e-book format is a proprietary format. There are versions of [[TomeRaider]] for Windows, Windows Mobile (aka Pocket PC), Palm, Symbian and iPhone. Several Wikipedias are available as [[Wikipedia:TomeRaider database|TomeRaider files]] with all articles unabridged, some even with nearly all images. Capabilities of the TomeRaider3 e-book reader vary considerably per platform: the Windows and Windows Mobile editions support full [[HTML]] and [[CSS]]. The Palm edition supports limited HTML (e.g., no tables, no fonts), and CSS support is missing. For Symbian there is only the older TomeRaider2  format, which does not render images or offer category search facilities. Despite these differences any TomeRaider e-book can be browsed on all supported platforms.  The Tomeraider website<ref name="tomeraider.com">{{cite web|url=http://www.tomeraider.com/ |title=tomeraider.com |publisher=tomeraider.com |date=2015-06-24 |accessdate=2015-08-31}}</ref> claims to have over 4000 e-books available, including free versions of the [[Internet Movie Database]] and Wikipedia.

===Open XML Paper Specification ===
{| style="text-align:left;"
|-
| style="background:#ddd; width:100px;"| ''Format'':
| OpenXPS
|-
| style="background:#ddd;"| ''Published as'':
| [[Open XML Paper Specification|.oxps, .xps]]
|}

'''Open XML Paper Specification''' (also referred to as '''OpenXPS''') is an open [[specification]] for a [[page description language]] and a fixed-document format. [[Microsoft]] developed it as the XML Paper Specification (XPS). In June 2009, [[Ecma International]] adopted it as international standard '''ECMA-388'''.<ref>{{cite web|url=http://www.ecma-international.org/publications/standards/Ecma-388.htm |title=Standard ECMA-388 |publisher=Ecma-international.org |date= |accessdate=2015-08-31}}</ref>

The format is intentionally restricted to sequences of:
Glyphs (a fixed run of text),
Paths (a geometry that can be filled, or stroked, by a brush), and
Brushes (a description of a shaped brush used to in rendering paths).

This reduces the possibility of inadvertent introduction of malicious content and simplifies the implementation of compatible renderers.

== Comparison tables ==

=== Features ===
{| class="wikitable sortable" style="text-align: center; width:75%;"
|-
! '''Format'''
! [[Filename extension]]
! DRM support
! Image support
! Table support
! Sound support
! Interactivity support
! [[Word wrap]] support
! [[Open standard|Open]] [[Open standard|standard]]
! Embedded annotation support
! Book- marking
! Video support
|-
| [[Comic Book Archive]]
| .cbr, .cbz, .cb7, .cbt, .cba
| ?
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
|-
| [[DjVu]]
| .djvu
| ?
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Doc (computing)|DOC]]
| .doc
| ?
| {{yes}}
| {{yes}}
| ?
| ?
| {{yes}}
| {{no}}
| ?
| ?
| ?
|-
| [[DOCX]]
| .docx
| ?
| {{yes}}
| {{yes}}
| {{yes}}
| ?
| {{yes}}
| {{no}}
| ?
| ?
| {{yes}}
|-
| [[EPUB]] (IDPF)
| .epub
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>
| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>
| {{yes}}<ref group="f">With ePub 3</ref>
|-
| [[FictionBook]]
| .fb2
| {{no}}
| {{yes}}
| {{yes-no}}<ref group="f">Table support added in FictionBook V2.1. Not supported in V2.0</ref>
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
| ?
|-
| [[HTML]]
| .html
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}<ref group="f" name="html5">With HTML 5</ref>
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}<ref group="f" name="html5" />
|-
| [[iBooks]]
| .ibook
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
|-
| [[Information Presentation Facility|INF]]
| .inf
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| ?
| {{yes}}
| {{yes}}
| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>
| {{yes-no}}<ref name="depreader" group="f">Depends on the eReader application</ref>
| {{no}}
|-
| [[Amazon Kindle|Kindle]]
| .azw
| {{yes}}
| {{yes}}
| {{yes}}<ref group="f">Supported in all except 1st Generation Kindle. (Support level is as it is in mobipocket)</ref><ref>{{cite web|author=Joshua Tallent |url=http://kindleformatting.com/blog/2009/02/kindle-2-review-formatting-perspective.php |title=Kindle 2 Review, the Formatting Perspective |publisher=Kindle Formatting |date=2009-02-25 |accessdate=2015-08-31}}</ref>
| {{yes}}<ref group="f" name="iOS">Supported only in kindle for iPhone, iPod, iPad.</ref><ref name="amazon.com">{{cite web|url=http://www.amazon.com/b?ie=UTF8&node=2248263011 |title=Kindle Editions with Audio-Video: Kindle Store |publisher=Amazon.com |date= |accessdate=2015-08-31}}</ref>
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}<ref group="f" name="iOS"/><ref name="amazon.com"/>
|-
| [[Microsoft Reader]]
| .lit
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| ?
| {{yes}}
| ?
|-
| [[Mobipocket]]
| .prc, .mobi
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Multimedia EBook]]
| .exe
| {{yes}}
| {{yes}}
| ?
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Newton Book]]
| .pkg
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
|-
| [[#eReader|eReader]]
| .pdb
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| ?
|-
| [[Plain text]]
| .txt
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
|-
| [[Plucker]]
| .pdb
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| ?
|-
| [[Portable Document Format]]
| .pdf
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes-no}}<ref group="f">"Reflow" is implemented by some readers.</ref><ref>{{cite web|url=https://www.adobe.com/uk/epaper/tips/acr5reflow/ |title=Reflow the contents of Adobe PDF documents: Tutorial |publisher=Adobe.com |date=2001-04-02 |accessdate=2015-08-31}}</ref>
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}<ref group="f">With Flash Embeded</ref>
|-
| [[PostScript]]
| .ps
| {{no}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| ?
| ?
| ?
|-
| [[Tome Raider]]
| .tr2, .tr3
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| ?
| ?
| ?
|-
| [[OpenXPS]]
| .oxps, .xps
| ?
| {{yes}}
| {{yes}}
| ?
| {{no}}
| {{no}}
| {{yes}}
| ?
| ?
| ?
|}
<references group="f"/>

=== Supporting platforms ===
{| class="wikitable sortable" style="text-align: center; width:75%;"
|-
! '''Reader&nbsp;'''
! Plain text
! PDF
! ePub
! HTML
! Mobi- Pocket
! Fiction- Book (Fb2)
! DjVu
! Broadband eBook (BBeB)<ref group=h name=propr>Proprietary format</ref>
! eReader<ref group=h name=propr/>
! Kindle<ref group=h name=propr/>
! WOLF<ref group=h name=propr/>
! Tome Raider<ref group=h name=propr/>
! Open eBook<ref group=h>Predecessor of ePUB</ref>
! Comic Book
! OpenXPS
|-
| Amazon Kindle&nbsp;1
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Amazon Kindle&nbsp;2,&nbsp;DX
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Amazon Kindle]]&nbsp;3
| {{yes}}
| {{yes}}
| {{no}}<ref group=h name=3part>Yes, if the Duokan alternate Kindle OS (third-party software add-on) is used.</ref>
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Kindle Fire|Amazon Kindle Fire]]
| {{yes}}
| {{yes}}
| {{yes}}<ref group=h>By adding epub capable apps, such as [[Aldiko]]</ref>
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Android Devices
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}<ref group=h name=firm>Requires latest firmware</ref><ref>{{cite web|url=http://ireader.over-blog.com/ |title=iReader |publisher=Ireader.over-blog.com |date= |accessdate=2015-08-31}}</ref>
| {{yes}}
| {{yes}}<ref group=h name=firm/><ref>{{cite web|url=https://code.google.com/p/vudroid/ |title=vudroid - Android djvu and pdf viewer - Google Project Hosting |publisher=Code.google.com |date= |accessdate=2015-08-31}}</ref>
| {{no}}
| {{yes}}<ref group=h name=firm/><ref>{{cite web|author= |url=http://www.barnesandnoble.com/u/nook-for-android/379002287 |title=Rise of the Android by Apps for Nook &#124; 2940147132807 &#124; NOOK App &#124; Barnes & Noble |publisher=Barnesandnoble.com |date= |accessdate=2015-08-31}}</ref>
| {{yes}}
| {{no}}
| {{yes}}<ref group=h name=firm/><ref name="tomeraider.com"/>
| {{yes}}<ref group=h name=firm/>
| {{dunno}}
| {{yes}}
|-
| Apple iOS Devices
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}<ref group=h name=firm/>
| {{yes}}<ref group=h name=firm/>
| {{yes}}<ref group=h name=firm/>
| {{no}}
| {{yes}}<ref group=h name=firm/>
| {{yes}}<ref group=h name=firm/>
| {{no}}
| {{yes}}<ref group=h name=firm/>
| {{yes}}<ref group=h name=firm/>
| {{yes}}<ref group=h>With third party apps, such as CloudReader</ref>
| {{dunno}}
|-
| Azbooka WISEreader
| {{yes}}
| {{no}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Barnes & Noble Nook]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Barnes & Noble Nook Color]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Bookeen Cybook Gen3, Opus
| {{yes}}
| {{yes}}
| {{yes}}<ref group=h name=epmb>Versions support either ePUB or MobiPocket</ref>
| {{yes}}
| {{yes}}<ref group=h name=epmb/>
| {{yes}}<ref group=h>Only ePUB version and with FW 2.0+</ref>
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{dunno}}
| {{dunno}}
|-
| COOL-ER Classic
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Linux]] Operating System
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}<ref group=h>KDE's [[Okular]] supports fb2</ref>
| {{yes}}
| {{yes}}<ref group=h>[[Calibre (software)|Calibre]] supports lrf/lrx</ref>
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{partial|?}}
| {{yes}}
| {{yes}}
|-
| [[eSlick|Foxit eSlick]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Hanlin e-Reader&nbsp;V3
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Hanvon WISEreader
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| iRex iLiad
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Iriver Story
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}<ref group=h name=firm/>
| {{yes}}<ref group=h name=firm/>
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| [[Kobo eReader]]
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{dunno}}
|-
| Nokia N900
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{dunno}}
|-
| NUUTbook&nbsp;2
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| OLPC XO, Sugar
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Onyx Boox 60
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Mac OS X
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{dunno}}
| {{yes}}
| {{yes}}
| {{dunno}}
| {{dunno}}
| {{yes}}
| {{dunno}}
| {{dunno}}
|-
| TrekStor eBook Reader Pyrus<ref>{{cite web|url=http://www.trekstor.co.uk/detail-ebook-reader-en/product/ebook-reader-pyrus-mini.html |title=Home - SurfTabs, smart phones, MiniPCs, data storage, MP3-Player - TrekStor GmbH |publisher=Trekstor.co.uk |date= |accessdate=2015-08-31}}</ref>
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{yes}}
| {{yes}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
| {{dunno}}
|-
| Windows
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}<ref group=h>[[ICE Book Reader]] for Windows supports fb2</ref>
| {{yes}}
| {{dunno}}
| {{yes}}
| {{yes}}<ref group=h>DRM-protected publications are supported as of Kindle for PC v1.3.0</ref>
| {{dunno}}
| {{dunno}}
| {{yes}}
| {{dunno}}
| {{yes}}<ref group="h">XP or later, not on Windows 2000</ref>
|-
| Pocketbook 301&nbsp;Plus, 302, 360°
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Pocketbook Aqua
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Sony Reader
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Viewsonic VEB612
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|-
| Windows Phone 7
| {{yes}}
| {{yes}}
| {{yes}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{no}}
| {{yes}}
| {{no}}
| {{no}}
| {{no}}
| {{dunno}}
| {{dunno}}
|}
<references group=h/>

== See also ==
* [[Comparison of e-book readers]]
* [[Comparison of Android e-book reader software]] – includes software e-book readers for Android devices
* [[Comparison of iOS e-book reader software]] – includes software e-book readers for iOS devices
* [[ICUE]], a British company using mobile phone (cellphone) technology to deliver books and other publications

== References ==
;General information
{{Refbegin}}
* {{cite book|last=Cavanaugh|first=T W|title=The Digital Reader: Using E-Books in K-12 Education|year=2006|publisher=International Society for Technology in Education|location=Eugene, Oregon|isbn=1564842215}}
* {{cite book|last=Chandler|first=S|title=From Entrepreneur to Infopreneur: Make Money with Books, EBooks, and Information Products|year=2010|publisher=John Wiley & Sons|location=Hoboken, New Jersey|isbn=1118044770}}
* Cope, B., & Mason, D. (2002). Markets for electronic book products. C-2-C series, bk. 3.2. Altona, Vic: Common Ground Pub.
* {{cite book|last=Henke|first=H|title=Electronic Books and Epublishing: A Practical guide for Authors.|year=2001|publisher=Springer|location=London|isbn=1852334355}}
* Hanttula, D. (2001). Pocket PC handbook.
* {{cite book|last=Rich|first=J|title=Self-Publishing For Dummies|year=2006|publisher=John Wiley & Sons|location=Hoboken, New Jersey|isbn=0470100370}}
{{Refend}}
;Footnotes
{{Reflist|30em}}

==External links==
*[http://wiki.mobileread.com/wiki/Main_Page ebook reader articles at Mobile Read Wiki]
*[http://digbib.ubka.uni-karlsruhe.de/volltexte/1000010574 Daisy 3: A Standard for Accessible Multimedia Books]
*[https://www.eff.org/deeplinks/2009/12/e-book-privacy An E-Book Buyer's Guide to Privacy]

{{Ebooks}}

{{DEFAULTSORT:Comparison Of E-Book Formats}}
[[Category:Electronic documents]]
[[Category:Electronic publishing]]
[[Category:Computing comparisons]]
<=====doc_Id=====>:418
<=====title=====>:
Category:Customer communications management
<=====text=====>:
[[Category:Software]]
[[Category:Document management systems]]
[[Category:Electronic documents]]
[[Category:Customer relationship management software]]
[[Category:Information technology management]]
{{Commons category|Customer communications management}}
<=====doc_Id=====>:421
<=====title=====>:
Data paper
<=====text=====>:
#REDIRECT [[Data publishing#Paper]]


[[Category:Data publishing]]
[[Category:Electronic documents]]
<=====doc_Id=====>:424
<=====title=====>:
E-receipt
<=====text=====>:
An '''E-receipt''' is an electronic [[receipt]] of any goods/services that have been purchased, opposed to a paper receipt. They are usually sent via [[email]] to avoid wasting [[paper]] and for marketing purposes.<ref>{{cite news |last=Perring |first=Rebecca |url=http://www.express.co.uk/news/uk/639219/Ereceipts-British-shops-shopping-electronic-Internet |title=Retailers are now monitoring YOUR shopping habits and transactions with the eReceipt... |work=[[Daily Express#Sunday Express]] |date=2016-01-29 |accessdate=2016-12-11 }}</ref>

==References==
{{reflist}}

[[Category:Electronic documents]]
[[Category:Accounting source documents]]

{{retailing-stub}}
<=====doc_Id=====>:427
<=====title=====>:
Category:Legal citators
<=====text=====>:
{{Cat main|Citator}}

[[Category:Citation indices]]
[[Category:Legal research]]
[[Category:Legal citation]]
<=====doc_Id=====>:430
<=====title=====>:
Scopus
<=====text=====>:
{{Use dmy dates|date=August 2013}}
{{other uses}}
{{infobox bibliographic database
| title = Scopus
| image = [[File:Scopus_type_logo.jpg]]
| caption = 
| producer = [[Elsevier]]
| country = 
| history = 
| languages = English
| providers = 
| cost = Subscription
| disciplines= 
| depth = 
| formats = 
| temporal = 1995-present
| geospatial = Worldwide
| number = 55 million
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://www.scopus.com
| titles = 
}}
'''Scopus''' is a [[bibliographic database]] containing [[Abstract (summary)|abstracts]] and [[citation]]s for [[academic journal]] [[Article (publishing)|articles]]. It covers nearly 22,000 titles from over 5,000 publishers, of which 20,000 are [[peer review|peer-reviewed]] journals in the scientific, technical, medical, and social sciences (including arts and humanities).<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview |title=Scopus Content Overview |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> It is owned by [[Elsevier]] and is available online by [[subscription business model|subscription]]. Searches in Scopus also incorporate searches of patent databases.<ref>{{cite journal |doi=10.1001/jama.2009.1307 |title=Comparisons of Citations in Web of Science, Scopus, and Google Scholar for Articles Published in General Medical Journals |year=2009 |last1=Kulkarni |first1=A. V. |last2=Aziz |first2=B. |last3=Shams |first3=I. |last4=Busse |first4=J. W. |journal=[[JAMA (journal)|JAMA]] |volume=302 |issue=10 |pages=1092–6 |pmid=19738094}}</ref>

==Overview==

Since Elsevier is the owner of Scopus and is also one of the main international publishers of scientific journals, an independent and international Scopus Content Selection and Advisory Board was established to prevent a potential conflict of interest in the choice of journals to be included in the database and to maintain an open and transparent content coverage policy, regardless of publisher.<ref>{{cite web |url=http://www.elsevier.com/online-tools/scopus/content-overview#content-policy-and-selection |title=Scopus Content Overview: Content Policy and Selection |work=Scopus Info |publisher=Elsevier |accessdate=2013-09-04}}</ref> The board consists of scientists and subject librarians.

Evaluating ease of use and coverage of Scopus and the [[Web of Science]] (WOS), a 2006 study concluded that "Scopus is easy to navigate, even for the novice user. ... The ability to search both forward and backward from a particular citation would be very helpful to the researcher. The multidisciplinary aspect allows the researcher to easily search outside of his discipline" and "One advantage of WOS over Scopus is the depth of coverage, with the full WOS database going back to 1945 and Scopus going back to 1966. However, Scopus and WOS complement each other as neither resource is all inclusive."<ref>{{Cite journal |pmid=16522216 |year=2006 |last1=Burnham |first1=JF |title=Scopus database: A review |volume=3 |pages=1 |doi=10.1186/1742-5581-3-1 |pmc=1420322 |journal=Biomedical Digital Libraries}}</ref>

Scopus also offers author profiles which cover affiliations, number of publications and their [[bibliographic]] data, [[references]], and details on the number of citations each published document has received. It has [[alerts|alerting]] features that allows registered users to track changes to a profile and a facility to calculate authors' [[h-index]].

Scopus IDs for individual authors can be integrated with the nonproprietary digital identifier [[ORCID]].<ref name="Scopus">{{cite web |url=http://orcid.scopusfeedback.com |title=Scopus2Orcid |publisher=Scopus |accessdate=7 May 2014}}</ref>

==See also==
*[[Source Normalized Impact per Paper]]
*[[Web of Science]]

== References ==
{{reflist|30em}}

== External links ==
{{Wikidata property|P1153}}
{{Wikidata property|P1154}}
{{Wikidata property|P1155}}
{{Wikidata property|P1156}}
* {{Official website|http://www.scopus.com/}}

{{Reed Elsevier}}

[[Category:Bibliographic databases and indexes]]
[[Category:Elsevier]]
[[Category:Citation indices]]
[[Category:Library cataloging and classification]]
<=====doc_Id=====>:433
<=====title=====>:
SPIN bibliographic database
<=====text=====>:
{{Infobox Bibliographic Database
|title =SPIN  (Searchable Physics Information Notices)  
|image = 
|caption = 
|producer =[[American Institute of Physics]] (AIP) 
|country =USA, Russia, Ukraine
|history = 
|languages =English, [[Russian language|Russian]], [[Ukrainian language|Ukrainian]] 
|providers =[[Dialog (online database)|Dialog]], [[American Institute of Physics|AIP website]], [[SPIE|SPIE Digital Library]] 
|cost = 
|disciplines =Physics, Astronomy, Mathematics, Geophysics, Geosciences, Nuclear Science, Science & Technology 
|depth =Word, Phrase, Abstract, Author and Author affiliations, Descriptor, Errata (coden, or date, or volume) Identifier, Title, Astronomical objects, CODEN, Conference (location, or title, or year), Journal name, and more...   
|formats =Journal Articles, Book Reviews, Conferences, Meetings, Patents, Symposia
|temporal =1975 to the present  
|geospatial =International 
|number =over 1.5 million 
|updates =Weekly 
|p_title =No print counterparts 
|p_dates = 
|ISSN =
|web =https://scitation.aip.org/jhtml/scitation/coverage.jsp 
|titles =  
}}

'''SPIN''' (Searchable Physics Information Notices) '''bibliographic database''' is an indexing and abstracting service produced by the [[American Institute of Physics]] (AIP). The content focus of SPIN is described as the most significant areas of [[physics]] [[research]]. This type of [[scientific literature|literature coverage]] spans the major [[scientific journal|physical science journals]] and magazines. Major [[conference proceedings]] that are reported by the American Institute of Physics, member societies, as well as affiliated organizations are also included as part of this database. References, or citations, provide access to more than 1.5 million articles as of 2010. ''SPIN''  has no print counterpart.<ref name=DialogSpin/><ref name=AIP-SPIN/>

==Journals==
Delivery of timely indexing and abstracting is for, what are deemed to be, the significant or important [[physics]] and [[astronomy]] journals from the [[United States]], [[Russia]], and the [[Ukraine]]. Citations for journal articles are derived from original publications of the ''AIP'', which includes published translated works. At the same time, citations are included from member societies, and selectively chosen American journals. Citations become typically available online on the same date as the corresponding journal article.<ref name=DialogSpin/><ref name=AIP-SPIN> {{Cite web
  | title =What is the SPIN database? 
  | work =Information about SPIN 
  | publisher =[[American Institute of Physics]] 
  | date =July 2010 
  | url =http://scitation.aip.org/servlet/HelpSystem?KEY=SCI&TYPE=HELP/FAQ#ques3 
  | format = 
  | accessdate =2010-07-12}}</ref>

==Sources==
Overall, the source citations are derived from material published by the AIP and member societies,  which are English-speaking, Russian, and Ukrainian journals and conference proceedings. Certain American physics-related articles are also sources of citations. About 60 journals have cover to cover indexing, and about 100 journals, overall, are indexed.<ref name=DialogSpin/><ref name=pub-coverage>{{Cite web
  | title =SPIN Publication Coverage 
  | work =Complete list of publications covered and coverage years. 
  | publisher =American Institute of Physics 
  | date =July 2010 
  | url =http://scitation.aip.org/jhtml/scitation/spincodens.jsp 
  | format = 
  | accessdate =2010-07-12}}</ref>  

==Scope==
Subject coverage encompasses the following: <ref name=DialogSpin>  {{Cite web
  | title =Indexes and Databases 
  | work =SPIN: Searchable Physics Information Notices
  | publisher =Raymond H. Fogler Library, The University of Maine
  | date =October 2010 
  | url =http://www.library.umaine.edu/indexesdb/dbdetails.asp?field=Name&search=SPIN:+Searchable+Physics+Information+Notices 
  | format = 
  | accessdate =2010-07-12}}</ref>

*[[Applied physics]], [[Electromagnetic spectrum|Electromagnetic]] technology, [[Microelectronics]] 
*[[Atomic physics]] and [[Molecular physics]] 
*[[Biological physics]] and [[Medical physics]] 
*[[Classical physics]] and [[Quantum physics]] 
*[[Condensed matter physics]] 
*[[Elementary particle physics]] 
*[[Physics|General physics]], [[Optics]], [[Acoustics]], and [[Fluid dynamics]] 
*[[Geophysics]], [[Astronomy]], [[Astrophysics]] 
*[[Materials science]] 
*[[Nuclear physics]] 
*[[Plasma physics]] 
*[[Physical chemistry]]

==See also==
*[[List of academic databases and search engines]]

==References==
{{Reflist}}

==External links==
*[http://www.aip.org/press_release/spin.html AIP'S SPIN Database Reaches One Million Records].  American Institute of Physics. March 1, 2002.
*[http://scholarlykitchen.sspnet.org/2009/06/17/physics-papers-and-the-arxiv/ Can everything published in physics can be found in the [[arXiv]]?]. The Scholarly Kitchen. [[Society for Scholarly Publishing]]. June, 2010.
*[http://www.pub4stm.org/ AIP partnerships] (society publishing). July 2010.


[[Category:Bibliographic databases and indexes]]
[[Category:Citation indices]]
[[Category:Scientific databases]]
<=====doc_Id=====>:436
<=====title=====>:
Conference Proceedings Citation Index
<=====text=====>:
{{Incomplete|date=July 2015}}
{{infobox bibliographic database
| title = Conference Proceedings Citation Index
| image = 
| caption = 
| producer = [[Thomson Reuters]]
| country = United States
| history = 
| languages = 
| providers = 
| cost = 
| disciplines = 
| depth = 
| formats = 
| temporal = 
| geospatial = 
| number = 
| updates = 
| p_title = 
| p_dates = 
| ISSN = 
| web = http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/
| titles = 
}}
The '''Conference Proceedings Citation Index''' ('''CPCI''') is a [[citation index]] produced by [[Thomson Reuters]] covering [[conference proceedings]].<ref>{{cite web|url=http://wokinfo.com/media/pdf/proceedingswhtpaper.pdf |format=PDF |title=White Paper : Conference Proceddings and Their Impact on Global Research |publisher=Wokinfo.com |accessdate=2015-07-09}}</ref><ref>{{cite web|url=http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/cpciessay/|title=CPCI Essay - IP & Science - Thomson Reuters|author=Thomson Reuters|publisher=Wokinfo.com|accessdate=2015-07-09}}</ref>

==References==
{{Reflist}}

==External links==
*{{official website|http://wokinfo.com/products_tools/multidisciplinary/webofscience/cpci/}}

{{Thomson Reuters}}

[[Category:Citation indices]]
[[Category:Online databases]]
[[Category:Thomson Reuters]]
[[Category:Conference proceedings]]
<=====doc_Id=====>:439
<=====title=====>:
Scientific Information Database
<=====text=====>:
{{Refimprove|date=February 2014}}
{{Notability|Web|date=December 2013}}
{{Infobox company
| name             = Scientific Information Database, Iran
| type             = [Journal Citation Database]
| location         = Tehran, Iran
| homepage         = [http://www.sid.ir]
| footnotes        = پایگاہ اطلاعات علمی و پژوھشی
}}

'''Scientific Information Database''' (or '''SID''') is the Iranian database for the calculation of Persian and English articles citation. It is the like the [[Institute for Scientific Information]] '''ISI''', a local citation counting manager.<ref>{{cite web|url=http://sid.ir |title=Scientific Information Database |publisher=Sid.ir |date= |accessdate=2014-02-03}}</ref>

==Categories==
This database, does not include just the journal citation reports, it has different categories:
* English Journals, English Journals Database of Iran
* Persian Journals, بانک نشریات فارسی ایران
* Research Projects, طرح ھای پژوھشی
* English Scientific Community
* Persian Scientific Community, بانک مجامع علمی فارسی ایران 
* Science Centers, بانک مراکز علمی ایران

==References==
{{Reflist}}

==External links==
* [http://sid.ir Homepage of SID]

[[Category:Science and technology in Iran]]
[[Category:Citation indices]]
<=====doc_Id=====>:442
<=====title=====>:
Kelly's Directory
<=====text=====>:
'''Kelly's Directory''' (or more formally, the '''Kelly's, Post Office and Harrod & Co Directory''') was a [[trade directory]] in the United Kingdom that listed all businesses and tradespeople in a particular city or town, as well as a general directory of postal addresses of local [[gentry]], landowners, charities, and other facilities.  In effect, it was a Victorian version of today's [[Yellow Pages]].<ref>{{cite web|url=http://www.cottinghamhistory.co.uk/Directories.htm|title=Cottingham History|accessdate=11 July 2010}}</ref>  Many reference libraries still keep their copies of these directories, which are now an important source for historical research.

==Origins==
The eponymous originator of the directory was [[Frederic Festus Kelly]].  In 1835 or 1836 he became chief inspector of letter-carriers for the inland or general post office, and took over publication of the Post Office London Directory, whose copyright was in private hands despite its semi-official association with the post office, and which Kelly had to purchase from the widow of his predecessor.

He founded Kelly & Co. and he and various family members gradually expanded the company over the next several decades, producing directories for an increasing number of UK [[county|counties]] and buying out or putting out of business various competing publishers of directories.<ref name="pollard"/><ref>http://www.huthwaite-online.net/hucknall/gazetteers/</ref>

Other publications followed, including the ''Handbook to the Titled, Landed and Official Classes'' (1875) and ''Merchants, Manufacturers and Shippers'' (1877). In 1897, Kelly & Co Ltd became '''Kelly’s Directories Ltd.'''<ref name="lg"/>  This name stuck for another 106 years before being renamed Kellysearch in 2003 to reflect its focus away from hard copy directories and towards an Internet-based product search engine.

The front cover of a Kelly's Directory sometimes stated "Kelly's Directories Ltd., established 1799",<ref>{{cite web|title=Trade Directories|work=Stella & Rose's Books|url=http://www.stellabooks.com/articles/trade_directories.php|accessdate=2011-03-28}}</ref> however this was based on the date of issue of the first Post Office London Directory by an earlier inspector of letter carriers several decades before Kelly's involvement with that publication.<ref name="jenorton" />

== Kellysearch ==
For a short time, Kelly's existed online as [http://www.kellysearch.co.uk/ Kellysearch (broken link)], a directory similar to the online [[Yellow Pages]]. Kellysearch.com was established in Boston in 2004. It was in many different languages and introduced a fully searchable online-catalogue library and product [[News release|press release]] section.

The old editions of the Kelly’s Directories are seen as highly collectable by many and have also become a useful reference tool for people tracing the history of local areas (with the ancient data now available to buy on CD Rom from many entrepreneurial sources for this purpose.)  Every edition of the Kelly’s Directory ever published is held in the [[Guildhall Library]]<ref>[http://www.cityoflondon.gov.uk/things-to-do/archives-and-city-history/guildhall-library/Documents/8-trade-directories-at-guildhall-library.pdf Trade directories and telephone books at Guildhall Library]</ref> in [[London]].

==References==
{{reflist | refs=

<ref name="jenorton">{{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | author=Jane Elizabeth Norton | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293–299 | quote=The Post Office London Directory was started by two inspectors of the Inland letter-carriers called Ferguson and Sparkes… A third inspector, called B. Critchett, joined the enterprise in 1803 and later it was carried on by Critchett alone, then by Critchett and Woods, and then again by Critchett alone until his death in 1835. ''[sic; he died 18 September 1836]''}}<!-- Library (1966) s5-XXI(4): 293-299 --></ref>

<ref name="pollard">{{cite book | title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83–84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785 | quote=The first directories of counties outside London were published by Kelly in 1845; and during the next sixteen years the series was extended throughout England. In 1892 Kelly's Directories Ltd. acquired the majority of shares in [[Isaac Slater]] Ltd. [...]; and the firm of [[White's Directories|William White]] of Sheffield [...] was absorbed in 1898.}}</ref>

<ref name="lg">{{LondonGazette|issue=26876|date=23 July 1897|startpage=4149}}</ref>

}}

==Bibliography==
*{{cite book| title=Guide to the national and provincial directories of England and Wales, excluding London, published before 1856 | author=Jane Elizabeth Norton | year=1950 | edition=1984 reprint | publisher=Offices of the Royal Historical Society | isbn=0-86193-102-5}} (original edition: ISBN 0-901050-15-6)
*{{cite book| title=The development and growth of city directories| author=A. V. Williams| year=1913 | publisher=Williams directory co.}}
*{{cite book| title=The Earliest Directory of the Book Trade | first=John | last=Pendred | editor-first=Graham | editor-last=Pollard | chapter=Appendix H: General Directories | pages=83–84 | isbn=0-19-721759-1 | year=1955 | edition=reprint of 1785}}
*{{cite book| title=The directories of London, 1677-1977| author=Peter J. Atkins | publisher=Cassell and Mansell | year=1990
| isbn=0-7201-2063-2}}

==External links==
{{Commons category|Kelly's Directory}}
* [http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4/hd/ Historical Directories] has extensive online versions of old editions for England and Wales
* [http://forebears.co.uk/news/kellys-directories-project-complete#kellys Forebears] has transcriptions of one edition for each county
* {{cite journal | doi=10.1093/library/s5-XXI.4.293 | title=The Post Office London Directory | last=Norton | first=Jane Elizabeth | journal=The Library (The Transactions of the [[Bibliographical Society]]) | series=5th series | volume=21 | issue=4 | year=1966 | pages=293–299}}
* {{cite journal | journal=The London Journal | title=The Compilation and Reliability of London Directories | last=Atkins | first=Peter J. | volume=14 | issue=1 |date=May 1989 | pages=17–28 | publisher=Maney Publishing | issn=0305-8034 | url=http://www.ingentaconnect.com/content/maney/ldn/1989/00000014/00000001/art00002 | doi=10.1179/ldn.1989.14.1.17}}

{{Reed Elsevier}}

[[Category:Directories]]
[[Category:Waltham, Massachusetts]]
<=====doc_Id=====>:445
<=====title=====>:
Category:Telephone directory publishing companies
<=====text=====>:
[[Category:Publishing companies by medium]]
[[Category:Directories]]
[[Category:Directory assistance services]]
[[Category:Telephone numbers]]
<=====doc_Id=====>:448
<=====title=====>:
Thomas Register
<=====text=====>:
[[Image:ThomasRegister.png|frame|1905 ''Thomas' Register of American Manufacturers'']]
The '''''Thomas Register of American Manufacturers''''', now '''ThomasNet''', is an online platform for supplier discovery and product sourcing in the USA and Canada. It was once known  as the "big green books" and "Thomas Registry", and was a multi-volume [[Yellow Pages|directory]] of [[Industry|industrial]] product information covering 650,000 [[distributors]], [[manufacturers]] and service companies within 67,000-plus [[industry|industrial]] categories that is now published on ThomasNet.

==History==
The books were first published in 1898 by Harvey Mark Thomas as ''Hardware and Kindred Trades. ''In their heyday, '''''Thomas Register of American Manufacturers ''''' was a 34-volume, 3 section buying guide offering sourcing information on industrial products and services, along with comprehensive specifications and detailed product information from thousands of manufacturers. The Thomas Regional Directory Company began as a division of Thomas Publishing in 1976. Thomas Regional Regional Industrial Buying Guides provided information in print and on CD-ROM,  on local OEMs, distributors, MRO services and other custom manufacturing services in 19 regional editions covering much of the United States. Thomas Register and Thomas Regional were available online from the mid 1990s. The company stopped publishing its print products in 2006.

Thomas has moved its database [[online]] as ThomasNet, published and maintained by Thomas Industrial Network, one of Thomas’ five business units. ThomasNet has expanded to provide not only product and company information, but also [[Online shopping|online catalog]]s, [[computer-aided design]] ([[Computer-aided design|CAD]]) drawings, [[news]], [[press releases]] and [[blogs]].

==Thomas Publishing Company, LLC==

Thomas Publishing Company, LLC of [[New York City]] has been [[privately held]] since its inception. It used independent representatives to sell advertising space around its listings in print products like the Thomas Register and the Thomas Industrial Regional Directories, and these representatives continue to sell Internet related products to manufacturers, distributors, and other companies.

==ThomasNet==

ThomasNet is an information and technology company based in New York City. In April 2006 the [[New York Public Library]] named ThomasNet.com as one of its [http://www.nypl.org/branch/books/index2.cfm?ListID=300 25 Best of Reference] sources for the [[reference librarian]], and is currently listed in their [http://www.nypl.org/weblinks/1382 Best of the Web] list for Industry Information.

Since November 2010, ThomasNet has been a founding partner of GlobalTrade.net, a marketplace for international trade service providers.

==ThomasNet News==
ThomasNet News is a product of Thomas Publishing Company, LLC. ThomasNet News was introduced with “the mission of delivering timely, new industrial product information covering the whole range of products …” It manually reviews press releases submitted through the website and publishes with a small description in one of 51 different categories.

In 2000, ThomasNet News released Industry Market Trends (IMT), its first Journal. In the IMT, editors published editorials, interviews, and long form journalism on issues ranging from career skills, developments in the industry, and discussions with leading experts. Soon after, IMT Green & Clean was launched in response to the growing interest in green technology and its impact on the world. In 2011, the IMT Machining Journal was launched followed by the IMT Fluid & Gas Flow Journal, the IMT Career Journal, and the IMT Procurement Journal.

==Research==
Starting in 2010, ThomasNet began reaching out to its database of manufacturers to get a better understanding of where the community was, where their shortcomings were, and where they saw the landscape going in the future. This yearly survey is called the Industry Market Barometer.

== External links ==
* {{Official website}}

[[Category:Promotion and marketing communications]]
[[Category:Marketing books]]
[[Category:Directories]]
<=====doc_Id=====>:451
<=====title=====>:
The Milepost
<=====text=====>:
{{italic title}}
[[Image:49MilePost.gif|right|thumb|The original 1949 Milepost]]'''''The Milepost''''' is an extensive [[guide book]] covering [[Alaska]], the [[Yukon]], the [[Northwest Territories]], and [[British Columbia]].  It was first published in 1949 as a guide about traveling along the [[Alaska Highway]], often locally referred to as "The ALCAN".<ref name="morris">[http://morris.com/divisions/mcc_magazines/the_milepost.shtml ''The MILEPOST''] from the website of  [[Morris Communications]]</ref>  It has since expanded to cover all major highways in the northwest corner of [[North America]], including the [[Alaska Marine Highway]].  It is updated annually.

==History==
<!-- Deleted image removed: [[Image:Milepost2008cover.jpg|right|thumb|The 2008 edition<br />{{deletable image-caption|Sunday, 10 February 2013}}]] -->''The Milepost'' is packaged and distributed like a [[book]]  (2008 edition: ISBN 978-189215431-6), but like the [[Yellow Pages]] it includes paid [[advertising]].<ref>[http://www.themilepost.com/media_kit/testimonials.shtml Testimonials from Advertisers from ''The MILEPOST'' website]</ref> The original 1949 edition was a mere 72 pages, by 2014 it had expanded to 752 pages, detailing every place a traveler might eat, sleep, or just pull off the road for a moment on all of the highways of northwestern North America. In addition to the paid ads, descriptions are provided of interesting hikes or side trip drives near the highways, campgrounds and other public facilities, as well as short histories of most of the settlements on the highways. Newer additions include special sections on selected areas popular with tourists, such as the [[Kenai Peninsula]]. It is also exhaustively cross-indexed and maps and charts are provided so that travelers can determine the total driving distance between any two points covered by the guide.<ref>http://milepost.com/index.php?option=com_content&task=view&id=71&Itemid=62</ref>

==Publishing==
Since 1997 ''The Milepost'' has been published by [[Morris Communications]] and currently shares publishing offices with [[Alaska magazine|''Alaska'' magazine]].<ref name="morris" /> Beginning in 2009, The Milepost is also available in an interactive digital format or download.<ref>[http://milepost.com/images/media_kit/mp_mediakit_09_email_lr.pdf The Milepost media kit]</ref>

==References==
<references />

==External links==
* {{Official website|http://www.themilepost.com}}

{{Morris Communications}}

{{DEFAULTSORT:Milepost, The}}
[[Category:1949 establishments in Alaska]]
[[Category:1949 books]]
[[Category:Books about Alaska]]
[[Category:Directories]]
[[Category:Morris Communications]]
[[Category:Publications established in 1949]]
[[Category:Roads in Alaska]]
[[Category:Travel guide books]]
<=====doc_Id=====>:454
<=====title=====>:
MiM
<=====text=====>:
{{Other uses|MIM (disambiguation)}}
{{Orphan|date=February 2009}}
'''MIM''' stands for Music Industry Manual. It was founded in 1996 by James Robertson and in its first year was called The Promoter's Handbook. The Promoters Handbook was a reference manual for the [[dance music]] industry including [[DJ]]s agents, [[nightclub]]s and unusual venues, promoters, flyer designers. The following year its title was changed to give it a broader appeal.

It still caters for the dance music industry, but is now fully international with over 100,000 contacts from countries as remote as [[Azerbaijan]] to developed nations. Whilst the focus is still on DJ and club culture it has over 100 categories including bar designer, music lawyers, event management.

==External links==
* [http://www.mim.dj Official web site]

{{primary sources|date=August 2007}}

[[Category:Directories]]
[[Category:Electronic dance music]]
<=====doc_Id=====>:457
<=====title=====>:
Crockford's Clerical Directory
<=====text=====>:
{{italictitle}}'''''Crockford's Clerical Directory''''' ('''''Crockford''''') is the authoritative directory of the [[Anglican Communion]] in the United Kingdom, containing details of English, Welsh and Irish benefices and churches, and biographies of around 26,000 clergy. It was first issued in 1858 by [[John Crockford]],{{sfn|Hough & Matthew|2004}} a London printer and publisher whose father – also named John – had been a Somerset schoolmaster.

''Crockford'' is currently compiled and published for the [[Archbishops' Council]] by Church House Publishing.<ref>{{Cite web
| title = Crockford's Clerical Directory
| author =
| work = The Church of England
| date =
| accessdate = 2014-08-12
| url = https://www.churchofengland.org/clergy-office-holders/crockford.aspx
| ref={{sfnref|CofE}}
}}</ref>  It covers in detail the whole of the [[Church of England]] (including the [[Diocese in Europe]]), the [[Church in Wales]], the [[Scottish Episcopal Church]], and the [[Church of Ireland]], and it also gives some information – now more limited – about the world-wide [[Anglican Communion]].

== Previous publishers ==

[[File:Crockford1868-titlepage.jpg|thumb|''Crockford's Clerical Directory 1868'', published by Horace Cox, London]]The actual title of the first edition was simply ''The Clerical Directory'', but a footnote showed that it was published by John Crockford, 29 Essex Street, [[Strand, London|the Strand]].  The original publisher died suddenly in 1865, shortly before the appearance of the third edition of what had by then become ''Crockford’s Clerical Directory''.  For many subsequent issues the volumes were anonymously edited, but they were published under the imprint of Horace Cox – the nephew of John Crockford’s closest business associate, solicitor and publisher [[Edward William Cox]] (1809Ω–1879).  (His family was probably quite unrelated to the Charles Cox who coincidentally was the publisher of ''Crockford''{{'}}s chief rival, the ''[[Clergy List]]''.{{efn|A two-part article "Shop-talk and mordant wit" by Christopher Currie & Glyn Paflin describes the background to the directory's first hundred editions, {{sfn|Currie & Paflin|7 December 2007}} }}) Horace Cox died in 1918{{efn|Horace Cox’s very brief obituary in ''The Times'', 11 October 1918; p. 5, states that he had retired in 1912 and had ceased to take an active part in his business, which also produced ''The Field'', ''The Queen'' and ''The Law Times''}} and the title was subsequently sold in 1921 to the [[Oxford University Press]],{{sfn|Currie & Paflin|7 December 2007}} who continued as publishers right up until the early 1980s.  For the 1985/86 issue publication was transferred to the [[Church Commissioners]] and their Central Board of Finance (who worked from their own administrative lists and databases).  It is now collated by Church House Publishing.

== Frequency of publication ==

The first four issues came  out in 1858, 1860 (with a supplement in 1861),{{efn|The 1861 supplement, experimentally issued when a switch to biennial publication was being contemplated, may be downloaded free of charge from Google Play}} 1865 {{efn|The 1865 edition was reprinted in a 1995 facsimile limited edition of 100 copies by Peter Bell (bookseller), Edinburgh.{{sfn|Bell|1995|p=}} It can also now be downloaded free of charge from Google play}} and 1868.  ''Crockford'' then reappeared biennially until 1876, when it began a long run of annual appearances which lasted until 1917. The next issue was a delayed 1918/19 edition, which had for the first time incorporated its main rival publication, the ''[[Clergy List]]''. Further issues appeared for 1920 and 1921/22; then between 1923 and 1927{{efn|There was no issue in 1928, for what the editor called "technical reasons". Production difficulties in 1941/42, 1943 and 1944 meant that it was only possible to issue short supplements to the 1941 edition. ''Crockford Prefaces: The Editor Looks Back'' (Oxford, 1947), pp. i, 257, 272, 283.}} and 1929–1940 the directory reappeared annually, followed by more late issues in 1941 and 1947/48. Since that time ''Crockford'' has generally appeared every two years, although gradually worsening delays meant that the 87th and 88th editions were dated 1977/79 and 1980/82, and the book failed to appear at all during 1983/84. Biennial publication was once again resumed in 1985/86, although the volume issued late in 1997 was designated the 1998/99 edition. The 100th edition – eventually published for 2008/09 – included within its hardback version a few facsimile pages from the first edition, together with an extended historical note describing some of the earlier volumes.

The 1858 edition was later described as seemingly “assembled in a very haphazard fashion, with names added ‘as fast as they could be obtained’, out of alphabetical order and with an unreliable index”. But nevertheless the 1860 directory “had become a very much more useful work of reference”.{{efn|Quoted by Brenda Hough in her biographical note on John Crockford, published in the 1998/99 ''Crockford'' and reprinted (with minor modifications) in all subsequent editions; also on the official Crockford's website.<ref>{{Cite web
| title = About John Crockford
| author = Brenda Hough
| work = Crockford's clerical Directory - online
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/standard.asp?id=126
| quote =
}}</ref>}}  However the original volume was actually a consolidation of what in 1857 had been conceived as a mere series of supplements to an entirely different publication, the ''Clerical Journal''.{{efn|The ''Oxford Dictionary of National Biography'' article on Edward William Cox states that he, together with John Crockford, had founded the ''Clerical Journal'' in 1853.}}  The editors explained in the preface that they  wished it to be understood that it was “but the foundation of a great work which, with the Cordial aid of the clergy, we shall hope to make more and more perfect every year”.

== Scope of the directory ==

[[File:Crockford1910BpLichfield.jpg|thumb|''Crockford'', 1910: a biographical page in an older edition would typically include many abbreviations, including clergy academic backgrounds, and their dates ordained deacon [d] and priest [p] (the presiding bishop being indicated). Diocesan coats of arms were shown alongside episcopal entries; any publications were listed, and parish incomes and patrons were mentioned. Many overseas clergy would be covered.]] The 1858 issue was based on postal returns from the clergy in England and Wales, involving an outlay – as the preface pointed out – of "more than Five Hundred Pounds for Postage Stamps alone".  Simpler lists for the [[Scottish Episcopal Church]] and for a number of colonial clergy – obtained from alternative sources – had been added by the 1865 edition, whilst details of Irish clergy had also been extracted from [[Alexander Thom (almanac editor)|Alexander Thom]]'s ''Irish Almanack and Official Directory''.  From the 1870s onwards the scope was progressively extended to all parts of the Anglican communion with the notable exception of the [[Episcopal Church (United States)]].  The 1870 edition contained 940 pages, but this had increased to over 2,100 pages by 1892.

The earliest editions had also gradually added some details of diocesan office holders and administrators, together with the theological colleges, and the royal chapels.  They also acquired much fuller indexes – along with outline maps of dioceses, and increasingly complete lists of bishops, dating right back to the earliest years of their sees.  They further offered to all clergy an opportunity to list their publications, although these lists eventually had to be cut back as their overall length started to increase dramatically.

By the early 1980s severe economies had become necessary and 1985/86 edition had to be restricted to the "home" churches of England, Scotland and Wales.<ref>''Crockford’s Clerical Directory 1987/88'', pp. 47-48</ref>  Retired clergy were temporarily restricted to just a few details of their final appointment, although it became possible to restore the Irish clergy in time for the 1987/88 edition.  Later editions saw a further return of the retired clergy, together with details of those overseas clergy who had originally been licensed or trained in the UK, or who occupied senior positions within their respective church hierarchies.  Details which had also become obtainable from the ''[[Church of England Yearbook]]'' or from similar sources were generally excluded. For a time too clergy who made their livings though secular jobs  were excluded from the biographies section, with the abbreviation NQ (Non-Qualifying Position) being used to cover such periods when clerics returned to parish work and were again eligible for inclusion. In that many such clergy retained diocesan licences or episcopal "Permissions to Officiate" during their periods of secular employment, this approach may have caused a degree of difficulty for clerics who needed to prove their clerical status.

By 1985/86 the first women deacons were being included (although [[Hong_Kong_Sheng_Kung_Hui#Social_issues|women priests ordained in Hong Kong]] were included even in the 1970s) while other more recent innovations – from the 1990s onwards – have included optional email addresses, together with lists of those clergy who have died since the previous edition.  Notes on "How to Address the Clergy"<ref>{{Cite web
| title = How to address the clergy
| author =
| work = Crockford's Clerical Directory
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/standard.asp?id=116
| quote =
}}</ref> have been retained. A small number of clergy have been excluded at their own request, or have allowed their biographies to appear minus a contact address.  The Church Commissioners soon replaced the traditional black hardback bindings in favour of red and also introduced a separate softback alternative version.

Since 2004 there has also been a frequently updated Internet edition of ''Crockford'', which is available by subscription.<ref>{{Cite web
| title = Welcome to the Crockford web site...
| author =
| work = Crockford's Clerical Directory
| date =
| accessdate = 2014-08-12
| url = http://www.crockford.org.uk/
| subscription=yes
}}</ref> More recently the directory has also joined in with [[social networking]], operating a [[Twitter]] account since 2012. 

An alternative to the main work, ''Crockford's Shorter Directory'', focused almost entirely on the Church of England and omitting all past biographical details, was issued as a single edition in 1953–54.

== Prefaces ==

The well-known tradition of having an extensive but anonymous preface offering a general review of events within the Anglican communion – together with some occasionally sharp and controversial commentary – evolved gradually during the early part of the 20th century.{{sfn|Currie & Paflin|7 December 2007}} Previous prefaces had tended to be much briefer and they had often been limited merely to explaining the directory’s in-house policies.  After the events following the publication of the 1987/88 edition, which had ended with the death of Dr [[Gareth Bennett]], this tradition of the anonymous preface was discontinued.

An anthology ''Crockford Prefaces: The Editor Looks Back'', anonymously edited by [[Richard Henry Malden]]{{sfn|Currie & Paflin|7 December 2007}} and covering the previous 25 years, was published by the Oxford University Press in 1947.{{sfn|Anon|1947|p=}}

== Locating previous issues ==

County libraries each have their own policies, but there are good collections in a number of major academic and ecclesiastical libraries, including [[Cambridge University Library]], [[Lambeth Palace Library]], [[Canterbury Cathedral]] Library, [[York Minster]] Library, the [[Guildhall Library]] and the [[Society of Genealogists]].

Besides the 1865 reprint,{{sfn|Bell|1995|p=}} a small number of early editions have been reissued in CD format by various publishers, including [[Archive CD Books]].  Scanned copies of other early editions have also begun to appear on the World Wide Web.

== Crockford references in fiction ==

Crockford is referenced in [[Dorothy Sayers]]'s 1927 detective novel ''[[Unnatural Death]]'' (chapter XI) where [[Lord Peter Wimsey]] uses "this valuable work of reference" in trying to trace a clergyman who is important for solving the book's mystery.

Another fictional character holding Crockford on his bookshelves was [[Sherlock Holmes]], who during one of his final short stories ("The Adventure of the Retired Colourman"), consulted his copy before dispatching his colleague Dr Watson, together with another companion, to a distant part of Essex. There they interviewed “a big solemn rather pompous clergyman” who received them angrily in his study.

The character Dulcie Mainwaring prefers Crockford's format to ''[[Who's Who]]'' while reflecting on researching in the [[Public Record Office]] in London in [[Barbara Pym]]'s ''No Fond Return of Love.  ''

== Footnotes ==
{{notelist}}

== Notes ==
{{reflist}}

== References ==
{{refbegin}}

* {{Citation

| title = Crockford, John (1824/5–1865)
| first1 = Brenda
| last1= Hough
| first2=H. C. G.
| last2= Matthew
| work = Oxford Dictionary of National Biography
| date =  2004
| accessdate = 2014-08-12
| url = http://dx.doi.org/10.1093/ref:odnb/37324
| language =
| ref={{sfnref|Hough & Matthew|2004}}
}}

* {{Cite web
 |title=Shop-talk and mordant wit 
 |first1=Christopher 
 |last1=Currie 
 |first2=Glyn 
 |last2=Paflin 
 |work=The [[Church Times]] 
 |date=7 December 2007 
 |issue=7552 
 |url=http://www.churchtimes.co.uk/content.asp?id=48255 
 |accessdate=2014-08-12 
 |archive-url=https://web.archive.org/web/20120407024756/http://www.churchtimes.co.uk/content.asp?id=48255 
 |archive-date=2012-04-07 
 |ref={{sfnref|Currie & Paflin|7 December 2007}} 
 |subscription=yes 
 |deadurl=yes 
 |df= 
}}

* {{cite book|ref={{sfnref|Bell|1995|p=}}|last=Bell|first=Peter|title=Crockford's Clerical Directory for 1865: Being a Biographical and Statistical Book of Reference for Facts Relating to the Clergy and the Church|url=https://books.google.com/books?id=oSqjAQAACAAJ|year=1995|publisher=Horace Cox  in 1865, republished by Peter Bell in 1995|location=Oxford and Edinburgh|isbn=978-1-871538-21-2}}
* {{cite book|ref={{sfnref|Anon|1947|p=}}|author=Anon|authorlink=Richard Henry Malden|title=Crockford prefaces: the editor looks back|url=https://books.google.com/books?id=rItbAAAAMAAJ|year=1947|publisher=Oxford Univ. Press}}
*{{cite book|ref=harv|author1=Church of England|author2=Central Board of Finance|author3=Church Commissioners|title=Crockford's Clerical Directory|url=https://books.google.com/books?id=BzkFAAAAYAAJ|year=1865|publisher=Oxford University Press}}

{{refend}}

== External links ==

* {{Official website|http://www.crockford.org.uk/}}
* [https://play.google.com/store/books/details/Church_of_England_Crockford_s_Clerical_Directory?id=BzkFAAAAYAAJ Crockford's Clerical Directory 1865 free download from Google play]
* [http://www.chpublishing.co.uk/ Church House Publishing]
* [https://archive.org/details/crockfordscleri00commgoog 1868 version available for free download at the archive.org]

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]
<=====doc_Id=====>:460
<=====title=====>:
American Art Directory
<=====text=====>:
{{Infobox Magazine
| title           = American Art Directory
| image_file      = American Art Directory logo, 1898.svg
| image_size      = 140px
| image_caption   = ''Frontispiece from 1898 volume''
| editor          = 
| editor_title    = 
| previous_editor = 
| staff_writer    = 
| frequency       = [[Annual publication|Annual]]
| circulation     = 
| category        = 
| company         = 
| publisher       = 
| firstdate       = 1898
| country         = 
| based           = 
| language        = [[English language|English]]
| website         = http://www.americanartdir.com/
| issn            = 0065-6968
}}

The '''''American Art Directory''''' is a yearly publication covering [[art museum]]s, [[Arts centre|arts centers]], and [[Art school|art educational institutions]] as well as news, obituaries, book and magazine publications, etc. related to the artistic community in the [[United States]].  Established in 1898, it was originally entitled ''American Art Annual''.

Art consultant, advisor, author, and independent appraiser [[Alan Bamberger]] describes the Directory as "...a required reference for art museums, libraries, arts organizations, art schools, and corporations with art holdings."<ref>{{citation
 |first1=Alan S. 
 |last1=Bamberger 
 |title=Who's Who In American Art, Official Museum Directory, American Art Directory 
 |publisher=ArtBusiness.com 
 |accessdate=2009-01-14 
 |url=http://www.artbusiness.com/revs0608.html 
 |deadurl=bot: unknown 
 |archiveurl=http://www.webcitation.org/5dphZr5S6?url=http%3A%2F%2Fwww.artbusiness.com%2Frevs0608.html 
 |archivedate=2009-01-15 
 |df= 
}}, archived by [[WebCite]] </ref>

A yearly feature is the "Review of the Year" article discussing the touring exhibitions, commissions, grants to organizations, construction starts at museums and other facilities, and various other events that occur within the art community.

Initially the directory was the work of the [[New York City|New York]] area artist [[Florence Nightingale Levy]] and published by [[Macmillan Publishers|The Macmillan Company]].<ref name="NYTObit">{{Citation
  | title = NOTES AND NEWS.; Items Gathered During This Week's Tour of the Publishing Houses.
  | newspaper = [[The New York Times]]
  | date = April 1, 1899
  | url = http://query.nytimes.com/gst/abstract.html?res=9407E1DF1538E433A25752C0A9629C94689ED7CF}}</ref>  The [[American Federation of Arts]], with which Mrs. Levy was associated and which she would later become the president of, was founded in 1909<ref>{{citation
  | title = About the AFA
  | publisher = [[American Federation of Arts]]
  | accessdate = 2009-01-14
  | url = http://www.afaweb.org/about/}}</ref> and in 1913 the directory became an official publication of that organization.<ref name="Torchbearers">{{citation
  | first1 = Karen J.
  | last1 = Blair
  | authorlink1 = Karen J. Blair
  | title = The Torchbearers: Women and Their Amateur Arts Associations in America, 1890-1930
  | publisher = [[Indiana University Press]]
  | location = [[Bloomington, Indiana|Bloomington]]
  | year = 1994
  | isbn = 978-0-253-31192-4
  | page = 80
  | oclc = 27677514
  | url = https://books.google.com/books?id=wP5pq2aBYBAC&printsec=frontcover#PPA80,M1}}</ref>  It later became the independent publication it exists as currently.

In 1952 the ''American Art Annual'' was split into two separate publications, ''[[Who's Who in American Art]]'' and the ''American Art Directory''.<ref>{{citation
  | title = Who’s Who in American Art
  | publisher = [[R. R. Bowker]]
  | location = [[New York City|New York]]
  | year = 1953
  | issn = 0000-0191}}</ref>

==References==
{{Reflist}}
<div style="text-align:center;height:3em;">&#160;</div>

==External links==
* [http://www.americanartdir.com/ Official website]

[[Category:Books about visual art]]
[[Category:Annual magazines]]
[[Category:Magazines established in 1898]]
[[Category:Directories]]
[[Category:Arts in the United States]]
[[Category:American arts magazines]]


{{art-mag-stub}}
{{US-arts-org-stub}}
{{art-book-stub}}
<=====doc_Id=====>:463
<=====title=====>:
Annuario Pontificio
<=====text=====>:
{{italic title}}
{{Refimprove|date=April 2010}}
{{Infobox book
| name = Annuario Pontificio 
| title_orig =
| translator =
| image = Annuario Pontificio 2008 (MK).jpg
| caption =
| author = [[Libreria Editrice Vaticana]], Secretary of State
| illustrator =
| cover_artist =
| country = [[Vatican City|Vatican]]
| language = Italian
| series =
| subject =
| genre = [[Annual publication]], [[Reference]]
| publisher = [[Holy See]]
| pub_date = December 2, 2014
| media_type = Printed Book
| pages = 
| isbn = 9788820997472
| isbn_note = <ref>[http://www.libreriaeditricevaticana.va/content/libreriaeditricevaticana/it/news-ed-eventi/annuario-pontificio-2016.html 2016]</ref>
| oclc= 
| preceded_by = Annuario Pontificio 2015
| followed_by = Annuario Pontificio 2016
}}
The '''''Annuario Pontificio''''' ([[Italian language|Italian]] for ''Pontifical Yearbook'') is the annual directory of the [[Holy See]]. It [[List of popes|lists all the popes]] to date and all officials of the Holy See's [[dicastery|departments]]. It also gives complete lists, with contact information, of the [[Cardinal (Catholicism)|cardinals]] and [[Catholic Church|Catholic]] bishops throughout the world, the [[diocese]]s (with statistics about each), the departments of the [[Roman Curia]], the Holy See's [[diplomatic mission]]s abroad, the [[embassy|embassies]] accredited to the Holy See, the headquarters of [[religious institute]]s (again with statistics on each), certain academic institutions, and other similar information. The index includes, along with all the names in the body of the book, those of all priests who have been granted the title of "[[Monsignor]]".
As the title suggests, the red-covered yearbook, compiled by the Central Statistics Office of the Church and published by [[Libreria Editrice Vaticana]], is mostly in Italian.

The 2015 edition has more than 2,400 pages and costs {{€|78}}.<ref>{{cite web |url=http://www.vaticanum.com/en/annuario-pontificio-2015-book-2|access-date=April 5, 2016|title=Annuario Pontificio 2015|publisher=Città del Vaticano}}</ref> According to the ''Pontifical Yearbook of 2010'', the number of Catholics in the world increased from 1,147 million to 1,166 million between 2007 and 2008, a growth of 1.7 percent.<ref>{{cite web| url=http://www.zenit.org/article-28425?l=english |title=Number of Catholics Increases Worldwide: 2010 "Annuario" Shows Growth in Asia and Africa |publisher=Zenit |date=February 21, 2010 |accessdate=April 11, 2010}}</ref> By the ''Yearbook of 2016'' it was 1,272,281,000 at the end of 2014.

==History==
A [[yearbook]] of the Catholic Church was published, with some interruptions, from 1716 to 1859 by the Cracas printing firm in Rome, under the title (in Italian) ''Information for the Year ...'' From 1851, a department of the Holy See began producing a different publication called (in Italian) ''Hierarchy of the Holy Catholic Apostolic Church Worldwide and in Every Rite, with historical notes'', which took the title ''Annuario Pontificio'' in 1860, but ceased publication in 1870. This was the first yearbook published by the Holy See itself, but its compilation was entrusted to the newspaper ''[[Giornale di Roma]]''. The publishers "Fratelli Monaldi" (Monaldi Brothers) began in 1872 to produce their own yearbook entitled (in Italian) ''The Catholic Hierarchy and the Papal Household for the Year ... with an appendix of other information concerning the Holy See''.

The [[Holy See Press Office|Vatican Press]] took this over in 1885, thus making it a semi-official publication.  It bore the indication "official publication" from 1899 to 1904, but this ceased when, giving the word "official" a more restricted sense, the ''Acta Sanctae Sedis'', forerunner of the ''[[Acta Apostolicae Sedis]]'', was declared the only "official" publication of the Holy See. In 1912, it resumed the title ''Annuario Pontificio''. From 1912 to 1924, it included not only lists of names, but also brief illustrative notes on departments of the Roman Curia and on certain posts within the [[papal court]], a practice to which it returned in 1940.

For some years, beginning in 1898, the ''Maison de [[la Bonne Presse]]'' publishing house of [[Paris]] produced a similar yearbook in [[French language|French]] called ''Annuaire Pontifical Catholique'', not compiled by the Holy See. This contained much additional information, such as detailed historical articles on the [[Swiss Guards]] and the [[Apostolic Palace|Papal Palace]] at the [[Vatican City|Vatican]].

== Statistical data ==
According to the ''Annuario Pontificio 2012'' the statistical data given in the yearbook regarding [[archdiocese]]s and [[diocese]]s are furnished by the diocesan curias concerned and reflect the diocesan situation on 31 December of the year prior to the date on the yearbook, unless there is another indication.  The data recorded are shown in the following order next to these abbreviations:
* Su – area in square kilometers of the diocesan territory
* pp – population of the diocese
* ct – number of Catholics
* pr – parishes and quasi-parishes
* ch – churches or mission stations
* sd – secular priests resident in the diocese
* dn – diocesan priests ordained during the year
* sr – religious priests resident in the diocese
* rn – religious priests ordained during the year
* dp – permanent deacons
* sm – seminarians taking courses of philosophy and theology
* rm – members of men's religious institutes
* rf – members of women's religious institutes
* ie – educational institutes
* ib – charitable institutes
* ba – baptisms

== See also ==
{{Portal|Catholicism}}
* [[Catholic Church by country]]
* [[History of the papacy]]
* [[Oldest popes]]
* [[Vatican Publishing House]]

==References==
{{reflist}}

==Bibliography==
* Secretary of State, ''Annuario Pontificio 2010.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8355-0
* Secretary of State, ''Annuario Pontificio 2009.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8191-4
* Secretary of State, ''Annuario Pontificio 2008.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-8021-4
* Secretary of State, ''Annuario Pontificio 2007.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7908-9
* Secretary of State, ''Annuario Pontificio 2006.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7806-8
* Secretary of State, ''Annuario Pontificio 2005.'' Vatican City: [[Vatican Publishing House]]. ISBN 978-88-209-7678-1

==External links==
* [http://www.catholic-hierarchy.org/ CatholicHierarchy.org]
* [http://www.gcatholic.org/ GCatholic.org]

[[Category:Documents of the Catholic Church]]
[[Category:Directories]]
[[Category:Holy See]]
<=====doc_Id=====>:466
<=====title=====>:
Ves Peterburg
<=====text=====>:
{{italic title}}
'''''Ves Peterburg''''' (/vʲesʲ pʲɪtʲɪrˈburg/, Literally translated "''All Petersburg''" or "''The Entire Saint Petersburg''"
") (Full name in [[cyrillic]] "Ves Petersburg; Adresnaja i spravočnaja kniga g. Petersburga") (often referred to as the ''Suvorin directories'' from the publisher's name) was the title of a series of [[city directory|city directories]] of [[Saint Petersburg]], [[Russia]] published on a yearly basis from 1894 to 1940 by [[Aleksei Sergeevich Suvorin]]. Each volume was anywhere between 500 and 1500 pages long. After changes in the name of the city the directories were called '''''Ves Petrograd''''' from 1914 to 1923 and '''''Ves Leningrad''''' from 1924 to 1940.

The directories contained detailed lists of private residents, names of streets and squares across the city with the details of their occupants and owners, government offices, public services and medium and large businesses present in the city.  They are often used by [[genealogists]] for family research in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historian]]s use them to research the [[social histories]] of the city.

== List of residents of St. Petersburg ==

Each directory was written exclusively in Russian Cyrillic only, and contains various sections among which was an alphabetical list of residents in the city. Those listed usually were the head of their respective household and so spouses and minors are not listed.

The following information can be found:
*Person's surname and first name
*[[Patronymic]]
*Street address with apartment number
*Profession
*Telephone numbers (only appear sparingly as few private residents could afford a telephone before 1918)

== List of occupants of each building on every street and square ==

A section immediately preceding or following that listing residents in alphabetical order was a directory of all streets, houses and flats with the names of their owners and occupants. In this way readers could determine all those people who lived on a particular street of in a certain apartment block.

== Other sections ==

The following information can also be found in each directory

*information on the royal family
*Maps of the city
*cultural establishments (with interior theatre hall layouts and seating plans)
*Lists of personnel in state, public and private institutions
*information on academic institutions of all ranks
*information on churches and monasteries of St. Petersburg
*Original commercial advertisements of Russian and foreign companies which had offices in St. Petersburg

== Historical and genealogical value ==

Because numerous residents emigrated from Saint Petersburg after the [[Russian Revolution of 1917]] and tens of thousands more were either arrested, shot, or sent to the [[gulag]] by the [[Cheka]] and the [[NKVD]] after 1918 the section detailing residents names is especially useful in determining until when a certain person was still living in the city.

== Interruption in the series ==

No volumes were published in the following years:
*1918
*1919
*1920
*1921

This was due to the events of the [[Russian revolution of 1917]] and the subsequent [[Russian civil war]].

The edition of 1922 was very concise and only contained details of businesses in the city but not residents.

== Termination of series ==

Publication came to a halt after the edition of 1935, coinciding with the time of [[Joseph Stalin]]'s [[great purge]]s and [[Moscow Trials]]. The only further volumes were issued in 1939 and 1940, but these (like the edition in 1922) only contained details of state run businesses and public and governmental offices, but not residents.

== Availability ==

Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including [[The Baltic]], Finland the United Kingdom and Germany) however most only have an incomplete collection. The [[Russian National Library]] in Saint Petersburg has a complete run of all volumes published available.

== Other city directories ==

Suvorin also published city directories for [[Moscow]] under the title ''[[Vsia Moskva]]'' (All Moscow) for the years 1875 to 1936 and for the whole country under the titles ''[[Vsia Rossiia]]'' (All Russia) continued under than name ''[[Ves SSSR]]'' (All USSR) from 1924 to 1931.

Since 1993 a telephone directory under the title "Ves Petersburg" has been published annually by the publishing House Presskom but this is vastly different in content then the original directories and does not list residents.

== Sources ==

http://www.encspb.ru/en/article.php?kod=2804017249
Ves Peterburg - http://www.allinform.ru

==See also==

*''[[Vsia Moskva]]''
*''[[Vsia Rossiia]]''

== External links ==
*[http://www.nlr.ru Official website of the Russian National Library in Saint Petersburg]
*[http://surname.litera-ru.ru/ A russian website offering a search engine in cyrillic for some city directories.]

[[Category:Directories]]
[[Category:History of Saint Petersburg]]
[[Category:Russian non-fiction books]]
[[Category:Media in Saint Petersburg]]
[[Category:1894 books]]
<=====doc_Id=====>:469
<=====title=====>:
Blue pages
<=====text=====>:
{{Unreferenced|date=December 2009}}
'''Blue pages''' are a [[telephone directory]] listing of American and Canadian state agencies,  [[government]] agencies, federal government and other official entities, along with specific offices, departments, or bureaus located wherein.

==Canada==
Canadian yellow-page listings currently indicate "Government Of Canada-See Government Listings In The Blue Pages"; in markets where the local telephone directory is a single volume, the blue pages and community information normally appear after the alphabetical white-page listings but before the yellow pages advertising. The blue page listings include both provincial and federal entities.{{cn|date=April 2013}}

==United States==
In the [[United States]], the blue pages included state, federal, and local offices, including [[service district]]s such as school districts, port authorities, public utility providers, parks districts, fire districts, and the like. The blue pages also provided information about government services, in addition to officials' names, addresses, telephone numbers, and other contact information. The color blue is likely derived from so-called government blue books, official publications printed by a government (such as that of a state) describing its organization, and providing a list of contact information. (The blue pages published in a printed telephone directory is usually quite abridged, compared to official blue books).

==Other==
The name "blue pages" has been used for various specialised directories by private-sector entities such as the internal IBM Staff directory. 

{{DEFAULTSORT:Blue Pages}}
[[Category:Telephone numbers]]
[[Category:Directories]]

{{telephony-stub}}
<=====doc_Id=====>:472
<=====title=====>:
Dalilmasr
<=====text=====>:
{{orphan|date=August 2010}}
'''Dalilmasr''' is an online [[Egyptian language|Egyptian]] directory, which provides information about any service, product, and/or tool available in the Egyptian market. The website includes information about companies, shops, showrooms, and service providers in the Egyptian Region.

In the five main languages it serves more than two billion people.
<br>'''Logo Identity:'''The logo is in two palms of the hand applauding (conducting the valued promised service) in black and red color along with white background (Egyptian Flag Colors), the other right and left acute blue triangles meaning the Nile River welfare.
<ref>[[Al-Ahram|Al-Ahram newspaper]] in large two columns, says Dalilmasr is the first Egyptian search engine focused on contents rather only information and care about all Egyptian cities & suburbs. Issued on June 15, 2010 - page 22.</ref>
<ref>[[Akhbar El Yom]] newspaper considered Dalilmasr as best Egyptian directory, appendix 3-page 6 on June 12, 2010</ref>
<ref>[http://www.minia.edu.eg/doHtml.aspx?page=useful-sites.html Minia University's] directory proposed Dalilmasr as one of the recommended Egyptian websites.
</ref>
<ref>Wikiwak indexed Dalilmasr since website domain was dalil-masr.com</ref>
<ref>Google rank it on the top for best Egyptian directory keyword [http://www.google.com.eg/search?hl=en&q=best+egyptian+directory&aq=0&aqi=g3&aql=&oq=best+egyptian+d&gs_rfai= Search Result]</ref>

== References ==
{{Reflist}}

== External links ==
* [http://www.dalilmasr.com Dalimasr website]

[[Category:Directories]]
<=====doc_Id=====>:475
<=====title=====>:
City directory
<=====text=====>:
{{Multiple issues|
{{Refimprove|date=July 2015}}
{{Citation style|date=July 2015}}
}}

A '''city directory''' is a listing of residents, streets,  businesses, organizations or institutions, giving their location in a [[city]].  Antedating [[telephone directories]], they have been in use for centuries.

Examples include [[Kelly's Directory]] and the [[Boston Directory]].

==See also==
* [[:de:Adressbuch]]

==References==
{{refbegin}}
*{{cite journal |title=How Reliable is the Modern City Directory? | volume= 30| issue =2| pages =154–158|date=June 1986 |journal=Canadian Geographer |author=Richard Harris, Ben Moffat |doi=10.1111/j.1541-0064.1986.tb01040.x}}
*{{cite web |url=http://www.ancestry.com/learn/library/article.aspx?article=4062 |title= City vs. Telephone Directories |work=[[Ancestry.com]] |author=George G. Morgan}}
*{{cite book 
|author=A. V. Williams 
|title=The Development and Growth of City Directories 
|publisher=
|location=Cincinnati
|year=1913 
|url=http://catalog.hathitrust.org/Record/008698693
}}
*{{cite book |author=Florence May Hopkins |title=Reference Guides that Should be Known and how to Use Them: Atlases; City Directories; Gazetteers  |publisher=The Willard Company |location= |year=1919  |pages= |isbn= |url=https://books.google.com/books?id=SPEVAAAAIAAJ |doi= |accessdate=}}
{{refend}}

==Further reading==
* {{citation |title=Direct Me NYC 1786: A History of City Directories in the United States and New York City |author= Philip Sutton |year=2012 |publisher=New York Public Library |url= http://www.nypl.org/blog/2012/06/08/direct-me-1786-history-city-directories-US-NYC |work=NYPL Blogs }}

[[Category:Directories]]
<=====doc_Id=====>:478
<=====title=====>:
Who Owns Whom
<=====text=====>:
'''''Who Owns Whom''''' is a set of annual directories published by GAP Books in association with [[Dun & Bradstreet]] (D&B).  They provide the relationship between companies worldwide showing who is the ultimate [[parent company]] and who are their [[subsidiaries]].  Details include parent name, address and telephone number, country of incorporation and [[SIC code]] for each ultimate parent company along with the names of the subsidiaries, where they are based, and who owns whom for each subsidiary.  The set of directories are broken down into seven geographic regions: UK & Ireland; West Europe; North Europe; South, Central & East Europe; North & South America; Australasia, Asia, Africa & Middle East.

''Who Owns Whom'' was first published in 1958 by D&B, who still own the rights to the data.  Within the set of ten directories are listed approximately 2,500,000 [[corporate groups]], ranging from companies with hundreds of members across all continents to single-country groups with only a handful of members.

==Criteria for entry==
In order to maintain full coverage, all entries in ''Who Owns Whom'' are published entirely [[free of charge]].  Every effort is made by D&B, who supply the data, to include all corporate groups within the coverage area.  The corporate group may be a vast [[multinational corporation|multinational]] with a range of subsidiaries spanning many different countries and industries, or it may consist of two companies – a parent and a subsidiary.  There is no size criterion for entry.  All corporate groups, which come to the attention of D&B, are included regardless of their [[revenue|sales turnover]] or number of employees.  Companies and organisations are only included if ownership is greater than 50% by other companies or organisations.

==Coverage==
All types of industries are covered, including [[agriculture]], [[forestry]] and [[fishing]], [[mining]], [[construction]], [[manufacturing]], [[transportation]], [[communication]] and [[public utilities]], [[wholesale]], [[retail]], [[finance]] and [[insurance]], [[public services|services]] and [[public administration]].  Both public and private companies are covered, along with many companies owned by official bodies such as governments, nationalised industries and state holding companies which have subsidiaries, but are not themselves independently registered.

==External links==
* http://library.dialog.com/bluesheets/html/bl0522.html
* http://www.gapbooks.com/shop/dandb-who-owns-whom/
* http://www.loc.gov/rr/business/duns/duns30.html

[[Category:Directories]]
[[Category:Yearbooks]]
[[Category:Corporate subsidiaries]]
[[Category:Books about multinational companies]]


{{globalization-book-stub}}
{{Ref-book-stub}}
<=====doc_Id=====>:481
<=====title=====>:
Almanach de Gotha
<=====text=====>:
{{Multiple issues|
{{refimprove|date=July 2014}}
{{primary sources|date=July 2014}}
}}
{{italic title}}
{{Use dmy dates|date=June 2013}}	
{{Infobox book series
| name             = Almanach de Gotha
| image            =  DeGotha1851.jpg
| image_caption    = The Almanach de Gotha  1851
| books            = 
| author           =
| editors          = 
| title_orig       = 
| translator       = 
| illustrator      = 
| cover_artist     = 
| country          =
| language         =
| genre            =
| discipline       =
| publisher        = [[Johann Christian Dieterich|J.C. Dieterich]]<br>C.W. Ettinger<br>C.G. Ettinger<br>[[Justus Perthes (publishing company)|Justus Perthes]]<br>Almanach de Gotha, Ltd.
| pub_date         = 1763-1944<br>1998-
| english_pub_date = 1998-
| media_type       = 
| number_of_books  = 
| list_books       = 
| preceded by      = 
| followed by      = 
}}

The '''''Almanach de Gotha''''' was a directory of Europe's [[Royal family|royalty]] and higher [[nobility]], also including the major [[government]]al, [[military]] and [[diplomatic corps|diplomatic]] [[corps]], as well as statistical data by country. First published in 1763 by C.W. Ettinger in [[Gotha (town)|Gotha]] at the [[duke|ducal]] [[court]] of [[Frederick III, Duke of Saxe-Gotha-Altenburg|Frederick III]], Duke of [[Saxe-Gotha-Altenburg]], it came to be regarded as an authority in the classification of monarchies and their courts, reigning and former dynasties, princely and ducal families, and the genealogical, biographical and titulary details of Europe's highest level of aristocracy. It was published from 1785 annually by [[Justus Perthes (publishing company)|Justus Perthes]] Publishing House in Gotha, until 1944 when the [[Soviet]]s destroyed the ''Almanach de Gotha's'' archives.

In 1998, a London-based publisher acquired the rights for use of the title of ''Almanach de Gotha'' from Justus Perthes Verlag Gotha GmbH. Perthes regard the resultant volumes as new works, and not as a continuation of the editions which Perthes had published from 1785 to 1944.<ref name=Perthes>{{Cite web|url=http://www.perthes.de/geschichte_justus_perthes/almanach_de_gotha/almanach_de_gotha_english.html |title=Almanach de Gotha |accessdate=9 June 2008 |publisher=Justus Perthes }}</ref> Two volumes have been printed since 1998, with Volume I containing lists of the sovereign, formerly sovereign and mediatised houses of Europe, and a diplomatic and statistical directory; and Volume II containing lists of the non-sovereign princely and ducal houses of Europe.

==Gotha publication, 1763–1944==
The original ''Almanach de Gotha'' provided detailed facts and statistics on nations of the world, including their [[reign]]ing and formerly reigning houses, those of [[Europe]] being more complete than those of other continents. It also named the highest incumbent [[Great Officer of State (disambiguation)|officers of state]], members of the [[diplomatic corps]], and Europe's upper nobility with their families. Although at its most extensive the ''Almanach'' numbered more than 1200 pages, fewer than half of which were dedicated to monarchical or aristocratic data,<ref name="gotha">Almanach de Gotha. [[Justus Perthes]], Gotha, 1944, pp. 7-12, 131, 169, 363-364, 558, 581-584. French.</ref> it acquired a reputation for the breadth and precision of its information on royalty and nobility compared to other [[almanac]]s.<ref name="diesbach">{{Cite book|title=Secrets of the Gotha|last=de Diesbach|first=Ghislain|authorlink = Ghislain de Diesbach|year=1967|publisher=Chapman & Hall|location=UK|pages=21, 23–24, 28–30}}</ref>
[[File:London Library book, Gothaisches Genealogisches Taschenbuch der Freiherrlichen Häuser, 1910, Justus Perthes, Gotha.jpg|thumb|[[London Library]]'s copy of ''Gothaisches Genealogisches Taschenbuch der Freiherrlichen Häuser'', 1910.]]
The ''Almanach'''s publication by [[Justus Perthes]] began at the ducal court of [[Saxe-Coburg and Gotha]] in Germany and, its reigning dynasty was listed first therein well into the 19th century, usually followed by kindred sovereigns of the [[House of Wettin]] and then, in alphabetical order, other families of princely rank, ruling and non-ruling. Although always published in French, other almanacs in French and English were more widely sold internationally. The almanac's structure changed and its scope expanded over the years. The second portion, called the ''Annuaire diplomatique et statistique'' ("Diplomatic and Statistical Yearbook"), provided [[demography|demographic]] and governmental information by nation, similar to other [[almanac]]s. Its first portion, called the ''Annuaire généalogique'' ("Genealogical Yearbook"), came to consist essentially of three sections: reigning and formerly reigning families, [[mediatization|mediatized families]] and non-sovereign families at least one of whose members bore the title of prince or duke.<ref name="diesbach"/>

The first section always listed Europe's [[sovereignty|sovereign]] houses, whether they ruled as emperor, king, grand duke, duke, prince (or some other title, e.g., [[prince elector]], [[margrave]], [[landgrave]], [[count palatine]] or [[pope]]). Until 1810 these sovereign houses were listed alongside such families and entities as Barbiano-Belgiojoso, Clary, Colloredo, Furstenberg, the Emperor, Genoa, Gonzaga, Hatzfeld, Jablonowski, Kinsky, Ligne, Paar, Radziwill, Starhemberg, Thurn and Taxis, Turkey, Venice and the [[Order of Malta]] and the [[Teutonic Knights]]. In 1812, these entries began to be listed in groups.<ref name="diesbach"/> First were German sovereigns who held the rank of grand duke or prince elector and above (the Duke of Saxe-Gotha was, however, listed here along with, but before, France—see below).

Listed next were Germany's reigning ducal and princely dynasties under the heading "College of Princes", e.g., [[Hohenzollern]], [[County of Isenburg|Isenburg]], [[Leyen]], [[Liechtenstein]] and the other [[Ernestine duchies|Saxon duchies]]. They were followed by heads of non-German monarchies, i.e. Austria, Brazil, Great Britain, etc. Fourthly were listed non-reigning dukes and princes, whether mediatized or not, including [[Arenberg]], [[House of Croÿ|Croy]], [[Fürstenberg (princely family)|Furstenberg]] alongside [[Batthyany]], [[Jablonowski]], [[Sulkowski]], Porcia and [[Prince of Benevento|Benevento]].

In 1841 a third group was added to those of the sovereign dynasties and the non-reigning princely and ducal families. It was composed exclusively of the mediatized families of comital rank recognized as belonging, since 1825, to the same historical category and sharing some of the same privileges as reigning dynasties by the various states of the [[German Confederation]]; these families were German with a few exceptions (e.g. [[Bentinck]], [[Van Rechteren|Rechteren-Limpurg]]). The 1815 treaty of the [[Congress of Vienna]] had authorized — and Article 14 of the German Confederation's ''Bundesakt'' (charter) recognized — retention from the [[Holy Roman Empire|German Imperial]] regime of [[Royal intermarriage|equality of birth]] for marital purposes of mediatized families (called ''Standesherren'') to reigning dynasties.<ref name="diesbach"/> The almanac added a third section consisting exclusively of mediatized families of comital rank.

In 1877, the mediatized comital families were moved from section III to section II A, where they joined the princely mediatized families. For the first time in the century of its existence, the largely non-German, un-mediatized princely and ducal families of the ''Almanach de Gotha'' were removed from the same section as other non-reigning families bearing princely titles.<ref name="diesbach"/> While non-mediatized German and Austrian families (e.g. [[Prince Lichnowsky|Lichnowsky]], [[Wrede]]), were likewise relocated from the almanac's second to its third section, the second section's new preponderance of German families, princely and comital, which were henceforth recognized as possessing the exclusive privilege of inter-marriage with reigning dynasties was salient:<ref name="diesbach"/> Excluded were members of such historically notable families as the [[House of Rohan|Rohan]]s, [[Orsini]]s, [[Duke of Ursel|Ursels]], [[Duke of Norfolk|Norfolks]], [[Czartoryski]]s, [[Galitzine]]s, [[Duc de La Rochefoucauld|La Rochefoucaulds]], [[House of Kinsky|Kinskys]], [[Radziwiłł family|Radziwills]], [[De Mérode|Merodes]], [[Dohna (Disambiguation)#People|Dohnas]] and [[Duke of Alba|Albas]].

Although theoretically mediatized families were distinguished from Europe's other nobility by the former status of their territories as ''[[Imperial State|Reichsstand]]'' and their exercise within the Holy Roman Empire of "semi-sovereignty" or [[imperial immediacy]] (''Reichsunmittelbarkeit''), many ''Standesherr'' families, especially those bearing the [[count|comital]] title, had not been fully recognized as legally possessing immediate status within the Empire prior to its collapse in 1806. No other families whose highest title was count were admitted to any section of the almanac.<ref name="diesbach"/>

Moreover, other [[deposition (politics)|deposed]] European dynasties (e.g. [[House of Arenberg|Arenberg]], [[Ernst-Johann Biron, Prince of Courland|Biron]], [[Dadiani]], [[Boncompagni]]-[[Ludovisi (family)|Ludovisi]], [[Giray dynasty|Giray]], [[House of Murat|Murat]]) did not benefit ''vis-a-vis'' the almanac from a similar interpretation of their historical status. Many princely or ducal families were listed only in its third, non-dynastic section or were excluded altogether, evoking criticism in the 20th century from such genealogists as [[Cyril Toumanoff]], [[Jean-Engelbert d'Arenberg|Jean-Engelbert, Duke d'Arenberg]] and [[William Addams Reitwiesner]],<ref>Fra Cyril Toumanoff, "Genealogical Imperialism" (1985) vol 6 (no 134) (NS) Coat of Arms pp. 145, 147.</ref><ref>Duke and Prince Jean Engelbert d'[[Arenberg]], "The Lesser Princes of the Holy Roman Empire in the Napoleonic Era" dissertation, Washington, DC, 1950, published as Les Princes du St-Empire à l'époque napoléonienne (Louvain, 1951) 15ff, quoted in Almanach de Gotha (Almanach de Gotha, London, 1998) pp. 275–286.</ref> the latter commenting that the changes displayed "pan-German triumphalism" and even a "fairly nasty bit of Germanic chauvinism."<ref>{{Cite web|url= http://www.wargs.com/essays/mediatize.html |title= Mediatization |accessdate= 19 April 2011 |last= Reitwiesner |first= William Addams |date=January 1998 |authorlink= William Addams Reitwiesner}}</ref>

Even in the early 19th century the almanac's retention of [[deposition (politics)|deposed]] dynasties evoked objections, although not necessarily the desired changes. The elected Emperor [[Napoleon]] protested in writing to his foreign minister, [[Jean-Baptiste Nompère de Champagny|Champagny]]: <blockquote>''Monsieur de Champagny, this year's "Almanach de Gotha" is badly done. First comes the Comte de Lille [title used in exile by [[Louis XVIII of France|Louis de Bourbon, Count of Provence]] -- future King Louis XVIII of France], followed by all the princes of the [[Confederation of the Rhine|Confederation]] as if no change has been made in the constitution of Germany; the family of France is named inappropriately therein. Summon the Minister of Gotha, who is to be made to understand that in the next Almanach all of this is to be changed. The House of France must be referred to as in the [French] Imperial Almanac; there must be no further mention of the Comte de Lille, nor of any German prince other than those retained by the Articles of Confederation of the Rhine. You are to insist that the article be transmitted to you prior to publication. If other almanacs are printed in my allies' realms with inappropriate references to the Bourbons and the House of France, instruct my ministers to make it known that you have taken note, and that this is to be changed by next year.''<ref>{{Cite book|url= https://books.google.com/books?id=ScM3AQAAMAAJ&pg=PA124&dq=Napol%C3%A9on+13275+Champagny&hl=en&sa=X&ei=W7-5U8PHMpHooATfkYCoBg&ved=0CCEQ6AEwAA#v=onepage&q=Napol%C3%A9on%2013275%20Champagny&f=false |title= Correspondance de Napoléon I|volume=XVI |publisher=Imprimerie Impériale|date=1864|location=France|accessdate=6 July 2014}}</ref></blockquote>

The response of the publishers was to humour Napoleon by producing two editions: one for France, with the recently ennobled, and another which included dynasties deposed since abolition of the [[Holy Roman Empire]]. A merged version, whose first section including recently reigning dynasties but also families which lost sovereignty after the fall of Napoleon in 1815, remained in publication until 1944, and has been replicated in subsequent dynastic compilations (e.g., ''Genealogisches Handbuch des Adels, Fürstliche Häuser'', ''Le Petit Gotha'', Ruvigny's "Titled Nobility of Europe").

In 1887 the ''Almanach'' began to include non-European dynasties in its first section, with the inclusion of one of the ruling families of India.

===World War II and aftermath===
When Soviet troops entered [[Gotha (town)|Gotha]] in 1945, they systematically destroyed all archives of the ''Almanach de Gotha''.{{Citation needed|date=September 2010}}

In 1951 a different publisher, C.A. Starke, began publication of a multi-volume German-language publication entitled the ''Genealogisches Handbuch des Adels'' ([[:de:Genealogisches Handbuch des Adels|GHdA]]). The publication is divided into subsets; the ''Fürstliche Häuser'' subset is largely equivalent to the German language ''Gothaischer Hofkalender'' and its ''Fürstlichen Häuser'' volume which was also published by Perthes, or sections 1, 2 and 3 of the ''Almanach de Gotha''. However, no single volume of the ''Fürstliche Häuser'' includes all the families included in the ''Hofkalender'' or ''Almanach de Gotha''. It is necessary to use multiple volumes to trace the majority of European royal families.

==London publication, since 1998==
[[File:2014 Almanach de Gotha Covers.jpg|right|200px|thumb|''Almanach de Gotha'', 2014, Volumes I & II]]

In 1989 the family of [[Justus Perthes]] re-established its right to the use of the name ''Almanach de Gotha''. The family then sold these rights in 1995 to a new company, Almanach de Gotha Limited, formed in London.<ref>[https://www.thegazette.co.uk/notice/L-60158-1601227 Notice of Disclaimer]</ref> The new publishers launched with the 182nd edition on 16 March 1998 at [[Claridge's Hotel]].<ref>{{Cite web|url= http://www.almanachdegotha.com/site/modern.htm|title=The Modern Gotha |accessdate=30 May 2008 |publisher=Almanach de Gotha |archiveurl=https://web.archive.org/web/20060211154624/http://www.almanachdegotha.com/site/modern.htm |archivedate=11 February 2006}}</ref><ref>Jury, Louise. [http://www.independent.co.uk/news/upper-crust-toasts-aristocrat-studbook-1150076.html Upper crust toasts aristocrat studbook] The Independent (14 March 1998)</ref>  It was written in English instead of French as the editor felt that English was now the language of diplomacy.<ref name=Runciman>{{Cite news|first= Steven |last= Runciman |authorlink=Steven Runciman |title=The first book of kings |url=http://findarticles.com/p/articles/mi_qa3724/is_199805/ai_n8792875 |publisher=[[The Spectator]] |date=2 May 1998 |accessdate=6 June 2008 }}</ref> Charlotte Pike served as editor of the 1998 edition only and John Kennedy as managing director and publisher. The new publishers also revived the Committee of Patrons under the presidency of King [[Juan Carlos I of Spain]] and chairmanship of King [[Michael I of Romania]].<ref>{{Cite web|url= http://www.gotha1763.com/society.html|title=The Société des Amis de l'Almanach de Gotha |accessdate=1 May 2014 |publisher=Almanach de Gotha}}</ref>

The London publisher produced a further four editions of volume I (1999, 2000, 2003 and 2004) based on the 1998 edition of volume I which include Europe's and South America's reigning, formerly reigning, and mediatised princely houses, and a single edition of volume II in 2001 edited by John Kennedy and Ghislain Crassard which include other non-sovereign princely and ducal houses of Europe.<ref name=Hardman>{{Cite news|first=Robert |last=Hardman |title=Family almanac will unmask the noble pretenders |url=http://www.telegraph.co.uk/news/worldnews/europe/germany/1317982/Family-almanac-will-unmask-the-noble-pretenders.html |publisher=[[Daily Telegraph]] |date=19 June 2001 |accessdate=6 June 2008 }}</ref> A review in ''[[The Economist]]'' criticised the low editorial standards and attacked volume II for a lack of genealogical accuracy.<ref name=review_economist>{{Cite web| title=The Almanach de Gotha -- Gothic horror | url=http://www.economist.com/books/displayStory.cfm?Story_ID=949183 | publisher=[[The Economist]] | date = 24 January 2002 | accessdate=7 October 2007}}</ref> After a gap of eight years a new edition of volume I was published in 2012 under the editorship of John James.<ref>{{Cite web|url=http://www.boydellandbrewer.com/store/viewitem.asp?idproduct=13798 |title=Almanach de Gotha 2012. Volume I, parts I & II |accessdate=8 February 2012 |publisher=[[Boydell & Brewer]] }}</ref> A review in ''[[The Times Literary Supplement]]'' praised the 2012 volume I for a "punctilious itemization of titles, lineage and heraldry [aiming] for scholarship rather than sensation...Some family legends&nbsp;– such as the Ottoman boast of descent from a grandson of Noah&nbsp;– do not merit inclusion in a work with authoritative aspirations. Most quixotically of all, the title page displays the word 'Annual', although it has been eight years since the last edition appeared."<ref>{{Cite web|url=http://www.the-tls.co.uk/tls/public/article1139358.ece |title=And dark the Sun and Moon, and the Almanach de Gotha... |accessdate=2013-01-17 |publisher=The Times Literary Supplement |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20130117050945/http://www.the-tls.co.uk/tls/public/article1139358.ece |archivedate=17 January 2013 |df=dmy }}</ref>

==Structure==
As it was the practice of the diplomatic corps to employ official titles, adhere to local [[Order of precedence|precedence]] and etiquette, and to tender congratulations and condolences to members of the dynasty of the nation to which they were assigned, the almanac included a ''Calendrier des Diplomates'' ("Diplomats' Calendar") section, which detailed major national holidays, anniversaries, ceremonies and royal birthdates.<ref name="gotha"/>

Following [[World War I]] and the fall of many [[royal house]]s, fewer regulatory authorities remained to authenticate use of titles; however the ''Almanach de Gotha'' continued the practice of strict verification of information, requesting certified copies of [[letters patent]], genealogies confirmed by competent authorities, documents, decrees and references for titles claimed.<ref name="gotha"/> Europe's middle and lower nobility (families whose principal title ranked below that of prince or duke&nbsp;— except [[German mediatisation|mediatized]] families, listed in a section of their own) were not included in the almanac. Nor were the [[grandee]]s or [[Portuguese dukedoms|ducal families]] of Portugal and Spain (where titles, being habitually transmissible through both male and [[cognatic|female lines]], were often inherited by relatives of non-[[patrilineality|patrilineal]] lineage). Families of some Italian and East European nations (e.g., Russia, Romania), where the princely title was claimed by many, were also incomplete. Yet the reigning, formerly reigning and noble families included in the almanac numbered in the hundreds by the time it ceased publication in 1944.<ref name="gotha"/>

In 1890 the almanac renamed II A to section II, and II B to section III. Dynasties ruling non-European nations were located in section I B. Families which became extinct were listed for the final time in the year following death of the last member, male or female, and subsequent editions referred readers to that volume.<ref name="gotha"/>

Families that ceased to be included for other reasons, such as lack of proof of a family's legitimate descendants or discovery that it did not hold a valid princely or ducal title, were henceforth excluded but added, along with dates of previous insertion, to a list following the last section of each ''Annuaire Genealogique'' (Genealogical Yearbook), which page was entitled ''Liste des Maisons authrefois publiees dans la 3e partie de l'Almanach de Gotha'' ("List of Houses formerly published in the 3rd section of the ''Almanach de Gotha''.") <ref name="gotha"/>

From 1927, the almanac ceased to include all families in each year's edition, henceforth rotating entries every few years. Where titles and [[style (manner of address)|style]]s (such as [[Serene Highness]]) had ceased to be recognized by national governments (e.g. Germany, Austria, Czechoslovakia), the almanac provided associated dates and details, but continued to attribute such titles and styles to individuals and families, consistent with its practice since the [[French revolution]]; deposed sovereigns and dynasties continued to be accorded their former titles and rank, but dates of deposition were noted,<ref name="diesbach"/> and titles exclusively associated with sovereignty (e.g. emperor, queen, grand duke, crown princess) were not accorded to those who had not borne them during the monarchy. Titles of [[pretender|pretence]] below sovereign rank were accorded to members of formerly reigning dynasties as reported by heads of their houses, otherwise self-assumed titles were not used. The almanac included an explicit disclaimer announcing that known biographical details, such as birthdates and divorces, would not be suppressed.<ref name="gotha"/>

==See also==
*[[Burke's Peerage]]
*[[Debrett's|Debrett’s Peerage & Baronetage]]

==References==
{{Reflist|2}}

==Further reading==
*[[Ghislain de Diesbach|Diesbach, Ghislain de]]. ''Secrets of the Gotha''. Meredith Press, 1964.

==External links==
*[http://gallica.bnf.fr/ Scanned versions of the old almanachs]
*[https://archive.org/search.php?query=almanach+de+gotha%20AND%20mediatype%3Atexts Almanach de Gotha at Internet Archive]
*[http://www.gotha1763.com/ Almanach de Gotha]

{{DEFAULTSORT:Almanach De Gotha}}
[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Publications established in 1763]]
[[Category:Almanacs]]
<=====doc_Id=====>:484
<=====title=====>:
Navy List
<=====text=====>:
{{for|a list of countries with navies|List of navies}}
{{multiple issues|
{{more footnotes|date = March 2013}}
{{Globalize|date=May 2009}}
}}

A '''Navy List''' or '''Naval Register''' is an official list of [[navy|naval]] officers, their ranks and seniority, the ships which they command or to which they are appointed, etc., that is published by the government or naval authorities of a country.

==Background==
The Navy List fulfills an important function in [[international law]] in that warships are required by article 29 of the [[United Nations Convention on the Law of the Sea]] to be commanded by a [[commissioned officer]] whose name appears in the appropriate service list.{{why|date=June 2016}}

Past copies of the Navy List are also important sources of information for historians and genealogists.

The Navy List for the Royal Navy is no longer published in hard-copy.

The [[Royal Navy]] (United Kingdom) publishes annual lists of active and reserve officers, and biennial lists of retired officers. The equivalent in the [[United States Navy]] is the Naval Register, which is updated online on a continuous basis.  When a ship is removed from the [[Naval Vessel Register]] in the United States, or from a Naval List of any other country, the ship is said to be "[[:wikt:stricken|stricken]]".<ref>Edwards, Paul.  ''[https://books.google.com/books?id=OydzBgAAQBAJ&pg=PA37 Small United States and United Nations Warships in the Korean War]'', p. 37 (McFarland, 2008).</ref>

== Resources ==
Good sources of historical data on UK's Navy Lists are
*The Naval Historical Branch, Portsmouth Naval Base.
*The Central Library Portsmouth, Guildhall Square.
*[[The National Archives (United Kingdom)|The National Archives]], Kew, that has an almost complete set including unpublished editions produced during the Second World War for internal use by the Admiralty.
*The Caird Library of the [[National Maritime Museum]] has in its collection bound monthly lists published by the Admiralty, and the concurrently published Steel's lists

The current editor of the Navy List is Cliona Willis

== Bibliography ==

* ''The 1766 Navy List'', Edited by E. C. Coleman, Published by Ancholme Publishing, ISBN 0-9541443-0-9

==See also==
* [[Army List]]
* [[Naval Vessel Register]]

==References==
<references />

== External links ==
* [http://www.royalnavy.mod.uk/~/media/royal%20navy%20responsive/documents/useful%20resources/navy%20list.pdf Navy List 2013]
* [https://navalregister.bol.navy.mil US Naval Register] (US Navy)
* [http://www.NavyListResearch.co.uk Navy List Research] (Royal Navy)

[[Category:Royal Navy]]
[[Category:United States Navy]]
[[Category:Directories]]
<=====doc_Id=====>:487
<=====title=====>:
Clerical Guide or Ecclesiastical Directory
<=====text=====>:
{{italic title}}
The '''''Clerical Guide or Ecclesiastical Directory''''' was the earliest ever specialist directory to cover the clergy of the [[Church of England]]. In its initial format it appeared just four times – in 1817, 1822, 1829 and 1836, under the editorial direction of [[Richard Gilbert (printer)|Richard Gilbert]].

Another edition was actually advertised for 1838,<ref name="paflin">[http://www.churchtimes.co.uk/content.asp?id=48255] [[Church Times]]: two-part article ''Shop-talk and mordant wit'', by Christopher Currie & Glyn Paflin, describing the background to [[Crockford's Clerical Directory]]'s first hundred editions, 6–13 December 2007</ref> but no copies have in fact been found within the main academic libraries.

The title was briefly revived by Thomas Bosworth & Company during the 1880s.

==Contents of the Clerical Guide==

The main alphabetical section of the directory included:

*A list of benefices together with their populations, counties, dioceses and  archdeaconries
*Their incumbents with the year of his institution
*Their values (up to the 1829 edition) in the [[Valor Ecclesiasticus]] or King's Books
*The names of their patrons.
*The 1836 edition additionally gave the income of the benefice during the year 1831, the available capacity or "church room" for the congregation, and the name of any [[impropriator]].

The preliminary pages included:

*Current lists of [[bishops]], members of [[cathedral chapter]], and other dignitaries, showing the values of their [[first fruits]]
*A section on the Doctors of Laws, the [[canon law|canonical]] specialists
*A section on the [[Chapel Royal]] together with the king's preachers and chaplains
*Sections on [[Sion College]] and [[Gresham College]]
*Sections on the two English universities ([[University of Oxford|Oxford]] and [[University of Cambridge|Cambridge]])
*Sections on the fellows and schoolmasters of [[Eton College|Eton]], [[Winchester College|Winchester]], [[Westminster School|Westminster]], [[Harrow School|Harrow]], [[Manchester Grammar School|Manchester]] and [[St Paul's School, London|St Paul's]].

The alphabetical list of benefices was also followed by an alphabetical list of the prelates, dignitaries and beneficed clergy of the Church of England (generally omitting the unbeneficed clergy).

The directories concluded with lists of ecclesiastical patronage, giving the names of those benefices within the gift of the king and also those of the lord chancellor, the chancellor of the duchy of Lancaster, the various archbishops and bishops, and the two universities.

==The publishers==

The 1817 edition stated that it was "printed for [[Rivington (publishers)|J. C. & F. Rivington]], 62 St Paul's Churchyard, by R. & R. Gilbert, St John's Square, [[Clerkenwell]]".  '''Richard Gilbert''' was a printer and an accountant with the [[SPCK]].  Although he appeared in the 1817 edition merely as the "printer" (alongside his brother Robert, who died the following year), he thereafter seems to have taken a more prominent role in its production.  The 1822 edition was "corrected by Richard Gilbert", as though he had been engaged in putting right someone else's mistakes.  He similarly wrote the prefaces for subsequent editions, and the 1836 edition still bore the names "Gilbert and Rivington, printers, St John's Square".

Gilbert, an industrious compiler who was additionally very active in the religious life of Clerkenwell, also produced a pocket-sized '''Clergyman's Almanack''' in 1819 <ref>Oxford Dictionary of National Biography: article on Richard Gilbert</ref>

==The Clerical Guide after 1836==

The failure of the directory to appear after 1836 left open an opportunity for a rival publication.  This was filled after 1841 by the [[Clergy List]].

After lying dormant for fifty years, the title '''Clerical Guide and Ecclesiastical Directory''' was briefly revived in 1886 by Thomas Bosworth & Company, 65 [[Great Russell Street]]. Once again the volume offered alternative listings of the clergy and the benefices, together with other "valuable information … from the office of the [[Ecclesiastical Commission (Church of England)|Ecclesiastical Commission]].<ref>The Times newspaper, Thursday, Mar 18, 1886; pg. 12</ref>  However the relaunched title was very quickly acquired by Hamilton Adams of [[Paternoster Row]], who in 1889 merged it with their other recent acquisition, the aforementioned Clergy List.<ref name="paflin" />

In the issue for 1918/19 the Clergy List was merged in its turn with [[Crockford's Clerical Directory]]. Thereafter until the 1930s the latter title still continued to advertise on its preliminary pages that it "incorporated the Clergy List", together with the "Clerical Guide and Ecclesiastical Directory".

A microfiche version of the 1829 directory was produced during the 1980s by the [[Society of Genealogists]]. In more recent years scanned copies of the early editions have also appeared on the World Wide Web.<ref>All four editions of the Clerical Guide from 1817-1836 may be downloaded free of charge from the Google eBookstore [https://books.google.com/ebooks]</</ref>

==See also==
*[[Clergy of the Church of England database]]

==References==
{{reflist}}

[[Category:Directories]]
[[Category:Church of England]]
[[Category:Church in Wales]]
[[Category:Scottish Episcopal Church]]
[[Category:Anglicanism]]
<=====doc_Id=====>:490
<=====title=====>:
Almanach de Bruxelles
<=====text=====>:
{{one source|date=July 2012}}
The '''''Almanach de Bruxelles''''' is a [[Belgian]] website that lists [[royal family|royal]] and [[nobility|noble]] [[dynasties]] out of [[Europe]] in the form of a database. 

It was established in 1996 and lists around 2,690 world dynasties.<ref>créé en 1996, est le site de référence des dynasties en dehors de l'Europe...2.690 dynasties, beaucoup d'entre elles introuvables ailleurs [http://www.almanach.be/about/index.htm About the ''Almanach'']</ref>

==See also==
* ''[[Almanach de Gotha]]''
* ''[[Almanach de Bruxelles (defunct)]]''

==Sources==
{{reflist}}

==External links==
*{{Official|www.almanach.be}}

[[Category:Directories]]
[[Category:Biographical dictionaries]]
[[Category:European nobility]]
[[Category:Genealogy publications]]


{{website-stub}}
{{royal-bio-book-stub}}
{{bio-dict-stub}}
<=====doc_Id=====>:493
<=====title=====>:
Novel & Short Story Writer's Market
<=====text=====>:
{{italic title}}
[[File:Mosko.jpg|thumb|200px|right|''Novel & Short Story Writer's Market'']]

'''''Novel & Short Story Writer's Market''''' (''NSSWM'') is an annual resource guide for fiction writers that compiles hundreds of listings for book publishers, magazines literary agents, writing contests, and conferences. ''NSSWM'' is published by [[Writer's Digest Books]] and usually hits bookstores around August of each year.

==The Market Listings==
For 26 years, ''NSSWM'' has listed hundreds of U.S. and international magazines and book publishers who are open to submissions from fiction writers. Listings provide current contact information, editorial needs, schedules, submission guidelines, and payment and contract terms. All listings are updated annually.

==The Articles==
In addition to the market listings, the book contains interviews with and essays by best-selling and award-winning writers, as well as editors and agents.

==Writer's Digest Books==
[[File:TheFaulknerPortable.jpg|350px|right|thumb|A copy of the 1939 edition of ''Writer's Market'' rests next to William Faulkner's [[Underwood Typewriter Company|Underwood]] Universal Portable typewriter in his office at his home, [[Rowan Oak]], which is now maintained by the [[University of Mississippi]] in [[Oxford, Mississippi|Oxford]] as a museum.]]''Novel & Short Story Writer's Market'' is one of eight "[[Market (economics)|market]] books" published each year by [[Writer's Digest Books]] - the most famous of which is ''[[Writer's Market]]'', a book that lists thousands of magazine and book publishers listings for writers. Others include: ''Photographer's Market'', ''Children's Writer's and Illustrator's Market'', ''Guide to Literary Agents'', ''Artist and Graphic Designer's Market'', ''Poet's Market'' and ''Songwriter's Market''. Each book is designed to give creatives instructions on how to submit work for publication.

==See also==
* [[Publishing]] 
* ''[[Writer's Digest]]''
* ''[[Writer's Market]]''
* ''[[Writers' & Artists' Yearbook]]''
* [[Literary agent#Querying|query]]
* [[royalties]]
* [[Authors Guild]]

==External links==
* [http://www.writersdigest.com/competitions Official site for the competitions of Writer's Digest Books]
* [http://www.writersdigest.com ''Writer's Digest'' magazine official site]
* [http://www.fwpublications.com F+W Publications - parent company of Writer's Digest Books]

{{DEFAULTSORT:Novel and Short Story Writer's Market}}
[[Category:Directories]]
<=====doc_Id=====>:496
<=====title=====>:
Association of Directory Publishers
<=====text=====>:
The '''Association of Directory Publishers''' (ADP), is an international [[trade association]] founded in 1898 and is headquartered in [[Traverse City, Michigan]].<ref name=associations2012>{{cite book |title= Encyclopedia of Associations |issn=0071-0202 |volume= 1 |edition=51st |year= 2012 |page= 318 |via=[[Boston Public Library]] Reference & Reader's Advisory Department }}<!--|accessdate=April 8, 2013 --></ref> 

==About==
ADP is the oldest international trade association serving the Yellow Pages industry.  The Association represents the various interests of its membership which includes publishers of print, online and mobile directories, Certified Marketing Representatives (CMRs), advertising agencies and suppliers to the Yellow Pages and local search industry.

ADP represents the $35 billion Yellow Pages industry known as the original "local search engine" that brings buyers to sellers at the exact moment they are ready to buy.

The Association helps its members expand their businesses by offering them services and tools targeted to assisting them in achieving their clients' advertising objectives. ADP offers a wide variety of research, marketing and sales materials created with information from leading organization that are developed specifically to help members increase their company's bottom line.

ADP is a unique Association because of the governance structure of one company, one vote.  Every publisher from the smallest to largest has an equal opportunity to determine the leadership and direction of the Association.  ADP represents member companies of all sizes and from numerous countries.

==History==
The group formed in 1898 as the '''Association of American Directory Publishers,''' headquartered in New York. It aimed "to improve the [[Reference work|directory]] business."<ref>{{citation |url=https://books.google.com/books?id=gt7UAAAAMAAJ |year=1908 |work=Boyd's Directory of Harrisburg |title=(Advertisement for the Association of American Directory Publishers)}}</ref> It changed its name to the '''Association of North American Directory Publishers''' in 1919.<ref>{{citation |title=Printers' Ink |location=NY |date=September 11, 1919 }}</ref><ref>{{cite web |url=http://www.worldcat.org/identities/lccn-no2011-132108 |title=Association of North American Directory Publishers |work=WorldCat |publisher=[[OCLC]] |accessdate=April 5, 2013 }}</ref> It has held annual meetings starting in 1899 and has published the ''Directory Bulletin''.<ref>{{citation |title=Directory Bulletin |volume =1 |year=1901 |location=Milwaukee |publisher=Association of American Directory Publishers |url=https://books.google.com/books?id=2zTZAAAAMAAJ }}</ref> Officers have included George W. Overton and [[Ralph Lane Polk]].<ref name=members1921 /> Among the members in the 1920s:<ref name=members1921>{{citation |chapter=Members of Association of North American Directory Publishers |year=1921 |url=https://books.google.com/books?id=qG4UAAAAYAAJ&pg=PA480 |title=Manchester Directory |publisher=Sampson & Murdock Co. }}</ref>

{{Col-begin}}
{{Col-1-of-3}}
* Action Pages
* Atkinson Erie Directory Company
* Atlanta City Directory Company
* W.H. Boyd Company
* Burch Directory Company
* Caron Directory Company
* Chicago Directory Company
* J.W. Clement Company
* Cleveland Directory Company
* Connelly Directory Company
* Fitzgerald Directory Company
* Gate City Directory Company
* Hartford Printing Company
* Henderson Directories Ltd.
* Hill Directory Company
{{Col-2-of-3}}
* C.E. Howe Company
* Kimball Directory Company
* Leshnick Directory Company
* Los Angeles Directory Company
* John Lovell & Son Ltd.
* McCoy Directory Company
* H.A. Manning Company
* Maritime Directory Company
* Henry M. Meek Publishing Company
* Might Directories Ltd.
* Minneapolis Directory Company
* Piedmont Directory Company
* [[R.L. Polk & Company]]
{{Col-3-of-3}}
* Polk-Gould Directory Company
* Polk-Husted Directory Company
* Polk-McAvoy Directory Company
* Polk's Southern Directory Company
* Portland Directory Company
* Price & Lee Company
* W.L. Richmond
* Roberts Bros Company
* Sampson & Murdock Company
* Soards Directory Company 
* Utica Directory Publishing Company
* Williams Directory Company<ref>{{Citation |publisher = Williams Directory Co. |publication-place = Cincinnati, Ohio |author = A.V. Williams |url = http://hdl.handle.net/2027/nyp.33433082423645 |title = The development and growth of city directories |publication-date = 1913 }}</ref>
* John F. Worley Directory Company
* Wright Directory Company
{{Col-end}}

In 1992 the group renamed itself the "Association of Directory Publishers."<ref name=associations2012 />

==References==
{{Reflist|30em}}

==Further reading==
* {{citation |title=Pacific Bell fends off feisty competitors seeking confidential Yellow Pages data |work=San Francisco Business Times |date=March 1, 1991 }}

==External links==
* [http://www.adp.org/ Official website]
* {{cite web |url=http://www.adp.org/committees |title=Our Industry: Timeline |publisher=Association of Directory Publishers }}

[[Category:Organizations established in 1898]]
[[Category:1898 establishments in the United States]]
[[Category:Professional associations based in the United States]]
[[Category:Publishing organizations]]
[[Category:Directories]]
[[Category:Yellow pages]]
<=====doc_Id=====>:499
<=====title=====>:
White's Directories
<=====text=====>:
{{italic title}}
'''''White's Directories''''' were a series of directory publications issued by William White of [[Sheffield]], England, beginning in the 1820s.<ref>{{citation |chapter=White's Directories (advert) |quote=Established 1822 |url= https://books.google.com/books?id=dMwUAAAAQAAJ&pg=PA83 |title=White's general and commercial directory of Hull |year=1882}}</ref><ref>{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =[[James Pigot|J. Pigot & Co.]] }}</ref> White began his career in publishing by working for [[Edward Baines (1774–1848)|Edward Baines]].<ref>{{cite journal |title=Locational Behaviour of Urban Retailing during the Nineteenth Century: The Example of Kingston upon Hull |author= M. T. Wild and G. Shaw |journal=Transactions of the Institute of British Geographers |number= 61 |year=1974 |jstor=621602 }}</ref>{{refn|group=nb|By the 1850s Sheffield had two professional directory publishers: William White (34 Collegiate Crescent, Broomhall Park) and Francis White (Broomhall Terrace, 104 Ecclesial New Road)<ref>{{cite book |title=Post office directory of Sheffield |year=1854 |publisher=Kelley and Co. |url=https://books.google.com/books?id=bO4NAAAAQAAJ }}</ref><ref>{{Citation |publisher = W. Satchell |publication-place = London |title = Book of British Topography: a Classified Catalogue of the Topographical Works in the Library of the British Museum Relating to Great Britain and Ireland |author = John Parker Anderson |publication-date = 1881 |chapter=Yorkshire: Sheffield |chapterurl=https://archive.org/stream/bookofbritishtop00andeuoft#page/327/mode/1up }}</ref>}}

==Notes==
{{reflist|group=nb}}

==References==
{{reflist}}

==Further reading==

=== 1820s-1830s ===
* {{cite book |title=History, directory, and gazetteer, of the counties of Durham and Northumberland, and the towns and counties of Newcastle-upon-Tyne and Berwick-upon-Tweed |location=Newcastle |publisher= Printed for W. White & Co. by E. Baines and Son |year= 1827–1828 |url= http://catalog.hathitrust.org/Record/009725890 }}
* {{cite book |title=Directory of the Borough of Leeds, the City of York, and the Clothing District of Yorkshire |location=Leeds |publisher= Printed for Wm. Parson & Wm. White by Edward Baines and Son |year= 1830 |url=http://catalog.hathitrust.org/Record/007973427 }}
* {{Citation |publisher = Printed for the author by R. Leader |title = History, Gazetteer, and Directory of Norfolk, and the City and County of the City of Norwich |url = http://openlibrary.org/books/OL20613547M/History_Gazetteer_and_Directory_of_Norfolk_and_the_City_and_County_of_the_City_of_Norwich_... |author = William White |publication-date = 1836 |oclc = 25166377 }}
** [https://archive.org/stream/historygazettee01whitgoog#page/n3/mode/2up 1845 ed.]
** [https://archive.org/stream/historygazetteer00whit#page/n5/mode/2up 1864 ed.]
* {{Citation |publisher = W. White |publication-place = Sheffield |author =William White |title = History, Gazetteer, and Directory, of the West-Riding of Yorkshire, with the City of York and Port of Hull |publication-date = 1837 |url=https://archive.org/stream/historygazetteer01whit#page/n5/mode/2up }}
** {{cite book |title=History, gazetteer and directory of the East and North Ridings of Yorkshire |author=William White |location= Sheffield |publisher= Robert Leader for the author |year= 1840 |url=http://catalog.hathitrust.org/Record/011724851 }}

=== 1840s ===
* {{cite book |title=History, gazetteer, and directory of Suffolk, and the towns near its borders |location=Sheffield |publisher= Printed for the author by R. Leader and sold by W. White |year=1844 |url= http://catalog.hathitrust.org/Record/000194916 }}
** [http://catalog.hathitrust.org/Record/011595374 1874 ed.]
* {{Citation |url = http://openlibrary.org/books/ia:generaldirectory00whit/General_directory_of_the_town_and_borough_of_Sheffield_with_Rotherham_Chesterfield_and_all_the_paris |title = General directory of the town and borough of Sheffield |publication-date = 1845 |publisher = William White |location=Sheffield }}
* {{Citation |publisher = Printed for the author, by R. Leader |publication-place = Sheffield |author = William White |url = http://openlibrary.org/books/OL14012344M/History_gazetteer_and_directory_of_Leicestershire_and_the_small_county_of_Rutland |title = History, gazetteer, and directory of Leicestershire, and the small county of Rutland |publication-date = 1846 }}

=== 1870s ===
* {{cite book |title=History, gazetteer and directory of Lincolnshire, and the city and diocese of Lincoln |location=Sheffield |publisher= W. White |year= 1872 |url=http://catalog.hathitrust.org/Record/008912723 }}
* {{Citation |publisher = W. White |publication-place = Sheffield |title = History, gazetteer and directory of the county of Hampshire, including the Isle of Wight |url = http://catalog.hathitrust.org/Record/009009769 |publication-date = 1878 }}
* {{Citation |publisher = William White |publication-place = Sheffield |title = History, Gazetteer and Directory of the County of Devon including the City of Exeter |url = http://openlibrary.org/books/OL14012345M/History_gazetteer_and_directory_of_the_County_of_Devon_including_the_City_of_Exeter_and_comprising_a |publication-date = 1878 |edition=2nd }}

==External links==
* {{citation |title=Historical Directories |publisher=[[University of Leicester]] |location=UK |url=http://www.historicaldirectories.org/hd/findbykeyword.asp }}. Includes digitized White's directories, various dates
* {{citation |work=WorldCat |url=http://www.worldcat.org/wcidentities/lccn-n50-38455 |title=William White of Sheffield }}

[[Category:Directories]]
[[Category:Publications established in the 1820s]]


{{ref-book-stub}}
<=====doc_Id=====>:502
<=====title=====>:
Deutsches Geschlechterbuch
<=====text=====>:
{{italic title}}
The '''''Deutsches Geschlechterbuch''''', until 1943 known as the '''''Genealogisches Handbuch bürgerlicher Familien''''', is a major German genealogical handbook of [[Bourgeoisie|bourgeois]] or [[patrician (post-Roman Europe)|patrician]] families. It is the bourgeois and patrician equivalent of the ''[[Genealogisches Handbuch des Adels]]'' and the former ''[[Almanach de Gotha]]''. It includes genealogies and coats of arms of the included families. The ''Genealogisches Handbuch bürgerlicher Familien'' was started in 1889 and prior to 1943, 119 volumes covering around 1,200 families were published under the original title. From 1956, the series were continued under the title ''Deutsches Geschlechterbuch''. In 2007, the 219th and latest volume was published. In total, around 4,000 families have been covered.

The ''Hamburgisches Geschlechterbuch'', comprising 17 volumes on the [[Hanseaten (class)|Hanseatic]] families of Hamburg, is an integral part of the work, and is regarded as the most comprehensive reference work of its kind on a single city.<ref>Hildegard von Marchthaler: ''Die Bedeutung des Hamburger Geschlechterbuchs für Hamburgs Bevölkerungskunde und Geschichte'', in: ''Hamburgisches Geschlechterbuch'', Bd. 9, Limburg an der Lahn 1961, S. XXIII</ref>

The publication has been highly influential and inspired several similar publications, such as ''[[Nederland's Patriciaat]]''. To some extent it corresponds to ''[[Burke's Landed Gentry]]'' in the United Kingdom, although it could also be said to be the equivalent of ''[[Burke's Peerage]]'' in its coverage of [[Hanseaten (class)|Hanseatic]] and patrician families who comprised the highest class in the former city-republics.

==References==
{{reflist}}

==Bibliography==
*Genealogisches Handbuch bürgerlicher Familien (1889–1943)
*Deutsches Geschlechterbuch (1956-)

[[Category:Biographical dictionaries]]
[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Publications established in 1889]]


{{bio-dict-stub}}
<=====doc_Id=====>:505
<=====title=====>:
Category:Indexes
<=====text=====>:
An '''index''' is something that points the reader to other [[information]]. This category is for articles about indexes.
{{distinguish|Category:Index numbers}}
{{Cat more|Index (publishing)}}
{{For|Wikipedia content|Category:Wikipedia indexes}}

[[Category:Index (publishing)]]
[[Category:Publications]]
[[Category:Directories]]
<=====doc_Id=====>:508
<=====title=====>:
Category:Library cataloging and classification
<=====text=====>:
{{Commons cat|Library cataloging and classification}}

{{Cat main|Library catalog|Library classification|Inventory (library)}}

[[Category:Library science|Cataloging and classification]]
[[Category:Classification systems]]
[[Category:Metadata]]
[[Category:Directories]]
<=====doc_Id=====>:511
<=====title=====>:
Category:Web directories
<=====text=====>:
{{Cat main|Web directory|List of web directories}}
A [[web directory]] is a [[directory (databases)|directory]] on the [[World Wide Web]] that specializes in [[hyperlink|linking]] to other [[web site]]s and categorizing those links. This category includes online web directories.


[[Category:Websites|Directories]]
[[Category:Indexes]]
[[Category:Directories]]
[[Category:Online services|Directories]]
<=====doc_Id=====>:514
<=====title=====>:
Category:Lists
<=====text=====>:
{{Commons category|Information lists}}
{{Category see also|Timelines}}
{{Category diffuse}}

[[Category:Wikipedia navigation]]
[[Category:Directories]]
<=====doc_Id=====>:517
<=====title=====>:
Category:Metadata registry
<=====text=====>:
All Wikipedia articles in this category are either instances of a metadata registry or related to metadata registries.
{{Cat main|Metadata registry}}
[[Category:Metadata]]
[[Category:Directories]]
<=====doc_Id=====>:520
<=====title=====>:
Encyclopedia of Associations
<=====text=====>:
{{Italic title}}
The '''''Encyclopedia of Associations''''' (''EA'') is a comprehensive directory of more than 20,000 [[Voluntary associations|associations]], [[Society|societies]], and other non-profit membership organizations in the United States of America.<ref>[http://www.gale.cengage.com/DirectoryLibrary/GML33507EA%20GDL.pdf Encyclopedia of Associations]</ref>

Originally titled the ''Encyclopedia of American Associations'', ''EA'' was created by [[Frederick Gale Ruffner, Jr.]] in 1954 while working as a market researcher in [[Detroit, Michigan]].<ref>[http://lj.libraryjournal.com/2014/08/publishing/gale-founder-frederick-ruffner-dies-at-88/#_ "Gale Founder Frederick Ruffner Dies at 88" ''Library Journal''. – Retrieved October 3, 2014]</ref>

More than 140 scholarly articles have made use of ''EA''.<ref>[http://www.unc.edu/~fbaum/papers/JSTOR-EA-annotated-bibliography.pdf "An Annotated Bibliography of Articles Using the ''Encyclopedia of Associations''" - Retrieved October 3, 2014.]</ref>

Past extracts from ''EA'' have included "Organized Obsessions" <ref>[http://lccn.loc.gov/92219621 - Library of Congress LCCN Permalink for 92219621]</ref> and the "Gale Encyclopedia of Business and Professional Associations".<ref>[http://lccn.loc.gov/95649648 - Library of Congress LCCN Permalink for 95649648]</ref>

A detailed history of ''EA'' is available in an article in ''Distinguished Classics of Reference Publishing''<ref>[https://archive.org/stream/DistinguishedClassicsOfReferencePublishing#page/n101/mode/2up - Tobin, Carol M. "The Book that Built Gale Research: The ''Encyclopedia of Associations''."  ''Distinguished Classics of Reference Publishing'']</ref><ref>[http://lccn.loc.gov/91033629 - Library of Congress LCCN Permalink for 91033629]</ref>

== See also ==
* [[Gale Research]]
* [[Frederick Gale Ruffner, Jr.]]

== References ==
<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using<ref></ref> tags, these references will then appear here automatically -->
{{Reflist}}

== External links ==
* [http://gale.cengage.com/ Gale website]

<!--- Categories --->
[[Category:Directories]]
[[Category:Specialized encyclopedias]]
<=====doc_Id=====>:523
<=====title=====>:
Category:Australian directories
<=====text=====>:
{{portal|Australia}}
''Directories - of businesses, addresses and people in Australia''
[[Category:Directories]]
[[Category:Books about Australia]]
<=====doc_Id=====>:526
<=====title=====>:
Lighthouse Directory
<=====text=====>:
#REDIRECT [[Lists of lighthouses and lightvessels]]
{{R from merge}}
[[Category:Directories]]
<=====doc_Id=====>:529
<=====title=====>:
Category:Internet search engines
<=====text=====>:
{{Commons category|Internet search engines}}
General [[search engine (computing)|search engine]]s that search for information on the [[Internet]]. 

[[Category:Websites|Search engines]]
[[Category:Internet search]]
[[Category:Online databases]]
[[Category:Indexes]]
[[Category:Aggregation websites]]
[[Category:Search engine software]]
<=====doc_Id=====>:532
<=====title=====>:
Web search query
<=====text=====>:
A '''web search query''' is a query that a user enters into a [[web search engine]] to satisfy his or her [[information needs]]. Web search queries are distinctive in that they are often plain text or [[hypertext]] with optional search-directives (such as "and"/"or" with "-" to exclude). They vary greatly from standard [[query language]]s, which are governed by strict syntax rules as [[command language]]s with keyword or positional [[Parameter (computer science)|parameters]].

== Types ==
There are three broad categories that cover most web search queries: informational, navigational, and transactional.<ref>Broder, A. (2002). A taxonomy of Web search. SIGIR Forum, 36(2), 3–10.</ref> These are also called "do, know, go."<ref>{{cite web|last=Gibbons|first=Kevin|title=Do, Know, Go: How to Create Content at Each Stage of the Buying Cycle|url=http://searchenginewatch.com/article/2235624/Do-Know-Go-How-to-Create-Content-at-Each-Stage-of-the-Buying-Cycle|publisher=Search Engine Watch|accessdate=24 May 2014}}</ref> Although this model of searching was not theoretically derived, the classification has been  empirically validated with actual search engine queries.<ref>Jansen, B. J., Booth, D., and Spink, A. (2008) [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_user_intent.pdf Determining the informational, navigational, and transactional intent of Web queries], Information Processing & Management. 44(3), 1251-1266.</ref>

* '''Informational queries''' – Queries that cover a broad topic (e.g., ''colorado'' or ''trucks'') for which there may be thousands of relevant results.
* '''Navigational queries''' – Queries that seek a single website or web page of a single entity (e.g., ''youtube'' or ''delta air lines'').
* '''Transactional queries''' – Queries that reflect the intent of the user to perform a particular action, like purchasing a car or downloading a screen saver.

Search engines often support a fourth type of query that is used far less frequently:

* '''Connectivity queries''' – Queries that report on the connectivity of the indexed [[web graph]] (e.g., Which links point to this [[Uniform Resource Locator|URL]]?, and How many pages are indexed from this [[domain name]]?).<ref>{{cite web|last=Moore|first=Ross|title=Connectivity servers|url=http://nlp.stanford.edu/IR-book/html/htmledition/connectivity-servers-1.html|publisher=Cambridge University Press|accessdate=24 May 2014}}</ref>

== Characteristics ==

Most commercial web search engines do not disclose their search logs, so information about what users are searching for on the Web is difficult to come by.<ref>Dawn Kawamoto and Elinor Mills (2006), [http://news.cnet.com/AOL-apologizes-for-release-of-user-search-data/2100-1030_3-6102793.html AOL apologizes for release of user search data]</ref> Nevertheless, research studies appeared in 1998.<ref>Jansen, B. J., Spink, A., Bateman, J., and Saracevic, T. 1998. [https://faculty.ist.psu.edu/jjansen/academic/jansen_sigir_forum.pdf Real life information retrieval: A study of user queries on the web]. SIGIR Forum, 32(1), 5 -17.</ref><ref>Silverstein, C., Henzinger, M., Marais, H., & Moricz, M. (1999). Analysis of a very large Web search engine query log. SIGIR Forum,
33(1), 6–12.</ref> Later, a study in 2001<ref>{{cite journal|author1=Amanda Spink |author2=Dietmar Wolfram |author3=Major B. J. Jansen |author4=Tefko Saracevic | year = 2001 | title = [https://faculty.ist.psu.edu/jjansen/academic/jansen_public_queries.pdf Searching the web: The public and their queries] | journal = Journal of the American Society for Information Science and Technology | volume = 52 | issue = 3 | pages = 226–234 | doi = 10.1002/1097-4571(2000)9999:9999<::AID-ASI1591>3.3.CO;2-I }}</ref> analyzed the queries from the [[Excite]] search engine showed some interesting characteristics of web search:

* The average length of a search query was 2.4 terms. 
* About half of the users entered a single query while a little less than a third of users entered three or more unique queries. 
* Close to half of the users examined only the first one or two pages of results (10 results per page).
* Less than 5% of users used advanced search features (e.g., [[boolean operators]] like AND, OR, and NOT).
* The top four most frequently used terms were, '' (empty search), and, of, ''and'' sex.

A study of the same Excite query logs revealed that 19% of the queries contained a geographic term (e.g., place names, zip codes, geographic features, etc.).<ref>{{cite conference |author1=Mark Sanderson  |author2=Janet Kohler  |lastauthoramp=yes | year = 2004 | title = Analyzing geographic queries | booktitle = Proceedings of the Workshop on Geographic Information (SIGIR '04) | url =http://supremacyseo.com/analyzing-geographic-queries }}</ref> Studies also show that, in addition to short queries (i.e., queries with few terms), there are also predictable patterns to how users change their queries.<ref>Jansen, B. J., Booth, D. L., & Spink, A. (2009). [[Patterns of query modification during Web searchinhttps://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_patterns_query_reformulation.pdf|Patterns of query modification during Web searchin]]g. Journal of the American Society for Information Science and Technology. 60(3), 557-570. 60(7), 1358-1371.</ref>

A 2005 study of Yahoo's query logs revealed 33% of the queries from the same user were repeat queries and that 87% of the time the user would click on the same result.<ref>{{cite conference |author1=Jaime Teevan |author2=Eytan Adar |author3=Rosie Jones |author4=Michael Potts | year = 2005 | title = History repeats itself: Repeat Queries in Yahoo's query logs | booktitle = Proceedings of the 29th Annual ACM Conference on Research and Development in Information Retrieval (SIGIR '06) | pages = 703–704 | url =http://www.csail.mit.edu/~teevan/work/publications/posters/sigir06.pdf | doi=10.1145/1148170.1148326 }}</ref> This suggests that many users use repeat queries to revisit or re-find information. This analysis is confirmed by a Bing search engine blog post telling about 30% queries are navigational queries <ref>http://www.bing.com/community/site_blogs/b/search/archive/2011/02/10/making-search-yours.aspx</ref>

In addition, much research has shown that query term frequency distributions conform to the [[power law]], or ''long tail'' distribution curves. That is, a small portion of the terms observed in a large query log (e.g. > 100 million queries) are used most often, while the remaining terms are used less often individually.<ref name="baezayates1">{{cite journal | author = Ricardo Baeza-Yates | year = 2005 | title = Applications of Web Query Mining | booktitle = Lecture Notes in Computer Science | pages = 7–22 | volume = 3408 | publisher = Springer Berlin / Heidelberg | url = http://www.springerlink.com/content/kpphaktugag5mbv0/ | ISBN = 978-3-540-25295-5}}</ref> This example of the [[Pareto principle]] (or ''80–20 rule'') allows search engines to employ [[optimization techniques]] such as index or [[Partition (database)|database partitioning]], [[web cache|caching]] and pre-fetching. In addition, studies have been conducted on discovering linguistically-oriented attributes that can recognize if a web query is navigational, informational or transactional.<ref>{{cite journal | author = Alejandro Figueroa | year = 2015 | title = Exploring effective features for recognizing the user intent behind web queries | booktitle = Computers in Industry | pages = 162–169 | volume = 68 | publisher = Elsevier | url = https://www.researchgate.net/publication/271911317_Exploring_effective_features_for_recognizing_the_user_intent_behind_web_queries}}</ref>

But in a recent study in 2011 it was found that the average length of queries has grown steadily over time and average length of non-English languages queries had increased more than English queries.<ref>{{cite journal |author1=Mona Taghavi |author2=Ahmed Patel |author3=Nikita Schmidt |author4=Christopher Wills |author5=Yiqi Tew | year = 2011 | title = An analysis of web proxy logs with query distribution pattern approach for search engines | booktitle = Journal of Computer Standards & Interfaces | pages = 162–170 | volume = 34 | issue = 1 |publisher = Elsevier  | url = http://www.sciencedirect.com/science/article/pii/S0920548911000808 | doi=10.1016/j.csi.2011.07.001}}</ref> Google has implemented the [[Google Hummingbird|hummingbird]] update in August 2013 to handle longer search queries since more searches are conversational (i.e. "where is the nearest coffee shop?").<ref>{{cite web|last=Sullivan|first=Danny|title=FAQ: All About The New Google "Hummingbird" Algorithm|url=http://searchengineland.com/google-hummingbird-172816|publisher=Search Engine Land|accessdate=24 May 2014}}</ref> 
For longer queries, [[Natural language processing]] helps, since parse trees of queries can be matched with that of answers and their snippets.<ref>{{vcite journal |author=Galitsky B|title=Machine learning of syntactic parse trees for search and classification of text|journal=Engineering Applications of Artificial Intelligence |volume=26 |issue=3 |date=2013 |pages=153-172|doi=10.1016/j.engappai.2012.09.017}}</ref> For multi-sentence queries where keywords statistics and [[Tf–idf]] is not very helpful, [[Parse thicket]] technique comes into play to structurally represent complex questions and answers.<ref>{{vcite journal |author=Galitsky B, Ilvovsky D, Kuznetsov SO, Strok F|title=Finding Maximal Common Sub-parse Thickets
for Multi-sentence Search |journal=Lecture Notes In Artificial Intelligence |volume = 8323 |date=2013 |http://www.aclweb.org/anthology/R13-1037
}}</ref>

== Structured queries ==
With search engines that support Boolean operators and parentheses, a technique traditionally used by librarians can be applied. A user who is looking for documents that cover several topics or ''facets'' may want to describe each of them by a [[logical disjunction|disjunction]] of characteristic words, such as <code>vehicles OR cars OR automobiles</code>. A ''faceted query'' is a [[logical conjunction|conjunction]] of such facets; e.g. a query such as <code>(electronic OR computerized OR DRE) AND (voting OR elections OR election OR balloting OR electoral)</code> is likely to find documents about electronic voting even if they omit one of the words "electronic" and "voting", or even both.<ref>{{Cite web
|url=http://eprints.eemcs.utwente.nl/6918/01/TR-CTIT-06-57.pdf
|title=Exploiting Query Structure and Document Structure to Improve Document Retrieval Effectiveness
|author1=Vojkan Mihajlović |author2=Djoerd Hiemstra |author3=Henk Ernst Blok |author4=Peter M.G. Apers |postscript=<!--None-->}}</ref>

== See also ==
* [[Information retrieval]]
* [[Web search engine]]
* [[Web query classification]]
* [[Taxonomy for search engines]]

== References ==
{{reflist|2}}

{{Internet search}}

[[Category:Internet search]]
<=====doc_Id=====>:535
<=====title=====>:
Sponsored search auction
<=====text=====>:
A '''sponsored search auction (SSA)''', also known as a '''keyword auction''', is an indispensable part of the [[business model]] of modern [[web host]]s. It refers to results from a search engine that are not output by the main search algorithm, but
rather clearly separate advertisements paid for by third parties. These advertisements
are typically related to the terms for which the user searched. They come in the form
of a link and some information, with the hope that a search user will click on the link
and buy something from the advertiser.
In sponsored search auctions, there are typically some fixed number of slots for advertisements and more advertisers that want these slots than there are slots. The advertisers
have different valuations for these slots and slots are a scarce resource, so an auction
is done to determine how the slots will be assigned.

==History==
Prior to 1998, many advertisements were charged by impression, as it was the
easiest metric to calculate. In 1998, GoTo.com, Inc debuted a pay-per-click charging
system, with pricing and slot placement determined by an auction. GoTo used a first
price auction, where bidders were placed according to their bids and charged their bids
when they won. GoTo faced bidders who were constantly changing their bid
in response to new information and changing information from other bidders.

Currently, charging per action is a common pricing scheme in affiliate networks,
such as the Amazon Associates Program.

In 2002, [[Google AdWords]] began using a second price auction to sell the single advertisement
slot. Shortly thereafter, pages had multiple advertisements slots, which were allocated
and sold via [[generalized second-price auction]] (GSP) auction, the natural generalization of a second price, single item, multi bidder
auction.<ref>Hal Varian, Christopher Harris. The VCG Auction in Theory and Practice, In The
American Economic Review, Volume 104, Issue 5, pages 442-452. Elsevier B.V.,
2014.</ref>

==Auction Mechanisms==
===Generalized Second Price Auction===
[[Generalized second-price auction]] (GSP) is the most commonly used auction mechanism for sponsored search.

====Untruthfulness====
An issue with GSP is that it's not a truthful auction and it is not the optimal strategy. To illustrate this, consider the following example.

There are three bidders with only two possible slots. The values of
each bidders 1, 2, and 3 are $10, $5, and $3 respectively. Suppose that the first slot click
through rate (CTR) is 300 and the second slot CTR is 290. If bidder 1 is truthful, he
would have to pay <math>p_1 = $5(300) = $1500</math> for a utility of <math>u_1 = $10(300)-$1500 = $1500</math>.
However, if bidder 1 decides to lie and reports a value of $4 instead then his utility
would be <math>u_2 = $10(290) - $3(290) = $2030</math>. Notice that <math>u_2 > u_1</math> which makes GSP
untruthful and bidders have an incentive to lie.

====Quality Variant====
Google uses a minor variant of GSP to auction off advertisement slots. Potential
advertisements may be of varying quality. Suppose that there are two advertisements
for eggs. One advertisement simply fills its space with the word “egg” repeated over
and over, while the other advertisement shows a picture of eggs, contains branding
information, and mentions positive qualities about their eggs, such as cage-freeness.
The second advertisement may be thought of as having higher quality than the first
advertisement, being more useful to consumers, more likely to be clicked on, and more
likely to generate revenue for both the advertiser and Google. Advertisements that
have a history of high click through rates, are geographically targeted at the user, or
have a high quality landing page may also be thought of as having higher quality.<ref>Google AdWords, Check and understand Quality Score. support.google.com/adwords/answer/2454010</ref>

Google assigns a numeric “quality” score <math>\gamma_i</math> to each bidder <math>i</math>. Bidders, rather than
being ordered purely by their bid, are instead ordered by rank, which is the product
of their bid and quality score <math>\gamma_1 b_1 \geq \gamma_2 b_2 \geq \dots \geq \gamma_n b_n</math> . Slots are still assigned in
decreasing rank order. Bidders are charged, rather than the bid of the bidder one rank
lower (<math>p_i(b_i, b_{-i}) = b_{i+1}</math>), are charged the minimum price for which, if it was their bid,
would keep them in their current rank: <math>p_i(b_i, b_{-i}) = \frac{\gamma_{i+1}b_{i+1}}{\gamma_i}</math>

===Vickrey–Clarke–Groves Auction===
[[Vickrey–Clarke–Groves auction]] (VCG) is a truthful auction optimizing social welfare. VCG is more complicated to explain than GSP and that might deter many websites from using a VCG auction mechanism even though it's truthful. However, some websites use VCG as their auction mechanism, most notably [[Facebook]].

==See also==
*[[Generalized second-price auction]]
*[[Vickrey–Clarke–Groves auction]]

==References==
{{reflist}}

[[Category:Internet search]]
<=====doc_Id=====>:538
<=====title=====>:
SpyFu
<=====text=====>:
'''SpyFu''', originally GoogSpy, is a [[search analytics]] company based out of Scottsdale, AZ. Started in April 2005, SpyFu shows the keywords that websites buy on [[Google Adwords]]<ref>{{Cite web|url=http://searchenginewatch.com/3632613|accessdate=December 28, 2009|title=Advanced Keyword Research Checklist: Using Multiple Datasets}}</ref> as well as the keywords that websites are showing up for within search results. The service also gives cost per click and search volume statistics on keywords and uses that data to approximate what websites are spending on advertising.<ref>{{Cite web|url=http://www.entrepreneur.com/ebusiness/searchoptimization/searchengineoptimizationcolumnistjonrognerud/article175856.html|accessdate=December 28, 2009|title=Using the Competition to Boost Your SEO Performance }}</ref><ref>{{Cite web|url=http://www.pcworld.com/businesscenter/article/141728/top_11_moneywasting_adwords_mistakes.html|accessdate=December 28, 2009|title=Top 11 Money-Wasting AdWords Mistakes}}</ref> Historical advertising budgets offered by SpyFu also help advertisers project what an advertising campaign will cost in the future.<ref>{{Cite web|url=http://searchengineland.com/spying-on-your-paid-search-competitors-13235|accessdate=December 28, 2009|title=Spying On Your Paid Search Competitors}}</ref> The main value proposition is to see or to "spy on" the keywords that competitors use and improve [[Search Engine Marketing|SEM]] and [[Search Engine Optimization|SEO]] strategies based on those.<ref>{{Cite news|url=http://www.wired.com/epicenter/2009/06/coolsearchengines/|accessdate=December 28, 2009|title=Cool Search Engines That Are Not Google | work=Wired|first=Ryan|last=Singel|date=June 30, 2009}}</ref> SpyFu's data was also used in the [[Washington Post]] during the [[United States presidential election, 2008|2008 Presidential election]] to disclose various keywords that candidates were advertising on.<ref>{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2008/10/15/AR2008101503574_2.html?sid=ST2008101503923|accessdate=December 28, 2009|title=In Targeting Online Ads, Campaigns Ask: Who's Searching for What? | work=The Washington Post | first=Peter | last=Whoriskey | date=October 16, 2008}}</ref> SpyFu can also uncover emerging or niche markets.<ref>{{Cite book|url=https://books.google.com/books?id=DrQtbOroN0UC&pg=PT188&dq=spyfu&ei=oz44S4ehGqmKkATX1KjNAQ&cd=1#v=onepage&q=spyfu&f=false|accessdate=December 28, 2009|title=The Findability Formula | first=Heather F. | last=Lutze | isbn=978-0-470-42090-4 | year=2009 | publisher=John Wiley and Sons}}</ref> SpyFu has been mentioned in ''[[4_hour_work_week|The 4-Hour Work Week]]'', Oreilly's ''[[Complete Web Monitoring]]'', and ''[[SEO Warrior]]''.

SpyFu's data is obtained via [[web scraping]], based on technology developed by [[Velocityscape]], a company that makes web scraping software. The accuracy of its data, especially advertising budgets, was found to be somewhat dependent on the size of the website in question.<ref>{{Cite web|url=http://www.seoptimise.com/blog/2008/09/the-small-but-great-spyfu-experiment.html|accessdate=December 28, 2009|title=The Small (but Great) SpyFu Experiment}}</ref> SpyFu refreshes its data on a monthly basis, and as such is used as a guide to what's happening with larger trends in SEM/SEO rather than as a real time tracking engine.<ref>{{Cite book|url=https://books.google.com/books?id=5G4kfJ3DG2kC&pg=PA113&dq=spyfu&ei=gvg4S-WvLZPIlAT3nuzSAQ&cd=2#v=onepage&q=spyfu&f=false|accessdate=December 28, 2009|title=The complete guide to Google advertising | first=Bruce C. | last=Brown | isbn=978-1-60138-045-6 | year=2007 | publisher=Atlantic Publishing Company}}</ref>
==References==
{{Reflist}}

==External links==
* [http://spyfu.com/ SpyFu Corporate Website]

[[Category:Internet search]]
[[Category:Companies based in Scottsdale, Arizona]]
[[Category:Companies established in 2005]]
<=====doc_Id=====>:541
<=====title=====>:
Google (verb)
<=====text=====>:
{{cleanup|reason= Substantive entry outdated, summary does not refer to content in body of entry, body reads like dictionary entry for 'ungoogleable')|date=October 2016}}

{{about|the verb|the use of the verb in cricket|Googly|other uses|Google (disambiguation)}}
{{redirect|Googled|the book of the same name|Googled: The End of the World as We Know It}}
{{wiktionary|google}}
As a result of the increasing popularity and dominance of the [[Google Search|Google search engine]],<ref>{{cite web |last=Burns |first=Enid |date=June 19, 2007 |url=http://searchenginewatch.com/showPage.html?page=3626208 |title=Top 10 Search Providers, April 2007 |publisher=SearchEngineWatch.com |accessdate=2007-08-11 }}</ref> usage of the [[transitive verb]]<ref>{{cite web|url=http://www.merriam-webster.com/dictionary/google |title=Google - Definition and More from the Free Merriam-Webster Dictionary |publisher=Merriam-webster.com |date= |accessdate=2011-09-19}}</ref> '''to google''' (also spelled '''Google''') grew ubiquitously. The [[neologism]] commonly refers to searching for information on the [[World Wide Web]], regardless of which [[search engine]] is used.<ref>{{cite web|url=http://www.thelinguafile.com/2013/02/how-google-became-verb.html |title=How Google Became a Verb |publisher=The Lingua File - The Language Blog |date= |accessdate=2013-11-22}}</ref> The [[American Dialect Society]] chose it as the "most useful word of 2002."<ref>{{cite web |date=January 13, 2003 |url=http://www.americandialect.org/index.php/amerdial/2002_words_of_the_y/ |title=2002 Words of the Year |publisher=American Dialect Society |accessdate=2007-08-11 }}</ref> It was added to the ''[[Oxford English Dictionary]]'' on June 15, 2006,<ref>Bylund, Anders. "[http://www.fool.com/investing/dividends-income/2006/07/05/to-google-or-not-to-google.aspx To Google or Not to Google]." ''[[The Motley Fool]].'' July 5, 2006. Retrieved on March 28, 2007.</ref> and to the eleventh edition of the ''[[Merriam-Webster|Merriam-Webster Collegiate Dictionary]]'' in July 2006.<ref>Harris, Scott D. "[http://www.mercurynews.com/mld/mercurynews/business/14985574.htm Dictionary adds verb: to google]." ''[[San Jose Mercury News]].'' July 7, 2006. Retrieved on July 7, 2006.</ref>

==Etymology==
The first recorded usage of ''google'' used as a [[participle]], thus supposing an [[intransitive verb]], was on July 8, 1998, by [[Google]] co-founder [[Larry Page]] himself, who wrote on a mailing list: "Have fun and keep googling!"<ref>{{cite web |last=Page |first=Larry |authorlink=Larry Page |date=July 8, 1998 |url=http://www.egroups.com/group/google-friends/3.html |title=Google Search Engine: New Features |publisher=Google Friends Mailing List |accessdate=2007-08-06 |archiveurl=https://web.archive.org/web/19991009052012/http://www.egroups.com/group/google-friends/3.html |archivedate=1999-10-09 }}</ref> Its earliest known use (as a transitive verb) on American television was in the "[[Help (Buffy episode)|Help]]" episode of ''[[Buffy the Vampire Slayer (TV series)|Buffy the Vampire Slayer]]'' (October 15, 2002), when [[Willow Rosenberg|Willow]] asked [[Buffy Summers|Buffy]], "Have you googled her yet?"<ref>{{Cite book |title=Digital Wars: Apple, Google, Microsoft and the Battle for the Internet |last=Arthur |first=Charles |year=2012 |publisher=Kogan Page Publishers |location= |isbn= |page=48 |url=https://books.google.com/books?id=IXiYi-dQenEC&pg=PA48#v=onepage&q&f=false |accessdate=January 2, 2013 }}</ref>
<!-- Fearing the [[generic trademark|genericizing]] and potential loss of its [[trademark]], Google has discouraged use of the word as a verb, particularly when used as a synonym for general web searching. --> 
On February 23, 2003,<ref>{{cite web |last=McFedries |first=Paul |date=February 23, 2003 |url=http://listserv.linguistlist.org/cgi-bin/wa?A2=ind0302D&L=ads-l&P=R2450 |title=Google trademark concerns |publisher=American Dialect Society Mailing List |accessdate=2007-08-11 }}</ref> the company sent a [[cease and desist]] letter to [[Paul McFedries]], creator of [[Word Spy]], a website that tracks [[neologism]]s.<ref>Duffy, Jonathan. "[http://news.bbc.co.uk/2/hi/uk_news/3006486.stm Google calls in the 'language police']." ''[[BBC News]].'' June 20, 2003. Retrieved on July 7, 2006.</ref> In an article in the ''[[Washington Post]]'', Frank Ahrens discussed the letter he received from a Google lawyer that demonstrated "appropriate" and "inappropriate" ways to use the verb "google".<ref>{{cite news|url=http://www.washingtonpost.com/wp-dyn/content/article/2006/08/04/AR2006080401536.html|title=So Google Is No Brand X, but What Is 'Genericide'?|author=Frank Ahrens|date=2006-08-05|accessdate=2006-08-05|publisher=Washington Post}}</ref> It was reported that, in response to this concern, [[lexicographer]]s for the ''Merriam-Webster Collegiate Dictionary'' lowercased the actual entry for the word, ''google'', while maintaining the capitalization of the search engine in their definition, "to use the [[Google search|Google search engine]] to seek online information" (a concern which did not deter the Oxford editors from preserving the history of both "cases").<ref>Noon, Chris. "[http://www.forbes.com/2006/07/06/page-brin-google-cx_cn_0706autofacescan01.html Brin, Page See 'Google' Take Its Place In Dictionary]." ''[[Forbes]].'' July 6, 2006. Retrieved on July 7, 2006.</ref> On October 25, 2006, Google sent a request to the public requesting that "You should please only use 'Google' when you’re actually referring to Google Inc. and our services."<ref>{{cite web |last=Krantz |first=Michael |date=October 25, 2006 |url=http://googleblog.blogspot.com/2006/10/do-you-google.html |title=Do you "Google?" |publisher=The Official Google Blog |accessdate=2007-08-11 }}</ref>

==Ungoogleable==
{{main|Censorship by Google|Deep Web (search indexing)}}
{{wiktionary|unGoogleable}}
Ungoogleable, (or unGoogleable) is a term for something that cannot be "googled" – i.e. it is a term for something that cannot be found easily using the [[Google Search]] [[web search engine]]. It is increasingly used to mean something that cannot be found using any web search engine.<ref>{{cite news| url=http://www.bbc.co.uk/news/magazine-21956743 | title=Who, What, Why: What is 'ungoogleable'? | publisher=[[BBC]] |work=[[BBC News Magazine]] | date=27 March 2013 | accessdate=5 April 2013 }}</ref>

In 2013 the [[Swedish Language Council]] attempted to include the [[Swedish language|Swedish]] version of the word ("''[[:sv:Ogooglebar|ogooglebar]]''") in its list of new words, but Google objected to the definition not being specifically related to Google, and the Council was forced to briefly remove it to avoid a legal confrontation with Google.<ref>{{cite news| url=http://www.bbc.co.uk/news/world-europe-21944834 | title=Google gets ungoogleable off Sweden's new word list | first=Sean | last=Fanning | publisher=[[BBC]] | work=[[BBC News]] | date=26 March 2013 | accessdate=5 April 2013 }}</ref><ref>{{cite news| url=http://www.independent.co.uk/news/world/europe/ungoogleable-removed-from-list-of-swedish-words-after-row-over-definition-with-google-8550096.html | title='Ungoogleable' removed from list of Swedish words after row over definition with Google: California based search engine giant asked Swedish to amend definition | first=Rob | last=Williams | newspaper=[[The Independent]] | date=26 March 2013 | accessdate=5 April 2013 }}</ref>

===Causes===
Google Search generally ignores punctuation and [[letter case]] even when using the "quotation" operator to denote exact searches.<ref>[https://support.google.com/websearch/answer/2466433?hl=en Search operators - Search Help]</ref> Thus, Google may not be able to differentiate terms for which punctuation impacts meaning{{--}}for example, "man eating chicken" and "man-eating chicken" (the former meaning a human who is consuming chicken meat and the latter a chicken that eats humans). Because Google treats upper and lower case letters as one and the same, it also is unable to differentiate between the pronoun ''[[he]]'' and the surname ''[[He (surname)|He]]'', which, when combined with its disregard for punctuation, could bury results for an obscure person named <code>"Thomas He"</code> among results such as:
:<q>... Assisted by '''Thomas, he''' was able to provide incontrovertible proof of this theory, and in so doing, he gained wide recognition in the medical ...</q><ref>[https://www.google.com/webhp?hl=en&sa=X#hl=en&q=%22Thomas+He%22 "Thomas He" - Google Search]</ref>

The above also exemplifies how Google's [[PageRank]] algorithm, which sorts results by "importance", could also cause something to become ungoogleable: results for those with the 17th most common Chinese surname<ref>{{cite web|url=http://cdn.theatlantic.com/newsroom/img/posts/2013/10/chinassurnames/0886ab335.jpg |title=China's Surnames |publisher=Cdn.theatlantic.com |accessdate=2016-07-12}}</ref> are difficult to separate from results containing the 16th [[most common words in English|most common word in English]]. In other words, a specific subject may be ungoogleable because its results are a [[wikt:needle in a haystack|needle in a haystack]] of results for a more "important" term.

==See also==
{{Portal|Internet}}
* [[grep#Usage as a verb|grep]]<!--lowercase-->
* [[Swedish_Language_Council#Controversy|Ogooglebar, Swedish for Ungoogleable]]
* [[Photo manipulation#Photoshopping|Photoshop (verb)]], a similar neologism referring to digital photo editing

==References==
{{reflist|30em}}

{{Google Inc.}}

{{DEFAULTSORT:Google (Verb)}}
[[Category:Google]]
[[Category:Verbs]]
[[Category:Internet terminology]]
[[Category:Internet search]]
[[Category:Words coined in the 1990s]]
[[Category:Computer-related introductions in 1998]]

[[ja:Google#派生語]]
[[ru:Google (компания)#to google]]
<=====doc_Id=====>:544
<=====title=====>:
Instant indexing
<=====text=====>:
{{Orphan|date=February 2009}}

'''Instant indexing''' is a feature offered by [[Internet]] [[search engine]]s that enables users to submit content for immediate inclusion into the [[search engine indexing|index]].

==Delayed inclusion==
Certain search engine services may require an extended period of time for inclusion, which is seen as a delay and a frustration by [[website]] administrators who wish to have their websites appear in [[search engine results page|search engine results]].<ref>{{cite web|url=https://www.youtube.com/watch?v=HBDC35Vgj34|title=How to index your domains|accessdate=2015-12-24}}</ref>

Delayed inclusion may due to the size of the index that the service must maintain or due to corporate, political or social policies{{Citation needed|date=February 2007}}. Some services, such as [[Ask.com]] only index content collected by a [[web crawling|crawler program]] which does not allow for manual adding of content to index.<ref>{{cite web|url=https://www.smartz.com/web-marketing/search-engine-optimization/submit-site-to-search-engines/|title=How to Submit Your Site to Search Engines|accessdate=2015-12-24}}</ref>

==Criticisms==
A criticism of instant indexing is that certain services filter results manually or via algorithms that prevent instant inclusion to avoid inclusion of content that violates the service's policies.{{Citation needed|date=February 2007}}

Instant indexing impacts the timeliness of the content included in the index. Given the manner in which many [[web crawling|crawlers]] operate in the case of Internet search engines, websites are only visited if a some other website links to them. Unlinked web sites are never visited (see [[invisible web]]) by the crawler because it cannot reach the website during its traversal. It is assumed that unlinked websites are less authoritative and less popular, and therefore of less quality. Over time, if a website is popular or authoritative, it is assumed that other websites will eventually link to it. If a search engine service provides instant indexing, it bypasses this quality control mechanism by not requiring incoming links. This infers that the search engine's service produces lower quality results.

Select search services that offer such a service typically also offer [[paid inclusion]], also referred to as [[pay per click|inorganic search]]. This may reduce the quality of search results.

==External links==
* {{cite web | url = http://www.web-cite.com/search_marketing/000078.html | title = Don't Blink: Instant Indexing? | publisher = Web-Cite Exposure | date = 2003-03-26 | accessdate = 2006-09-23 |archiveurl = http://web.archive.org/web/20060427184004/http://www.web-cite.com/search_marketing/000078.html <!-- Bot retrieved archive --> |archivedate = 2006-04-27}}
* {{cite web | url = http://www.earthstation9.com/index.html?us_searc.htm | title = The Wonderful World of Search Engines and Web Directories — A Search Engine Guide | author = Stan Daniloski | publisher = Earth Station 9 | date = 2004-09-17 | accessdate = 2006-09-23}}

== See also ==
* [[Search engine]]
* [[Search engine indexing]]
* [[Web crawling]]

== References ==
{{Reflist}}

[[Category:Internet terminology]]
[[Category:Internet search]]


{{website-stub}}
<=====doc_Id=====>:547
<=====title=====>:
Online search
<=====text=====>:
'''Online search''' is the process of interactively searching for and retrieving requested information via a computer from [[database]]s that are [[online]].<ref name="whatis?">{{cite journal|last1=Hawkins|first1= Donald T.|last2= Brown|first2= Carolyn P.|date=Jan 1980|title=What Is an Online Search?|journal=Online|volume=4|issue=1|pages=12–18|id=Eric:EJ214713| accessdate=2011-04-04}}</ref> Interactive searches became possible in the 1980s with the advent of faster databases and [[smart terminal]]s.<ref name="whatis?"/> In contrast, [[computerized batch searching]] was prevalent in the 1960s and 1970s.<ref name="whatis?"/> Today, searches through [[web search engine]]s constitute the majority of online searches.

Online searches often supplement reference transactions.{{cn|date=June 2015}}

==References==
{{reflist}}

{{Internet search}}

[[Category:Internet terminology]]
[[Category:Information retrieval genres]]
[[Category:Internet search| ]]

{{web-stub}}
<=====doc_Id=====>:550
<=====title=====>:
National Centre for Text Mining
<=====text=====>:
{{Multiple issues|
{{COI|date=December 2015}}
{{advert|date=December 2015}}
{{external links|date=December 2015}}
}}

{{Infobox academic division
| name           = National Centre for Text Mining (NaCTeM)
| image_name     = 
| image_alt      = National Centre for Text Mining 
| established    = 2004
| type           = 
| parent         = [[School of Computer Science, University of Manchester]] 
| affiliation    = [[University of Manchester]]
| city           = [[Manchester]]
| country        = [[United Kingdom]]
| director       = Prof. Sophia Ananiadou
| website        = {{URL|www.nactem.ac.uk}}
| logo           =<!-- Deleted image removed:  [[File:Nactem hires.tif|300px]] -->
}}

The '''National Centre for Text Mining''' ('''NaCTeM''')<ref name="ariadne">{{cite journal| author=Ananiadou S| title=The National Centre for Text Mining: A Vision for the Future | journal=Ariadne | year= 2007 | issue= 53 | url=http://www.ariadne.ac.uk/issue53/ananiadou/  }}</ref> is a publicly funded [[text mining]] (TM) centre. It was established to provide support, advice, and information on TM technologies and to disseminate information from the larger TM community, while also providing tailored services and tools in response to the requirements of the [[United Kingdom]] academic community.

The [[software]] tools and services which NaCTeM supplies allow researchers to apply text mining techniques to problems within their specific areas of interest – examples of these tools are highlighted below. In addition to providing services, the Centre is also involved in, and makes significant contributions to, the text mining research community both nationally and internationally in initiatives such as [[Europe PubMed Central]].

The Centre is located in the [[Manchester Institute of Biotechnology]] and is operated and organized by the [[University of Manchester School of Computer Science]]. NaCTeM contributes expertise in [[natural language processing]] and [[information extraction]], including [[named-entity recognition]] and extractions of complex relationships (or events) that hold between named entitites, along with parallel and distributed data mining systems in biomedical and clinical applications.

==Services==
[http://www.nactem.ac.uk/software/termine/ '''TerMine'''] is a domain independent method for automatic term recognition which can be used to help locate the most important terms in a document and automatically ranks them.<ref name="multi-word">{{cite journal| author=Frantzi, K., Ananiadou, S. and Mima, H.| title=Automatic recognition of multi-word terms | journal=International Journal of Digital Libraries | year= 2007 | volume=3 |issue= 2 | pages= 117–132|  url=http://personalpages.manchester.ac.uk/staff/sophia.ananiadou/IJODL2000.pdf }}</ref>

[http://www.nactem.ac.uk/software/acromine/ '''AcroMine'''] finds all known expanded forms of [[acronyms]] as they have appeared in [[Medline]] entries or conversely, it can be used to find possible acronyms of expanded forms as they have previously appeared in [[Medline]] and [[disambiguation (metadata)|disambiguates]] them.<ref name="pmid17050571">{{cite journal|vauthors=Okazaki N, Ananiadou S | title=Building an abbreviation dictionary using a term recognition approach. | journal=Bioinformatics | year= 2006 | volume= 22 | issue= 24 | pages= 3089–95 | pmid=17050571 | doi=10.1093/bioinformatics/btl534 | pmc= | url=http://www.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi?dbfrom=pubmed&tool=sumsearch.org/cite&retmode=ref&cmd=prlinks&id=17050571  }}</ref>

[http://www.nactem.ac.uk/medie/ '''Medie'''] is  an intelligent search engine, for semantic retrieval of sentences containing biomedical correlations from [[Medline]] abstracts <ref>{{cite conference |author=Miyao, Y., Ohta, T., Masuda, K., Tsuruoka, Y., Yoshida, K., Ninomiya, T. and Tsujii, J.|title=Semantic Retrieval for the Accurate Identification of Relational Concepts in Massive Textbases |year=2006 |conference=Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics |pages=1017–1024 |doi=10.3115/1220175.1220303}}</ref>

[http://www.nactem.ac.uk/facta/ '''Facta+'''] is a [[Medline]]  search engine for finding associations between biomedical concepts.<ref name="pmid18772154">{{cite journal|vauthors=Tsuruoka Y, Tsujii J, Ananiadou S | title=FACTA: a text search engine for finding associated biomedical concepts | journal=Bioinformatics | year= 2008 | volume= 24 | issue= 21 | pages= 2559–60 | pmid=18772154 | doi=10.1093/bioinformatics/btn469 | pmc=2572701   }}</ref>

[http://www.nactem.ac.uk/facta-visualizer/ '''Facta+ Visualizer'''] is a web application that aids in understanding FACTA+ search results through intuitive graphical visualisation.<ref>{{Cite journal| last1 = Tsuruoka| first1 = Y
| last2 = Miwa| first2 = M| last3 = Hamamoto| first3 = K| last4 = Tsujii| first4 = J| last5 = Ananiadou| first5 = S
 | year = 2011| title = Discovering and visualizing indirect associations between biomedical concepts
 | journal = Bioinformatics| volume = 27| issue = 13 | pages = i111-9| publisher =  | jstor = | doi = 10.1093/bioinformatics/btr214 }}</ref>

[http://www.nactem.ac.uk/software/kleio/ '''KLEIO'''] is a faceted semantic information retrieval system over [[Medline]] abstracts.

[http://labs.europepmc.org/evf '''Europe PMC EvidenceFinder'''] helps users to explore facts that involve entities of interest within the full text articles of the [[Europe PubMed Central]] database.<ref>{{cite journal| author=The Europe PMC Consortium| title=Europe PMC: a full-text literature database for the life sciences and platform for innovation | journal=Nucleic Acids Research| year= 2014 | volume= 43 | issue=D1 | pages=D1042-D1048 | doi=10.1093/nar/gku1061 | pmid=25378340 | pmc=4383902}}</ref>

[http://www.nactem.ac.uk/EvidenceFinderAnatomyMK/ '''EUPMC Evidence Finder for Anatomical entities with meta-knowledge'''] – similar to the Europe PMC EvidenceFinder, allowing exploration of facts involving anatomical entities within the full text articles of the [[Europe PubMed Central]] database.  Facts can be filtered according to various aspects of their interpretation (e.g., negation, certainly level, novelty).

[http://www.nactem.ac.uk/info-pubmed/ '''Info-PubMed'''] provides information and graphical representation of biomedical interactions extracted from [[Medline]] using deep [[Semantic analysis (machine learning)|semantic parsing]] technology. This is supplemented with a term dictionary consisting of over 200,000 [[protein]]/[[gene]] names  and identification of [[disease]] types and [[organisms]].

[http://www.nactem.ac.uk/ClinicalTrialProtocols/ '''Clinical Trial Protocols (ASCOT) '''] is an efficient, semantically-enhanced search application, customised for clinical trial documents.<ref>{{cite journal| author=Korkontzelos, I., Mu, T. and Ananiadou, S.| title=ASCOT: a text mining-based web-service for efficient search and assisted creation of clinical trials | journal=BMC Medical Informatics and Decision Making | year= 2012 | volume= 12 | issue= Suppl 1 | pages= S3 | doi=10.1186/1472-6947-12-S1-S3}}</ref>

[http://www.nactem.ac.uk/hom/ '''History of Medicine (HOM)'''] is a semantic search system over historical medical document archives

==Resources==

[http://www.nactem.ac.uk/biolexicon/ '''BioLexicon'''] – a large-scale terminological resource for the biomedical domain.<ref>{{cite journal| author=Thompson, P., McNaught, J., Montemagni, S., Calzolari, N., del Gratta, R., Lee, V., Marchi, S., Monachini, M., Pezik, P., Quochi, V., Rupp, C. J., Sasaki, Y., Venturi, G., Rebholz-Schuhmann, D. and Ananiadou, S.| title=The BioLexicon: a large-scale terminological resource for biomedical text mining | journal=BMC Bioinformatics | year= 2011 | volume= 12 | pages=397 | doi=10.1186/1471-2105-12-397}}</ref>

[http://www.nactem.ac.uk/genia/ '''GENIA'''] – a collection of reference materials for the development of biomedical text mining systems.

[http://www.nactem.ac.uk/GREC/ '''GREC''']  – a semantically annotated corpus of [[Medline]] abstracts intended for training IE systems and/or resources which are used to extract events from biomedical literature.<ref>{{cite journal| author=Thompson, P., Iqbal, S. A., McNaught, J. and Ananiadou, S.| title=Construction of an annotated corpus to support biomedical information extraction| journal=BMC Bioinformatics| year= 2009| volume= 10 | pages=349| doi=10.1186/1471-2105-10-349}}</ref>

[http://www.nactem.ac.uk/metabolite-corpus/ '''Metabolite and Enzyme Corpus''']  – a corpus of [[Medline]] abstracts annotated by experts with metabolite and enzyme names.

[http://www.nactem.ac.uk/anatomy_corpora/ '''Anatomy Corpora''']  – A collection of corpora manually annotated with fine-grained, species-independent anatomical entities, to facilitate the development of text mining systems that can carry out detailed and comprehensive analyses of biomedical scientific text.<ref>{{cite journal| author=Pyysalo, S., Ohta, T., Miwa, M., Cho, H. -C., Tsujii, J. and Ananiadou, S.| title=Event extraction across multiple levels of biological organization| journal=Bioinformatics| year= 2012| volume= 28 | issue=18 |pages=i575-i581| doi=10.1093/bioinformatics/bts407}}</ref>
<ref>{{cite journal|author1=Pyysalo, S.  |author2=Ananiadou, S. |lastauthoramp=yes | title=Anatomical Entity Mention Recognition at Literature Scale| journal=Bioinformatics| year= 2014| volume= 30 | issue=6 |pages=868–875| doi=10.1093/bioinformatics/btt580}}</ref>

[http://www.nactem.ac.uk/meta-knowledge/ '''Meta-knowledge corpus''']  – an enrichment of the [http://www.nactem.ac.uk/genia/ '''GENIA Event corpus'''], in which events are enriched with various levels of information pertaining to their interpretation. The aim is to allow systems to be trained that can distinguish between events that factual information or experimental analyses, definite information from speculated information, etc.<ref>{{cite journal| author=Thompson, P., Nawaz, R., McNaught, J. and Ananiadou, S.| title=Enriching a biomedical event corpus with meta-knowledge annotation| journal=BMC Bioinformatics| year= 2011| volume= 12  |pages=393| doi=10.1186/1471-2105-12-393}}</ref>

==Projects==

[http://nactem.ac.uk/argo/ '''Argo'''] – The objective of the Argo project is to develop a workbench for analysing (primarily annotating) textual data. The workbench, which is accessed as a web application, supports the combination of elementary text-processing components to form comprehensive processing workflows. It provides functionality to manually intervene in the otherwise automatic process of annotation by correcting or creating new annotations, and facilitates user collaboration by providing sharing capabilities for user-owned resources. Argo benefits users such as text-analysis designers by providing an integrated environment for the development of processing workflows; annotators/curators by providing manual annotation functionalities supported by automatic pre-processing and post-processing; and developers by providing a workbench for testing and evaluating text analytics.

[http://nactem.ac.uk/big_mechanism/ '''Big Mechanism'''] –  Big mechanisms are large, explanatory models of complicated systems in which interactions have important causal effects. Whilst the collection of big data is increasingly automated, the creation of big mechanisms remains a largely human effort, which is becoming made increasingly challenging, according to the fragmentation and distribution of knowledge. The ability to automate the construction of big mechanisms could have a major impact on scientific research. As one of a number of different projects that make up the big mechanism programme, funded by [[DARPA]], the aim is to assemble an overarching big mechanism from the literature and prior experiments and to utilise this for the probabilistic interpretation of new patient panomics data. We will integrate machine reading of the cancer literature with probabilistic reasoning across cancer claims using specially-designed ontologies, computational modeling of cancer mechanisms (pathways), automated hypothesis generation to extend knowledge of the mechanisms and a 'Robot Scientist' that performs experiments to test the hypotheses. A repetitive cycle of text mining, modelling, experimental testing, and worldview updating is intended to lead to increased knowledge about cancer mechanisms.

[http://nactem.ac.uk/copious/ '''COPIOUS'''] – This project aims to produce a knowledge repository of Philippine biodiversity by combining the domain-relevant expertise and resources of Philippine partners with the text mining-based big data analytics of the University of Manchester's National Centre for Text Mining. The repository will be a synergy of different types of information, e.g., taxonomic, occurrence, ecological, biomolecular, biochemical, thus providing users with a comprehensive view on species of interest that will allow them to (1) carry out predictive analysis on species distributions, and (2) investigate potential medicinal applications of natural products derived from Philippine species.

[http://nactem.ac.uk/europepmc/ '''Europe PMC Project'''] – This is a collaboration with the Text-Mining group at the [[European Bioinformatics Institute]] (EBI) and [[Mimas (data centre)]], forming a work package in the [[Europe PubMed Central]] project (formerly UKPMC) hosted and coordinated by the [[British Library]]. Europe PMC, as a whole, forms a European version of the [[PubMed Central]] paper repository, in collaboration with the [[National Institutes of Health]] (NIH) in the United States. Europe PMC is funded by a consortium of key funding bodies from the biomedical research funders. The contribution to this major project is in the application of text mining solutions to enhance information retrieval and knowledge discovery. As such this is an application of technology developed in other NaCTeM projects on a large scale and in a prominent resource for the Biomedicine community.

[http://nactem.ac.uk/DID-MIBIO/ '''Mining Biodiversity'''] – This project aims to transform the [[Biodiversity Heritage Library]] (BHL) into a next-generation social digital library resource to facilitate the study and discussion (via social media integration) of legacy science documents on biodiversity by a worldwide community and to raise awareness of the changes in biodiversity over time in the general public. The project integrates novel text mining methods, visualisation, crowdsourcing and social media into the BHL. The resulting digital resource will provide fully interlinked and indexed access to the full content of BHL library documents, via semantically enhanced and interactive browsing and searching capabilities, allowing users to locate precisely the information of interest to them in an easy and efficient manner.

[http://nactem.ac.uk/text-mining-mrc/ '''Mining for Public Health''']  – This project aims to conduct novel research in text mining and machine learning to transform the way in which evidence-based public health (EBPH) reviews are conducted. The aims of the project are to develop new text mining unsupervised methods for deriving term similarities, to support screening while searching in EBPH reviews and to develop new algorithms for ranking and visualising meaningful associations of multiple types in a dynamic and iterative manner. These newly developed methods will be evaluated in EBPH reviews, based on implementation of a pilot, to ascertain the level of transformation in EBPH reviewing.

==References==
{{Reflist}}

==External links==
* http://www.nactem.ac.uk

[[Category:Computational linguistics]]
[[Category:Computer science organizations]]
[[Category:Information retrieval organizations]]
[[Category:Linguistics organizations]]
[[Category:School of Computer Science, University of Manchester]]
<=====doc_Id=====>:553
<=====title=====>:
IFACnet
<=====text=====>:
'''IFACnet''', the KnowledgeNet for Professional Accountants, is the global, multilingual search engine developed by the [[International Federation of Accountants]] (IFAC) and its members to provide professional accountants worldwide with one-stop access to [[good practice guidance]], articles, management tools and other resources. This enterprise search engine was launched on October 2, 2006 by INDEZ. Originally marketed to professional accountants in business, IFACnet was expanded in March 2007 to provide resources and information relevant to small and medium accounting practices. It now includes resources and information for accountants in all sectors of the profession.

The following 31 organizations participate in IFACnet:

*[[American Institute of Certified Public Accountants]] (AICPA)
*[[Association of Chartered Certified Accountants]] (ACCA)
*[[Canadian Institute of Chartered Accountants]]
*[[Certified General Accountants Association of Canada]]
*[[Chartered Institute of Management Accountants]] (CIMA)
*[[Chartered Institute of Public Finance and Accountancy]]
*[[CMA Canada]]
*[[Compagnie Nationale des Commissaires aux Comptes]]
*[[Conseil Supérieur de l'Ordre des Experts-Comptables]]
*[[Consiglio Nazionale Dottori Commercialisti]]
*[[CPA Australia]]
*[[Délégation Internationale Pour l'Audit et la Comptabilité]]
*[[Hong Kong Institute of Certified Public Accountants]] (HKICPA)
*[[International Federation of Accountants]]  (IFAC)
*[[Institut der Wirtschaftspruefer in Deutschland]] e.V. (IDW)
*[[Institute of Certified Public Accountants in Ireland]]
*[[Institute of Certified Public Accountants of Singapore]]
*[[Institute of Chartered Accountants of Australia]]
*[[Institute of Chartered Accountants in England & Wales]] (ICAEW)
*[[Institute of Chartered Accountants in Ireland]]
*[[Institute of Chartered Accountants of India]]
*[[Institute of Chartered Accountants of Pakistan]]
*[[Institute of Chartered Accountants of Scotland]] (ICAS)
*[[Institute of Management Accountants]]
*[[Japanese Institute of Certified Public Accountants]] (JICPA)
*[[Koninklijk Nederlands Instituut van Registeraccountants]] (Royal NIVRA)
*[[Malaysian Institute of Accountants]]
*[[Malta Institute of Accountants]]
*[[National Association of State Boards of Accountancy]] (NASBA)
*[[South African Institute of Chartered Accountants]] (SAICA)
*[[Union of Chambers of Certified Public Accountants of Turkey]] (TÜRMOB)

==External links==
*[http://www.ifacnet.com/ IFACnet - A KnowledgeNet for Professional Accountants]
*[http://www.ifac.org/ International Federation of Accountants Homepage]

[[Category:Information retrieval organizations]]
[[Category:Internet search engines]]
[[Category:Accounting organizations]]
<=====doc_Id=====>:556
<=====title=====>:
European Conference on Information Retrieval
<=====text=====>:
The '''European Conference on Information Retrieval''' (ECIR) is the main 
European research conference for the presentation of new results in the field of [[information retrieval]] (IR).
It is organized by the [[Information Retrieval Specialist Group]] of the [[British Computer Society]] (BCS-IRSG).
      
The event started its life as the ''Annual Colloquium on Information Retrieval Research'' in 1978 and was 
held in the UK each year until 1998 when it was hosted in Grenoble, France. Since then the venue has
alternated between the United Kingdom and continental Europe. To mark the metamorphosis
from a small informal colloquium to a major event in the IR research calendar, the 
BCS-IRSG later renamed the event to ''European Conference on Information Retrieval''. In recent years,
ECIR has continued to grow and has become the major European forum for the discussion
of research in the field of Information Retrieval.

Some of the topics dealt with include:
* IR models, techniques, and algorithms
* IR applications
* IR system architectures
* Test and evaluation methods for IR
* [[Natural Language Processing]] for IR
* Distributed IR
* Multimedia and cross-media IR

==Time and Location==

Traditionally, the ECIR is held in Spring, near the Easter weekend. A list of locations and planned venues are presented below.

* [[Padova, Italy]], 2016 [http://ecir2016.dei.unipd.it/]
* [[Vienna, Austria]], 2015 [http://www.ecir2015.org/]
* [[Amsterdam, Netherlands]], 2014 [http://ecir2014.org/]
* [[Moscow, Russia]], 2013 [http://ecir2013.org/]
* [[Barcelona, Spain]], 2012 [http://ecir2012.upf.edu/]
* [[Dublin, Ireland]], 2011 [http://www.ecir2011.dcu.ie/]
* [[Milton Keynes]], 2010 [http://kmi.open.ac.uk/events/ecir2010/]
* [[Toulouse]], 2009 [http://ecir09.irit.fr/]
* [[Glasgow]], 2008 [http://ecir2008.dcs.gla.ac.uk/]
* [[Rome]], 2007 [http://ecir2007.fub.it/]
* [[London]], 2006 [http://ecir2006.soi.city.ac.uk/]
* [[Santiago de Compostela|Santiago]], 2005 [http://www-gsi.dec.usc.es/ecir05/]
* [[Sunderland, Tyne and Wear|Sunderland]], 2004 [http://ecir04.sunderland.ac.uk/]
* [[Pisa]], 2003 [http://ecir03.isti.cnr.it/]
* [[Glasgow]], 2002 [http://irsg.bcs.org/past_ecir.php]*
* [[Darmstadt]], 2001* (organized by GMD)
* [[Cambridge]], 2000* (organized by Microsoft Research)
* [[Glasgow]], 1999*
* [[Grenoble]], 1998*
* [[Aberdeen, Scotland|Aberdeen]], 1997*
* [[Manchester]], 1996*
* [[Crewe]], 1995* (organized by Manchester Metropolitan University)
* [[Drymen]], Scotland, 1994* (organized by Strathclyde University)
* [[Glasgow]], 1993* (organized by Strathclyde University)
* [[Lancaster, Lancashire|Lancaster]], 1992*
* [[Lancaster, Lancashire|Lancaster]], 1991*
* [[Huddersfield]], 1990*
* [[Huddersfield]], 1989*
* [[Huddersfield]], 1988*
* [[Glasgow]], 1987*
* [[Glasgow]], 1986*
* [[Bradford]], 1985*
* [[Bradford]], 1984*
* [[Sheffield]], 1983*
* [[Sheffield]], 1982*
* [[Birmingham]], 1981*
* [[Leeds]], 1980*
* [[Leeds]], 1979*

<br /> *as the Annual Colloquium on Information Retrieval Research

==External links==
* [http://irsg.bcs.org/ecir.php Official page at the website of the British Computer Society]

[[Category:Information retrieval organizations]]
[[Category:Computer science conferences]]
<=====doc_Id=====>:559
<=====title=====>:
Clearinghouse for Networked Information Discovery and Retrieval
<=====text=====>:
The '''Clearinghouse for Networked Information Discovery and Retrieval''' or '''CNIDR''' was an organization funded by the U.S. [[National Science Foundation]] from 1993 to 1997 and based at the Microelectronics Center of North Carolina (MCNC) in [[Research Triangle Park]].<ref>National Science Foundation, [http://www.nsf.gov/awardsearch/showAward?AWD_ID=9216963 Award Abstract #9216963: Clearinghouse for Network Information Discovery Retrieval]</ref><ref>Brett, George. [http://grantome.com/grant/NSF/CNS-9315306 Clearinghouse for Networked Information Discovery and Retrieval (CNIDR)]</ref>  CNIDR was active in the research and development of open source software and open standards, centered on information discovery and retrieval, in the emerging Internet.

Among the software developed at CNIDR were Isite, an open source [[Z39.50]] implementation and successor to the free version of [[Wide area information server|WAIS]],<ref>Gamiel, Kevin and Nassar, Nassib.  1995.  Structural components of the Isite information system.  In ''Z39.50 Implementation Experiences'', P. Over, R. Denenberg, W. E. Moen, and L. Stovel, Eds.  National Institute of Standards and Technology Special Publication 500-229, US Department of Commerce, Gaithersburg, MD, 71-74.</ref><ref>Nebert, Douglas D. and Fullton, James.  [http://www.csdl.tamu.edu/DL95/papers/nebert/nebert.html Use of the ISite Z39.50 software to search and retrieve spatially-referenced data]</ref><ref>[http://inkdroid.org/tmp/www-talk/8133.html CNIDR Announces Isite v1.00 Integrated Information System]</ref><ref>[http://isite.awcubed.com/Isite.html The Isite Information System]</ref><ref>[http://www.loc.gov/z3950/mums.html Library of Congress Search Form]</ref> and [[Isearch]], an open source text retrieval system.  CNIDR staff were involved in the development of open standards in the [[Internet Engineering Task Force]], the Z39.50 Implementors Group and [[Dublin Core]].<ref>[https://www.ietf.org/meeting/past.html IETF Past Meetings]</ref><ref>[http://www.loc.gov/z3950/agency/zig/meetings/output.html ZIG Meeting Output]</ref><ref>[http://dublincore.org/workshops/dc1/ DC1: OCLC/NCSA Metadata Workshop: The Essential Elements of Network Object Description]</ref>

CNIDR collaborated with the [[United States Patent and Trademark Office|U.S. Patent and Trademark Office (USPTO)]] to develop the USPTO's first Internet-based patent search systems.  One of these provided full text searching and images of medical patents related to the research and treatment of HIV/AIDS and issued by the US, Japanese and European patent offices.  Another system, known as the US Patent Bibliographic Database, provided searching of "front page" bibliographic information for all US patents since 1976.<ref>Miller, Annetta.  1994.  [http://www.newsweek.com/new-online-aids-database-186740 "A New Online Aids Database."]  In ''Newsweek'', November 13, 1994.</ref><ref>[http://www.pubzpro.com/Pubz/#!search/a/6105 MCNC and U.S. Patent Office Launch Internet AIDS Library]</ref><ref>Kawakami, Alice K.  [http://www.istl.org/98-summer/article5.html "Patents and Patent Searching."]</ref><ref>[http://www2.iastate.edu/~cyberstacks/hyb_t_7.htm Patents and Trademarks]</ref>

==References==
{{Reflist}}

[[Category:Information retrieval organizations]]
[[Category:Internet Standards]]
[[Category:Internet protocols]]
[[Category:Internet search engines]]
[[Category:Organizations established in 1992]]
[[Category:Computer-related organizations]]
<=====doc_Id=====>:562
<=====title=====>:
Dandelon.com
<=====text=====>:

{{more footnotes|date=January 2014}}
'''Dandelon.com''' is a collaborative community of libraries in multiple countries as well as a [[Search engine (computing)|search engine]], a search or discovery service, a library information system for the academic community. It is additionally a platform allowing registered libraries to exchange library catalogue enrichment data: tables of content of monographs, deep indexing data, cover pages and bibliographic descriptions of articles published in periodicals, with abstracts and / or full texts provided for part of the items. The domain name was created in 2004. It is derived from the plant [[Taraxacum|dandelion]]. The name is an allusion to the flower's worldwide occurrence: It is thought to spread around the world as easily as human words and thoughts. Dandelon's aim is to uncover knowledge assets for students from around the world. It is free of charge for private use and without user tracking or advertising.<ref>[http://www.dandelon.com Dandelon.com<!-- Bot generated title -->]</ref>

Traditionally, the number of searchable relevant subject words comes up to about five semantically different subjects words, located in titles and generated by human indexing. A book registered at dandelon.com typically is assigned between 20 and 500 subject words depending on the size of the book and the knowledge domain. Based on this extended set of terms representing each library item, queries can be more specific, and relevance ranking can be more efficient. Dandelon.com also expands user queries by adding closely related words (default: synonyms and translations, optionally: narrower terms) from multilingual [[Thesaurus|thesauri]] from various knowledge domains.

Search results can be restricted to a specific library. Automatic backlinks to the related library management system allows online access or requesting a book. Dandelon.com does not replace library management systems, it is an additional option for searching and first of all a platform for data exchange between libraries associated with its community. Its user interface supports a number of languages, and it provides content in about 130 languages.

The core of dandelon.com is the content production software ““intelligentCAPTURE mobile”” employed by all member libraries. It reads from and sends data to each library management system, receiving text content via digitization and [[optical character recognition]] (OCR) for close to 200 languages or via native digital content import. Additionally, it automatically extracts major subject words, which are translated into 60 languages by machine translation. Computers and scanners can be placed in a special mobile furniture to be used between shelves and narrow compactus.

The provider of production software and search and distribution services is the German-based company AGI-Information Management Consultants <ref>[http://www.agi-imc.de<!-- Bot generated title -->]</ref> as well as the hosting center of  GBV - Gemeinsamer Bibliotheksverbund - , a state-owned German library service center for more than 800 libraries.<ref>[http://www.gbv.de<!-- Bot generated title -->]</ref> The solution was invented in 2001 by Manfred Hauer of AGI and Karl Raedler from Vorarlberger Landesbibliothek, Austria.<ref>[http://vlb.vorarlberg.at<!-- Bot generated title -->]</ref> Dandelon.com shares part of its data with GBV. GBV, in turn, exchanges some of its catalogue enrichment data with OCLC, [[WorldCat]] and other service centers. HEBIS,<ref>[http://www.hebis.de<!-- Bot generated title -->]</ref> another state-owned service center shares with the German National Library. German National Library charges fees for enrichment content.<ref>[http://www.dnb.de/kataloganreicherung<!-- Bot generated title -->]</ref> AGI and a number of the producing libraries have been pioneering catalogue enrichment in Europe since 2001 and form one of the largest communities of producers of digitalized tables of content of monographs in Europe. In 2013 close to 2&nbsp;million tables of contents were digitalized, not all of which are available on dandelon.com for the general public. The large collection produced for the [[German National Library]] is not yet shared and was announced for public use in 2014. Dandelon.com and intelligentCAPTURE are [[IBM Lotus Domino|IBM Domino and Notes]] applications. Dandelon.com runs [[Apache Lucene]] as retrieval engine.

== References ==
{{Reflist}}
*Manfred Hauer: 2012 "[http://www.agi-imc.de/internet.nsf/dda9df579aa6429dc12567f5004ad7ed/659e168d74f0bc38c12579bb004ea5b8/$FILE/HAUER_SLA_Bahrain_2012_gb.pdf Web 2.0: Which features are wanted by academic library clients? A HEBIS Survey Report]" (PDF; 172&nbsp;kB) ''Gulf special library association (SLA)'', Conference Proceeding - on CD
*Nienerza,Heike / Sunckel, Bettina / Meier, Berthold: 2011 "[http://www.degruyter.com/view/j/abitech.2011.31.issue-3/ABI.2011.020/ABI.2011.020.xml?format=INT Unser Katalog soll besser werden! Kataloge und Portale im Web-2.0-Zeitalter. Ergebnisse einer Online-Umfrage im HeBIS-Verbund]"  ''ABI-Technik'', De Gruyter, Berlin, Issue 31, pp. 130-149, DOI: 10.1515/ABI.2011.020
*Manfred Hauer: 2013 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3d26118ce2a8ebccc1257b1800356e8b?OpenDocument Zur Bedeutung normierter Terminologien in Zeiten moderner Sprach- und Information-Retrieval-Technologien]" (PDF; 205&nbsp;kB) ''[http://www.degruyter.com/view/j/abitech] ABI-Technik'', De Gruyter, Berlin, issue 1, pp. 2-6
*Manfred Hauer, Rainer Diedrichs: 2010 "[http://www.agi-imc.de/internet.nsf/26efb65f701b0871c125751a00413614/3f191bb231f0d57ec1257749004a9e7d/$FILE/Kataloganreicherung_in_Europa_2010_c.pdf  Kataloganreicherung in Europa]" (PDF; 525&nbsp;kB) ''Buch und Bibliothek [http://www.b-u-b.de/]'', issue 5, pp.&nbsp;394–397
*Manfred Hauer: 2005 ''[http://www.agi-imc.de/internet.nsf/94280a18b17ee318c12567d2003c3bb2/3267dae6428c5f02c125711600527ffd?OpenDocument/Vergleich der Retrievalleistungen von Bibliothekskatalogen gegen erweiterte und neue Konzept. Benchmarking: Google Scholar, dandelon.com, Vorarlberger Landesbibliothek, weitere OPACs.] In: [http://www.degruyter.com/view/j/abitech] ABI-Technik, De Gruyter, Berlin, December, pp.&nbsp;295–301.

[[Category:Information retrieval organizations]]
[[Category:Information retrieval systems]]
[[Category:Digital library projects]]
<=====doc_Id=====>:565
<=====title=====>:
Information Retrieval Specialist Group
<=====text=====>:
{{Unreferenced|date=January 2010}}

The '''Information Retrieval Specialist Group''' ('''IRSG''') or '''BCS-IRSG''' is a Specialist Group of the [[British Computer Society]] concerned with supporting communication between researchers and practitioners, promoting the use of [[Information Retrieval]] (IR) methods in industry and raising public awareness. There is a newsletter called ''The Informer'', an annual European Conference (ECIR), and continual organisation and sponsorship of conferences, workshops and seminars. The current chair is Dr. Andy MacFarlane.{{Citation needed|date=January 2010}}

==European Conference on Information Retrieval==
Organising [[European Conference on Information Retrieval|ECIR]] is one of the major activities of the Information Retrieval Specialist Group. The conference began in 1979 and has grown to become one of the major Information Retrieval conferences alongside [[Special Interest Group on Information Retrieval|SIGIR]] receiving hundreds of paper and poster submissions every year from around the world.{{Citation needed|date=January 2010}} ECIR was initially established by the IRSG under the name "Annual Colloquium on Information Retrieval Research", and held in the UK until 1997. It was renamed ECIR in 2003 to better reflect its status as an international conference.

== External links ==
* [http://irsg.bcs.org/ IRSG website]

[[Category:Information retrieval organizations]]
[[Category:BCS Specialist Groups]]
<=====doc_Id=====>:568
<=====title=====>:
Automatic Content Extraction
<=====text=====>:
{{Multiple issues|
{{citation style|date=December 2011}}
{{technical|date=October 2012}}
{{abbreviations|date=October 2012}}
}}
'''Automatic Content Extraction (ACE)''' is a research program for developing advanced [[Information extraction]] [[technologies]] convened by the [[National Institute of Standards and Technology | NIST]] from 1999 to 2008, succeeding [[Message Understanding Conference | MUC]] and preceding [http://www.nist.gov/tac/ Text Analysis Conference]. 

==Goals and Efforts==
In general objective, the ACE program is motivated by and addresses the same issues as the MUC program that preceded it. The ACE program, however, defines the research objectives in terms of the target objects (i.e., the entities, the relations, and the events) rather than in terms of the words in the text. For example, the so-called “named entity” task, as defined in MUC, is to identify those words (on the page) that are names of entities. In ACE, on the other hand, the corresponding task is to identify the entity so named. This is a different task, one that is more abstract and that involves inference more explicitly in producing an answer. In a real sense, the task is to detect things that “aren’t there”.

While the ACE program is directed toward extraction of information from [[Sound|audio]] and [[image]] sources in addition to pure text, the research effort is restricted to information extraction from text. The actual [[transduction (machine learning)|transduction]] of audio and image data into text is not part of the ACE research effort, although the processing of [[Speech recognition | ASR]] and [[Optical character recognition | OCR]] output from such transducers is.

The effort involves:
* defining the research tasks in detail,
* collecting and annotating data needed for training, development, and evaluation,
* supporting the research with evaluation tools and [[research workshop]]s.

==Topics and exercises==
Given a text in [[natural language]], the ACE challenge is to detect:
# '''entities''' mentioned in the text, such as: persons, organizations, locations, facilities, weapons, vehicles, and geo-political entities.
# '''relations''' between entities, such as: person A is the manager of company B. Relation types include: role, part, located, near, and social.
# '''events''' mentioned in the text, such as: interaction, movement, transfer, creation and destruction.

The program relates to [[English language|English]], [[Arabic language|Arabic]] and [[Chinese language|Chinese]] texts.

The ACE corpus is one of the standard benchmarks for testing new information extraction [[algorithm]]s.

==References==
* George Doddington@NIS T, Alexis Mitchell@LD C, Mark Przybocki@NIS T, Lance Ramshaw@BB N, Stephanie Strassel@LD C, Ralph Weischedel@BB N. [http://www.citeulike.org/user/erelsegal-halevi/article/10003935 The automatic content extraction (ACE) program–tasks, data, and evaluation.] 2004

==External links==
* [http://www.itl.nist.gov/iaui/894.02/related_projects/muc/ MUC] - ACE's predecessor.
* [http://projects.ldc.upenn.edu/ace/ ACE] (LDC)
* [http://www.itl.nist.gov/iad/894.01/tests/ace/ ACE] (NIST)

[[Category:Information retrieval organizations]]
<=====doc_Id=====>:571
<=====title=====>:
Ness Computing
<=====text=====>:
{{Notability|Companies|date=July 2011}}

'''Ness Computing''' was a personal search company. It was acquired by OpenTable in March 2014 and was shut down later that year.<ref>{{cite web|last=Lunden|first=Ingrid|title=OpenTable Buys Ness For $17.3M|url=http://techcrunch.com/2014/02/06/opentable-ness/|work=TechCrunch|accessdate=26 March 2014}}</ref> 

It was founded in October 2009 by Corey Reese,<ref>http://www.linkedin.com/in/coreyreese</ref> Paul Twohey,<ref>http://www.linkedin.com/in/twohey</ref> Nikhil Raghavan,<ref>http://www.linkedin.com/in/nikhilraghavan</ref> and Steven Schlansker.<ref>http://www.linkedin.com/in/stevenschlansker</ref> The company was headquartered in Los Altos, California.

Ness aimed to help people make decisions about dining, nightlife, entertainment, shopping, music, travel and more. The company referred to its technology as the "Likeness Engine", a combination of a [[recommendation engine]] that used [[machine learning]] to look at data from diverse sources and a traditional [[search engine]] that served up results based on these signals. 

The free Ness Dining App (for iPhone) was referred to as the [[Netflix]] <ref>http://eater.com/archives/2011/08/26/ness-iphone-app-recommends-restaurants-using-likeness-score.php</ref> or [[Pandora Radio|Pandora]] <ref>http://gigaom.com/2011/08/25/ness-restaurant-app/</ref> for restaurants. Based on a user's ratings and preferences, the service delivered recommendations for a particular time, location, price range, and cuisine preference. Users could view the menu for a place via SinglePlatform,<ref>http://www.singleplatform.com/</ref> browse [[Instagram]] photos tagged at the restaurant, and make reservations in the app via [[OpenTable]].

==References==
{{Reflist}}

[[Category:Information retrieval organizations]]
[[Category:Software companies based in California]]
<=====doc_Id=====>:574
<=====title=====>:
Category:Alphabet Inc.
<=====text=====>:
{{Commons category|Alphabet Inc.}}
{{Portal|Alphabet|Google}}
*'''[[Alphabet Inc.]]''' — an {{C|Multinational companies headquartered in the United States|American multinational}} {{C|Conglomerate companies of the United States|conglomerate company}} based in {{C|Mountain View, California|Mountain View}}, {{C|San Francisco Bay Area}}, {{C|California}}.
:::::*It is the parent corporation of {{C|Google}}; and other [[information technology]], investment, life sciences, and research companies.


{{clr}}
::{{Cat main|Alphabet Inc.}}

{{Alphabet Inc.|state=collapsed}}
{{Google Inc.}}

[[Category:Conglomerate companies of the United States]]
[[Category:Holding companies of the United States]]
[[Category:Multinational companies headquartered in the United States]]
[[Category:Technology companies of the United States]]
[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Wikipedia categories named after conglomerate companies of the United States]]
[[Category:Wikipedia categories named after information technology companies of the United States]]











[[Category:Software companies based in the San Francisco Bay Area]]
[[Category:Information retrieval organizations]]
[[Category:Internet companies of the United States]]
<=====doc_Id=====>:577
<=====title=====>:
EXCLAIM
<=====text=====>:
{{For|the Canadian magazine|Exclaim!}}
The '''EXtensible Cross-Linguistic Automatic Information Machine (EXCLAIM)''' was an integrated tool for [[cross-language information retrieval]] (CLIR), created at the [[University of California, Santa Cruz]] in early 2006, with some support for more than a dozen languages. The lead developers were Justin Nuger and Jesse Saba Kirchner.

Early work on CLIR depended on manually constructed parallel corpora for each pair of languages. This method is labor-intensive compared to parallel corpora created automatically. A more efficient way of finding data to train a CLIR system is to use matching pages on the [[World Wide Web|web]] which are written in different languages.<ref>
{{cite web
|title=Cross-Language Information Retrieval based on Parallel Texts and Automatic Mining of Parallel Texts in the Web
|url=http://www.iro.umontreal.ca/%7Enie/Publication/nie-sigir99.pdf
|format=PDF|publisher=ACM-SIGIR 1999
|accessdate=2006-12-02
}}
</ref>

EXCLAIM capitalizes on the idea of latent parallel corpora on the [[World Wide Web|web]] by automating the alignment of such corpora in various domains. The most significant of these is [[Wikipedia]] itself, which includes articles in [http://meta.wikimedia.org/wiki/Complete_list_of_language_Wikipedias_available 250 languages]. The role of EXCLAIM is to use [[semantics]] and [[linguistics|linguistic]] analytic tools to align the information in these Wikipedias so that they can be treated as parallel corpora. EXCLAIM is also extensible to incorporate information from many other sources, such as the [[Chinese Community Health Resource Center]] (CCHRC).

One of the main goals of the EXCLAIM project is to provide the kind of computational tools and CLIR tools for [[minority languages]] and [[endangered languages]] which are often available only for powerful or prosperous majority languages.

==Current status==

In 2009, EXCLAIM was in a beta state, with varying degrees of functionality for different languages. Support for CLIR using the Wikipedia dataset and the most current version of EXCLAIM (v.0.5), including full UTF-8 support and Porter stemming for the English component, was available for the following twenty-three languages:

{| class="wikitable"
| [[Albanian language|Albanian]]
|-
| [[Amharic]]
|-
| [[Bengali language|Bengali]]
|-
| [[Gothic language|Gothic]]
|-
| [[Greek language|Greek]]
|-
| [[Icelandic language|Icelandic]]
|-
| [[Indonesian language|Indonesian]]
|-
| [[Irish language|Irish]]
|-
| [[Javanese language|Javanese]]
|-
| [[Latvian language|Latvian]]
|-
| [[Malagasy language|Malagasy]]
|-
| [[Mandarin Chinese]]
|-
| [[Nahuatl]]
|-
| [[Navajo language|Navajo]]
|-
| [[Quechua languages|Quechua]]
|-
| [[Sardinian language|Sardinian]]
|-
| [[Swahili language|Swahili]]
|-
| [[Tagalog language|Tagalog]]
|-
| [[Standard Tibetan|Tibetan]]
|-
| [[Turkish language|Turkish]]
|-
| [[Welsh language|Welsh]]
|-
| [[Wolof language|Wolof]]
|-
| [[Yiddish]]
|}

Support using the Wikipedia dataset and an earlier version of EXCLAIM (v.0.3) is available for the following languages:

{| class="wikitable"
|-
| [[Dutch language|Dutch]]
|-
| [[Spanish language|Spanish]]
|}

Significant developments in the most recent version of EXCLAIM include support for Mandarin Chinese. By developing support for this language, EXCLAIM has added solutions to [[text segmentation|segmentation]] and [[character encoding|encoding]] problems which will allow the system to be extended to many other languages written with non-European orthographic conventions. This support is supplied through the Trimming And Reformatting Modular System ([[TARMS]]) toolkit.

Future versions of EXCLAIM will extend the system to additional languages. Other goals include incorporation of available latent datasets in addition to the Wikipedia dataset.

The EXCLAIM development plan calls for an integrated CLIR instrument usable searching from English for information in any of the supported languages, or searching from any of the supported languages for information in English when EXCLAIM 1.0 is released. Future versions will allow searching from any supported language into any other, and searching from and into multiple languages.

==Further applications==

EXCLAIM has been incorporated into several projects which rely on cross-language [[query expansion]] as part of their [[Front and back ends|backend]]s. One such project is a cross-linguistic [[readability]] software generation framework, detailed in work presented at [[Association for Computational Linguistics|ACL 2009]].<ref>{{cite web
|title=A crosslinguistic readability framework
|url=http://www.aclweb.org/anthology/W/W09/W09-3103.pdf
|format=PDF|publisher=ACL-IJNLP 2009
|accessdate=2009-09-04
}}
</ref>

==Notes and references==

{{reflist}}

==External links==
*[http://www.soe.ucsc.edu/~jnuger/cgi-bin/exclaim.cgi EXCLAIM Website] (dead link)
*[http://www.w3.org/DesignIssues/Semantic.html Semantic Web Roadmap]
*[http://www.cchphmo.com/cchrchealth/index_E.html Chinese Cultural Health Resource Center]
*[http://ju-st.in/ Justin Nuger's professional webpage]

{{DEFAULTSORT:Exclaim}}
[[Category:Information retrieval systems]]
<=====doc_Id=====>:580
<=====title=====>:
AUTINDEX
<=====text=====>:
{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
}}

'''AUTINDEX''' is a commercial [[text mining]] software package based on sophisticated linguistics.<ref>Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen</ref><ref>Paul Schmidt, Mahmoud Gindiyeh & Gintare Grigonyte, 2009: Language Technology for Information Systems. In: Proceedings of KDIR - The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira, 6–8 October 2009, Portugal</ref><ref>Paul Schmidt & Mahmoud Gindiyeh, 2009: Language Technology for Multilingual Information and Document Management. In: Proceedings of ASLIB, London, 19–20 November</ref>

'''AUTINDEX''' resulting from research in [[information extraction]] <ref>Paul Schmidt, Thomas Bähr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt</ref><ref>Ursula Deriu, Jörn Lehmann & Paul Schmidt, 2009: ‚Erstellung einer Technik-Ontologie auf der Basis ausgefeilter Sprachtechnologie’. In: Proceedings Knowtech, Frankfurt</ref> is a product of the Institute of Applied Information Sciences (IAI) which is a non-profit institute that has been researching and developing [[language technology]] since its foundation in 1985. IAI is an institute affiliated to [[Saarland University]] in Saarbrücken, Germany.

'''AUTINDEX''' is the result of a number of research projects funded by the EU (Project BINDEX <ref>[//www.lrec-conf.org/proceedings/lrec2002/pdf/255.pdf]. Dieter Maas, Nuebel Rita, Catherine Pease, Paul Schmidt: Bilingual Indexing for Information Retrieval with AUTINDEX. LREC 2002.</ref>), by Deutsche Forschungsgemeinschaft and the German Ministry for Economy. Amongst the latter there are the projects LinSearch <ref>[//www.l3s.de/AR07/layout/L3S-AR2007_screen.pdf]. Project LinSearch. P. 32.</ref> and WISSMER,<ref>[//www.wissmer.info/index.php/de/]. Project Wissmer.</ref> see also the reference to IAI-Webite.<ref>[//www.iai-sb.de/forschung/content/view/67/89/]. Wissmer-Project on IAI-Site.</ref>

The basic functionality of AUTINDEX is the extraction of key words from a document to represent the semantics of the document.<ref>Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.</ref> Ideally the system is integrated with a [[thesaurus]] that defines the standardised terms to be used for key word assignment.<br> 
AUTINDEX is used in library applications (e.g. integrated in [[dandelon.com]]) as well as in high quality (expert) information systems <ref>[//www.wti-frankfurt.de]. WTI Information system.</ref> and in document management and content management environments. <br> 
 
Together with AUTINDEX a number of additional software comes along such as an integration with [[Apache Solr]] / [[Lucene]] to provide a complete [[information retrieval]] environment, a classification and [[categorisation]] system on the basis of a [[machine learning]] <ref>Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung, Logos Verlag, Berlin, 2013.</ref> software that assigns domains to the document, and a system for searching with semantically similar terms that are collected in so called [[tag clouds]].<ref>[//www.wissmer.info]. Electro mobility information system.</ref>

==See also==

* [[Information retrieval]]
* [[Linguistics]]
* [[Knowledge Management]]
* [[Natural Language Processing]]
* [[Semantics]]

== References ==
{{reflist}}

== Publications ==
* Ripplinger, Bärbel 2001: Das Indexierungssystem AUTINDEX, in GLDV Tagung, Giessen.
* Paul Schmidt, Thomas Bähr & Dr.-Ing. Jens Biesterfeld &Thomas Risse & Kerstin Denecke & Claudiu Firan, 2008: LINSearch. Aufbereitung von Fachwissen für die gezielte Informationsversorgung. In: Proceedings of Knowtech, Frankfurt.
* Paul Schmidt, Mahmoud Gindiyeh, Gintare Grigonyte: ''Language Technology for Information Systems.'' In: ''Proceedings of KDIR – The International Joint Conference on Knowledge Discovery, Knowledge Engineering and Knowledge Management Madeira.'' 6.–8. Oktober 2009, Portugal. 2009, S. 259 - 262.
* Paul Schmidt, Mahmoud Gindiyeh: ''Language Technology for Multilingual Information and Document Management.'' In: ''Proceedings of ASLIB.'' London, 19.–20. November 2009.
* Rösener, Christoph, Ulrich Herb: ''Automatische Schlagwortvergabe aus der SWD für Repositorien.'' Zusammen mit Ulrich Herb in ''Proceedings.'' Berufsverband Information Bibliothek, Bibliothekartage. 97. Deutscher Bibliothekartag, Mannheim, 2008.
* Svenja Siedle: ''Suchst du noch oder weißt du schon? Inhaltserschließung leicht gemacht mit automatischer Indexierung.'' In: ''tekom-Jahrestagung und tcworld conference 2013''
* Michael Gerards, Adreas Gerards, Peter Weiland: ''Der Einsatz der automatischen Indexierungssoftware AUTINDEX im Zentrum für Psychologische Information und Dokumentation (ZPID).'' 2006 ([http://zpid.de/download/PSYNDEXmaterial/autindex.pdf Online] bei zpid.de, PDF-Datei)
* Mahmoud Gindiyeh: Anwendung wahrscheinlichkeitstheoretischer Methoden in der linguistischen Informationsverarbeitung. Logos Verlag, Berlin, 2013.

== External links ==
* http://www.iai-sb.de/ Institute for Applied Information Sciences

[[Category:Natural language processing]]
[[Category:Information retrieval systems]]
<=====doc_Id=====>:583
<=====title=====>:
IBM Omnifind
<=====text=====>:
'''IBM OmniFind''' was an [[enterprise search]] platform from [[IBM]].
It did come in several packages adapted to different business needs, including OmniFind Enterprise Edition, OmniFind Enterprise Starter Edition, and OmniFind Discovery Edition.<ref>[http://www-01.ibm.com/software/ecm/omnifind/library.html IBM - OmniFind - Library]</ref> IBM OmniFind as a standalone product was withdrawn in April 2011<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS911-075 IBM US Announcement Letter]</ref> and is now part of [[IBM Watson Content Analytics with Enterprise Search]].<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?infotype=AN&subtype=CA&htmlfid=897/ENUS211-133 IBM US Announcement Letter]</ref>

'''IBM OmniFind Yahoo! Edition''' was a free-of-charge version that could handle up to 500,000 documents in its index and was intended for small businesses. IBM OmniFind Yahoo! Edition was simple to install, provided a user friendly front end for administration, and incorporated technology from the open source [[Lucene]] project. IBM withdrew this product from marketing effective September 22, 2010 and withdrew support effective June 30, 2011.<ref>[http://www-01.ibm.com/common/ssi/cgi-bin/ssialias?subtype=ca&infotype=an&appname=iSource&supplier=897&letternum=ENUS910-115 IBM US Announcement Letter]</ref>

'''IBM OmniFind Personal E-mail Search''' was a research product launched in 2007 for doing [[semantic search]] over personal emails by extracting and organizing concepts and relationships (such as phone numbers and addresses). The project appears to have been silently abounded sometimes around 2010.

== See also ==
* [[Languageware]]
* [[UIMA]]
* [[Comparison of enterprise search software]]
* [[List of enterprise search vendors]]

==External links==
* [http://www.ibm.com/software/data/enterprise-search/ IBM OmniFind]
* [http://omnifind.ibm.yahoo.com/ IBM OmniFind Yahoo! Edition] {{Dead link|date=May 2012}}
* [https://web.archive.org/web/20071030125647/http://www.alphaworks.ibm.com/tech/emailsearch IBM OmniFind Personal E-mail Search] 
* [http://www.opentestsearch.com/search-engines/ibm-omnifind-yahoo-edition-review/ Online demo and review of IBM OmniFind Yahoo! Edition]

==Notes==
{{reflist}}

[[Category:IBM software|OmniFind]]
[[Category:Information retrieval systems]]
<=====doc_Id=====>:586
<=====title=====>:
Find
<=====text=====>:
{{other uses}}
{{refimprove|date=June 2016}}
{{lowercase|title=find}} 
In [[Unix-like]] and some other [[operating system]]s, <code>'''find'''</code> is a [[command-line utility]] that [[Search engine (computing)|searches]] one or more [[directory tree]]s of a [[file system]], locates [[Computer file|file]]s based on some [[user (computing)|user]]-specified criteria and applies a user-specified action on each matched file. The possible search criteria include a [[pattern matching|pattern]] to match against the [[filename]] or a time range to match against the modification time or access time of the file. By default, <code>find</code> returns a list of all files below the current [[working directory]].

The related <code>[[locate (Unix)|locate]]</code> programs use a database of indexed files obtained through <code>find</code> (updated at regular intervals, typically by <code>[[cron]]</code> job) to provide a faster method of searching the entire file system for files by name.

==History==
<code>find</code> appeared in [[Version 5 Unix]] as part of the [[PWB/UNIX|Programmer's Workbench]] project, and was written by Dick Haight alongside ''cpio'',<ref name="reader">{{cite techreport |first1=M. D. |last1=McIlroy |authorlink1=Doug McIlroy |year=1987 |url=http://www.cs.dartmouth.edu/~doug/reader.pdf |title=A Research Unix reader: annotated excerpts from the Programmer's Manual, 1971–1986 |series=CSTR |number=139 |institution=Bell Labs}}</ref> which were designed to be used together.<ref>{{Cite web|title = libarchive/libarchive|url = https://github.com/libarchive/libarchive/wiki/FormatCpio|website = GitHub|accessdate = 2015-10-04}}</ref>

==Find syntax==
{{expand section|date=August 2008}}
<source lang="bash">
$ find [-H] [-L] [-P] path... [expression]
</source>
The three options control how the <code>find</code> command should treat symbolic links. The default behaviour is never to follow symbolic links. This can be explicitly specified using the -P flag. The -L flag will cause the <code>find</code> command to follow symbolic links. The -H flag will only follow symbolic links while processing the command line arguments. These flags are not available with some older versions of <code>find</code>.

At least one path must precede the expression. <code>find</code> is capable of interpreting [[Wildcard character|wildcards]] internally and commands must be constructed carefully in order to control [[Glob (programming)|shell globbing]].

Expression elements are whitespace-separated and evaluated from left to right. They can contain logical elements such as AND (&#x2011;and or &#x2011;a) and OR (&#x2011;or &#x2011;o) as well as more complex predicates.

The [[GNU Find Utilities|GNU]] <code>find</code> has a large number of additional features not specified by POSIX.

==POSIX protection from infinite output==
Real-world file systems often contain looped structures created through the use of [[hard link|hard]] or [[symbolic link|soft links]]. The [[POSIX|POSIX standard]] requires that
{{Quotation|
The <code>find</code> utility shall detect infinite loops; that is, entering a previously visited
directory that is an ancestor of the last file encountered. When it detects an infinite
loop, <code>find</code> shall write a diagnostic message to standard error and shall either recover
its position in the hierarchy or terminate.
}}

==Operators==
Operators can be used to enhance the expressions of the find command. Operators are listed in order of decreasing precedence:
* '''( expr )''' - forces precedence;
* '''! expr''' - true if expr is false;
* '''expr1 expr2''' (or '''expr1 -a expr2''') - AND. expr2 is not evaluated if expr1 is false;
* '''expr1 -o expr2''' - OR. expr2 is not evaluated if expr1 is true.

<source lang="bash">
$ find . -name 'fileA_*' -o -name 'fileB_*'
</source>
This command searches the current working directory tree for files whose names start with "fileA_" or "fileB_".

<source lang="bash">
$ find . -name 'foo.cpp' '!' -path '.svn'
</source>
This command searches the current working directory tree except the subdirectory tree ".svn" for files whose name is "foo.cpp". We quote the <code>!</code> so that it's not interpreted by the shell as the history substitution character.

==Type filter explanation==
Various type filters are supported by <code>find</code>. They are activated using the configuration switch:
<source lang="bash">
$ find -type x
</source>
where x may be any of:
* '''b''' - [[Device file|block device (buffered)]];
* '''c''' - [[Device file|character device (unbuffered)]];
* '''d''' - '''[[Directory (computing)|directory]]''';
* '''f''' - '''[[regular file]]''';
* '''l''' - [[symbolic link]]. This is never true if the -L option or the -follow operator is in effect, unless the symbolic link is broken. If you want to search for symbolic links when -L is in effect, use -xtype (though that is a GNU extension);
* '''p''' - [[named pipe]];
* '''s''' - [[Unix domain socket|socket]];
* '''D''' - [[Doors (computing)|door]].

The configuration switches listed in bold are most commonly used.

==Examples==
{{howto|section|date=September 2016}}

===From the current working directory===
<source lang="bash">
$ find . -name 'my*'
</source>
This searches the current working directory tree for files whose names start with ''my''. The single quotes avoid the [[shell (computing)|shell]] expansion—without them the shell would replace ''my*'' with the list of files whose names begin with ''my'' in the current working directory. In newer versions of the program, the directory may be omitted, and it will imply the current working directory.

===Regular files only===
<source lang="bash">
$ find . -name 'my*' -type f
</source>
This limits the results of the above search to only regular files, therefore excluding directories, special files, symbolic links, etc. ''my*'' is enclosed in single quotes (apostrophes) as otherwise the shell would replace it with the list of files in the current working directory starting with ''my''…

===Commands===
The previous examples created listings of results because, by default, <code>find</code> executes the <code>-print</code> action. (Note that early versions of the <code>find</code> command had no default action at all; therefore the resulting list of files would be discarded, to the bewilderment of users.)

<source lang="bash">
$ find . -name 'my*' -type f -ls
</source>
This prints extended file information.

===Search all directories===
<source lang="bash">
$ find / -name myfile -type f -print
</source>
This searches every directory for a regular file whose name is ''myfile'' and prints it to the screen. It is generally not a good idea to look for files this way. This can take a considerable amount of time, so it is best to specify the directory more precisely. Some operating systems may mount dynamic file systems that are not congenial to <code>find</code>. More complex filenames including characters special to the shell may need to be enclosed in single quotes.

===Search all but one subdirectory tree===
<source lang="bash">
$ find / -path excluded_path -prune -o -type f -name myfile -print
</source>
This searches every directory except the subdirectory tree ''excluded_path'' (full path including the leading /) that is pruned by the <code>-prune</code> action, for a regular file whose name is ''myfile''.

===Specify a directory===
<source lang="bash">
$ find /home/weedly -name myfile -type f -print
</source>
This searches the ''/home/weedly'' directory tree for regular files named ''myfile''. You should always specify the directory to the deepest level you can remember.

===Search several directories===
<source lang="bash">
$ find local /tmp -name mydir -type d -print
</source>
This searches the ''local'' subdirectory tree of the current working directory and the ''/tmp'' directory tree for directories named ''mydir''.

===Ignore errors===
If you're doing this as a user other than root, you might want to ignore permission denied (and any other) errors. Since errors are printed to [[stderr]], they can be suppressed by redirecting the output to /dev/null. The following example shows how to do this in the bash shell:
<source lang="bash">
$ find / -name myfile -type f -print 2> /dev/null
</source>

If you are a [[C shell|csh]] or [[tcsh]] user, you cannot redirect [[stderr]] without redirecting [[stdout]] as well. You can use sh to run the <code>find</code> command to get around this:
<source lang="bash">
$ sh -c "find / -name myfile -type f -print 2> /dev/null"
</source>
An alternate method when using [[C shell|csh]] or [[tcsh]] is to pipe the output from [[stdout]] and [[stderr]] into a [[grep]] command. This example shows how to suppress lines that contain permission denied errors.
<source lang="bash">
$ find . -name myfile |& grep -v 'Permission denied'
</source>

===Find any one of differently named files===
<source lang="bash">
$ find . \( -name '*jsp' -o -name '*java' \) -type f -ls
</source>
The <code>-ls</code> operator prints extended information, and the example finds any regular file whose name ends with either 'jsp' or 'java'. Note that the parentheses are required. TIn many shells the parentheses must be escaped with a backslash (<code>\(</code> and <code>\)</code>) to prevent them from being interpreted as special shell characters. The <code>-ls</code> operator is not available on all versions of <code>find</code>.

===Execute an action===
<source lang="bash">
$ find /var/ftp/mp3 -name '*.mp3' -type f -exec chmod 644 {} \;
</source>
This command changes the [[File system permissions|permissions]] of all regular files whose names end with ''.mp3'' in the directory tree ''/var/ftp/mp3''. The action is carried out by specifying the statement <code>-exec [[chmod]] 644 {} \;</code> in the command. For every regular file whose name ends in <code>.mp3</code>, the command <code>chmod 644 {}</code> is executed replacing <code>{}</code> with the name of the file. The semicolon (backslashed to avoid the shell interpreting it as a command separator) indicates the end of the command. Permission <code>644</code>, usually shown as <code>rw-r--r--</code>, gives the file owner full permission to read and write the file, while other users have read-only access. In some shells, the <code>{}</code> must be quoted. The trailing ";" is customarily quoted with a leading "\", but could just as effectively be enclosed in single quotes.

Note that the command itself should *not* be quoted; otherwise you get error messages like
<source lang="console">
find: echo "mv ./3bfn rel071204": No such file or directory
</source>
which means that <code>find</code> is trying to run a file called 'echo "mv ./3bfn rel071204"' and failing.

If you will be executing over many results, it is more efficient to use a variant of the exec primary that collects filenames up to ARG_MAX and then executes COMMAND with a list of filenames.

<source lang="bash">
$ find . -exec COMMAND {} +
</source>
This will ensure that filenames with whitespaces are passed to the executed COMMAND without being split up by the shell.

===Delete files and directories===
The <code>-delete</code> action is a GNU extension, and using it turns on <code>-depth</code>. So, if you are testing a find command with <code>-print</code> instead of <code>-delete</code> in order to figure out what will happen before going for it, you need to use <code>-depth -print</code>.

Delete empty files and print the names (note that <code>-empty</code> is a vendor unique extension from GNU <code>find</code> that may not be available in all <code>find</code> implementations):
<source lang="bash">
$ find . -empty -delete -print
</source>

Delete empty regular files:
<source lang="bash">
$ find . -type f -empty -delete
</source>

Delete empty directories:
<source lang="bash">
$ find . -type d -empty -delete
</source>

Delete empty files named 'bad':
<source lang="bash">
$ find . -name bad -empty -delete
</source>

Warning. — The <code>-delete</code> action should be used with conditions such as <code>-empty</code> or <code>-name</code>:
<source lang="bash">
$ find . -delete # this deletes all in .
</source>

===Search for a string===
This command will search all files from the /tmp directory tree for a string:
<source lang="bash">
$ find /tmp -type f -exec grep 'search string' '{}' /dev/null \+
</source>
The <tt>[[/dev/null]]</tt> argument is used to show the name of the file before the text that is found. Without it, only the text found is printed. 
GNU <code>grep</code> can be used on its own to perform this task:
<source lang="bash">
$ grep -r 'search string' /tmp
</source>

Example of search for "LOG" in jsmith's home directory tree:
<source lang="bash">
$ find ~jsmith -exec grep LOG '{}' /dev/null \; -print
/home/jsmith/scripts/errpt.sh:cp $LOG $FIXEDLOGNAME
/home/jsmith/scripts/errpt.sh:cat $LOG
/home/jsmith/scripts/title:USER=$LOGNAME
</source>

Example of search for the string "ERROR" in all XML files in the current working directory tree:
<source lang="bash">
$ find . -name "*.xml" -exec grep "ERROR" /dev/null '{}' \+ 
</source>
The double quotes (" ") surrounding the search string and single quotes (<nowiki>' '</nowiki>) surrounding the braces are optional in this example, but needed to allow spaces and some other special characters in the string. Note with more complex text (notably in most popular shells descended from `sh` and `csh`) single quotes are often the easier choice, since ''double quotes do not prevent all special interpretation''. Quoting filenames which have English contractions demonstrates how this can get rather complicated, since a string with an apostrophe in it is easier to protect with double quotes:
<source lang="bash">
$ find . -name "file-containing-can't" -exec grep "can't" '{}' \; -print
</source>

===Search for all files owned by a user===
<source lang="bash">
$ find . -user <userid>
</source>

===Search in case insensitive mode===
Note that <code>-iname</code> is not in the standard and may not be supported by all implementations.
<source lang="bash">
$ find . -iname 'MyFile*'
</source>

If the <code>-iname</code> switch is not supported on your system then workaround techniques may be possible such as:
<source lang="bash">
$ find . -name '[mM][yY][fF][iI][lL][eE]*'
</source>

This uses [[Perl]] to build the above command for you (though in general this kind of usage is dangerous, since special characters are not properly quoted before being fed into the standard input of `sh`):
<source lang="bash">
$ echo 'MyFile*' | perl -pe 's/([a-zA-Z])/[\L\1\U\1]/g;s/(.*)/find . -name \1/' | sh
</source>

===Search files by size===
Searching files whose size is between 100 kilobytes and 500 kilobytes:
<source lang="bash">
$ find . -size +100k -a -size -500k
</source>

Searching empty files:
<source lang="bash">
$ find . -size 0k
</source>

Searching non-empty files:
<source lang="bash">
$ find . ! -size 0k
</source>

===Search files by name and size ===
<source lang="bash">
$ find /usr/src ! \( -name '*,v' -o -name '.*,v' \) '{}' \; -print
</source>
This command will search the /usr/src directory tree. All files that are of the form '*,v' and '.*,v' are excluded. Important arguments to note are in the [[tooltip]] that is displayed on mouse-over.

<source lang="bash" enclose="div">
for file in `find /opt \( -name error_log -o -name 'access_log' -o -name 'ssl_engine_log' -o -name 'rewrite_log' -o
 -name 'catalina.out' \) -size +300000k -a -size -5000000k`; do 
 cat /dev/null > $file
done
</source>
The units should be one of [bckw], 'b' means 512-byte blocks, 'c' means byte, 'k' means kilobytes and 'w' means 2-byte words. The size does not count indirect blocks, but it does count blocks in sparse files that are not actually allocated.

==Related utilities==
* <code>[[locate (Unix)|locate]]</code> is a Unix search tool that searches a prebuilt database of files instead of directory trees of a file system. This is faster than <code>find</code> but less accurate because the database may not be up-to-date.
* <code>[[grep]]</code> is a command-line utility for searching plain-text data sets for lines matching a regular expression and by default reporting matching lines on [[standard output]].
* <code>[[tree (Unix)|tree]]</code> is a command-line utility that recursively lists files found in a directory tree, indenting the filenames according to their position in the file hierarchy.
* [[GNU Find Utilities]] (also known as findutils) is a [[GNU package]] which contains implementations of the tools <code>find</code> and [[xargs]].
* [[BusyBox]] is a utility that provides several stripped-down Unix tools in a single executable file, intended for embedded operating systems with very limited resources. It also provides a version of <code>find</code>.
* <code>[[dir (command)|dir]]</code> has the /s option that recursively searches for files or directories.

==See also==
* [[mdfind]], a similar utility that utilizes metadata for [[Mac OS X]] and [[Darwin (operating system)|Darwin]]
* [[List of Unix programs]]
* [[List of DOS commands]]
* [[Filter (higher-order function)]]
* [[find (command)]], a DOS and Windows command that is very different from UNIX <code>find</code>

==References==
{{reflist}}

==External links==
* {{man|cu|find|SUS|find files}}
* [https://www.gnu.org/software/findutils/manual/html_mono/find.html Official webpage for GNU find]
* [http://www.librebyte.net/en/gnulinux/command-find-25-practical-examples/ Command find – 25 practical examples]

{{Unix commands}}

[[Category:Information retrieval systems]]
[[Category:Standard Unix programs]]
[[Category:Unix SUS2008 utilities]]
<=====doc_Id=====>:589
<=====title=====>:
Indexing Service
<=====text=====>:
{{distinguish|Indexing and abstracting service}}
{{Use dmy dates|date=February 2011}}
{{Infobox Windows component
| name                = Indexing Service
| screenshot          = Indexing Service Query Form.PNG
| screenshot_size     = 300px
| caption             = The Indexing Service Query Form, used to query Indexing Service catalogs, hosted in [[Microsoft Management Console]].
| type                = [[Desktop search]]
| service_name        = Indexing Service
| service_description = Indexes contents and properties of files on local and remote computers; provides rapid access to files through flexible querying language.
| replaced_by         = [[Windows Search]]
| included_with       = [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /><br/>[[Windows 2000]]<ref name="MIS-v3" /><br/>[[Windows XP]]<ref name="TnC-144" /><br/>[[Windows Server 2003]]<ref name="TnC-144" /><br/>[[Windows Server 2008]]<ref name="WIS-Install2008" />
}}

'''Indexing Service''' (originally called '''Index Server''') was a [[Windows service]] that maintained an index of most of the [[Computer file|files]] on a computer to improve searching performance on PCs and corporate [[computer network]]s. It updated indexes without user intervention. In [[Windows 7]], it has been replaced by a newer [[Windows Search]] indexer. The [[IFilter]] plugins to extend the indexing capabilities to more file formats and protocols are compatible between the legacy Indexing Service and the newer Windows Search indexer.

== History ==
Indexing Service was a [[desktop search]] service included with [[Windows NT 4.0#Option Pack|Windows NT 4.0 Option Pack]]<ref name="MIS-Intro" /> as well as [[Windows 2000]] and later.<ref name="MIS-v3" /><ref name="TnC-144" /><ref name="WIS-What" /> The first incarnation of the indexing service was shipped in August 1996<ref name="MIS-Intro" /> as a content search system for Microsoft's web server software, [[Internet Information Services]].{{Citation needed|date=February 2011}} Its origins, however, date further back to Microsoft's [[Cairo (operating system)|Cairo operating system]] project, with the component serving as the Content Indexer for the [[Object File System]]. Cairo was eventually shelved, but the content indexing capabilities would go on to be included as a standard component of later Windows desktop and server operating systems, starting with [[Windows 2000]], which includes Indexing Service 3.0.{{Citation needed|date=February 2011}}

In [[Windows Vista]], the content indexer was replaced with the [[Windows Search]] indexer which was enabled by default. Indexing Service is still included with Windows Server 2008 but is not installed or running by default.<ref name="WIS-Install2008" />

Indexing Service has been deprecated in Windows 7 and Windows Server 2008 R2.<ref>{{cite web|title=Deprecated Features for Windows 7 and Windows Server 2008 R2|url=http://technet.microsoft.com/en-us/library/ee681698%28WS.10%29.aspx|work=Windows 7 Technical Library|publisher=Microsoft Corporation|accessdate=8 November 2011|location=Indexing Service|date=October 16, 2009}}</ref> It has been removed from [[Windows 8]].

== Search interfaces ==

Comprehensive searching is available after initial building of the index, which can take up to hours or days, depending on the size of the specified directories, the speed of the hard drive, user activity, indexer settings and other factors. Searching using Indexing service works also on [[Uniform Naming Convention|UNC]] paths and/or mapped network drives if the sharing server indexes appropriate directory and is aware of its sharing.

Once the indexing service has been turned on and has built its index it can be searched in three ways. The search option available from the [[Start Menu]] on the [[Microsoft windows|Windows]] [[Taskbar]] will use the indexing service if it is enabled and will even accept complex queries. Queries can also be performed using either the ''Indexing Service Query Form'' in the [[Microsoft Management Console#Common snap-ins|Computer Management snap-in]] of Microsoft Management Console, or, alternatively, using third-party applications such as 'Aim at File' or 'Grokker Desktop'.

Microsoft Index Server 2.0 does not detect changes to a catalog if the data is located on a [[Volume Mount Point|mounted partition]]. It does not support mounted volumes because of technical limitations in the file system.<ref>{{cite web
 | url = http://support.microsoft.com/kb/319506
 | title = INFO: Index Server Does Not Support Mounted Volumes (Revision: 1.0)
 | work = Microsoft Support
 | publisher = 10 May 2002
 | accessdate = 1 February 2011
}}</ref>

== References ==
{{Reflist|refs=
<ref name = "MIS-Intro">{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms951563.aspx
  |title = Introduction to Microsoft Index Server
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date = 15 October 1997
  |accessdate = 1 February 2011
  |first1 = Krishna
  |last1 = Nareddy
  }}</ref>
<ref name = "MIS-v3">{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689644.aspx
  |title = Indexing Service Version 3.0
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}</ref>
<ref name = "WIS-What">{{Cite web
  |url = http://msdn.microsoft.com/en-us/library/ms689718.aspx
  |title = What is Indexing Service?
  |work = [[Microsoft Developer Network]]
  |publisher = Microsoft Corporation
  |date =
  |accessdate = 1 February 2011
  |first1 =
  |last1 =
  }}</ref>
<ref name="WIS-Install2008">{{Cite web
  |url = http://support.microsoft.com/kb/954822
  |title = How to install and configure the Indexing Service on a Windows Server 2008-based computer (Revision: 3.0)
  |work = Microsoft Support
  |publisher = Microsoft Corporation
  |date = 3 May 2010
  |accessdate = 1 February 2011
  }}</ref>
<ref name="TnC-144">{{Cite book
  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyId=1B6ACF93-147A-4481-9346-F93A4081EEA8&displaylang=en
  |format = Microsoft Word
  |title = Threats and Countermeasures: Security Settings in Windows Server 2003 and Windows XP
  |edition = 2.0
  |publisher = Microsoft Corporation
  |page = 144
  |date=December 2005
  |first1 = Mike
  |last1  = Danseglio
  |first2 = Kurt
  |last2  = Dillard
  |first3 = José
  |last3  = Maldonado
  |first4 = Paul
  |last4  = Robichaux
  |editor1-first = Reid
  |editor1-last  = Bannecker
  |editor2-first = John
  |editor2-last  = Cobb
  |editor3-first = Jon
  |editor3-last  = Tobey
  |editor4-first = Steve
  |display-editors = 3 |editor4-last  = Wacker
  }}</ref>
}}

{{Microsoft Windows components}}
{{DEFAULTSORT:Indexing Service}}
[[Category:Windows communication and services]]
[[Category:Desktop search engines|Desktop search engines]]
[[Category:Information retrieval systems]]
[[Category:Windows components]]
<=====doc_Id=====>:592
<=====title=====>:
MAREC
<=====text=====>:
{{other uses}}
The '''MA'''trixware '''RE'''search '''C'''ollection ('''MAREC''') is a standardised patent data corpus available for research purposes. MAREC seeks to represent patent documents of several languages in order to answer specific research questions.<ref>Merz C., (2003) A Corpus Query Tool For Syntactically Annotated Corpora Licentiate Thesis, The University of Zurich, Department of Computation linguistic, Switzerland</ref><ref>Biber D., Conrad S., and Reppen R. (2000) Corpus Linguistics: Investigating Language Structure and Use. Cambridge University Press, 2nd edition</ref> It consists of 19 million patent documents in different languages, normalised to a highly specific [[XML]] schema.

MAREC is intended as raw material for research in areas such as [[information retrieval]], [[natural language processing]] or [[machine translation]], which require large amounts of complex documents.<ref>Manning, C. D. and Schütze, H. (2002) Foundations of statistical natural language processing Cambridge, MA, Massachusetts Institute of Technology (MIT)  ISBN 0-262-13360-1.</ref> The collection contains documents in 19 languages, the majority being English, German and French, and about half of the documents include full text.

In MAREC, the documents from different countries and sources are normalised to a common XML format with a uniform patent numbering scheme and citation format. The standardised fields include dates, countries, languages, references, person names, and companies as well as subject classifications such as [[International Patent Classification|IPC]] codes.<ref>European Patent Office (2009) [http://documents.epo.org/projects/babylon/eponet.nsf/0/1AFC30805E91D074C125758A0051718A/$File/guidelines_2009_complete_en.pdf Guidelines for examination in the European Patent Office], Published by European Patent Office, Germany (April 2009)</ref>

MAREC is a comparable corpus, where many documents are available in similar versions in other languages. A comparable corpus can be defined as consisting of texts that share similar topics – news text from the same time period in different countries, while a parallel corpus is defined as a collection of documents with aligned translations from the source to the target language.<ref>Järvelin A. , Talvensaari T. , Järvelin Anni, (2008) Data driven methods for improving mono- and cross-lingual IR performance in noisy environments, Proceedings of the second workshop on Analytics for noisy unstructured text data, (Singapore)</ref> Since the patent document refers to the same “invention” or “concept of idea” the text is a translation of the invention, but it does not have to be a direct translation of the text itself – text parts could have been removed or added for clarification reasons.

The 19,386,697 XML files measure a total of 621 GB and are hosted by the [[Information Retrieval Facility]]. Access and support are free of charge for research purposes.

== Use Cases ==
* MAREC is used in the [[Patent Language Translations Online (PLuTO)]] project.

== References ==
{{Reflist}}

== External links ==
* [http://www.ir-facility.org/prototypes/marec User guide and statistics]
* [http://ir-facility.org Information Retrieval Facility]

[[Category:Corpora]]
[[Category:Information retrieval systems]]
[[Category:Machine translation]]
[[Category:Natural language processing]]
[[Category:XML]]
<=====doc_Id=====>:595
<=====title=====>:
Comparison of enterprise search software
<=====text=====>:
{{Cleanup-list|date=June 2014}}

The following tables compare the major [[List of enterprise search vendors|enterprise search software]] vendors in their classes.

== General information ==

{| class="wikitable sortable"
|-
! Product !! formerly k.a. !! Vendor !! [[Software release life cycle|Stable release]] !! Update!! Platforms !! API !! Target Customer !! Software License !! Open source !! Multilingual !! Website
|-
|Mindbreeze InSpire
|Mindbreeze InSpire
|Mindbreeze 
|2016 Summer Release
|October 17th, 2016
|Windows Server Linux
|REST, SOAP, .NET, Java, Push API
|Information Insight, Knowledge management for departments/organizations, Big Data Search & Analytics
|?
|No
|Yes (including languages like CJK)
|www.mindbreeze.com 
|-
| [[Lookeen Desktop Search#Lookeen Server|Lookeen Server]]<ref>[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]</ref> || Lookeen Server || [[Axonic Informationssysteme GmbH]] || 1.3.1.1118 || April 2014 || Windows Server || [[.NET Framework|.NET]] || ? || ? || {{no}} || {{yes}} || http://www.lookeen-server.com
|-
| intergator || intergator Enterprise Search || [[intergator|interface projects GmbH]] || 5.3 || March 2016 || Windows Server, Linux Server || Java, Groovy und XML/JSON || Enterprise Search,Knowledge-Management, Content Analytics, Big Data || {{Yes}} || {{No}} || {{Yes}} || http://www.intergator.de 
|-
| Coveo Enterprise Search || Coveo Platform || [[Coveo|Coveo Solutions Inc]] || 7.0 || February 6, 2014 || Windows || [[REST]], [[SOAP]], [[.NET Framework|.NET]] || Web customer service, customer interaction hubs || ? || {{no}} || {{yes|Yes, multilingual user interfaces}} || http://www.coveo.com/en/advanced-enterprise-search
|-
| 3RDi Search || 3RDi Search || The Digital Group Inc || 1.0 || NA || Generic || Supported || Enterprise Search, Knowledge Management, Big Data, BI, Analytics || Commercial || {{no}} || {{Yes}} || http://www.3rdisearch.com
|-
| Endeca Guided Search || ? || [[Oracle Corporation|Oracle]] || 6.2.2 || March 2012 || ? || ? || ? || ? || ? || ? || http://www.oracle.com/us/products/applications/commerce/endeca/endeca-guided-search/overview/index.html
|-
| EXALEAD CloudView || ? || [[Dassault Systèmes]] || R2014 || July 2013 || Windows Server, Linux Server || Push API (PAPI) || ? || ? || {{no}} || {{yes|Yes, <br />125 languages supported}} || http://www.3ds.com/products-services/exalead/
|-
|Datafari
|
|France Labs
|2.2.1
|April 2016
|Linux Server, Windows (for test)
|REST
|Enterprise Search, Knowledge management, Big Data, BI, Analytics
|No
|Yes
|Yes
|<nowiki>http://www.datafari.com/en</nowiki>
|-
| FAST for SharePoint 2010 (F4SP) || [[Fast Search & Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?
|-
| SharePoint 2013 || [[Fast Search & Transfer]] || [[Microsoft]] || 2010 (SP1) || ? || Windows Server, SharePoint || [[.NET Framework|.NET]] || ? || ? || {{no}} || ? || ?
|-
| [[Google Search Appliance]] (GSA) || ? || [[Google]] || 7.4 || March 2015 || ? || [[.NET Framework|.NET]], [[Java]] || ? || ? || ? || {{yes}} || https://www.google.com/enterprise/search/products/gsa.html
|-
| HP IDOL<ref>[http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/ HP IDOL]</ref> || . || [[Hewlett Packard Enterprise|HPE BigData]] || 11 || March 2016 || Windows, HP-UX, Linux, Solaris || [[SOAP]], [[REST]] || big data and analytics, Enterprise search, Video, Image or Audio analytics, Knowledge-Management, Info-Governance || Yes, licensed by volume. Starting on 250Gb of Metadata || No || {{yes}} || http://www8.hp.com/us/en/software-solutions/information-data-analytics-idol/
|-
| Haven Search OnDemand<ref>[http://search.havenondemand.com/ Haven Search OnDemand]</ref> || . || [[Hewlett Packard Enterprise|HPE BigData]] || 20160329|| March 2016 || SAAS Offering || [[REST]] || ? || Consumption-based pricing. || No || {{yes}} || http://search.havenondemand.com
|-
|-
| [[Funnelback]] Search || Panoptic Search || [[Funnelback]] || 15.6 || June 2016 || SaaS Offering, Windows server, Linux server || [[REST]] || Enterprise search, website search, vertical search || Document- and server-based licensing || {{no}} || {{yes|Yes.  Multi-lingual support includes indexing, querying and localised UIs}} || https://www.funnelback.com
|-
| IBM Infosphere Data Explorer || [[Vivisimo|Vivisimo Velocity]] || [[IBM]] || ? || ? || ? || ? || big data and analytics projects || ? || ? || {{yes}} || http://www-01.ibm.com/software/data/information-optimization/
|-
| [[Swiftype]] Search || [[Swiftype]] || [[Swiftype]] || ? || September 2014 || Windows and Linux, MacOS || [[REST]] APIs || websites and mobile applications || ? No{{No}} || ? || https://swiftype.com
|-
| Lucidworks Fusion || N/A || [[Lucidworks|Lucidworks Inc.]] || 2.1.4 || March 2016 || Windows, Linux, MacOS || [[REST]] APIs || Enterprise search, online retail, search-based data analytics || Licensed by CPU cores || {{No}} ||{{yes}}|| https://lucidworks.com/products/fusion/

|-
| Perceptive Search/ISYS || Enterprise Server || [[Lexmark|Lexmark Perceptive Software]] || 4.2 || ? || Windows, Linux, Solaris, Mac OS, HP-UX and AIX || [[REST]], [[SOAP]] || ? || ? || ? || {{yes}} || http://www.perceptivesoftware.com/products/perceptive-search
|-
| Secure Enterprise Search (SES) || ? || [[Oracle Corporation|Oracle]] || 11.2.2.2 || January 2014 || Windows, Linux (Oracle, RedHat and SUSE, 32-bit and 64 bit), Solaris || ? || ? || ? || ? || ? || http://www.oracle.com/technetwork/search/oses/documentation/ses-096384.html
|-
| RAVN Connect ||   || RAVN Systems || 3.3 || February 2014 || Windows Server, Linux Server || REST API || ? || ? || {{no}} || {{yes}} || http://www.ravn.co.uk/capabilities/enterprise-search/
|}

== Features ==

=== Content Collection & Indexing ===

This section compares the ability of the products to collect and index content, both textual and non textual, from different data source types and document types (formats).

==== Indexing and connectivity ====

This is about indexing pipeline tools and processes; included connectors, support for connectors, etc.

===== Web-based =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server<ref>[http://www.lookeen-server.com/produkt/overview Lookeen Server Enterprise Search]</ref>!! HP IDOL<ref>[http://www.ndm.net/archiving/HP-Autonomy/information-connectivity Autonomy Information Connectivity]</ref>!! Coveo for Advanced Enterprise Search<ref>[http://www.coveo.com/en/platform-features-connect#connectorsSection Coveo connectors]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Swiftype Search !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search<ref>[http://www.oracle.com/technetwork/search/oses/overview/ses11222ds-1969734.pdf SES 11.2.2.2 Datasheet - Oracle]</ref>!! RAVN Connect !! intergator !! Funnelback Search
|-
| [[HTTP]] || For crawling of Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || ? || {{yes}} || {{yes}} ||  {{yes}}
|-
| [[HTTPS]] || For crawling of secured Web servers. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}} || {{yes}}  
|-
| [[XML]] || For indexing any XML-compliant data source. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || Yes || ? || Yes || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}} 
|-
|}

===== File-based =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2 !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| [[NetWare File System|Netware file systems]]  || For incremental indexing of Netware file systems. || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}
|-
| [[Samba]]/[[Unix File System|Unix file systems]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Windows File System|Windows file systems]], Windows NT Filesystems (NTFS) ||  || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
|}

===== Archiving/Directory =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What Connectors Are Available With Coveo]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| LDAP || For indexing a company directory stored on a LDAP (v2 or v3) server. || ? ||  {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| [[Microsoft Active Directory]] || Supports Microsoft Active Directory. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| [[Symantec Enterprise Vault]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{no}}
|-
|}

===== Messaging =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server || HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Akonix || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| [[Facetime]] || . || ? || {{yes}}|| ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| [[IBM Lotus Connections]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[IMAP]] || For indexing e-mail messages and attached files stored on an IMAP server. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[IBM Lotus Notes]] || For indexing e-mail messages and attached files stored on a Lotus notes server. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CLN)}} 
|?|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[Microsoft Exchange Server|Microsoft Exchange]] || Ability to retrieve and index e-mail messages and attached files <br /> (Mailboxes, Public Folders). || {{yes}} || {{yes}} || {{yes|Yes, <br /> Exchange 2003/2007/2010/2013 Servers}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CXG)}} 
|Yes|| ? || ? || {{yes|Yes <br />(via Adhere Solutions partner)}} || ? || ? || ? || {{yes|Yes, <br /> Exchange 2003 Servers}} || {{yes}}|| {{yes}}
|-
| [[Microsoft Exchange Server|Microsoft Exchange Online]] || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || . 
|Yes (IMAP)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[NNTP]] || For real-time indexing of Usenet news groups. || ? ||  ? || ? || ? || ? || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
| [[Gmail]] || . || ? || ? || {{yes}} || ? || {{yes}} || . 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
|}

===== [[Content management system|CMS]], [[Document management system|DMS]] & Social =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL<ref>[http://www8.hp.com/us/en/software-solutions/asset/software-asset-viewer.html?asset=1997065&module=1970565&docname=4AA5-8715ENW&page=1970341 KeyView IDOL - Product Brief]</ref>!! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/what_connectors_are_available_with_ces.htm What connectors are available with Coveo]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Alfresco (software)|Alfresco]] || . || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but no native support}} 
|Yes|| ? || ? || {{yes|Yes (via Incentro partner)}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| [[EMC Documentum|EMC Documentum Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CDO)}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}}  || {{yes}}|| {{yes}}
|-
| [[EPiServer|EpiServer]] || . || ? || {{yes}} || {{yes}} || ? || ? || {{no}} 
|?|| ? || ? || ? || ? || {{no}} || ? || ?  || ?|| {{yes}}
|-
| IBM Content Manager || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || {{yes|Yes <br />(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}
|-
| IBM FileNet || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes (P8)|| ? || ? || {{yes|Yes <br />(via Adhere Solutions partner)}} || ? || ? || ? || ?  || {{yes}}|| {{yes}}
|-
| [[WebSphere|IBM WebSphere]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, Informatica PowerCenter 9.x connectivity}} 
|?|| ? || ? || {{yes|Yes (via Adhere Solutions partner)}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Jalios || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes <br /> (trigram:CJA)}} 
|?|| ? || ? || ? || ? || ? || ? || ?  || ?|| ?
|-
| [[SharePoint]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CXG)}} 
|Yes|| ? || ? || {{yes|Microsoft SharePoint Portal Server <br /> Microsoft SharePoint Services}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| SharePoint Online || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| [[Dropbox (service)|Dropbox]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| Windows File Share || . || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}}
|-
| [[Google Docs]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}
|-
| Jive || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}
|-
| [[Plumtree Software|Plumtree]] || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{no}} || ? || ? || ?|| {{yes}}
|-
| Lithium || . || ? || ? || {{yes}} || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ?
|-
| [[Confluence]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ?  || ? || {{yes}} 
|-
| [[Twitter]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} 
|-
| [[Facebook]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|No|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| ? 
|-
| [[LinkedIn]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? 
|-
|}

===== Databases =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://www.arnoldit.com/search-wizards-speak/coveo.html Coveo Solutions Inc., An Interview with Laurent Simoneau]</ref>!! Endeca Guided Search !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| [[IBM DB2]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter <br /> DB2 for Linux}} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[JDBC]] ||  || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> translates [[SQL database]] fields <br /> into XML documents <br /> than are then indexed together <br />  with the document metadata}} 
|Yes|| ? || ? || Yes || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft SQL Server]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[MySQL]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ?  || {{yes}}|| {{yes}} || {{yes}}
|-
| [[ODBC]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Oracle RDBMS]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{?}}
|-
| [[Sybase]] || . || ? || {{yes}} || ? || ? || {{yes}} || {{yes|Yes, <br /> ETL:Informatica PowerCenter }} 
|?|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
|}

===== [[Customer relationship management|CRM]], [[Enterprise resource planning|ERP]], [[Product lifecycle management|PLM]], [[Business intelligence|BI]] =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance<ref>[https://www.google.com/enterprise/marketplace/viewListing?productListingId=3905631+7212827882376498737 IBM DB2 Content Manager OD Connector for Google Search Appliance, by Adhere Solutions]</ref>!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Salesforce.com|Salesforce]] || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> (trigram:CSF)}} 
|Yes (via partner)|| ? || ? || {{yes|Yes (via partner)}} || ? || {{yes}} || ? || ? || {{yes}} || ?
|-
| [[SAP Business Suite|SAP]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} 
|-
| [[Siebel Systems|Siebel]]/Oracle || . || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, ETL:Informatica }} 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[Microsoft Dynamics]] || . || ? || ? || {{yes}} || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} 
|-
| [[Business Objects|Business Object]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[IBM]] [[Cognos]] || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}} 
|-
| [[Informatica|Informatica PowerCenter]] || . || ? || ? || ? || ? || ? || {{yes|Yes <br /> (trigram:CJA)}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ? || ? 
|-
| [[MicroStrategy]] || . || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes (via partners)}} || ? || ? 
|-
| [[Windchill (software)|PTC Windchill]] || . || ? || {{yes}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ? ||{{yes}}
|-
| [[ENOVIA MatrixOne|ENOVIA]] || Support for MatrixOne/Enovia data. || ? || {{yes}} || ? || ? || ? || {{yes|Yes <br /> (trigram:CEN)}} 
|?|| ? || ? || {{yes|Yes (via partner)}} || ? || ? || ? || ? || ?|| {{yes}}
|-
|}

===== Miscellaneous =====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| [[Jira (software)|Jira]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| {{yes}}
|-
| [[GitHub]] || . || ? || ? || ? || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?
|-
| [[Slack (software)|Slack]] || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || ?|| ?
|-
| [[Mantis Bug Tracker|Mantis]] || . || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
| [[JD Edwards]]/Oracle || EnterpriseOne, World. || ? || {{yes}} || ? || ? || ? || {{yes|Yes <br />ETL:Informatica PowerCenter }} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
|-
| [[PeopleSoft]]/Oracle || . || ? || {{yes}} || ? || ? || ? || {{yes|Yes <br />ETL:Informatica PowerCenter}} 
|?|| ? || ? || ? || ? || ? || ? || {{coming soon}} || ?|| ?
|-
|}

==== Supported Formats ====

{| class="wikitable sortable"
|-
! File type !! Description !! Lookeen Server !! HP IDOL  !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/CES/7.0/User/Supported_File_Formats.htm Coveo Platform 7, Supported File Formats]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView<ref>[http://3ds.exalead.com/software/common/pdfs/products/cloudview/Exalead-Connectors-and-Formats.pdf Exalead Cloudview Connectors + Formats]</ref>
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Adobe [[PDF]] || Includes Adobe Acrobat or other PDF documents. || {{yes}} || {{yes}} || {{yes|Yes, <br /> Version 1.0 to 1.7}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Web pages || [[HTML]], [[XHTML]], etc. || {{yes}} || {{yes|Yes, <br /> versions v.3, 4, 5}} || {{yes|.asp, .aspx, .cgi, .col, <br /> .dochtml, .dothtml, .fphtml, <br /> .hta, .htm, .html, .jsp, .php, <br /> .pothtml, .ppthtml, .shtm, <br /> .shtml, .xlshtml}} || ? || {{yes}} || {{yes|Yes, <br /> versions v.4.01 and above, and [[XHTML]]}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[XML]] || Extensible Markup Language (.xml) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes, <br /> any [[Document type definition|DTD]]}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Text || Raw text. || {{yes}} || {{yes}} || {{yes|.ascx, .bat, .cmd, .config, <br /> .csv, .dic, .exc, .inf, .ini, <br /> .js, .jsl, .log, .nfo, .scp, <br /> .sdl, .sln, .txt, .vbdproj, <br /> .vbs, .vdp, .vdproj, .vjp, <br /> .vjsproj, .vjsprojdata, <br /> .wsdl, .wsf, .wtx, .xsd <br /> ANSI, ASCII, Unicode}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Excel]] || Microsoft Excel Charts (.xls), <br /> Microsoft Excel XML (.xlsx, .xltm, .xltx) <br /> Others (.xlam, .xlb, .xlm, .xlsm) || {{yes}} || {{yes}} || {{yes|<br /> Version 5.0, 95(7.0), 97, 2000, XP, 2003, 2007, 2010. <br /> Indexes Excel 2010 attachments.}} || {{yes}} || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Word]] || Microsoft Word (.doc) <br /> Microsoft Word XML (.docx, .dotx, .dotm) || {{yes}} || {{yes|Yes <br /> for Mac, Windows (multiple versions)}} || {{yes|Version 6.0, 6.0 (for MAC), 95 (7.0), 97, 98 (for MAC), 2000, <br /> XP, 2003, 2007, 2010 <br /> Indexes Word 2010 attachments.}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft PowerPoint]] || .pot, .potm, .potx, .ppam,  <br /> .pps, .ppsm, .ppsx, .ppt,  <br /> .pptm, .pptx || {{yes}} || {{yes}} || {{yes|Yes, <br /> Indexes PowerPoint 2010 attachments.}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Access]] || MDB || ? || {{yes}} || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Microsoft Project]] || MPP || ? || {{yes}} || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{no}}
|-
| [[Microsoft Visio]] || VSD || ? || {{yes|Yes <br /> (multiple versions)}} || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Microsoft Outlook]] || Message, archives, and templates (.msg, .oft, .pst) || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[MIME|MIME documents]] || Multipurpose Internet Mail Extension. || {{yes}} || {{yes|Yes, <br /> Microsoft Outlook Express Mac and PC (multiple versions) (.eml)}} || {{yes|.email, .eml, .ews, .mime <br /> MIME converter available with CES 7.0.5935+}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[StarOffice|Sun StarOffice]] || . || ? || ? || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes}} || ?|| {{yes}} || {{no}}
|-
| Lotus 1-2-3 || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Lotus Freehand || . || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Corel WordPerfect]] || Corel WordPerfect Linux (.wps) <br /> Corel WordPerfect Macintosh (.wps) <br /> Corel WordPerfect Windows (.wo) <br /> Corel WordPerfect Windows (.wpd) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, <br /> version 6 and 7}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Archive files || . || ? || {{yes|7Z, DMG, HQX, BIZIP2, GZ, ISO, JAR, <br /> EMX, BIN, BKF, CAB, LZH/LHA <br /> ZIP, RAR, RTFD, TAR, Z, UUE <br /> multiple versions.}} || ? || ? || {{yes}} || {{yes|RAR, ZIP}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| RTF || Rich Text Format || {{yes}} || {{yes|Yes, <br />multiple versions.}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Image files <br /> (text extraction) ||  || {{yes}} || ? || {{yes|Yes, <br /> .bmp, .jpeg, .max, .pcx/.dcx, <br /> .pdf, .png, .tiff, .tiff-fx <br /> requires the Optical Character Recognition (OCR) module.}} || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ?  || {{yes}}|| {{yes}} || {{no}}
|-
| Images <br /> (metadata extraction) || Creation of thumbnail. || {{yes}} || {{yes}} || {{yes|.bmp, .emf, .exif, .gif, <br /> .icon, .jpeg, .png, .tiff, <br /> .wmf}} || ? || {{yes}} || {{yes|Yes, <br /> JPEG, PNG, GIF, PNG}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Audio <br /> (text extraction) ||  || ? || ? || {{yes|.aif, .aifc, .aiff, .asf, .au, <br /> .cda .mid, .midi, .mp1, .mp3, <br /> .mpga, .rmi, .snd, .wav , .wma <br /> Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Audio (metadata) || Creation of thumbnail. || ? || ? || ? || ? || {{yes}} || {{yes|Yes, <br /> MP3, OGG.}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Video ||  || ? || {{yes|.264, video/3gpp, .3mm, .4mp ... <br /> .avi, .m1v, .mov, .mp2, ...}} || {{yes|.avi, .m1v, .mov, .mp2, .mp2v, <br /> .mpa, .mpeg, .mpg, .mpv2 .qt, <br /> .rec, .rm, .rnx, .wm, .wmv <br /> Requires the Coveo Audio Video Search (CAVS) module.}} || ? || ? || ? 
|Yes (metadata)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata extraction.}}
|-
| MacroMedia Flash || . || ? || ? || ? || ? || {{yes|Yes, <br /> MacroMedia Flash text section and hypertext links}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| Autovue 2D & 3D CAD files || . || ? || ? || ? || ?  || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || {{yes|Yes, <br /> Open CAD files directly inside}} Autovue || ?|| {{yes}} || {{no}}
|-
| AutoCAD Drawing || AutoCAD Drawing (DWF), AutoCAD Drawing Exchange (DXF) || ? || {{yes|Yes, <br /> (multiple versions)}} || ? || ? || ? || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[CATIA]] versions 4 and 5 || Drawing document (CATDrawing), Part document (CATPart), <br />  Assembly document (CATProduct), Model (V4 only) || ? || {{yes}} || ? || ? || ? || {{yes|Yes, <br /> any 5 version}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| [[SolidWorks]] v1 || Drawing (.slddrw), Assembly (.sldasm), <br /> Part (.sldprt) || ? ||{{yes}} || ? || ? || ? || {{yes|Yes, <br /> 2003 to 2013 releases}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| [[Pro/ENGINEER]] || Part (.prt), Assembly (.asm) || ? || ? || ? || ? || ? || {{yes|Yes, <br /> any release from R16 to 2001, from WF1 to WF5}} 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{no}}
|-
| CAD Open formats || . || ? || ? || ? || ? || ? || {{yes|Yes, <br /> [[IGES]] version 5.2 and 5.3, [[STEP-File]]}} 
|Yes (metadata from the DWG CAD format)|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Metadata from DWG CAD}}
|-
|}

==== Text Analytics ====

{| class="wikitable sortable"
|-
! Linguistics !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance<ref>[http://www.bainsight.com/resources/Google-vs-Microsoft-FAST-Search-Whitepaper.pdf Microsoft and Google - BA Insight]</ref>!! IBM Infosphere Data Explorer !! Lucidworks Fusion!! Perceptive Search !! Secure Enterprise Search<ref>[http://docs.oracle.com/cd/E14507_01/admin.1112/e14130.pdf Oracle Secure Enterprise Search Administrator's Guide]</ref>!! RAVN Connect !! intergator !! Funnelback Search
|-
| Language detection || Ability to identify the languages at indexing time. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Synonyms/stemming  || Ability to treat as synonyms variations of keywords. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Entity extraction|Named Entity Extraction]] || Ability to automatically extract entities such as persons, locations and organizations from indexed content. || ? || {{yes|Yes  <br /> named "Eduction"}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes|Yes <br/>With automatic entity linking}}|| {{yes}} || {{yes}}
|-
| Stop words || Ability to exclude stop words (e.g. 'an', 'the') in order to improve relevance. || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Audio & Video Analytics ====

{| class="wikitable sortable"
|-
! Multimedia !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search 6.2.2  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Audio analytics || Ability to understand topics being discussed, genders and emotional tones of speech, music, etc. || ? || ? || ? || ? || ? || ? 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes|Yes <br/> speech, topics, music, keyword spotting }}|| ? 
|-
| Video analytics || Ability to understand the content of the video without relying on metadata (e.g. key framing, facial identification, logo recognition, etc.) || ? || {{yes}} || ? || ? || ? || ? 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| ? 
|-
| Image analytics || Ability to detect patterns in image (e.g. faces, bodies, gender, age range, expression, etc.) || ? || {{yes}} || ? || ? || ? || ? 
|Yes (via partner)|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}}
|-
|}

=== Search Experience ===

This section compares the ability of the products to :
* Enable the user to enter and execute the query
* Present the data to the user within seconds after the query is parsed and processed so that the user can find what he seeks quickly and act on it.

==== Search Language ====

{| class="wikitable sortable"
|-
! Query Parser !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/User/search_prefix_and_operators.htm Search Prefixes and Operators]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect<ref>[http://www.ravn.co.uk/wp-content/uploads/2014/02/CORE-Whitepaper-W.V.pdf RAVN Connect, CORE White paper]</ref>!! intergator !! Funnelback Search <ref>https://docs.funnelback.com/query_language_help.html</ref>
|-
| Wildcard search || Does the system use the asterisk ("*") and question mark ("?") character as a wildcard? || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Fuzzy search || Does the system offer phonetic and approximate spelling search? (distinctions of syntax and semantics) || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Exact phrase search || Does the system enable to find words as a phrase? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| [[Proximity search (text)|Proximity search]] || Support for advanced proximity operators - NEAR, BEFORE, AFTER. || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Range search || Ability to match all terms which are lexically between square brackets ("[]") and curly braces ("{}"). || ? || ? || ? || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Boosting a term || Automatic bigram and trigram relevancy boosting. || ? || ? || {{yes}} || ? || ? || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Boolean search || Does the system interprets Boolean operators? || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Graph search || Does the system keep relationships between fields and allows searching for them (while enabling full-text search)? || ? || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
|}

==== Usability ====

===== Search Query =====

This is the process of searching (querying).

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://www.coveo.com/en/news-releases/Coveo-Reports-Accelerated-Demand-for-Search-and-Relevance-Technology Coveo Reports Accelerated Demand for Search & Relevance Technology in 2013]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Auto-complete || Does the system provide an automatic query guidance in the search box while typing? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}<ref>https://docs.funnelback.com/auto_completion_collection_cfg.html</ref>
|-
| Spell-checking || Does the system checks if the words in the query are spelled correctly and suggest corrections? || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}} <ref>https://docs.funnelback.com/spelling_suggestions.html</ref>
|-
| [[Federated search]] || The ability to send the same query simultaneously to several searchable sources. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}<ref>https://docs.funnelback.com/ui_modern_extra_searches_collection_cfg.html</ref>
|-
| Advanced search page || Does the system allow users to perform complex and sophisticated queries? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}|| {{yes}}
|-
|}

===== Result-list =====

This is the process of scanning the content of any document directly from the result lists.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL<ref>[http://www.ndm.net/archiving/pdf/20130902_PI_B_HP_AUTN_IDOL10_web.pdf Autonomy KeyView IDOL - Product Brief - Ndm.net]</ref>!! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance<ref>[http://static.googleusercontent.com/media/www.google.com/fr//support/enterprise/static/gsa/docs/admin/70/gsa_doc_set/quick_start/quick_start.pdf Getting the Most from Your Google Search Appliance]</ref>!! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator
|-
| Relevance ranking || Ability to find the highest quality and most relevant documents and bring them to the top of a search results list. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{yes}} || {{yes}} || {{yes|Yes <br /> Google Site Search factors in more than 100 variables <br /> for each query}} || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}
|-
| Find similar || Ability to find similar links. || {{yes}} || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}}
|-
| Hit highlighting || Ability to highlight query key terms within the document in search result. || {{yes}} || {{yes}} || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| Summarization <br /> (view as HTML) || Does the system offer content preview in the search result, so that users can judge relevance of results? || {{yes}} || {{yes}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| {{partial|*only available for Office file types (Powerpoint, Excel, Visio, etc.)}} || {{partial}} || {{yes|Yes <br /> converts over 220 file formats into HTML}} || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
|  || Does the system enable to copy/paste from within the preview? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| {{no}} || {{no}} || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Thumbnails and preview || Ability to generate thumbnails for a large amount of different file types. || ? || {{yes|Yes <br /> 100+ doc types}} || {{yes}} || ? || {{yes}} || {{yes|Yes, but low resolution for CATProducts <br /> no thumbnails for Pro/E assemblies}} 
|Yes|| {{partial|*only available for Office file types (.[[DOCX|DOCx]], .[[PPTX|PPTx]]) <br /> First page thumbnail preview}} || {{partial}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Full document graphical preview || Ability to access the content of any document without having to open a windows client application. || ? || ? || ? || ? || {{yes}} || {{yes|Yes <br /> regardless of the file's original application}} 
|Yes|| {{partial|*only available for PowerPoint file types}} || {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes <br/> Asynchronous loading}}|| {{yes}}
|-
| Document comparison || Ability to compare. || ? ||{{yes|Yes <br /> for version management, signature identification, among other features}} || ? || ? || {{yes}} || {{yes}} 
|?|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Clustering || Ability to dynamically organize search results into groups. || ? || {{yes}} || ? || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes|Yes <br /> group search results by topic}} || ? || ? || ? || ? || {{yes}}|| {{yes}}
|-
| Sort by fields || Ability to sort all results by order of date or other attribute. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || {{yes|Yes, <br /> „hard sort‟}} || {{yes}}|| {{yes}}
|-
|}

==== Faceted Navigation ====

This is the process of browsing the content by narrowing search results quickly in clicking filters that refine results based on related categories, so that users extract more meaning and insight from the content.

{| class="wikitable sortable"
|-
! [[Faceted Search|Facets]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/user/about_facets.htm Coveo Facets]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| Multiple filters || Does the system enable the user to filter results in selecting multiple facet values? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || {{yes}} || {{yes}} || ? || {{yes}}|| {{yes}}
|-
| Facet values and counts || Ability to display the term and the number of documents containing that term in the search results. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| [[Faceted classification|Hierarchical]]- and range facets || . || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
| Date, number and string types || Ability to filter by date/time, number and string data types. || {{yes}} || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}}
|-
|}

==== Social and collaborative ====

This is the process of asking social network.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010<ref>[http://www.hcsolutions.at/produkte/ontolica/Documents/Ontolica_2010_feature_matrix.pdf Search Solution for Microsoft SharePoint]</ref>!! SharePoint 2013<ref>[http://www.slideshare.net/SurfRay/new-sharepoint-server-2013-search-features Introduction to SharePoint 2013 Search]</ref>!! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search<ref>[http://www.perceptivesoftware.com/images/psi_ds_perceptiveenterprisesearch.pdf Perceptive Enterprise Search - Product Datasheet]</ref>!! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Search result tagging || Ability to improve relevancy by creating or adding existing tags. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|No|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes|Yes<br/>Real time}}|| {{yes}} || {{no|No. Deprecated in v14.0.1}}
|-
| Tag searching || Ability to search for a tag. || ? || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Personalization/ Audience targeting || Ability to deliver more accurate targeted results. || ? || {{yes| Yes, <br /> browse histories, content contributors, and interactions, etc}}  || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || {{yes| Yes, <br /> source, date, metadata and entities biaising }} || ? || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Expertise location || Ability to find experts in users organization by searching on related keywords. || ? || {{yes}} || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} 
|Yes|| ? || ? || {{yes}} || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Saved search || Ability to save searches. || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} 
|No|| {{no}} || {{no}} || ? || {{yes}} || ? || {{yes}} || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Saved alerts || Ability to save alerts in order to notified when new content matching your queries has been added to the system. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || {{no}} || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Saved RSS feeds || Ability to save RSS feeds. || ? || ? || {{yes}} || ? || {{yes}} || ? 
|No|| ? || {{no}} || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Mobile support ====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari
!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Mobile search || Does the system support mobile device access and search? || {{yes}} || ? || {{yes}} || ? || {{yes}} || {{yes}} 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}} || {{yes}} || {{yes}}
|-
| Mobile UI || Does the system detect the user device (desktop, smartphone, tablet, etc.) and adapt itself based on it?  || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes. Admin UI and Default Public UI use responsive designs.}}
|-
| Geolocation || Does the system enable from the end-user’s geolocation to provide additional context to filter? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || {{yes}}|| ? || {{yes}}
|-
| Compatibility || Is the system compatible with iOS. || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}
|-
|  || Is the system compatible with Android? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || {{yes}}
|-
|  || Is the system compatible with Windows Phone? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| {{yes}} || ?
|-
|}

=== Administration & Architecture ===

This section compares the flexibility in the underlying architecture, application development, the scalability and the administrative services of the products.

==== Management & Search analytics ====

This table is about the ability to report on usage and activity (most popular queries, documents not found, etc.)

{| class="wikitable sortable"
|-
! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search<ref>[http://docs.oracle.com/cd/E35643_01/Workbench.211/pdf/WorkbenchUserGuide.pdf Endeca® Workbench: User's Guide - Oracle Documentation]</ref>!! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator !! Funnelback Search
|-
| Web based administration interfaces <br /> (HTML) || . || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Search statistics || Does the system collect search statistics? || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Search reports || Does the system enable to report search statistics? || ? || ? || {{yes}} || {{yes}} || {{yes}} || ? 
|Yes|| {{yes}} || ? || ? || ? || ? || ? || {{yes|Yes, <br /> most popular queries}} || {{yes}}|| {{yes}} || {{yes}}
|-
| Portal usage reports || Does the system enable to report on portal usage? || ? || ? || ? || {{yes|Yes, <br /> popular navigation}} || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} 
|?|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Document storage reports || . || ? || ? || {{yes}} || ? || {{yes}} || ? 
|Yes|| {{no}} || ? || ? || ? || ? || ? || {{yes|Yes, <br /> for documents not found}} || ?|| {{yes}} || {{yes}}
|-
| Custom reports || . || ? || ? || ? || ? || {{yes}} || {{yes|Yes <br /> with [[Software development kit|SDK]]}} 
|Yes|| {{no}} || ? || ? || ? || ? || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
| Click Scoring || Ability to improve relevancy by enabling to track which results are most often clicked. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{yes}}
|-
|}

==== Interface flexibility ====

This is about tools to customize the interface,so that it adds value to any industry or business process.

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator !! Funnelback Search
|-
| Standard based open interface || Does the system support all client platforms? || ? || {{yes|Yes, <br /> HTTP and XML/JSON}} || ? || {{yes}} || {{yes}} || ? 
|Yes|| ? || ? || ? || ? || {{yes|Yes, <br /> XML, JSON and HTTP}} || ? || ? || {{yes}}|| {{yes}} || {{yes|Yes.  HTML, JSON, XML, RSS and OpenSearch.}}
|-
| Page Layout Helper || Ability to change easily to global attributes (logo, fonts, header, and footer) and to the look of the Search Box and Search Results. || ? || ? || ? || ? || {{yes}} || ? 
|Yes|| {{no|No, action menu on search results not configurable}} || ? || {{yes}} || ? || {{yes}} || ? || ? || {{yes}}|| {{yes}} || {{no}}
|-
| Stylesheet editor || Ability to make more extensive changes using [[XSLT]] stylesheet. || ? || ? || {{yes}} || ? || {{yes}} || {{yes|Yes <br /> Full customization with CSS or Java API & XML}}  || {{yes}}
|Yes|| {{yes}} || ? || {{yes|Yes <br />XSLT stylesheet editor}} || ? || {{yes}} || ? || {{yes}} || {{yes}}|| {{yes|Yes. Full results customisation via [[FreeMarker]]<ref>https://docs.funnelback.com/freemarker.html</ref> and CSS/JS. Cached copies of XML templated via XSLT<ref>https://docs.funnelback.com/xslt_processing.html</ref>}}
|-
|}

==== Scalability ====

{| class="wikitable sortable"
|-
!  !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search<ref>[http://onlinehelp.coveo.com/en/ces/7.0/Administrator/coveo_scalability_model.htm Scalability]</ref>!! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect!! intergator
|-
| Index capacity || How many total documents can be indexed into the system? || up to 10 Million || ? || ? || ? || ? || ? 
|?|| Up to 100 Million || Up to 100 Million || ? || ? || ? || ? || ? || Scalable to billions|| Scalable to billions
|-
| Indexing rate || How rapidly documents can be added or reprocessed into the index? || real time || ? || ? || ? || ? || ? 
|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || ? || per requirement, scalable || ?
|-
| Query-processing speed || How many queries per second ([[Queries per second|QPS]]) the engine can process? || ? || 2,000 <br /> across all indexed data with sub-second response times || ? || ? || ? || ? 
|Depends on setup and hardware|| ? || ? || ? || ? || ? || ? || scalable || || Depands on the datasource. (sub)-second times 
|-
|}

==== Platform readiness ====

{| class="wikitable sortable"
|-
! [[Search analytics]] !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect !! intergator
|-
| General system requirement || Minimum of available disk space. || 100 MB || ? || ? || ? || ? || 25 GB 
|?|| ? || ? || ? || ? || ? || ? || ? || 10 GB || 25 GB
|-
|  || Minimum RAM. || 8GB || ? || ? || ? || ? || 8 GB 
|?|| ? || ? || ? || ? || ? || ? || ? || 8 GB || 4 GB
|-
|  || Hard drives to store the data files. || ? || ? || ? || ? || ? || SCSI, SAS, SAN, over FC or SSD disks <br /> (as opposed to SATA disks) 
|?|| ? || ? || ? || ? || ? || ? || ? || optimised for throughput<br/> SCSI, SAS, SAN || SCSI, SAS, SAN, SSD, HDD
|-
|}

=== Vendor Intangibles ===

This section compares each software investment.

{| class="wikitable sortable"
|-
! Investment !! Description !! Lookeen Server !! HP IDOL !! Coveo for Advanced Enterprise Search !! Endeca Guided Search  !! Sinequa Enterprise Search !! EXALEAD CloudView 
!Datafari!! FAST for SharePoint 2010 !! SharePoint 2013 !! Google Search Appliance !! IBM Infosphere Data Explorer !! Lucidworks Fusion !! Perceptive Search !! Secure Enterprise Search !! RAVN Connect || intergator
|-
| Hardware costs || Total costs of servers. || 0 || ? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ? || Depands on the base licence
|-
| Installation costs || . || ? ||? || ? || ? || ? || ? 
|?|| ? || ? || ? || ? || ? || ? || ? || ?|| ?
|-
| License costs || . || ? ||? || ? || ? || ? || ? 
|0|| ? || ? || 500,000 Documents – $28,387 <br /> 1 million documents – $66,236 <br /> Upgrade from 1 million to 2 million documents – $1,971 <br /> 2 Million documents – $123,010 <br /> 2 Million documents – $123,010 <br /> 3 Million documents – $113, 548 <br /> 3 Million documents – $158,967 <br /> 5 Million documents – $433,766 <br /> 10 Million documents – $305,066 <br /> 10 Million – $423,913 <br /> 15 Million documents – $533,896 <br /> 15 Million documents with hot backup – $615,053 <br /> 30 Million documents with hot backup – $993,548 || ? || ? || ? || ? || ?|| ?
|-
| Annual Maintenance || Annual Maintenance fees. || ? || ? || ? || ? || ? || ? 
|Per server|| ? || ? || ? || ? || ? || ? || ?|| ?|| ?
|-
|}

== References ==

{{reflist}}

== First version ==

[[Category:Information retrieval systems]]
<=====doc_Id=====>:598
<=====title=====>:
RetrievalWare
<=====text=====>:
{{COI|date=June 2009}}
{{Infobox software
| name                   = RetrievalWare
| logo                   =
| screenshot             =
| caption                =
| developer              = [[Fast Search & Transfer]], [[Convera]], Excalibur Technologies, ConQuest Software, Microsoft
| latest release version = 8.2
| latest release date    = {{release date|2006|10|13}}
| latest preview version =
| latest preview date    =
| operating system       = [[Cross-platform]]
| programming language   = [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]]
| genre                  = [[Search algorithm|Search]] and [[Index (search engine)|Index]]
| website                =
}}
'''RetrievalWare''' is an [[enterprise search|enterprise search engine]] emphasizing [[natural language processing]] and [[semantic networks]] which was commercially available from 1992 to 2007 and is especially known for its use by government intelligence agencies.<ref>{{cite news
| url = http://www.washingtonpost.com/ac2/wp-dyn/A30161-2004Dec2
| title = Agencies Find What They're Looking For|publisher = The Washington Post
| date = 2004-12-03
| first=David A.
| last=Vise
| accessdate=2010-05-22
}}</ref>

== History ==

RetrievalWare was initially created by [http://www.linkedin.com/pub/paul-nelson/3/316/146 Paul Nelson], [http://kenclark7.home.comcast.net/~kenclark7/ Kenneth Clark], and [http://www.linkedin.com/in/edaddison Edwin Addison] as part of ConQuest Software. Development began in 1989, but the software was not commercially available on a wide scale until 1992. Early funding was provided by [[Rome Laboratory]] via a [[Small Business Innovation Research]] grant.<ref>
{{citation
| title = FY 1991 SBIR SOLICITATION - PHASE I AWARD ABSTRACTS - AIR FORCE PROJECTS - VOLUME III
| pages = 70–71
| date = 1992-07-06
| url = http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA252509&Location=U2&doc=GetTRDoc.pdf
}} - Note that "Synchronetics" was the original name for ConQuest Software Incorporated.
</ref>

On July 6, 1995, ConQuest Software was merged with Excalibur Technologies<ref>{{cite press release
 | title = Excalibur Technologies to merge with ConQuest Software; text and multimedia information retrieval leaders join forces to expand products, channels and markets| publisher = Business Wire
 | date = 1995-07-06
 | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_July_6/ai_17215774/?tag=content;col1
 }}</ref> and the product was rebranded as RetrievalWare. On December 21, 2000, Excalibur Technologies was combined with [[Intel|Intel Corporation]]'s Interactive Media Services division to form the [[Convera|Convera Corporation]].<ref>{{cite news
 | title = Intel and Excalibur Form Convera Corporation| publisher = Silicon Valley / San Jose Business Journal
 | date = 2000-12-21
 | url = http://sanjose.bizjournals.com/sanjose/stories/2000/12/25/daily5.html
 }}</ref> Finally, on April 9, 2007, the RetrievalWare software and business was purchased by [[Fast Search & Transfer]] at which point the product was officially retired.<ref name="fastpurchase">{{cite news
 | title = FAST Acquires Convera’s RetrievalWare Business| publisher = Information Today, Inc.
 | date = 2007-04-09
 | url = http://newsbreaks.infotoday.com/NewsBreaks/FAST-Acquires-Converas-RetrievalWare-Business-35840.asp
 | quote = While FAST will continue to support the RetrievalWare platform, it will not continue development on it or add new features. RetrievalWare customers will be offered an upgrade path to FAST’s own offering.
 }}</ref> [[Microsoft|Microsoft Corporation]] continues to maintain the product for its existing customer base.

Annual revenues for RetrievalWare peaked in 2001 at around $40 million US dollars.<ref>{{citation
| title = Convera Corp · 10-K · For 1/1/01
| date = 2001-01-01
| url = http://www.secinfo.com/d12B5f.4f89a.c.htm
}} - Indicates that Convera products accounted for 85% of the total revenue of $51.5 million.</ref>

== Use of natural language techniques ==

RetrievalWare is a relevancy ranking text search system with processing enhancements drawn from the fields of [[natural language processing|natural language processing (NLP)]] and [[semantic networks]]. NLP algorithms include dictionary-based [[stemming]] (also known as [[lemmatisation]]) and dictionary-based phrase identification. Semantic networks are used by RetrievalWare to expand the query words entered by the user to related terms with terms weights determined by the distance from the user's original terms. In addition to automatic expansion, a feedback-mode whereby users could choose the meaning of the word before performing the expansion was available. The first semantic networks were built using [[WordNet]].

In addition, RetrievalWare implemented a form of [[n-gram]] search (branded as APRP - Adaptive Pattern Recognition Processing<ref>[http://www.thefreelibrary.com/Excalibur+Announces+Excalibur+RetrievalWare+6.5+Featuring+...-a019849416 Excalibur Announces Excalibur RetrievalWare 6.5 Featuring RetrievalWare FileRoom] - Contains a description of APRP</ref>), designed to search over documents with [[Optical character recognition|OCR]] errors. Query terms are divided into sets of 2-grams which are used to locate similarly matching terms from the [[inverted index]]. The resulting matches are weighted based on similarly measures and then used to search for documents.

All of these features were available no later than 1993<ref name="trec2">[http://trec.nist.gov/pubs/trec2/papers/txt/25.txt Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC2)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec2/t2_proceedings.html here]</ref> and ConQuest software has claimed that it was the first commercial text-search system to implement these techniques.<ref>{{cite press release
 | title = Homework Helper debuts on Prodigy using ConQuest search engine| publisher = Business Wire
 | date = 1995-02-09
 | url = http://findarticles.com/p/articles/mi_m0EIN/is_1995_Feb_9/ai_16432681/
 | quote = ConQuest is the only search engine which uses dictionaries, thesauri and other lexical resources to build in a semantic knowledgebase of over 440,000 word meanings, and 1.6 million word relationships.
 }}</ref>

== Other notable features ==

Other notable features of RetrievalWare include distributed search servers,<ref name="trec2"/> synchronizers for indexing external [[content management system]]s and [[relational database]]s,<ref name="kmref">{{cite news
| url = http://www.kmworld.com/Articles/Editorial/Feature/Excalibur-RetrievalWare-more-than-information-retrieval--9139.aspx
| title = Excalibur RetrievalWare: more than information retrieval
| publisher = KMWorld
| date = 1999-10-01
}}</ref> a heterogeneous security model,<ref name="kmref"/> [[document classification|document categorization]],<ref name="kmref"/> real-time document-query matching (profiling),<ref name="trec2"/> multi-lingual searches (queries containing terms from multiple languages searching for documents containing terms from multiple languages), and cross-lingual searches (queries in one language searching for documents in a different language).<ref>{{cite news
| title = Multimedia search, retrieval, categorization
| url = http://www.kmworld.com/Articles/News/Breaking-News/Multimedia-search,-retrieval,-categorization-12763.aspx
| date = 2002-03-25
| publisher = KMWorld
}}</ref>

== Participation in TREC ==

RetrievalWare participated in the [[Text REtrieval Conference (TREC)|Text REtrieval Conference]] in 1992 (TREC-1), 1993 (TREC-2), and 1995 (TREC-4).

In TREC-1<ref name="trec1">[http://trec.nist.gov/pubs/trec1/papers/21.txt   Site Report for the Text REtrieval Conference by ConQuest Software Inc. (TREC-1)] - Find the complete proceedings [http://trec.nist.gov/pubs/trec1/t1_proceedings.html here]</ref> and TREC-4,<ref>[http://trec.nist.gov/pubs/trec4/papers/excalibur.ps.gz The Excalibur TREC-4 System, Preparations, and Results] - A PDF version of which can be found [http://www.pnelsoncomposer.com/writings/excalibur-trec4.pdf here] and the complete proceedings can be found [http://trec.nist.gov/pubs/trec4/t4_proceedings.html here]</ref> the RetrievalWare runs for manually entered queries produced the best results based on the 11-point averages over all search engines which participated in the ''ad hoc'' category where search engines are allowed a single opportunity to process previously unknown queries against an existing database.

== References ==
{{Reflist}}

== External links ==

*  [http://www.saoug.org.za/archive/1999/9907.pdf Marketing presentation on RetrievalWare semantic networks and adaptive pattern recognition algorithms]

{{DEFAULTSORT:Retrievalware}}
[[Category:Information retrieval systems]]
<=====doc_Id=====>:601
<=====title=====>:
Greenpilot
<=====text=====>:
{{COI|date=April 2010}}
The online portal '''Greenpilot''' is a service provided by the German National Library of Medicine, ZB MED.

The project is funded by the German Research Foundation ([[Deutsche Forschungsgemeinschaft]]) and gets its technical support from  [[Averbis]] Ltd. The portal first went online May 29, 2009 and currently runs in the updated beta version. In the context of the 'Germany - Land of Ideas' (Deutschland - Land der Ideen) initiative under the patronage of the [[President of Germany]] [[Horst Köhler]] the ZB MED was awarded the distinction 'Selected Landmark 2009' (Ausgewählter Ort 2009).<ref>[http://idw-online.de/pages/de/news315583 Pressemitteilung im Informationsdienst Wissenschaft vom 15. Mai 2009 ]</ref>

==Objective==
The Greenpilot portal is a [[digital library]] specialised in the fields of Nutritional, Agricultural and Environmental Sciences. It aims to provide researchers in the three fields with a collection of scientific literature which is easy to access and of high quality. Especially the [[gray literature]] is often difficult to find and retrieve for the average user so Greenpilot also aims to make access to these sources easier. The service addresses itself not only to scientists and students but also to the broadly interested public. Greenpilot has been modelled after the corresponding digital library for Medicine, Medpilot,<ref>[http://www.medpilot.de/ Medpilot portal]</ref> also a project of the German National Library of Medicine. The ZB MED has chosen the slogan 'Greenpilot - all about life and science' as a motto. In Greenpilot scientifically relevant databases, library catalogues and websites can be searched by entering a search term and the results are presented in a standardised web interface.

==Technical Background==
Greenpilot is a search engine based on intuitive search engine technology. The portal's software was developed in the programming language [[Perl]]. The search engine technology is based upon the 'Averbis Search Platform' software developed by the Averbis Ltd. and uses the [[open source]] software [[Lucene]]. Functionally this is an expert search engine which centres around the intelligent semantic connection of search terms by means of a standardised vocabulary. This is made possible by Averbis's MSI software which provides:

* semantic search optimised for the fields of Medicine and Life Sciences
* a contextual analysis of texts taking synonyms and compounds into account
* multilingual and cross-language search
* linking of lay and expert vocabulary
The search results are generated from a search index.

Additionally a [[metasearch]] can be conducted in order to search other databases not contained in the index. This search is based upon individual results from the specific database searched.

==Contents==
The Greenpilot portal integrates various scientifically relevant information resources under a uniform search interface. These resources are diverse and encompass national and international expert databases, library catalogues of national libraries with a focus on specific topics, full text documents from [[open access (publishing)|open access]] journals as well as information contained on about one thousand scientifically relevant websites selected for Greenpilot.
The following is a list of sources from November 2009:<ref>[http://www.greenpilot.de/beta2/app/misc/help/8cafcf93601eb861aaef86b5ce99ecdc/Datenbanken List of databases in Greenpilot]</ref>

===Library Catalogues===
* Catalogue of the German National Library of Medicine (ZB MED Nutrition. Environment. Agriculture)
* Catalogue of the German National Library of Medicine (ZB MED Medicine. Health)
* Catalogue of the Bonn University Library
* Library catalogues of scientifically relevant departments within the collective library network (GBV)
* Catalogue of the Federal Ministry of Food, Agriculture and Consumer Protection (BMELV)
* Catalogue of the Johann Heinrich von Thünen-Institut (vTI), Federal Research Institute for Rural Areas, Forestry and Fisheries
* Catalogue of the Julius Kühn-Institut, Federal Research Centre for Cultivated Plants
* Catalogue of the Friedrich Löffler-Institut, Federal Research Institute for Animal Health
* Catalogue of the Max Rubner-Institut, Federal Research Institute for Nutrition and Food
* Catalogue of the Federal Institute for Risk Assessment
* Catalogue of the Leibniz Institute for Marine Science (IFM-GEOMAR)
* Catalogue of the Leibniz Institute for Plant Genetics and Crop Plant Research (IPK-Plant Genetics and Crop Plant)
* Catalogue of the Leibniz Institute for Plant Biochemistry (IPB-Plant Chemistry)
* Catalogue of the special collection inshore and deep-sea fishery
* Catalogue of the University of Veterinary Medicine Hannover (TiHo-Veterinary Sciences)
* Catalogue of the German National Library of Economics (ZBW)

===Bibliographic databases===
* AGRIS (1975–2008), FAO ( Food and Agriculture Organization of the United Nations)
* VITIS-VEA, Viticulture and Enology Abstracts
* Medline (2004–2009)
* UFORDAT, Environmental Research Database (UBA)
* ULIDAT, Environmental Literature Database (UBA)
* ELFIS, International Information System for the Agricultural Sciences and Technology

===Relevant Internet Sources===
* Reviewed list of [[URL]]s selected by the ZB MED Nutrition. Environment. Agriculture
* Open Access journals with full text documents

===Metasearch===
* GetInfo, the knowledge portal for Technical Science provided by the Library for Technical Sciences (TIB) and the professional information centres FIZ Technik Frankfurt, FIZ Karlsruhe and FIZ CHEMIE Berlin.
* ECONIS, Catalogue of the German National Library of Economics (ZBW).

==Other Features==

===Search and results page===
* Search and advanced search
* Context sensitive help function
* [[Truncation]] and [[Boolean function]]s
* Personalised refining of search results by filtering for a specific document type, language or database
* [[Bookmark]]s

===Document ordering===
* Ordering directly from the results page is made possible by using the document delivery service of the ZB MED or the Electronic Journals Library ([[Elektronische Zeitschriftenbibliothek]]).

===Personalisation===
* My Greenpilot: a feature requiring the user to sign up for an account. The service is free of charge and offers an overview of ordered documents as well as enabling individual managing of customer data.

==See also==
*[[List of digital library projects]]
*[[vascoda]]

==References==
<references />

==External links==
* [http://www.greenpilot.de Greenpilot website]
* [http://www.zbmed.de/home.html?lang=en Website of the German National Library of Medicine, ZB MED]
* [http://www.land-of-ideas.org Germany - Land of Ideas website]

{{coord missing|Germany}}

[[Category:Libraries in Germany]]
[[Category:Information retrieval systems]]
[[Category:Internet search engines]]
<=====doc_Id=====>:604
<=====title=====>:
Collaborative search engine
<=====text=====>:
{{Recommender systems}}
'''Collaborative search engines''' (CSE) are [[Web search engine]]s and [[enterprise search]]es within company intranets that let users combine their efforts in [[information retrieval]] (IR) activities, share information resources collaboratively using [[knowledge tags]], and allow experts to guide less experienced people through their searches. Collaboration partners do so by providing query terms, collective tagging, adding comments or opinions, rating search results, and links clicked of former (successful) IR activities to users having the same or a related [[information need]].

== Models of collaboration ==

Collaborative search engines can be classified along several dimensions: intent (explicit and implicit) and synchronization
<ref name=Golo2007>{{citation
 | title = Collaborative Exploratory Search
 | year = 2007
 | author = Golovchinsky Gene
 | author2 = Pickens Jeremy
 | journal = Proceedings of HCIR 2007 workshop
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://projects.csail.mit.edu/hcir/web/hcir07.pdf
}}</ref> and depth of mediation 
,<ref name=Pickens2008>{{citation
 | title = Collaborative Exploratory Search
 | year = 2008
 | author = Pickens Jeremy
 | author2 = Golovchinsky Gene
 | author3 = Shah Chirag
 | author4 = Qvarfordt Pernilla
 | author5 = Back Maribeth
 | booktitle = SIGIR '08: Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval
 | pages = 315–322
 | volume = 
 | issue = 
 | doi = 10.1145/1390334.1390389
 | isbn = 
 9781605581644| url = http://portal.acm.org/citation.cfm?id=1390389
| chapter = Algorithmic mediation for collaborative exploratory search
 }}</ref> task vs. trait,<ref name=Morris2008>{{citation
 | contribution = Understanding Groups’ Properties as a Means of Improving Collaborative Search Systems
 | year = 2008
 | author = Morris Meredith
 | author2 = Teevan Jaime
 | title = 1st International Workshop on Collaborative Information Retrieval, held in conjunction with [[Joint Confrence on Digital Libraries|JCDL]] 2008
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | contribution-url = http://workshops.fxpal.com/jcdl2008/submissions/tmpDF.pdf
}}</ref> and division of labor and sharing of knowledge.<ref name=Foley2008>{{citation
 | title = Division of Labour and Sharing of Knowledge for Synchronous Collaborative Information Retrieval
 | year = 2008
 | author = Foley Colum
 | booktitle = PhD Thesis, Dublin City University
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.computing.dcu.ie/~cfoley/cfoley-PhD_thesis.pdf
}}</ref>

=== Explicit vs. implicit collaboration ===

Implicit collaboration characterizes [[Collaborative filtering]] and [[recommendation systems]] in which the system infers similar information needs. I-Spy,<ref name=Smith2003>{{citation
 | title = Collaborative Web Search
 | year = 2003
 | author = Barry Smyth
 | author2 = Evelyn Balfe
 | author3 = Peter Briggs
 | author4 = Maurice Coyle
 | author5 = Jill Freyne
 | journal = IJCAI
 | pages = 1417–1419
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> [[Jumper 2.0]], [[Seeks]], the Community Search Assistant,<ref name=Glance2001>{{citation
 | title = Community search assistant
 | year = 2001
 | author = Natalie S. Glance
 | journal = Workshop on AI for Web Search AAAI'02
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> the CSE of Burghardt et al.,<ref name=BurghardtWI2008>{{citation
 | title = Discovering the Scope of Privacy Needs in Collaborative Search
 | year = 2008
 | author = Thorben Burghardt
 | author2 = Erik Buchmann
 | author3 = Klemens Böhm
 | journal = Web Intelligence (WI)
 | pages = 
 910| volume = 
 | issue = 
 | doi = 10.1109/WIIAT.2008.165
 | isbn = 
 978-0-7695-3496-1}}</ref> and the works of Longo et al.
<ref name=Longo2009a>{{citation
 | title = Toward Social Search - From Explicit to Implicit Collaboration to Predict Users' Interests
 | year = 2009
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = ''[[Webist]]'' 2009 - Proceedings of the Fifth International Conference                on Web Information Systems and Technologies, Lisbon, Portugal,                March 23–26, 2009
 | pages = 693–696
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-989-8111-81-4
 | url = 
}}</ref> 
<ref name=Longo2010>{{citation
 | title = Enhancing Social Search: A Computational Collective Intelligence Model of Behavioural Traits, Trust and Time
 | year = 2010
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = Transaction Computational Collective Intelligence II
 | pages = 46–69
 | volume = 2
 | issue = 
 | doi = 10.1007/978-3-642-17155-0_3
 | isbn = 
 978-3-642-17154-3| url = http://www.springerlink.com/content/e12233858017h042/
| series = Lecture Notes in Computer Science
 }}</ref> 
<ref name=Longo2009b>{{citation
 | title = Information Foraging Theory as a Form of Collective Intelligence                for Social Search
 | year = 2009
 | author = Longo Luca
 | author2 = Barrett Stephen
 | author3 = Dondio Pierpaolo
 | journal = Computational Collective Intelligence. Semantic Web, Social                Networks and Multiagent Systems, First International Conference,                ICCCI 2009, Wroclaw, Poland, October 5–7, 2009. Proceedings
 | pages = 63–74
 | volume = 1
 | issue = 
 | doi = 
 | isbn = 978-3-642-04440-3
 | url = http://dl.acm.org/citation.cfm?id=1692026
}}</ref> 
all represent examples of implicit collaboration. Systems that fall under this category identify similar users, queries and links clicked automatically, and recommend related queries and links to the searchers.

Explicit collaboration means that users share an agreed-upon information need and work together toward that goal. For example, in a chat-like application, query terms and links clicked are automatically exchanged. The most prominent example of this class is SearchTogether<ref name=Morris2007>{{citation
 | title = SearchTogether: An Interface for Collaborative Web Search
 | year = 2007
 | author = Meredith Ringel Morris
 | author2 = Eric Horvitz
 | journal = UIST
| url = http://portal.acm.org/citation.cfm?id=1294211.1294215
}}</ref> published in 2007. SearchTogether offers an interface that combines search results from standard search engines and a chat to exchange queries and links. Reddy et al.<ref name=Redy2008>{{citation
 | title = The Role of Communication in Collaborative Information Searching
 | year = 2008
 | author = Madhu C. Reddy
 | author2 = Bernhard J. Jansen
 | author3 = Rashmi Krishnappa
 | journal = ASTIS
}}</ref> (2008) follow a similar approach and compares two implementations of their CSE called MUSE and MUST. Reddy et al. focuses on the role of communication required for efficient CSEs. Representatives for the class of implicit collaboration are I-Spy,<ref name="Smith2003"/> the Community Search Assistant,<ref name="Glance2001"/> and the CSE of Burghardt et al.<ref name="BurghardtWI2008" /> Cerciamo <ref name=Pickens2008 /> supports explicit collaboration by allowing one person to concentrate on finding promising groups of documents, while having the other person make in-depth judgments of relevance on documents found by the first person.

However, in Papagelis et al.<ref name=Papagelis2007>{{citation| title = Searchius: A Collaborative Search Engine| year = 2007| author = Athanasios Papagelis| author2 = Christos Zaroliagis| journal = ENC '07: Proceedings of the Eighth Mexican International Conference on Current Trends in Computer Science| pages = 88–98| doi = 10.1109/ENC.2007.34| url = http://portal.acm.org/citation.cfm?id=1302894| isbn = 0-7695-2899-6}}</ref> terms are used differently: they combine explicitly shared links and implicitly collected browsing histories of users to a hybrid CSE.

=== Community of practice  ===

Recent work in collaborative filtering and information retrieval has shown that sharing of search experiences among users having similar interests, typically called a [[community of practice]] or [[community of interest]], reduces the effort put in by a given user in retrieving the exact information of interest.<ref name=Rohini&Ambati>{{citation
 | title = A Collaborative Filtering based Re-ranking Strategy for Search in Digital Libraries
 | year = 2002
 | author = Rohini U
 | author2 = Vamshi Ambati
 | journal = ICADL2005: the 8th International Conference on Asian Digital Libraries
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://www.aaai.org/Papers/Workshops/2006/WS-06-10/WS06-10-004.pdf }}</ref>

Collaborative search deployed within a community of practice deploys novel techniques for exploiting context during search by indexing and ranking search results based on the learned preferences of a community of users.<ref name=Coyle2008>{{citation
 | title = Social Aspects of a Collaborative, Community-Based Search Network
 | editor4-first = Eelco
 | editor3-first = Pearl
 | editor2-first = Judy
 | editor1-first = Wolfgang
 | year = 2008
 | editor1-last = Nejdl
 | author = Maurice Coyle
 | author2 = Barry Smyth
 | last-author-amp = yes
 | journal = Adaptive Hypermedia and Adaptive Web-Based Systems
 | pages =  103–112  
 | volume = 5149/2008
 | issue = 
 | series = | doi = 10.1007/978-3-540-70987-9
 | isbn = 978-3-540-70984-8
 | url = http://portal.acm.org/citation.cfm?id=1485050
 | editor2-last = Kay
 | editor4-last = Herder
 | editor3-last = Pu| display-editors = 3}}</ref> The users benefit by sharing information, experiences and awareness to personalize result-lists to reflect the preferences of the community as a whole. The community representing a group of users who share common interests, similar professions.  The best known example is the open-source project [[ApexKB]] (previously known as Jumper 2.0).<ref name=Jumper2010>{{citation
 | title = Jumper Networks Releases Jumper 2.0.1.5 Platform with New Community Search Features
 | year = 2010
 | author = Jumper Networks Inc.
 | journal = Press release
 | pages = 
 | volume =
 | issue = 
 | doi =
 | isbn =
 | url = http://www.trilexnet.com/labs/jumper}}</ref>

=== Depth of mediation ===

This refers to the degree that the CSE mediates search.<ref name=Pickens2008 /> SearchTogether<ref name=Morris2007 /> is an example of UI-level mediation: users exchange query results and judgments of relevance, but the system does not distinguish among users when they run queries. Cerchiamo<ref name=Pickens2008 /> and recommendation systems such as I-Spy<ref name=Smith2003 /> keep track of each person's search activity independently, and use that information to affect their search results. These are examples of deeper algorithmic mediation.

=== Task vs. trait ===

This model classifies people's membership in groups based on the task at hand vs. long-term interests; these may be correlated with explicit and implicit collaboration.<ref name=Morris2008 />

== Privacy-aware collaborative search engines ==

Search terms and links clicked that are shared among users reveal their interests, habits, social
relations and intentions.<ref name=EUArticle29>{{citation
 | title = Article 29 EU Data Protection Working Party
 | year = 2008
 | author = Data Protection Working Party
 | journal = EU
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> In other words, CSEs put the privacy of the users at risk. Studies have shown that CSEs increase efficiency. 
<ref name="Morris2007"/><ref name=Smith2005>{{citation
 | title = A Live-User Evaluation of Collaborative Web Search
 | year = 2005
 | author = Barry Smyth
 | author2 = Evelyn Balfe
 | author3 = Oisin Boydell
 | author4 = Keith Bradley
 | author5 = Peter Briggs
 | author6 = Maurice Coyle
 | author7 = Jill Freyne
 | journal = IJCAI
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref>
<ref name=Smith2006>{{citation
 | title = Anonymous personalization in collaborative web search
 | year = 2005
 | author = Smyth, Barry
 | author2 = Balfe, Evelyn
 | last-author-amp = yes
 | journal = Inf. Retr.
 | pages = 165–190
 | volume = 9
 | issue = 2| doi = 10.1007/s10791-006-7148-z| isbn = 
 | url = 
}}</ref>
<ref name=Jung2004>{{citation
 | title = Applying Collaborative Filtering for Efficient Document Search
 | year = 2004
 | author = Seikyung Jung
 | author2 = Juntae Kim
 | author3 = Herlocker, JL
 | journal = Inf. Retr.
 | pages = 640–643
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = 
}}</ref> Unfortunately, by the lack of privacy enhancing technologies, a privacy aware user who wants to benefit from a CSE has to disclose his entire search log. (Note, even when explicitly sharing queries and links clicked, the whole (former) log is disclosed to any user that joins a search session).  Thus, sophisticated mechanisms that allow on a more fine grained level which information is disclosed to whom are desirable.

As CSEs are a new technology just entering the market, identifying user privacy preferences and integrating [[Privacy enhancing technologies]] (PETs) into collaborative search are in conflict. On one hand, PETs have to meet user preferences, on the other hand one cannot identify these preferences without using a CSE, i.e., implementing PETs into CSEs. Today, the only work addressing this problem comes from Burghardt et al.<ref name=BurghardtCC2008>{{citation
 | title = Collaborative Search And User Privacy: How Can They Be Reconciled?
 | year = 2008
 | author = Thorben Burghardt
 | author2 = Erik Buchmann
 | author3 = Klemens Böhm
 | author4 = Chris Clifton
 | journal = CollaborateCom
 | pages = 
 | volume = 
 | issue = 
 | doi = 
 | isbn = 
 | url = http://dbis.ipd.uni-karlsruhe.de/1184.php
}}</ref> They implemented a CSE with experts from the information system domain and derived the scope of possible privacy preferences in a user study with these experts. Results show that users define preferences referring to (i) their current context (e.g., being at work), (ii) the query content (e.g., users exclude topics from sharing), (iii) time constraints (e.g., do not publish the query X hours after the query has been issued, do not store longer than X days, do not share between working time), and that users intensively use the option to (iv) distinguish between different social groups when sharing information. Further, users require (v) anonymization and (vi) define reciprocal constraints, i.e., they refer to the behavior of other users, e.g., if a user would have shared the same query in turn.

== References ==
{{reflist|2}}
{{Internet search}}

[[Category:Information retrieval systems]]
<=====doc_Id=====>:607
<=====title=====>:
Search engine (computing)
<=====text=====>:
{{multiple issues|
{{more footnotes|date=August 2014}}
{{one source|date=August 2014}}
}}

A '''search engine''' is an [[information retrieval|information retrieval system]] designed to help find information stored on a [[computer system]]. The search results are usually presented in a list and are commonly called ''hits''. Search engines help to minimize the time required to find information and the amount of information which must be consulted, akin to other techniques for managing [[information overload]]. {{Citation needed|date=December 2007}}

The most public, visible form of a search engine is a [[Web search engine]] which searches for information on the [[World Wide Web]].

==How search engines work==
Search engines provide an [[interface (computer science)|interface]] to a group of items that enables users to specify criteria about an item of interest and have the engine find the matching items. The criteria are referred to as a [[Web search query|search query]]. In the case of text search engines, the search query is typically expressed as a set of words that identify the desired [[concept]] that one or more [[document]]s may contain.<ref>Voorhees, E.M. [http://www.indexnist.gov/itl/iad/894.02/works/papers/nlp_ir.ps Natural Language Processing and Information Retrieval]. National Institute of Standards and Technology. March 2000.</ref> There are several styles of search query [[syntax]] that vary in strictness. It can also switch names within the search engines from previous sites.  Whereas some text search engines require users to enter two or three words separated by [[Whitespace (computer science)|white space]], other search engines may enable users to specify entire documents, pictures, sounds, and various forms of [[natural language]]. Some search engines apply improvements to search queries to increase the likelihood of providing a quality set of items through a process known as [[query expansion]].

[[Image:search-engine-diagram-en.svg|right|thumb|Index-based search engine]]

The list of items that meet the criteria specified by the query is typically sorted, or ranked. Ranking items by relevance (from highest to lowest) reduces the time required to find the desired information. [[probability|Probabilistic]] search engines rank items based on measures of [[String metric|similarity]] (between each item and the query, typically on a scale of 1 to 0, 1 being most similar) and sometimes [[popularity]] or [[authority]] (see [[Bibliometrics]]) or use [[relevance feedback]]. [[Boolean logic|Boolean]] search engines typically only return items which match exactly without regard to order, although the term ''boolean search engine'' may simply refer to the use of boolean-style syntax (the use of operators [[Logical_conjunction|AND]], [[Logical_disjunction|OR]], NOT, and [[Exclusive nor gate|XOR]]) in a probabilistic context.

To provide a set of matching items that are sorted according to some criteria quickly, a search engine will typically collect [[metadata]] about the group of items under consideration beforehand through a process referred to as [[Index (search engine)|indexing]]. The index typically requires a smaller amount of [[computer storage]], which is why some search engines only store the indexed information and not the full content of each item, and instead provide a method of navigating to the items in the [[Serpent (album)|search engine result page]]. Alternatively, the search engine may store a copy of each item in a [[cache (computing)|cache]] so that users can see the state of the item at the time it was indexed or for archive purposes or to make repetitive processes work more efficiently and quickly.

Other types of search engines do not store an index. Crawler, or spider type search engines (a.k.a. real-time search engines) may collect and assess items at the time of the search query, dynamically considering additional items based on the contents of a starting item (known as a seed, or seed URL in the case of an Internet crawler). [[Meta search engine]]s store neither an index nor a cache and instead simply reuse the index or results of one or more other search engines to provide an aggregated, final set of results.

==Types of search engines==

; By source

*[[Desktop search]]
*[[Federated search]]
*[[Human search engine]]
*[[Metasearch engine]]
*[[Multisearch]]
*[[Search aggregator]]
*[[Web search engine]]

; By content type

*[[Full text search]]
*[[Image search]]
*[[Video search engine]]

; By interface

*[[Incremental search]]
*[[Instant answer]]
*[[Semantic search]]
*[[Selection-based search]]

; By topic

*[[Bibliographic database]]
*[[Enterprise search]]
*[[Medical literature retrieval]]
*[[Vertical search]]

==See also==
{{Portal|Computer Science}}
{{div col|colwidth=30em}}
*[[Automatic summarization]]
*[[Emanuel Goldberg]] (inventor of early search engine)
*[[Index (search engine)]]
*[[Inverted index]]
*[[List of search engines]]
*[[List of enterprise search vendors]]
*[[Search engine optimization]]
*[[Search suggest drop-down list]]
* [[Solver (computer science)]]
*[[Spamdexing]]
*[[SQL]]
*[[Text mining]]
{{div col end}}

==References==
{{Reflist}}
{{Internet search}}

{{Authority control}}
{{DEFAULTSORT:Search Engine (Computing)}}
[[Category:Information retrieval systems]]
<=====doc_Id=====>:610
<=====title=====>:
Relevance feedback
<=====text=====>:
'''Relevance [[feedback]]''' is a feature of some [[information retrieval]] systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback.

== Explicit feedback ==

Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as [[Relevance (information retrieval)|relevance]] judgments.

Users may indicate relevance explicitly using a ''binary'' or ''graded'' relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as "not relevant", "somewhat relevant", "relevant", or "very relevant"). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the [[SearchWiki]] feature implemented by [[Google]] on their search website.

The relevance feedback information needs to be interpolated with the original query to improve retrieval performance, such as the well-known [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

A performance [[Metric (mathematics)|metric]] which became popular around 2005 to measure the usefulness of a ranking [[algorithm]] based on the explicit relevance feedback is [[NDCG]]. Other measures include [[Precision (information retrieval)|precision]] at ''k'' and [[Mean average precision#Mean average precision|mean average precision]].

== Implicit feedback ==

Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions [http://www.scils.rutgers.edu/etc/mongrel/kelly-belkin-SIGIR2001.pdf]. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response<ref>Jansen, B. J. and McNeese, M. D. 2005. [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_assistance_jasist2005.pdf Evaluating the effectiveness of and patterns of interactions with automated assistance in IR systems]. Journal of the American Society for Information Science and Technology. 56(14), 1480-1503</ref><ref>Kelly, Diane, and Jaime Teevan. "Implicit feedback for inferring user preference: a bibliography." ACM SIGIR Forum. Vol. 37. No. 2. ACM, 2003.</ref>

The key differences of implicit relevance feedback from that of explicit include [http://haystack.lcs.mit.edu/papers/kelly.sigirforum03.pdf]:

# the user is not assessing relevance for the benefit of the IR system, but only satisfying their own needs and
# the user is not necessarily informed that their behavior (selected documents) will be used as relevance feedback

An example of this is [[dwell time (information retrieval)|dwell time]], which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.
Another example of this is the [[Surf Canyon]] [[browser extension]], which advances search results from later pages of the result set based on both user interaction (clicking an icon) and time spent viewing the page linked to in a search result.

== Blind feedback ==

Pseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top "k" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:

# Take the results returned by initial query as relevant results (only top k with k being between 10 and 50 in most experiments).
# Select top 20-30 (indicative number) terms from these documents using for instance [[tf-idf]] weights.
# Do Query Expansion, add these terms to query, and then match the returned documents for this query and finally return the most relevant documents.

Some experiments such as results from the Cornell SMART system published in (Buckley et al.1995), show improvement of retrieval systems performances using pseudo-relevance feedback in the context of TREC 4 experiments.

This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.<ref>Jinxi Xu and W. Bruce Croft, [http://portal.acm.org/citation.cfm?id=243202''Query expansion using local and global document analysis''], in Proceedings of the 19th annual international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 1996.</ref> Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task {{Citation needed|date=March 2011}}. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents.<ref>Yuanhua Lv and ChengXiang Zhai, [http://portal.acm.org/citation.cfm?id=1835546''Positional relevance model for pseudo-relevance feedback''], in Proceedings of the 33rd international ACM SIGIR conference on Research and development in information retrieval (SIGIR), 2010.</ref> 
Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.

Blind feedback automates the manual part of relevance feedback and has the advantage that assessors are not required.

== Using relevance information ==

Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the [[Rocchio Classification#Algorithm|Rocchio Algorithm]].

==Further reading==
*[http://www.umiacs.umd.edu/~jimmylin/LBSC796-INFM718R-2006-Spring/lecture7.ppt Relevance feedback lecture notes] - Jimmy Lin's lecture notes, adapted from Doug Oard's
*[http://www.ischool.berkeley.edu/~hearst/irbook/chapters/chap10.html] - chapter from ''Modern Information Retrieval''
*Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

== References ==
{{reflist|2}}

[[Category:Internet search algorithms]]
[[Category:Information retrieval evaluation]]
[[zh:相关反馈]]
<=====doc_Id=====>:613
<=====title=====>:
Overlap coefficient
<=====text=====>:
{{unreferenced|date=June 2016}}
The '''overlap coefficient''' (or, '''Szymkiewicz-Simpson coefficient''') is a [[String_metric|similarity measure]] related to the [[Jaccard index]] that measures the overlap between two sets, and is defined as the size of the [[intersection (set theory)|intersection]] divided by the smaller of the size of the two sets:

:<math>\mathrm{overlap}(X,Y) = \frac{| X \cap Y | }{\min(|X|,|Y|)}</math>

If set ''X'' is a [[subset]] of ''Y'' or the converse then the overlap coefficient is equal to one.

==References==
<references />

[[Category:Information retrieval techniques]]
[[Category:Information retrieval evaluation]]
[[Category:String similarity measures]]
[[Category:Measure theory]]
<=====doc_Id=====>:616
<=====title=====>:
Mooers' law
<=====text=====>:
{{For|the observation regarding integrated circuits|Moore's law}}
{{Refimprove|date=September 2011}}

'''Mooers' law''' is an empirical observation of behavior made by American [[computer scientist]] [[Calvin Mooers]] in 1959. The observation is made in relation to [[information retrieval]] and the interpretation of the observation is used commonly throughout the information profession both within and outside its original context.

{{quote|An information retrieval system will tend not to be used whenever it is more painful and troublesome for a customer to have information than for him not to have it.|[[Calvin Mooers]]<ref name="morville">{{cite book|url=https://books.google.com/books?id=xJNLJXXbhusC&printsec=frontcover&dq=isbn:9780596007652&hl=en&sa=X&ei=qvWhT5DfHITs2QX1rNzPCA&ved=0CDAQ6AEwAA#v=onepage&q=mooers'%20law&f=false |title= Ambient findability |series= O'Reilly Series. Marketing/Technology & Society |author= Peter Morville |edition= illustrated |publisher= O'Reilly Media |year= 2005 |page= 44|isbn= 978-0-596-00765-2}}</ref>}}

==Original interpretation==

Mooers argued that information is at risk of languishing unused due not only on the effort required to assimilate it but also to any fallout that could arise from the discovery of information that conflicts with the user's personal, academic or corporate interests. In interacting with new information, a user runs the risk of proving their work incorrect or even irrelevant. Instead, Mooers argued, users prefer to remain in a state of safety in which new arguments are ignored in an attempt to save potential embarrassment or reprisal from supervisors.<ref>{{cite web|last=Mooers|first=Calvin|title=Mooers Law, or Why some Retrieval Systems are Used and Others Are not|url=http://findarticles.com/p/articles/mi_qa3633/is_199610/ai_n8749122/|work=Business Library|accessdate=25 October 2011}}</ref>

==Out-of-context interpretation==

The more commonly used interpretation of Mooers' law is considered to be a derivation of the [[principle of least effort]] first stated by [[George Kingsley Zipf]]. This interpretation focuses on the amount of effort that will be expended to use and understand a particular information retrieval system before the information seeker 'gives up', and the Law is often paraphrased to increase the focus on the retrieval system:

{{quote|The more difficult and time consuming it is for a customer to use an information system, the less likely it is that he will use that information system.|J. Michael Pemberton}}
{{quote|Mooers' Law tells us that information will be used in direct proportion to how easy it is to obtain.|Roger K. Summit <ref name="morville"/>}}

In this interpretation, "painful and troublesome" comes from ''using'' the retrieval system.

==References==
{{reflist}}

*{{cite journal |last=Austin |first=Brice |date=June 2001 |title=Mooers' Law: In and out of Context |journal=Journal of the American Society for Information Science and Technology |volume=25 |issue=8 |pages=607–609 |url=http://spot.colorado.edu/~norcirc/Mooers.html |accessdate=2007-05-23 |doi=10.1002/asi.1114}}

==External links==
* [http://special.lib.umn.edu/findaid/xml/cbi00081.xml Calvin N. Mooers Papers, 1930-1992] at the [[Charles Babbage Institute]], University of Minnesota.
* [http://purl.umn.edu/107510 Oral history interview with Calvin N. Mooers and Charlotte D. Mooers] at the [[Charles Babbage Institute]].  Interview discusses information retrieval and programming language research from World War II through the early 1990s.
[[Category:Empirical laws]]
[[Category:Information retrieval evaluation]]
<=====doc_Id=====>:619
<=====title=====>:
Spearman's rank correlation coefficient
<=====text=====>:
[[File:spearman fig1.svg|300px|thumb|A Spearman correlation of 1 results when the two variables being compared are monotonically related, even if their relationship is not linear. This means that all data-points with greater x-values than that of a given data-point will have greater y-values as well. In contrast, this does not give a perfect Pearson correlation.]][[File:spearman fig2.svg|300px|thumb|When the data are roughly elliptically distributed and there are no prominent outliers, the Spearman correlation and Pearson correlation give similar values.]]
[[File:spearman fig3.svg|300px|thumb|The Spearman correlation is less sensitive than the Pearson correlation to strong outliers that are in the tails of both samples. That is because Spearman's rho limits the outlier to the value of its rank.]] In [[statistics]], '''Spearman's rank correlation coefficient''' or '''Spearman's rho''', named after [[Charles Spearman]] and often denoted by the Greek letter [[rho (letter)|<math>\rho</math>]] (rho) or as <math>r_s</math>, is a [[non-parametric statistics|nonparametric]] measure of [[rank correlation]] ([[correlation and dependence|statistical dependence]] between the [[ranking]] of two [[Variable (mathematics)#Applied statistics|variables]]). It assesses how well the relationship between two variables can be described using a [[monotonic]] function.

The '''Spearman correlation''' between two variables is equal to the [[Pearson product-moment correlation coefficient|Pearson correlation]] between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other.

Intuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) [[Ranking|rank]] (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of -1) rank between the two variables.

Spearman's coefficient is appropriate for both [[continuous variable|continuous]] and [[discrete variable]]s, including [[Level of measurement#Ordinal scale|ordinal]] variables.<ref>[[Level of measurement#Typology|Scale types]]</ref><ref>{{cite book|title=Jmp For Basic Univariate And Multivariate Statistics: A Step-by-step Guide|last=Lehman|first=Ann|publisher=SAS Press|year=2005|isbn=1-59047-576-3|location=Cary, NC|page=123}}</ref> Both Spearman's <math>\rho</math> and [[Kendall tau rank correlation coefficient|Kendall's <math>\tau</math>]] can be formulated as special cases of a more [[general correlation coefficient]].

==Definition and calculation==
The Spearman correlation coefficient is defined as the [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]] between the [[Ranking|ranked variables]].<ref name="myers2003">{{Cite book | last1=Myers | first1=Jerome L. | first2=Arnold D.  |last2= Well | title=Research Design and Statistical Analysis | publisher=Lawrence Erlbaum | year=2003 | edition=2nd | isbn=0-8058-4037-0 | pages=508}}</ref>

For a sample of size ''n'', the ''n'' [[raw score]]s <math>X_i, Y_i</math> are converted to ranks <math>\operatorname{rg} X_i, \operatorname{rg} Y_i</math>, and <math>r_s</math> is computed from:

:<math>r_s = \rho_{\operatorname{rg}_X,\operatorname{rg}_Y} = \frac {\operatorname{cov}(\operatorname{rg}_X,\operatorname{rg}_Y)} { \sigma_{\operatorname{rg}_X} \sigma_{\operatorname{rg}_Y} }</math>
:: where
::* <math>\rho</math> denotes the usual [[Pearson product-moment correlation coefficient|Pearson correlation coefficient]], but applied to the rank variables.
::* <math>\operatorname{cov}(\operatorname{rg}_X, \operatorname{rg}_Y)</math> is the [[covariance]] of the rank variables.
::* <math>\sigma_{\operatorname{rg}_X}</math> and <math>\sigma_{\operatorname{rg}_Y}</math> are the [[standard deviation]]s of the rank variables.

Only if all ''n'' ranks are ''distinct integers'', it can be computed using the popular formula

:<math> r_s = {1- \frac {6 \sum d_i^2}{n(n^2 - 1)}}.</math>
:: where
::* <math>d_i = \operatorname{rg}(X_i) - \operatorname{rg}(Y_i)</math>, is the difference between the two ranks of each observation.
::* ''n'' is the number of observations

Identical values are usually{{citation needed|date=May 2016}} each assigned [[Ranking#Fractional ranking .28.221 2.5 2.5 4.22 ranking.29|fractional ranks]] equal to the average of their positions in the ascending order of the values, which is equivalent to averaging over all possible permutations.

If ties are present in the data set, this equation yields incorrect results: Only if in both variables all ranks are distinct, then <math>\sigma_{\operatorname{rg}_X} \sigma_{\operatorname{rg}_Y} = \operatorname{Var}{\operatorname{rg}_X} = \operatorname{Var}{\operatorname{rg}_Y} = n(n^2 - 1)/6</math> (cf. [[tetrahedral number]] <math>T_{n-1}</math>).
The first equation—normalizing by the standard deviation—may even be used even when ranks are normalized to [0;1] ("relative ranks") because it is insensitive both to translation and linear scaling.
<!-- For example, if [1,2,3,4,5] vs. [1,3,3,3,5] has r_s=0.894, but the simplified formula yields 0.877. -->

This method should also not be used in cases where the data set is truncated; that is, when the Spearman correlation coefficient is desired for the top X records (whether by pre-change rank or post-change rank, or both), the user should use the Pearson correlation coefficient formula given above.{{citation needed|date=September 2015}}

The standard error of the coefficient (''σ'') was determined by Pearson in 1907 and Gosset in 1920. It is

: <math> \sigma_{r_s} = \frac{ 0.6325 }{ \sqrt{n-1} } </math>

==Related quantities==
{{Main article|Correlation and dependence}}

There are several other numerical measures that quantify the extent of [[statistical dependence]] between pairs of observations. The most common of these is the [[Pearson product-moment correlation coefficient]], which is a similar correlation method to Spearman's rank, that measures the “linear” relationships between the raw numbers rather than between their ranks.

An alternative name for the Spearman [[rank correlation]] is the “grade correlation”;<ref name="Yule and Kendall">{{cite book |last=Yule |first=G.  U. |last2=Kendall |first2=M.  G. |orig-year=1950 |title=An Introduction to the Theory of Statistics |edition=14th |year=1968 |publisher=Charles Griffin & Co. |page=268 }}</ref> in this, the “rank” of an observation is replaced by the “grade”. In continuous distributions, the grade of an observation is, by convention, always one half less than the rank, and hence the grade and rank correlations are the same in this case. More generally, the “grade” of an observation is proportional to an estimate of the fraction of a population less than a given value, with the half-observation adjustment at observed values. Thus this corresponds to one possible treatment of tied ranks. While unusual, the term “grade correlation” is still in use.<ref>{{cite journal |last=Piantadosi |first=J. |last2=Howlett |first2=P. |last3=Boland |first3=J. |year=2007 |title=Matching the grade correlation coefficient using a copula with maximum disorder |journal=Journal of Industrial and Management Optimization |volume=3 |issue=2 |pages=305–312 |doi= |url=http://aimsciences.org/journals/pdfs.jsp?paperID=2265&mode=abstract }}</ref>

==Interpretation==
{| style="float: right;"
|+ '''Positive and negative Spearman rank correlations'''
|- 
| [[File:spearman fig5.svg|300px|left|thumb|A positive Spearman correlation coefficient corresponds to an increasing monotonic trend between ''X'' and ''Y''.]]
| [[File:spearman fig4.svg|300px|thumb|A negative Spearman correlation coefficient corresponds to a decreasing monotonic trend between ''X'' and ''Y''.]]
|}

The sign of the Spearman correlation indicates the direction of association between ''X'' (the independent variable) and ''Y'' (the dependent variable).  If  ''Y'' tends to increase when ''X'' increases, the Spearman correlation coefficient is positive.  If ''Y'' tends to decrease when ''X'' increases, the Spearman correlation coefficient is negative.  A Spearman correlation of zero indicates that there is no tendency for ''Y'' to either increase or decrease when ''X'' increases.  The Spearman correlation increases in magnitude as ''X'' and ''Y'' become closer to being perfect monotone functions of each other.  When ''X'' and ''Y'' are perfectly monotonically related, the Spearman correlation coefficient becomes 1.  A perfect monotone increasing relationship implies that for any two pairs of data values {{math|''X''<sub>''i''</sub>, ''Y''<sub>''i''</sub>}} and {{math|''X''<sub>''j''</sub>, ''Y''<sub>''j''</sub>}}, that {{math|''X''<sub>''i''</sub> − ''X''<sub>''j''</sub>}} and {{math|''Y''<sub>''i''</sub> − ''Y''<sub>''j''</sub>}} always have the same sign.  A perfect monotone decreasing relationship implies that these differences always have opposite signs.

The Spearman correlation coefficient is often described as being "nonparametric".  This can have two meanings:  First, a perfect Spearman correlation results when ''X'' and ''Y'' are related by any [[monotonic function]]. Contrast this with the Pearson correlation, which only gives a perfect value when ''X'' and ''Y'' are related by a ''linear'' function. The other sense in which the Spearman correlation is nonparametric in that its exact sampling distribution can be obtained without requiring knowledge (''i.e.'', knowing the parameters) of the joint [[probability distribution]] of ''X'' and ''Y''.

==Example==
In this example, the raw data in the table below is used to calculate the correlation between the [[IQ]] of a person with the number of hours spent in front of [[TV]] per week.
{| class="wikitable sortable" style="text-align:right;"
|-
![[IQ]], <math>X_i</math>
!Hours of [[TV]] per week, <math>Y_i</math>
|-
|106
|7
|-
|86
|0
|-
|100
|27
|-
|101
|50
|-
|99
|28
|-
|103
|29
|-12
|97
|20
|-
|113
|12
|-
|112
|6
|-
|110
|17
|}

Firstly, evaluate <math>d^2_i</math>. To do so use the following steps, reflected in the table below.
# Sort the data by the first column (<math>X_i</math>). Create a new column <math>x_i</math> and assign it the ranked values 1,2,3,...''n''.
# Next, sort the data by the second column (<math>Y_i</math>). Create a fourth column <math>y_i</math> and similarly assign it the ranked values 1,2,3,...''n''.
# Create a fifth column <math>d_i</math> to hold the differences between the two rank columns (<math>x_i</math> and <math>y_i</math>).
# Create one final column <math>d^2_i</math> to hold the value of column <math>d_i</math> squared.

{| class="wikitable sortable" style="text-align:right;"
|-
![[IQ]], <math>X_i</math>
!Hours of [[TV]] per week, <math>Y_i</math>
!rank <math>x_i</math>
!rank <math>y_i</math>
!<math>d_i</math>
!<math>d^2_i</math>
|-
|86
|0
|1
|1
|0
|0
|-
|97
|20
|2
|6
| −4
|16
|-
|99
|28
|3
|8
| −5
|25
|-
|100
|27
|4
|7
| −3
|9
|-
|101
|50
|5
|10
| −5
|25
|-
|103
|29
|6
|9
| −3
|9
|-
|106
|7
|7
|3
|4
|16
|-
|110
|17
|8
|5
|3
|9
|-
|112
|6
|9
|2
|7
|49
|-
|113
|12
|10
|4
|6
|36
|}

With <math>d^2_i</math> found, add them to find <math>\sum d_i^2 = 194</math>. The value of ''n'' is 10. These values can now be substituted back into the equation: <math> \rho = 1- {\frac {6 \sum d_i^2}{n(n^2 - 1)}}.</math> to give

:<math> \rho = 1- {\frac {6\times194}{10(10^2 - 1)}}</math>

which evaluates to {{math|1=''ρ'' = -29/165 = −0.175757575...}}
with a [[P-value]] = 0.627188 (using the [[Student's t-distribution|t distribution]])

[[File:Spearman's Rank chart.png|thumb|Chart of the data presented. It can be seen that there might be a negative correlation, but that the relationship does not appear definitive.]]
This low value shows that the correlation between IQ and hours spent watching TV is very low, although the negative value suggests that the longer the time spent watching television the lower the IQ. In the case of ties in the original values, this formula should not be used; instead, the Pearson correlation coefficient should be calculated on the ranks (where ties are given ranks, as described above).

==Determining significance==
One approach to test whether an observed value of ρ is significantly different from zero (''r'' will always maintain −1 ≤ ''r'' ≤ 1) is to calculate the probability that it would be greater than or equal to the observed ''r'', given the [[null hypothesis]], by using a [[Resampling (statistics)#Permutation tests|permutation test]]. An advantage of this approach is that it automatically takes into account the number of tied data values there are in the sample, and the way they are treated in computing the rank correlation.

Another approach parallels the use of the [[Fisher transformation]] in the case of the Pearson product-moment correlation coefficient. That is, [[confidence intervals]] and [[hypothesis test]]s relating to the population value ρ can be carried out using the Fisher transformation:

: <math>F(r) = {1 \over 2}\ln{1+r \over 1-r} = \operatorname{artanh}(r).</math>

If ''F''(''r'') is the Fisher transformation of ''r'', the sample Spearman rank correlation coefficient, and ''n'' is the sample size, then

:<math>z = \sqrt{\frac{n-3}{1.06}}F(r)</math>

is a [[standard score|z-score]] for ''r'' which approximately follows a standard [[normal distribution]] under the [[null hypothesis]] of [[statistical independence]] ({{math|1=''ρ'' = 0}}).<ref>{{cite journal |last=Choi |first=S. C. |year=1977 |title=Tests of Equality of Dependent Correlation Coefficients |journal=[[Biometrika]] |volume=64 |issue=3 |pages=645–647 |doi=10.1093/biomet/64.3.645 }}</ref><ref>{{cite journal |last=Fieller |first=E. C. |last2=Hartley |first2=H. O. |last3=Pearson |first3=E. S. |year=1957 |title=Tests for rank correlation coefficients. I |journal=Biometrika |volume=44 |issue= |pages=470–481 |doi=10.1093/biomet/44.3-4.470}}</ref>

One can also test for significance using

:<math>t = r \sqrt{\frac{n-2}{1-r^2}}</math>

which is distributed approximately as [[Student's t distribution]] with {{math|''n'' − 2}} degrees of freedom under the [[null hypothesis]].<ref>{{cite book |last=Press |last2=Vettering |last3=Teukolsky |last4=Flannery |year=1992 |title=Numerical Recipes in C: The Art of Scientific Computing |edition=2nd |page=640 }}</ref> A justification for this result relies on a permutation argument.<ref>{{cite book |last=Kendall |first=M. G. |last2=Stuart |first2=A. |year=1973 |title=The Advanced Theory of Statistics, Volume 2: Inference and Relationship |publisher=Griffin |isbn=0-85264-215-6 }} (Sections 31.19, 31.21)</ref>

pvrank<ref>{{cite web|last1=Amerise|first1=I.L.|last2=Marozzi|first2=M.|last3=Tarsitano|first3=A.|title=R package pvrank|url=https://cran.r-project.org/web/packages/pvrank/index.html}}</ref> is a very recent [[R (programming language)|R]] package that computes rank correlations and their p-values with various options for tied ranks. It is possible to compute exact Spearman coefficient test p-values for ''n'' ≤ 26.

A generalization of the Spearman coefficient is useful in the situation where there are three or more conditions, a number of subjects are all observed in each of them, and it is predicted that the observations will have a particular order.  For example, a number of subjects might each be given three trials at the same task, and it is predicted that performance will improve from trial to trial.  A test of the significance of the trend between conditions in this situation was developed by E. B. Page<ref>{{cite journal |author=Page, E. B. |title=Ordered hypotheses for multiple treatments: A significance test for linear ranks |journal=Journal of the American Statistical Association |volume=58 |pages=216–230 |year=1963 |doi=10.2307/2282965 |issue=301}}
</ref> and is usually referred to as [[Page's trend test]] for ordered alternatives.

==Correspondence analysis based on Spearman's rho==
Classic [[correspondence analysis]] is a statistical method that gives a score to every value of two nominal variables. In this way the Pearson [[Pearson product-moment correlation coefficient|correlation coefficient]] between them is maximized.

There exists an equivalent of this method, called [[grade correspondence analysis]], which maximizes Spearman's rho or [[Kendall's tau]].<ref>{{cite book|editor1-last=Kowalczyk|editor1-first=T.|editor2-last=Pleszczyńska|editor2-first=E.|editor3-last=Ruland|editor3-first=F.| year=2004|title=Grade Models and Methods for Data Analysis with Applications for the Analysis of Data Populations|series=Studies in Fuzziness and Soft Computing |volume=151|publisher=Springer Verlag|location=Berlin Heidelberg New York|isbn=978-3-540-21120-4}}</ref>

==See also==
{{Portal|Statistics}}
* [[Kendall tau rank correlation coefficient]]
* [[Chebyshev's sum inequality]], [[rearrangement inequality]] (These two articles may shed light on the mathematical properties of Spearman's ρ.)
*[[Distance correlation]]

==References==
{{Reflist|30em}}

==Further reading==
* Corder, G.W. & Foreman, D.I. (2014). Nonparametric Statistics: A Step-by-Step Approach, Wiley. ISBN 978-1118840313.
* {{cite book |last=Daniel |first=Wayne W. |chapter=Spearman rank correlation coefficient |title=Applied Nonparametric Statistics |location=Boston |publisher=PWS-Kent |edition=2nd |year=1990 |isbn=0-534-91976-6 |pages=358–365 |chapterurl=https://books.google.com/books?id=0hPvAAAAMAAJ&pg=PA358 }}
* {{Cite journal |author=Spearman C |title=The proof and measurement of association between two things |journal=American Journal of Psychology |volume=15 |year=1904 |pages=72–101 |doi=10.2307/1412159}}
* {{Cite journal |author=Bonett DG, Wright, TA |title=Sample size requirements for Pearson, Kendall, and Spearman correlations |journal=Psychometrika |volume=65 |year=2000 |pages=23–28 |doi=10.1007/bf02294183}}
* {{Cite book |author=Kendall MG |title=Rank correlation methods |location=London |publisher=Griffin |year=1970 |edition=4th |isbn=978-0-852-6419-96 |oclc=136868}}
* {{Cite book |vauthors=Hollander M, Wolfe DA |title=Nonparametric statistical methods |location=New York |publisher=Wiley |year=1973 |isbn=978-0-471-40635-8 |oclc=520735}}
* {{Cite journal |vauthors=Caruso JC, Cliff N |title=Empirical size, coverage, and power of confidence intervals for Spearman's Rho |journal=Educational and Psychological Measurement |volume=57 |year=1997 |pages=637–654 |doi=10.1177/0013164497057004009}}

==External links==
{{Wikiversity}}
*[http://www.crystalballservices.com/Resources/ConsultantsCornerBlog/EntryId/73/Copulas-Vs-Correlation.aspx "Understanding Correlation vs. Copulas in Excel"] by Eric Torkia, Technology Partnerz 2011
*[http://www.sussex.ac.uk/Users/grahamh/RM1web/Rhotable.htm Table of critical values of ρ for significance with small samples]
*[http://www.maccery.com/maths Spearman's rank online calculator]
*[https://www.answerminer.com/calculators/correlation-test Spearman correlation calculator with human-readable explanation]
*[http://faculty.vassar.edu/lowry/webtext.html Chapter 3 part 1 shows the formula to be used when there are ties]
*[http://statistical-research.com/wp-content/uploads/2012/08/Spearman.pdf An example of how to calculate Spearman's Rho along with basic R code.]
* [https://www.rgs.org/NR/rdonlyres/4844E3AB-B36D-4B14-8A20-3A3C28FAC087/0/OASpearmansRankExcelGuidePDF.pdf Spearman’s Rank Correlation Coefficient – Excel Guide]: sample data and formulae for Excel, developed by the [[Royal Geographical Society]].
*[http://udel.edu/~mcdonald/statspearman.html Spearman's rank correlation]: Simple notes for students with an example of usage by biologists and a spreadsheet for [[Microsoft Excel]] for calculating it (a part of materials for a ''Research Methods in Biology'' course).
{{Statistics|descriptive}}

{{DEFAULTSORT:Spearman's Rank Correlation Coefficient}}
[[Category:Covariance and correlation]]
[[Category:Information retrieval evaluation]]
[[Category:Nonparametric statistics]]
[[Category:Statistical tests]]
<=====doc_Id=====>:622
<=====title=====>:
Category:Search algorithms
<=====text=====>:
{{Commons category|Search algorithms}}
{{Cat main|Search algorithms}}

[[Category:Algorithms]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:625
<=====title=====>:
Controlled vocabulary
<=====text=====>:
{{refimprove|date=June 2012}}

'''Controlled vocabularies''' provide a way to organize knowledge for subsequent retrieval.  They are used in [[subject indexing]] schemes, [[subject heading]]s, [[thesauri]],<ref>[https://web.archive.org/web/20101204132228/http://www.imresources.fit.qut.edu.au:80/vocab/ Controlled Vocabularies]  Links to examples of thesauri and classification schemes.</ref><ref>[https://web.archive.org/web/20090314094707/http://www.fao.org/aims/kos_list_type.htm Controlled Vocabularies]  Links to examples of thesauri and classification schemes used in the domain of Agriculture, Fisheries, Forestry etc.</ref> [[Taxonomy (general)|taxonomies]] and other forms of [[knowledge organization system]]s. Controlled vocabulary schemes mandate the use of predefined, authorised terms that have been preselected by the designers of the schemes, in contrast to natural language vocabularies, which have no such restriction.

== In library and information science ==

In [[library and information science]] controlled vocabulary is a carefully selected list of [[word (linguistics)|word]]s and [[phrase]]s, which are used to [[Tag (metadata)|tag]] units of information (document or work) so that they may be more easily retrieved by a search.<ref>Amy Warner, [http://www.ischool.utexas.edu/~i385e/readings/Warner-aTaxonomyPrimer.html A taxonomy primer].</ref><ref>Karl Fast, Fred Leise and Mike Steckel, [http://boxesandarrows.com/what-is-a-controlled-vocabulary/]</ref> Controlled vocabularies solve the problems of [[homographs]], [[synonyms]] and [[polyseme]]s by a [[bijection]] between concepts and authorized terms. In short, controlled vocabularies reduce ambiguity inherent in normal human languages where the same concept can be given different names and ensure consistency.

For example, in the [[Library of Congress Subject Headings]] (a subject heading system that uses a controlled vocabulary), authorized terms—subject headings in this case—have to be chosen to handle choices between variant spellings of the same word (American versus British), choice among scientific and popular terms (''cockroach'' versus ''Periplaneta americana''), and choices between synonyms (''automobile'' versus ''car''), among other difficult issues.

Choices of authorized terms are based on the principles of ''user warrant'' (what terms users are likely to use), ''literary warrant'' (what terms are generally used in the literature and documents), and ''structural warrant'' (terms chosen by considering the structure, scope of the controlled vocabulary).

Controlled vocabularies also typically handle the problem of [[homographs]], with qualifiers. For example, the term ''pool'' has to be qualified to refer to either ''swimming pool'' or the game ''pool'' to ensure that each authorized term or heading refers to only one concept.

There are two main kinds of controlled vocabulary tools used in libraries: subject headings and thesauri. While the differences between the two are diminishing, there are still some minor differences.

Historically subject headings were designed to describe books in library catalogs by catalogers while thesauri were used by indexers to apply index terms to documents and articles. Subject headings tend to be broader in scope describing whole books, while thesauri tend to be more specialized covering very specific disciplines. Also because of the card catalog system, subject headings tend to have terms that are in indirect order (though with the rise of automated systems this is being removed), while thesaurus terms are always in direct order. Subject headings also tend to use more pre-coordination of terms such that the designer of the controlled vocabulary will combine various concepts together to form one authorized subject heading. (e.g., children and terrorism) while thesauri tend to use singular direct terms. Lastly thesauri list not only equivalent terms but also narrower, broader terms and related terms among various authorized and non-authorized terms, while historically most subject headings did not.

For example, the [[Library of Congress Subject Heading]] itself did not have much syndetic structure until 1943, and it was not until 1985 when it began to adopt the thesauri type term "[[Hypernym|Broader term]]" and "[[Hyponym|Narrow term]]".

The [[terminology|terms]] are chosen and organized by trained professionals (including librarians and information scientists) who possess expertise in the subject area. Controlled vocabulary terms can accurately describe what a given document is actually about, even if the terms themselves do not occur within the document's text. Well known subject heading systems include the [[Library of Congress Subject Headings|Library of Congress system]], [[Medical Subject Headings|MeSH]], and [[Sears Subject Headings|Sears]]. Well known thesauri include the [[Art and Architecture Thesaurus]] and the [[Education Resources Information Center|ERIC]] Thesaurus.

Choosing authorized terms to be used is a tricky business, besides the areas already considered above, the designer has to consider the specificity of the term chosen, whether to use direct entry, inter consistency and stability of the language. Lastly the amount of pre-co-ordinate (in which case the degree of enumeration versus synthesis becomes an issue) and post co-ordinate in the system is another important issue.

Controlled vocabulary elements (terms/phrases) employed as [[Tag (metadata)|tags]], to aid in the content identification process of documents, or other information system entities (e.g. DBMS, Web Services) qualifies as [[metadata]].

== Indexing languages ==

There are three main types of indexing languages.

* Controlled indexing language – only approved terms can be used by the indexer to describe the document
* [[Natural language]] indexing language – any term from the document in question can be used to describe the document
* Free indexing language – any term (not only from the document) can be used to describe the document

When indexing a document, the indexer also has to choose the level of indexing exhaustivity, the level of detail in which the document is described. For example, using low indexing exhaustivity, minor aspects of the work will not be described with index terms. In general the higher the indexing exhaustivity, the more terms indexed for each document.

In recent years [[free text search]] as a means of access to documents has become popular. This involves using natural language indexing with an indexing exhaustively set to maximum (every word in the text is ''indexed''). Many studies have been done to compare the efficiency and effectiveness of free text searches against documents that have been indexed by experts using a few well chosen controlled vocabulary descriptors.

Controlled vocabularies are often claimed to improve the accuracy of free text searching, such as to reduce [[Relevance (Information Retrieval)|irrelevant]] items in the retrieval list. These irrelevant items ([[false positives]]) are often caused by the inherent ambiguity of [[natural language]]. Take the English word [[football (word)|''football'']] for example. ''Football'' is the name given to a number of different [[team sport]]s. Worldwide the most popular of these team sports is [[Football (soccer)|association football]], which also happens to be called ''[[soccer]]'' in several countries. The word ''football'' is also applied to [[rugby football]] ([[rugby union]] and [[rugby league]]), [[American football]], [[Australian rules football]], [[Gaelic football]], and [[Canadian football]]. A search for ''football'' therefore will retrieve documents that are about several completely different sports. Controlled vocabulary solves this problem by [[Tag (metadata)|tagging]] the documents in such a way that the ambiguities are eliminated.

Compared to free text searching, the use of a controlled vocabulary can dramatically increase the performance of an information retrieval system, if performance is measured by precision (the percentage of documents in the retrieval list that are actually [[relevance|relevant]] to the search topic).

In some cases controlled vocabulary can enhance recall as well, because unlike natural language schemes, once the correct authorized term is searched, you don't need to worry about searching for other terms that might be synonyms of that term.

However, a controlled vocabulary search may also lead to unsatisfactory [[Recall (information retrieval)|recall]], in that it will fail to retrieve some documents that are actually relevant to the search question.

This is particularly problematic when the search question involves terms that are sufficiently tangential to the subject area such that the indexer might have decided to tag it using a different term (but the searcher might consider the same). Essentially, this can be avoided only by an experienced user of controlled vocabulary whose understanding of the vocabulary coincides with the way it is used by the indexer.

Another possibility is that the article is just not tagged by the indexer because indexing exhaustivity is low. For example, an article might mention football as a secondary focus, and the indexer might decide not to tag it with "football" because it is not important enough compared to the main focus. But it turns out that for the searcher that article is relevant and hence recall fails. A free text search would automatically pick up that article regardless.

On the other hand, free text searches have high exhaustivity (you search on every word) so it has potential for high recall (assuming you solve the problems of synonyms by entering every combination) but will have much lower precision.

Controlled vocabularies are also quickly out-dated and in fast developing fields of knowledge, the authorized terms available might not be available if they are not updated regularly. Even in the best case scenario, controlled language is often not as specific as using the words of the text itself. Indexers trying to choose the appropriate index terms might misinterpret the author, while a free text search is in no danger of doing so, because it uses the author's own words.

The use of controlled vocabularies can be costly compared to free text searches because human experts  or expensive automated systems are necessary to index each entry.  Furthermore, the user has to be familiar with the controlled vocabulary scheme to make best use of the system. But as already mentioned, the control of synonyms, homographs can help increase precision.

Numerous methodologies have been developed to assist in the creation of controlled vocabularies, including [[faceted classification]], which enables a given data record or document to be described in multiple ways.

==Applications==
Controlled vocabularies, such as the [[Library of Congress Subject Headings]],  are an essential component of [[bibliography]], the study and classification of books. They were initially developed in [[library and information science]]. In the 1950s, government agencies  began to develop controlled vocabularies for the burgeoning journal literature in specialized fields; an example is the [[Medical Subject Headings]] (MeSH) developed by the [[United States National Library of Medicine|U.S. National Library of Medicine]]. Subsequently, for-profit firms (called Abstracting and indexing services) emerged to index the fast-growing literature in every field of knowledge. In the 1960s, an online bibliographic database industry developed based on dialup [[X.25]] networking. These services were seldom made available to the public because they were difficult to use; specialist librarians called search intermediaries handled the searching job. In the 1980s, the first [[full text]] databases appeared; these databases contain the full text of the index articles as well as the bibliographic information. Online bibliographic databases have migrated to the Internet and are now publicly available; however, most are proprietary and can be expensive to use. Students enrolled in colleges and universities may be able to access some of these services without charge; some of these services may be accessible without charge at a public library.

In large organizations, controlled vocabularies may be introduced to improve [[technical communication]]. The use of controlled vocabulary ensures that everyone is using the same word to mean the same thing.  This consistency of terms is one of the most important concepts in [[technical writing]] and [[knowledge management]], where effort is expended to use the same word throughout a [[document]] or [[organization]] instead of slightly different ones to refer to the same thing.

Web searching could be dramatically improved by the development of a controlled vocabulary for describing Web pages; the use of such a vocabulary could culminate in a [[Semantic Web]], in which the content of Web pages is described using a machine-readable [[metadata]] scheme. One of the first proposals for such a scheme is the [[Dublin Core]] Initiative. An example of a controlled vocabulary which is usable for [[Web indexing|indexing web pages]] is [[Polythematic Structured Subject Heading System|PSH]].

It is unlikely that a single metadata scheme will ever succeed in describing the content of the entire Web.<ref>Cory Doctorow, [http://www.well.com/~doctorow/metacrap.htm Metacrap].</ref> To create a Semantic Web, it may be necessary to draw from two or more metadata systems to describe a Web page's contents. The [[eXchangeable Faceted Metadata Language]] (XFML) is designed to enable controlled vocabulary creators to publish and share metadata systems. XFML is designed on [[faceted classification]] principles.<ref>Mark Pilgrim, [http://petervandijck.com/xfml/ eXchangeable Faceted Metadata Language].</ref>

Controlled vocabularies of the [[Semantic Web]] define the concepts and relationships (terms) used to describe a field of interest or area of concern. For instance, to declare a person in a machine-readable format, a vocabulary is needed that has the formal definition of “Person”, such as the Friend of a Friend ([[FOAF]]) vocabulary, which has a Person class that defines typical properties of a person including, but not limited to, name, honorific prefix, affiliation, email address, and homepage, or the Person vocabulary of [[Schema.org]].<ref>{{cite web |url=http://schema.org/Person |title=The Person vocabulary of Schema.org |accessdate=13 March 2015}}</ref> Similarly, a book can be described using the Book vocabulary of [[Schema.org]]<ref>{{cite web |url=http://schema.org/Book |title=The Book vocabulary of Schema.org |accessdate=13 March 2015}}</ref> and general publication terms from the [[Dublin Core]] vocabulary,<ref>{{cite web |url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |accessdate=13 March 2015}}</ref> an event with the Event vocabulary of [[Schema.org]],<ref>{{cite web |url=http://schema.org/Event |title=The Event vocabulary of Schema.org |accessdate=13 March 2015}}</ref> and so on.

To use machine-readable terms from any controlled vocabulary, web designers can choose from a variety of annotation formats, including RDFa, [[Microdata (HTML)|HTML5 Microdata]], or [[JSON-LD]] in the markup, or [[Resource Description Framework|RDF]] serializations (RDF/XML, Turtle, N3, TriG, TriX) in external files.

==See also==
*[[Authority control]]
*[[Controlled natural language]]
*[[IMS VDEX|IMS Vocabulary Definition Exchange]]
*[[Named-entity recognition]]
*[[Nomenclature]]
*[[Ontology (computer science)]]
*[[Terminology]]
*[[Thesaurus]]
*[[Universal Data Element Framework]]
*[[Vocabulary-based transformation]]

==References==
{{Reflist|2}}

==External links==
* [http://www.controlledvocabulary.com/ controlledvocabulary.com] — explains how controlled vocabularies are useful in describing images and information for classifying content in electronic databases.
* [http://www.photo-keywords.com/ photo-keywords.com/] — useful guides to creating and editing your own controlled vocabulary suitable for image cataloging.
* [http://www.niso.org/standards/resources/Z39-19.html ANSI/NISO Z39.19 - 2005 Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies]

{{Lexicography}}

[[Category:Information retrieval techniques]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Technical communication]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)]]
[[Category:Controlled vocabularies]]
[[Category:Information science]]
<=====doc_Id=====>:628
<=====title=====>:
Stop words
<=====text=====>:
{{distinguish|Safeword}}
In [[computing]], '''stop words''' are words which are filtered out before or after [[Natural language processing|processing of natural language]] data (text).<ref>{{Cite book | last1 = Rajaraman | first1 = A. | last2 = Ullman | first2 = J. D. | doi = 10.1017/CBO9781139058452.002 | chapter = Data Mining | title = Mining of Massive Datasets | pages = 1–17| year = 2011 | isbn = 9781139058452 | pmid =  | pmc = | url = http://i.stanford.edu/~ullman/mmds/ch1.pdf}}</ref> Though '''stop words''' usually refer to the most common words in a language, there is no single universal list of stop words used by all [[natural language processing]] tools, and indeed not all tools even use such a list. Some tools specifically avoid removing these '''stop words''' to support [[phrase search]].

Any group of words can be chosen as the stop words for a given purpose. For some [[search engine]]s, these are some of the most common, short [[function word]]s, such as ''the'', ''is'', ''at'', ''which'', and ''on''. In this case, stop words can cause problems when searching for phrases that include them, particularly in names such as "[[The Who]]", "[[The The]]", or "[[Take That]]". Other search engines remove some of the most common words—including [[lexical word]]s, such as "want"—from a query in order to improve performance.<ref>[http://blog.stackoverflow.com/2008/12/podcast-32 Stackoverflow]: "One of our major performance optimizations for the "related questions" query is removing the top 10,000 most common English dictionary words (as determined by Google search) before submitting the query to the SQL Server 2008 full text engine. It’s shocking how little is left of most posts once you remove the top 10k English dictionary words. This helps limit and narrow the returned results, which makes the query dramatically faster".</ref>

[[Hans Peter Luhn]], one of the pioneers in [[information retrieval]], is credited with coining the phrase and using the concept.<ref>{{Cite book|title = Keyword-in-Context Index for Technical Literature (KWIC Index)|last = Luhn|first = H. P.|publisher = International Business Machines Corp.|year = 1959|isbn = |location = Yorktown Heights, NY|pages = |doi = 10.1002/asi.5090110403}}</ref> The phrase "stop word", which is not in Luhn's 1959 presentation, and the associated terms "stop list" and "stoplist" appear in the literature shortly afterwards.<ref>{{cite journal|last1=Flood|first1=Barbara J.|title=Historical note: The Start of a Stop List at Biological Abstracts|journal=Journal of the American Society for Information Science|date=1999|volume=50|issue=12|page=1066|doi=10.1002/(SICI)1097-4571(1999)50:12<1066::AID-ASI5>3.0.CO;2-A|url=http://dx.doi.org/10.1002/(SICI)1097-4571(1999)50:12<1066::AID-ASI5>3.0.CO;2-A|accessdate=16 February 2016}}</ref>

A predecessor concept was used in creating some [[Bible concordance|concordance]]s. For example, the first Hebrew concordance, Me’ir nativ, contained a one-page list of unindexed words, with nonsubstantive prepositions and conjunctions which are similar to modern stop words.<ref>{{cite journal|last1=Weinberg|first1=Bella Hass|title=Predecessors of scientific indexing structures in the domain of religion|journal=Second Conference on the History and Heritage of Scientific and Technical Information Systems|date=2004|pages=126–134|url=https://www.asis.org/History/11-weinberg.pdf|accessdate=17 February 2016}}</ref>

== See also ==
{{Div col|cols=3}}
* [[Text mining]]
* [[Concept mining]]
* [[Information extraction]]
* [[Natural language processing]]
* [[Query expansion]]
* [[Stemming]]
* [[Index (search engine)|Search engine indexing]]
* [[Poison words]]
* [[Function words]]
* [[Filler_(linguistics) | Filler]]
{{Div col end}}

==References==
{{Reflist|2}}

== External links ==
* [http://xpo6.com/list-of-english-stop-words/  List of English Stop Words (PHP array, CSV) ]
* [http://dev.mysql.com/doc/refman/5.5/en/fulltext-stopwords.html  Full-Text Stopwords in MySQL ]
* [http://www.textfixer.com/resources/common-english-words.txt English Stop Words (CSV)]
* [http://mail.sarai.net/private/prc/Week-of-Mon-20080204/001656.html Hindi Stop Words]
* [http://solariz.de/de/deutsche_stopwords.htm German Stop Words],[http://aniol-consulting.de/uebersicht-deutscher-stop-words/ German Stop Words and phrases], another list of [http://www.ranks.nl/stopwords/german.html German stop words]
* [[:pl:Wikipedia:Stopwords|Polish Stop Words]]
* [https://code.google.com/p/stop-words/ Collection of stop words in 29 languages] [https://web.archive.org/web/*/http://tonyb.sk/_my/ir/stop-words-collection-2014-02-24.zip]
* [http://www.text-analytics101.com/2014/10/all-about-stop-words-for-text-mining.html A Detailed Explanation of Stop Words by Kavita Ganesan]

{{Natural Language Processing}}
{{SearchEngineOptimization}}

[[Category:Information retrieval techniques]]
<=====doc_Id=====>:631
<=====title=====>:
Uncertain inference
<=====text=====>:
'''Uncertain inference''' was first described by [[C. J. van Rijsbergen]]<ref>{{cite | author=C. J. van Rijsbergen | title=A non-classical logic for information retrieval | publisher=The Computer Journal | pages=481–485 | year=1986}}</ref> as a way to formally define a query and document relationship in [[Information retrieval]]. This formalization is a [[logical consequence|logical implication]] with an attached measure of uncertainty.

==Definitions==
Rijsbergen proposes that the measure of [[uncertainty]] of a document ''d'' to a query ''q'' be the probability of its logical implication, i.e.:

:<math>P(d \to q)</math>

A user's query can be interpreted as a set of assertions about the desired document. It is the system's task to [[inference|infer]], given a particular document, if the query assertions are true. If they are, the document is retrieved.
In many cases the contents of documents are not sufficient to assert the queries. A [[knowledge base]] of facts and rules is needed, but some of them may be uncertain because there may be a probability associated to using them for inference. Therefore, we can also refer to this as ''plausible inference''. The [[plausibility]] of an inference <math>d \to q</math> is a function of the plausibility of each query assertion. Rather than retrieving a document that exactly matches the query we should rank the documents based on their plausibility in regards to that query.
Since ''d'' and ''q'' are both generated by users, they are error prone; thus <math>d \to q</math> is uncertain. This will affect the plausibility of a given query.

By doing this it accomplishes two things:
* Separate the processes of revising probabilities from the logic
* Separate the treatment of relevance from the treatment of requests

[[Multimedia]] documents, like images or videos, have different inference properties for each datatype. They are also different from text document properties. The framework of plausible inference allows us to measure and combine the probabilities coming from these different properties.

Uncertain inference generalizes the notions of [[autoepistemic logic]], where truth values are either known or unknown, and when known, they are true or false.

==Example==
If we have a query of the form:

:<math>q = A \wedge B \wedge C</math>

where A, B and C are query assertions, then for a document D we want the probability:

:<math>P (D \to (A \wedge B \wedge C))</math>

If we transform this into the [[conditional probability]] <math>P ((A \wedge B \wedge C) | D)</math> and if the query assertions are independent we can calculate the overall probability of the implication as the product of the individual assertions probabilities.

==Further work==
Croft and Krovetz<ref>{{cite | title=Interactive retrieval office documents | url=http://doi.acm.org/10.1145/45410.45435 | author1=W. B. Croft | author2=R. Krovetz | year=1988 }}</ref> applied uncertain inference to an information retrieval system for office documents they called ''OFFICER''. In office documents the independence assumption is valid since the query will focus on their individual attributes. Besides analysing the content of documents one can also query about the author, size, topic or collection for example. They devised methods to compare document and query attributes, infer their plausibility and combine it into an overall rating for each document. Besides that uncertainty of document and query contents also had to be addressed.

[[Probabilistic logic network]]s is a system for performing uncertain inference; crisp true/false truth values are replaced not only by a probability, but also by a confidence level, indicating the certitude of the probability.

[[Markov logic network]]s allow uncertain inference to be performed; uncertainties are computed using the [[maximum entropy principle]], in analogy to the way that [[Markov chain]]s describe the uncertainty of [[finite state machine]]s.

== See also ==
* [[Fuzzy logic]]
* [[Probabilistic logic]]
* [[Plausible reasoning]]
* [[Imprecise probability]]

==References==
{{reflist}}

[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]
[[Category:Inference]]
<=====doc_Id=====>:634
<=====title=====>:
Term Discrimination
<=====text=====>:
'''Term Discrimination''' is a way to rank keywords in how useful they are for [[information retrieval]].

== Overview ==

This is a method similar to [[tf-idf]] but it deals with finding keywords suitable for [[information retrieval]] and ones that are not.  Please refer to [[Vector Space Model]] first.

This method uses the concept of ''Vector Space Density'' that the less dense an [[occurrence matrix]] is, the better an information retrieval query will be.

An optimal index term is one that can distinguish two different documents from each other and relate two similar documents.  On the other hand, a sub-optimal index term can not distinguish two different document from two similar documents.

The discrimination value is the difference in the occurrence matrix's vector-space density versus the same matrix's vector-space without the index term's density.

 Let:
 <math>A</math> be the occurrence matrix
 <math>A_k</math> be the occurrence matrix without the index term <math>k</math>
 and <math>Q(A)</math> be density of <math>A</math>.
 Then:
 The discrimination value of the index term <math>k</math> is: 
 <math>DV_k = Q(A) - Q(A_k)</math>

== How to compute ==

Given an [[occurrency matrix]]: <math>A</math> and one keyword: <math>k</math>
* Find the global document [[centroid]]: <math>C</math> (this is just the average document vector)
* Find the average [[euclidean distance]] from every document vector, <math>D_i</math> to <math>C</math>
* Find the average euclidean distance from every document vector, <math>D_i</math> to <math>C</math> ''IGNORING'' <math>k</math>
* The difference between the two values in the above step is the ''discrimination value'' for keyword <math>K</math>

A higher value is better because including the keyword will result in better information retrieval.

== Qualitative Observations ==
Keywords that are ''[[Sparse matrix|sparse]]'' should be poor discriminators because they have poor ''[[Precision and recall|recall]],''
whereas
keywords that are ''frequent'' should be poor discriminators because they have poor ''[[Precision and recall|precision]].''

== References ==
* [[Gerard Salton|G. Salton]], A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613–620. ''(The article in which the vector space model was first presented)''
* Can, F., Ozkarahan, E. A (1987), "Computation of term/document discrimination values by use of the cover coefficient concept." ''Journal of the American Society for Information Science'', vol. 38, nr. 3, pages 171-183.

[[Category:Information retrieval techniques]]
<=====doc_Id=====>:637
<=====title=====>:
Search-oriented architecture
<=====text=====>:
{{unreferenced|date=October 2007}}
The use of [[search engine technology]] is the main integration component in an [[information system]]. In a traditional business environment the [[architectural layer]] usually occupied by a [[relational database management system]] (RDBMS) is supplemented or replaced with a search engine or the indexing technology used to build search engines. Queries for information which would usually be performed using [[Structured Query Language]] (SQL) are replaced by keyword or fielded (or field-enabled) searches for structured, [[Semi-structured model|semi-structured]], or unstructured data.

In a typical [[Multitier architecture|multi-tier]] or [[Multitier architecture|N tier]] architecture information is maintained in a data tier where it can be stored and retrieved from a database or file system. The data tier is queried by the logic or business tier when information is needed using a data retrieval language like SQL.

In a '''search-oriented architecture''' the data tier may be replaced or placed behind another tier which contains a search engine and search engine index which is queried instead of the database management system. Queries from the business tier are made in the search engine query language instead of SQL. The search engine itself crawls the relational database management system in addition to other traditional data sources such as web pages or traditional file systems and consolidates the results when queried.

The benefit of adding a search layer to the architecture stack is rapid response time large dynamic datasets made possible by search indexing technology such as an [[inverted index]]. 

== Contrast with ==
* [[Service-oriented architecture]] (SOA)
* [[Service-Oriented Modeling]]

== See also ==
* [[Hibernate search]]
 
[[Category:Software architecture]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:640
<=====title=====>:
Category:Substring indices
<=====text=====>:
{{cat main|Substring index}}

[[Category:String (computer science)]]
[[Category:Algorithms on strings]]
[[Category:String data structures]]
[[Category:Database index techniques]]
[[Category:Information retrieval techniques]]
[[Category:Bioinformatics algorithms]]
<=====doc_Id=====>:643
<=====title=====>:
Latent semantic mapping
<=====text=====>:
'''Latent semantic mapping (LSM)''' is a data-driven framework to model globally meaningful relationships implicit in large volumes of (often textual) data. It is a generalization of [[latent semantic analysis]]. In information retrieval, LSA enables retrieval on the basis of conceptual content, instead of merely matching words between queries and documents.

LSM was derived from earlier work on latent semantic analysis.  There are 3 main characteristics of latent semantic analysis: Discrete entities, usually in the form of words and documents, are mapped onto continuous vectors, the mapping involves a form of global correlation pattern, and dimensionality reduction is an important aspect of the analysis process. These constitute generic properties, and have been identified as potentially useful in a variety of different contexts.  This usefulness has encouraged great interest in LSM. The intended product of latent semantic mapping, is a data-driven framework for modeling relationships in large volumes of data.

[[Mac OS X v10.5]] and later includes a [[Software framework|framework]] implementing latent semantic mapping.<ref>[http://developer.apple.com/documentation/TextFonts/Reference/LatentSemanticMapping/index.html API Reference: Latent Semantic Mapping Framework Reference<!-- Bot generated title -->]</ref>

== See also ==
* [[Latent semantic analysis]]

== Notes ==
{{reflist}}

== References ==
* {{cite journal
 | url=http://ieeexplore.ieee.org/iel5/79/32367/01511825.pdf
 | title=Latent semantic mapping [information retrieval]
 | author=Bellegarda, J.R.
 | date=2005
}}
* {{cite conference
 | url=https://www.securecms.com/ICASSP2006/Tutorial_06.asp
 | title=Latent semantic mapping: Principles and applications
 | author=J. Bellegarda
 | booktitle=ICASSP 2006
 | date=2006
}}

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]


{{semantics-stub}}
{{compu-stub}}
<=====doc_Id=====>:646
<=====title=====>:
Statistical semantics
<=====text=====>:
{{linguistics}}
'''Statistical semantics''' is the study of "how the statistical patterns of human word usage can be used to figure out what people mean, at least to a level sufficient for information access" {{citation needed|date=July 2012}}<!--([[George Furnas|Furnas]], 2006)--this page has been moved and the new version no longer contains this quotation-->. How can we figure out what words mean, simply by looking at patterns of words in huge collections of text? What are the limits to this approach to understanding words?

==History==

The term ''statistical semantics'' was first used by [[Warren Weaver]] in his well-known paper on [[machine translation]].<ref>{{harvnb|Weaver|1955}}</ref> He argued that [[word sense disambiguation]] for machine translation should be based on the [[co-occurrence]] frequency of the context words near a given target word. The underlying assumption that "a word is characterized by the company it keeps" was advocated by [[J. R. Firth|J.R. Firth]].<ref>{{harvnb|Firth|1957}}</ref> This assumption is known in [[linguistics]] as the [[distributional hypothesis]].<ref>{{harvnb|Sahlgren|2008}}</ref> Emile Delavenay defined ''statistical semantics'' as the "Statistical study of meanings of words and their frequency and order of recurrence."<ref>{{harvnb|Delavenay|1960}}</ref> "[[George Furnas|Furnas]] et al. 1983" is frequently cited as a foundational contribution to statistical semantics.<ref>{{harvnb|Furnas|Landauer|Gomez|Dumais|1983}}</ref>  An early success in the field was [[latent semantic analysis]].

==Applications==

Research in statistical semantics has resulted in a wide variety of algorithms that use the distributional hypothesis to discover many aspects of [[semantics]], by applying statistical techniques to [[Text corpus|large corpora]]:
* Measuring the [[Semantic similarity|similarity in word meanings]]<ref>{{harvnb|Lund|Burgess|Atchley|1995}}</ref><ref>{{harvnb|Landauer|Dumais|1997}}</ref><ref>{{harvnb|McDonald|Ramscar|2001}}</ref><ref>{{harvnb|Terra|Clarke|2003}}</ref>
* Measuring the similarity in word relations <ref>{{harvnb|Turney|2006}}</ref>
* Modeling [[similarity-based generalization]]<ref>{{harvnb|Yarlett|2008}}</ref>
* Discovering words with a given relation<ref>{{harvnb|Hearst|1992}}</ref>
* Classifying relations between words<ref>{{harvnb|Turney|Littman|2005}}</ref>
* Extracting keywords from documents<ref>{{harvnb|Frank|Paynter|Witten|Gutwin|1999}}</ref><ref>{{harvnb|Turney|2000}}</ref>
* Measuring the cohesiveness of text<ref>{{harvnb|Turney|2003}}</ref>
* Discovering the different senses of words<ref>{{harvnb|Pantel|Lin|2002}}</ref>
* Distinguishing the different senses of words<ref>{{harvnb|Turney|2004}}</ref>
* Subcognitive aspects of words<ref>{{harvnb|Turney|2001}}</ref>
* Distinguishing praise from criticism<ref>{{harvnb|Turney|Littman|2003}}</ref>

==Related fields==

Statistical Semantics focuses on the meanings of common words and the relations between common words, unlike [[text mining]], which tends to focus on whole documents, document collections, or named entities (names of people, places, and organizations). Statistical Semantics is a subfield of [[computational semantics]], which is in turn a subfield of [[computational linguistics]] and [[natural language processing]].

Many of the applications of Statistical Semantics (listed above) can also be addressed by [[lexicon]]-based algorithms, instead of the [[text corpus|corpus]]-based algorithms of Statistical Semantics. One advantage of corpus-based algorithms is that they are typically not as labour-intensive as lexicon-based algorithms. Another advantage is that they are usually easier to adapt to new languages than lexicon-based algorithms. However, the best performance on an application is often achieved by combining the two approaches.<ref>{{harvnb|Turney|Littman|Bigham|Shnayder|2003}}</ref>

==See also==
{{Portal|Linguistics}}
{{div col|3}}
*[[Co-occurrence]]
*[[Computational linguistics]]
*[[Information retrieval]]
*[[Latent semantic analysis]]
*[[Latent semantic indexing]]
*[[Natural language processing]]
*[[Semantic analytics]]
*[[Semantic similarity]]
*[[Text corpus]]
*[[Text mining]]
*[[Web mining]]
{{div col end}}

==References==
{{reflist|2}}

===Sources===
{{refbegin}}
* {{cite book | last = Delavenay | first = Emile | year = 1960 | title = An Introduction to Machine Translation | location = New York, NY | publisher = [[Thames and Hudson]] | oclc = 1001646 | ref = harv }}
* {{cite journal | last = Firth | first = John R. | authorlink = John Rupert Firth | year = 1957 | title = A synopsis of linguistic theory 1930-1955 | journal = [[Studies in Linguistic Analysis]] | pages = 1–32 | location = Oxford | publisher = [[Philological Society]] | ref = harv }}
*: Reprinted in {{cite book | editor1-first = F.R. | editor1-last = Palmer | title = Selected Papers of J.R. Firth 1952-1959 | location = London | publisher = Longman | year = 1968 | oclc = 123573912 }}
* {{cite conference | last1 = Frank | first1 = Eibe | last2 = Paynter | first2 = Gordon W. | last3 = Witten | first3 = Ian H. | last4 = Gutwin | first4 = Carl | last5 = Nevill-Manning | first5 = Craig G. | year = 1999 | title = Domain-specific keyphrase extraction | booktitle = Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence | conference = [[International Joint Conference on Artificial Intelligence|IJCAI-99]] | volume = 2 | pages = 668–673 | location = California | publisher = Morgan Kaufmann | isbn = 1-55860-613-0 | citeseerx = 10.1.1.148.3598 | ref = harv }}
* {{cite journal | last1 = Furnas | first1 = George W. | authorlink = George Furnas | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | year = 1983 | title = Statistical semantics: Analysis of the potential performance of keyword information systems | url = https://web.archive.org/web/*/http://furnas.people.si.umich.edu/Papers/FurnasEtAl1983_BSTJ_p1753.pdf | journal = [[Bell System Technical Journal]] | volume = 62 | issue = 6 | pages = 1753–1806 | ref = harv | doi=10.1002/j.1538-7305.1983.tb03513.x}}
* {{cite conference | last = Hearst | first = Marti A. | year = 1992 | title = Automatic Acquisition of Hyponyms from Large Text Corpora | booktitle = Proceedings of the Fourteenth International Conference on Computational Linguistics | conference = [[COLING|COLING '92]] | pages = 539–545 | location = Nantes, France | url = http://acl.ldc.upenn.edu/C/C92/C92-2082.pdf | doi = 10.3115/992133.992154 | citeseerx = 10.1.1.36.701 | ref = harv }}
* {{cite journal | last1 = Landauer | first1 = Thomas K. | last2 = Dumais | first2 = Susan T. | year = 1997 | title = A solution to Plato's problem: The latent semantic analysis theory of the acquisition, induction, and representation of knowledge | journal = [[Psychological Review]] | volume = 104 | issue = 2 | pages = 211–240 | url = http://lsa.colorado.edu/papers/plato/plato.annote.html | citeseerx = 10.1.1.184.4759 | ref = harv | doi=10.1037/0033-295x.104.2.211}}
* {{cite conference | last1 = Lund | first1 = Kevin | last2 = Burgess | first2 = Curt | last3 = Atchley | first3 = Ruth Ann | year = 1995 | title = Semantic and associative priming in high-dimensional semantic space | booktitle = Proceedings of the 17th Annual Conference of the Cognitive Science Society | publisher = [[Cognitive Science Society]] | pages = 660–665 | url = http://locutus.ucr.edu/reprintPDFs/lba95csp.pdf | ref = harv }}
* {{cite conference | last1 = McDonald | first1 = Scott | last2 = Ramscar | first2 = Michael | year = 2001 | title = Testing the distributional hypothesis: The influence of context on judgements of semantic similarity | booktitle = Proceedings of the 23rd Annual Conference of the Cognitive Science Society | pages = 611–616 | url = http://homepages.inf.ed.ac.uk/smcdonal/cogsci2001.pdf | citeseerx = 10.1.1.104.7535 | ref = harv }}
* {{cite conference | last1 = Pantel | first1 = Patrick | last2 = Lin | first2 = Dekang | year = 2002 | title = Discovering word senses from text | booktitle = Proceedings of ACM SIGKDD Conference on Knowledge Discovery and Data Mining | isbn = 1-58113-567-X | conference = [[KDD Conference|KDD '02]] | pages = 613–619 | citeseerx = 10.1.1.12.6771 | doi = 10.1145/775047.775138 | ref = harv }}
* {{cite journal | last1 = Sahlgren | first1 = Magnus | year = 2008 | title = The Distributional Hypothesis | url = http://soda.swedish-ict.se/3941/1/sahlgren.distr-hypo.pdf | journal = Rivista di Linguistica | volume = 20 | issue = 1 | pages = 33–53 | ref = harv}}
* {{cite conference | last1 = Terra | first1 = Egidio L. | last2 = Clarke | first2 = Charles L. A. | year = 2003 | title = Frequency estimates for statistical word similarity measures | booktitle = Proceedings of the Human Language Technology and North American Chapter of Association of Computational Linguistics Conference 2003 | conference = HLT/NAACL 2003 | pages = 244–251 | url = http://acl.ldc.upenn.edu/N/N03/N03-1032.pdf | citeseerx = 10.1.1.12.9041 | doi = 10.3115/1073445.1073477 | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. |date=May 2000 | title = Learning algorithms for keyphrase extraction | journal = [[Information Retrieval (journal)|Information Retrieval]] | volume = 2 | issue = 4 | pages = 303–336 | arxiv = cs/0212020 | citeseerx = 10.1.1.11.1829 | doi = 10.1023/A:1009976227802 | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. | year = 2001 | title = Answering subcognitive Turing Test questions: A reply to French | journal = [[Journal of Experimental and Theoretical Artificial Intelligence]] | volume = 13 | issue = 4 | pages = 409–419 | arxiv = cs/0212015 | citeseerx = 10.1.1.12.8734 | ref = harv | doi=10.1080/09528130110100270}}
* {{cite conference | last = Turney | first = Peter D. | year = 2003 | title = Coherent keyphrase extraction via Web mining | booktitle = Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence | conference = IJCAI-03 | location = Acapulco, Mexico | pages = 434–439 | arxiv = cs/0308033 | citeseerx = 10.1.1.100.3751 | ref = harv }}
* {{cite conference | last = Turney | first = Peter D. | year = 2004 | title = Word sense disambiguation by Web mining for word co-occurrence probabilities | booktitle = Proceedings of the Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | conference = SENSEVAL-3 | location = Barcelona, Spain | pages = 239–242 | arxiv = cs/0407065 | url = http://cogprints.org/3732/ | ref = harv }}
* {{cite journal | last = Turney | first = Peter D. | year = 2006 | title = Similarity of semantic relations |journal = [[Computational Linguistics (journal)|Computational Linguistics]] | volume = 32 | issue = 3 | pages = 379–416 | arxiv = cs/0608100 | url = http://cogprints.org/5098/ | doi = 10.1162/coli.2006.32.3.379 | citeseerx = 10.1.1.75.8007 | ref = harv }}
* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. |date=October 2003 | title = Measuring praise and criticism: Inference of semantic orientation from association | journal = [[ACM Transactions on Information Systems]] | volume = 21 | issue = 4 | pages = 315–346 | arxiv = cs/0309034 | url = http://cogprints.org/3164/ | citeseerx = 10.1.1.9.6425 | doi = 10.1145/944012.944013 | ref = harv }}
* {{cite journal | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | year = 2005 | title = Corpus-based Learning of Analogies and Semantic Relations | journal = [[Machine Learning (journal)|Machine Learning]] | volume = 60 | issue = 1–3 | pages = 251–278 | arxiv = cs/0508103 | citeseerx = 10.1.1.90.9819 | doi = 10.1007/s10994-005-0913-1 | url = http://cogprints.org/4518/ | ref = harv }}
* {{cite conference | last1 = Turney | first1 = Peter D. | last2 = Littman | first2 = Michael L. | last3 = Bigham | first3 = Jeffrey | last4 = Shnayder | first4 = Victor | year = 2003 | title = Combining Independent Modules to Solve Multiple-choice Synonym and Analogy Problems | booktitle = Proceedings of the International Conference on Recent Advances in Natural Language Processing | conference = RANLP-03 | location = [[Borovets]], Bulgaria | pages = 482–489 | arxiv = cs/0309035 | citeseerx = 10.1.1.5.2939 | url = http://cogprints.org/3163/ | ref = harv }}
* {{cite book | last = Weaver | first = Warren | authorlink = Warren Weaver | year = 1955 | chapter = Translation | chapter-url = http://www.mt-archive.info/Weaver-1949.pdf | editor1-first = W.N. | editor1-last = Locke | editor2-first = D.A. | editor2-last = Booth | title = Machine Translation of Languages | location = [[Cambridge, Massachusetts]] | publisher = [[MIT Press]] | isbn = 0-8371-8434-7 | pages = 15–23 | ref = harv }}
* {{cite thesis | last = Yarlett | first = Daniel G. | year = 2008 | title = Language Learning Through Similarity-Based Generalization | url = http://psych.stanford.edu/~michael/papers/Draft_Yarlett_Similarity.pdf | degree = PhD | publisher = Stanford University | ref = harv }}
{{refend}}


{{DEFAULTSORT:Statistical Semantics}}
[[Category:Artificial intelligence applications]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]
[[Category:Semantics]]
[[Category:Statistical natural language processing]]
[[Category:Applied statistics]]
<=====doc_Id=====>:649
<=====title=====>:
Category:Music information retrieval
<=====text=====>:
{{Commons category|Music information retrieval}}
[[Category:Information retrieval genres]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:652
<=====title=====>:
Binary Independence Model
<=====text=====>:
{{context|date=June 2012}}
The '''Binary Independence Model''' (BIM)<ref name="cyu76" /><ref name="jones77"/> is a probabilistic [[information retrieval]] technique that makes some simple assumptions to make the estimation of document/query similarity probability feasible.

==Definitions==
The Binary Independence Assumption is that documents are [[bit array|binary vector]]s. That is, only the presence or absence of terms in documents are recorded. Terms are [[independence (probability theory)|independently]] distributed in the set of relevant documents and they are also independently distributed in the set of irrelevant documents.
The representation is an ordered set of [[Boolean data type|Boolean]] variables. That is, the representation of a document or query is a vector with one Boolean element for each term under consideration. More specifically, a document is represented by a vector ''d = (x<sub>1</sub>, ..., x<sub>m</sub>)'' where ''x<sub>t</sub>=1'' if term ''t'' is present in the document ''d'' and ''x<sub>t</sub>=0'' if it's not. Many documents can have the same vector representation with this simplification. Queries are represented in a similar way.
"Independence" signifies that terms in the document are considered independently from each other and  no association between terms is modeled. This assumption is very limiting, but it has been shown that it gives good enough results for many situations. This independence is the "naive" assumption of a [[Naive Bayes classifier]], where properties that imply each other are nonetheless treated as independent for the sake of simplicity. This assumption allows the representation to be treated as an instance of a [[Vector space model]] by considering each term as a value of 0 or 1 along a dimension orthogonal to the dimensions used for the other terms.

The probability ''P(R|d,q)'' that a document is relevant derives from the probability of relevance of the terms vector of that document ''P(R|x,q)''. By using the [[Bayes rule]] we get:

<math>P(R|x,q) = \frac{P(x|R,q)*P(R|q)}{P(x|q)}</math>

where ''P(x|R=1,q)'' and ''P(x|R=0,q)'' are the probabilities of retrieving a relevant or nonrelevant document, respectively. If so, then that document's representation is ''x''.
The exact probabilities can not be known beforehand, so use estimates from statistics about the collection of documents must be used.

''P(R=1|q)'' and ''P(R=0|q)'' indicate the previous probability of retrieving a relevant or nonrelevant document respectively for a query ''q''. If, for instance, we knew the percentage of relevant documents in the collection, then we could use it to estimate these probabilities.
Since a document is either relevant or nonrelevant to a query we have that:

<math>P(R=1|x,q) + P(R=0|x,q) = 1</math>

=== Query Terms Weighting ===
Given a binary query and the [[dot product]] as the similarity function between a document and a query, the problem is to assign weights to the
terms in the query such that the retrieval effectiveness will be high. Let <math>p_i</math> and <math>q_i</math> be the probability that a relevant document and an irrelevant document has the <math>i^{th}</math> term respectively. Yu and [[Gerard Salton|Salton]],<ref name="cyu76" /> who first introduce BIM, propose that the weight of the <math>i^{th}</math> term is an increasing function of <math>Y_i =  \frac{p_i *(1-q_i)}{(1-p_i)*q_i}</math>. Thus, if <math>Y_i</math> is higher than <math>Y_j</math>, the weight
of term <math>i</math> will be higher than that of term <math>j</math>. Yu and Salton<ref name="cyu76" /> showed that such a weight assignment to query terms yields better retrieval effectiveness than if query terms are equally weighted. [[Stephen Robertson (computer scientist)|Robertson]] and [[Karen Spärck Jones|Spärck Jones]]<ref name="jones77"/> later showed that if the <math>i^{th}</math> term is assigned the weight of <math>log Y_i</math>, then optimal retrieval effectiveness is obtained under the Binary Independence Assumption.

The Binary Independence Model was introduced by Yu and Salton.<ref name="cyu76" /> The name Binary Independence Model was coined by Robertson and Spärck Jones.<ref name="jones77"/>

== See also ==

* [[Bag of words model]]

==Further reading==
* {{citation | url=http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html | title=Introduction to Information Retrieval | author=Christopher D. Manning | author2=Prabhakar Raghavan|author3=Hinrich Schütze | publisher=Cambridge University Press | year=2008}}
* {{citation | url=http://www.ir.uwaterloo.ca/book/ | title=Information Retrieval: Implementing and Evaluating Search Engines | author=Stefan B&uuml;ttcher | author2=Charles L. A. Clarke |author3= Gordon V. Cormack | publisher=MIT Press | year=2010}}

==References==
{{Reflist|refs=
<ref name="cyu76">{{Cite journal | doi = 10.1145/321921.321930| title = Precision Weighting – An Effective Automatic Indexing Method| journal = Journal of the ACM| volume = 23| pages = 76| year = 1976| last1 = Yu | first1 = C. T.| last2 = Salton | first2 = G. | authorlink2 = Gerard Salton}}</ref>
<ref name="jones77">{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}</ref> 
}}

[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]
<=====doc_Id=====>:655
<=====title=====>:
Compound term processing
<=====text=====>:
'''Compound term processing''' refers to a category of techniques used in [[information retrieval]] applications to perform matching on the basis of [[compound term]]s. Compound terms are built by combining two or more simple terms; for example, "triple" is a single word term, but "triple heart bypass" is a compound term.

Compound term processing is a new approach to an old problem: how can one improve the relevance of search results while maintaining ease of use? Using this technique, a search for ''survival rates following a triple heart bypass in elderly people'' will locate documents about this topic even if this precise phrase is not contained in any document. This can be performed by a [[concept search]], which itself uses compound term processing. This will extract the key concepts automatically (in this case "survival rates", "triple heart bypass" and "elderly people") and use these concepts to select the most relevant documents.

== Techniques ==

In August 2003, [[Concept Searching Limited]] introduced the idea of using statistical Compound Term Processing.<ref>{{cite journal|url=http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf|title=Lateral Thinking in Information Retrieval|journal=INFORMATION MANAGEMENT AND TECHNOLOGY|volume=36 PART 4}} The British Library Direct catalogue entry can be found here:[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN]</ref>

CLAMOUR is a European collaborative project which aims to find a better way to classify when collecting and disseminating industrial information and statistics. CLAMOUR appears to use a linguistic approach, rather than one based on statistical modelling.<ref>[http://webarchive.nationalarchives.gov.uk/20040117000117/statistics.gov.uk/methods_quality/clamour/default.asp] National Statistics CLAMOUR project</ref>

== History ==

Techniques for probabilistic weighting of single word terms date back to at least 1976 in the landmark publication by [[Stephen Robertson (computer scientist)|Stephen E. Robertson]] and [[Karen Spärck Jones]].<ref>{{Cite journal | doi = 10.1002/asi.4630270302| title = Relevance weighting of search terms| journal = Journal of the American Society for Information Science| volume = 27| issue = 3| pages = 129| year = 1976| last1 = Robertson | first1 = S. E. | authorlink1 = Stephen Robertson (computer scientist)| last2 = Spärck Jones | first2 = K. | authorlink2 = Karen Spärck Jones}}</ref> Robertson stated that the assumption of word independence is not justified and exists as a matter of mathematical convenience. His objection to the term independence is not a new idea, dating back to at least 1964 when H. H. Williams stated that "[t]he assumption of independence of words in a document is usually made as a matter of mathematical convenience".<ref>{{cite journal|last=WILLIAMS |first=J.H. |title=Results of classifying documents with multiple discriminant functions |url=http://oai.dtic.mil/oai/oai?verb=getRecord&metadataPrefix=html&identifier=AD0612272 |journal= Statistical Association Methods for Mechanized Documentation, National Bureau of Standards |location=Washington |pp=217-224 |year=1965}}</ref>

In 2004, Anna Lynn Patterson filed patents on "phrase-based searching in an information retrieval system"<ref>{{patent|US|20060031195}}</ref> to which [[Google]] subsequently acquired the rights.<ref>[http://www.seobythesea.com/2012/02/google-acquires-cuil-patent-applications/ Google Acquires Cuil Patent Applications]</ref>

== Adaptability ==

Statistical compound term processing is more adaptable than the process described by Patterson. Her process is targeted at searching the [[World Wide Web]] where an extensive statistical knowledge of common searches can be used to identify candidate phrases. Statistical compound term processing is more suited to [[enterprise search]] applications where such [[A priori and a posteriori|a priori]] knowledge is not available.

Statistical compound term processing is also more adaptable than the linguistic approach taken by the CLAMOUR project, which must consider the syntactic properties of the terms (i.e. part of speech, gender, number, etc.) and their combinations. CLAMOUR is highly language-dependent, whereas the statistical approach is language-independent.

== Applications ==
Compound Term Processing allows information retrieval applications, such as [[search engines]], to perform their matching on the basis of multi-word concepts, rather than on single words in isolation which can be highly ambiguous.

Early search engines looked for documents containing the words entered by the user into the search box . These are known as [[keyword search]] engines. [[Boolean search]] engines add a degree of sophistication by allowing the user to specify additional requirements. For example, "Tiger NEAR Woods AND (golf OR golfing) NOT Volkswagen" uses the operators "NEAR", "AND", "OR" and "NOT" to specify that these words must follow certain requirements. A [[phrase search]] is simpler to use, but requires that the exact phrase specified appear in the results.

==See also==
* [[Concept Searching Limited]]
* [[Enterprise search]]
* [[Information retrieval]]

== References ==
{{Reflist|30em}}

==  External links ==

{{Natural Language Processing}}

{{DEFAULTSORT:Compound Term Processing}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:658
<=====title=====>:
Natural language user interface
<=====text=====>:
'''Natural language user interfaces''' ('''LUI''' or '''NLUI''') are a type of [[User interface|computer human interface]] where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.

In [[interface design]] natural language interfaces are sought after for their speed and ease of use, but most suffer the challenges to [[natural language understanding|understanding]] wide varieties of ambiguous input.<ref>Hill, I. (1983). "Natural language versus computer language." In M. Sime and M. Coombs (Eds.) Designing for Human-Computer Communication. Academic Press.</ref>
Natural language interfaces are an active area of study in the field of [[natural language processing]] and [[computational linguistics]]. An intuitive general natural language interface is one of the active goals of the [[Semantic Web]].

Text interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional [[keyword search]] engine could be described as a "shallow" natural language user interface.

==Overview==
A natural language search engine would in theory find targeted answers to user questions (as opposed to keyword search). For example, when confronted with a question of the form 'which [[United States|U.S.]] state has the highest [[income tax]]?', conventional search engines ignore the question and instead search on the [[index term|keywords]] 'state', 'income' and 'tax'. Natural language search, on the other hand, attempts to use natural language processing to understand the nature of the question and then to search and return a subset of the web that contains the answer to the question. If it works, results would have a higher relevance than results from a keyword search engine.{{Citation needed|date=October 2015}}

==History==

Prototype Nl interfaces had already appeared in the late sixties and early seventies.<ref name="edin">Natural Language Interfaces to Databases – An Introduction,
I. Androutsopoulos,
G.D. Ritchie,
P. Thanisch,
Department of Artificial Intelligence, University of Edinburgh</ref>

*[[SHRDLU]], a natural language interface that manipulates blocks in a virtual "blocks world"
*''Lunar'', a natural language interface to a database containing chemical analyses of Apollo-11 moon rocks by [http://parsecraft.com/ William A. Woods].
*''Chat-80'' transformed English questions into [[Prolog]] expressions, which were evaluated against the Prolog database.  The code of Chat-80 was circulated widely, and formed the basis of several other experimental Nl interfaces. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem5.htm Chat-80 demo]</ref>
*[[ELIZA]], written at MIT by Joseph Weizenbaum between 1964 and 1966, mimicked a psychotherapist and was operated by processing users' responses to scripts. Using almost no information about human thought or emotion, the DOCTOR script sometimes provided a startlingly human-like interaction. An online demo is available on the LPA website.<ref>[http://www.lpa.co.uk/pws_dem4.htm ELIZA demo]</ref>
* ''Janus'' is also one of the few systems to support temporal questions.
* ''Intellect'' from [[Trinzic]] (formed by the merger of AICorp and Aion).
* BBN’s ''Parlance'' built on experience from the development of the ''Rus''  and ''Irus'' systems.
* [[IBM]] ''Languageaccess''
* [[Q&A (software)|Q&A]] from [[Symantec]].
* ''Datatalker'' from Natural Language Inc.
* ''Loqui''  from [[Bim]].
* ''English Wizard'' from [[Linguistic Technology Corporation]].
* ''iAskWeb'' from Anserity Inc. fully implemented in [[Prolog]] was providing interactive recommendations in NL to users in tax and investment domains in 1999-2001<ref>{{cite book | last = Galitsky
 | first = Boris
 | title = Natural Language Question Answering: technique of semantic headers
 | publisher = Advance Knowledge International
 | date = 2003
 | location = Adelaide, Australia
 | url = http://www.amazon.com/Natural-Language-Question-Answering-system/dp/0868039799
 | isbn = 0868039799
  }}</ref>

==Challenges==
Natural language interfaces have in the past led users to anthropomorphize the computer, or at least to attribute more intelligence to machines than is warranted. On the part of the user, this has led to unrealistic expectations of the capabilities of the system. Such expectations will make it difficult to learn the restrictions of the system if users attribute too much capability to it, and will ultimately lead to disappointment when the system fails to perform as expected as was the case in the [[AI winter]] of the 1970s and 80s.

A [http://arxiv.org/abs/cmp-lg/9503016 1995 paper] titled 'Natural Language Interfaces to Databases – An Introduction', describes some challenges:<ref name="edin"/>
;Modifier attachment
:The request "List all employees in the company with a driving licence" is ambiguous unless you know that companies can't have driving licences.
;Conjunction and disjunction
:"List all applicants who live in California and Arizona" is ambiguous unless you know that a person can't live in two places at once.
;[[Anaphora resolution]]
:resolve what a user means by 'he', 'she' or 'it', in a self-referential query.

Other goals to consider more generally are the speed and efficiency of the interface, in all algorithms these two points are the main point that will determine if some methods are better than others and therefore have greater success in the market. In addition, localisation across multiple language sites requires extra consideration - this is based on differing sentence structure and language syntax variations between most languages.

Finally, regarding the methods used, the main problem to be solved is creating a general algorithm that can recognize the entire spectrum of different voices, while disregarding nationality, gender or age. The significant differences between the extracted features - even from speakers who says the same word or phrase - must be successfully overcome.

==Uses and applications==

The natural language interface gives rise to technology used for many different applications.

Some of the main uses are:

* ''Dictation'', is the most common use for [[automated speech recognition]] (ASR) systems today. This includes medical transcriptions, legal and business dictation, and general word processing. In some cases special vocabularies are used to increase the accuracy of the system.
* ''Command and control'', ASR systems that are designed to perform functions and actions on the system are defined as command and control systems. Utterances like "Open Netscape" and "Start a new xterm" will do just that.
* ''Telephony'', some PBX/[[Voice Mail]] systems allow callers to speak commands instead of pressing buttons to send specific tones.
* ''Wearables'', because inputs are limited for wearable devices, speaking is a natural possibility.
* ''Medical, disabilities'', many people have difficulty typing due to physical limitations such as repetitive strain injuries (RSI), muscular dystrophy, and many others. For example, people with difficulty hearing could use a system connected to their telephone to convert a caller's speech to text.
* ''Embedded applications'', some new cellular phones include C&C speech recognition that allow utterances such as "call home". This may be a major factor in the future of automatic speech recognition and [[Linux]].

Below are named and defined some of the applications that use natural language recognition, and so have integrated utilities listed above.

===Ubiquity===
{{main|Ubiquity (Firefox)}}
Ubiquity, an [[add-on (Mozilla)|add-on]] for [[Mozilla Firefox]], is a collection of quick and easy natural-language-derived commands that act as [[mashup (web application hybrid)|mashups]] of web services, thus allowing users to get information and relate it to current and other webpages.

===Wolfram Alpha===
{{main|Wolfram Alpha}}
Wolfram Alpha is an online service that answers factual queries directly by computing the answer from structured data, rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] would.<ref>{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=2009-03-09 |work=[[The Guardian]] |accessdate=2009-03-09}}</ref> It was announced in March 2009 by [[Stephen Wolfram]], and was released to the public on May 15, 2009.<ref name="launch date">{{cite web|url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |publisher=Wolfram Alpha Blog |date=2009-05-08 |accessdate=2009-10-20}}</ref>

===Siri===
{{main|Siri (software)}}
Siri is an [[intelligent personal assistant]] application integrated with operating system [[iOS]]. The application uses [[natural language processing]] to answer questions and make recommendations.

Siri's marketing claims include that it adapts to a user's individual preferences over time and personalizes results, and performs tasks such as making dinner reservations while trying to catch a cab.<ref>[https://www.apple.com/iphone/features/siri.html Siri webpage]</ref>

===Others===
* [[Ask.com]] - The original idea behind Ask Jeeves (Ask.com) was traditional keyword searching with an ability to get answers to questions posed in everyday, natural language. The current Ask.com still supports this, with added support for math, dictionary, and conversion questions.
* [[Braina]]<ref>[http://www.brainasoft.com/braina/ Braina]</ref> - Braina is a natural language interface for [[Windows OS]] that allows to type or speak English language sentences to perform a certain action or find information.
* [http://www.cmantik.com/ CMANTIK] - CMANTIK is a semantic information search engine which is trying to answer user's questions by looking up relevant information in Wikipedia and some news sources.
* [http://minock.github.io/c-phrase/ C-Phrase] - is a web-based natural language front end to relational databases. C-Phrase runs under Linux, connects with PostgreSQL databases via ODBC and supports both select queries and updates. Currently there is only support for English.
* [http://devtools.korzh.com/easyquery/ EasyQuery] - is a component library (for .NET framework first of all) which allows you to implement natural language query builder in your application. Works both with relational databases or ORM solutions like Entity Framework.
* [https://friendlydata.io/ FriendlyData] is a natural language interface for relational databases.
[[File:GNOME Do Classic.png|thumb|Screenshot of GNOME DO classic interface.]]
* [http://yagadi.com/ Enguage] - this is an open source text understanding interface for web/mobile devices, using publicly available speech-to-text and text-to-speech facilities. This is directed at controlling apps, rather than as a front-end database query or web search. The interpretation of utterances is programmed, and programmable, in natural language utterances; thus, it is (or at least asserts that language is) an [[autopoiesis|autopoietic]] system.<ref>http://www.academia.edu/10177437/An_Autopoietic_Repertoire</ref> It can achieve a deep understanding of text.<ref>http://cit.srce.unizg.hr/index.php/CIT/article/view/2278/1658 if we are holding hands whose hand am i holding</ref> A reference app is available on [https://play.google.com/store/apps/details?id=com.yagadi.iNeed Google Play]
* [[GNOME Do]] - Allows for quick finding miscellaneous artifacts of GNOME environment (applications, Evolution and Pidgin contacts, Firefox bookmarks, Rhythmbox artists and albums, and so on) and execute the basic actions on them (launch, open, email, chat, play, etc.).<ref>Ubuntu 10.04 Add/Remove Applications description for GNOME Do</ref>
* [[hakia]] - hakia is an Internet search engine. The company has invented an alternative new infrastructure to indexing that uses SemanticRank algorithm, a solution mix from the disciplines of ontological semantics, fuzzy logic, computational linguistics, and mathematics.
* [[Lexxe]] - Lexxe is an Internet search engine that uses natural language processing for queries (semantic search). Searches can be made with keywords, phrases, and questions, such as "How old is Wikipedia?" When it comes to facts, Lexxe is quite effective, though needs much improvement in natural language analysis in the area of facts and in other areas.
* [http://www.mnemoo.com/ Mnemoo] - Mnemoo is an answer engine that aimed to directly answer questions posed in plain text (Natural Language), which is accomplished using a database of facts and an inference engine.
* [http://www.naturaldateandtime.com/ Natural Date and Time] - Natural language date and time zone engine. It allows you to ask questions about time, daylight saving information and to do time zone conversions via plain English questions such as 'What is the time in São Paulo when it is 6pm on the 2nd of June in Detroit'.
* [http://www.linguasys.com/web_production/server-item/NLUI%20Server NLUI Server] - an enterprise-oriented multilingual application server by LinguaSys for natural language user interface scripts, supporting English, Spanish, Portuguese, German, Japanese, Chinese, Pashto, Thai, Russian, Vietnamese, Malay, with Arabic, French, and more languages in development.
* [[Pikimal]] - Pikimal uses natural language tied to user preference to make search recommendations by template.
* [[Powerset (company)|Powerset]] — On May 11, 2008, the company unveiled a tool for searching a fixed subset of [[Wikipedia]] using conversational phrases rather than keywords.<ref>{{cite news |url=http://bits.blogs.nytimes.com/2008/05/12/powerset-debuts-with-search-of-wikipedia/ |title=Powerset Debuts With Search of Wikipedia |publisher=The New York Times |first=Miguel |last=Helft |date=May 12, 2008}}</ref> On July 1, 2008, it was purchased by [[Microsoft]].<ref>{{cite web |url=http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archiveurl=https://web.archive.org/web/20090225064356/http://www.powerset.com/blog/articles/2008/07/01/microsoft-to-acquire-powerset |archivedate=February 25, 2009 |title=Microsoft to Acquire Powerset |publisher=Powerset Blog |first=Mark |last=Johnson |date=July 1, 2008}}</ref>
* [[Q-go]] - The Q-go technology provides relevant answers to users in response to queries on a company’s internet website or corporate intranet, formulated in natural sentences or keyword input alike. Q-go was acquired by [[RightNow Technologies]] in 2011
* [[START (MIT project)]] - [http://start.csail.mit.edu/ START], Web-based question answering system. Unlike information retrieval systems such as search engines, START aims to supply users with "just the right information," instead of merely providing a list of hits. Currently, the system can answer millions of English questions about places, movies, people and dictionary definitions.
* [https://www.statmuse.com/ StatMuse] - Natural language analytics platform, currently in private beta with NBA data. Ask natural questions and get rich visualizations and raw data.
* [http://swingly.com/ Swingly] - Swingly is an answer engine designed to find exact answers to factual questions. Just ask a question in plain English - and Swingly will find you the answer (or answers) you're looking for (according to their site).
* [[Yebol]] - Yebol is a vertical "decision" search engine that had developed a knowledge-based, semantic search platform. Yebol's artificial intelligence human intelligence-infused algorithms automatically cluster and categorize search results, web sites, pages and content that it presents in a visually indexed format that is more aligned with initial human intent. Yebol uses association, ranking and clustering algorithms to analyze related keywords or web pages. Yebol integrates natural language processing, metasynthetic-engineered open complex systems, and machine algorithms with human knowledge for each query to establish a web directory that actually 'learns', using correlation, clustering and classification algorithms to automatically generate the knowledge query, which is retained and regenerated forward.<ref>Humphries, Matthew. [http://www.geek.com/articles/news/yebolcom-steps-into-the-search-market-20090731/ "Yebol.com steps into the search market"] ''Geek.com''. 31 July 2009.</ref>

==See also==
*[[Attempto Controlled English]]
*[[Natural user interface]]
*[[Natural language programming]]
**[[xTalk]], a family of English-like programming languages
*[[Chatterbot]], a computer program that simulates human conversations
*[[Noisy text]]
*[[Question answering]]
*[[Selection-based search]]
*[[Semantic search]]
*[[Semantic query]]
*[[Semantic Web]]

==References==
{{reflist}}

{{Internet search}}
{{Computable knowledge}}

{{DEFAULTSORT:Natural language user interface}}
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:661
<=====title=====>:
Preference learning
<=====text=====>:
'''Preference learning''' is a subfield in [[machine learning]] in which the goal is to learn a predictive [[Preference (economics)|preference]] model from observed preference information.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.</ref> In the view of [[supervised learning]], preference learning trains on a set of items which have preferences toward labels or other items and predicts the preferences for all items.

While the concept of preference learning has been emerged for some time in many fields such as [[economics]],<ref name="SHOG00" /> it's a relatively new topic in [[Artificial Intelligence]] research. Several workshops have been discussing preference learning and related topics in the past decade.<ref name="WEB:WORKSHOP" />

==Tasks==

The main task in preference learning concerns problems in "[[learning to rank]]". According to different types of preference information observed, the tasks are categorized as three main problems in the book ''Preference Learning'':<ref name="FURN11" />

===Label ranking===

In label ranking, the model has an instance space <math>X=\{x_i\}\,\!</math> and a finite set of labels <math>Y=\{y_i|i=1,2,\cdots,k\}\,\!</math>. The preference information is given in the form <math>y_i \succ_{x} y_j\,\!</math> indicating instance <math>x\,\!</math> shows preference in <math>y_i\,\!</math> rather than <math>y_j\,\!</math>. A set of preference information is used as training data in the model. The task of this model is to find a preference ranking among the labels for any instance.

It was observed some conventional [[Classification in machine learning|classification]] problems can be generalized in the framework of label ranking problem:<ref name="HARP03" /> if a training instance <math>x\,\!</math> is labeled as class <math>y_i\,\!</math>, it implies that <math>\forall j \neq i, y_i \succ_{x} y_j\,\!</math>. In the [[Multi-label classification|multi-label]] case, <math>x\,\!</math> is associated with a set of labels <math>L \subseteq Y\,\!</math> and thus the model can extract a set of preference information <math>\{y_i \succ_{x} y_j | y_i \in L, y_j \in Y\backslash L\}\,\!</math>. Training a preference model on this preference information and the classification result of an instance is just the corresponding top ranking label.

===Instance ranking===

Instance ranking also has the instance space <math>X\,\!</math> and label set <math>Y\,\!</math>. In this task, labels are defined to have a fixed order <math>y_1 \succ y_2 \succ \cdots \succ y_k\,\!</math> and each instance <math>x_l\,\!</math> is associated with a label <math>y_l\,\!</math>. Giving a set of instances as training data, the goal of this task is to find the ranking order for a new set of instances.

===Object ranking===

Object ranking is similar to instance ranking except that no labels are associated with instances. Given a set of pairwise preference information in the form <math>x_i \succ x_j\,\!</math> and the model should find out a ranking order among instances.

==Techniques==

There are two practical representations of the preference information <math>A \succ B\,\!</math>. One is assigning <math>A\,\!</math> and <math>B\,\!</math> with two real numbers <math>a\,\!</math> and <math>b\,\!</math> respectively such that <math>a > b\,\!</math>. Another one is assigning a binary value <math>V(A,B) \in \{0,1\}\,\!</math> for all pairs <math>(A,B)\,\!</math> denoting whether <math>A \succ B\,\!</math> or <math>B \succ A\,\!</math>. Corresponding to these two different representations, there are two different techniques applied to the learning process.

===Utility function===

If we can find a mapping from data to real numbers, ranking the data can be solved by ranking the real numbers. This mapping is called [[utility function]]. For label ranking the mapping is a function <math>f: X \times Y \rightarrow \mathbb{R}\,\!</math> such that <math>y_i \succ_x y_j \Rightarrow f(x,y_i) > f(x,y_j)\,\!</math>. For instance ranking and object ranking, the mapping is a function <math>f: X \rightarrow \mathbb{R}\,\!</math>.

Finding the utility function is a [[Regression analysis|regression]] learning problem which is well developed in machine learning.

===Preference relations===

The binary representation of preference information is called preference relation. For each pair of alternatives (instances or labels), a binary predicate can be learned by conventional supervising learning approach. Fürnkranz, Johannes and Hüllermeier proposed this approach in label ranking problem.<ref name="FURN03" /> For object ranking, there is an early approach by Cohen et al.<ref name="COHE98" />

Using preference relations to predict the ranking will not be so intuitive. Since preference relation is not transitive, it implies that the solution of ranking satisfying those relations would sometimes be unreachable, or there could be more than one solution. A more common approach is to find a ranking solution which is maximally consistent with the preference relations. This approach is a natural extension of pairwise classification.<ref name="FURN03" />

==Uses==

Preference learning can be used in ranking search results according to feedback of user preference. Given a query and a set of documents, a learning model is used to find the ranking of documents corresponding to the relevance with this query. More discussions on research in this field can be found in Tie-Yan Liu's survey paper.<ref name="LIU09" />

Another application of preference learning is [[recommender systems]].<ref name="GEMM09" /> Online store may analyze customer's purchase record to learn a preference model and then recommend similar products to customers. Internet content providers can make use of user's ratings to provide more user preferred contents.

==See also==
*[[Learning to rank]]

==References==

{{Reflist|
refs=

<ref name="SHOG00">{{
cite journal
|last       = Shogren
|first      = Jason F. |author2=List, John A. |author3=Hayes, Dermot J.
|year       = 2000
|title      = Preference Learning in Consecutive Experimental Auctions
|url        = http://econpapers.repec.org/article/oupajagec/v_3a82_3ay_3a2000_3ai_3a4_3ap_3a1016-1021.htm
|journal    = American Journal of Agricultural Economics
|volume     = 82
|pages      = 1016–1021
|doi=10.1111/0002-9092.00099
}}</ref>

<ref name="WEB:WORKSHOP">{{
cite web
|title      = Preference learning workshops
|url        = http://www.preference-learning.org/#Workshops
}}</ref>

<ref name="FURN11">{{
cite book
|last       = F&uuml;rnkranz
|first      = Johannes
|coauthors  = H&uuml;llermeier, Eyke
|year       = 2011
|title      = Preference Learning
|url        = https://books.google.com/books?id=nc3XcH9XSgYC
|chapter    = Preference Learning: An Introduction
|chapterurl = https://books.google.com/books?id=nc3XcH9XSgYC&pg=PA4
|publisher  = Springer-Verlag New York, Inc.
|pages      = 3–8
|isbn       = 978-3-642-14124-9
}}</ref>

<ref name="HARP03">{{
cite journal
|last       = Har-peled
|first      = Sariel |author2=Roth, Dan |author3=Zimak, Dav
|year       = 2003
|title      = Constraint classification for multiclass classification and ranking
|journal    = In Proceedings of the 16th Annual Conference on Neural Information Processing Systems, NIPS-02
|pages      = 785–792
}}</ref>

<ref name="FURN03">{{
cite journal
|last       = F&uuml;rnkranz
|first      = Johannes
|coauthors  = H&uuml;llermeier, Eyke
|year       = 2003
|title      = Pairwise Preference Learning and Ranking
|journal    = Proceedings of the 14th European Conference on Machine Learning
|pages      = 145–156
}}</ref>

<ref name="COHE98">{{
cite journal
|last       = Cohen
|first      = William W. |author2=Schapire, Robert E. |author3=Singer, Yoram
|year       = 1998
|title      = Learning to order things
|url        = http://dl.acm.org/citation.cfm?id=302528.302736
|journal    = In Proceedings of the 1997 Conference on Advances in Neural Information Processing Systems
|pages      = 451–457
}}</ref>

<ref name="LIU09">{{
cite journal
|last       = Liu
|first      = Tie-Yan
|year       = 2009
|title      = Learning to Rank for Information Retrieval
|url        = http://dl.acm.org/citation.cfm?id=1618303.1618304
|journal    = Foundations and Trends in Information Retrieval
|volume     = 3
|issue      = 3
|pages      = 225–331
|doi        = 10.1561/1500000016
}}</ref>

<ref name="GEMM09">{{
cite journal
|last       = Gemmis
|first      = Marco De
|author2=Iaquinta, Leo |author3=Lops, Pasquale |author4=Musto, Cataldo |author5=Narducci, Fedelucio |author6= Semeraro,Giovanni 
|year       = 2009
|title      = Preference Learning in Recommender Systems
|url        = http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/preference-learning.pdf#page=45
|journal    = PREFERENCE LEARNING
|volume     = 41
|pages      = 387–407
|doi=10.1007/978-3-642-14125-6_18
}}</ref>

}}

==External links==
*[http://www.preference-learning.org/ Preference Learning site]

[[Category:Information retrieval techniques]]
[[Category:Machine learning]]
<=====doc_Id=====>:664
<=====title=====>:
Anchor text
<=====text=====>:
{{Use dmy dates|date=February 2013}}
The '''anchor text''', '''link label''', '''link text''', or '''link title''' is the visible, clickable text in a [[hyperlink]]. The words contained in the anchor text can determine the ranking that the page will receive by search engines. Since 1998, some [[web browser]]s have added the ability to show a [[tooltip]] for a hyperlink before it is selected. Not all links have anchor texts because it may be obvious where the link will lead due to the context in which it is used. Anchor texts normally remain below 60 [[Character (computing)|characters]]. Different browsers will display anchor texts differently. Usually, web search engines analyze anchor text from hyperlinks on web pages. Other services apply the basic principles of anchor text analysis as well. For instance, [[List of academic databases and search engines|academic search engines]] may use [[citation]] context to classify [[Academic publishing|academic articles]],<ref>{{cite web|author1=Bader Aljaber |author2=Nicola Stokes |author3=James Bailey |author4=Jian Pei |url=http://www.springerlink.com/content/p278617582u5x3x1/|title=Document clustering of scientific texts using citation contexts |date=1 April 2010|publisher=Springer}}</ref> and anchor text from documents linked in [[mind maps]] may be used too.<ref>Needs new reference link</ref> [[File:Anchor text.png|thumb|Visual implementation of anchor text]]

==Overview==
Anchor text usually gives the user relevant descriptive or contextual information about the content of the link's destination. The anchor text may or may not be related to the actual text of the [[Uniform Resource Locator|URL]] of the link. For example, a hyperlink to the [[English Wikipedia|English-language Wikipedia]]'s [[homepage]] might take this form:

:<code><nowiki><a href="http://en.wikipedia.org/wiki/Main_Page">Wikipedia</a></nowiki></code>

The anchor text in this example is "Wikipedia"; the longer, but vital, URL <code><nowiki>http://en.wikipedia.org/wiki/Main_Page</nowiki></code> needed to locate the target page, displays on the web page as {{srlink|Main Page|Wikipedia}}, contributing to clean, easy-to-read text.

==Common misunderstanding of the concept==

This proper method of linking is beneficial to users and [[webmaster]]s as anchor text holds significant [[weight]] in [[search engine]] rankings. The limit of the [[concept]] is building [[Sentence (linguistics)|sentence]]s only composed with linked [[word]]s.{{citation needed|date=September 2011}}

==Search engine algorithms==
Anchor text is weighted (ranked) highly in [[search engine]] [[algorithm]]s, because the linked text is usually relevant to the [[landing page]]. The objective of search engines is to provide highly relevant search results; this is where anchor text helps, as the tendency was, more often than not, to hyperlink words relevant to the landing page. Anchor text can also serve the purpose of directing the user to internal pages on the site, which can also help to rank the website higher in the search rankings.<ref name="Search Engine Watch 1">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2169750/How-the-Web-Uses-Anchor-Text-in-Internal-Linking-Study|title=
How the Web Uses Anchor Text in Internal Linking [Study]|accessdate=6 July 2012}}</ref>

[[Webmaster]]s may use anchor text to procure high results in [[search engine results page]]s. [[Google]]'s [[Google Webmaster Tools|Webmaster Tools]] facilitate this optimization by letting [[website]] owners view the most common words in anchor text linking to their site.<ref>{{cite web
|last=Fox
|first=Vanessa
|url=http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html
|title=Get a more complete picture about how other sites link to you
|date=15 March 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= https://web.archive.org/web/20070331195216/http://googlewebmastercentral.blogspot.com/2007/03/get-more-complete-picture-about-how.html| archivedate= 31 March 2007 <!--DASHBot-->| deadurl= no}}</ref>
In the past, [[Google bomb]]ing was possible through anchor text manipulation; however, in January 2007, Google announced it had updated its algorithm to minimize the impact of Google bombs, which refers to a prank where people attempt to cause someone else's site to rank for an obscure or meaningless query.<ref>{{cite web
|last=Cutts
|first=Matt
|url=http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html
|title=A quick word about Googlebombs
|date=25 January 2007
|publisher=Official Google Webmaster Central Blog
|accessdate=2007-03-27
| archiveurl= https://web.archive.org/web/20070324043013/http://googlewebmastercentral.blogspot.com/2007/01/quick-word-about-googlebombs.html| archivedate= 24 March 2007 <!--DASHBot-->| deadurl= no}}</ref>

In April 2012, Google announced in its March "[[Google Penguin|Penguin]]" update that it would be changing the way it handled anchor text, implying that anchor text would no longer be as important an element for their ranking metrics.<ref>{{cite web|url=http://insidesearch.blogspot.co.uk/2012/04/search-quality-highlights-50-changes.html|title=Google's March Update|publisher=Google}}</ref> Moving forward, Google would be paying more attention to a diversified link profile which has a mix of anchor text and other types of links.
.<ref name="Search Engine Watch 2">{{cite web|publisher=[[Search Engine Watch]]|url=http://searchenginewatch.com/article/2172839/Google-Penguin-Update-Impact-of-Anchor-Text-Diversity-Link-Relevancy|title=
Google Penguin Update: Impact of Anchor Text Diversity & Link Relevancy|accessdate=6 July 2012}}</ref>

However a 2016 study of anchor text influence across 16,000 keywords found that presence of exact and partial match anchor links continues to have a strong correlation with Google rankings.<ref>{{cite web|publisher=Ahrefs|url=https://ahrefs.com/blog/anchor-text|title=
Everything You Ever Wanted To Know About Anchor Text|accessdate=27 July 2016}}</ref>

August 2016 study conducted by Moz, found that Exact and partial match domains can be affected by over optimization penalty since Google considers domain Brand and naked URL links as Exact match.<ref>{{Cite news|url=https://moz.com/ugc/case-study-the-interconnectedness-of-local-seo-and-exact-match-domains|title=Case Study: The Interconnectedness of Local SEO and Exact Match Domains|newspaper=Moz|access-date=2016-12-12}}</ref>

==Terminology==
There are different classifications of anchor text that are used within the search engine optimization community such as the following:

;Exact Match: an anchor that is used with a keyword that mirrors the page that is being linked to. Example: "[[search engine optimization]]" is an exact match anchor because it's linking to a page about "search engine optimization.
;Branded: a brand that is used as the anchor. "[[Wikipedia]]" is a branded anchor text.
;Naked Link: a URL that is used as an anchor. "[[www.wikipedia.com]]" is a naked link anchor.
;Generic: a generic word or phrase that is used as the anchor. "Click here" is a generic anchor. Other variations may include "go here", "visit this website", etc.
;Images: whenever an image is linked, Google will use the "ALT" tag as the anchor text.

==References==

{{reflist|colwidth=30em}}

[[Category:Information retrieval techniques]]
[[Category:Internet search engines]]
[[Category:Internet terminology]]
[[Category:Search engine optimization]]
[[Category:Hypertext]]
<=====doc_Id=====>:667
<=====title=====>:
Communication engine
<=====text=====>:
{{Orphan|date=February 2009}}
A '''communication engine''' is a tool that sends user requests to several other [[communication protocols]] and/or [[database]]s and aggregates the results into a single list or displays them according to their source. Communication engines enable users to enter communication account authorization once and access several communication avenues simultaneously. Communication engines operate on the premise that the [[World Wide Web]] is too large for any one engine to index it all and that more productive results can be obtained by combining the results from several engines dynamically. This may save the user from having to use multiple engines separately.

[[Category:Information retrieval techniques]]
[[Category:Computing terminology]]


{{Web-stub}}
<=====doc_Id=====>:670
<=====title=====>:
Hashtag
<=====text=====>:
{{Use mdy dates|date=May 2016}}
{{Use American English|date=May 2016}}

[[File:Global Summit to End Sexual Violence in Conflict (14203190979).jpg|thumb|A sign suggesting the use of a #TimeToAct hashtag at a 2014 conference]]

A '''hashtag''' is a type of label or [[Tag (metadata)|metadata tag]] used on [[Social networking service|social network]] and [[microblogging]] services which makes it easier for users to find messages with a specific theme or content. Users create and use hashtags by placing the [[Number sign|hash character <code>#</code>]] (also known as the number sign or pound sign) in front of a word or unspaced phrase, either in the main text of a message or at the end. Searching for that hashtag will yield each message that has been tagged with it. A hashtag archive is consequently collected into a single stream under the same hashtag.<ref>{{Cite journal|last=Chang|first=Hsia-Ching|last2=Iyer|first2=Hemalata|title=Trends in Twitter Hashtag Applications: Design Features for Value-Added Dimensions to Future Library Catalogues|url=http://muse.jhu.edu/article/485537|journal=Library Trends|volume=61|issue=1|pages=248–258|doi=10.1353/lib.2012.0024|issn=1559-0682}}</ref> For example, on the [[photo sharing|photo-sharing]] service [[Instagram]], the hashtag ''#bluesky'' allows users to find all the posts that have been tagged using that hashtag.

Because of its widespread use, ''hashtag'' was added to the ''[[Oxford English Dictionary]]'' in June 2014.<ref>{{cite web |title='Hashtag' added to the OED – but # isn't a hash, pound, nor number sign|url=http://www.theregister.co.uk/2014/06/13/hashtag_added_to_the_oed/|work=The Register|date=June 13, 2014}}</ref><ref>{{cite web |title=New words notes June 2014|url=http://public.oed.com/the-oed-today/recent-updates-to-the-oed/june-2014-update/new-words-notes-june-2014/|work=Oxford English Dictionary|date=June 2014}}</ref>  The term ''hashtag'' can also refer to the hash symbol itself when used in the context of a hashtag.<ref>{{cite web |title=Oxford English Dictionary – Hash|url=http://www.oed.com/view/Entry/389023#eid301493073|work=Oxford English Dictionary|date=June 2014}}</ref>

== Origin and use ==
The [[Number sign|pound sign]] or [[Number sign|hash symbol]] was often used in [[information technology]] to highlight a special meaning. (It should be noted that the words "Pound Sign" in the UK refer specifically to currency "£" - extended ASCII character 156 - and not weight.) In 1970 for example, the number sign was used to denote ''immediate'' [[address mode]] in the assembly language of the [[PDP-11]]<ref>{{cite web|url=https://programmer209.wordpress.com/2011/08/03/the-pdp-11-assembly-language/ |title=PDP-11 assembly language |publisher=Programmer209.wordpress.com |date=August 3, 2011 |accessdate=August 25, 2014}}</ref> when placed next to a symbol or a number. In 1978, [[Brian Kernighan]] and [[Dennis Ritchie]] used ''#'' in the [[C (programming language)|C programming language]] for special keywords that had to be processed first by the [[C preprocessor]].<ref>{{cite book|title=[[The C Programming Language]]|authors=B.W. Kernighan & d. Ritchie|publisher=Prentice Hall|year=1978|pages=86 and 207|isbn=0-13-110163-3}}</ref> Since before the invention of the hashtag, the pound sign has been called the "hash symbol" in some countries outside of North America.<ref>{{cite book|last1=Bourke|first1=Jane|title=Communication Technology Resource Book|date=2004|publisher=Ready-Ed Publications|pages=19|url=https://books.google.com/books?id=gPNBTmxzpIIC&lpg=PA19&dq=hash%20key%20telephone&pg=PA19#v=onepage&q=hash&f=false|accessdate=November 7, 2014|isbn=978-1-86397-585-8}}</ref><ref>{{cite book|last1=Hargraves|first1=Orin|title=Mighty fine words and smashing expressions : making sense of transatlantic English|date=2003|publisher=Oxford Univ. Press|location=Oxford [u.a.]|isbn=978-0-19-515704-8|pages=33, 260|url=https://books.google.com/books?id=dUTdk93cq9UC&lpg=PA260&dq=hash%20telephone&pg=PA260#v=onepage&q=hash%20mark&f=false}}</ref>

The pound sign appeared and was used by people within [[Internet Relay Chat|IRC]] networks to label groups and topics.<ref>"Channel Scope". Section 2.2. RFC 2811</ref> Channels or topics that are available across an entire IRC network are prefixed with a hash symbol # (as opposed to those local to a server, which use an [[ampersand]] '&').<ref>{{cite IETF |title=Internet Relay Chat Protocol |rfc=1459 |sectionname=Channels |section=1.3 |page= |last1=Oikarinen |first1=Jarkko |authorlink1=Jarkko Oikarinen |last2=Reed |first2=Darren |authorlink2= |year=1993 |month=May |publisher=[[Internet Engineering Task Force|IETF]] |accessdate=June 3, 2014}}</ref>

The use of the pound sign in IRC inspired<ref>{{cite web|url=http://www.cmu.edu/homepage/computing/2014/summer/originstory.shtml |title=#OriginStory|publisher=Carnegie Mellon University|date=August 29, 2014}}</ref> [[Chris Messina (open source advocate)|Chris Messina]] to propose a similar system to be used on Twitter to tag topics of interest on the microblogging network.<ref>{{cite news | url=http://www.nytimes.com/2011/06/12/fashion/hashtags-a-new-way-for-tweets-cultural-studies.html?_r=1&pagewanted=all | title=Twitter's Secret Handshake | work=The New York Times | date=June 10, 2011 | accessdate=July 26, 2011 | author=Parker, Ashley}}</ref> He posted the first hashtag on Twitter:
{{quote |1=How do you feel about using # (pound) for groups. As in #barcamp [msg]? |author = Chris Messina |source = ("factoryjoe"), August 23, 2007<ref>{{cite web|url = https://twitter.com/factoryjoe/statuses/223115412|title = Twitter post|author = Chris Messina ("factoryjoe")|date = August 23, 2007<!-- 3:25 PM-->}}</ref> |width  = 50% |align  = center }}
Messina’s suggestion to use the hashtag was not adopted by Twitter, but the practice took off after hashtags were widely used in tweets relating to the [[2007 San Diego forest fires]] in Southern California.<ref>[http://mashable.com/2013/10/08/what-is-hashtag/ What is hashtag?"], Mashable, 8 October 2013</ref><ref>https://factoryjoe.com/2007/10/22/twitter-hashtags-for-emergency-coordination-and-disaster-relief/</ref>

According to Messina, he suggested use of the hashtag to make it easy for "lay" users to search for content and find specific relevant updates; they are for people who do not have the technological knowledge to navigate the site. Therefore, the hashtag “was created organically by Twitter users as a way to categorize messages." <ref>{{Cite journal|last=Scott|first=Kate|date=2015-05-01|title=The pragmatics of hashtags: Inference and conversational style on Twitter|url=http://www.sciencedirect.com/science/article/pii/S037821661500096X|journal=Journal of Pragmatics|volume=81|pages=8–20|doi=10.1016/j.pragma.2015.03.015}}</ref>

Internationally, the hashtag became a practice of writing style for Twitter posts during the [[2009–2010 Iranian election protests]]; Twitter users inside and outside Iran used both English- and [[Persian language|Persian]]-language hashtags in communications during the events.<ref>{{cite news|title=The story of the hashtag began with Iranians|url=http://www.dw.de/حکایت-هشتگی-که-ایرانیان-آغاز-کردند/g-18012627|accessdate=March 12, 2015|publisher=Deutsche Welle Persian|date=2009}}</ref>

The first published use of the term "hash tag" was in a blog post by Stowe Boyd, "Hash Tags = Twitter Groupings,"<ref>{{cite web|url=http://stoweboyd.com/post/39877198249/hash-tags-twitter-groupings |title=Stowe Boyd, Hash Tags = Twitter Groupings |publisher=Stoweboyd.com |date= |accessdate=September 19, 2013}}</ref> on August 26, 2007, according to lexicographer [[Ben Zimmer]], chair of the American Dialect Society's New Words Committee.

Beginning July 2, 2009,<ref>{{cite web|title=Twitter Makes Hashtags More #Useful|url=http://techcrunch.com/2009/07/02/twitter-makes-hashtags-more-useful/|accessdate=December 27, 2015}}</ref> Twitter began to hyperlink all hashtags in tweets to Twitter search results for the hashtagged word (and for the standard spelling of commonly misspelled words). In 2010, Twitter introduced "[[Twitter#Trending topics|Trending Topics]]" on the Twitter front page, displaying hashtags that are rapidly becoming popular. Twitter has an algorithm to tackle attempts to [[spamming|spam]] the trending list and ensure that hashtags trend naturally.<ref>{{cite web|url=http://www.allisayis.com/the-secret-of-twitters-trending-hashtags-with-insight-and-tips/ |title=The Secret of Twitter's Trending Hashtags With Insight and Tips |publisher=AllISayIs.com |date= |accessdate=December 3, 2014}}</ref>

Although the hashtag started out most popularly on Twitter as the main social media platform for this use, the use has extended to other social media sites including Instagram, Facebook, Flickr, Tumblr, and Google+.<ref>{{Cite web|url=http://www.cnn.com/2013/06/12/tech/social-media/facebook-hashtags/index.html|title=Facebook finally gets #hashtags - CNN.com|last=Mashable|first=By Christina Warren|website=CNN|access-date=2016-05-16}}</ref>

In China, microblogs [[Sina Weibo]] and [[Tencent Weibo]] use a double-hashtag #HashName# format, since the lack of spacing between [[Chinese characters]] necessitates a closing tag. In contrast, when using Chinese characters (and [[orthographies]] with similar spacing conventions) on [[Twitter]], users must insert spacing before and after the hashtagged element (e.g. '我 #爱 你' instead of '我#爱你')<ref>{{cite news|last1=Martin|first1=Rick|title=Twitter Rolls Out Hashtag Support for Japanese, Korean, Chinese, and Russian|url=https://www.techinasia.com/twitter-hashtag-languages/|accessdate=March 5, 2015|publisher=Tech in Asia|date=July 13, 2011}}</ref> or insert a [[zero-width non-joiner]] character before and after the hashtagged element, to retain a linguistically natural appearance, such as '我#爱你'.<ref>{{cite news|last1=International services team|title=Right-to-left languages on Twitter|url=https://blog.twitter.com/2012/right-to-left-languages-on-twitter|accessdate=March 5, 2015|publisher=Twitter|date=April 5, 2012}}</ref>

== Style ==
On microblogging or social networking sites, hashtags can be inserted anywhere within a sentence, either preceding it, following it as a [[postscript]], or being included as a word within the sentence (e.g. "It is #sunny today").

The quantity of hashtags used in a post or tweet is just as important as the types of hashtags used. It is currently considered acceptable to tag a post once when contributing to a specific conversation. Two hashtags are considered acceptable when adding a location to the conversation. Three hashtags are seen by some as the "absolute maximum", and any contribution exceeding this risks "raising the ire of the community."<ref>{{cite web|title=What is a (#) Hashtag?|url=http://www.hashtags.org/how-to/history/what-is-a-hashtag/|publisher=Hashtags.org|accessdate=February 22, 2014}}</ref>

As well as frustrating other users, the misuse of hashtags can lead to account suspensions. Twitter warns that adding hashtags to unrelated tweets, or repeated use of the same hashtag without adding to a conversation, could cause an account to be filtered from search, or even suspended.<ref>{{cite web|title=The Twitter Rules|url=https://support.twitter.com/groups/56-policies-violations/topics/236-twitter-rules-policies/articles/18311-the-twitter-rules|publisher=Twitter, Inc.|accessdate=February 22, 2014}}</ref>{{failed verification|date=August 2014}}

[[Jimmy Fallon]] and [[Justin Timberlake]] performed a sketch parodying the often misused and misunderstood usage of hashtags on ''[[Late Night with Jimmy Fallon]]'' in September 2013.<ref>{{cite web|author=The Tonight Show Starring Jimmy Fallon |url=https://www.youtube.com/watch?v=57dzaMaouXA |title="#Hashtag" with Jimmy Fallon & Justin Timberlake (Late Night with Jimmy Fallon) |publisher=YouTube |date=September 24, 2013 |accessdate=August 25, 2014}}</ref>

== Function ==

[[File:Seguir hashtags.png|upright=1.3|right|thumb|Search bar in the header of a social networking site, searching for most recent posts containing the hashtag #science]]

Hashtags are mostly used in unmoderated, ad hoc discussion forums; any combination of characters led by a hash symbol is a hashtag, and any hashtag, if promoted by enough individuals, can "trend" and attract more individual users to discussion. On Twitter, when a hashtag becomes extremely popular, it will appear in the "Trending Topics" area of a user's homepage. The trending topics can be organized by geographic area or by all of Twitter. Hashtags are neither registered nor controlled by any one user or group of users. They cannot be "retired" from public usage, meaning that any given hashtag can theoretically be used in perpetuity. They do not contain any set definitions, meaning that a single hashtag can be used for any number of purposes, as chosen by those who make use of them.

Hashtags intended for discussion of a particular event tend to use an obscure wording to avoid being caught up with generic conversations on similar subjects, such as a cake festival using #cakefestival rather than simply #cake. However, this can also make it difficult for topics to become "trending topics" because people often use different spelling or words to refer to the same topic.  In order for topics to trend, there has to be a consensus, whether silent or stated, that the hashtag refers to that specific topic.

Hashtags also function as beacons in order for users to find and "follow" (subscribe) or "list" (organize into public contact lists) other users of similar interest.

In recent years, broadcasters such as [[Channel 4]] have employed the hashtag during the airing of programmes such as [[First Dates]] and [[The Undateables]]. Research has shown that audience numbers go up when individuals can be interactive - by tweeting while viewing a programme on TV.

Hashtags can be used on the social network [[Instagram]], by posting a picture and hashtagging it with its subject. As an example, a photo of oneself and a friend posted to the social network can be hashtagged #bffl or #friends. Instagram has banned certain hashtags, some because they are too generic, such as #photography #iPhone #iphoneography, and therefore do not fulfill a purpose. They have also blocked hashtags that can be linked to illegal activities, such as drug use.<ref>{{cite web|url=http://www.bbc.co.uk/news/technology-24842750 |title=Instagram banned hashtags | date = November 7, 2013|publisher=[[BBC.co.uk]] |accessdate=November 25, 2013}}</ref> The ban against certain hashtags has a consequential role in the way that particular [[subaltern]] communities are built and maintained on Instagram. Despite Instagram's content policies, users are finding creative ways of maintaining their practices and ultimately circumventing censorship.<ref>Olszanowski, M. (2014). "Feminist Self-Imaging and Instagram: Tactics of Circumventing Sensorship{{sic|hide=y}}". ''Visual Communication Quarterly,'' 21(1), 83-95. Retrieved February 8, 2015, from http://www.tandfonline.com/doi/abs/10.1080/15551393.2014.928154#.VNgGT7DF-7FF-7F</ref>

Hashtags are also used informally to express context around a given message, with no intent to categorize the message for later searching, sharing, or other reasons. One of the functions of the hashtag is to serve as a reflexive meta-commentary, which contributes to the idea of how written communication in new media can be paralleled to how pragmatic methodology is applied to speech.<ref>{{Cite web|url=http://www.linguistics.fi/julkaisut/SKY2014/Wikstrom.pdf|title=#srynotfunny: Communicative Functions of Hashtags on Twitter|last=Wilkström|first=Peter|date=2014|work=SKY Journal of Linguistics|publisher=|access-date=May 15, 2016}}</ref>

This can help express contextual cues or offer more depth to the information or message that appears with the hashtag. "My arms are getting darker by the minute. #toomuchfaketan". Another function of the hashtag can be used to express personal feelings and emotions. For example, with "It's Monday!! #excited #sarcasm" in which the adjectives are directly indicating the emotions of the speaker. It can also be used as a disclaimer of the information that the hashtag accompanies, as in, "BREAKING: US GDP growth is back! #kidding". In this case, the hashtag provides an essential piece of information in which the meaning of the utterance is changed entirely by the disclaimer hashtag. This may also be conveyed with #sarcasm, as in the previous example. Self-mockery is another informal function of the hashtag used by writers, as in this tweet: "Feeling great about myself till I met an old friend who now races at the Master's level. Yup, there's today's #lessoninhumility," where the informality of the hashtag provides commentary on the tweet itself.<ref name=":0">{{Cite web|url=http://www.skase.sk/Volumes/JTL28/pdf_doc/05.pdf|title=The ‘hashtag’: A new word or a new rule?|last=Caleffi|first=Paola-Maria|date=|website=Skase Journal of Theoretical Linguistics|publisher=|access-date=}}</ref>

== Other uses ==
The feature has been added to other, non-short-message-oriented services, such as the user comment systems on [[YouTube]] and [[Gawker Media]]. In the case of the latter, hashtags for blog comments and directly submitted comments are used to maintain a more constant rate of user activity even when paid employees are not logged into the website.<ref>{{cite web|url = http://gawker.com/5382267/anarchy-in-the-machine-welcome-to-gawkers-open-forums|title = Anarchy in the Machine: Welcome to Gawker's Open Forums|author = Gabriel Snyder|publisher = Gawker|date = October 15, 2009<!-- 3:25 PM-->}}</ref><ref>{{cite web|url = http://www.niemanlab.org/2009/10/got-a-tip-gawker-media-opens-tag-pages-to-masses-expecting-chaos/|title = Got a #tip? Gawker Media opens tag pages to masses, expecting "chaos"|author = Zachary M. Seward|publisher = Nieman Journalism Lab|date = October 15, 2009 <!-- 8 a.m. -->}}</ref> Real-time search aggregators such as the former [[Google Real-Time Search]] also support hashtags in syndicated posts, meaning that hashtags inserted into Twitter posts can be hyperlinked to incoming posts falling under that same hashtag; this has further enabled a view of the "river" of Twitter posts that can result from search terms or hashtags.{{citation needed|date=September 2014}}

== Uses ==

=== Broadcast media ===

The use of hashtags has extended to [[television]]{{nsmdns}}a concept that began rising in prominence in the early 2010s. Broadcasters may display a hashtag as an on-screen [[digital on-screen graphic|bug]], encouraging viewers to participate in a [[backchannel]] of discussion via social media prior to, during, or after the program. [[Television commercial]]s have sometimes contained hashtags for similar purposes.<ref>{{cite web|url = http://www.tvguide.com/News/New-TV-Screen-1032111.aspx|title = New to Your TV Screen: Twitter Hashtags|date = April 21, 2011<!-- 3:25 PM-->|author = Michael Schneider|publisher = TV Guide}}</ref> Hashtag bugs appear on either corner of the screen, or they may appear at the end of an advertisement.<ref>{{cite web|url = http://mashable.com/2012/12/03/mcdonalds-tv-ad-twitter-hashtag/|title = McDonald's Releases First TV Ad With Twitter Hashtag|date = Dec 3, 2012|author = Todd Wasserman|publisher = Mashable}}</ref>

While personalities associated with broadcasts, such as hosts and correspondents, also promote their corporate or personal Twitter usernames in order to receive mentions and replies to posts, usage of related or "branded" hashtags alongside Twitter usernames (e.g., [[The Ed Show|#edshow]] as well as [[Ed Schultz|@edshow]]) is increasingly encouraged as a microblogging style in order to "trend" the hashtag (and, hence, the discussion topic) in Twitter and other search engines. Broadcasters also make use of such a style in order to index select posts for live broadcast. Chloe Sladden, Twitter's director of media partnerships, identified two types of television-formatted usage of hashtags: hashtags which identify a series being broadcast (i.e. [[It's Always Sunny in Philadelphia|#SunnyFX]]) and instantaneous, "temporary" hashtags issued by television personalities to gauge topical responses from viewers during broadcasts.<ref>{{cite web|url = http://www.fastcompany.com/1747437/twitter-tv-hashtag-tips-twitters-own-expert|title = Twitter TV Hashtag Tips From Twitter's Own Expert|author = Gregory Ferenstein|date = April 15, 2011|publisher = Fast Company}}</ref> Some have speculated that hashtags might take the place of (or co-exist with) the [[Nielsen ratings|Nielsen television ratings system]].<ref>{{cite web|url=http://www.ibtimes.com/twitter-chatter-correlates-tv-ratings-good-or-bad-news-nielsen-1144311 |title=Twitter Chatter Correlates With TV Ratings, But Is That Good Or Bad News For Nielsen? |work=International Business Times |date=March 22, 2013 |accessdate=September 19, 2013}}</ref>

An example of trending "temporary" hashtags garnering viewers during broadcasts is observed on ''[[The Tonight Show]]'' with [[Jimmy Fallon]], a variety [[talk show]] on [[NBC]]. Every Wednesday, Fallon hosts a segment on his show called "Tonight Show Hashtags," which engages viewers by inviting them via Twitter to post humorous stories based on a specific hashtag topic, such as #WhydidIsaythat, #Worstfirstdate, to #Onetimeinclass, reflecting on funny experiences in daily life. By using hashtags, Fallon creates a sense of community and solidarity among his viewers and draws a wider range of viewers through an online platform while they watch a classic, non-interactive television program. Because of its popularity, the "Tonight Show Hashtags" are usually the 'most tweeted hashtag' on Twitter, which promotes the show. By engaging viewers with a lighthearted subject and simple hashtags, Fallon can gauge topical responses from viewers during broadcasts and also use the hashtags to brand his show.{{citation needed|date=April 2016}}

The increased usage of hashtags as brand promotion devices has been compared to the promotion of branded "[[Index term|keywords]]" by [[AOL]] in the late 1990s and early 2000s, as such keywords were also promoted at the end of television commercials and series episodes.<ref>{{cite web|url = http://techcrunch.com/2012/06/10/twitter-hashtag-pages-aol-keywords/|title = Twitter's Hashtag Pages Could Be The New AOL Keywords — But Better|author = Ryan Lawler|date = June 10, 2012|publisher = Techcrunch}}</ref>

The late-night television comedy [[game show]] [[@midnight]] with [[Chris Hardwick]] on [[Comedy Central]] features a daily game entitled "Hashtag Wars," in which three comedians compete against one another to come up with phrases based on a given hashtag theme.

Some hashtags have become famous worldwide. For instance the slogan "''[[Je suis Charlie]],''" which was first used on Twitter as the hashtag #jesuischarlie and #iamcharlie to indicate solidarity with ''Charlie Hebdo'' offices attacked in Paris, spread to the internet at large.

=== Purchasing ===

Since February 2013 Twitter and [[American Express]] have collaborated to enable users to pay for discounted goods online by tweeting a special hashtag.<ref>{{cite news | first = Kelly | last = Heather | title = Twitter and Amex let you pay with a hashtag | date = February 12, 2013 | url = http://edition.cnn.com/2013/02/11/tech/social-media/twitter-hashtag-purchases/| work = CNN | accessdate = November 25, 2013}}</ref> American Express members can sync their card with Twitter and pay for offers by tweeting; American Express tweets a response to the member that confirms the purchase.<ref>{{cite web|url=https://sync.americanexpress.com/Twitter/Index |title=Sync with Twitter|publisher=Amex Sync |accessdate=November 25, 2013}}</ref>

=== Event promotion ===

[[File:Occupy for Rights.JPG|thumb|[[Stencil graffiti]] promoting the hashtag #OccupyForRights]]

Organized real-world events have used hashtags and ad hoc lists for discussion and promotion among participants. Hashtags are used as beacons by event participants in order to find each other, both on Twitter and, in many cases, during actual physical events.

Companies and advocacy organizations have taken advantage of hashtag-based discussions for promotion of their products, services or campaigns.

Political protests and campaigns in the early 2010s, such as [[Occupy Wall Street|#OccupyWallStreet]] and [[2011 Libyan civil war|#LibyaFeb17]], have been organized around hashtags or have made extensive usage of hashtags for the promotion of discussion.

=== Consumer complaints ===
Hashtags are often used by consumers on social media platforms in order to complain about the customer service experience with large companies.  The term "bashtag" has been created to describe situations in which a user refers to a corporate social media hashtag in order to criticise the company or to tell others about poor customer service. For example, in January 2012, [[McDonald's]] created the #McDStories hashtag so that customers could share positive experiences about the restaurant chain. But, the marketing effort was cancelled after two hours when McDonald's received numerous complaint tweets rather than the positive stories they were anticipating.<ref>{{cite news | first = Alexis | last = Akwagyiram | title = Are Twitter and Facebook changing the way we complain? | date = May 17, 2012 | url = http://www.bbc.co.uk/news/uk-18081651 | work = BBC News | accessdate = June 12, 2012}}</ref>

=== Sentiment analysis ===
The use of hashtags also reveals what feelings or sentiment an author attaches to a statement. This can range from the obvious, where a hashtag directly describes the state of mind, to the less obvious. For example, words in hashtags are the strongest predictor of whether or not a statement is [[sarcasm|sarcastic]].<ref>{{cite journal|last=Maynard|title=Who cares about sarcastic tweets? Investigating the impact of sarcasm on sentiment analysis|journal=Proceedings of the Conference on Language Resources and Evaluation|year=2014}}</ref>—a difficult [[Artificial Intelligence|AI]] problem.<ref>http://www.huffingtonpost.com/entry/power-yourself-with-viral-marketing-become-a-hashtag_us_57bf13e6e4b06384eb3e7f1d?dxrywr9zcw30rizfr</ref>

=== Sports ===
The YouTuber Spencer FC used the hashtag for the name and crest of his YouTube-based association football team, [[Hashtag United]].

== In popular culture ==
During the [[2011 Canadian leaders debates|April 2011 Canadian party leader debate]], [[Jack Layton]], then-leader of the [[New Democratic Party of Canada|New Democratic Party]], referred to [[Conservative Party of Canada|Conservative]] Prime Minister [[Stephen Harper]]'s crime policies as "a hashtag fail" (presumably #fail).<ref>{{cite news|url = http://www.theglobeandmail.com/news/politics/jack-laytons-debatable-hashtag-fail/article576224/|title = Jack Layton's debatable 'hashtag' #fail|author = Anna Mehler Paperny|publisher = The Globe and Mail|date = April 13, 2011 <!-- , 6:00 AM EDT --> }}</ref><ref>{{cite news|url = http://www.cbc.ca/news/politics/canadavotes2011/story/2011/04/13/cv-debate-twitter.html|title = Canadians atwitter throughout debate|date = April 13, 2011<!-- 3:25 PM-->|publisher = CBC News}}</ref>

The term "hashtag [[Hip hop music|rap]]", coined by [[Kanye West]],<ref>{{cite web |url = http://blogs.villagevoice.com/music/2010/11/the_ten_best_qu.php|title = The Ten Best Quotes From Kanye West's Epic Hot 97 Interview With Funkmaster Flex|author = Zach Baron|publisher = The Village Voice|date = November 3, 2010}}</ref> was developed in the 2010s to describe a style of rapping which, according to Rizoh of the ''[[Houston Press]],'' uses "three main ingredients: a metaphor, a pause, and a one-word [[punch line]], often placed at the end of a rhyme".<ref>{{cite web|url = http://blogs.houstonpress.com/rocks/2011/07/a_brief_history_of_hashtag_rap.php|title = A Brief History Of Hashtag Rap|author = Rizoh|publisher = Houston Press|date = July 7, 2011 <!-- at 9:00 AM --> }}</ref> Rappers [[Nicki Minaj]], [[Big Sean]], [[Drake (rapper)|Drake]], and [[Lil Wayne]] are credited with the popularization of hashtag rap, while the style has been criticized by [[Ludacris]], [[The Lonely Island]],<ref>{{cite web|url = http://www.tucsonweekly.com/TheRange/archives/2013/05/22/the-lonely-island-puts-hashtag-rap-in-its-place-looking-at-you-drake|title = The Lonely Island Puts Hashtag Rap In Its Place (Looking at You, Drake)|author = David Mendez|date = May 22, 2013 <!-- AT 11:43 AM --> |publisher = Tucson Weekly}}</ref> and various music writers.<ref>{{cite web|url = http://www.joplinglobe.com/enjoy/x1666506743/Jeremiah-Tucker-Hashtag-rap-is-2010s-lamest-trend|title = Jeremiah Tucker: Hashtag rap is 2010's lamest trend|author = Jeremiah Tucker|date = December 17, 2010|publisher = Joplin Globe}}</ref>

On September 13, 2013, a hashtag, #TwitterIPO, appeared in the headline of a ''[[The New York Times|New York Times]]'' front page article regarding Twitter's [[initial public offering]].<ref>{{cite web
| title = Twitter / nickbilton: My first byline on A1 of the …
| url = https://twitter.com/nickbilton/status/378534272962793472/photo/1
| accessdate = September 14, 2013
 }}</ref>

[[Bird's Eye]] foods released in 2014 a shaped [[mashed potato]] food that included forms of @-symbols and hashtags, called "Mashtags".<ref>{{cite web|title=Birds Eye launches Mashtags – social media potato shapes|url=http://www.thegrocer.co.uk/fmcg/birds-eye-launches-mashtags-potato-shapes/354514.article|work=The Grocer}}</ref>

In May 2014, Twitter users began using the hashtag [[YesAllWomen|#YesAllWomen]] to raise awareness about personal experiences of [[sexism]] and [[violence against women]].<ref name="Nytimes">{{cite news |last=Medina| first=Jennifer | title = Campus Killings Set Off Anguished Conversation About the Treatment of Women | work = [[The New York Times]] | accessdate = September 23, 2014 | date = May 27, 2014 | url =http://www.nytimes.com/2014/05/27/us/campus-killings-set-off-anguished-conversation-about-the-treatment-of-women.html?ref=us&_r=0 }}</ref>

In September 2014, in response to the "[[blame the victim]]" public reactions to videotaped footage of [[NFL]] player [[Ray Rice]] assaulting his then-fiancée Janay Palmer in the elevator of an [[Atlantic City]] casino, Beverly Gooden shared on Twitter her own story of [[domestic abuse]], using the hashtag #WhyIStayed, and encouraged others to share theirs.<ref>{{cite news|work=Today|title=WhyIStayed: Woman behind Ray Rice-inspired hashtag writes to past self, other abuse victims|author=Gooden, Beverly|date=September 10, 2014| url= http://www.today.com/news/whyistayed-woman-behind-ray-rice-inspired-hashtag-writes-past-self-1D80139011}}</ref><ref>{{cite news|work=The Leonard Lopate Show|authors=Lopate, Leonard & Gooden, Beverly|title=#WhyIStayed|date=September 10, 2014}}</ref>

Hashtags have been used verbally to make a humorous point in informal conversations,<ref>[http://www.macmillandictionary.com/dictionary/british/hashtag]</ref> such as "I’m hashtag confused!"<ref name=":0" /> In August 2012, British journalist Tom Meltzer reported in ''[[The Guardian]]'' about a new [[hand gesture]] that mimicked the hashtag, sometimes called the "finger hashtag", in which both hands form a [[Peace sign#The V sign|peace sign]], and then the fingers are crossed to form the symbol of a hashtag.<ref>{{cite web |url=https://www.theguardian.com/technology/shortcuts/2012/aug/01/how-to-say-hashtag-fingers |title=How to say 'hashtag' with your fingers |work=[[The Guardian]] |author=Tom Meltzer |date=August 1, 2012 |accessdate=March 20, 2014}}</ref> The emerging gesture was reported about in ''[[Wired (magazine)|Wired]]'' by [[Nimrod Kamer]],<ref>{{cite web |url=http://www.wired.co.uk/news/archive/2013-03/06/hashtags |title=Finger-Hashtags |work=[[Wired (magazine)|Wired]] |author=[[Nimrod Kamer]] |date=March 2013 |accessdate=March 20, 2014}}</ref> and during 2013, it was seen on TV as used by [[Jimmy Fallon]], and on ''[[The Colbert Report]],'' among other programs.<ref>{{cite web |url=http://www.dailydot.com/lol/finger-hashtag-jimmy-fallon-twitter/ |title=I invented finger hashtags—and I regret nothing |work=[[The Daily Dot]] |author=[[Nimrod Kamer]] |date=February 26, 2014 |accessdate=March 20, 2014}}</ref> Writing in 2015, Paola Maria Caleff considered this usage a [[fad]], but noted that people talking the way that they write was a consequence of computer-mediated communication.<ref name=":0" />

=== Adaptations ===
*Hashflags: In 2010, Twitter introduced "hashflags" during the 2010 World Cup in South Africa.<ref>{{cite web|author=|url=http://www.ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |title=Twitter Supports World Cup Fever with Hashflags |publisher=Ryanseacrest.com |date=June 11, 2010 |accessdate=August 5, 2015 |archiveurl=https://web.archive.org/web/20101129201517/http://ryanseacrest.com/2010/06/11/twitter-supports-world-cup-fever-with-hashflags/ |archivedate=November 29, 2010}}</ref> They reintroduced the feature on June 10, 2014, in time for the 2014 World Cup in Brazil,<ref>{{cite web|url=http://howto.digidefen.se/twitter/What-are-hashflags.php |title=What are Hashflags? |publisher=Howto.digidefen.se |date=June 10, 2014 |accessdate=August 25, 2014}}</ref><ref>{{cite web|author=Ben Woods |url=http://thenextweb.com/twitter/2014/06/10/twitter-brings-back-hashflags-just-time-world-cup-2014-kick/ |title=Twitter brings back hashflags just in time for World Cup 2014 kick-off |publisher=Thenextweb.com |date=June 10, 2014 |accessdate=August 25, 2014}}</ref> and then again on April 10, 2015, with UK political party logos for the 2015 UK General Election.<ref>{{cite web|title=Twitter just launched election hashflags|url=http://www.bbc.co.uk/newsbeat/article/32249518/twitter-just-launched-election-hashflags|website=BBC News|accessdate=April 15, 2015}}</ref> When a user tweets a hashtag consisting of the three letter country code of any of the 32 countries represented in the tournament, Twitter automatically embeds a flag emoticon for that country.
*Cashtags: In 2009, [[StockTwits]] used [[ticker symbol]]s preceded by the [[dollar sign]] to create "cashtags".<ref name=Wong2012>{{cite journal |author=Wong, Matthew |title=VCs and Start-Ups Pin Their Hopes on Pinterest |date=2012-08-17 |work=[[The Wall Street Journal]] |url=http://blogs.wsj.com/venturecapital/2012/08/17/vcs-and-start-ups-pin-their-hopes-on-pinterest/ |accessdate=2013-05-28 }}</ref><ref name=Taylor2012>{{cite journal |author=Taylor, Colleen |title=Howard Lindzon on Why He Sold His Twitter Stock, And The 'Hijack' Of StockTwits’ Cashtags [TCTV] |date=2012-07-01 |publisher=[[TechCrunch]] |url=http://techcrunch.com/2012/08/01/howard-lindzon-on-why-he-sold-his-twitter-stock-and-the-hijack-of-stocktwits-cashtags-tctv/ |accessdate=2013-05-09 }}</ref> In July 2012, Twitter adapted the hashtag style to make company ticker symbols preceded by the dollar sign clickable (as in [[Apple, Inc.|$AAPL]]), a method that Twitter dubbed the "cashtag".<ref>{{cite web|last=Kim |first=Erin |url=http://money.cnn.com/2012/07/31/technology/twitter-cashtag/ |title=Twitter unveils 'cashtags' to track stock symbols – Jul. 31, 2012 |publisher=Money.cnn.com |date=July 31, 2012 |accessdate=November 12, 2013}}</ref><ref>{{cite web|author= |url=http://www.theverge.com/2012/7/30/3205284/twitter-stock-ticker-cashtag-links-official |title=Twitter makes stock symbol $ 'cashtag' links official, following # and @ |publisher=The Verge |date=July 30, 2012 |accessdate=November 12, 2013}}</ref> This is intended to allow users to search posts discussing companies and their stocks. This is also used for discussion of currency fluctuations on twitter, eg. using #USDGBP or $USDGBP when mentioning the US Dollar's level expressed in Pounds Sterling.

== References ==

{{Reflist|30em}}

== External links ==

{{Commons category|Hashtags}}

* [//tools.wmflabs.org/hashtags/search/artandfeminism Wikipedia internal hashtag search engine] – for hashtags used in edit summaries

{{Microblogging}}
{{Online social networking}}
{{Web syndication}}
{{authority control}}

[[Category:Hashtags| ]]
[[Category:2010s slang]]
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Social media]]
[[Category:Web 2.0]]
[[Category:Twitter]]
<=====doc_Id=====>:673
<=====title=====>:
Probabilistic relevance model
<=====text=====>:

The '''probabilistic relevance model'''<ref>{{citation | author=S. E. Robertson and  K. S. Jones | title=Relevance weighting of search terms | publisher=Journal of the American Society for Information Science | pages=129–146 | date=May–June 1976 | url=http://portal.acm.org/citation.cfm?id=106783 }}</ref><ref name="robertson2009">{{Cite journal | author=Stephen Robertson and Hugo Zaragoza | title=The Probabilistic Relevance Framework: BM25 and Beyond | date=2009 | url=http://dl.acm.org/citation.cfm?id=1704810 | publisher=Found. Trends Inf. Retr. | volume=3 | issue=4 | pages=333-389 | doi=10.1561/1500000019 }}</ref> was devised by Robertson and Jones as a framework for [[Statistical model | probabilistic models]] to come. It is a formalism of [[information retrieval]] useful to derive [[ranking function]]s used by [[search engine]]s and  [[web search engine]]s in order to rank matching documents according to their [[Relevance (information retrieval)|relevance]] to a given search query.
 
It makes an estimation of the probability of finding if a document ''d<sub>j</sub>'' is relevant to a query ''q''. This model assumes that this probability of relevance depends on the query and document representations. Furthermore, it assumes that there is a portion of all documents that is preferred by the user as the answer set for query ''q''. Such an ideal answer set is called ''R'' and should maximize the overall probability of relevance to that user. The prediction is that documents in this set ''R'' are relevant to the query, while documents not present in the set are non-relevant.

<math>sim(d_{j},q) = \frac{P(R|\vec{d}_j)}{P(\bar{R}|\vec{d}_j)}</math>

==Related models==
There are some limitations to this framework that need to be addressed by further development:
* There is no accurate estimate for the first run probabilities
* Index terms are not weighted
* Terms are assumed mutually independent

To address these and other concerns there are some developed models from the probabilistic relevance framework. The [[Binary Independence Model]] for one, as it is from the same author. The most known derivative of this framework is the [[Probabilistic relevance model (BM25)|Okapi(BM25)]] weighting scheme and its BM25F brother.

==References==
{{reflist}}

[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]
<=====doc_Id=====>:676
<=====title=====>:
Voice search
<=====text=====>:
'''Voice search''', also called voice-enabled search, allows the user to use a voice command to search the Internet, or a portable device.  Currently, voice search is commonly used in (in a narrow sense) "directory assistance", or local search. Examples include [[Google 411]], [[Tellme]] directory assistance and [[Yellowpages.com]]'s 1-800-YellowPages. 

In a broader definition, voice search include open-domain keyword query on any information on the Internet, for example in [[Google Voice Search]], [[Cortana (software)|Cortana]], [[Siri]] and [[Amazon Echo]]. Given that voice-based systems are interactive, such systems are also called open-domain [[question answering]] systems. 

Voice search is often interactive, involving several rounds of interaction that allows a system to ask for clarification. Voice search is a type of [[Dialog systems|dialog system]].

== References ==
{{No footnotes|date=April 2009}}
*Ye-Yi Wang, Dong Yu, Yun-Cheng Ju, Alex Acero, An Introduction to Voice Search, IEEE Signal Processing Magazine (Special Issue on Spoken Language Technology), Institute of Electrical and Electronics Engineers, Inc., May 2008
*J. Sherwani, Dong Yu, Tim Paek, Mary Czerwinski, Yun-Cheng Ju, and Alex Acero. 'VoicePedia: Towards Speech-Based Access to Unstructured Information', Proceedings of the 8th Annual Conference of the International Communication Association (Interspeech 2007). Antwerp, Belgium, August, 2007
{{Internet search}}

[[Category:Information retrieval genres]]
<=====doc_Id=====>:679
<=====title=====>:
Desktop search
<=====text=====>:
{{Multiple issues|
{{technical|date=October 2014}}
{{manual||date=October 2016}}
}}
[[File:AdunaAutoFocus5.png|thumb|OSL Desktop Search engines software Aduna AutoFocus 5]]
'''Desktop search''' tools search within a user's own [[computer files]] as opposed to searching the Internet. These tools are designed to find information on the user's PC, including web browser history, e-mail archives, text documents, sound files, images, and video.

One of the main advantages of desktop search programs is that search results are displayed quickly due to the use of proper indexes.

A variety of desktop search programs are now available; see [[List of search engines#Desktop search engines|this list]] for examples.  Most desktop search programs are standalone applications, whereas a few also provide search capabilities in an [[integrated writing environment]] (IWE).

Desktop search emerged as a concern for large firms for two main reasons: untapped productivity and security. On the one hand, users need to be able to quickly find relevant files, but on the other hand, they shouldn't have access to restricted files. According to analyst firm Gartner, up to 80% of some companies' data is locked up inside [[unstructured data]] — the information stored on an end user's PC, the directories (folders) and files they've created on a [[Computer network|network]], documents stored in repositories such as corporate [[intranet]]s and a multitude of other locations.<ref>{{Citation | url = http://www.computerweekly.com/Articles/2006/04/25/215622/security-special-report-who-sees-your-data.htm | title = Security special report: Who sees your data? | newspaper = Computer Weekly | date = 2006-04-25}}.</ref>  Moreover, many companies have structured or unstructured information stored in older [[file formats]] to which they don't have ready access.

Companies doing business in the [[United States]] are frequently required under regulatory mandates like [[Sarbanes-Oxley]], [[Health Insurance Portability and Accountability Act|HIPAA]] and [[FERPA]] to make sure that access to sensitive information is 100% controlled. This creates a challenge for IT organizations, which may not have a desktop search standard, or lack strict central control over end users [[downloading]] tools from the [[Internet]]. Some consumer-oriented desktop search tools make it possible to generate indexes outside the corporate [[Firewall (computing)|firewall]] and share those indexes with unauthorized users. In some cases, end users are able to index — but not preview — items they should not even know exist.{{Citation needed|date = November 2009}}

Historically, full desktop search comes from the work of [[Apple inc.|Apple Computer's]] [[Apple Advanced Technology Group|Advanced Technology Group]], resulting in the underlying [[AppleSearch]] technology in the early 1990s. It was used to build the [[Sherlock (software)|Sherlock]] search engine and then developed into [[Spotlight (software)|Spotlight]], which brought automated, non-timer-based full indexing into the operating system.

== Technologies ==
Most desktop search engines build and maintain an [[Index (search engine)|index database]] to achieve reasonable performance when searching several [[gigabyte]]s of [[data]]. Indexing usually takes place when the computer is idle and most search applications can be set to suspend indexing if a portable computer is running on batteries, in order to save power. There are notable exceptions, however: Voidtools' Everything Search Engine,<ref>{{cite web|title=Everything Search Engine|url=http://www.voidtools.com/|publisher=voidtools|accessdate=27 December 2013}}</ref> which performs searches over only filenames &mdash; not the files' contents &mdash; for NTFS volumes only, is able to build its index from scratch in just a few seconds. Another exception is Vegnos Desktop Search Engine,<ref>{{cite web|title=Vegnos|url=http://www.vegnos.com|publisher=Vegnos|accessdate=27 December 2013}}</ref> which performs searches over filenames and files' contents without building any indices. The benefits to not having indices is that, in addition to not requiring persistent storage, more powerful queries (e.g., [[regular expressions]]) can be issued, whereas indexed search engines are limited to keyword-based queries. An index may also not be up-to-date, when a query is performed. In this case, results returned will not be accurate (that is, a hit may be shown when it is no longer there, and a file may not be shown, when in fact it is a hit). Some products have sought to remedy this disadvantage by building a real-time indexing function into the software. There are disadvantages to not indexing. Namely, the time to complete a query can be significant, and the issued query can also be resource-intensive.

Desktop search tools typically collect three types of information about files:
* file and folder names
* [[metadata]], such as titles, authors, comments in file types such as [[MP3]], [[Portable Document Format|PDF]] and [[JPEG]]
* file content (for supported types of documents only)

To search effectively within documents, the tools need to be able to parse many different types of documents. This is achieved by using filters that interpret selected file formats. For example, a ''Microsoft Office Filter'' might be used to search inside [[Microsoft Office]] documents.

Long-term goals for desktop search include the ability to search the contents of image files, sound files and video by context.<ref>{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/video-search.html|title=The current state of video search|author=Niall Kennedy|date=17 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}</ref><ref>{{cite web|url=http://www.niallkennedy.com/blog/archives/2006/10/audio-search.html|title=The current state of audio search|author=Niall Kennedy|date=15 October 2006|work=Niall Kennedy|accessdate=24 June 2015}}</ref>

The sector attracted considerable attention from the struggle between Microsoft and Google.<ref>{{cite web|url=http://news.bbc.co.uk/1/hi/technology/3952285.stm|title=BBC NEWS - Technology - Search wars hit desktop computers|work=bbc.co.uk|accessdate=24 June 2015}}</ref> According to market analysts, both companies were attempting to leverage their monopolies (of [[web browser]]s and [[search engine]]s, respectively) to strengthen their dominance. Due to [[Google]]'s complaint that users of Windows Vista cannot choose any competitor's desktop search program over the built-in one, an agreement was reached between [[US Justice Department]] and [[Microsoft]] that [[Windows Vista Service Pack 1]] would enable users to choose between the built-in and other desktop search programs, and select which one is to be the default.<ref>{{cite web|url=http://goebelgroup.com/searchtoolblog/2007/06/20/microsoft-agrees-to-change-vista-desktop-search-tool/|title=SearchMax|work=goebelgroup.com|accessdate=24 June 2015}}</ref>

As of September, 2011, Google ended life for Google Desktop, a program designed to make it easy for users to search their own PCs for emails, files, music, photos, Web pages and more.<ref>[http://googledesktop.blogspot.com/2011/09/google-desktop-update.html/ "Google Desktop Update" (Sept 2011)]</ref>

Desktop search products are software alternatives to Windows Desktop and Outlook Search, helping business professional sift through desktop files, emails, attachments, SharePoint data, and more.,<ref>[http://www.brianmadden.com/blogs/brianmadden/archive/2015/03/11/what-do-you-do-for-desktop-search-in-vdi-and-rdsh.aspx  „What do you do for desktop search in VDI and RDSH?“]. Blogpost by Brian Madden on brainmadden.com. Retrieved on March 25, 2015.</ref><ref>{{cite web|url=http://venturebeat.com/2008/06/02/lookeen-offers-a-new-way-way-for-outlook-users-to-search/|title=Lookeen offers a new way for Outlook users to search|author=Anthony Ha|date=2 June 2008|work=VentureBeat|accessdate=8 March 2016}}</ref><ref>{{cite web|url=http://www.computerworld.com/article/2475293/desktop-apps/x1-rises-again-with-desktop-search-8--virtual-edition.html/|title=X1 rises again with Desktop Search 8, Virtual Edition|author=Robert L. Mitchell|date=8 May 2013|work=Computerworld|accessdate=24 June 2015}}</ref>

==Platforms & their histories==
There are three main platforms that desktop search falls into. [[Microsoft Windows|Windows]], [[Mac OS|Mac]] OS & [[Linux]]. This article will focus on the history of these search platforms, the features they had, and how those features evolved.

=== Windows ===
Today's [[Windows Search]] replaced WDS ([[Windows Desktop Search]]). WDS, in turn, replaced [[Indexing Service]]. A "a base service that extracts content from files and constructs an indexed catalog to facilitate efficient and rapid searching"<ref>{{cite web|url=https://msdn.microsoft.com/en-us/library/ee805985%28v=vs.85%29.aspx|title=Indexing Service|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}</ref> Indexing service was originally released in August 1996, it was built in order to speed up manually searching for files on Personal Desktops and Corporate Computer Network. Indexing service helped by using Microsoft web servers to index files on the desired hard drives. Indexing was done by file format. By using terms that users provided, a search was conducted that matched terms to the data within the file formats. The largest issue that Indexing service faced was the fact that every time a file was added, it had to be indexed. This coupled with the fact that the indexing cached the entire index in RAM, made the hardware a huge limitation.<ref>{{cite web|url=https://msdn.microsoft.com/en-us/library/dd582937%28v=office.11%29.aspx|title=Indexing with Microsoft Index Server|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}</ref> This made indexing large amounts of files require extremely powerful hardware and very long wait times.

In 2003, Windows Desktop Search (WDS) replaced Microsoft Indexing Service. Instead of only matching terms to the details of the file format and file names, WDS brings in content indexing to all Microsoft files and text-based formats such as e-mail and text files. This means, that WDS looked into the files and indexed the content. Thus, when a user searched a term, WDS no longer matched just information such as file format types and file names, but terms, and values stored within those files. WDS also brought "Instant searching" meaning the user could type a character and the query would instantly start searching and updating the query as the user typed in more characters.<ref>{{cite web|url=http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|archiveurl=https://web.archive.org/web/20110924212903/http://www.microsoft.com/windows/products/winfamily/desktopsearch/technicalresources/techfaq.mspx|title=Windows Search: Technical FAQ|archivedate=24 September 2011|publisher=Microsoft|work=microsoft.com|accessdate=24 June 2015}}</ref> Windows Search apparently used up a lot of processing power, as Windows Desktop Search would only run if it was directly queried or while the PC was idle. Even only running while directly queried or while the computer was idled, indexing the entire hard drive still took hours. The index would be around 10% of the size of all the files that it indexed. For example, if the indexed files amounted to around 100GB of space, the index would, itself, be 10GB large.

With the release of [[Windows Vista]] came Windows Search 3.1. Unlike its predecessors WDS and Windows Search 3.0, 3.1 could search through both indexed and non indexed locations seamlessly. Also, the [[RAM]] and [[CPU]] requirements were greatly reduced, cutting back indexing times immensely. Windows Search 4.0 is currently running on all PCs with [[Windows 7]] and up.

=== Mac OS ===
Mac OS was the first to implement Desktop Search with its [[AppleSearch]] search engine, allowing users to fully search all documents within their Macintosh computer, including file format types, meta-data on those files, and content within the files. AppleSearch was a [[Client–server model|client/server application]], and as such required a server separate from the main device in order to function. The biggest issue with AppleSearch were its large resource requirements: "AppleSearch requires at least a 68040 processor and 5MB of RAM."<ref>{{cite web|url=http://infomotions.com/musings/tricks/manuscript/1600-0001.html|title=AppleSearch|work=infomotions.com|accessdate=24 June 2015}}</ref> At the time, a Macintosh computer with these specifications was priced at approximately $1400; equivalent to $2050 in 2015.<ref>{{cite web|url=http://stats.areppim.com/calc/calc_usdlrxdeflator.php|title=Converter of current to real US dollars - using the GDP deflator|author=eduardo casais|work=areppim.com|accessdate=24 June 2015}}</ref> On top of this, the software itself cost an additional $1400 for a single license.

In 1997, [[Sherlock (software)|Sherlock]] was released alongside Mac OS 8.5. Sherlock (named after the famous fictional detective [[Sherlock Holmes]]) was integrated into Mac OS's file browser&nbsp;– [[Finder (software)|Finder]]. Sherlock extended the desktop search function to the World Wide Web, allowing users to search both locally and externally. Adding additional functions—such as internet access—to Sherlock was relatively simple, as this was done through plugins written as plain text files. Sherlock was included in every release of Mac OS from [[Mac OS 8]], before being deprecated and replaced by [[Spotlight (software)|Spotlight]] and [[Dashboard (Mac OS)|Dashboard]] in [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It was officially removed in [[Mac OS X Leopard|Mac OS X 10.5 Leopard]]

[[Spotlight (software)|Spotlight]] was released in 2005 as part of [[Mac OS X Tiger|Mac OS X 10.4 Tiger]]. It is a Selection-based search tool, which means the user invokes a query using only the mouse. Spotlight allows the user to search the Internet for more information about any keyword or phrase contained within a document or webpage, and uses a built-in calculator and Oxford American Dictionary to offer quick access to small calculations and word definitions.<ref>{{cite web|url=http://www.apple.com/pr/library/2005/04/12Apple-to-Ship-Mac-OS-X-Tiger-on-April-29.html|title=Apple - Press Info - Apple to Ship Mac OS X "Tiger" on April 29|work=apple.com|accessdate=24 June 2015}}</ref> While Spotlight initially has a long startup time, this decreases as the hard disk is indexed. As files are added by the user, the index is constantly updated in the background using minimal CPU & RAM resources.

=== Linux ===
There are a wide range of desktop search options for Linux users, depending upon the skill level of the user, their preference to use desktop tools which tightly integrate into their desktop environment, command-shell functionality (often with advanced scripting options), or browser-based users interfaces to locally running software.  In addition, many users create their own indexing from a variety of indexing packages (e.g. one which does extraction and indexing of PDF/DOC/DOCX/[[OpenDocument|ODT]] documents well, another search engine which works w/ vcard, LDAP, and other directory/contact databases, as well as the conventional <tt>find</tt> and <tt>locate</tt> commands.

====Ubuntu====
The [[Ubuntu distribution]] is a popular version of Linux. Strangely enough, Ubuntu didn't have desktop search until Feisty Fawn 7.04. Using [[Tracker (search software)|Tracker]]<ref>{{cite web|url=http://arstechnica.com/information-technology/2007/07/afirst-look-at-tracker-0-6-0/|title=A first look at Tracker 0.6.0|work=Ars Technica|accessdate=24 June 2015}}</ref> desktop search, the desktop search feature was very similar to Mac OS's AppleSearch and Sherlock. Considering the fact that both are UNIX-based systems. Tracker, released in late 2007, was built to have a relatively low impact on system resources. But unfortunately occasionally had sporadic control over what resources it was using. It not only featured the basic features of file format sorting and meta-data matching, but support for searching through emails and instant messages was added. Years later, in 2014 [[Recoll]]<ref>{{cite web|url=http://www.lesbonscomptes.com/recoll/usermanual/index.html#RCL.INDEXING|title=Recoll user manual|work=lesbonscomptes.com|accessdate=24 June 2015}}</ref> was added to Linux distributions, it works with other search programs such as Tracker and [[Beagle (software)|Beagle]] to provide efficient full text search. This greatly increased the types of queries that Linux desktop searches could handle as well as file types. A major advantage of Recoll is that it allows for greater customization of what is indexed. For example, Recoll will index the entire hard disk by default, but will and can index just a few select directories instead of wasting time indexing directories you know you will never need to look at. It also allows for more search options, you may actually narrow down what kind of query you want to ask. For example, you could search for just file types or by content.<ref>{{cite web|url=http://archive09.linux.com/feature/114283|title=Linux.com|work=linux.com|accessdate=24 June 2015}}</ref>

====[[openSUSE]]<ref>http://www.opensuse.org/</ref>====
<!--TODO! Prior desktop search before KDE 3.5-->
Starting with [[KDE4]], the [[NEPOMUK (software)|NEPOMUK]] was introduced.  It provided the ability to index a wide range of desktop content, email, and use semantic web technologies (e.g. [[Resource Description Framework|RDF]]) to annotate the database.  The introduction faced a few glitches, much of which seemed to be based on the [[triplestore]].  Performance improved (at least for queries) by switching the backend to a stripped own version of the [[Virtuoso]] Open Source Edition, however indexing remained a common user complaint.  
Based on user feedback, the Nepomuk indexing and search has been replaced with the Baloo framework<ref>https://community.kde.org/Baloo</ref> based on [[Xapian]].

==See also==
*[[List of search engines#Desktop search engines|List of desktop search engines]]

== References ==
{{reflist|2}}

{{Navigationbox Desktopsearch}}

{{DEFAULTSORT:Desktop Search}}
[[Category:Desktop search engines| ]]
[[Category:Information retrieval genres]]
<=====doc_Id=====>:682
<=====title=====>:
Multimedia search
<=====text=====>:
'''Multimedia search''' enables information [[Search engine technology|search]] using queries in multiple data types including text and other [[multimedia]] formats.
Multimedia search can be implemented through [[multimodal search]] interfaces, i.e., interfaces that allow to submit [[search queries]] not only as textual requests, but also through other media.
We can distinguish two methodologies in multimedia search:
*'''Metadata search''': the search is made on the layers of [[metadata]].
* '''[[Query by example]]''': The interaction consists in submitting a piece of information (e.g., a video, an image, or a piece of audio) at the purpose of finding similar multimedia items.

==Metadata search==

Search is made using the layers in metadata which contain information of the content of a multimedia file. Metadata search is easier, faster and effective because instead of working with complex material, such as an audio, a video or an image, it searches using text.

There are three processes which should be done in this method:
*'''[[Multimedia information retrieval#Feature extraction methods|Summarization of media content]]''' ([[feature extraction]]). The result of feature extraction is a description.
*'''[[Multimedia information retrieval#Feature extraction methods|Filtering of media descriptions]]''' (for example, elimination of [[Redundancy (linguistics)|Redundancy]])
*'''[[Multimedia information retrieval#Categorization methods|Categorization of media descriptions]]''' into classes.

==Query by example==

In [[query by example]], the element used to search is a [[multimedia]] content (image, audio, video). In other words, the query is a media. Often, it's used [[Search engine indexing|audiovisual indexing]]. It will be necessary to choose the criteria we are going to use for creating metadata. The process of search can be divided in three parts:
*Generate descriptors for the media which we are going to use as query and the descriptors for the media in our [[database]].
*Compare descriptors of the query and our database’s media.
*List the media sorted by maximum coincidence.

==Multimedia search engine==
There are two big search families, in function of the content:
* [[Visual search engine]]
*[[Audio search engine]]

===Visual search engine===
Inside this family we can distinguish two topics: [[image search]] and [[video search]]

*'''[[Image search]]''': Although usually it's used simple metadata search, increasingly is being used indexing methods for making the results of users queries more accurate using [[query by example]]. For example, [[QR codes]].
*'''[[Video search]]''': Videos can be searched for simple metadata or by complex metadata generated by indexing. The audio contained in the videos is usually scanned by audio search engines.

===Audio search engine===
There are different methods of [[Audio search engine|audio searching]]:
*Voice search engine: Allows the user to search using speech instead of text. It uses algorithms of [[speech recognition]]. An example of this technology is [[Google Voice Search]].
*Music search engine: Although most of applications which searches music works on simple metadata (artist, name of track, album…) . There are some programs of [[music recognition]], for example [[Shazam (service)|Shazam]] or [[SoundHound]].

==See also==
*''[[Journal of Multimedia]]''
*[[List of search engines#Multimedia|List of search engines]]
*[[Multimedia]]
*[[Multimedia information retrieval]]
*[[Search engine indexing]]
*[[Streaming media]]
*[[Video search engine]]

==External links==

[[Category:Information retrieval genres]]
[[Category:Multimedia]]
<=====doc_Id=====>:685
<=====title=====>:
Category:Personalized search
<=====text=====>:
{{catmore}}

[[Category:Information retrieval genres]]
[[Category:Internet search engines]]
<=====doc_Id=====>:688
<=====title=====>:
Legal information retrieval
<=====text=====>:
'''Legal information retrieval''' is the science of [[information retrieval]] applied to legal text, including [[legislation]], [[case law]], and scholarly works.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 1</ref> Accurate legal information retrieval is important to provide access to the law to laymen and legal professionals. Its importance has increased because of the vast and quickly increasing amount of legal documents available through electronic means.<ref name=Jackson>Jackson et al., p. 60</ref> Legal information retrieval is a part of the growing field of [[legal informatics]].

== Overview ==

In a legal setting, it is frequently important to retrieve all information related to a specific query. However, commonly used [[boolean search]] methods (exact matches of specified terms) on full text legal documents have been shown to have an average [[recall rate]] as low as 20 percent,<ref name="Blair, D.C. 1985, p.293">Blair, D.C., and Maron, M.E., 1985, p.293</ref> meaning that only 1 in 5 relevant documents are actually retrieved. In that case, researchers believed that they had retrieved over 75% of relevant documents.<ref name="Blair, D.C. 1985, p.293"/> This may result in failing to retrieve important or [[precedential]] cases. In some jurisdictions this may be especially problematic, as legal professionals are [[legal ethics|ethically]] obligated to be reasonably informed as to relevant legal documents.<ref>American Bar Association, Model Rules of Professional Conduct Rule 1.1, http://www.abanet.org/cpr/mrpc/rule_1_1.html</ref>

Legal Information Retrieval attempts to increase the effectiveness of legal searches by increasing the number of relevant documents (providing a high [[recall rate]]) and reducing the number of irrelevant documents (a high [[precision rate]]). This is a difficult task, as the legal field is prone to [[jargon]],<ref>Peters, W. et al. 2007, p. 118</ref> [[polysemes]]<ref>Peters, W. et al. 2007, p. 130</ref> (words that have different meanings when used in a legal context), and constant change.

Techniques used to achieve these goals generally fall into three categories: [[boolean search|boolean]] retrieval, manual classification of legal text, and [[natural language processing]] of legal text.

== Problems ==

Application of standard [[information retrieval]] techniques to legal text can be more difficult than application in other subjects. One key problem is that the law rarely has an inherent [[Taxonomy (general)|taxonomy]].<ref name=LOIS1>Peters, W. et al. 2007, p. 120</ref> Instead, the law is generally filled with open-ended terms, which may change over time.<ref name=LOIS1 /> This can be especially true in [[common law]] countries, where each decided case can subtly change the meaning of a certain word or phrase.<ref>Saravanan, M. et al.  2009, p. 101</ref>

Legal information systems must also be programmed to deal with law-specific words and phrases. Though this is less problematic in the context of words which exist solely in law, legal texts also frequently use polysemes, words may have different meanings when used in a legal or common-speech manner, potentially both within the same document. The legal meanings may be dependent on the area of law in which it is applied. For example, in the context of European Union legislation, the term "worker" has four different meanings:<ref name="Peters, W. et al. 2007, p. 131">Peters, W. et al. 2007, p. 131</ref>

#Any worker as defined in Article 3(a) of [[Directive 89/391/EEC]] who habitually uses display screen equipment as a significant part of his normal work.
#Any person employed by an employer, including trainees and apprentices but excluding domestic servants;
#Any person carrying out an occupation on board a vessel, including trainees and apprentices, but excluding port pilots and shore personnel carrying out work on board a vessel at the quayside;
#Any person who, in the Member State concerned, is protected as an employee under national employment law and in accordance with national practice;

In addition, it also has the common meaning: 
<ol start="5">
<li>A person who works at a specific occupation.<ref name="Peters, W. et al. 2007, p. 131"/> </li>
</ol>

Though the terms may be similar, correct information retrieval must differentiate between the intended use and irrelevant uses in order to return the correct results.

Even if a system overcomes the language problems inherent in law, it must still determine the relevancy of each result. In the context of judicial decisions, this requires determining the precedential value of the case.<ref name=MaxwellA >Maxwell, K.T., and Schafer, B. 2008, p. 8</ref> Case decisions from senior or [[superior court]]s may be more relevant than those from [[lower court]]s, even where the lower court's decision contains more discussion of the relevant facts.<ref name=MaxwellA  /> The opposite may be true, however, if the senior court has only a minor discussion of the topic (for example, if it is a secondary consideration in the case).<ref name=MaxwellA  /> A information retrieval system must also be aware of the authority of the jurisdiction. A case from a binding authority is most likely of more value than one from a non-binding authority.

Additionally, the intentions of the user may determine which cases they find valuable. For instance, where a legal professional is attempting to argue a specific interpretation of law, he might find a minor court's decision which supports his position more valuable than a senior courts position which does not.<ref name=MaxwellA  /> He may also value similar positions from different areas of law, different jurisdictions, or dissenting opinions.<ref name=MaxwellA />

Overcoming these problems can be made more difficult because of the large number of cases available. The number of legal cases available via electronic means is constantly increasing (in 2003, US appellate courts handed down approximately 500 new cases per day<ref name=Jackson />), meaning that an accurate legal information retrieval system must incorporate methods of both sorting past data and managing new data.<ref name=Jackson /><ref>Maxwell, K.T., and Schafer, B. 2007, p.1</ref>

== Techniques ==

===Boolean searches===

[[Boolean search]]es, where a user may specify terms such as use of specific words or judgments by a specific court, are the most common type of search available via legal information retrieval systems. They are widely implemented by services such as [[Westlaw]], [[LexisNexis]], and [[Findlaw]].  However, they overcome few of the problems discussed above.

The recall and precision rates of these searches vary depending on the implementation and searches analyzed. One study found a basic boolean search's [[recall rate]] to be roughly 20%, and its precision rate to be roughly 79%.<ref name="Blair, D.C. 1985, p.293"/> Another study implemented a generic search (that is, not designed for legal uses) and found a recall rate of 56% and a precision rate of 72% among legal professionals. Both numbers increased when searches were run by non-legal professionals, to a 68% recall rate and 77% precision rate. This is likely explained because of the use of complex legal terms by the legal professionals.<ref>Saravanan M., et al. 2009, p. 116</ref>

===Manual classification===

In order to overcome the limits of basic boolean searches, information systems have attempted to classify case laws and statutes into more computer friendly structures. Usually, this results in the creation of an [[ontology]] to classify the texts, based on the way a legal professional might think about them.<ref name="Maxwell, K.T. 2008, p. 2">Maxwell, K.T., and Schafer, B. 2008, p. 2</ref> These attempt to link texts on the basis of their type, their value, and/or their topic areas. Most major legal search providers now implement some sort of classification search, such as [[Westlaw]]'s “Natural Language”<ref name=WL>Westlaw Research, http://www.westlaw.com</ref> or [[LexisNexis]]' Headnote<ref name=LN>Lexis Research, http://www.lexisnexis.com</ref> searches. Additionally, both of these services allow browsing of their classifications, via Westlaw's West Key Numbers<ref name=WL /> or Lexis' Headnotes.<ref name=LN /> Though these two search algorithms are proprietary and secret, it is known that they employ manual classification of text (though this may be computer-assisted).<ref name="Maxwell, K.T. 2008, p. 2"/>

These systems can help overcome the majority of problems inherent in legal information retrieval systems, in that manual classification has the greatest chances of identifying landmark cases and understanding the issues that arise in the text.<ref name="Maxwell, K.T. 2008, p. 3">Maxwell, K.T., and Schafer, B. 2008, p. 3</ref> In one study, ontological searching resulted in a precision rate of 82% and a recall rate of 97% among legal professionals.<ref>Saravanan, M. et al.  2009, p. 116</ref> The legal texts included, however, were carefully controlled to just a few areas of law in a specific jurisdiction.<ref>Saravanan, M. et al. 2009, p. 103</ref>

The major drawback to this approach is the requirement of using highly skilled legal professionals and large amounts of time to classify texts.<ref name="Maxwell, K.T. 2008, p. 3"/><ref>Schweighofer, E. and Liebwald, D. 2008, p. 108</ref> As the amount of text available continues to increase, some have stated their belief that manual classification is unsustainable.<ref>Maxwell, K.T., and Schafer, B. 2008, p. 4</ref>

===Natural language processing===

In order to reduce the reliance on legal professionals and the amount of time needed, efforts have been made to create a system to automatically classify legal text and queries.<ref name=Jackson /><ref name=AshleyA>Ashley, K.D. and Bruninghaus, S. 2009, p. 125</ref><ref name=Gelbart>Gelbart, D. and Smith, J.C. 1993, p. 142</ref> Adequate translation of both would allow accurate information retrieval without the high cost of human classification. These automatic systems generally employ [[Natural Language Processing]] (NLP) techniques that are adapted to the legal domain, and also require the creation of a legal [[ontology]]. Though multiple systems have been postulated,<ref name=Jackson /><ref name=AshleyA /><ref name=Gelbart /> few have reported results. One system, “SMILE,” which attempted to automatically extract classifications from case texts, resulted in an [[f-measure]] (which is a calculation of both recall rate and precision) of under 0.3 (compared to perfect f-measure of 1.0).<ref name=AshleyB >Ashley, K.D. and Bruninghaus, S. 2009, p. 159</ref> This is probably much lower than an acceptable rate for general usage.<ref name=AshleyB /><ref>Maxwell, K.T., and Schafer, B. 2009, p. 3</ref>

Despite the limited results, many theorists predict that the evolution of such systems will eventually replace manual classification systems.<ref>Maxwell, K.T., and Schafer, B. 2009, p. 9</ref><ref>Ashley, K.D. and Bruninghaus, S. 2009, p. 126</ref>

== List of retrieval systems ==
Free-to-use law-texts and associated oficial metadata:

* [[LexML Brazil]]
* [http://www.legislation.gov.uk/ legislation.gov.uk]
* [[EUR-Lex#N-Lex|N-Lex]]
* ...

== Notes ==
{{Reflist|2}}

==References==
{{Refbegin}}
*{{cite journal
|author1=Maxwell, K.T. |author2=Schafer, B.
|year       = 2008
|title      = Concept and Context in Legal Information Retrieval
|url        = http://portal.acm.org/citation.cfm?id=1564016
|journal    = Frontiers in Artificial Intelligence and Applications
|volume     = 189
|pages      = 63–72
|publisher  = IOS Press
|accessdate = 2009-11-07
}}
*{{cite journal
|author     = Jackson, P.|year       = 1998
|title      = Information extraction from case law and retrieval of prior cases by partial parsing and query generation
|url        = http://portal.acm.org/citation.cfm?id=288627.288642
|journal    = Conference on Information and Knowledge Management
|pages      = 60–67
|publisher  = ACM
|accessdate = 2009-11-07
|display-authors=etal}}
*{{cite journal
|author1=Blair, D.C. |author2=Maron, M.E.
|year       = 1985
|title      = An evaluation of retrieval effectiveness for a full-text document-retrieval
|url        = http://portal.acm.org/citation.cfm?id=3166.3197&coll=GUIDE&dl=GUIDE&CFID=61732097&CFTOKEN=95519997
|journal    = Communications of the ACM
|volume     = 28
|issue      = 3 
|pages      = 289–299
|publisher  = ACM
|accessdate = 2009-11-07
|doi=10.1145/3166.3197
}}
*{{cite journal
|author     = Peters, W.|year       = 2007
|title      = The structuring of legal knowledge in LOIS
|url        = http://www.springerlink.com/content/d04l7h2507700g45/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 117–135
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9034-4
|display-authors=etal}}
*{{cite journal
|author     = Saravanan, M.|year       = 2007
|title      = Improving legal information retrieval using an ontological framework 
|url        = http://www.springerlink.com/content/h66412k08h855626/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 101–124
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9075-y
|display-authors=etal}}
*{{cite journal
|author1=Schweighofer, E.  |author2=Liebwald, D.
|year       = 2007
|title      = Advanced lexical ontologies and hybrid knowledge based systems: First steps to a dynamic legal electronic commentary
|url        = http://www.springerlink.com/content/v62v7131x10413v0/
|journal    = Artificial Intelligence and Law
|volume     = 15
|issue      = 2
|pages      = 103–115
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-007-9029-1
}}
*{{cite journal
|author1=Gelbart, D.  |author2=Smith, J.C.
|year       = 1993
|title      = FLEXICON: an evaluation of a statistical ranking model adapted to intelligent legal text management
|url        = http://portal.acm.org/citation.cfm?id=158994
|journal    = International Conference on Artificial Intelligence and Law
|pages      = 142–151
|publisher  = ACM
|accessdate = 2009-11-07
}}
*{{cite journal
|author1=Ashley, K.D.  |author2=Bruninghaus, S.
|year       = 2009
|title      = Automatically classifying case texts and predicting outcomes
|url        = http://www.springerlink.com/content/lhg8837331hgu024/
|journal    = Artificial Intelligence and Law
|volume     = 17
|issue      = 2
|pages      = 125–165
|publisher  = Springer Netherlands
|accessdate = 2009-11-07
|doi=10.1007/s10506-009-9077-9
}}
{{Refend}}

{{DEFAULTSORT:Legal Information Retrieval}}
[[Category:Information retrieval genres]]
[[Category:Natural language processing]]
[[Category:Legal research]]
<=====doc_Id=====>:691
<=====title=====>:
Cognitive models of information retrieval
<=====text=====>:
{{Orphan|date=September 2012}}

'''Cognitive models of information retrieval''' rest on the mix of areas such as [[cognitive science]], [[human-computer interaction]], [[information retrieval]], and  [[library science]]. They describe the relationship between a person's cognitive model of the information sought and the organization of this information in an information system.  These models attempt to understand how a person is searching for information so that the database and the search of this database can be designed in such a way as to best serve the user. [[Information retrieval]] may incorporate multiple tasks and cognitive problems, particularly because different people may have different methods for attempting to find this information and expect the information to be in different forms.  Cognitive models of information retrieval may be attempts at something as apparently prosaic as improving search results or may be something more complex, such as attempting to create a database which can be queried with natural language search.

==Berrypicking==
One way of understanding how users search for information has been described by [[Marcia Bates]]<ref>[[Marcia Bates]] (1989). "The Design of Browsing and Berrypicking Techniques for the Online Search Interface." https://pages.gseis.ucla.edu/faculty/bates/berrypicking.html</ref> at the [[University of California at Los Angeles]]. Bates argues that "berrypicking" better reflects how users search for information than previous models of information retrieval.  This may be because previous models were strictly linear and did not incorporate cognitive questions.  For instance, one typical model is of a simple linear match between a query and a document.  However, Bates points out that there are simple modifications that can be made to this process.  For instance, Salton has argued that user feedback may help improve the search results.<ref>[[Gerard Salton]] (1968). ''Automatic Information and Retrieval'' (Computer Science). Dubuque, Iowa: Mcgraw-Hill Inc.</ref>

Bates argues that searches are evolving and occur bit by bit.  That is to say, a person constantly changes his or her search terms in response to the results returned from the information retrieval system.  Thus, a simple linear model does not capture the nature of information retrieval because the very act of searching causes feedback which causes the user to modify his or her [[cognitive model]] of the information being searched for.  In addition, information retrieval can be bit by bit.  Bates gives a number of examples.  For instance, a user may look through footnotes and follow these sources.  Or, a user may scan through recent journal articles on the topic.  In each case, the user's question may change and thus the search evolves.

==Exploratory Search==
Researchers in the areas of [[human-computer interaction]] and [[cognitive science]] focus on how people explore for information when interacting with the WWW. This kind of search, sometimes called [[exploratory search]], focuses on how people iteratively refine their search activities and update their internal representations of the search problems.<ref>Qu, Yan & Furnas, George. "Model-driven formative evaluation of exploratory search: A study under a sensemaking framework"</ref> Existing search engines were designed based on traditional library science theories related to retrieval basic facts and simple information through an interface. However, exploratory information retrieval often involves ill-defined search goals and evolving criteria for evaluation of relevance. The interactions between humans and the information system will therefore involve more cognitive activity, and systems that support exploratory search will therefore need to take into account the cognitive complexities involved during the dynamic information retrieval process.

==Natural language searching==

Another way in which cognitive models of information may help in information retrieval is with natural language searching.  For instance, How Stuff Works imagines a world in which, rather than searching for local movies, reading the reviews, then searching for local Mexican restaurants, and reading their reviews, you will simply type ""I want to see a funny movie and then eat at a good Mexican restaurant. What are my options?" into your browser, and you will receive a useful and relevant response.<ref>Strickland, J. (n.d.). HowStuffWorks "How Web 3.0 Will Work". Howstuffworks "Computer". Retrieved November 4, 2009, from http://computer.howstuffworks.com/web-30.htm</ref>  Although such a thing is not possible today, it represents a holy grail for researchers into cognitive models of information retrieval.  The goal is to somehow program information retrieval programs to respond to natural language searches.  This would require a fuller understanding of how people structure queries.

==Notes==
{{Reflist}}

[[Category:Information retrieval genres]]
[[Category:Cognitive modeling]]
<=====doc_Id=====>:694
<=====title=====>:
Noisy text analytics
<=====text=====>:
{{multiple issues|
{{COI|date=December 2015}}
{{notability|date=December 2015}}
{{Orphan|date=June 2016}}
}}
'''Noisy text analytics''' is a process of [[information extraction]] whose goal is to automatically extract structured or semistructured information from [[noisy text|noisy unstructured text data]]. While [[Text analytics]] is a growing and mature field that has great value because of the huge amounts of data being produced, processing of noisy text is gaining in importance because a lot of common applications produce noisy text data. Noisy unstructured text data is found in informal settings such as [[online chat]], [[Text messaging|text messages]], [[e-mail]]s, [[message boards]], [[newsgroups]], [[blogs]], [[wikis]] and [[web pages]]. Also, text produced by processing spontaneous speech using [[automatic speech recognition]] and printed or handwritten text using [[optical character recognition]] contains processing noise. Text produced under such circumstances is typically highly noisy containing spelling errors, [[abbreviation]]s, non-standard words, false starts, repetitions, missing [[punctuation]]s, missing [[letter case]] information, pause filling words such as “um” and “uh” and other texting and [[speech disfluencies]]. Such text can be seen in large amounts in [[contact centre (business)|contact centers]], [[chat room]]s, [[optical character recognition]] (OCR) of text documents, [[short message service]] (SMS) text, etc. Documents with [[historical language]] can also be considered noisy with respect to today’s knowledge about the language. Such text contains important historical, religious, ancient medical knowledge that is useful. The nature of the noisy text produced in all these contexts warrants moving beyond traditional text analysis techniques.

== Techniques for noisy text analysis ==
Missing punctuation and the use of non-standard words can often hinder standard [[natural language processing]] tools such as [[part-of-speech tagging]]
and [[parsing]]. Techniques to both learn from the noisy data and then to be able to process the noisy data are only now being developed.

== Possible source of noisy text ==
* [[World wide web]]: Poorly written text is found in web pages, [[online chat]], [[blogs]], [[wikis]], [[discussion forum]]s, [[newsgroups]]. Most of these data are unstructured and the style of writing is very different from, say, well-written news articles. Analysis for the web data is important because they are sources for market buzz analysis, market review, [[trend estimation]], etc. Also, because of the large amount of data, it is necessary to find efficient methods of [[information extraction]], [[Statistical classification|classification]], [[automatic summarization]] and analysis of these data.
* [[Contact centre (business)|Contact centers]]: This is a general term for help desks, information lines and customer service centers operating in domains ranging from computer sales and support to mobile phones to apparels. On an average a person in the developed world interacts at least once a week with a contact center agent. A typical contact center agent handles over a hundred calls per day. They operate in various modes such as voice, [[online chat]] and [[E-mail]]. The contact center industry produces gigabytes of data in the form of [[E-mails]], chat logs, voice conversation [[Transcription (linguistics)|transcription]]s, customer feedback, etc. A bulk of the contact center data is voice conversations. Transcription of these using state of the art [[automatic speech recognition]] results in text with 30-40% [[word error rate]]. Further, even written modes of communication like online chat between customers and agents and even the interactions over email tend to be noisy. Analysis of contact center data is essential for customer relationship management, customer satisfaction analysis, call modeling, customer profiling, agent profiling, etc., and it requires sophisticated techniques to handle poorly written text.
* Printed Documents: Many libraries, government organizations and national defence organizations have vast repositories of [[hard copy]] documents. To retrieve and process the content from such documents, they need to be processed using [[Optical Character Recognition]]. In addition to printed text, these documents may also contain handwritten annotations. OCRed text can be highly noisy depending on the font size, quality of the print etc. It can range from 2-3% [[word error rate]]s to as high as 50-60% [[word error rate]]s. Handwritten annotations can be particularly hard to decipher, and error rates can be quite high in their presence.
* [[Text messaging|Short Messaging Service]] (SMS): Language usage over computer mediated discourses, like chats, emails and SMS texts, significantly differs from the standard form of the language. An urge towards shorter message length facilitating faster typing and the need for semantic clarity, shape the structure of this non-standard form known as the texting language.

== References ==
*[http://www.springerlink.com/content/ql711884654q/?p=c6beb20b8dfa4389b5e4daf2dd63618e&pi=0 "Special Issue on Noisy Text Analytics - International Journal on Document Analysis and Recognition (2007), Springer, Guest Editors Craig Knoblock, Daniel Lopresti, Shourya Roy and L. Venkata Subramaniam, Vol. 10, No. 3-4, December 2007."]
*[http://arXiv.org/abs/0810.0332 "Wong, W., Liu, W. & Bennamoun, M. Enhanced Integrated Scoring for Cleaning Dirty Texts. In: IJCAI Workshop on Analytics for Noisy Unstructured Text Data (AND), 2007; Hyderabad, India."].
*"L. V. Subramaniam, S. Roy, T. A. Faruquie, S. Negi, A survey of types of text noise and techniques to handle noisy text. In: Third Workshop on Analytics for Noisy Unstructured Text Data (AND), 2009".
<references />

==See also==
* [[Text analytics]]
* [[Information extraction]]
* [[Computational linguistics]]
* [[Natural language processing]]
* [[Named entity recognition]]
* [[Text mining]]
* [[Automatic summarization]]
* [[Statistical classification]]
* [[Data quality]]

[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval genres]]
[[Category:Statistical natural language processing]]
<=====doc_Id=====>:697
<=====title=====>:
Multimedia information retrieval
<=====text=====>:
{{COI|date=July 2014}}
{{Original research|date=July 2014}}
{{Use dmy dates|date=February 2012}}
'''Multimedia information retrieval''' ('''MMIR''' or '''MIR''') is a research discipline of [[computer science]] that aims at extracting semantic information from [[multimedia]] data sources.<ref name=Eidenberger>H Eidenberger. ''Fundamental Media Understanding'', atpress, 2011, p. 1.</ref>{{FV|date=July 2014}} Data sources include directly perceivable media such as [[Content (media and publishing)|audio]], [[image]] and [[video]], indirectly perceivable sources such as [[Written language|text]], biosignals as well as not perceivable sources such as bioinformation, stock prices, etc. The methodology of MMIR can be organized in three groups:

# Methods for the summarization of media content ([[feature extraction]]). The result of feature extraction is a description.
# Methods for the filtering of media descriptions (for example, elimination of [[Data redundancy|redundancy]])
# Methods for the [[categorization]] of media descriptions into classes.

== Feature extraction methods ==

Feature extraction is motivated by the sheer size of multimedia objects as well as their redundancy and, possibly, noisiness.<ref name=Eidenberger/>{{rp|2}}{{FV|date=July 2014}} Generally, two possible goals can be achieved by feature extraction:

* Summarization of media content. Methods for summarization include in the audio domain, for example, [[Mel-frequency cepstrum|mel-frequency cepstral coefficients]], Zero Crossings Rate, Short-Time Energy. In the visual domain, color histograms<ref>A Del Bimbo. ''Visual Information Retrieval'', Morgan Kaufmann, 1999.</ref> such as the [[MPEG-7]] Scalable Color Descriptor can be used for summarization.
* Detection of patterns by [[auto-correlation]] and/or [[cross-correlation]]. Patterns are recurring media chunks that can either be detected by comparing chunks over the media dimensions (time, space, etc.) or comparing media chunks to templates (e.g. face templates, phrases). Typical methods include Linear Predictive Coding in the audio/biosignal domain,<ref>HG Kim, N Moreau, T Sikora.'' MPEG-7 Audio and Beyond", Wiley, 2005.</ref> texture description in the visual domain and n-grams in text information retrieval.

== Merging and filtering methods ==

Multimedia Information Retrieval implies that multiple channels are employed for the understanding of media content.<ref>MS Lew (Ed.). ''Principles of Visual Information Retrieval'', Springer, 2001.</ref> Each of this channels is described by media-specific feature transformations. The resulting descriptions have to be merged to one description per media object. Merging can be performed by simple concatenation if the descriptions are of fixed size. Variable-sized descriptions – as they frequently occur in motion description – have to be normalized to a fixed length first.

Frequently used methods for description filtering include [[factor analysis]] (e.g. by PCA), singular value decomposition (e.g. as latent semantic indexing in text retrieval) and the extraction and testing of statistical moments. Advanced concepts such as the [[Kalman filter]] are used for merging of descriptions.

== Categorization methods ==

Generally, all forms of machine learning can be employed for the categorization of multimedia descriptions<ref name=Eidenberger/>{{rp|125}}{{FV|date=July 2014}} though some methods are more frequently used in one area than another. For example, [[hidden Markov models]] are state-of-the-art in [[speech recognition]], while [[dynamic time warping]] – a semantically related method – is state-of-the-art in gene sequence alignment. The list of applicable classifiers includes the following:

* Metric approaches ([[Cluster analysis]], [[vector space model]], [[Minkowski]] distances, dynamic alignment)
* Nearest Neighbor methods ([[K-nearest neighbors algorithm]], K-means, [[self-organizing map]])
* Risk Minimization (Support vector regression, [[support vector machine]], [[linear discriminant analysis]])
* Density-based Methods (Bayes nets, [[Markov process]]es, mixture models)
* Neural Networks ([[Perceptron]], associative memories, spiking nets)
* Heuristics ([[Decision trees]], random forests, etc.)

The selection of the best classifier for a given problem (test set with descriptions and class labels, so-called [[ground truth]]) can be performed automatically, for example, using the [[Weka]] Data Miner.

== Open problems ==

The quality of MMIR Systems<ref>JC Nordbotten. "[http://nordbotten.com/ADM/ADM_book/MIRS-frame.htm Multimedia Information Retrieval Systems]". Retrieved 14 October 2011.</ref> depends heavily on the quality of the training data. Discriminative descriptions can be extracted from media sources in various forms. Machine learning provides categorization methods for all types of data. However, the classifier can only be as good as the given training data. On the other hand, it requires considerable effort to provide class labels for large databases. The future success of MMIR will depend on the provision of such data.<ref>H Eidenberger. ''Frontiers of Media Understanding'', atpress, 2012.</ref> The annual [[TRECVID]] competition is currently one of the most relevant sources of high-quality ground truth.

== Related areas ==

MMIR provides an overview over methods employed in the areas of information retrieval.<ref>H Eidenberger. ''Professional Media Understanding'', atpress, 2012.</ref><ref>{{cite journal |last=Raieli |first=Roberto ||date= |title=Introducing Multimedia Information Retrieval to libraries |url=http://leo.cineca.it/index.php/jlis/article/view/11530 |journal=JLIS.it |volume=7 |issue=3 |pages=9-42 |doi=10.4403/jlis.it-11530 |access-date=8 October 2016 }}</ref> Methods of one area are adapted and employed on other types of media. Multimedia content is merged before the classification is performed. MMIR methods are, therefore, usually reused from other areas such as:

* [[Bioinformatics|Bioinformation analysis]]
* [[Biosignal|Biosignal processing]]
* [[Content-based image retrieval|Content-based image and video retrieval]]
* [[Facial recognition system|Face recognition]]
* [[Music information retrieval|Audio and music classification]]
* [[Speech recognition]]
* [[Technical analysis|Technical chart analysis]]
* [[Video Browsing|Video browsing]]
* [[Information retrieval|Text information retrieval]]

The ''Journal of Multimedia Information Retrieval''<ref>"[http://www.springer.com/computer/journal/13735 Journal of Multimedia Information Retrieval]", Springer, 2011, Retrieved 21 October 2011.</ref> documents the development of MMIR as a research discipline that is independent of these areas. See also ''Handbook of Multimedia Information Retrieval''<ref>H Eidenberger. ''Handbook of Multimedia Information Retrieval'', atpress, 2012.</ref> for a complete overview over this research discipline.

==References==
{{reflist}}

[[Category:Information retrieval genres]]
<=====doc_Id=====>:700
<=====title=====>:
Dragomir R. Radev
<=====text=====>:
'''Dragomir R. Radev''' is a [[University of Michigan]] computer science professor and [[Columbia University]] computer science adjunct professor working on [[natural language processing]] and [[information retrieval]]. From January 2017 he will join [[Yale University]] as a professor of computer science.
He is currently working on the fields of open domain [[question answering]],  [[multi-document summarization]], and the application of NLP in Bioinformatics and Political Science.

Radev received his PhD in [[Computer Science]] from [[Columbia University]] in 1999. He is the secretary of [http://www.aclweb.org [[Association for Computational Linguistics|ACL]]] (2006–present) and associate editor of [http://www.jair.org JAIR].

== Awards ==
As [[NACLO]] founder, Radev shared the [[Linguistic Society of America]] 2011 [http://www.lsadc.org/info/lsa-awards.cfm ''Linguistics, Language and the Public Award'']. He is the  Co-winner of the [http://polmeth.wustl.edu/about.php?page=awards Gosnell Prize (2006)].

In 2015 he was named a [[fellow]] of the [[Association for Computing Machinery]] "for contributions to natural language processing and computational linguistics."<ref>{{citation|url=http://www.acm.org/press-room/news-releases/2015/fellows-2015|title=ACM Fellows Named for Computing Innovations that Are Advancing Technology in the Digital Age|publisher=[[Association for Computing Machinery]]|year=2015|accessdate=2015-12-10}}.</ref>

== IOL==
Radev has served as the coach and led the US national team in the [[International Linguistics Olympiad|International Linguistics Olympiad (IOL)]] to several gold medals [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073][http://www.nsf.gov/news/news_summ.jsp?cntn_id=109891].

== Books ==
* Puzzles in Logic, Languages and Computation (2013) <ref>{{Cite web|url = http://www.springer.com/education+%26+language/linguistics/book/978-3-642-34371-1|title = Puzzles in Logic, Languages and Computation|date = |accessdate = |website = |publisher = |last = |first = }}</ref>
* Mihalcea and Radev (2011) [http://www.cambridge.org/gb/knowledge/isbn/item5980387/?site_locale=en_GB ''Graph-based methods for NLP and IR'']

== Selected Papers ==
* SIGIR 1995 Generating summaries of multiple news articles
* ANLP 1997 Building a generation knowledge source using internet-accessible newswire
* Computational Linguistics 1998 Generating natural language summaries from multiple on-line sources
* ACL 1998 Learning correlations between linguistic indicators and semantic constraints: Reuse of context dependent descriptions of entities
* ANLP 2000 Ranking suspected answers to natural language questions using predictive annotation
* CIKM 2001 Mining the web for answers to natural language questions
* AAAI 2002 Towards CST-enhanced summarization
* ACL 2003 Evaluation challenges in large-scale multi-document summarization: the Mead project
* Information Processing and Management 2004 Centroid-based summarization of multiple documents
* J. of Artificial Intelligence Research 2004 LexRank: Graph-based lexical centrality as salience in text summarization
* J. of the American Association of Information Science and Technology 2005 Probabilistic question answering on the web
* Communications of the ACM 2005 NewsInEssence: summarizing online news topics
* EMNLP 2007 Semi-supervised classification for extracting protein interaction sentences using dependency parsing
* Bioinformatics 2008 Identifying gene-disease associations using centrality on a literature mined gene-interaction network
* IEEE Intelligent Systems 2008 natural language processing and the web
* NAACL 2009 Generating surveys of scientific paradigms
* Nucleic Acids Research 2009 Michigan molecular interactions r2: from interacting proteins to pathways
* J. of the American Association of Information Science and Technology 2009 Visual overviews for discovering key papers and influences across research fronts
* KDD 2010 Divrank: the interplay of prestige and diversity in information networks
* American J. of Political Science 2010 How to Analyze Political Attention with Minimal Assumptions and Costs
* Arxiv 2011 The effect of linguistic constraints on the large scale organization of language
* J. of Biomedical Semantics 2011 Mining of vaccine-associated ifn-gamma gene interaction networks using the vaccine ontology

==External links==
* [http://www.nsf.gov/news/news_summ.jsp?cntn_id=112073 Team USA Brings Home the Linguistics Gold]
* [http://www.eecs.umich.edu/eecs/about/articles/2011/Radev-LSA11.html Dragomir Radev, Co-Founders Recognized as NACLO Receives Linguistics, Language and the Public Award]
* [http://www.eecs.umich.edu/eecs/about/articles/2010/Radev-Linguistics.html Dragomir Radev Coaches US Linguistics Team to Multiple Wins]
* [http://www.eecs.umich.edu/eecs/about/articles/2009/Radev-ACM-DM.html Dragomir Radev Honored as ACM Distinguished Scientist]
* [http://www.eecs.umich.edu/eecs/etc/news/shownews.cgi?428 Prof. Dragomir Radev Receives Gosnell Prize]

== References ==
{{reflist}}
<!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. --->
*
*
*
*


{{DEFAULTSORT:Radev, Dragomir R.}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]

[[Category:Columbia School of Engineering and Applied Science alumni]]
[[Category:American computer scientists]]
[[Category:University of Michigan faculty]]
[[Category:Natural language processing]]
[[Category:Information retrieval researchers]]
[[Category:Fellows of the Association for Computing Machinery]]
<=====doc_Id=====>:703
<=====title=====>:
Gerard Salton
<=====text=====>:
{{Infobox scientist
| name              = Gerard Salton
| birth_date        = {{birth date|1927|03|08}}
| birth_place       = [[Nuremberg]]
| death_date        = {{death date and age|1995|08|28 |1927|03|08}}
| death_place       = 
| fields            = [[information retrieval]]
| workplaces        = [[Cornell University]]
| alma_mater        = [[Harvard University]]
| thesis_title      = An automatic data processing system for public utility revenue accounting
| thesis_url        = http://hollis.harvard.edu/?itemid=%7Clibrary/m/aleph%7C003918090
| thesis_year       = 1958
| doctoral_advisor  = [[Howard Aiken]]
| doctoral_students = [[Amit Singhal]]
| known_for         = the father of information retrieval<ref name=father-IR /> <br> [[Gerard Salton Award]]
}}
'''Gerard A. "Gerry" Salton''' (8 March 1927 in [[Nuremberg]] – 28 August 1995), was a Professor of [[Computer Science]] at [[Cornell University]].  Salton was perhaps the leading computer scientist working in the field of [[information retrieval]] during his time, and "the father of information retrieval".<ref name=father-IR>{{cite web |url=http://www.cs.cornell.edu/gries/40brochure/pg24_25.pdf |title=The father of information retrieval |last1= |first1= |last2= |first2= |date= |website=cs.cornell.edu |publisher= |quote= a founding member of the department and the father of information retrieval. |access-date=10 March 2015}}</ref>  His group at Cornell developed the [[SMART Information Retrieval System]], which he initiated when he was at Harvard.

Salton was born Gerhard Anton Sahlmann on March 8, 1927 in [[Nuremberg, Germany]].  He received a Bachelor's (1950) and Master's (1952) degree in mathematics from [[Brooklyn College]], and a Ph.D. from [[Harvard University|Harvard]] in [[Applied Mathematics]] in 1958, the last of [[Howard Aiken]]'s doctoral students, and taught there until 1965, when he joined [[Cornell University]] and co-founded its department of Computer Science.

Salton was perhaps most well known for developing the now widely used [[vector space model]] for Information Retrieval.<ref>{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Wong | first2 = A. | last3 = Yang | first3 = C. S. | doi = 10.1145/361219.361220 | title = A vector space model for automatic indexing | journal = Communications of the ACM | volume = 18 | issue = 11 | pages = 613 | year = 1975 | pmid =  | pmc = }}</ref>  In this model, both documents and queries are represented as vectors of term counts, and the similarity between a document and a query is given by the cosine between the term vector and the document vector.  In this paper, he also introduced [[TF-IDF]], or term-frequency-inverse-document frequency, a model in which the score of a term in a document is the ratio of the number of terms in that document divided by the frequency of the number of documents in which that term occurs. (The concept of inverse document frequency, a measure of specificity, had been introduced in 1972 by [[Karen Spärck Jones|Karen Sparck-Jones]].<ref>{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11–21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}</ref>) Later in life, he became interested in automatic text summarization and analysis,<ref>{{Cite journal | last1 = Salton | first1 = G. | authorlink1 = Gerard Salton| last2 = Allan | first2 = J. | last3 = Buckley | first3 = C. | last4 = Singhal | first4 = A. | title = Automatic Analysis, Theme Generation, and Summarization of Machine-Readable Texts | doi = 10.1126/science.264.5164.1421 | journal = Science | volume = 264 | issue = 5164 | pages = 1421–1426 | year = 1994 | pmid =  17838425| pmc = }}</ref> as well as automatic hypertext generation.<ref>{{cite web|url=http://www.cs.cornell.edu/Info/Department/Annual95/Faculty/Salton.html |title=Gerard Salton |publisher=Cs.cornell.edu |date= |accessdate=2013-09-14}}</ref>  He published over 150 research articles and 5 books during his life.

Salton was editor-in-chief of the [[Communications of the ACM]] and the [[Journal of the ACM]], and chaired [[Special Interest Group on Information Retrieval]] (SIGIR).  He was an associate editor of the [[ACM Transactions on Information Systems]]. He was an [[List of Fellows of the Association for Computing Machinery|ACM Fellow]] (elected 1995),<ref name=fellow-acm>{{cite web |url=http://awards.acm.org/award_winners/salton_2316166.cfm |title=Gerard Salton ACM Fellows  1995 |last1= |first1= |last2= |first2= |date= |website=acm.org |publisher= |quote=contributions over 30 years to information organization and retrieval |access-date=10 March 2015}}</ref> received an Award of Merit from the [[American Society for Information Science]] (1989), and was the first recipient of the SIGIR Award for outstanding contributions to study of information retrieval (1983) -- now called the [[Gerard Salton Award]].

==Bibliography==
*Salton, ''Automatic Information Organization and Retrieval'', 1968.
*{{cite book
 | author     = Gerard Salton
 | title      = A Theory of Indexing
 | publisher  = Society for Industrial and Applied Mathematics
 | year       = 1975
 | page      = 56
}}
*--- and Michael J. McGill, ''Introduction to modern information retrieval'', 1983.  ISBN 0-07-054484-0
*{{cite book
 | author     = Gerard Salton
 | title      = Automatic Text Processing
 | publisher  = Addison-Wesley Publishing Company
 | year       = 1989
 | page      = 530
 | isbn       = 0-201-12227-8
}}
*[http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Salton:Gerard.html DBLP Bibliography]
* G. Salton, A. Wong, and C. S. Yang (1975), "[http://www.cs.uiuc.edu/class/fa05/cs511/Spring05/other_papers/p613-salton.pdf A Vector Space Model for Automatic Indexing]," ''Communications of the ACM'', vol. 18, nr. 11, pages 613–620. ''(Article in which a vector space model was presented)''

==See also==
* [[List of pioneers in computer science]]

==References==
{{Reflist}}

==External links==
* [http://www.cs.cornell.edu/Info/Department/Annual96/Beginning/salton.html In Memoriam]
* [http://blog.tomevslin.com/2006/01/search_down_mem.html Fractals of Change: Search Down Memory Lane]
* [http://www.ideals.uiuc.edu/bitstream/2142/1697/2/Dubin748764.pdf The Most Influential Paper Gerard Salton Never Wrote] - This 2004 Library Trends paper by David Dubin serves as a historical review of the metamorphosis of the term discrimination value model (TDV) into the vector space model as an information retrieval model (VSM as an IR model). This paper calls into question what the Information Retrieval research community believed Salton's vector space model was originally intended to model. What much later became an information retrieval model was originally a data-centric mathematical–computational model used as an explanatory device. In addition, Dubin's paper points out that a 1975 Salton paper oft cited does not exist but is probably a combination of two other papers, neither of which actually refers to the VSM as an IR model.

{{Authority control}}

{{DEFAULTSORT:Salton, Gerard}}
[[Category:1927 births]]
[[Category:1995 deaths]]
[[Category:American computer scientists]]
[[Category:Harvard University alumni]]
[[Category:Harvard University faculty]]
[[Category:Cornell University faculty]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Guggenheim Fellows]]
[[Category:Information retrieval researchers]]
<=====doc_Id=====>:706
<=====title=====>:
W. Bruce Croft
<=====text=====>:
'''W. Bruce Croft''' is a [[distinguished professor]] of [[computer science]] at the [[University of Massachusetts Amherst]] whose work focuses on [[information retrieval]].<ref>
{{Cite web
  | last = Croft
  | first = W. Bruce
  | title = Biography
  | url=http://ciir.cs.umass.edu/personnel/croftbio.pdf
  | accessdate = November 4, 2009}}</ref>
He is the founder of the [[Center for Intelligent Information Retrieval]] and served as the editor-in-chief of [[ACM Transactions on Information Systems]] from 1995 to 2002.  He was also a member of the [[United States National Research Council|National Research Council]] [http://sites.nationalacademies.org/CSTB/index.htm Computer Science and Telecommunications Board] from 2000 to 2003. Since 2015, he is the Dean of the College of Information and Computer Sciences at the University of Massachusetts Amherst. He was Chair of the UMass Amherst Computer Science Department from 2001 to 2007.

Bruce Croft formed the [[Center for Intelligent Information Retrieval]] (CIIR) in 1991, since when he and his students have worked with more than 90 industry and government partners on research and technology projects and have produced more than 900 papers. Bruce Croft has made major contributions to most areas of information retrieval, including pioneering work in clustering, passage retrieval, sentence retrieval, and distributed search. One of the most important areas of work for Croft  relates to ranking functions and retrieval models, where he has led the development of one of the major approaches to modeling search: language modelling. In later years, Croft also led the way in the development of feature-based ranking functions. Croft and his research group have also developed a series of search engines: InQuery, the Lemur toolkit, Indri, and Galago. These search engines are open source and offer unique capabilities that are not replicated in other research retrieval platforms source – consequently they are downloaded by hundreds of researchers world wide. As a consequence of his work, Croft is one of the most cited researchers in information retrieval.

==Education==
Croft earned a bachelor's degree with honors in 1973 and a master's degree in computer science in 1974 from [[Monash University]] in [[Melbourne|Melbourne, Australia]].  He earned his Ph.D in computer science from the [[University of Cambridge]] in 1979 and joined the [[University of Massachusetts Amherst|University of Massachusetts, Amherst]] faculty later that year.

==Honors and awards==
Croft has received several prestigious awards, including:
* [[ACM Fellow]] in 1997
* [[American Society for Information Science and Technology]] Research Award in 2000
* [[Gerard Salton Award]] (a lifetime achievement award) from ACM SIGIR in 2003
* [[Tony Kent Strix award|Tony Kent Strix Award]] in 2013
* IEEE Computer Society Technical Achievement Award in 2014
* [http://sigir.org/awards/best-student-paper-awards/ Best Student Paper Award] from SIGIR in 1997 and 2005
* [http://sigir.org/awards/test-of-time-awards/ Test of Time Award] from SIGIR for his papers published in 1990, 1995, 1996, 1998, 2001
* Many other publications are short-listed as the Best Paper Award in SIGIR and CIKM

==References==
<references/>

==External links==
* [http://ciir.cs.umass.edu/personnel/croft.html Faculty homepage]

{{DEFAULTSORT:Croft, W. Bruce}}
[[Category:American computer scientists]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:University of Massachusetts Amherst faculty]]
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:Information retrieval researchers]]


{{Compu-bio-stub}}
<=====doc_Id=====>:709
<=====title=====>:
Norbert Fuhr
<=====text=====>:
'''Norbert Fuhr''' (born 1956) is a professor of computer science
and the leader of the Duisburg Information Engineering Group based at
the [[University of Duisburg-Essen]], Germany.

==Education==
His first  degree is in  technical computer science, which he got  from the Electrical
Engineering Department of  the [[Technical University of Darmstadt]] in 1980, and in 1986 he finished his PhD
(Dr.-Ing) in the Computer Science Department of the same university on "Probabilistic
Indexing and Retrieval".<ref name="Fuhr1986">{{citation
 | author=Fuhr, Norbert
 | publisher=Fachinformationszentrum Karlsruhe
 | title=Probabilistisches Indexing und Retrieval
 | year=1986
}}</ref>

==Profession==
He held a PostDoc position in Darmstadt until
1991, when he was appointed Associate Professor in the  Computer
Science Department  of  the [[Technical University of Dortmund]]. Since 2002, he is a full professor at the
[[University of Duisburg-Essen]].

==Honors and awards==
Fuhr's dissertation was awarded the  "Gerhard Pietsch Award" of the German Society
of Documentation in 1987. In 2012, he received the  
[[Gerard Salton Award]].<ref name="Fuhr2012">{{citation
 | author=Fuhr, Norbert
 | journal=SIGIR '12 Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval
 | title=Salton award lecture: information retrieval as engineering science
 | pages=1–2
 | year=2012
 | doi=10.1145/2348283.2348285
}}</ref>

==References==
{{reflist}}

==External links==
*[http://www.is.inf.uni-due.de/staff/fuhr.html Norbert Fuhr - University of Duisburg-Essen]

{{Authority control}}
{{DEFAULTSORT:Fuhr, Norbert}}
[[Category:German computer scientists]]
[[Category:1956 births]]
[[Category:Living people]]
[[Category:University of Duisburg-Essen faculty]]
[[Category:Technical University of Dortmund faculty]]
[[Category:Technische Universität Darmstadt alumni]]
[[Category:Information retrieval researchers]]


{{Compu-bio-stub}}
<=====doc_Id=====>:712
<=====title=====>:
Cutter Expansive Classification
<=====text=====>:
The '''Cutter Expansive Classification''' system is a [[library classification]] system devised by [[Charles Ammi Cutter]]. The system was the basis for the top categories of the [[Library of Congress Classification]].<ref>LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 226.</ref>

==History of the Expansive Classification==
[[Charles Ammi Cutter]] (1837&ndash;1903), inspired by the decimal classification of his contemporary [[Melvil Dewey]], and with Dewey's initial encouragement, developed his own classification scheme for the Winchester Town Library and then the [[Boston Athenaeum]],<ref>LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 208.</ref> at which he served as librarian for twenty-four years. He began work on it around the year 1880, publishing an overview of the new system in 1882. The same classification would later be used, but with a different notation, also devised by Cutter, at the [[Cary Memorial Library|Cary Library]] in [[Lexington, Massachusetts]].<ref>Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93, p. 1.</ref>

Many libraries found this system too detailed and complex for their needs, and Cutter received many requests from librarians at small libraries who wanted the classification adapted for their collections. He devised the Expansive Classification in response, to meet the needs of growing libraries, and to address some of the complaints of his critics
.<ref>For the Expansive Classification as a response to Cutter's critics, see: Miksa, Francis L., ed. ''Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.
* For the Expansive Classification as a response to the growing needs of libraries, see Miksa, above, and also: LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 209.
* The above issues are also discussed by Cutter in his [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93.
</ref> Cutter completed and published an introduction and schedules for the first six classifications of his new system ([https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']), but his work on the seventh was interrupted by his death in 1903.<ref>LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 210.</ref>

The Cutter Expansive Classification, although adopted by comparatively few libraries, mostly in [[New England]]{{Citation needed|date=August 2011}}, has been called one of the most logical and scholarly of American classifications{{Citation needed|date=August 2011}}. Library historian Leo E. LaMontagne writes:

<blockquote>Cutter produced the best classification of the nineteenth century. While his system was less "scientific" than that of [[J. P. Lesley]], its other key features – notation, specificity, and versatility – make it deserving of the praise it has received.<ref>LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961, p. 215</ref></blockquote>

Its top level divisions served as a basis for the Library of Congress classification, which also took over some of its features.<ref>LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, Connecticut, Shoe String Press. 1961, p. 226.</ref> It did not catch on as did Dewey's system because Cutter died before it was completely finished, making no provision for the kind of development necessary as the bounds of knowledge expanded and scholarly emphases changed throughout the twentieth century.<ref>https://journals.ala.org/index.php/lrts/article/view/5419/6654</ref>

==Structure of the Expansive Classification==
The Expansive Classification uses seven separate schedules, each designed to be used by libraries of different sizes. After the first, each schedule was an expansion of the previous one,<ref>Miksa, Francis L., ed. ''Charles Ammi Cutter: Library Systematizer". Littleton, CO, Libraries Unlimited. 1977, p. 58.</ref> and Cutter provided instructions for how a library might change from one expansion to another as it grows.<ref>Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93, p. 21–23.</ref>

==Summary of the Expansive Classification Schedules==

===First Classification===
The first classification is meant for only the very smallest libraries. The first classification has only seven top level classes, and only eight classes in all.

* '''A''' Works of reference and general works which include several of the following sections, and so could not go in any one.
* '''B''' [[Outline of philosophy|Philosophy]] and [[Outline of religion|Religion]]
* '''E''' Biography
* '''F''' [[Outline of history|History]] and [[Outline of geography|Geography]] and Travels
* '''H''' [[Outline of social science|Social sciences]]
* '''L''' [[Outline of natural science|Natural sciences]] and [[The arts|Arts]]
* '''Y''' [[Outline of linguistics|Language]] and [[Outline of literature|Literature]]
* '''YF''' [[Outline of fiction|Fiction]]

===Further Classifications===
Further expansions add more top level classes and subdivisions. Many subclasses arranged systematically, with common divisions, such as those by geography and language, following a consistent system throughout.<ref>https://archive.org/details/cu31924092476229</ref>

By the fifth classification all the letters of the alphabet are in use for top level classes. These are:

* '''A''' General Works
* '''B''' [[Outline of philosophy|Philosophy]]
* '''C''' [[Outline of Christianity|Christianity]] and [[Outline of Judaism|Judaism]]
* '''D''' Ecclesiastical History
* '''E''' Biography
* '''F''' [[Outline of history|History]], Universal History
* '''G''' [[Outline of geography|Geography]] and Travels
* '''H''' [[Outline of social science|Social Sciences]]
* '''I''' Demotics, [[Outline of sociology|Sociology]]
* '''J''' Civics, Government, [[Outline of political science|Political Science]]
* '''K''' Legislation
* '''L''' [[Outline of science|Science]] and [[The arts|Arts]] together
* '''M''' Natural History
* '''N''' [[Outline of botany|Botany]]
* '''O''' [[Outline of zoology|Zoölogy]]
* '''P''' [[Outline of anthropology|Anthropology]] and Ethnology
* '''Q''' [[Outline of medicine|Medicine]]
* '''R''' Useful arts, [[Outline of technology|Technology]]
* '''S''' Constructive arts ([[Outline of engineering|Engineering]] and [[Outline of construction|Building]])
* '''T''' [[Outline of manufacturing|Manufactures]] and Handicrafts
* '''U''' [[Outline of military science and technology|Art of War]]
* '''V''' Recreative arts, [[Outline of sports|Sports]], [[Outline of games|Games]], [[Outline of festivals|Festivals]]
* '''W''' [[Outline of the visual arts|Art]]
* '''X''' English Language
* '''Y''' English and American literature
* '''Z''' Book arts

These schedules were not meant to be fixed, but were to be adapted to meet the needs of each library. For example, books on the English language may be put in X, and books on language in general in a subclass of X, or this can be reversed. The first option is less logical, but results in shorter marks for most English language libraries.<ref>Cutter, C. A. [https://books.google.com/books?id=L10oAAAAYAAJ&pg=PA1 ''Expansive Classification: Part I: The First Six Classifications'']. Boston, C. A. Cutter. 1891–93, p. 27.</ref>

==How Expansive Classification call numbers are constructed==
{{Expand section|citations and corrections|date=August 2011}}
Most call numbers in the Expansive Classification follow conventions offering clues to the book's subject. The first line represents the subject, the second the author (and perhaps title), the third and fourth dates of editions, indications of translations, and critical works on particular books or authors. All numbers in the Expansive Classification are (or should be) shelved as if in decimal order.

Size of volumes is indicated by points (.), pluses (+), or slashes (/ or //).

For some subjects a numerical geographical subdivision follows the classification letters on the first line. The number 83 stands for the United States&mdash;hence, F83 is U.S. history, G83 U.S. travel, JU83 U.S. politics, WP83 U.S. painting. Geographical numbers are often further expanded decimally to represent more specific areas, sometimes followed by a capital letter indicating a particular city.

The second line usually represents the author's name by a capital letter plus one or more numbers arranged decimally. This may be followed by the first letter or letters of the title in lower-case, and/or sometimes the letters a,b,c indicating other printings of the same title. When appropriate, the second line may begin with a 'form' number&mdash;e.g., 1 stands for history and criticism of a subject, 2 for a bibliography, 5 for a dictionary, 6 for an atlas or maps, 7 for a periodical, 8 for a society or university publication, 9 for a collection of works by different authors.

On the third line a capital Y indicates a work about the author or book represented by the first two lines, and a capital E (for English&mdash;other letters are used for other languages) indicates a translation into English. If both criticism and translation apply to a single title, the number expands into four lines.

=== Cutter Numbers (Cutter Codes) ===
{{Expand section|examples and additional citations|date=August 2011}}
One of the features adopted by other systems, including Library of Congress, is the Cutter number. It is an alphanumeric device to code text so that it can be arranged in alphabetical order using the fewest characters. It contains one or two initial letters and Arabic numbers, treated as a decimal. To construct a Cutter number, a cataloguer consults a Cutter table as required by the classification rules.  Although Cutter numbers are mostly used for coding the names of authors, the system can be used for titles, subjects, geographic areas, and more.

{| class=wikitable
|+Cutter table
|-
! || 2 || 3 || 4 || 5 || 6 || 7 || 8 || 9
|-
| S || a || ch || e || h-i || m-p || t || u || w-z
|-
| Qu || || a || e || i || o || r || t || y
|-
| other consonants || || a || e || i || o || r || u || y
|-
| vowels || b || d || l-m || n || p || r || s-t || u-y
|-
| additional letters || || a-d || e-h || i-l || m-o || p-s || t-v || w-z
|}

Initial letters Qa-Qt are assigned Q2-Q29, while entries beginning with numerals have a Cutter number A12-A19, therefore sorting before the first A entry.<ref>{{cite web|title=LC Cutter Tables |url=http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |website=Queen Elizabeth II Libraries |publisher=Memorial University of Newfoundland |accessdate=14 August 2014 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20140814173419/http://staff.library.mun.ca/staff/toolbox/tables/lccutter.htm |archivedate=14 August 2014 |df= }}</ref>

So to make the three digit Cutter number for "Cutter", you would start with "C", then looking under ''other consonants'', find that "u" gives the number 8, and under ''additional letters'', "t" is 8, giving a Cutter number of "C88".

==Notes==
{{reflist}}

==References==
* Bliss, Henry Evelyn. ''The Organization of Knowledge in Libraries: and the Subject-Approach to Books'', 2nd ed. New York: H. W. Wilson, 1939.
* Cutter, Charles A. ''Rules for a Dictionary Catalog''. W. P. Cutter, ed. 4th ed. Washington, D.C.: Government Printing Office, 1904. London: The Library Association, 1962.
* Cutter, William Parker. ''Charles Ammi Cutter''. Chicago: American Library Association, 1931. Ann Arbor, MI: University Microfilms, 1969.
* Foster, William E. "Charles Ammi Cutter: A Memorial Sketch". ''Library Journal'' 28 (1903): 697-704.
* Hufford, Jon R. "The Pragmatic Basis of Catalog Codes: Has the User Been Ignored?". ''Cataloging and Classification Quarterly'' 14 (1991): 27-38.
* Immroth, John Philip. "Cutter, Charles Ammi". ''Encyclopedia of Library and Information Science''. [[Allen Kent]] and Harold Lancour, ed. 47 vols. New York, M. Dekker [1968- ]
* LaMontagne, Leo E. ''American Library Classification: With Special Reference to the Library of Congress''. Hamden, CT, Shoe String Press. 1961.
*Slavis, Dobrica. "CUTT-x: An Expert System for Automatic Assignment of Cutter Numbers". ''Cataloging and Classification Quarterly''. Vol 22, no. 2, 1996.
* Tauber, Maurice F., and Edith Wise. "Classification Systems". [[Ralph R. Shaw (Librarian)|Ralph R. Shaw]], ed.. ''The State of the Library Art''. New Brunswick, NJ: Rutgers U. Graduate School of Library Service, 1961. 1-528.

==External links==
* [http://catalog.bostonathenaeum.org/cutterguide.html The Boston Athenaeum's Guide to the classification system developed by Cutter for their collection]
* [http://www.forbeslibrary.org/research/index.php?n=Main.CutterClassification Forbes Library's Outline of Cutter's Expansive Classification system]
* [http://www.forbeslibrary.org/pathfinders/Shelvingrules.pdf A brief guide to the Expansive Classification from Forbes Library]
* [http://digital.library.unt.edu/permalink/meta-dc-1048:1 ''Rules for a dictionary catalog, by Charles A. Cutter, fourth edition''], hosted by the [http://digital.library.unt.edu/ UNT Libraries Digital Collections]
* [http://www.loc.gov/aba/pcc/053/table.html Library of Congress Guidelines for using the LC Online Shelflist and formulating a literary author number: Cutter Table]
* [http://www.oclc.org/dewey/support/program/default.htm Dewey Cutter Program]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:715
<=====title=====>:
Suggested Upper Merged Ontology
<=====text=====>:
The '''Suggested Upper Merged Ontology''' or '''SUMO''' is an [[Upper ontology (information science)|upper ontology]] intended as a foundation [[ontology (computer science)|ontology]] for a variety of computer information processing systems. It was originally developed by the Teknowledge Corporation and now is maintained by [http://www.articulatesoftware.com Articulate Software]. SUMO is [[open source]].

SUMO originally concerned itself with meta-level concepts (general entities that do not belong to a specific problem domain), and thereby would lead naturally to a categorization scheme for encyclopedias.  It has now been considerably expanded to include a mid-level ontology and dozens of domain ontologies.

SUMO was first released in December 2000. It defines a hierarchy of ''SUMO classes'' and related rules and relationships. These are formulated in a version of the language [[SUO-KIF]] which has a [[LISP]]-like syntax. A [[Map (mathematics)|mapping]] from [[WordNet]] [[synsets]]  to SUMO has also been defined.  

SUMO is organized for interoperability of automated [[reasoning engine]]s. To maximize compatibility, [[logical schema|schema]] designers can try to assure that their [[naming convention]]s use the same meanings as SUMO for identical words (for example, "agent" or "process").  SUMO has an associated open source [[Sigma knowledge engineering environment]].

==See also==
* [[Semantic translation]]
* [[Upper ontology]]

== External links ==
* [http://www.ontologyportal.org/ Main page for SUMO]
* [http://suo.ieee.org/ Home page of the IEEE Standard Upper Ontology working group]
* The [http://sigmakee.sourceforge.net Sigma] reasoning system for SUMO
* [http://54.183.42.206:8080/sigma/Browse.jsp?kb=SUMO Online browser for SUMO]
* [http://www.adampease.org/professional/ Adam Pease, current Technical Editor of the standard]
[[Category:Java platform software]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Open data]]
[[Category:Knowledge bases]]
{{Compu-AI-stub}}
<=====doc_Id=====>:718
<=====title=====>:
Medical algorithm
<=====text=====>:
{{Expand Russian|Медицинский алгоритм|date=September 2015}}
{{Original research|date=October 2007}}
[[File:Assessment and treatment algorithm for overweight and obesity.png|thumb|450px|A medical algorithm for assessment and treatment of [[overweight]] and [[obesity]].]]
A '''medical algorithm''' is any [[computation]], [[formula]], [[statistical survey]], [[nomogram]], or [[look-up table]], useful in [[healthcare]].  [[Medical]] [[algorithm]]s include [[decision tree]] approaches to healthcare treatment (e.g., if [[symptom]]s A, B, and C are evident, then use treatment X) and also less clear-cut tools aimed at reducing or defining uncertainty.

==Scope==
Medical algorithms are part of a broader field which is usually fit under the aims of [[medical informatics]] and medical [[decision-making]]. Medical decisions occur in several areas of medical activity including medical test selection, [[diagnosis]], therapy and [[prognosis]], and [[automatic control]] of [[medical equipment]].

In relation to [[logic]]-based and [[artificial neural network]]-based [[clinical decision support system]], which are also computer applications to the medical decision-making field, algorithms are less complex in architecture, data structure and user interface. Medical algorithms are not necessarily implemented using digital computers. In fact, many of them can be represented on paper, in the form of diagrams, nomographs, etc.

==Examples==
A wealth of medical information exists in the form of published medical algorithms.  These algorithms range from simple [[calculation]]s to complex outcome [[prediction]]s.  Most [[clinician]]s use only a small subset routinely.

Examples of medical algorithms are:
* '''[[Calculators]],'''. e.g., an on-line or stand-alone calculator for [[body mass index]] (BMI) when stature and body weight are given;
* '''[[Flowcharts]],''' e.g., a [[Wiktionary:binary|binary]] [[decision tree]] for deciding what is the [[etiology]] of [[chest pain]]
* '''[[Look-up table]]s,''' e.g., for looking up [[food energy]] and nutritional contents of foodstuffs
* '''[[Nomogram]]s,''' e.g., a moving circular slide to calculate body surface area or drug dosages.

A common class of algorithms are embedded in guidelines on the choice of treatments produced by many national, state, financial and local healthcare organisations and provided as knowledge resources for day to day use and for induction of new physicians. A field which has gained particular attention is the choice of medications for psychiatric conditions. In the United Kingdom, guidelines or algorithms for this have been produced by most of the circa 500 primary care trusts, substantially all of the circa 100 secondary care psychiatric units and many of the circa 10 000 general practices. In the US, there is a national (federal) initiative to provide them for all states, and by 2005 six states were adapting the approach of the [[Texas Medication Algorithm Project]] or otherwise working on their production.

A grammar—the [[Arden syntax]]—exists for describing algorithms in terms of [[medical logic module]]s. An approach such as this should allow exchange of MLMs between doctors and establishments, and enrichment of the common stock of tools.

==Purpose==
The intended purpose of medical algorithms is to improve and standardize decisions made in the delivery of [[medical care]]. Medical algorithms assist in standardizing selection and application of treatment regimens, with algorithm [[automation]] intended to reduce potential introduction of errors.  Some attempt to predict the outcome, for example [[ICU scoring systems|critical care scoring systems]].

Computerized health diagnostics algorithms can provide timely clinical decision support, improve adherence to [[evidence-based medicine|evidence-based]] [[guideline (medical)|guidelines]], and be a resource for education and research. 

Medical algorithms based on best practice can assist everyone involved in delivery of standardized treatment via a wide range of clinical care providers. Many are presented as [[Clinical trial protocol|protocol]]s and it is a key task in training to ensure people step outside the protocol when necessary.  In our present state of knowledge, generating hints and producing guidelines may be less satisfying to the authors, but more appropriate.

==Cautions==
In common with most science and medicine, algorithms whose contents are not wholly available for scrutiny and open to improvement should be regarded with suspicion.  

[[Computation]]s obtained from medical algorithms should be compared with, and tempered by, clinical knowledge and [[physician]] judgment.

==See also==
* [[Consensus (medical)]]
* [[Evidence-based medicine]]
* [[Journal club]]
* [[Medical guideline]]
* [[Medical informatics]]
* [[Odds algorithm]]
* ''[[Treatment Guidelines from The Medical Letter]]''

==Further reading==
* {{cite journal| title=Automated Medical Algorithms:  Issues for Medical Errors| first1=Kathy A.| last1=Johnson| first2=John R.| last2=Svirbely| first3=M.G.| last3=Sriram| first4=Jack W.| last4=Smith| first5=Gareth |last5=Kantor| first6=Jorge Raul |last6=Rodriguez| journal=[[Journal of the American Medical Informatics Association]]| pmc=419420| doi=10.1197/jamia.M1228| volume=9| issue=6 Suppl 1| pages=s56-s57| date=November 2002}}

==External links==

* [http://www.alternativementalhealth.com/articles/fieldmanual.htm AlternativeMentalHealth.com] - 'Alternative Health Medical Evaluation Field Manual', Lorrin M. Koran, MD, [[Stanford University]] Medical Center (1991)

[[Category:Health informatics]]
[[Category:Algorithms]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:721
<=====title=====>:
Guideline execution engine
<=====text=====>:
A '''Guideline Execution Engine''' is a [[computer program]] which can interpret a [[guideline (medical)|clinical guideline]] represented in a computerized format and perform actions towards the user of an [[electronic medical record]].

A Guideline Execution Engine needs to communicate with a host [[Clinical information system]]. [[virtual Medical Record|vMR]] is one possible interface which can be used.

The engine's main function is to manage instances of executed guidelines of individual patients.

Delivering the inferred engine recommendations or impacts to the host Clinical information system  has to carefully respect current workflow of the clinicians (physicians, nurses, clerks, etc.)

== Architecture of Guideline Execution Engine ==
The following modules are generally needed for any engine

* interface to Clinical Information System
* new guidelines loading module
* guideline interpreter module
* clinical events parser
* alert/recommendations dispatch

== Guideline Interchange Format ==

The ''Guideline Interchange Format (GLIF)'' is computer representation format for [[clinical guideline]]s.<ref>{{cite web |url=http://mis.hevra.haifa.ac.il/~morpeleg/Intermed/ |title=Guideline Representation Page: GLIF 2.0, 3.4, 3.5 Specifications |work=Stanford University, School of Medicine, InterMed Collaboratory }}</ref> Represented guidelines can be executed using a guideline execution engine.

The format has several versions as it has been improved. In 2003 GLIF3 was introduced.

== Use of third party workflow engine as a guideline execution engine ==
Some commercial Electronic Health Record systems use a [[workflow engine]] to execute clinical guidelines. RetroGuide<ref name=eval>{{Cite journal 
| last1 = Huser | first1 = V. 
| last2 = Narus | first2 = S. P. 
| last3 = Rocha | first3 = R. A. 
| doi = 10.1016/j.jbi.2009.06.001 
| title = Evaluation of a flowchart-based EHR query system: A case study of RetroGuide☆ 
| journal = Journal of Biomedical Informatics 
| volume = 43 
| issue = 1 
| pages = 41–50 
| year = 2010 
| pmid = 19560553 
| pmc =2840619 
}}</ref> and HealthFlow<ref name=hf2010>{{citation|pmc=3079703|title=Implementation of workflow engine technology to deliver basic clinical decision support functionality|journal=BMC Med Res Methodol.|year=2011|volume= 11|page= 43|doi=10.1186/1471-2288-11-43|pmid=21477364|vauthors=Huser V, Rasmussen LV, Oberg R, Starren JB}}</ref>  are examples of such an approach.

== See also ==

*[[Electronic medical record]]
*[[Clinical practice guideline]]
*[[Medical algorithm]]
*[[Arden syntax]]
*[[Healthcare workflow]]
*[[Glif]]
*[[RetroGuide]]

== References ==
<references/>

== External links ==
*{{cite journal  |vauthors=Wang D, Peleg M, Tu SW, etal |title=Design and implementation of the GLIF3 guideline execution engine |journal=J Biomed Inform |volume=37 |issue=5 |pages=305–18 |date=October 2004 |pmid=15488745 |doi=10.1016/j.jbi.2004.06.002 |url=http://linkinghub.elsevier.com/retrieve/pii/S1532046404000668}} [http://bmir.stanford.edu/file_asset/index.php/940/BMIR-2004-1008.pdf (PDF)]
*{{cite journal  |vauthors=Ram P, Berg D, Tu S, etal |title=Executing clinical practice guidelines using the SAGE execution engine |journal=Stud Health Technol Inform |volume=107 |issue=Pt 1 |pages=251–5 |year=2004 |pmid=15360813 }}
*{{cite journal |vauthors=Tu SW, Campbell J, Musen MA |title=The structure of guideline recommendations: a synthesis |journal=AMIA Annu Symp Proc |volume= |issue= |pages=679–83 |year=2003 |pmid=14728259 |pmc=1480008 }} [http://bmir.stanford.edu/file_asset/index.php/1511/BMIR-2003-0966.pdf (PDF)]
*{{cite journal |vauthors=Tu SW, Musen MA |title=A flexible approach to guideline modeling |journal=Proc AMIA Symp |volume= |issue= |pages=420–4 |year=1999 |pmid=10566393 |pmc=2232509 }} [http://bmir.stanford.edu/file_asset/index.php/211/BMIR-1999-0789.pdf (PDF)]

[[Category:Health informatics]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:724
<=====title=====>:
Category:Classification systems
<=====text=====>:
'''Classification systems''' are [[system]]s with a distribution of classes created according to common relations or affinities.
{{Commons cat|Classification systems}}
See also: 
* [[Controlled vocabulary]]
* [[Scientific classification (disambiguation)]]
* [[Taxonomy (biology)|Taxonomy]]

[[Category:Classification|systems]]
[[Category:Conceptual systems]]
[[Category:Formal sciences]]
[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Information systems]]
<=====doc_Id=====>:727
<=====title=====>:
Findability
<=====text=====>:
'''Findability''' is a term for the ease with which information contained on a [[website]] can be found, both from outside the website (using [[search engine]]s and the like) and by users already on the website.<ref>{{Cite journal|url = |title = Information architecture|author1=Jacob, Elin K.  |author2=Loehrlein, Aaron|date = 2009|journal = Annual Review of Information Science and Technology|doi = 10.1002/aris.2009.1440430110|pmid = |access-date = |publication-date = }}</ref> Although findability has relevance outside the [[World Wide Web]], the term is usually used in that context. Most relevant websites do not come up in the top results because designers and engineers do not cater to the way ranking algorithms work currently.<ref>{{Cite book|title=Ambient Findability|last=Morville|first=Peter|publisher=Oreilly|year=2005|isbn=978-0-596-00765-2|location=Sebastopol, CA|pages=|quote=|via=}}</ref> Its importance can be determined from the first law of [[e-commerce]], which states "If the user can’t find the product, the user can’t buy the product."<ref>{{Cite web|url = http://www.nngroup.com/reports/ecommerce|title = E-Commerce user experience: High-level strategy, Nielsen Norman Group|date = 2001|accessdate = |website = |publisher = }}</ref> As of December 2014, out of 10.3 billion monthly [[Google]] searches by Internet users in the [[United States]], an estimated 78% are made to research products and services online.<ref>{{Cite web|url = http://www.cmocouncil.org/facts-stats-categories.php?category=internet-marketing|title = Internet Marketing|date = |accessdate = |website = |publisher = }}</ref>

Findability encompasses aspects of [[information architecture]], [[user interface design]], [[accessibility]] and [[search engine optimization]] (SEO), among others.

==Introduction==
Findability is similar to, but different from [[discoverability]], which is defined as the ability of something, especially a piece of content or information, to be found. It is different from web search in that the word 'find' refers to locating something in a known space while 'search' is in an unknown space or not in an expected location.<ref name="every-page" />

Mark Baker, the author of "Every Page is Page One",<ref name="every-page">{{Cite book|title = Every Page is Page One|last = Baker|first = Mark|publisher = XML Press|year = 2013|isbn = 978-1937434281|location = |pages = }}</ref> mentions that findability "is a content problem, not a search problem".<ref>{{cite web|last1=Baker|first1=Mark|title=Findability is a Content Problem, not a Search Problem|url=http://everypageispageone.com/2013/05/28/findability-is-a-content-problem-not-a-search-problem/|website=Every Page is Page One|accessdate=2015-04-25}}</ref> Even when the right content is present, users often find themselves deep within the content of a website but not in the right place. He further adds that findability is intractable, perfect findability is unattainable, but we need to focus on reducing the effort for finding that a user would have to do for themselves.

Findability can be divided into external findability and on-site findability, based on where the customers need to find the information.

==History==
[[Heather Lutze]] is thought to have created the term in the early 2000s.<ref>{{cite web | url=http://www.huffingtonpost.com/liz-wainger/the-shtickiness-factor_b_3471675.html | title=The Shtickiness Factor |last1=Wainger | first1=Liz | publisher=The Huffington Post | date=20 June 2013 | accessdate=12 September 2013}}</ref> The popularization of the term "findability" for the Web is usually credited to [[Peter Morville]].{{citation needed|date=April 2015}} In 2005 he defined it as: "the ability of users to identify an appropriate Web site and navigate the pages of the site to discover and retrieve relevant information resources", though it appears to have been first coined in a public context referring to the web and information retrieval by Alkis Papadopoullos in a 2005 article entitled "Findability".<ref>{{cite journal|author=Alkis Papadopoulos|title=The Key to Enterprise Search|journal=KM World|date=April 1, 2005|url=http://news-business.vlex.com/vid/findability-key-to-enterprise-search-62406335}}</ref><ref>Though the word has been used to mean "ease of finding information" since at least 1943: see Urban A. Avery, "The 'Findability' of the Law", ''Chicago Bar Record'' '''24''':272, April 1943, reprinted in the ''Journal of the American Judicature Society'' '''27''':25 [http://heinonline.org/HOL/LandingPage?collection=journals&handle=hein.journals/judica27&div=12&id=&page=]</ref>

==External findability==
External findability is the domain of [[internet marketing]] and [[Search engine optimization|search engine optimization (SEO)]] tactics. Several factors affect external findability:<ref>{{cite web|title=Findability Factors Found|url=http://www.econtentstrategies.com/Article_FindabilityFactorsFoundFinal_EContent_200701.pdf}}</ref>
#''Search Engine Indexing'': As the very first step, webpages need to be found by indexing crawler in order to be shown in the search results. It would be helpful to avoid factors that may lead to webpages being ignored by indexing crawlers. Those factors may include elements that require user interaction, such as entering log-in credentials. Algorithms for indexing vary by the search engine which means the number of webpages of a website successfully being indexed may be very different between Google and Yahoo!'s search engines. Also, in countries like [[China]], [[Great Firewall|government policies]] could significantly influence the indexing algorithms. In this case, local knowledge about laws and policies could be valuable.<ref>{{cite web|title=Online Marketing in China|url=http://chineseseoshifu.com/china-online-marketing/}}</ref>
#''Page Descriptions in Search Results'': Now that the webpages are successfully indexed by web crawlers and show in the search results with decent ranking, the next step is to attract customers to click the link to the web pages. However, the customers can't see the whole web pages at this point; they can only see an excerpt of the webpage's content and metadata. Therefore, displaying meaningful information in a limited space, usually a couple of sentences, in search results is important for increasing click traffic of the webpages, and thus the findability of the web content on your webpages. 
#''Keyword Matching'': At a semantic level, terminology used by the searcher and the content producer be different. Bridging the gap between the terms used by customers and developers is helpful for making web content more findable to more potential content consumers.

==On-site findability==
On-site findability is concerned with the ability of a potential customer to find what they are looking for within a specific site. More than 90 percent of customers use internal searches in a website compared to browsing. Of those, only 50 percent find what they are looking for.<ref name="findability-solution">{{cite web|title=The Findability Solution|url=http://marriottschool.byu.edu/strategy/docs/TheFindabilitySolution-StrategyWhitePaper.pdf}}</ref> Improving the quality of on-site searches highly improves the business of the website. Several factors affect findability on a website:

#''Site search'': If searchers within a site do not find what they are looking for, they tend to leave rather than browse through the website. Users who had successful site searches are twice as likely to ultimately convert.<ref name="findability-solution" />
#''Related Links and Products'': User experience can be enhanced by trying to understand the needs of the customer and provide suggestions for other, related information.
#''Site Match to Customer Needs and Preferences'': Site design, content creation, and recommendations are major factors for affecting the customer experience.
#''Cross Device Experience'': With the rise of computing devices other than desktop computers, companies like Microsoft have focused more on smoothing the transition between devices to increase customer satisfaction.<ref>{{Cite web|url = http://research.microsoft.com/en-us/projects/courier/|title = Cross-Device User Experiences|date = |accessdate = |website = |publisher = }}</ref>

==Evaluation and measures==
'''Baseline Findability''' is the existing findability before changes are made in order to improve it. This is measured by participants who represent the customer base of the website, who try to locate a sample set of items using the existing navigation of the website.<ref>{{Cite book|title = Customer Analytics For Dummies|last = Sauro|first = Jeff|publisher = John Wiley & Sons|year = |isbn = 978-1-118-93759-4|location = |pages = |url = http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118937597.html}}</ref><ref>{{Cite web|url = http://www.measuringu.com/blog/measure-findability.php|title = How to Measure Findability|date = |accessdate = |website = |publisher = }}</ref>

In order to evaluate how easily information can be found by searching a site using a search engine or information retrieval system, [[retrievability]] measures were developed, and similarly, navigability measures now measure ease of information access through browsing a site (e.g. [[PageRank]], MNav, InfoScent (see [[Information foraging|Information Foraging]]), etc.).

Findability also can be evaluated via the following techniques:
* [[Usability testing]]: Conducted to find out how and why users navigate through a website to accomplish tasks.
* [[Tree testing]]: An [[information architecture]] based technique, to determine if critical information can be found on the website.
* [[Card sorting|Closed card sorting]]: A usability technique based on information architecture, for evaluating the strength of categories.
* [[Click testing]]: Accounts for the implicit data collected through clicks on the user interface.<ref>{{Cite web|url = http://www.nngroup.com/articles/navigation-ia-tests/|title = Low Findability and Discoverability: Four Testing Methods to Identify the Causes|date = July 6, 2014|accessdate = |website = |publisher = }}</ref>

==Beyond findability==
Findability Sciences defines a findability index in terms of each user's influence, context, and sentiments. For seamless search, current websites focus on a combination of structured hypertext-based information architectures and rich Internet application-enabled visualization techniques.<ref>{{Cite journal|url = http://journalofia.org/volume2/issue1/03-spagnolo/|title = Beyond Findability - Search-Enhanced Information Architecture for Content-Intensive Rich Internet Applications|date = 2010|journal = |doi = |pmid = |access-date = }}</ref>

==See also==
* [[Information retrieval]]
* [[Knowledge mining]]
* [[Search engine optimization]]
* [[Subject (documents)]]
* [[Usability]]
* [[User interface]]

==References==
{{reflist}}

==Further reading==
* Morville, P. (2005) Ambient findability. Sebastopol, CA: O'Reilly
* Wurman, R.S. (1996). Information architects. New York: Graphis.

==External links==
* [http://findability.org/ findability.org]: a collection of links to people, software, organizations, and content related to findability
* [http://semanticstudios.com/publications/semantics/000007.php The age of findability] (article)
* [http://www.useit.com/alertbox/search-keywords.html Use Old Words When Writing for Findability] (article on the findability impact of a site's choice of words)
* [http://buildingfindablewebsites.com/ Building Findable Websites: Web Standards SEO and Beyond] (book)
* [http://www.FindabilityFormula.com The Findability Formula: The Easy, Non-Technical Guide to Search Engine Marketing by Heather Lutze]

[[Category:Web design]]
[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Information architecture]]
<=====doc_Id=====>:730
<=====title=====>:
Brinkler classification
<=====text=====>:
'''Brinkler Classification''' is the [[library classification]] system  of [[Bartol Brinkler]] described in his article "The Geographical Approach to Materials in the Library of Congress Subject Headings".<ref>Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings: report of a study project. s.l.: s.n., 1960. [Accession No: {{OCLC|3853830}}].</ref> The geographical aspect of a subject may be conveyed through three types of headings labeled A, B, and C. Heading A uses a primary topical description with geographical subdivisions (e.g. Art—Paris).  Type B uses a place-name for the main heading with a topical subdivision (e.g. Paris—Description). C headings use a geographical description of a phrase (e.g. Paris Literature).  

Brinkler explores what type of heading is more useful to a [[patron]], and he finds that it depends on the level of familiarity a patron has with a topic and what approach they take when searching for resources on their topic. Ideally readers will either be looking for everything on a particular topic, or everything regarding a particular place. Bartol Brinkler investigates a system of classification that will best serve these two ideal types of patrons.  He finds working with Type A headings will best assist a patron who is more topic oriented, while using Type B headings is preferable for those who are primarily interested in one place. 

However this is problematic in practice. One possibility is to assign Type A and Type B headings to every resource, but the cataloguing cost would be high.  A system that aids readers regardless of their approach to a topic involves using cross-references (e.g. Canada—Botany, See Botany—Canada).  Admitting that see and see also references would require more work on the part of librarians, Bartol Brinkler notes that librarians must keep in mind "...readers do not have the same knowledge [of classification] and do need all the help they can get..."{{Citation needed|date=March 2008}}

==References==
{{reflist}}
*Brinkler, Bartol. The geographical approach to materials in the Library of Congress subject headings. Library Resources & Technical Services 6, no. 1 (Winter 1962): 49-64.

==External links==
*[http://hcl.harvard.edu/libraries/#widener  Harvard University. Widener Library.]
*[http://www.loc.gov/catdir/cpso/lcco/lcco.html Library of Congress Classification Outline.]
*[http://www.princeton.edu/~paw/memorials/memorials_1930s/memorials_1937.html Princeton Alumni Weekly: Memorials 1937.]

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]
<=====doc_Id=====>:733
<=====title=====>:
National Library of Medicine classification
<=====text=====>:
The '''National Library of Medicine (NLM) classification system''' is a [[Library classification|library indexing system]] covering the fields of [[medicine]] and preclinical basic sciences. The [[National Library of Medicine|NLM]] classification is patterned after the [[Library of Congress Classification|Library of Congress (LC) Classification system]]: [[Alphabet|alphabetical letters ]]  denote broad subject categories which are subdivided by numbers.<ref name=NLM_Factsheet>{{cite web | title =Fact Sheet: NLM Classification | work = | url = https://www.nlm.nih.gov/pubs/factsheets/nlmclassif.html | date = 2005-07-15 | accessdate = 2007-05-12}}</ref> For example, ''QW 279'' would indicate a book on an aspect of [[microbiology]] or [[immunology]].

The one- or two-letter alphabetical codes in the NLM classification use a limited range of letters: only QS–QZ and W–WZ. This allows the NLM system to co-exist with the larger LC coding scheme as neither of these ranges are used in the LC system. There are, however, three pre-existing codes in the LC system which overlap with the NLM: ''Human Anatomy'' (QM), ''Microbiology'' (QR), and ''Medicine'' (R). To avoid further confusion, these three codes are not used in the NLM.

The headings for the individual ''schedules'' (letters or letter pairs) are given in brief form (e.g., QW - ''Microbiology and Immunology''; WG - ''Cardiovascular System'') and together they provide an outline of the subjects covered by the NLM classification. Headings are interpreted broadly and include the [[Physiology|physiological]] system, the specialties connected with them, the regions of the body chiefly concerned and subordinate related fields. The NLM system is [[hierarchical]], and within each schedule, division by [[Organ (anatomy)|organ]] usually has priority. Each main schedule, as well as some sub-sections, begins with a group of form numbers ranging generally from 1–49 which  classify materials by publication type, e.g., [[Dictionary|dictionaries]], [[atlas]]es, laboratory manuals, etc.

The main schedules QS-QZ, W-WY, and WZ (excluding the range WZ 220–270)  classify works published after 1913; the 19th century schedule is used for works published 1801-1913; and WZ 220-270 is used to provide century groupings for works published before 1801.

==Overview of the NLM Classification categories==

'''Preclinical Sciences'''
* QS Human Anatomy
* QT Physiology
* QU Biochemistry
* QV Pharmacology
* QW Microbiology & Immunology
* QX Parasitology
* QY Clinical Pathology
* QZ Pathology

'''Medicine and Related Subjects'''

* W  Health Professions
* WA Public Health
* WB Practice of Medicine
* WC Communicable Diseases
* WD Disorders of Systemic, Metabolic, or Environmental Origin, etc.
* WE Musculoskeletal System
* WF Respiratory System
* WG Cardiovascular System
* WH Hemic and Lymphatic Systems
* WI Digestive System
* WJ Urogenital System
* WK Endocrine System
* WL Nervous System
* WM Psychiatry
* WN Radiology. Diagnostic Imaging
* WO Surgery
* WP Gynecology
* WQ Obstetrics
* WR Dermatology
* WS Pediatrics
* WT Geriatrics. Chronic Disease
* WU Dentistry. Oral Surgery
* WV Otolaryngology
* WW Ophthalmology
* WX Hospitals & Other Health Facilities
* WY Nursing
* WZ History of Medicine
* 19th Century Schedule

==See also==
*[[Dewey Decimal Classification]]
*[[Colon Classification]]
*[[Library of Congress Classification]]
*[[Universal Decimal Classification]]

==References==
<!-- ---------------------------------------------------------------
See http://en.wikipedia.org/wiki/Wikipedia:Footnotes for a
discussion of different citation methods and how to generate
footnotes using the<ref> & </ref>  tags and the {{Reflist}} template
-------------------------------------------------------------------- -->
{{Reflist}}

{{refbegin}}
* {{USGovernment|sourceURL=[http://wwwcf.nlm.nih.gov/class/ The NLM Classification 2005]}}
{{refend}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
<=====doc_Id=====>:736
<=====title=====>:
Attribute-value system
<=====text=====>:
An '''attribute-value system''' is a basic [[knowledge representation]] framework comprising a table with columns designating "attributes" (also known as "properties", "predicates," "features," "dimensions," "characteristics", "[[Field (computer science)|fields]]", "headers" or "independent variables" depending on the context) and "[[Row (database)|rows]]" designating "objects" (also known as "entities," "instances," "exemplars," "elements", "[[Record (computer science)|records]]" or "dependent variables"). Each table cell therefore designates the value (also known as "state") of a particular attribute of a particular object.

== Example of attribute-value system==
Below is a sample attribute-value system. It represents 10 objects (rows) and five features (columns). In this example, the table contains only integer values. In general, an attribute-value system may contain any kind of data, numeric or otherwise. An attribute-value system is distinguished from a simple "feature list" representation in that each feature in an attribute-value system may possess a range of values (e.g., feature <math>P_{1}</math> below, which has domain of {0,1,2}), rather than simply being ''present'' or ''absent'' {{Harv|Barsalou|Hale|1993}}.

:{| class="wikitable" style="text-align:center; width:30%" border="1"
|+ Sample Attribute-Value System
! Object !! <math>P_{1}</math> !! <math>P_{2}</math> !! <math>P_{3}</math> !! <math>P_{4}</math> !! <math>P_{5}</math>
|-
! <math>O_{1}</math>
| 1 || 2 || 0 || 1 || 1
|-
! <math>O_{2}</math>
| 1 || 2 || 0 || 1 || 1
|-
! <math>O_{3}</math>
| 2 || 0 || 0 || 1 || 0
|-
! <math>O_{4}</math>
| 0 || 0 || 1 || 2 || 1
|-
! <math>O_{5}</math>
| 2 || 1 || 0 || 2 || 1
|-
! <math>O_{6}</math>
| 0 || 0 || 1 || 2 || 2
|-
! <math>O_{7}</math>
| 2 || 0 || 0 || 1 || 0
|-
! <math>O_{8}</math>
| 0 || 1 || 2 || 2 || 1
|-
! <math>O_{9}</math>
| 2 || 1 || 0 || 2 || 2
|-
! <math>O_{10}</math>
| 2 || 0 || 0 || 1 || 0
|}

== Other terms used for "attribute-value system"==
Attribute-value systems are pervasive throughout many different literatures, and have been discussed under many different names:
*''Flat data''
*''[[Spreadsheet]]''
*''Attribute-value system'' (Ziarko & Shan 1996)
*''Information system'' ([[Zdzislaw Pawlak|Pawlak]] 1981)
*''Classification system'' (Ziarko 1998)
*''Knowledge representation system'' (Wong & Ziarko 1986)
*''Information table'' (Yao & Yao 2002)
*''Object-predicate table'' (Watanabe 1985)
*''Aristotelian table'' (Watanabe 1985)
*''Simple frames'' {{Harv|Barsalou|Hale|1993}}
*''[[First normal form]]'' database

==See also==
*[[Bayes networks]]
*[[Entity–attribute–value model]]
*[[Joint distribution]]
*[[Knowledge representation]]
*[[wikibooks:Optimal Classification|Optimal classification]] (in Wikibooks)
*[[Rough set]]
*[[Triplestore]]

== References ==
* {{Cite book
 | last1=Barsalou
 | given1=Lawrence W.
 | surname2=Hale
 | given2=Christopher R.
 | year= 1993
 | chapter=Components of conceptual representation: From feature lists to recursive frames
 | editor=Iven Van Mechelen |editor2=James Hampton |editor3=Ryszard S. Michalski |editor4=Peter Theuns
 | title=Categories and Concepts: Theoretical Views and Inductive Data Analysis
 | pages=97–144
 | edition=
 | publisher=Academic Press
 | place=London
 | url=
 | accessdate=
 | ref=harv
 | postscript=<!--None-->}}
*{{cite book
  | last = Pawlak
  | first = Zdzisław
  | authorlink = Zdzislaw Pawlak
  | title = Rough sets: Theoretical Aspects of Reasoning about Data
  | publisher = Kluwer
  | year = 1991
  | location = Dordrecht}}
*{{cite journal
  | last = Ziarko
  | first = Wojciech 
  | last2 = Shan
  | first2 = Ning
  | title = A method for computing all maximally general rules in attribute-value systems
  | journal = Computational Intelligence
  | volume = 12
  | issue = 2
  | pages = 223–234
  | year = 1996
  | doi = 10.1111/j.1467-8640.1996.tb00260.x
  | ref = harv}}
*{{cite journal
  | last = Pawlak
  | first = Zdzisław
  | last2 = Shan
  | first2 = Ning
  | title = Information systems: Theoretical foundations
  | journal = Information Systems
  | volume = 6
  | issue = 3
  | pages = 205–218
  | year = 1981
  | doi = 10.1016/0306-4379(81)90023-5
  | ref = harv}}
*{{cite journal
  | last = Wong
  | first = S. K. M.
  | last2 = Ziarko
  | first2 = Wojciech
  | last3 = Ye
  | first3 = R. Li
  | title = Comparison of rough-set and statistical methods in inductive learning
  | journal = International Journal of Man-Machine Studies
  | volume = 24
  | pages = 53–72
  | year = 1986
  | ref = harv}}
*{{cite conference
  | first = Yao
  | last = J. T.
  |author2=Yao, Y. Y.
  | title = Induction of classification rules by granular computing
  | booktitle = Proceedings of the Third International Conference on Rough Sets and Current Trends in Computing (TSCTC'02)
  | pages = 331–338
  | publisher = Springer-Verlag
  | year = 2002
  | location = London, UK}}
*{{cite book
  | last = Watanabe
  | first = Satosi
  | title = Pattern Recognition: Human and Mechanical
  | publisher = John Wiley & Sons
  | year = 1985
  | location = New York}}
*{{cite conference
  | first = Wojciech
  | last = Ziarko
  | title = Rough sets as a methodology for data mining
  | booktitle = Rough Sets in Knowledge Discovery 1: Methodology and Applications
  | pages = 554–576
  | editor    = Polkowski, Lech |editor2=Skowron, Andrzej
  | publisher = Physica-Verlag
  | year = 1998
  | location = Heidelberg}}

[[Category:Knowledge representation]]
[[Category:Specific models]]
<=====doc_Id=====>:739
<=====title=====>:
MultiNet
<=====text=====>:
'''Multilayered extended semantic networks''' ('''MultiNets''') are both a [[knowledge representation]] paradigm and a language for meaning representation of [[natural language]] expressions that has been developed by Prof. Dr. Hermann Helbig on the basis of earlier [[Semantic network|Semantic Networks]].

MultiNet is claimed to be one of the most comprehensive and thoroughly described knowledge representation systems. It specifies conceptual structures by means of about 140 predefined relations and functions, which are systematically characterized and underpinned by a formal [[axiomatic]] apparatus. Apart from their relational connections, the concepts are embedded in a multidimensional space of layered attributes and their values. Another characteristic of MultiNet discerning it from simple semantic networks is the possibility to encapsulate whole partial networks and represent the resulting conceptual capsule as a node of higher order, which itself can be an argument of relations and functions. MultiNet has been used in practical [[Natural language processing|NLP]] applications such as natural language interfaces to the Internet or [[question answering]] systems over large semantically annotated [[Corpus linguistics|corpora]] with millions of sentences. MultiNet is also a cornerstone of the commercially available search engine SEMPRIA Search, where it is used for the description of the computational lexicon and the background knowledge, for the syntactic-semantic analysis, for logical answer finding, as well as for the generation of natural language answers.

MultiNet is supported by a set of [[software tools]] and has been used to build large semantically based computational lexicons. The tools include a semantic interpreter WOCADI which translates natural language expressions (phrases, sentences, texts) into formal MultiNet expressions, a workbench MWR+ for the knowledge engineer (comprising modules for automatic knowledge acquisition and reasoning), and a workbench LIA+ for the computer [[lexicographer]] supporting the creation of large semantically based computational lexica.

== References ==
* Hermann Helbig, ''Die semantische Struktur natürlicher Sprache - Wissensrepräsentation mit MultiNet''. Springer, Heidelberg, 2001.
* Hermann Helbig. ''Knowledge Representation and the Semantics of Natural Language'', (2006) Springer, Berlin
* Sven Hartrumpf, Hermann Helbig, Johannes Leveling, Rainer Osswald. ''An Architecture for Controlling Simple Language in Web Pages'', eMinds: International Journal on Human-Computer Interaction, 1(2), 2006.

== External links ==
* [http://pi7.fernuni-hagen.de/forschung/multinet/multinet_en.html MultiNet] and its software environment

[[Category:Semantic Web]]
[[Category:Knowledge representation]]


{{software-stub}}
<=====doc_Id=====>:742
<=====title=====>:
Ramification problem
<=====text=====>:
{{Cleanup|date=April 2011}}

In [[philosophy]] and [[artificial intelligence]] (especially, knowledge based systems), the '''ramification problem''' is concerned with the indirect consequences of an action. It might also be posed as ''how to represent what happens implicitly due to an action'' or how to control the secondary and tertiary effects of an action. It is strongly connected to, and is opposite the [[qualification problem|qualification side]] of, the [[frame problem]].

Limit theory helps in [[operational]] usage. For instance, in [[Knowledge-based engineering|KBE]] derivation of a populated design (geometrical objects, etc., similar concerns apply in shape theory), equivalence assumptions allow convergence where potentially large, and perhaps even computationally indeterminate, solution sets are handled deftly. Yet, in a chain of computation, downstream events may very well find some types of results from earlier resolutions of '''ramification''' as problematic for their own algorithms.

==See also==
*[[Non-monotonic logic]]
*[[Ramification (mathematics)]]

==External links==
*Nikos Papadakis [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/proceedings/&toc=comp/proceedings/ictai/2002/1849/00/1849toc.xml&DOI=10.1109/TAI.2002.1180791 "Actions with Duration and Constraints: the Ramification Problem in Temporal Databases"] IEEE ICTAI'02
*Deepak Kumar "[http://blackcat.brynmawr.edu/~dkumar/UGAI/planning.html AI Planning]" Bryn Mawr College

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Epistemology]]


{{epistemology-stub}}
<=====doc_Id=====>:745
<=====title=====>:
Category:Microformats
<=====text=====>:
{{Cat main|Microformat}}



[[Category:Knowledge representation]]
[[Category:Metadata publishing]]
[[Category:Metadata]]
[[Category:Semantic HTML]]
[[Category:Semantic Web]]
[[Category:Domain-specific knowledge representation languages]]
<=====doc_Id=====>:748
<=====title=====>:
Fuzzy cognitive map
<=====text=====>:
[[File:FCMdrug520.png|thumb|right|Rod Tabers FCM depicting eleven factors of the American drug market]]
A '''fuzzy cognitive map''' is a [[cognitive map]] within which the relations between the elements (e.g. concepts, events, project resources) of a "mental landscape" can be used to compute the "strength of impact" of these elements.  Fuzzy cognitive maps were introduced by [[Bart Kosko]].<ref>{{cite journal|author=[[Bart Kosko]]|title=''Fuzzy Cognitive Maps''|journal=International Journal of Man-Machine Studies|volume=24|date=1986|pages=65-75|url=http://sipi.usc.edu/~kosko/FCM.pdf|format=PDF}}</ref><ref>[http://sipi.usc.edu/~kosko/Virtual_Worlds_FCM.pdf] {{dead link|date=January 2017}}</ref>  Ron Axelord introduced Cognitive Maps as a formal way of representing social scientific knowledge and modeling [[decision making]] in social and political systems. Then brought in the computation [[fuzzy logic]].

==Details==
Fuzzy cognitive maps are signed fuzzy [[directed graph|digraph]]s.  They may look at [[first blush]] like [[Hasse diagrams]] but they are not.
[[Spreadsheet]]s or tables are used to map FCMs into [[matrix (Mathematics)|matric]]es for further computation.<ref>{{cite web|url=http://www.FCMappers.net/joomla/index.php?option=com_content&view=article&id=52&Itemid=53 |title=FCMapper - our Fuzzy Cognitive Mapping Software Solution |website=Fcmappers.net |date=2016-01-27 |accessdate=2017-01-09}}</ref><ref>{{cite web|url=http://www.ochoadeaspuru.com/fuzcogmap/index.php |title=Fuzzy Cognitive Maps |website=Ochoadeaspuru.com |date= |accessdate=2017-01-09}}</ref><ref>{{cite web|url=http://jfcm.megadix.it/ |title=JFCM - Java Fuzzy Cognitive Maps |website=Jfcm.megadix.it |date= |accessdate=2017-01-09}}</ref>
FCM is a technique used for causal knowledge acquisition and representation, it supports causal knowledge reasoning process and belong to the neuro-fuzzy system that aim at solving decision making problems, modeling and simulate [[complex system]]s. 
Learning algorithms  have been proposed for training and updating FCMs weights mostly based on ideas coming from the field of [[Artificial Neural Network]]s. Adaptation and learning methodologies used to adapt the FCM model and adjust its weights.  Kosko and Dickerson (Dickerson & Kosko, 1994) suggested the Differential [[Hebbian Learning]] (DHL) to train FCM.<ref>{{cite web|url=http://home.eng.iastate.edu/~julied/publications/FCM96.pdf |title=IEEEBook8.dvi |website=Home.eng.iastate.edu |format=PDF |date= |accessdate=2017-01-09}}</ref> There have been proposed algorithms based on the initial Hebbian algorithm;<ref>{{cite journal |doi=10.1016/j.ijar.2004.01.001 |title=Active Hebbian learning algorithm to train fuzzy cognitive maps |journal=International Journal of Approximate Reasoning |volume=37 |issue=3 |page=219 |year=2004 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}</ref> others algorithms come from the field of [[genetic algorithms]], [[swarm intelligence]]<ref>{{cite journal |doi=10.1007/s10844-005-0864-9 |title=Fuzzy Cognitive Maps Learning Using Particle Swarm Optimization |journal=Journal of Intelligent Information Systems |volume=25 |page=95 |year=2005 |last1=Papageorgiou |first1=Elpiniki I. |last2=Parsopoulos |first2=Konstantinos E. |last3=Stylios |first3=Chrysostomos S. |last4=Groumpos |first4=Petros P. |last5=Vrahatis |first5=Michael N. }}</ref> and  [[evolutionary computation]].<ref>{{cite book |doi=10.1109/FUZZY.2005.1452465 |chapter=Evolutionary Development of Fuzzy Cognitive Maps |title=The 14th IEEE International Conference on Fuzzy Systems, 2005. FUZZ '05 |pages=619– |year=2005 |last1=Stach |first1=W. |last2=Kurgan |first2=L. |last3=Pedrycz |first3=W. |last4=Reformat |first4=M. |isbn=0-7803-9159-4 }}</ref> [[Learning algorithms]] are used to overcome the shortcomings that the traditional FCM present i.e. decreasing the human intervention by suggested automated FCM candidates; or by activating only the most relevant concepts every execution time; or by making models more transparent and dynamic.<ref>{{cite journal |doi=10.1016/j.ijhcs.2006.02.009 |title=Unsupervised learning techniques for fine-tuning fuzzy cognitive map causal links |journal=International Journal of Human-Computer Studies |volume=64 |issue=8 |page=727 |year=2006 |last1=Papageorgiou |first1=Elpiniki I. |last2=Stylios |first2=Chrysostomos |last3=Groumpos |first3=Peter P. }}</ref>

Fuzzy cognitive maps (FCMs) have gained considerable research interest due to their ability in representing structured knowledge and model complex systems in various fields. This growing interest led to the need for enhancement and making more reliable models that can better represent real situations.
A first simple application of FCMs is described in a book<ref name="confusion">William R. Taylor: ''[http://www.americanconfusion.com/?p=122 Lethal American Confusion] (How Bush and the Pacifists Each Failed in the War on Terrorism)'', 2006, ISBN 0-595-40655-6 (FCM application in chapter 14) {{webarchive |url=https://web.archive.org/web/20070930103802/http://www.americanconfusion.com/?p=122 |date=September 30, 2007 }}</ref> of William R. Taylor, where the war in Afghanistan and Iraq is analyzed. And in [[Bart Kosko]]'s book ''Fuzzy Thinking'',<ref name="FuzzyThinking">Bart Kosko: ''Fuzzy Thinking'', 1993/1995, ISBN 0-7868-8021-X (Chapter 12: Adaptive Fuzzy Systems)''</ref> several Hasse diagrams illustrate the use of FCMs. As an example, one FCM quoted from Rod Taber<ref name="Drugs">Rod Taber: ''Knowledge Processing with Fuzzy Cognitive Maps'', Expert Systems with Applications, vol. 2, no. 1, 83-87, 1991 ([[:de:Bild:FCMdrug520.png|Hasse diagram]] in German Wikipedia)</ref> describes 11 factors of the American cocaine market and the relations between these factors. For computations, Taylor uses pentavalent logic (scalar values out of {-1,-0.5,0,+0.5,+1}). That particular map of Taber uses [[trivalent logic]] (scalar values out of {-1,0,+1}). Taber et al.  also illustrate the dynamics of map fusion and give a theorem on the convergence of combination in a related article <ref name='Medical'>{{cite journal |doi=10.1002/int.20185 |title=Quantization effects on the equilibrium behavior of combined fuzzy cognitive maps |journal=International Journal of Intelligent Systems |volume=22 |issue=2 |page=181 |year=2007 |last1=Taber |first1=Rod |last2=Yager |first2=Ronald R. |last3=Helgason |first3=Cathy M. }}</ref>

While applications in social sciences<ref name="confusion"/><ref name="FuzzyThinking"/><ref name="Drugs"/><ref>Costas Neocleous, Christos Schizas, Costas Yenethlis: ''[http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf Fuzzy Cognitive Models in Studying Political Dynamics  - The case of the Cyprus problem]'' {{webarchive |url=https://web.archive.org/web/20070929055849/http://www.congreso-info.cu/UserFiles/File/Info/Info2006/Ponencias/271.pdf |date=September 29, 2007 }}</ref> introduced FCMs to the public, they are used in a much wider range of applications, which all have to deal with creating and using models<ref>Chrysostomos D. Stylios, Voula C. Georgopoulos, Peter P. Groumpos: ''[http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF The Use of Fuzzy Cognitive Maps in Modeling Systems]'' {{webarchive |url=https://web.archive.org/web/20110720011915/http://med.ee.nd.edu/MED5/PAPERS/067/067.PDF |date=July 20, 2011 }}</ref> of uncertainty and complex processes and systems. Examples:
*In business FCMs can be used for product planning.<ref>Antonie Jetter: ''Produktplanung im Fuzzy Front End'', 2005, ISBN 3-8350-0144-2</ref>
*In economics, FCMs support the use of [[game theory]] in more complex settings.<ref>Vesa A. Niskanen: ''[http://www.ijicic.org/fic04-20.pdf Application of Fuzzy Linguistic Cognitive Maps to Prisoner's Dilemma]'', 2005, ICIC International pp. 139-152, ISSN 1349-4198 {{webarchive |url=https://web.archive.org/web/20070929040145/http://www.ijicic.org/fic04-20.pdf |date=September 29, 2007 }}</ref>
* In Medical applications to model systems, provide diagnosis,<ref>{{cite journal |doi=10.1016/S0933-3657(02)00076-3 |pmid=14656490 |title=A fuzzy cognitive map approach to differential diagnosis of specific language impairment |journal=Artificial Intelligence in Medicine |volume=29 |issue=3 |pages=261–78 |year=2003 |last1=Georgopoulos |first1=Voula C |last2=Malandraki |first2=Georgia A |last3=Stylios |first3=Chrysostomos D }}</ref> develop [[decision support systems]]<ref>{{cite journal |doi=10.1109/TBME.2003.819845 |pmid=14656062 |title=An integrated two-level hierarchical system for decision making in radiation therapy based on fuzzy cognitive maps |journal=IEEE Transactions on Biomedical Engineering |volume=50 |issue=12 |pages=1326–39 |year=2003 |last1=Papageorgiou |first1=E.I. |last2=Stylios |first2=C.D. |last3=Groumpos |first3=P.P. }}</ref> and [[medical assessment]].<ref>{{cite book |doi=10.1007/978-3-319-11457-6_18 |chapter=Supervisory Fuzzy Cognitive Map Structure for Triage Assessment and Decision Support in the Emergency Department |title=Simulation and Modeling Methodologies, Technologies and Applications |volume=319 |pages=255–69 |series=Advances in Intelligent Systems and Computing |year=2015 |last1=Georgopoulos |first1=Voula C. |last2=Stylios |first2=Chrysostomos D. |isbn=978-3-319-11456-9 }}</ref>
* In Engineering for [[process modeling|modeling]] and [[Process control|control]]<ref>{{cite web|url=http://content.iospress.com/articles/journal-of-intelligent-and-fuzzy-systems/ifs089?resultNumber=0&totalResults=11&start=0&q=stylios&resultsPageSize=10&rows=10 |title=Fuzzy Cognitive Maps in modeling supervisory control systems - IOS Press |website=Content.iospress.com |date= |accessdate=2017-01-09}}</ref> mainly of complex systems<ref>{{cite journal |doi=10.1109/TSMCA.2003.818878 |title=Modeling Complex Systems Using Fuzzy Cognitive Maps |journal=IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans |volume=34 |page=155 |year=2004 |last1=Stylios |first1=C.D. |last2=Groumpos |first2=P.P. }}</ref>
*In project planning FCMs help to analyze the mutual dependencies between project resources.
*In robotics<ref name="FuzzyThinking"/><ref>Marc Böhlen: ''[http://www.realtechsupport.org/pdf/SpaceRobotics2000.pdf More Robots in Cages]'',</ref> FCMs support machines to develop fuzzy models of their environments and to use these models to make crisp decisions.
*In computer assisted learning FCMs enable computers to check whether students understand their lessons.<ref>Benjoe A. Juliano, Wylis Bandler: ''Tracing Chains-of-Thought (Fuzzy Methods in Cognitive Diagnosis)'', Physica-Verlag Heidelberg 1996, ISBN 3-7908-0922-5</ref>
*In [[expert system]]s<ref name="Drugs"/> a few or many FCMs can be aggregated into one FCM in order to process estimates of knowledgeable persons.<ref>W. B. Vasantha Kandasamy, Florentin Smarandache: ''[http://www.gallup.unm.edu/~smarandache/NCMs.pdf Fuzzy Cognitive Maps and Neutrosophic Cognitive Maps]'', 2003, ISBN 1-931233-76-4</ref>
*In IT project management, a FCM-based methodology helps to success modelling.<ref>{{cite journal |doi=10.1016/j.eswa.2006.01.032 |title=Modelling IT projects success with Fuzzy Cognitive Maps |journal=Expert Systems with Applications |volume=32 |issue=2 |page=543 |year=2007 |last1=Rodriguez-Repiso |first1=Luis |last2=Setchi |first2=Rossitza |last3=Salmeron |first3=Jose L. }}</ref>

FCMappers<ref>FCMappers - international community for fuzzy cognitive mapping: http://www.FCMappers.net/</ref> - an international online community for the analysis and the visualization of fuzzy cognitive maps offer support for starting with FCM and also provide an MS-Excel-based tool that is able to check and analyse FCMs. The output is saved as [[Pajek]] file and can be visualized within 3rd party software like Pajek, Visone,... . They also offer to adapt the software to specific research needs. On their webpage you also will find a linklist for interesting scientific articles, related software, institutes, people and projects. The FCMappers have about one thousand registered members worldwide.

Additional FCM software tools, such as Mental Modeler,<ref>{{cite book |doi=10.1109/HICSS.2013.399 |chapter=Mental Modeler: A Fuzzy-Logic Cognitive Mapping Modeling Tool for Adaptive Environmental Management |title=2013 46th Hawaii International Conference on System Sciences |pages=965– |year=2013 |last1=Gray |first1=Steven A. |last2=Gray |first2=Stefan |last3=Cox |first3=Linda J. |last4=Henly-Shepard |first4=Sarah |isbn=978-1-4673-5933-7 }}</ref><ref>{{cite web|url=http://www.mentalmodeler.com/ |title=Fuzzy Logic Cognitive Mapping |publisher=Mental Modeler |date= |accessdate=2017-01-09}}</ref> have recently been developed as a decision-support tool for use in [[social science]] research, [[collaborative decision-making]], and [[Natural resource management|natural resource planning]].

==Bipolar Fuzzy Cognitive Maps==
Fuzzy cognitive maps have been further extended to bipolar fuzzy cognitive maps based on bipolar fuzzy sets <ref>Wen-Ran Zhang, 1998, (Yin)(Yang) Bipolar Fuzzy Sets. Proceedings of IEEE World Congress on Computational Intelligence – Fuzz-IEEE, Anchorage, AK, 835-840 </ref> and bipolar cognitive mapping.<ref>{{cite journal |doi=10.1109/21.24529 |title=Pool2: A generic system for cognitive map development and decision analysis |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=19 |page=31 |year=1989 |last1=Zhang |first1=W.R. |last2=Chen |first2=S.S. |last3=Bezdek |first3=J.C. }}</ref><ref>{{cite journal |doi=10.1109/21.141315 |title=A cognitive-map-based approach to the coordination of distributed cooperative agents |journal=IEEE Transactions on Systems, Man, and Cybernetics |volume=22 |page=103 |year=1992 |last1=Zhang |first1=W.-R. |last2=Chen |first2=S.-S. |last3=Wang |first3=W. |last4=King |first4=R.S. }}</ref><ref>{{cite journal |doi=10.1109/TSMCB.2003.810444 |title=Equilibrium relations and bipolar cognitive mapping for online analytical processing with applications in international relations and strategic decision support |journal=IEEE Transactions on Systems, Man and Cybernetics, Part B (Cybernetics) |volume=33 |issue=2 |page=295 |year=2003 |last1=Wen-Ran Zhang }}</ref><ref>Wen-Ran Zhang,  2003b, Equilibrium Energy and Stability Measures for Bipolar Decision and Global Regulation. Int’l J. of Fuzzy Sys. Vol. 5, No. 2, 2003, 114-122</ref> Bipolar fuzzy set theory as an equilibrium-based extension to fuzzy sets is recognized by [[L. A. Zadeh]]. <ref>L. A. Zadeh, 2008, Fuzzy logic. Scholarpedia, 3(3):1766, Created: 10 July 2006, reviewed: 27 March 2007, accepted: 31 March 2008.</ref>

==See also==
[[Soft Computing]]   

==References==
{{Reflist|30em}}

{{Commons category|Cognitive maps}}

[[Category:Knowledge representation]]
[[Category:Fuzzy logic]]
<=====doc_Id=====>:751
<=====title=====>:
Frame problem
<=====text=====>:
In [[artificial intelligence]], the '''frame problem''' describes an issue with using [[first-order logic]] (FOL) to express facts about a robot in the world. Representing the state of a robot with traditional FOL requires the use of many axioms that simply imply that things in the environment do not change arbitrarily. For example, Hayes describes a "[[block world]]" with rules about stacking blocks together. In a FOL system, additional [[axiom]]s are required to make inferences about the environment (for example, that a block cannot change position unless it's physically moved). The frame problem is the problem of finding adequate collections of axioms for a viable description of a robot environment.<ref>{{cite journal|last=Hayes|first=Patrick|title=The Frame Problem and Related Problems in Artificial Intelligence|journal=University of Edinburgh|url=http://aitopics.org/sites/default/files/classic/Webber-Nilsson-Readings/Rdgs-NW-Hayes-FrameProblem.pdf}}</ref>

[[John McCarthy (computer scientist)|John McCarthy]] and [[Patrick J. Hayes]] defined this problem in their 1969 article, ''Some Philosophical Problems from the Standpoint of Artificial Intelligence''.  In this paper and many that came after the formal mathematical problem was a starting point for more general discussions of the difficulty of knowledge representation for artificial intelligence. Issues such as how to provide rational default assumptions and what humans consider common sense in a virtual environment.<ref>{{cite journal|last=McCarthy|first=J|author2=P.J. Hayes|title=Some philosophical problems from the standpoint of artificial intelligence|journal=Machine Intelligence|year=1969|volume=4|pages=463–502}}</ref>  Later, the term acquired a broader meaning in [[philosophy]], where it is formulated as the problem of limiting the beliefs that have to be updated in response to actions. In the logical context, actions are typically specified by what they change, with the implicit assumption that everything else (the frame) remains unchanged.

==Description==
The frame problem occurs even in very simple domains. A scenario with a door, which can be open or closed, and a light, which can be on or off, is statically represented by two [[proposition]]s <math>\textit{open}</math> and <math>\textit{on}</math>. If these conditions can change, they are better represented by two [[Predicate (computer programming)|predicate]]s <math>\textit{open}(t)</math> and <math>\textit{on}(t)</math> that depend on time; such predicates are called [[fluent (artificial intelligence)|fluent]]s. A domain in which the door is closed and the light off at time 0, and the door opened at time 1, can be directly represented in logic{{clarify|reason=What kind of logic? If ordinary predicate logic is meant, what is the purpose of the 'true →' in the 3rd formula? If some other logic (situation calculus?) is meant, it should be stated explicitly here, together with the purpose of the 'true →' (e.g. some empty action?) in that logic.|date=August 2013}} by the following formulae:

:<math>\neg \textit{open}(0)</math>
:<math>\neg \textit{on}(0)</math>
:<math>\textit{true} \rightarrow \textit{open}(1)</math>

The first two formulae represent the initial situation; the third formula represents the effect of executing the action of opening the door at time 1. If such an action had preconditions, such as the door being unlocked, it would have been represented by <math>\neg \textit{locked}(0) \rightarrow \textit{open}(1)</math>. In practice, one would have a predicate <math>\textit{executeopen}(t)</math> for specifying when an action is executed and a rule <math>\forall t . \textit{executeopen}(t) \wedge \textit{true} \rightarrow \textit{open}(t+1)</math> for specifying the effects of actions.  The article on the [[situation calculus]] gives more details.

While the three formulae above are a direct expression in logic of what is known, they do not suffice to correctly draw consequences. While the following conditions (representing the expected situation) are consistent with the three formulae above, they are not the only ones.

:{|
| <math>\neg \textit{open}(0)</math> &nbsp; &nbsp;  || <math>\textit{open}(1)</math>
|-
| <math>\neg \textit{on}(0)</math>   || <math>\neg \textit{on}(1)</math>
|}

Indeed, another set of conditions that is consistent with the three formulae above is:

:{|
| <math>\neg \textit{open}(0)</math> &nbsp; &nbsp;  || <math>\textit{open}(1)</math>
|-
| <math>\neg \textit{on}(0)</math>   || <math>\textit{on}(1)</math>
|}

The frame problem is that specifying only which conditions are changed by the actions do not allow, in logic, to conclude that all other conditions are not changed. This problem can be solved by adding the so-called “frame axioms”, which explicitly specify that all conditions not affected by actions are not changed while executing that action. For example, since the action executed at time 0 is that of opening the door, a frame axiom would state that the status of the light does not change from time 0 to time 1:

:<math>\textit{on}(0) \leftrightarrow \textit{on}(1)</math>

The frame problem is that one such frame axiom is necessary for every pair of action and condition such that the action does not affect the condition.{{clarify|reason=Shouldn't then the frame axiom be the following modification of the above rule: '∀t.executeopen(t)→open(t+1)∧(on(t+1)↔on(t))' ? In contrast, the formula 'on(0)↔on(1)' seems to be too particular taylored to the 'executeopen(0)' situation.|date=August 2013}} In other words, the problem is that of formalizing a dynamical domain without explicitly specifying the frame axioms.

The solution proposed by McCarthy to solve this problem involves assuming that a minimal amount of condition changes have occurred; this solution is formalized using the framework of [[Circumscription (logic)|circumscription]]. The [[Yale shooting problem]], however, shows that this solution is not always correct. Alternative solutions were then proposed, involving predicate completion, fluent occlusion, [[successor state axiom]]s, etc.; they are explained below. By the end of the 1980s, the frame problem as defined by McCarthy and Hayes was solved{{clarify|reason=Mention the (combination of) approach(es) by which the frame problem was solved.|date=August 2013}}. Even after that, however, the term “frame problem” was still used, in part to refer to the same problem but under different settings (e.g., concurrent actions), and in part to refer to the general problem of representing and reasoning with dynamical domains.

== Solutions ==
The following solutions depict how the frame problem is solved in various formalisms. The formalisms themselves are not presented in full: what is presented are simplified versions that are sufficient to explain the full solution.

===Fluent occlusion solution===
This solution was proposed by [[Erik Sandewall]], who also defined a [[formal language]] for the specification of dynamical domains; therefore, such a domain can be first expressed in this language and then automatically translated into logic. In this article, only the expression in logic is shown, and only in the simplified language with no action names.

The rationale of this solution is to represent not only the value of conditions over time, but also whether they can be affected by the last executed action. The latter is represented by another condition, called occlusion. A condition is said to be ''occluded'' in a given time point if an action has been just executed that makes the condition true or false as an effect. Occlusion can be viewed as “permission to change”: if a condition is occluded, it is relieved from obeying the constraint of inertia.

In the simplified example of the door and the light, occlusion can be formalized by two predicates <math>\textit{occludeopen}(t)</math> and <math>\textit{occludeon}(t)</math>. The rationale is that a condition can change value only if the corresponding occlusion predicate is true at the next time point. In turn, the occlusion predicate is true only when an action affecting the condition is executed.

:<math>\neg \textit{open}(0)</math>
:<math>\neg \textit{on}(0)</math>
:<math>\textit{true} \rightarrow \textit{open}(1) \wedge \textit{occludeopen}(1)</math>
:<math>\forall t . \neg \textit{occludeopen}(t) \rightarrow (\textit{open}(t-1) \leftrightarrow \textit{open}(t))</math>
:<math>\forall t . \neg \textit{occludeon}(t) \rightarrow (\textit{on}(t-1) \leftrightarrow \textit{on}(t))</math>

In general, every action making a condition true or false also makes the corresponding occlusion predicate true. In this case, <math>\textit{occludeopen}(1)</math> is true, making the antecedent of the fourth formula above false for <math>t=1</math>; therefore, the constraint that <math>\textit{open}(t-1) \leftrightarrow \textit{open}(t)</math> does not hold for <math>t=1</math>. Therefore, <math>\textit{open}</math> can change value, which is also what is enforced by the third formula.

In order for this condition to work, occlusion predicates have to be true only when they are made true as an effect of an action. This can be achieved either by [[Circumscription (logic)|circumscription]] or by predicate completion. It is worth noticing that occlusion does not necessarily imply a change: for example, executing the action of opening the door when it was already open (in the formalization above) makes the predicate <math>\textit{occludeopen}</math> true and makes <math>\textit{open}</math> true; however, <math>\textit{open}</math> has not changed value, as it was true already.

===Predicate completion solution===
This encoding is similar to the fluent occlusion solution, but the additional predicates denote change, not permission to change. For example, <math>\textit{changeopen}(t)</math> represents the fact that the predicate <math>\textit{open}</math> will change from time <math>t</math> to <math>t+1</math>. As a result, a predicate changes if and only if the corresponding change predicate is true. An action results in a change if and only if it makes true a condition that was previously false or vice versa.

:<math>\neg \textit{open}(0)</math>
:<math>\neg \textit{on}(0)</math>
:<math>\neg \textit{open}(0) \wedge \textit{true} \rightarrow \textit{changeopen}(0)</math>
:<math>\forall t. \textit{changeopen}(t) \leftrightarrow (\neg \textit{open}(t) \leftrightarrow \textit{open}(t+1))</math>
:<math>\forall t. \textit{changeon}(t) \leftrightarrow (\neg \textit{on}(t) \leftrightarrow \textit{on}(t+1))</math>

The third formula is a different way of saying that opening the door causes the door to be opened. Precisely, it states that opening the door changes the state of the door if it had been previously closed. The last two conditions state that a condition changes value at time <math>t</math> if and only if the corresponding change predicate is true at time <math>t</math>. To complete the solution, the time points in which the change predicates are true have to be as few as possible, and this can be done by applying predicate completion to the rules specifying the effects of actions.

===Successor state axioms solution===
The value of a condition after the execution of an action can be determined by
the fact that the condition is true if and only if:

# the action makes the condition true; or
# the condition was previously true and the action does not make it false.

A [[successor state axiom]] is a formalization in logic of these two facts. For
example, if <math>\textit{opendoor}(t)</math> and <math>\textit{closedoor}(t)</math> are two
conditions used to denote that the action executed at time <math>t</math> was
to open or close the door, respectively, the running example is encoded as
follows.

: <math>\neg \textit{open}(0)</math>
: <math>\neg \textit{on}(0)</math>
: <math>\textit{opendoor}(0)</math>
: <math>\forall t . \textit{open}(t+1) \leftrightarrow \textit{opendoor}(t) \vee (\textit{open}(t) \wedge \neg \textit{closedoor}(t))</math>

This solution is centered around the value of conditions, rather than the
effects of actions. In other words, there is an axiom for every condition,
rather than a formula for every action. Preconditions to actions (which are not
present in this example) are formalized by other formulae. The successor state
axioms are used in the variant to the [[situation calculus]] proposed by
[[Ray Reiter]].

===Fluent calculus solution===
The [[fluent calculus]] is a variant of the situation calculus. It solves the frame problem by using first-order logic
[[First-order logic#Formation rules|terms]], rather than predicates, to represent the states. Converting
predicates into terms in first order logic is called [[Reification (knowledge representation)|reification]]; the
fluent calculus can be seen as a logic in which predicates representing the
state of conditions are reified.

The difference between a predicate and a term in first order logic is that a term is a representation of an object (possibly a complex object composed of other objects), while a predicate represents a condition that can be true or false when evaluated over a given set of terms.

In the fluent calculus, each possible state is represented by a term obtained by composition of other terms, each one representing the conditions that are true in state. For example, the state in which the door is open and the light is on is represented by the term <math>\textit{open} \circ \textit{on}</math>. It is important to notice that a term is not true or false by itself, as it is an object and not a condition. In other words, the term <math>\textit{open} \circ \textit{on}</math> represent a possible state, and does not by itself mean that this is the current state. A separate condition can be stated to specify that this is actually the state at a given time, e.g., <math>\textit{state}(\textit{open} \circ \textit{on}, 10)</math> means that this is the state at time <math>10</math>.

The solution to the frame problem given in the fluent calculus is to specify the effects of actions by stating how a term representing the state changes when the action is executed. For example, the action of opening the door at time 0 is represented by the formula:

: <math>\textit{state}(s \circ \textit{open}, 1) \leftrightarrow \textit{state}(s,0)</math>

The action of closing the door, which makes a condition false instead of true, is represented in a slightly different way:

: <math>\textit{state}(s, 1) \leftrightarrow \textit{state}(s \circ \textit{open}, 0)</math>

This formula works provided that suitable axioms are given about <math>\textit{state}</math> and <math>\circ</math>, e.g., a term containing two times the same condition is not a valid state (for example, <math>\textit{state}(\textit{open} \circ s \circ \textit{open}, t)</math> is always false for every <math>s</math> and <math>t</math>).

===Event calculus solution===
The [[event calculus]] uses terms for representing fluents, like the fluent calculus, but also has axioms constraining the value of fluents, like the successor state axioms. In the event calculus, inertia is enforced by formulae stating that a fluent is true if it has been true at a given previous time point and no action changing it to false has been performed in the meantime. Predicate completion is still needed in the event calculus for obtaining that a fluent is made true only if an action making it true has been performed, but also for obtaining that an action had been performed only if that is explicitly stated.

===Default logic solution===
The frame problem can be thought of as the problem of formalizing the principle that, by default, "everything is presumed to remain in the state in which it is" ([[Gottfried Wilhelm Leibniz|Leibniz]], "An Introduction to a Secret Encyclopædia", ''c''. 1679).  This default, sometimes called the ''commonsense law of inertia'', was expressed by [[Raymond Reiter]] in [[default logic]]:

: <math>\frac{R(x,s)\; :\ R(x,\textit{do}(a,s))}{R(x,\textit{do}(a,s))}</math>

(if <math>R(x)</math> is true in situation <math>s</math>, and it can be assumed<ref>i.e., no contradicting information is known</ref> that <math>R(x)</math> remains true after executing action <math>a</math>, then we can conclude that <math>R(x)</math> remains true).

Steve Hanks and [[Drew McDermott]] argued, on the basis of their [[Yale shooting problem|Yale shooting]] example, that this solution to the frame problem is unsatisfactory.  Hudson Turner showed, however, that it works correctly in the presence of appropriate additional postulates.

===Answer set programming solution===
The counterpart of the default logic solution in the language of [[answer set programming]] is a rule with [[stable model semantics#Strong negation|strong negation]]:

:<math>r(X,T+1) \leftarrow r(X,T),\ \hbox{not }\sim r(X,T+1)</math>

(if <math>r(X)</math> is true at time <math>T</math>, and it can be assumed that <math>r(X)</math> remains true at time <math>T+1</math>, then we can conclude that <math>r(X)</math> remains true).

===Action description languages===
[[Action description language]]s elude the frame problem rather than solving it. An action description language is a formal language with a syntax that is specific for describing situations and actions. For example, that the action <math>\textit{opendoor}</math> makes the door open if not locked is expressed by:

: <math>\textit{opendoor}</math> causes <math>\textit{open}</math> if <math>\neg \textit{locked}</math>

The semantics of an action description language depends on what the language can express (concurrent actions, delayed effects, etc.) and is usually based on [[transition system]]s.

Since domains are expressed in these languages rather than directly in logic, the frame problem only arises when a specification given in an action description logic is to be translated into logic. Typically, however, a translation is given from these languages to [[answer set programming]] rather than first-order logic.

==See also==
* [[Yale shooting problem]]
* [[Binding problem]]
* [[Ramification problem]]
* [[Qualification problem]]
* [[Common sense]]
* [[Commonsense reasoning]]
* [[Defeasible reasoning]]
* [[Non-monotonic logic]]
* [[Symbol grounding]]
* [[Linear logic]]

==Notes==
{{reflist}}

==References==
* {{cite journal | last1 = Doherty | first1 = P. | last2 = Gustafsson | first2 = J. | last3 = Karlsson | first3 = L. | last4 = Kvarnström | first4 = J. | year = 1998 | title = TAL: Temporal action logics language specification and tutorial | url = http://www.ep.liu.se/ej/etai/1998/009 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 273–306 }}
* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1993 | title = Representing action and change by logic programs | url = | journal = Journal of Logic Programming | volume = 17 | issue = | pages = 301–322 | doi=10.1016/0743-1066(93)90035-f}}
* {{cite journal | last1 = Gelfond | first1 = M. | last2 = Lifschitz | first2 = V. | year = 1998 | title = Action languages | url = http://www.ep.liu.se/ej/etai/1998/007 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 193–210 }}
* {{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic logic and temporal projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379–412 | doi=10.1016/0004-3702(87)90043-9}}
* {{cite journal | last1 = Levesque | first1 = H. | authorlink3 = Raymond Reiter | last2 = Pirri | first2 = F. | last3 = Reiter | first3 = R. | year = 1998 | title = Foundations for the situation calculus | url = http://www.ep.liu.se/ej/etai/1998/005 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 159–178 }}
* {{cite journal | last1 = Liberatore | first1 = P. | year = 1997 | title = The complexity of the language A | url = http://www.ep.liu.se/ej/etai/1997/002 | journal = [[Electronic Transactions on Artificial Intelligence]] | volume = 1 | issue = 1-3| pages = 13–37 }}
* {{cite journal |first=V. |last=Lifschitz |year=2012 |url=http://www.cs.utexas.edu/~vl/papers/jmc.pdf |title=The frame problem, then and now |publisher=[[University of Texas at Austin]]}} Presented at ''Celebration of John McCarthy's Accomplishments'', [[Stanford University]], March 25, 2012.
* {{cite journal | last1 = McCarthy | first1 = J. | last2 = Hayes | first2 = P. J. | year = 1969 | title = Some philosophical problems from the standpoint of artificial intelligence | url = http://www-formal.stanford.edu/jmc/mcchay69.html | journal = Machine Intelligence | volume = 4 | issue = | pages = 463–502 }}
* {{cite journal | last1 = McCarthy | first1 = J. | year = 1986 | title = Applications of circumscription to formalizing common-sense knowledge | url = http://www-formal.stanford.edu/jmc/applications.html | journal = Artificial Intelligence | volume = 28 | issue = | pages = 89–116 | doi=10.1016/0004-3702(86)90032-9}}
* {{cite journal | last1 = Miller | first1 = R. | last2 = Shanahan | first2 = M. | year = 1999 | title = The event-calculus in classical logic - alternative axiomatizations | url = http://www.ida.liu.se/ext/epa/ej/etai/1999/016/epapage.html | journal = Electronic Transactions on Artificial Intelligence | volume = 3 | issue = 1| pages = 77–105 }}
* {{cite journal | last1 = Pirri | first1 = F. | last2 = Reiter | first2 = R. | year = 1999 | title = Some contributions to the metatheory of the Situation Calculus | url = | journal = [[Journal of the ACM]] | volume = 46 | issue = 3| pages = 325–361 | doi = 10.1145/316542.316545 }}
* {{cite journal | last1 = Reiter | first1 = R. | authorlink = Raymond Reiter | year = 1980 | title = A logic for default reasoning | url = | journal = Artificial Intelligence | volume = 13 | issue = | pages = 81–132 | doi=10.1016/0004-3702(80)90014-4}}
* {{cite book |authorlink=Raymond Reiter |first=Raymond |last=R. |year=1991 |chapter=The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression |editor=Lifschitz, Vladimir |title=Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy |pages=359–380 |publisher=Academic Press |location=New York}}
* {{cite journal | last1 = Sandewall | first1 = E. | year = 1972 | title = An approach to the Frame Problem and its Implementation | url = | journal = Machine Intelligence | volume = 7 | issue = | pages = 195–204 }}
* {{cite book |first=E. |last=Sandewall |year=1994 |title=Features and Fluents |volume=(vol. 1) |publisher=Oxford University Press |location=New York |isbn=0-19-853845-6}}
* {{cite book |first1=E. |last1=Sandewall |first2=Y. |last2=Shoham |year=1995 |chapter=Non-monotonic Temporal Reasoning |editor1=Gabbay, D. M. |editor2=Hogger, C. J. |editor3=Robinson, J. A. |title=Handbook of Logic in Artificial Intelligence and Logic Programming |volume=(vol. 4) |pages=439–498 |publisher=Oxford University Press |isbn=0-19-853791-3}}
* {{cite journal | last1 = Sandewall | first1 = E. | year = 1998 | title = Cognitive robotics logic and its metatheory: Features and fluents revisited | url = http://www.ep.liu.se/ej/etai/1998/010 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 307–329 }}
* {{cite book |first=M. |last=Shanahan |year=1997 |title=Solving the frame problem: A mathematical investigation of the common sense law of inertia |publisher=MIT Press}}
* {{cite journal | last1 = Thielscher | first1 = M. | year = 1998 | title = Introduction to the fluent calculus | url = http://www.ep.liu.se/ej/etai/1998/006 | journal = Electronic Transactions on Artificial Intelligence | volume = 2 | issue = 3-4| pages = 179–192 }}
* {{cite journal | last1 = Toth | first1 = J.A. | year = 1995 | title = Book review. Kenneth M. and Patrick J. Hayes, eds | url = | journal = Reasoning agents in a dynamic world: The frame problem. Artificial Intelligence | volume = 73 | issue = | pages = 323–369 | doi=10.1016/0004-3702(95)90043-8}}
* {{cite journal | last1 = Turner | first1 = H. | year = 1997 | title = Representing actions in logic programs and default theories: a situation calculus approach | url = http://www.d.umn.edu/~hudson/papers/ralpdt6.pdf | format = PDF | journal = Journal of Logic Programming | volume = 31 | issue = | pages = 245–298 | doi=10.1016/s0743-1066(96)00125-2}}

==External links==
* {{cite SEP |url-id=frame-problem |title=The Frame Problem}}
* [http://www-formal.stanford.edu/jmc/mcchay69/mcchay69.html Some Philosophical Problems from the Standpoint of Artificial Intelligence]; the original article of McCarthy and Hayes that proposed the problem.

{{John McCarthy navbox}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Epistemology]]
[[Category:Logic programming]]
[[Category:Philosophical problems]]
[[Category:1969 introductions]]
<=====doc_Id=====>:754
<=====title=====>:
Qualification problem
<=====text=====>:
{{one source|date=July 2011}}
In [[philosophy]] and [[Artificial intelligence|AI]] (especially, knowledge based systems), the '''qualification problem''' is concerned with the impossibility of listing ''all'' the [[precondition]]s required for a real-world action to have its intended effect. It might be posed as ''how to deal with the things that prevent me from achieving my intended result''. It is strongly connected to, and opposite the [[ramification problem|ramification side]] of, the [[frame problem]]. John McCarthy gives the following motivating example, in which it is impossible to enumerate all the circumstances that may prevent a rowboat from performing its ordinary function:

:"[T]he successful use of a boat to cross a river requires, if the boat is a rowboat, that the oars and rowlocks be present and unbroken, and that they fit each other. Many other qualifications can be added, making the rules for using a rowboat almost impossible to apply, and yet anyone will still be able to think of additional requirements not yet stated."

==See also==
*[[Non-monotonic logic]]
*[[Circumscription (logic)|Circumscription]]

==External links==
* John McCarthy "[http://www-formal.stanford.edu/jmc/circumscription/node1.html Introduction: The Qualification Problem]" 

[[Category:Knowledge representation]]
[[Category:Logic programming]]
[[Category:Epistemology]]

{{epistemology-stub}}
<=====doc_Id=====>:757
<=====title=====>:
Belief revision
<=====text=====>:
'''Belief revision''' is the process of changing beliefs to take into account a new piece of information. The [[formal logic|logical]] formalization of belief revision is researched in [[philosophy]], in [[databases]], and in artificial intelligence for the design of [[intelligent agent|rational agent]]s.

What makes belief revision non-trivial is that several different ways for performing this operation may be possible. For example, if the current knowledge includes the three facts "<math>A</math> is true", "<math>B</math> is true" and "if <math>A</math> and <math>B</math> are true then <math>C</math> is true", the introduction of the new information "<math>C</math> is false" can be done preserving consistency only by removing at least one of the three facts. In this case, there are at least three different ways for performing revision. In general, there may be several different ways for changing knowledge.

==Revision and update==

Two kinds of changes are usually distinguished:

; update : the new information is about the situation at present, while the old beliefs refer to the past; update is the operation of changing the old beliefs to take into account the change;

; revision : both the old beliefs and the new information refer to the same situation; an inconsistency between the new and old information is explained by the possibility of old information being less reliable than the new one; revision is the process of inserting the new information into the set of old beliefs without generating an inconsistency.

The main assumption of belief revision is that of minimal change: the knowledge before and after the change should be as similar as possible. In the case of update, this principle formalizes the assumption of inertia. In the case of revision, this principle enforces as much information as possible to be preserved by the change.

===Example===

The following classical example shows that the operations to perform in the two settings of update and revision are not the same. The example is based on two different interpretations of the set of beliefs <math>\{a \vee b\}</math> and the new piece of information <math>\neg a</math>:

; update : in this scenario, two satellites, Unit A and Unit B, orbit around Mars; the satellites are programmed to land while transmitting their status to Earth; Earth has received a transmission from one of the satellites, communicating that it is still in orbit; however, due to interference, it is not known which satellite sent the signal; subsequently, Earth receives the communication that Unit A has landed; this scenario can be modeled in the following way; two [[propositional variable]]s <math>a</math> and <math>b</math> indicate that Unit A and Unit B, respectively, are still in orbit; the initial set of beliefs is <math>\{a \vee b\}</math> (either one of the two satellites is still in orbit) and the new piece of information is <math>\neg a</math> (Unit A has landed, and is therefore not in orbit); the only rational result of the update is <math>\neg a</math>; since the initial information that one of the two satellites had not landed yet was possibly coming from the Unit A, the position of the Unit B is not known;

; revision : the play "Six Characters in Search of an Author" will be performed in one of the two local theatres; this information can be denoted by <math>\{a \vee b\}</math>, where <math>a</math> and <math>b</math> indicates that the play will be performed at the first or at the second theatre, respectively; a further information that "Jesus Christ Superstar" will be performed at the first theatre indicates that <math>\neg a</math> holds; in this case, the obvious conclusion is that "Six Characters in Search of an Author" will be performed at the second but not the first theatre, which is represented in logic by <math>\neg a \wedge b</math>.

This example shows that revising the belief <math>a \vee b</math> with the new information <math>\neg a</math> produces two different results <math>\neg a </math> and <math>\neg a \wedge b</math> depending on whether the setting is that of update or revision.

==Contraction, expansion, revision, consolidation, and merging==

In the setting in which all beliefs refer to the same situation, a distinction between various operations that can be performed is made:

; contraction : removal of a belief;

; expansion : addition of a belief without checking consistency;

; revision : addition of a belief while maintaining consistency;

; consolidation : restoring consistency of a set of beliefs;

; merging : fusion of two or more sets of beliefs while maintaining consistency.

Revision and merging differ in that the first operation is done when the new belief to incorporate is considered more reliable than the old ones; therefore, consistency is maintained by removing some of the old beliefs. Merging is a more general operation, in that the priority among the belief sets may or may not be the same.

Revision can be performed by first incorporating the new fact and then restoring consistency via consolidation. This is actually a form of merging rather than revision, as the new information is not always treated as more reliable than the old knowledge.

==The AGM postulates==

The AGM postulates (named after the names of their proponents, [[Carlos Alchourrón|Alchourrón]], [[Peter Gärdenfors|Gärdenfors]], and [[David Makinson|Makinson]]) are properties that an operator that performs revision should satisfy in order for that operator to be considered rational. The considered setting is that of revision, that is, different pieces of information referring to the same situation. Three operations are considered: expansion (addition of a belief without a consistency check), revision (addition of a belief while maintaining consistency), and contraction (removal of a belief).

The first six postulates are called "the basic AGM postulates". In the settings considered by Alchourrón, Gärdenfors, and Makinson, the current set of beliefs is represented by a [[Deductive closure|deductively closed]] set of logical formulae <math>K</math> called belief base, the new piece of information is a logical formula <math>P</math>, and revision is performed by a binary operator <math>*</math> that takes as its operands the current beliefs and the new information and produces as a result a belief base representing the result of the revision. The <math>+</math> operator denoted expansion: <math>K+P</math> is the deductive closure of <math>K \cup \{P\}</math>. The AGM postulates for revision are:

# Closure: <math>K*P</math> is a belief base (i.e., a deductively closed set of formulae);
# Success: <math>P \in K*P</math>
# Inclusion: <math>K*P \subseteq K+P</math>
# Vacuity: <math>\text{If }(\neg P) \not \in K,\text{ then }K*P=K+P</math>
# <math>K*P</math> is [[inconsistent]] only if <math>P</math> is inconsistent or <math>K</math> is inconsistent
# Extensionality: <math>\text{If }P\text{ and }Q\text{ are logically equivalent, then }K*P=K*Q</math> (see [[logical equivalence]])
# <math>K*(P \wedge Q) \subseteq (K*P)+Q</math>
# <math>\text{If }(\neg Q) \not\in K*P\text{ then }(K*P)+Q \subseteq K*(P \wedge Q)</math>

A revision operator that satisfies all eight postulates is the full meet revision, in which <math>K*P</math> is equal to <math>K+P</math> if consistent, and to the deductive closure of <math>P</math> otherwise. While satisfying all AGM postulates, this revision operator has been considered to be too conservative, in that no information from the old knowledge base is maintained if the revising formula is inconsistent with it.{{Citation needed|date=November 2011}}

==Conditions equivalent to the AGM postulates==

The AGM postulates are equivalent to several different conditions on the revision operator; in particular, they are equivalent to the revision operator being definable in terms of structures known as selection functions, epistemic entrenchments, systems of spheres, and preference relations. The latter are [[reflexive relation|reflexive]], [[transitive relation|transitive]], and [[total relation]]s over the set of models.

Each revision operator <math>*</math> satisfying the AGM postulates is associated to a set of preference relations <math>\leq_K</math>, one for each possible belief base <math>K</math>, such that the models of <math>K</math> are exactly the minimal of all models according to <math>\leq_K</math>. The revision operator and its associated family of orderings are related by the fact that <math>K*P</math> is the set of formulae whose set of models contains all the minimal models of <math>P</math> according to <math>\leq_K</math>. This condition is equivalent to the set of models of <math>K*P</math> being exactly the set of the minimal models of <math>P</math> according to the ordering <math>\leq_K</math>.

A preference ordering <math>\leq_K</math> represents an order of implausibility among all situations, including those that are conceivable but yet currently considered false. The minimal models according to such an ordering are exactly the models of the knowledge base, which are the models that are currently considered the most likely. All other models are greater than these ones, and are indeed considered less plausible. In general, <math>I <_K J</math> indicates that the situation represented by the model <math>I</math> is believed to be more plausible than the situation represented by <math>J</math>. As a result, revising by a formula having <math>I</math> and <math>J</math> as models should select only <math>I</math> to be a model of the revised knowledge base, as this model represent the most likely scenario among those supported by <math>P</math>.

==Contraction==

Contraction is the operation of removing a belief <math>P</math> from a knowledge base <math>K</math>; the result of this operation is denoted by <math>K-P</math>. The operators of revision and contractions are related by the Levi and Harper identities:

: <math>K*P=(K-\neg P)+P</math>
: <math>K-P=K \cap (K*\neg P)</math>

Eight postulates have been defined for contraction. Whenever a revision operator satisfies the eight postulates for revision, its corresponding contraction operator satisfies the eight postulates for contraction, and vice versa. If a contraction operator satisfies at least the first six postulates for contraction, translating it into a revision operator and then back into a contraction operator using the two identities above leads to the original contraction operator. The same holds starting from a revision operator.

One of the postulates for contraction has been longly discussed: the recovery postulate:

: <math>K=(K-P)+P</math>

According to this postulate, the removal of a belief <math>P</math> followed by the reintroduction of the same belief in the belief base should lead to the original belief base. There are some examples showing that such behavior is not always reasonable: in particular, the contraction by a general condition such as <math>a \vee b</math> leads to the removal of more specific conditions such as <math>a</math> from the belief base; it is then unclear why the reintroduction of <math>a \vee b</math> should also lead to the reintroduction of the more specific condition <math>a</math>. For example, if George was previously believed to have German citizenship, it was also believed to be European. Contracting this latter belief amounts to stop believing that George is European; therefore, that George has German citizenship is also retracted from the belief base. If George is later discovered to have Austrian citizenship, then the fact that he is European is also reintroduced. According to the recovery postulate, however, the belief that he also has German citizenship should also be reintroduced.

The correspondence between revision and contraction induced by the Levi and Harper identities is such that a contraction not satisfying the recovery postulate is translated into a revision satisfying all eight postulates, and that a revision satisfying all eight postulates is translated into a contraction satisfying all eight postulates, including recovery. As a result, if recovery is excluded from consideration, a number of contraction operators are translated into a single revision operator, which can be then translated back into exactly one contraction operator. This operator is the only one of the initial group of contraction operators that satisfies recovery; among this group, it is the operator that preserves as much information as possible.

==The Ramsey test==
<!--[[Ramsey test]], [[Ramsey Test]], [[Ramsey's test]], [[Ramsey's Test]] redirect here.-->

The evaluation of a [[counterfactual conditional]] <math>a > b</math> can be done, according to the '''Ramsey test''' (named for [[Frank P. Ramsey]]), to the hypothetical addition of <math>a</math> to the set of current beliefs followed by a check for the truth of <math>b</math>. If <math>K</math> is the set of beliefs currently held, the Ramsey test is formalized by the following correspondence:

: <math>a > b</math> if and only if <math>b \in K * a</math>

If the considered language of the formulae representing beliefs is propositional, the Ramsey test gives a consistent definition for counterfactual conditionals in terms of a belief revision operator. However, if the language of formulae representing beliefs itself includes the counterfactual conditional connective <math>></math>, the Ramsey test leads to the Gardenfors triviality result: there is no non-trivial revision operator that satisfies both the AGM postulates for revision and the condition of the Ramsey test. This result holds in the assumption that counterfactual formulae like <math>a>b</math> can be present in belief bases and revising formulae. Several solutions to this problem have been proposed.

==Non-monotonic inference relation==

Given a fixed knowledge base <math>K</math> and a revision operator <math>*</math>, one can define a non-monotonic inference relation using the following definition: <math>P \vdash Q</math> if and only if <math>K*P \models Q</math>. In other words, a formula <math>P</math> [[logical consequence|entails]] another formula <math>Q</math> if the addition of the first formula to the current knowledge base leads to the derivation of <math>Q</math>. This inference relation is non-monotonic.

The AGM postulates can be translated into a set of postulates for this inference relation. Each of these postulates is entailed by some previously considered set of postulates for non-monotonic inference relations. Vice versa, conditions that have been considered for non-monotonic inference relations can be translated into postulates for a revision operator. All these postulates are entailed by the AGM postulates.

==Foundational revision==

In the AGM framework, a belief set is represented by a deductively closed set of [[propositional formula]]e. While such sets are infinite, they can always be finitely representable. However, working with deductively closed sets of formulae leads to the implicit assumption that equivalent belief bases should be considered equal when revising. This is called the ''principle of irrelevance of syntax''.

This principle has been and is currently debated: while <math>\{a, b\}</math> and <math>\{a \wedge b\}</math> are two equivalent sets, revising by <math>\neg a</math> should produce different results. In the first case, <math>a</math> and <math>b</math> are two separate beliefs; therefore, revising by <math>\neg a</math> should not produce any effect on <math>b</math>, and the result of revision is <math>\{\neg a, b\}</math>. In the second case, <math>a \wedge b</math> is taken a single belief. The fact that <math>a</math> is false contradicts this belief, which should therefore be removed from the belief base. The result of revision is therefore <math>\{\neg a\}</math> in this case.

The problem of using deductively closed knowledge bases is that no distinction is made between pieces of knowledge that are known by themselves and pieces of knowledge that are merely consequences of them. This distinction is instead done by the ''foundational'' approach to belief revision, which is related to [[foundationalism]] in philosophy. According to this approach, retracting a non-derived piece of knowledge should lead to retracting all its consequences that are not otherwise supported (by other non-derived pieces of knowledge).  This approach can be realized by using knowledge bases that are not deductively closed and assuming that all formulae in the knowledge base represent self-standing beliefs, that is, they are not derived beliefs. In order to distinguish the foundational approach to belief revision to that based on deductively closed knowledge bases, the latter is called the ''coherentist'' approach. This name has been chosen because the coherentist approach aims at restoring the coherence
(consistency) among ''all'' beliefs, both self-standing and derived ones. This approach is related to [[coherentism]] in philosophy.

Foundationalist revision operators working on non-deductively closed belief bases typically select some subsets of <math>K</math> that are consistent with <math>P</math>, combined them in some way, and then conjoined them with <math>P</math>. The following are two non-deductively closed base revision operators.

; WIDTIO : (When in Doubt, Throw it Out) the maximal subsets of <math>K</math> that are consistent with <math>P</math> are intersected, and <math>P</math> is added to the resulting set; in other words, the result of revision is composed by <math>P</math> and of all formulae of <math>K</math> that are in all maximal subsets of <math>K</math> that are consistent with <math>P</math>;

; Ginsberg-Fagin-Ullman-Vardi : the maximal subsets of <math>K \cup \{P\}</math> that are consistent and contain <math>P</math> are combined by disjunction;

; Nebel : similar to the above, but a priority among formulae can be given, so that formulae with higher priority are less likely to being retracted than formulae with lower priority.

A different realization of the foundational approach to belief revision is based on explicitly declaring the dependences among beliefs. In the [[truth maintenance system]]s, dependence links among beliefs can be specified. In other worlds, one can explicitly declare that a given fact is believed because of one or more other facts; such a dependency is called a ''justification''. Beliefs not having any justifications play the role of non-derived beliefs in the non-deductively closed knowledge base approach.

==Model-based revision and update==

A number of proposals for revision and update based on the set of models of the involved formulae were developed independently of the AGM framework. The principle behind this approach is that a knowledge base is equivalent to a set of ''possible worlds'', that is, to a set of scenarios that are considered possible according to that knowledge base. Revision can therefore be performed on the sets of possible worlds rather than on the corresponding knowledge bases.

The revision and update operators based on models are usually identified by the name of their authors: [[Marianne Winslett|Winslett]], Forbus, Satoh, Dalal, Hegner, and Weber. According to the  first four of these proposal, the result of revising/updating a formula <math>K</math> by another formula <math>P</math> is characterized by the set of models of <math>P</math> that are the closest to the models of <math>K</math>. Different notions of closeness can be defined, leading to the difference among these proposals.

; Dalal : the models of <math>P</math> having a minimal [[Hamming distance]] to models of <math>K</math> are selected to be the models that result from the change;

; Satoh : similar to Dalal, but distance between two models is defined as the set of literals that are given different values by them; similarity between models is defined as set containment of these differences;

; Winslett : for each model of <math>K</math>, the closest models of <math>P</math> are selected; comparison is done using set containment of the difference;

; Borgida : equal to Winslett's if <math>K</math> and <math>P</math> are inconsistent; otherwise, the result of revision is <math>K \wedge P</math>;

; [[Ken Forbus|Forbus]] : similar to Winslett, but the Hamming distance is used.

The revision operator defined by Hegner makes <math>K</math> not to affect  the value of the variables that are mentioned in <math>P</math>. What results from this operation is a formula <math>K'</math> that is consistent with <math>P</math>, and can therefore be conjoined with it. The revision operator by Weber is similar, but the literals that are removed from <math>K</math> are not all literals of <math>P</math>, but only the literals that are evaluated differently by a pair of closest models of <math>K</math> and <math>P</math> according to the Satoh measure of closeness.

==Iterated revision==

The AGM postulates are equivalent to a preference ordering (an ordering over models) to be associated to every knowledge base <math>K</math>. However, they do not relate the orderings corresponding to two non-equivalent knowledge bases. In particular, the orderings associated to a knowledge base <math>K</math> and its revised version <math>K*P</math> can be completely different. This is a problem for performing a second revision, as the ordering associated with <math>K*P</math> is necessary to calculate <math>K*P*Q</math>.

Establishing a relation between the ordering associated with <math>K</math> and <math>K*P</math> has been however recognized not to be the right solution to this problem. Indeed, the preference relation should depend on the previous history of revisions, rather than on the resulting knowledge base only. More generally, a preference relation gives more information about the state of mind of an agent than a simple knowledge base. Indeed, two states of mind might represent the same piece of knowledge <math>K</math> while at the same time being different in the way a new piece of knowledge would be incorporated. For example, two people might have the same idea as to where to go on holiday, but yet they differ on how they would change this idea if they win a million-dollar lottery. Since the basic condition of the preference ordering is that their minimal models are exactly the models of their associated knowledge base, a knowledge base can be considered implicitly represented by a preference ordering (but not vice versa).

Given that a preference ordering allows deriving its associated knowledge base but also allows performing a single step of revision, studies on iterated revision have been concentrated on how a preference ordering should be changed in response of a revision. While single-step revision is about how a knowledge base <math>K</math> has to be changed into a new knowledge base <math>K*P</math>, iterated revision is about how a preference ordering (representing both the current knowledge and how much situations believed to be false are considered possible) should be turned into a new preference relation when <math>P</math> is learned. A single step of iterated revision produces a new ordering that allows for further revisions.

Two kinds of preference ordering are usually considered: numerical and non-numerical. In the first case, the level of plausibility of a model is  representing by a non-negative integer number; the lower the rank, the more plausible the situation corresponding to the model. Non-numerical preference orderings correspond to the preference relations used in the AGM framework: a possibly total ordering over models. The non-numerical preference relation were initially considered unsuitable for iterated revision because of the impossibility of reverting a revision by a number of other revisions, which is instead possible in the numerical case.

Darwiche and [[Judea Pearl|Pearl]]<ref name="darwiche-pearl">Darwiche, A. and Pearl, J. (1997) On the logic of iterated belief revision.  ''Artificial Intelligence'' '''89'''(1-2): 1-29.</ref> formulated the following postulates for iterated revision.

# if <math>\alpha \models \mu</math> then <math>(\psi * \mu) * \alpha \equiv \psi * \alpha</math>;
# if <math>\alpha \models \neg \mu</math>, then <math>(\psi * \mu) * \alpha \equiv \psi * \alpha</math>;
# if <math>\psi * \alpha \models \mu</math>, then <math>(\psi * \mu) * \alpha \models \mu</math>;
# if <math>\psi * \alpha \not\models \neg \mu</math>, then <math>(\psi * \mu) * \alpha \not\models \neg \mu</math>.

Specific iterated revision operators have been proposed by Spohn, Boutilier, Williams, Lehmann, and others.

; Spohn rejected revision : this non-numerical proposal has been first considered by Spohn, who rejected it based on the fact that revisions can change some orderings in such a way the original ordering cannot be restored with a sequence of other revisions; this operator change a preference ordering in view of new information <math>P</math> by making all models of <math>P</math> being preferred over all other models; the original preference ordering is maintained when comparing two models that are both models of <math>P</math> or both non-models of <math>P</math>;

; Natural revision : while revising a preference ordering by a formula <math>P</math>, all minimal models (according to the preference ordering) of <math>P</math> are made more preferred by all other ones; the original ordering of models is preserved when comparing two models that are not minimal models of <math>P</math>; this operator changes the ordering among models minimally while preserving the property that the models of the knowledge base after revising by <math>P</math> are the minimal models of <math>P</math> according to the preference ordering;

; Transmutations : these are two forms of revision, conditionalization and adjustment, which work on numerical preference orderings; revision requires not only a formula but also a number indicating its degree of plausibility; while the preference ordering is still inverted (the lower a model, the most plausible it is) the degree of plausibility of a revising formula is direct (the higher the degree, the most believed the formula is);

; Ranked revision : a ranked model, which is an assignment of non-negative integers to models, has to be specified at the beginning; this rank is similar to a preference ordering, but is not changed by revision; what is changed by a sequence of revisions are a current set of models (representing the current knowledge base) and a number called the rank of the sequence; since this number can only monotonically non-decrease, some sequences of revision lead to situations in which every further revision is performed as a full meet revision.

==Merging==

The assumption implicit in the revision operator is that the new piece of information <math>P</math> is always to be considered more reliable than the old knowledge base <math>K</math>. This is formalized by the second of the AGM postulates: <math>P</math> is always believed after revising <math>K</math> with <math>P</math>. More generally, one can consider the process of merging several pieces of information (rather than just two) that might or might not have the same reliability. Revision becomes the particular instance of this process when a less reliable piece of information <math>K</math> is merged with a more reliable <math>P</math>.

While the input to the revision process is a pair of formulae <math>K</math> and <math>P</math>, the input to merging is a [[multiset]] of formulae <math>K</math>, <math>T</math>, etc. The use of multisets is necessary as two sources to the merging process might be identical.

When merging a number of knowledge bases with the same degree of plausibility, a distinction is made between arbitration and majority. This distinction depends on the assumption that is made about the information and how it has to be put together.

; arbitration : the result of arbitrating two knowledge bases <math>K</math> and <math>T</math> entails <math>K \vee T</math>; this condition formalizes the assumption of maintaining as much as the old information as possible, as it is equivalent to imposing that every formula entailed by both knowledge bases is also entailed by the result of their arbitration; in a possible world view, the "real" world is assumed one of the worlds considered possible according to at least one of the two knowledge bases;

; majority : the result of merging a knowledge base <math>K</math> with other knowledge bases can be forced to entail <math>K</math> by adding a sufficient number of other knowledge bases equivalent to <math>K</math>; this condition corresponds to a kind of vote-by-majority: a sufficiently large number of knowledge bases can always overcome the "opinion" of any other fixed set of knowledge bases.

The above is the original definition of arbitration. According to a newer definition, an arbitration operator is a merging operator that is insensitive to the number of equivalent knowledge bases to merge. This definition makes arbitration the exact opposite of majority.

Postulates for both arbitration and merging have been proposed. An example of an arbitration operator satisfying all postulates is the classical disjunction. An example of a majority operator satisfying all postulates is that selecting all models that have a minimal total Hamming distance to models of the knowledge bases to merge.

A merging operator can be expressed as a family of orderings over models, one for each possible multiset of knowledge bases to merge: the models of the result of merging a multiset of knowledge bases are the minimal models of the ordering associated to the multiset. A merging operator defined in this way satisfies the postulates for merging if and only if the family of orderings meets a given set of conditions. For the old definition of arbitration, the orderings are not on models but on pairs (or, in general, tuples) of models.

==Social choice theory==

Many revision proposals involve orderings over models representing the relative plausibility of the possible alternatives. The problem of merging amounts to combine a set of orderings into a single one expressing the combined  plausibility of the alternatives. This is similar with what is done in [[social choice theory]], which is the study of how the preferences of a group of agents can be combined in a rational way. Belief revision and social choice theory are similar in that they combine a set of orderings into one. They differ on how these orderings are interpreted: preferences in social choice theory; plausibility in belief revision. Another difference is that the alternatives are explicitly enumerated in social choice theory, while they are the propositional models over a given alphabet in belief revision.

==Complexity==

The problem about belief revision that is the most studied from the point of view of [[Computational complexity theory|computational complexity]] is that of query answering in the propositional case. This is the problem of establishing whether a formula follows from the result of a revision, that is, <math>K*P \models Q</math>, where <math>K</math>, <math>P</math>, and <math>Q</math> are propositional formulae. More generally, query answering is the problem of telling whether a formula is entailed by the result of a belief revision, which could be update, merging, revision, iterated revision, etc. Another problem that has received some attention is that of model checking, that is, checking whether a model satisfies the result of a belief revision. A related question is whether such result can be represented in space polynomial in that of its arguments.

Since a deductively closed knowledge base is infinite, complexity studies on belief revision operators working on deductively closed knowledge bases are done in the assumption that such deductively closed knowledge base are given in the form of an equivalent finite knowledge base.

A distinction is made among belief revision operators and belief revision schemes. While the former are simple mathematical operators mapping a pair of formulae into another formula, the latter depend on further information such as a preference relation. For example, the Dalal revision is an operator because, once two formulae <math>K</math> and <math>P</math> are given, no other information is needed to compute <math>K*P</math>. On the other hand, revision based on a preference relation is a revision scheme, because <math>K</math> and <math>P</math> do not allow determining the result of revision if the family of preference orderings between models is not given. The complexity for revision schemes is determined in the assumption that the extra information needed to compute revision is given in some compact form. For example, a preference relation can be represented by a sequence of formulae whose models are increasingly preferred. Explicitly storing the relation as a set of pairs of models is instead not a compact representation of preference because the space required is exponential in the number of propositional letters.

The complexity of query answering and model checking in the propositional case is in the second level of the [[polynomial hierarchy]] for most belief revision operators and schemas. Most revision operators suffer from the problem of representational blow up: the result of revising two formulae is not necessarily representable in space polynomial in that of the two original formulae. In other words, revision may exponentially increase the size of the knowledge base.

==Implementations==

Systems specifically implementing belief revision are: [http://portal.acm.org/citation.cfm?id=122296.122301 Immortal], [https://web.archive.org/web/20051018054730/http://magic.it.uts.edu.au:80/systems/saten.html SATEN], and [http://www.dis.uniroma1.it/~liberato/brels/brels.html BReLS]. Two systems including a belief revision feature are [http://www.cse.buffalo.edu/sneps/ SNePS] and [[Cyc]]. [[Truth maintenance systems]] are used in [[Artificial Intelligence]] to implement belief revision.

==See also==

* [[Artificial intelligence]]
* [[Inquiry]]
* [[Knowledge representation]]
* [[Belief propagation]]
* [[Reason maintenance]]
* [[Epistemic closure]]
* [[Non-monotonic logic]]
* [[Defeasible reasoning]]
* [[Reasoning]]
* [[Philosophy of science]]
* [[Discursive dilemma]]

==Notes==
{{reflist}}

==References==
* C. E. Alchourròn, P. Gärdenfors, and D. Makinson (1985). On the logic of theory change: Partial meet contraction and revision functions. ''Journal of Symbolic Logic'', 50:510–530.
* C. Boutilier (1993). Revision sequences and nested conditionals. In ''Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI'93)'', pages 519–525.
* C. Boutilier (1995). Generalized update: belief change in dynamic settings. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1550–1556.
* C. Boutilier (1996). Abduction to plausible causes: an event-based model of belief update. ''Artificial Intelligence'', 83:143–166.
* M. Cadoli, F. M. Donini, P. Liberatore, and M. Schaerf (1999). The size of a revised knowledge base. ''Artificial Intelligence'', 115(1):25–64.
* T. Chou and [[Marianne Winslett|M. Winslett]] (1991). Immortal: A model-based belief revision system. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 99–110. Morgan Kaufmann Publishers.
* M. Dalal (1988). Investigations into a theory of knowledge base revision: Preliminary report. In ''Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI'88)'', pages 475–479.
* T. Eiter and G. Gottlob (1992). On the complexity of propositional knowledge base revision, updates and counterfactuals. ''Artificial Intelligence'', 57:227–270.
* T. Eiter and G. Gottlob (1996). The complexity of nested counterfactuals and iterated knowledge base revisions. ''Journal of Computer and System Sciences'', 53(3):497–512.
* R. Fagin, J. D. Ullman, and M. Y. Vardi (1983). On the semantics of updates in databases. In ''Proceedings of the Second ACM SIGACT SIGMOD Symposium on Principles of Database Systems (PODS'83)'', pages 352–365.
* M. A. Falappa, G. Kern-Isberner, G. R. Simari (2002): Explanations, belief revision and defeasible reasoning. ''Artificial Intelligence'', 141(1–2): 1–28.
* M. Freund and D. Lehmann (2002). Belief Revision and Rational Inference. [http://arxiv.org/abs/cs.AI/0204032 Arxiv preprint cs.AI/0204032].
* N. Friedman and J. Y. Halpern (1994). A knowledge-based framework for belief change, part II: Revision and update. In ''Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR'94)'', pages 190–200.
* A. Fuhrmann (1991). Theory contraction through base contraction. ''Journal of Philosophical Logic'', 20:175–203.
* D. Gabbay, G. Pigozzi, and J. Woods (2003). Controlled Revision&nbsp;– An algorithmic approach for belief revision, ''Journal of Logic and Computation'', 13(1): 15–35.
* P. Gärdenfors and D. Makinson (1988). Revision of knowledge systems using epistemic entrenchment. In ''Proceedings of the Second Conference on Theoretical Aspects of Reasoning about Knowledge (TARK'88)'', pages 83–95.
* P. Gärdenfors and H. Rott (1995). Belief revision. In ''Handbook of Logic in Artificial Intelligence and Logic Programming, Volume 4'', pages 35–132. Oxford University Press.
* G. Grahne and [[Alberto O. Mendelzon]] (1995). Updates and subjunctive queries. ''Information and Computation'', 2(116):241–252.
* G. Grahne, [[Alberto O. Mendelzon]], and P. Revesz (1992). Knowledge transformations. In ''Proceedings of the Eleventh ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS'92)'', pages 246–260.
* S. O. Hansson (1999). ''A Textbook of Belief Dynamics''. Dordrecht: Kluwer Academic Publishers.
* A. Herzig (1996). The PMA revised. In ''Proceedings of the Fifth International Conference on the Principles of Knowledge Representation and Reasoning (KR'96)'', pages 40–50.
* A. Herzig (1998). Logics for belief base updating. In D. Dubois, D. Gabbay, H. Prade, and P. Smets, editors, ''Handbook of defeasible reasoning and uncertainty management'', volume 3 – Belief Change, pages 189–231. Kluwer Academic Publishers.
* H. Katsuno and A. O. Mendelzon (1991). On the difference between updating a knowledge base and revising it. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 387–394.
* H. Katsuno and A. O. Mendelzon (1991). Propositional knowledge base revision and minimal change. ''Artificial Intelligence'', 52:263–294.
* S. Konieczny and R. Pino Perez (1998). On the logic of merging. In ''Proceedings of the Sixth International Conference on Principles of Knowledge Representation and Reasoning (KR'98)'', pages 488–498.
* D. Lehmann (1995). Belief revision, revised. In ''Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI'95)'', pages 1534–1540.
* P. Liberatore (1997). The complexity of iterated belief revision. In ''Proceedings of the Sixth International Conference on Database Theory (ICDT'97)'', pages 276–290.
* P. Liberatore and M. Schaerf (1998). Arbitration (or how to merge knowledge bases). ''IEEE Transactions on Knowledge and Data Engineering'', 10(1):76–90.
* P. Liberatore and M. Schaerf (2000). BReLS: A system for the integration of knowledge bases. In ''Proceedings of the Seventh International Conference on Principles of Knowledge Representation and Reasoning (KR 2000)'', pages 145–152.
* D. Makinson (1985). How to give up: A survey of some formal aspects of the logic of theory change. ''Synthese'', 62:347–363.
* A. Perea (2003). ''Proper Rationalizability and Belief Revision in Dynamic Games''. Research Memoranda 048: METEOR, Maastricht Research School of Economics of Technology and Organization.
* B. Nebel (1991). Belief revision and default reasoning: Syntax-based approaches. In ''Proceedings of the Second International Conference on the Principles of Knowledge Representation and Reasoning (KR'91)'', pages 417–428.
* B. Nebel (1994). Base revision operations and schemes: Semantics, representation and complexity. In ''Proceedings of the Eleventh European Conference on Artificial Intelligence (ECAI'94)'', pages 341–345.
* B. Nebel (1996). How hard is it to revise a knowledge base? Technical Report 83, Albert-Ludwigs-Universität Freiburg, Institut für Informatik.
* G. Pigozzi (2005). Two aggregation paradoxes in social decision making: the Ostrogorski paradox and the [[discursive dilemma]], ''Episteme: A Journal of Social Epistemology'', 2(2): 33–42.
* G. Pigozzi (2006). [http://pigozzi.org/Pigozzi_Judgment_Aggregation.pdf Belief merging and the discursive dilemma: an argument-based account to paradoxes of judgment aggregation]. ''Synthese'' 152(2): 285–298.
* P. Z. Revesz (1993). On the semantics of theory change: Arbitration between old and new information. In ''Proceedings of the Twelfth ACM SIGACT SIGMOD SIGART Symposium on Principles of Database Systems (PODS'93)'', pages 71–82.
* K. Satoh (1988). Nonmonotonic reasoning by minimal belief revision. In ''Proceedings of the International Conference on Fifth Generation Computer Systems (FGCS'88)'', pages 455–462.
* {{cite book | last1=Shoham | first1=Yoav | last2=Leyton-Brown | first2=Kevin | title=Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations | publisher=[[Cambridge University Press]] | isbn=978-0-521-89943-7 | url=http://www.masfoundations.org | year=2009 | location=New York}} See Section 14.2; [http://www.masfoundations.org/download.html downloadable free online].
* V. S. Subrahmanian (1994). Amalgamating knowledge bases. ''ACM Transactions on Database Systems'', 19(2):291–331.
* A. Weber (1986). Updating propositional formulas. In ''Proc. of First Conf. on Expert Database Systems'', pages 487–500.
* M. Williams (1994). Transmutations of knowledge systems. In ''Proceedings of the Fourth International Conference on the Principles of Knowledge Representation and Reasoning (KR'94)'', pages 619–629.
* M. Winslett (1989). Sometimes updates are circumscription. In ''Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI'89)'', pages 859–863.
* M. Winslett (1990). ''Updating Logical Databases''. Cambridge University Press.
* Y. Zhang and N. Foo (1996). Updating knowledge bases with disjunctive information. In ''Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI'96)'', pages 562–568.

==External links==
* {{PhilPapers|category|belief-revision}}
* {{InPho|idea|1448|Logic of Belief Revision}}
* {{cite SEP |url-id=logic-belief-revision |title=Logic of Belief Revision}}
* [http://www.beliefrevision.org/ Beliefrevision.org]
* [http://plato.stanford.edu/entries/reasoning-defeasible/#4.3 Defeasible Reasoning: 4.3 Belief Revision Theory] at [[Stanford Encyclopedia of Philosophy]]

[[Category:Belief revision| ]]
[[Category:Belief]]
[[Category:Formal epistemology]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Logic programming]]
<=====doc_Id=====>:760
<=====title=====>:
Preferential entailment
<=====text=====>:
'''Preferential entailment''' is a [[non-monotonic logic]] based on selecting only [[model (logic)|model]]s that are considered the most plausible. The plausibility of models is expressed by an ordering among models called a preference relation, hence the name preference entailment.

Formally, given a [[propositional formula]] <math>F</math> and an ordering over propositional models <math>\leq</math>, preferential [[entailment]] selects only the models of <math>F</math> that are minimal according to <math>\leq</math>. This selection leads to a non-monotonic inference relation: <math>F \models_\text{pref} G</math> holds if and only if all minimal models of <math>F</math> according to <math>\leq</math> are also models of <math>G</math>.<ref name="s87">{{citation|last=Shoham|first=Y.|year=1987|contribution=Nonmonotonic logics: Meaning and utility|title=Proc. of the 10th Int. Joint Conf. on Artificial Intelligence (IJCAI’87)|pages=388–392|url=http://ijcai.org/Past%20Proceedings/IJCAI-87-VOL1/PDF/079.pdf}}.</ref>

[[Circumscription (logic)|Circumscription]] can be seen as the particular case of preferential entailment when the ordering is based on containment of the sets of variables assigned to true (in the propositional case) or containment of the extensions of predicates (in the first-order logic case).<ref name="s87"/>

==See also==
* [[Rational consequence relation]]

==References==
{{reflist}}

[[Category:Logic in computer science]]
[[Category:Knowledge representation]]
[[Category:Non-classical logic]]
<=====doc_Id=====>:763
<=====title=====>:
Category:Mereology
<=====text=====>:
{{Cat main|Mereology}}
[[Category:Philosophical logic]]
[[Category:Ontology]]
[[Category:Knowledge representation]]
[[Category:Materialism]]
[[Category:Quantity]]
<=====doc_Id=====>:766
<=====title=====>:
Region connection calculus
<=====text=====>:
{{no footnotes|date=November 2016}}
The '''region connection calculus''' ('''RCC''') is intended to serve for qualitative spatial representation and [[Spatial-temporal reasoning|reasoning]]. RCC abstractly describes regions (in [[Euclidean space]], or in a [[topological space]]) by their possible relations to each other. RCC8 consists of 8 basic relations that are possible between two regions:
* disconnected (DC)
* externally connected (EC)
* equal (EQ)
* partially overlapping (PO)
* tangential proper part (TPP)
* tangential proper part inverse (TPPi)
* non-tangential proper part (NTPP)
* non-tangential proper part inverse (NTPPi)
From these basic relations, combinations can be built. For example, proper part (PP) is the union of TPP and NTPP.
[[Image:RCC8.jpg]]

==Composition table==
The composition table of RCC8 are as follows:

<center>
{| class="wikitable" style="text-align:center;" border="1"
! o
! DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ
|-
! DC
| * || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC,EC,PO,TPP,NTPP || DC || DC || DC
|-
! EC
| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPP,NTPP || EC,PO,TPP,NTPP || PO,TPP,NTPP || DC,EC || DC || EC
|-
! PO
| DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || * || PO,TPP,NTPP || PO,TPP,NTPP || DC,EC,PO,TPPi,NTPPi || DC,EC,PO,TPPi,NTPPi || PO
|-
! TPP
| DC || DC,EC || DC,EC,PO,TPP,NTPP || TPP,NTPP || NTPP || DC,EC,PO,TPP,TPPi,EQ || DC,EC,PO,TPPi,NTPPi || TPP
|-
! NTPP
| DC || DC || DC,EC,PO,TPP,NTPP || NTPP || NTPP || DC,EC,PO,TPP,NTPP|| * || NTPP
|-
! TPPi
| DC,EC,PO,TPPi,NTPPi || EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,TPPi,EQ || PO,TPP,NTPP || TPPi,NTPPi || NTPPi || TPPi
|-
! NTPPi
| DC,EC,PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPPi,NTPPi || PO,TPP,NTPP,TPPi,NTPPi,EQ|| NTPPi || NTPPi || NTPPi
|-
! EQ
|  DC || EC || PO || TPP || NTPP || TPPi || NTPPi || EQ
|}
</center>

*  "*" denotes the universal relation.

==Examples==

The RCC8 calculus is intended for reasoning about spatial configurations. Consider the following example: two houses are connected via a road. Each house is located on an own property. The first house possibly touches the boundary of the property; the second one surely does not. What can we infer about the relation of the second property to the road? 

The spatial configuration can be formalized in RCC8 as the following [[constraint network]]:

 house1 DC house2
 house1 {TPP, NTPP} property1
 house1 {DC, EC} property2
 house1 EC road
 house2 { DC, EC } property1
 house2 NTPP property2
 house2 EC road
 property1 { DC, EC } property2
 road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property1
 road { DC, EC, TPP, TPPi, PO, EQ, NTPP, NTPPi } property2

Using the RCC8 [[composition table]] and the [[path-consistency algorithm]], we can refine the network in the following way:
 road { PO, EC } property1
 road { PO, TPP } property2

That is, the road either overlaps with the second property, or is even (tangential) part of it.

Other versions of the region connection calculus include RCC5 (with only five basic relations - the distinction whether two regions touch each other are ignored) and RCC23 (which allows reasoning about convexity).

==RCC8 use in GeoSPARQL==

RCC8 has been partially{{Clarify|date=January 2016}} implemented in [[GeoSPARQL]] as described below:
[[File:Region_Connection_Calculus_8_Relations_and_Open_Geospatial_Consortium_relations.svg|thumb|center|700px|alt=A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.|A graphical representation of Region Connection Calculus (RCC: Randell, Cui and Cohn, 1992) and the links to the equivalent naming by the Open Geospatial Consortium (OGC) with their equivalent URIs.]]

==References==
* Randell, D. A., Cui, Z. and Cohn, A. G.:  [http://wenxion.net/ac/randell92spatial.pdf A spatial logic based on regions and connection], Proc. 3rd Int. Conf. on Knowledge Representation and Reasoning, Morgan Kaufmann, San Mateo, pp. 165–176, 1992.
* Anthony G. Cohn, Brandon Bennett, John Gooday, Micholas Mark Gotts: Qualitative Spatial Representation and Reasoning with the Region Connection Calculus. GeoInformatica, 1, 275–316, 1997.
* J. Renz: [http://www.springerlink.com/content/d5g7fcjkd0q2/ Qualitative Spatial Reasoning with Topological Information]. Lecture Notes in Computer Science 2293, Springer Verlag, 2002.
* T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC⁺⁺]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352 

[[Category:Reasoning]]
[[Category:Knowledge representation]]
[[Category:Constraint programming]]
[[Category:Computational topology]]
[[Category:Logical calculi]]
<=====doc_Id=====>:769
<=====title=====>:
DogmaModeler
<=====text=====>:
{{unreferenced|date=March 2014}}
[[Image:DogmaModeler1.jpg|thumb|DogmaModeler Screenshot]]
'''DogmaModeler''' is a free and open source ([[GNU GPL]]) [[Ontology (computer science)|ontology]] modeling tool based on [[object-role modeling]] (ORM). The philosophy of DogmaModeler is to enable non-IT experts to model ontologies with a little or no involvement of an ontology engineer. This challenge is tackled in DogmaModeler through well-defined methodological principles: the (1) [[Ontology double articulation|double-articulation]] and the (2) [[modularization]] principles. Other important features are: (3) the use of ORM as a graphical notation for ontology modeling; (4) the verbalization of ORM diagrams into pseudo natural language (supporting flexible verbalization templates for 11 human languages, including English, Dutch, German, French, Spanish, Arabic, Russian, etc.) that allows non-experts to check, validate, or build ontologies; (5)the automatic composition of ontology modules, through a well-defined composition operator; (6) the incorporation of linguistic resources in [[ontology engineering]]; (7) the automatic mapping of ORM diagrams into the DIG [[description logic]] interface and reasoning using [[RACER system|Racer]]; and many other functionalities.

The first version of DogmaModeler was developed at the [[Vrije Universiteit Brussel]].

== See also ==
* [[DOGMA]] 
* [[NORMA (software modeling tool)]]
* [[Protégé (software)|Protégé]]

==References==
{{reflist}}

== External links ==
* {{Official website|http://www.jarrar.info/Dogmamodeler/index.htm}}

[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]
<=====doc_Id=====>:772
<=====title=====>:
FrameNet
<=====text=====>:
{{refimprove|date=March 2012}}
In [[computational linguistics]], '''FrameNet''' is a project housed at the [[International Computer Science Institute]] in [[Berkeley, California]] which produces an electronic resource based on a theory of meaning called
[[Frame semantics (linguistics)|frame semantics]]. FrameNet reveals for example that the sentence "John sold a car to Mary" essentially describes the same basic situation (semantic frame) as "Mary bought a car from John", just from a different perspective. A semantic frame can be thought of as a conceptual structure describing an event, relation, or object and the participants in it. The FrameNet [[lexical database]] contains over 1,200 semantic ''frames'', 13,000 ''lexical units'' (a pairing of a [[word]] with a [[Meaning (linguistics)|meaning]]; [[Polysemy|polysemous]] words are represented by several ''lexical units'') and 202,000 example sentences. FrameNet is largely the creation of [[Charles J. Fillmore]], who developed the theory of frame semantics that the project is based on, and was initially the project leader when the project began in 1997.<ref name="Goddard2011">{{cite book|author=Cliff Goddard|title=Semantic Analysis: A Practical Introduction|url=https://books.google.com/books?id=qfar1cmATvUC&pg=PA78|accessdate=21 March 2012|date=25 September 2011|publisher=Oxford University Press|isbn=978-0-19-956028-8|pages=78–81}}</ref> Collin Baker became the project manager in 2000.<ref name="Linguistic Analysis">{{cite book|title=The Oxford Handbook of Linguistic Analysis|url=https://books.google.com/books?id=7plqH2gSq1wC&pg=PP20|accessdate=21 March 2012|editor1-last=Heine|editor1-first=Bernd|editor2-last=Narrog|editor2-first=Heiko|publisher=Oxford University Press|isbn=978-0-19-160925-1|page=20}}</ref> The FrameNet project has been influential in both linguistics and natural language processing, where it led to the task of automatic [[Semantic Role Labeling]].

==Concepts==

===Frames===
A frame is a schematic representation of a situation involving various participants, props, and other conceptual roles. Examples of frame names are <tt>Being_born</tt> and <tt>Locative_relation</tt>. A frame in FrameNet contains a textual description of what it represents (a frame definition), associated frame elements, lexical units, example sentences, and frame-to-frame relations.

===Frame elements===
Frame elements (FE) provide additional information to the semantic structure of a sentence. Each frame has a number of core and non-core FEs which can be thought of as semantic roles. Core FEs are essential to the meaning of the frame while non-core FEs are generally descriptive (such as time, place, manner, etc.).<ref>https://framenet.icsi.berkeley.edu/fndrupal/glossary#core</ref>

Some examples include:

* The only core FE of the <tt>Being_born</tt> frame is called <tt>Child</tt>; non-core FEs being <tt>Time</tt>, <tt>Place</tt>, <tt>Relatives</tt>, etc.<ref>https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&name=Being_born</ref>
* Core FEs of the <tt>Commerce_goods-transfer</tt> include the <tt>Seller</tt>, <tt>Buyer</tt>, <tt>Goods</tt>, among other things, while non-core FEs include a <tt>Place</tt>, <tt>Purpose</tt>, etc.<ref>https://framenet.icsi.berkeley.edu/fndrupal/index.php?q=frame_report&name=Commerce_goods-transfer</ref>

FrameNet includes shallow data on syntactic roles that frame elements play in the example sentences. For an example sentence like "She was born about AD 460", FrameNet would mark "She" as a [[noun phrase]] referring to the <tt>Child</tt> FE, and "about AD 460" as a [[noun phrase]] corresponding to the <tt>Time</tt> frame element. Details of how frame elements can be realized in a sentence are important because this reveals important information about the [[subcategorization frame]]s as well as possible [[diathesis alternation]]s (e.g. "John broke the window" vs. "The window broke")
of a verb.

===Lexical units===
Lexical units (LU) are lemmas, with their part of speech, that evoke a specific frame. In other words, when a LU is identified in a sentence, that specific LU can be associated with its specific frame(s). For each frame, there are many LUs associated to one frame and many frames that share multiple LUs, this is typically the case with LUs that have multiple word senses.<ref>https://framenet.icsi.berkeley.edu/fndrupal/glossary</ref> Alongside the frame, each lexical unit is associated with specific frame elements by means of the annotated example sentences.

Example:

Lexical units that evoke the <tt>Complaining</tt> frame (or more specific perspectivized versions of it, to be precise), include the verbs "complain", "grouse", "lament", and others.<ref>https://framenet2.icsi.berkeley.edu/fnReports/data/frameIndex.xml?frame=Complaining</ref>

===Example sentences===
Frames are associated with example sentences and frame elements are marked within the sentences. Thus the sentence
:''She was '''born''' about AD 460''
is associated with the frame <tt>Being_born</tt>, while "She" is marked as the frame element <tt>Child</tt> and "about AD 460" is marked as <tt>Time</tt>.
(See the [http://framenet.icsi.berkeley.edu/fnReports/displayReport.php?anno=9791 FrameNet Annotation Report] for <tt>born.v</tt>.)
From the start, the FrameNet project has been committed to looking at evidence from actual language use as found in text collections like the [[British National Corpus]]. 
Based on such example sentences, automatic [[semantic role labeling]] tools are able to determine frames and mark frame elements in new sentences.

===Valences===
FrameNet also exposes the statistics on the ''valences'' of the ''frames'', that is the number and the position of the ''frame elements'' within example sentences. The sentence
:''She was '''born''' about AD 460''
falls in the valence pattern
:'''NP Ext, INI --, NP Dep'''
which occurs two times in the [https://framenet2.icsi.berkeley.edu/fnReports/data/lu/lu9791.xml example sentences] in FrameNet,
namely in:
:She'' was '''born''' ''about AD 460'', daughter and granddaughter of Roman and Byzantine emperors, whose family had been prominent in Roman politics for over 700 years.''
:''He was soon posted to north Africa, and never met their only child, ''a daughter'' '''born''' ''8 June 1941''.''

===Frame Relations===

FrameNet additionally captures relationships between different frames using relations. These include the following.

* Inheritance: When one frame is a more specific version of another, more abstract parent frame. Anything that is true about the parent frame must also be true about the child frame, and a mapping is specified between the frame elements of the parent and the frame elements of the child.
* Perspectivized_in: A neutral frame (like <tt>Commerce_transfer-goods</tt>) is connected to a frame with a specific perspective of the same scenario (e.g. the <tt>Commerce_sell</tt> frame, which assumes the perspective of the seller or the <tt>Commerce_buy</tt> frame, which assumes the perspective of the buyer)
* Subframe: Some frames like the <tt>Criminal_process</tt> frame refer to complex scenarios that consist of several individual states or events that can be described by separate frames like <tt>Arrest</tt>, <tt>Trial</tt>, and so on.
* Precedes: The Precedes relation captures a temporal order that holds between subframes of a complex scenario.
* Causative_of and Inchoative_of: There is a fairly systematic relationship between stative descriptions (e.g. the <tt>Position_on_a_scale</tt> frame, "She had a high salary") and causative descriptions (<tt>Cause_change_of_scalar_position</tt>, "She raised his salary") or inchoative descriptions (<tt>Change_position_on_a_scale</tt>, e.g. "Her salary increased").
* Using: A relationship that holds between a frame that in some way involves another frame. For instance, the <tt>Judgment_communication</tt> frame uses both the <tt>Judgment</tt> frame and the <tt>Statement</tt> frame, but does not inherit from either of them because there is no clear correspondence of the frame elements.
* See_also: Connects frames that bear some resemblance but need to be distinguished carefully.

==Applications==

FrameNet has proven useful in a number of computational applications, because computers need additional knowledge in order to recognize that "John sold a car to Mary" and "Mary bought a car from John" describe essentially the same situation, despite using two very different verbs, different prepositions and a different word order. FrameNet has been used in applications like [[question answering]], paraphrasing, recognizing textual entailment, and information extraction, either directly or by means of [[Semantic Role Labeling]] tools. The first automatic system for [[Semantic Role Labeling]] (SRL, sometimes also referred to as "shallow semantic parsing") was developed by Daniel Gildea and Daniel Jurafsky based on FrameNet in 2002, and Semantic Role Labelling has since become one of the standard tasks in natural language processing.

Since frames are essentially semantic descriptions, they are similar across languages, and several projects have arisen over the years that have relied on the original FrameNet as the basis for additional non-English FrameNets, for Spanish, Japanese, German, and Polish, among others.

==See also==
*[[BabelNet]]: a multilingual semantic network integrating FrameNet
*[[PropBank]]
*[[Null instantiation]]
*[[Frame language]]
*[[UBY]]: a database of 10 resources including FrameNet

==References==
{{Reflist}}

===Further reading===
*[https://framenet2.icsi.berkeley.edu/docs/r1.5/book.pdf FrameNet II: Extended Theory and Practice] (e-book)

==External links==
*[http://framenet.icsi.berkeley.edu/ FrameNet home page]
*[http://sccfn.sxu.edu.cn/ Chinese FrameNet]
*[http://framenet.dk/ Danish FrameNet]
*[http://gframenet.gmc.utexas.edu/ German FrameNet]
*[http://jfn.st.hc.keio.ac.jp/ Japanese FrameNet]
*[http://www.ramki.uw.edu.pl/en/index.html Polish FrameNet]
*[http://www.ufjf.br/framenetbr/ Portuguese FrameNet (Brazil)]
*[http://gemini.uab.es/SFN/ Spanish FrameNet]
*[http://spraakbanken.gu.se/eng/swefn/ Swedish FrameNet]

[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Corpus linguistics]]
[[Category:History of the Internet]]
[[Category:Hypertext]]
[[Category:Online dictionaries]]
[[Category:Science and technology in the San Francisco Bay Area]]
<=====doc_Id=====>:775
<=====title=====>:
BCM Classification
<=====text=====>:
The '''British Catalogue of Music Classification''' (BCM Classification)<ref>The British Catalogue of Music Classification / compiled for the Council of the British National Bibliography Ltd. by E. J. Coates, F.L.A.  London : Council of the British National Bibliography, 1960</ref> is a [[faceted classification]] that was commissioned from E. J. Coates by the Council of the British National Bibliography to organize the content of the British Catalogue of Music.<ref>{{cite web|url=https://archive.org/details/britishcatalogue001781mbp |title=Internet Archive: Details: The British Catalogue Of Music 1960 |publisher=Archive.org |date= |accessdate=2016-10-06}}</ref> The published schedule (1960) was considerably expanded by Patrick Mills of the British Library up until its use was abandoned in 1998. Entries in the catalogue were organized by BCM classmark from the catalogue's inception in 1957 until 1982. From that year the British Catalogue of Music (which from 1974 onward was published by [[The British Library]]) was organized instead by [[Dewey Decimal Classification]] number, though BCM classmarks continued to be added to entries up to the 1998 annual cumulation.

The schedule is divided into two main parts: A-B representing Musical literature and C-Z representing Music — Scores and Parts.  There are also seven auxiliary tables dealing with various sub-arrangements, sets of ethnic/locality subdivisions and chronological reference points.

The notation is retroactive using uppercase alphabetic characters omitting I and O, with the addition of slash / and parentheses ( ) which have specific anteriorizing functions.  Retroactive notation requires that the classifier combines terms in reverse schedule order. This has the benefit of producing a compact notation by removing the need for facet indicators.

The schedule at A (Music Literature) parallels that from the Scores and Parts schedules thus Choral Music is at D while books about Choral Music are at AD; Harp Music is at TQ so books on harp music are at ATQ.  The schedule at B accommodates books about specific composers and music in non-European traditions.

As a fully faceted scheme after the ideas of [[S. R. Ranganathan]], BCM class numbers are capable of being chain-indexed, allowing index access to each step of the hierarchy.

BCM classification had a strong influence on Russell Sweeney's so-called Phoenix Dewey 780 schedule<ref>DDC Dewey Decimal Classification : proposed revision of 780 music / prepared under the direction of Russell Sweeney and John Clews with assistance from Winton E. Mathews, Jr. Albany N.Y. : Forest Press, 1980. ISBN 0-910608-25-3</ref> which in turn influenced the 780 Music schedule in the 20th edition of [[Dewey Decimal Classification]].  The music schedule of the second edition of the Bliss Classification<ref>{{cite web|url=http://library.music.indiana.edu/tech_s/mla/facacc.rev |title=Archived copy |accessdate=2008-06-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20080512000152/http://library.music.indiana.edu:80/tech_s/mla/facacc.rev |archivedate=2008-05-12 |df= }}</ref> is also strongly influenced by BCM.

This classification system is still in use at a number of libraries, including the [[State Library of Western Australia]]<ref>{{cite web|url=http://www.slwa.wa.gov.au/find/guides/music/general_information/british_catalogue_of_music_classification_scheme |title=(accessed 2015-12-17) |publisher=Slwa.wa.gov.au |date=2013-08-20 |accessdate=2016-10-06}}</ref> and the Library at [[Edith Cowan University]].<ref>{{cite web|url=http://ecu.au.libguides.com/c.php?g=410622&p=2797056 |title=(accessed 2015-12-17) |publisher=Ecu.au.libguides.com |date=2016-08-05 |accessdate=2016-10-06}}</ref>

==References==
{{Reflist}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:778
<=====title=====>:
Semantic knowledge management
<=====text=====>:
{{Orphan|date=February 2009}}
'''Semantic knowledge management''' is a set of practices that seeks to classify content so that the knowledge it contains may be immediately accessed and transformed for delivery to the desired audience, in the required format. This classification of content is semantic in its nature &ndash; identifying content by its type or meaning within the content itself and via external, descriptive metadata – and is achieved by employing [[XML]] technologies.

The specific outcomes of these practices are:

* Maintain content for multiple audiences together in a single document 
* Transform content into various delivery formats without re-authoring  
* Search for content more effectively 
* Involve more [[subject-matter expert]]s in the creation of content without reducing quality 
* Reduce production costs for delivery formats 
* Reduce the manual administration of getting the right knowledge to the right people 
* Reduce the cost and time to localize content

==References==
{{refbegin}}
* {{cite book|title=Semantic Knowledge Management: Integrating Ontology Management, Knowledge Discovery, and Human Language Technologies|author1=John Davies |author2=Marko Grobelnik |author3=Dunja Mladenic |isbn=3-540-89164-1|year=2008}}
{{refend}}

== Notable semantic knowledge management systems ==
*Learn eXact
*Thinking Cap LCMS
*Thinking Cap LMS
*Xyleme LCMS

[[Category:Knowledge representation]]
<=====doc_Id=====>:781
<=====title=====>:
Darwin Core
<=====text=====>:
'''Darwin Core''' (often abbreviated to '''DwC''') is an extension of [[Dublin Core]] for [[biodiversity informatics]]. It is meant to provide a stable standard reference for sharing information on biological diversity.<ref>{{cite journal|last=Wieczorek|first=John|author2=D. Bloom |author3=R. Guralnick |author4=S. Blum |author5=M. Döring |author6=R. De Giovanni |author7=T. Robertson |author8=D. Vieglais |title=Darwin Core: An Evolving Community-developed Biodiversity Data Standard.|journal=[[PLoS ONE]] |year=2012|volume=7|issue=1|doi=10.1371/journal.pone.0029715|pmid=22238640|pmc=3253084}}</ref> The terms described in this standard are a part of a larger set of vocabularies and technical specifications under development and maintained by [[Biodiversity Information Standards (TDWG)]] (formerly known as the Taxonomic Databases Working Group (TDWG)).

== Description ==
The Darwin Core is a body of standards. It includes a glossary of terms (in other contexts these might be called properties, elements, fields, columns, attributes, or concepts) intended to facilitate the sharing of information about biological diversity by providing reference definitions, examples, and commentaries. The Darwin Core is primarily based on [[taxon|taxa]], their occurrence in nature as documented by observations, specimens, and samples, and related information. Included in the standard are documents describing how these terms are managed, how the set of terms can be extended for new purposes, and how the terms can be used. The '''Simple Darwin Core''' <ref name="simpledwc">[http://rs.tdwg.org/dwc/terms/simple/index.htm  The Simple Darwin Core]</ref> is a specification for one particular way to use the terms and to share data about taxa and their occurrences in a simply-structured way. It is likely what is meant if someone were to suggest "formatting your data according to the Darwin Core".

Each '''term''' has a definition and commentaries that are meant to promote the consistent use of the terms across applications and disciplines. Evolving commentaries that discuss, refine, expand, or translate the definitions and examples are referred to through links in the Comments attribute of each term. This approach to documentation allows the standard to adapt to new purposes without disrupting existing applications. There is meant to be a clear separation between the terms defined in the standard and the applications that make use of them. For example, though the data types and constraints are not provided in the term definitions, recommendations are made about how to restrict the values where appropriate.

In practice, Darwin Core decouples the definition and semantics of individual terms from application of these terms in different technologies such as [[XML]], [[Resource Description Framework|RDF]] or simple [[Comma-separated values|CSV]] text files. Darwin Core provides separate guidelines on how to encode the terms as XML<ref name="dwc-xml" >[http://rs.tdwg.org/dwc/terms/guides/xml/index.htm Darwin Core XML Guide]</ref> or text files.<ref name="dwc-text">[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guide]</ref>

== History ==
Darwin Core was originally created as a [[Z39.50]] profile by the Z39.50 Biology Implementers Group (ZBIG), supported by funding from a USA National Science Foundation award.<ref name="zbig">An Experimental Z39.50 Information Retrieval Protocol Test Bed for Biological Collection and Taxonomic Data, #9811443 [http://nsf.gov/awardsearch/showAward.do?AwardNumber=9811443]</ref>  The name "Darwin Core" was first coined by Allen Allison at the first meeting of the ZBIG held at the University of Kansas in 1998 while commenting on the profile's conceptual similarity with Dublin Core. The Darwin Core profile was later expressed as an XML Schema document for use by the Distributed Generic Information Retrieval (DiGIR) protocol. A [[TDWG]] task group was created to revise the Darwin Core, and a ratified metadata standard was officially released on 9 October 2009.

Though ratified as a TDWG/[[Biodiversity Information Standards]] standard since then, Darwin Core has had numerous previous versions in production usage. The published standard contains a history<ref name="history">[http://rs.tdwg.org/dwc/terms/history/index.htm Darwin Core History]</ref> with details of the versions leading to the current standard.

{| class="wikitable" style="text-align:left"
|+Darwin Core Versions
|-
! Name !! Namespace !! Number of terms !! XML Schema !! Date Issued
|-
! Darwin Core 1.0
| Not Applicable || 24 || (Z39.50 GRS-1) || 1998
|-
! Darwin Core 1.2 (Classic)
| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 46 || [http://digir.net/schema/conceptual/darwin/2003/1.0/darwin2.xsd] || 2001-09-11
|-
! Darwin Core 1.21 (MaNIS/HerpNet/ORNIS/FishNet2)
| http://digir.net/schema/conceptual/darwin/2003/1.0  {{dead link|date=October 2016}} || 63 || [http://digir.net/schema/conceptual/darwin/manis/1.21/darwin2.xsd] || 2003-03-15
|-
! Darwin Core OBIS
| http://www.iobis.org/obis  {{dead link|date=October 2016}} || 27 || [http://iobis.org/obis/obis.xsd] || 2005-07-10
|-
! Darwin Core 1.4 (Draft Standard)
| http://rs.tdwg.org/dwc/dwcore/  {{dead link|date=October 2016}} || 45 || [http://rs.tdwg.org/dwc/tdwg_dw_core.xsd] || 2005-07-10
|-
! Darwin Core Terms (properties)
| http://rs.tdwg.org/dwc/terms/ || 172 || [http://rs.tdwg.org/dwc/xsd/tdwg_dwcterms.xsd] || 2009-10-09
|-
|}

== Key Projects Using Darwin Core ==
* The [[Global Biodiversity Information Facility]] (GBIF)<ref>{{cite web|url=http://www.gbif.org/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives/ |title=Darwin Core |publisher=[[Global Biodiversity Information Facility]] |accessdate=April 12, 2011 |deadurl=yes |archiveurl=https://web.archive.org/web/20110412022331/http://www.gbif.org:80/informatics/standards-and-tools/publishing-data/data-standards/darwin-core-archives |archivedate=April 12, 2011 |df= }}</ref>
* The [[Ocean Biogeographic Information System]] (OBIS)<ref>{{cite web|url=http://www.iobis.org/data/schema-and-metadata|title=Data Schema and metadata|publisher=[[Ocean Biogeographic Information System]]|accessdate=April 12, 2011}}</ref>
*[http://www.ala.org.au/datastandards.htm The Atlas of Living Australia (ALA)]
*[http://www3.interscience.wiley.com/cgi-bin/fulltext/120713092/PDFSTART Online Zoological Collections of Australian Museums (OZCAM)]
*[http://manisnet.org Mammal Networked Information System (MaNIS)]
*[http://ornisnet.org Ornithological Information System (ORNIS)]
*[http://www.fishnet2.net/index.html FishNet 2]
*[http://vertnet.org VertNet]
*[http://www.canadensys.net/ Canadensys]
*[http://w3.ufsm.br/herbarioflorestal/nature/site/ Sistema Nature 3.0]
*[http://eol.org Encyclopedia of Life]
*[https://www.idigbio.org Integrated Digitized Biocollections (iDigBio)] <ref>{{cite web|title=Data Ingestion Guidance|url=https://www.idigbio.org/wiki/index.php/Data_Ingestion_Guidance|publisher=[[iDigBio]]|accessdate=26 September 2016}}</ref> <ref>{{cite web|title=Getting your data out there: Data publishing & data standards with iDigBio|url=https://www.idigbio.org/content/getting-your-data-out-there-data-publishing-data-standards-idigbio|publisher=[[iDigBio]]|accessdate=26 September 2016}}</ref>

== See also ==
* [[Darwin Core Archive]]
* [[Biodiversity Information Standards]] (TDWG)
* [[Biodiversity]]
* [[Biodiversity informatics]]
* [[Metadata standards]]

==References==
{{reflist}}

==External links==
*[http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]
*[https://github.com/tdwg/dwc/ Darwin Core Development Site]
*[http://www.tdwg.org/activities/darwincore/ Official Darwin Core Website]
*[http://www.tdwg.org/fileadmin/subgroups/dwc/exec_summary_dwc.doc Executive Summary of Darwin Core]

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]
[[Category:Metadata standards]]
<=====doc_Id=====>:784
<=====title=====>:
Sears Subject Headings
<=====text=====>:
#REDIRECT [[Minnie Earl Sears]] {{R with possibilities}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:787
<=====title=====>:
Chinese Library Classification
<=====text=====>:
The '''Chinese Library Classification''' ({{Zh|c = 中国图书馆分类法|s = |t = }}; CLC), also known as '''Classification for Chinese Libraries''' (CCL){{FACT|date=December 2016}}, is effectively the national [[library classification]] scheme in [[China]]. It is used in almost all primary and secondary schools, universities, academic institutions, as well as public [[libraries]]. It is also used by publishers to classify all books published in China.

The '''Book Classification of Chinese Libraries''' (BCCL) was first published in 1975, under the auspices of China's Administrative Bureau of Cultural Affairs. Its fourth edition (1999) was renamed CLC. In September 2010, the fifth edition was published by National Library of China Publishing House.
CLC has twenty-two top-level categories, and inherits a [[Marxist]] orientation from its earlier editions.<ref>[http://research.dils.tku.edu.tw/joemls/41/41-1/1-22.pdf Zhang, Wenxian (2003). ''Classification for Chinese Libraries (CCL): Histories, accomplishments, problems and its comparisons''. Journal of Educational Media & Library Sciences, vol. 41, nr. 1, p. 1-22.] (PDF)</ref> (For instance, category A is [[Marxism]], [[Leninism]], [[Maoism]] & [[Deng Xiaoping Theory]].) It contains a total of 43600 categories, many of which are recent additions, meeting the needs of a rapidly changing nation.<ref>''The Standardization of Chinese Library Classification'', Xiaochun Liu, [[Cataloging & Classification Quarterly]], Volume 16, Issue 2, ISSN 0163-9374, Pub Date: 8/13/1993
</ref>

== The CLC System ==
The 22 top categories and selected sub-categories of CLC (5th Edition) are as follows:

=== A.  [[Marxism]], [[Leninism]], [[Maoism]] & [[Deng Xiaoping Theory]] ===
* A1 The Works of [[Karl Marx]] and [[Friedrich Engels]]
* A2 The Works of [[Vladimir Lenin]]
* A3 The Works of [[Joseph Stalin]]
* A4 The Works of [[Mao Zedong]]
**   A49 The works of [[Deng Xiaoping]]
* A5 The Symposium/Collection of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping
* A7 The biobibliography and [[biography]] of Marx, Engels, Lenin, Stalin, Mao and Deng Xiaoping
* A8 Study and Research of Marxism, Leninism, Maoism & Deng Xiaoping Theory

=== B.  [[Philosophy]] and [[Religions]] ===
* B-4 Education and dissemination of philosophy
** B-49 Learners' book und popular literature of philosophy
* B0 theory of philosophy
** B0-0 Marxist philosophy
** B01 Basic problems of philosophy
*** B014 Object, purpose and method of philosophy
*** B015 [[Materialism]] and [[idealism]]
*** B016 [[Ontology]]
**** B016.8 [[Cosmology]]
**** B016.9 Time-space-theory
*** B017 [[Epistemology]]
**** B017.8 [[Determinism]] and [[Indeterminism]]
**** B017.9 Self Theory
*** B018 [[Axiology]]
**** B019.1 Materialism
***** B019.11 Naive materialism
***** B019.12 Metaphysical materialism
***** B019.13 [[Dialectical materialism]]
**** B019.2 [[Idealism]]
** B02 [[Dialectical materialism]]
*** B024 [[Materialist dialectics]]
*** B025 Categories of materialist dialectics
*** B026 [[Methodology]]
*** B027 Application of dialectical materialism
*** B028 [[Natural philosophy]]
*** B029 [[Dialectics of nature]]
** B03 [[Historical materialism]]
*** B031 Social material requirements of life
*** B032 Basic social conflict
**** B032.1 [[Productive forces]] und [[relations of production]]
**** B032.2 [[Base and superstructure]]
*** B033 [[Marxian Class Theory|Class Theory]]
*** B034 [[Marxism#Revolution|Theory of Revolution]]
*** B035 Theory of country
*** B036 [[Social being]] and [[social consciousness]]
*** B037 [[On Contradiction#Basics of Contradiction and its History|Contradictions among the People]]
*** B038 Role of the people in historical development
** B08 [[Philosophical schools]] and research
*** B081 [[Idealism]]
**** B081.1 [[Metaphysics]]
**** B081.2 Epistemology of idealism, apriorism
*** B082 [[Positivism]], [[Machism]]
*** B083 [[Voluntarism (metaphysics)|Voluntarism]] and [[philosophy of life]]
*** B084 [[Neo-Kantianism]] and [[Neohegelianism]]
*** B085 [[New realism (philosophy)|Neorealism]], [[logical positivism]] (new positivism, logical empiricism)
*** B086 [[Existentialism]] ([[survivalism]])
*** B087 [[Pragmatism]]
*** B088 [[Neo-Thomism]] (new scholasticism)
*** B089 Other philosophical schools
**** B089.1 [[Western Marxism]]
**** B089.2 [[Philosophical hermeneutics]]
**** B089.3 [[Philosophical anthropology]]
*  B1 Philosophy (Worldwide)
*  B2 Philosophy in China
**    B22 Pre-[[Qin Dynasty]] Philosophy (~before 220 BC)
***       B222 The Confucian School
****           B222.2 [[Confucius]] (Kǒng Qiū, 551-479 BC)
*  B3 Philosophy in Asia
*  B4 [[African philosophy|Philosophy in Africa]]
*  B5 Philosophy in Europe
*  B6 Philosophy in Australasia
*  B7 [[Philosophy in America]]
*  B8 [[Cognitive science]]
*  B9 Religions
**    B91 [[Sociology of Religion]], [[Religion]] and [[Science]]
**    B92 [[Philosophy of religion|Philosophy]] and [[History of Religion]]
**    B93 [[Mythology]] and [[Animism|Primitive religion]]
**    B94 [[Buddhism]]
**    B95 [[Taoism]]
**    B96 [[Islam]]
**    B97 [[Christianity]]
***       B971 [[Bible]]
****           B971.1 [[Old Testament]]
****           B971.2 [[New Testament]]
***       B972 [[Doctrine]], [[Theology]]
***       B975 [[Evangelism]], [[Sermon]]
***       B976 [[Christian Denomination]]
****           B976.1 [[Roman Catholic Church]]
****           B976.2 Orthodox Christianity ([[Eastern Orthodox Church|Eastern Orthodoxy]], [[Oriental Orthodoxy]])
****           B976.3 [[Protestantism]] ([[Protestant Reformation]])
***       B977 [[Ecclesiastical polity]]
***       B978 Research on Christianity
***       B979 [[History of Christianity]]
****           B979.9 Biography
**    B98 Other Religions
**    B99 [[Augury]], [[Superstition]]

=== C.  [[Social Sciences]] ===
*  C0 Social Scientific Theory and Methodology
*  C1 Present and Future of Social Sciences
*  C2 Organisations, Groups, Conferences
*  C3 Method of Research in Social Sciences
*  C4 Education and Popularization of Social Sciences
*  C5 Serials, [[Anthology|Anthologies]], Periodicals in Social Sciences
*  C6 Reference Materials in Social Sciences
*  C7 (no longer used)
*  C8 Statistics in Social Sciences
*  C9 [[Sociology]]

=== D.  [[Politics]] and [[Law]] ===
*  D0 [[Political theory]]
*  D1 International Campaign of [[Communism]]
*  D2 [[Communist Party of China]]
*  D3 [[Communist Parties]] of other Countries
*  D4 Labor, Peasant, Youth, Female Organizations and Movements
*  D5 Politics (worldwide)
*  D6 Politics in China
*  D7 Politics in individual [[Countries]]
*  D8 [[Diplomacy]], [[International relations]]
*  D9 [[Law]]

=== E.  [[Military Science]] ===
*  E0 Military Theory
*  E1 Military (worldwide)
*  E2 [[Military in China]]
*  E3 Military in Asia
*  E4 Military in Africa
*  E5 Military in Europe
*  E6 Military in Australasia
*  E7 Military in America
*  E8 Strategies, Tactics, and Battles
*  E9 [[Military technology|Military Technology]]

=== F.  [[Economics]] ===
*  F0 [[Economics]]
*  F1 [[Economics]], [[Economic history]] and [[Economic geography]] of individual countries
*  F2 Economic Planning and Management
*  F3 [[Agricultural Economics]]
*  F4 Industrial Economics
*  F5 Economics of [[Transport]]
*  F6 Economics of Postal and Cable Services
*  F7 Economics of [[Commerce]]
*  F8 [[Finance]], [[Banking]]

=== G.  [[Culture]], [[Science]], [[Education]] and [[Sports]] ===
*  G0 Philosophy of Culture
*  G1 Culture
*  G2 Knowledge transmission
*  G3 [[Science]], [[Scientific Research]]
*  G4 [[Education]]
*  G5 Education in individual [[Countries]]
*  G6 Education (Primary, Secondary, Tertiary)
*  G7 Education (specialized)
*  G8 Sports

=== H.  [[Languages]] and [[Linguistics]] ===
*  H0 Linguistics
**    H01 [[Phonetics]]
***      H109 Method of Recitation, Oratory of Speech
**    H02 [[Grammatology]]
**    H03 [[Semantics]], [[Lexicology]] and Meaning of words
***      H033 [[Idiom]]
***      H034 [[Adage]]
**    H04 [[Syntax]]
**    H05 Study of writing, [[Rhetoric]]
***      H059 Study of translation
**    H06 [[Lexicography]]
***      H061 [[Dictionary]]
*  H1 [[Chinese language]]
**    H10<!--to be filled-->
***      H102 Regulation, Standardisation of Chinese language, Promotion of [[Putonghua]]
***      H109<!--to be filled-->
****        H109.2 Ancient Chinese language
****        H109.4 Modern Chinese language
**    H11 [[Phone (phonetics)|Phone]] ([[Historical Chinese phonology]])
**    H12 Grammatology
*  H2  [[Languages of China|Languages of China's ethnic minorities]]
*  H3 Commonly Used Foreign Languages
**    H31 [[English language]]
**    H32 [[French language]]
**    H33 [[German language]]
**    H34 [[Spanish language]]
**    H35 [[Russian language]]
**    H36 [[Japanese language]]
**    H37 [[Arabic language]]
*  H4 Family of [[Sino-Tibetan languages]] ([[China]], [[Tibet]] and [[Burma]])
*  H5 Family of [[Altaic languages]] ([[Turkic languages|Turkic]], [[Mongolian language|Mongolian]] and [[Tungusic languages|Tungusic]])
*  H6 [[Language family|Language families]] in other areas of the World
**    H61 [[Austroasiatic languages]] and [[Tai languages]] ([[Southeast Asia|Mainland Southeast Asia]]))
**    H62 [[Dravidian languages]] ([[South India]])
**    H63 [[Austronesian languages]] ([[Malayo-Polynesian]])
**    H64 [[Paleosiberian languages]] ([[Siberia]])
**    H65 [[Ibero-Caucasian languages]] ([[Caucasus Mountains]])
**    H66 [[Uralic languages]]
**    H67 [[Afroasiatic languages]] ([[Southwest Asia]], [[Arabian Peninsula]], [[North Africa]])
*  H7 [[Indo-European languages]]
*  H8 [[Language family|Language families]] on other Continents
**    H81 [[African languages]]
**    H83 [[Indigenous languages of the Americas|American languages]]
**    H84 [[Papuan languages]]
*  H9 International Auxiliary Languages ([[Interlingua]], [[Ido (language)|Ido]], [[Esperanto]], etc.)

=== I.  [[Literature]] ===
*  I0 [[Literary Theory]]
*  I1 Literature (worldwide)
*  I2 Literature in China
*  I3 Literature in Asia
*  I4 Literature in Africa
*  I5 Literature in Europe
*  I6 Literature in Australasia
*  I7 Literature in America

=== J.  Art ===
*  J0 Theory of [[Fine Art]]
*  J1 Fine Art of the World
*  J2 [[Painting]]
*  J3 [[Sculpture]]
*  J4 [[Photography]]
*  J5 [[Applied arts]]
*  J6 [[Music]]
*  J7 [[Dance]]
*  J8 [[Drama]]
*  J9 [[Cinematography]], [[Television]]

=== K.  [[History]] and [[Geography]] ===
* K0 Historical Theory
* K1 [[History of the World]]
* K2 [[History of China]]
* K3 [[History of Asia]]
* K4 [[History of Africa]]
* K5 [[History of Europe]]
* K6 [[History of Australasia]]
* K7 [[History of the Americas|History of America]]
* K8 Biography, [[Archaeology]]
* K9 Geography

=== N.  [[Natural Science]] ===
*  N0 Theory and Methodology
*  N1 Present state
*  N2 Organisations, Groups, Conferences
*  N3 Research Methodology
*  N4 Education and Popularization
*  N5 Serials, Anthologies, Periodicals
*  N6 Reference Materials
*  N8 Field Surveys
*  N9 Minor Sciences

=== O.  Mathematics, Physics and Chemistry ===
* O1 [[Mathematics]]
* O2 [[Applied Mathematics]]
* O3 [[Mechanics]]
* O4 [[Physics]]
* O6 [[Chemistry]]
* O7 [[Crystallography]]

=== P.  [[Astronomy]] and [[Geoscience]] ===
*  P1 [[Astronomy]]
*  P2 [[Geodesy]]
*  P3 [[Geophysics]]
*  P4 [[Meteorology]]
*  P5 [[Geology]]
*  P6 [[Mineralogy]]
*  P7 [[Oceanography]]
*  P9 [[Physiography]]

=== Q.  [[Life Sciences]] ===
*  Q1 [[Biology|General Biology]]
*  Q2 [[Cell biology|Cytology]]
*  Q3 [[Genetics]]
*  Q4 [[Physiology]]
*  Q5 [[Biochemistry]]
*  Q6 [[Biophysics]]
*  Q7 [[Molecular Biology]]
*  Q8 [[Bioengineering]]
*  Q9 [[Zoology]] and [[Botany]]

=== R.  [[Medicine]] and [[Health Sciences]] ===
*  R1 [[Preventive Medicine]], [[Public health]]
*  R2 [[Traditional Chinese Medicine]]
*  R3 [[Human anatomy]], [[Physiology]], [[Pathology]], [[Microbiology]], [[Parasitology]]
*  R4 [[Clinical Medicine]]
*  R5 [[Internal medicine]]
*  R6 [[Surgery]]
*  R7 [[:Category:Medical specialties|Medical Specialties]]
** R71 [[Obstetrics]], [[Gynecology]]
** R72 [[Pediatrics]]
** R73 [[Oncology]]
** R74 [[Neurology]], [[Psychiatry]]
** R75 [[Dermatology]], [[Venereology]]
** R76 [[Otolaryngology]]
** R77 [[Ophthalmology]]
** R78 [[Dentistry]]
** R79 Non-Chinese [[Traditional medicine|Traditional Medicine]]
*  R8 [[Radiology]], [[Sport medicine]], [[Diving medicine]], [[Aerospace medicine]]
*  R9 [[Pharmacology]], [[Pharmacy]]

=== S.  [[Agricultural Science]] ===
*  S1 Fundamental Agricultural Science
*  S2 [[Agricultural Engineering]]
*  S3 [[Agronomy]]
*  S4 [[Phytopathology]]
*  S5 [[Crop|Individual Crops]]
*  S6 [[Horticulture]]
*  S7 [[Forestry]]
*  S8 [[Animal Husbandry]], [[Veterinary medicine]], [[Hunting]], [[Sericulture]], [[Apiculture]]
*  S9 [[Aquaculture]], [[Fishery]]

=== T.  Industrial Technology ===
*  TB General Industrial Technology
*  TD [[Mining Engineering]]
*  TE [[Petroleum]], [[Natural Gas]]
*  TF [[Extractive metallurgy]], [[Smelting]]
*  TG [[Metallurgy]], [[Metalworking]]
*  TH [[Machinery]], [[Instrumentation]]
*  TJ [[Military technology and equipment|Military Technology]]
*  TK [[Power Plant]]
*  TL [[Nuclear technology]]
*  TM [[Electrical Engineering]]
*  TN [[Electronic Engineering]], [[Telecommunication|Telecommunication Engineering]]
*  TP [[Automation]], [[Computer Engineering]]
*  TQ [[Chemical Engineering]]
*  TS [[Light industry|Light Industry]], [[Handicraft]]
*  TU [[Construction Engineering]]
*  TV [[Water Resources]], [[Hydraulics|Hydraulic Engineering]]

=== U.  [[Transportation]] ===
*  U1 General [[Transport]]
*  U2 [[Railway]] Transport
*  U4 [[Highway]] Transport
*  U6 [[Ship transport|Marine Transport]]

=== V.  [[Aviation]] and Aerospace ===
*  V1 Research and Exploration of Aviation and Aerospace Technology
*  V2 Aviation
*  V4 [[Aerospace]] ([[Spaceflight]])

=== X.  [[Environmental Science]] ===
*  X1 Fundamental Environmental Science
*  X2 Environmental Research
*  X3 [[Environmental Protection]] and Management
*  X4 Disaster Protection
*  X5 [[Pollution|Pollution Control]]
*  X7 [[Waste Management]] and [[Recycling]]
*  X8 Environmental Quality Monitoring
*  X9 [[Occupational safety and health]]

=== Z.  General Works ===
*  Z1 Collectanea/Generalia ([[Book series]])
** Z12 Collectanea of China
*** Z121 General Collectanea
**** Z121.2 Song Dynasty
**** Z121.3 Yuan Dynasty
**** Z121.4 Ming Dynasty
**** Z121.5 Qing Dynasty
**** Z121.6 Republic period
**** Z121.7 Modern
*** Z122 Collectanea of a particular locality
*** Z123 Collectanea by members of a particular family
*** Z124 Collectanea by individual writers
*** Z125 Collectanea of lost books
*** Z126 Collectanea of [[Chinese Classics]]
**** Z126.1 Collection of [[Confucian Classics]]
**** Z126.2 Collection of treatises
***** Z126.21 General Collection
***** Z126.22 Remake of lost books
***** Z126.23 Collection of a particular theme
***** Z126.24 [[Timeline of Chinese history|Chronological tables]], tablets, illustrated works
***** Z126.25 Works on [[phonetics]], [[semantics]] and [[Verisimilitude|authenticity]]
***** Z126.27 Research, critics and proves
** Z13 Collectanea and Book series of Asia
** Z14 Book series of Africa
** Z15 Book series of Europe
** Z16 Book series of Oceania
** Z17 Book series of America
*  Z2 [[Encyclopedia]] and Chinese Encyclopedia (Leishu)
** Z22 Chinese Encyclopedia
*** Z221 Tang Dynasty
*** Z222 Song Dynasty
*** Z223 Yuan Dynasty
*** Z224 Ming Dynasty
*** Z225 Qing Dynasty
*** Z226 Republic
*** Z227 Modern
*** Z228 General popular Literature 
**** Z228.1 Children's book
**** Z228.2 Popular Youth Book
**** Z228.3 Elders' book
**** Z228.4 Women's reader
**** Z228.5 Men's reader
** Z23 Encyclopedia of Asia
** Z24 Encyclopedia of Africa
** Z25 Encyclopedia of Europe
** Z26 Encyclopedia of Oceania
** Z27 Encyclopedia of America
** Z28 Encyclopedia of a particular field
*  Z3 [[Dictionary]]
*  Z4 [[Symposium]], [[Anthologies]], Selected Works, [[Essay]]
*  Z5 [[Almanac]]
*  Z6 [[Serial (literature)|Serial]], [[Periodicals]]
*  Z8 Catalogue, Abstract, Index

== Other classifications ==
The other library classifications in China are:

* [[Library Classification of the People’s University of China]] (LCPUC)
* [[Library Classification of the Chinese Academy of Sciences]] (LCCAS)
* [[Library Classification for Medium and Small Libraries]] (MSL)
* [[Library Classification of Wuhan University]] (LCWU)

The other library classifications for Chinese materials outside mainland China are:
* [http://www.lib.cam.ac.uk/mulu/class.html Cambridge University Library Chinese Classification System], Classification Scheme for Chinese Books devised by Profs. Haloun and P. van der Loon for Cambridge University, UK.
* ''University of Leeds Classification of Books in Chinese, UK'' ([http://library.leeds.ac.uk/downloads/file/126/chinese 36 pages of Catalog in pdf])
* [[Harvard-Yenching Classification]] System
* [[New Classification Scheme for Chinese Libraries]] (commonly used in [[Taiwan]], [[Hong Kong]] and [[Macau]].)

== See also ==
* [[Libraries in the People's Republic of China]]

==References==
{{reflist}}

== External links ==
* [http://clc.nlc.gov.cn/ Official website ]
* [http://www.ifla.org/IV/ifla62/62-qiyz.htm Contemporary Classification Systems and Thesauri in China, Zhang Qiyu, Liu Xiangsheng, Wang Dongbo, 62nd IFLA General Conference - Conference Proceedings - August 25-31, 1996]
* [http://www.nlc.gov.cn/old/old/newpages/english/org/clce.htm Chinese Library Classification Editorial Board]
* [http://www.zju.edu.cn/jzus/download/clc.pdf Abridged third (obsolete) edition of CLC ]{{Zh icon}}
* [http://www.33tt.com/tools/ztf/ CLC Online ]{{Zh icon}}
* [http://journals.sfu.ca/dcpapers/2004/Paper_12.pdf Research on Interoperability of Metadata in Classification Schemes-construction of automatic mapping system between CLC and DDC, Jianbo Dai, Hanqing Hou, Ling Cao, Dept. of Libr. & Inform. Sci., Nanjing Agri. Univ., Nanjing, China 210095]
* [ftp://ext-ftp.fao.org/GI/Agris/aims/publications/workshops/AOS_5/ppt/3-3.pdf Construction of Knowledge Base for Automatic Indexing and Classification based on CLC, Hanqing Hou, Chunxiang Xue, Nanjing Agri. Univ., Nanjing, China 210095]
* [http://www.fao.org/Agris/AOS/ConferencesW/FifthAOS_China04/AOS_Proceedings/docs/4-1.pdf#search='chinese%20library%20classification' An Intelligent Retrieval System for Chinese Agricultural Literature indexed by Chinese Classification System, Ping Qian, Xiaolu Su, Chinese Academy of Agricultural Sciences, China]
* [http://www.freewebs.com/yahnkim/East%20Asian%20Library%20Classification%20Systems%5B1%5D.doc East Asian Library Classification Systems], [http://archive.is/20121209181441/http://webcache.googleusercontent.com/search?q=cache:z3hxqowOTFoJ:www.freewebs.com/yahnkim/East%2520Asian%2520Library%2520Classification%2520Systems%255B1%255D.doc+chinese+library+classification&hl=en&gl=hk&ct=clnk&cd=410 archived]
* [http://www.nii.ac.jp/publications/CJK-WS3/cjk3-04a.pdf The Development of Authority Database in National Library of China (NLC), March 2002, Beixin Sun of NLC] NLC's classification subject thesaurus database based on CLC.
* [http://www.ifla.org/IV/ifla72/papers/109-Gu-en.pdf National Bibliographies: the Chinese Experience, 72nd IFLA Conference at Seoul in Korea, August 2006, Ben Gu of NLC] An overview of the current situation of the National Bibliography and classification systems in China.
* [http://pubs.nrc-cnrc.gc.ca/jchla/jchla26/c05-018.pdf A month at the Shanghai Library, November 2004, Helen Michael, University of Toronto] A librarian from Canada shared her experience of working in a library of China.

{{Library classification systems}}

[[Category:1975 introductions]]
[[Category:1975 establishments in China]]
[[Category:Library cataloging and classification]]
[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Chinese culture]]
<=====doc_Id=====>:790
<=====title=====>:
AGRIS
<=====text=====>:
'''AGRIS''' (International System for Agricultural Science and Technology) is a global public domain database with more than 8 million structured bibliographical records on agricultural science and technology. The database is maintained by [[CIARD]], and its content is provided by more than 150 participating institutions from 65 countries. The AGRIS Search system,<ref>{{cite web|url=http://agris.fao.org |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2016-03-14}}</ref> allows scientists, researchers and students to perform sophisticated searches using keywords from the [[AGROVOC]] thesaurus, specific journal titles or names of countries, institutions, and authors.

== Early AGRIS years ==
As [[information management]] flourished in the 1970s, the AGRIS [[metadata]] [[Text corpus|corpus]] was developed to allow its users to have free access to knowledge available in agricultural science and technology. AGRIS was developed to be an international cooperative system to serve both developed and [[developing countries]].

With the advent of the Internet, along with the promises offered by [[open access (publishing)|open access]] publishing, there was growing awareness that the management of agricultural science and technology information, would have various facets: [[Technical standard|standards]] and methodologies for [[interoperability]] and facilitation of knowledge exchange; tools to enable information management specialists to process data; information and knowledge exchange across countries. Common [[interoperability]] criteria were thus adopted in its implementation, and the AGRIS AP [[metadata]] was accordingly created in order to allow exchange and retrieval of Agricultural information Resources.<ref>{{cite web|url=http://www.fao.org/docrep/008/ae909e/ae909e00.htm |title=The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description |publisher=Fao.org |date= |accessdate=2013-07-09}}</ref>

== AGRIS 2.0 ==
AGRIS covers the wide range of subjects related to agriculture science and technology, including forestry, animal husbandry, aquatic sciences and fisheries, human nutrition, and extension. Its content includes unique grey literature such as unpublished scientific and technical reports, theses, conference papers, government publications, and more. A growing number (around 20%) of bibliographical records have a corresponding full text document on the web which can easily be retrieved by Google.

On 5th December 2013 AGRIS 2.0 was released. AGRIS 2.0 is at the same time:

# A collaborative network of more than 150 institutions from 65 countries, maintained by FAO of the UN, promoting free access to agricultural information.
# A multilingual bibliographic database for agricultural science, fuelled by the AGRIS network, containing more than 8 million records largely enhanced with AGROVOC, FAO’s multilingual thesaurus covering all areas of interest to FAO, including food, nutrition, agriculture, fisheries, forestry, environment etc.
# A mash-up web application that links the bibliographic AGRIS knowledge to related resources on the web using the [[Linked Open Data]] methodology. An AGRIS mashup page (e.g. http://agris.fao.org/agris-search/search.do?recordID=QM2008000025 ) is a web page where an AGRIS resource is displayed together with relevant knowledge extracted from external data sources (as the World Bank, DBPedia, and Nature). The availability of external data sources is not under AGRIS control. Thus, if an external data source is temporary unreachable, it won’t be displayed in AGRIS mashup pages.

Access to the AGRIS Repository is provided through the AGRIS Search Engine.<ref>{{cite web|url=http://agris.fao.org/ |title=agris.fao.org |publisher=agris.fao.org |date= |accessdate=2013-07-09}}</ref>  As such, it:
# enables retrieval of bibliographic records contained in the AGRIS Repository,
# allows users to perform either full-text or fielded, parametric and assisted queries.

AGRIS data was converted to RDF and the resulting linked dataset created some 200 million triples.
AGRIS is also registered in the Data Hub at http://thedatahub.org/dataset/agris

The AGRIS partners contributing to the AGRIS Database use several formats for exchanging data, including simple DC, from [[OAI-PMH]] systems.
The AGRIS AP format is anyway adopted directly by:
# Open Archive Initiative (OAI) partners: Scielo, Viikki Science Library
# BIBSYS, Norway, National Library of Portugal, Wageningen UR Library.
# ''National networks'': NARIMS<ref>[http://www.arc.sci.eg/ arc.sci.eg]</ref> in [[Egypt]], [[PhilAgriNet]] in [[Philippines]], [[KAINet]] in [[Kenya]], NAC in [[Thailand]], GAINS in [[Ghana]].
# ''National institutional repositories'': Russia, Belarus, Uruguay, Spain, Iran.
# ''Information service providers'': [[Wolters Kluwer]], [[NISC]], CGIR, [[CGIAR]], [[Agriculture Network Information Center|AgNIC]], GFIS.
# ''Database systems/tools'': AgriOceanDspace,<ref>{{cite web|url=http://aims.fao.org/agriocean-dspace |title=AgriOcean DSpace &#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}</ref> NewGenlib, WebAGRIS, NERAKIN, AgriDrupal.<ref>{{cite web|url=http://aims.fao.org/tools/agridrupal |title=AgriDrupal &#124; Agricultural Information Management Standards (AIMS) |publisher=Aims.fao.org |date= |accessdate=2013-07-09}}</ref>

=== AGRIS under the CIARD umbrella ===
Falling under the umbrella of CIARD,<ref>{{cite web|url=http://www.ciard.net |title=What is CIARD? &#124; Coherence in Information for Agricultural Research for Development |publisher=Ciard.net |date= |accessdate=2013-07-09}}</ref> a joint initiative co-led by the CGIAR,<ref>{{cite web|last=Rijsberman |first=Frank |url=http://www.cgiar.org |title=CGIAR Home |publisher=Cgiar.org |date=2013-07-04 |accessdate=2013-07-09}}</ref> GFAR<ref>{{cite web|url=http://www.egfar.org |title=EGFAR web Site |publisher=Egfar.org |date= |accessdate=2013-07-09}}</ref> and [[FAO]], the new AGRIS aims to promote the sharing and management of agricultural science and technology information through the use of common [[Technical standard|standards]] and methodologies. These will incorporate [[Web 2.0]] features, in order to make the search experience as comprehensive, intuitive and far-reaching as possible for users of the new AGRIS.

Furthermore, the new AGRIS will also leverage the data and infrastructure of one of CIARD's projects: the CIARD RING. An acronym standing for Routemap to Information Nodes and Gateways (RING), the CIARD RING project is led by [[Global Forum on Agricultural Research|GFAR]] and it aims to:

* give an overview of the current offer of information services in ARD; as well as
* support those who want to implement new services.

A directory of ARD (Agricultural Research for Development) information services will allow the monitoring, describing and classifying of existing services, whilst benchmarking them against [[interoperability]] criteria, to ensure for maximum outreach and global availability.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AgMES]]
* [[Agricultural Ontology Service]]
* [[AGROVOC]]
* [[Information management]]
* [[IMARK]]
* [[Disciplinary repository]]

== References ==
{{Reflist|2}}

== Other publications ==
* [http://f1000research.com/articles/4-432/v2 Discovering, Indexing and Interlinking Information Resources (F1000research 2015)]
* [http://f1000research.com/articles/4-110/v1 AGRIS: providing access to agricultural research data exploiting open data on the web (F1000research 2015)]
* [http://iospress.metapress.com/content/l15562xk70234n79/fulltext.pdf Migrating bibliographic datasets to the Semantic Web: The AGRIS case (Semantic Web journal 2013)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
* [https://web.archive.org/web/20140908155657/http://eprints.rclis.org/21112/1/2013_EFITA%20Pushing_Pulling.pdf Pushing, Pulling, Harvesting, Linking - Rethinking Bibliographic Workflows for the Semantic Web (EFITA-2013)] 
* [http://www.fao.org/documents/advanced_s_result.asp?FORM_C=AND&SERIES=339 Agricultural Information and Knowledge Management Papers]
* [http://www.fao.org/docrep/008/ae909e/ae909e00.htm The AGRIS Application Profile for the International Information System on Agricultural Sciences and Technology Guidelines on Best Practices for Information Object Description]
* [http://www.fao.org/kce/consultations/coaim/coaim-2000/en/ Consultations on Agricultural Management (COAIM)]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
* [http://www.fao.org/docrep/x7936e/x7936e00.htm First Consultation on Agricultural Information Management (2000 COAIM Report)]
* [http://www.fao.org/docrep/meeting/005/y7963e/Y7963e00.htm#Top Report on the Second Consultation on Agricultural Information Management (2002 COAIM Report)]
* [http://departments.agri.huji.ac.il/economics/gelb-agris-10.pdf#AGRIS 1968-1994: Insights and Lessons]

== External links ==
* [http://agris.fao.org '''AGRIS''']
* [http://agris.fao.org/agris-search/agrisMap.do AGRIS network map]
* [http://agrovoc.fao.org/axis/services/SKOSWS?wsdl AGROVOC Web services]
* [http://aims.fao.org/ Agricultural Information Management Standards (AIMS)]
* [http://www.fao.org Food and Agriculture Organization of the United Nations (FAO) Web site]
* [http://www.ciard.net Coherence in Information for Agricultural Research for Development (CIARD) Wed site]
* [http://www.fao.org/nems/rss/rss_nems_results.asp?owner=615&status=10&dateto=31/12/2006&lang=en&sites=1 RSS feed of news and events]

{{DEFAULTSORT:Agris}}
[[Category:Agricultural organizations]]
[[Category:Interoperability]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Food and Agriculture Organization]]
[[Category:Public domain databases]]
[[Category:Agricultural databases]]
<=====doc_Id=====>:793
<=====title=====>:
Ontology (information science)
<=====text=====>:
{{redirect|Knowledge graph|the Google knowledge base|Knowledge Graph||Knowledge engine (disambiguation)}}
{{About|ontology in information science|the study of the nature of being|Ontology}}
{{Information science}}

In [[computer science]] and [[information science]], an '''ontology''' is a formal naming and definition of the types, properties, and interrelationships of the [[entities]] that really or fundamentally exist for a particular [[domain of discourse]]. It is thus a practical application of philosophical [[ontology]], with a [[taxonomy (general)|taxonomy]].

An ontology compartmentalizes the variables needed for some set of computations and establishes the relationships between them.<ref name="TRG93">{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199–220 |doi=10.1006/knac.1993.1008}}</ref><ref>{{cite web |first1=F. |last1=Arvidsson |first2=A. |last2=Flycht-Eriksson |url=http://www.ida.liu.se/~janma/SemWeb/Slides/ontologies1.pdf |title=Ontologies I |format=PDF |accessdate=26 November 2008}}</ref>

The fields of [[artificial intelligence]], the [[Semantic Web]], [[systems engineering]], [[software engineering]], [[biomedical informatics]], [[library science]], [[enterprise bookmarking]], and [[information architecture]] all create ontologies to limit complexity and to organize information. The ontology can then be applied to [[problem solving]].

==Etymology and definition==
The term ''[[ontology]]'' has its origin in [[philosophy]] and has been applied in many different ways. The word element ''[[wiktionary:onto-|onto-]]'' comes from the [[Greek language|Greek]] ''[[wiktionary:ὤν|ὤν]], ὄντος'', ("being", "that which is"), present participle of the verb ''[[wiktionary:εἰμί|εἰμί]]'' ("be"). The core meaning within [[computer science]] is a model for describing the world that consists of a set of types, properties, and relationship types. There is also generally an expectation that the features of the model in an ontology should closely resemble the real world (related to the object).<ref>{{cite web |first=L. M. |last=Garshol |year=2004 |url=http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html#N773 |title=Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all'' |accessdate=13 October 2008 }}</ref>

==Overview==
What many ontologies have in common in both computer science and in philosophy is the representation of entities, ideas, and events, along with their properties and relations, according to a system of categories. In both fields, there is considerable work on problems of [[Confirmation holism|ontological relativity]] (e.g., [[Willard Van Orman Quine|Quine]] and [[Saul Kripke|Kripke]] in philosophy, [[John F. Sowa|Sowa]] and [[Nicola Guarino|Guarino]] in computer science),<ref>{{cite journal |first=J. F. |last=Sowa |title=Top-level ontological categories
|journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 (November/December)
|year=1995 |pages=669–85 |doi=10.1006/ijhc.1995.1068 }}</ref> and debates concerning whether a [[normative]] ontology is viable (e.g., debates over [[foundationalism]] in philosophy, and over the [[Cyc]] project in AI). Differences between the two are largely matters of focus. Computer scientists are more concerned with establishing fixed, controlled vocabularies, while philosophers are more concerned with first principles, such as whether there are such things as [[Essence|fixed essences]] or whether enduring objects must be ontologically more primary than processes.

Other fields make ontological assumptions that are sometimes explicitly elaborated and explored.  For instance, the [[philosophy and economics#Definition and ontology of economics|definition and ontology of economics]] (also sometimes called the [[political economy]]) is hotly debated especially in [[Marxist economics]]<ref>{{cite journal|first=Giulio |last=Palermo|url=http://cje.oxfordjournals.org/content/31/4/539.short |title=The ontology of economic power in capitalism: mainstream economics and Marx |journal=Cambridge Journal of Economics |volume=31 |issue=4 |pages=539–561 |date=10 January 2007 |doi=10.1093/cje/bel036 |accessdate=16 June 2013 |via=Oxford Journals }}</ref> where it is a primary concern, but also in other subfields.<ref>{{cite web|author=Zuniga, Gloria L. |url=https://ideas.repec.org/p/pra/mprapa/5566.html |title=An Ontology Of Economic Objects |website=Ideas |publisher=Research Division of the Federal Reserve Bank of St. Louis |date=1999-02-02 |accessdate=2013-06-16}}</ref>  Such concerns intersect with those of information science when a simulation or model is intended to enable decisions in the economic realm; for example, to determine what [[capital asset]]s are at risk and if so by how much (see [[risk management]]).  Some claim all social sciences have explicit ontology issues because they do not have hard [[falsifiability]] criteria like most models in physical sciences and that indeed the lack of such widely accepted hard falsification criteria is what defines a social or soft science.{{Citation needed|date=March 2012}}

==History==
Historically, ontologies arise out of the branch of [[philosophy]] known as [[metaphysics]], which deals with the nature of reality&nbsp;– of what exists. This fundamental branch is concerned with analyzing various types or modes of existence, often with special attention to the relations between [[particular]]s and [[Universal (metaphysics)|universals]], between [[Intrinsic and extrinsic properties (philosophy)|intrinsic and extrinsic properties]], and between [[essence]] and [[existence]]. The traditional goal of ontological inquiry in particular is to divide the world "at its joints" to discover those fundamental categories or kinds into which the world’s objects naturally fall.<ref name="PCB94">{{cite web |first1=Perakath C. |last1=Benjamin |first2=Christopher P. |last2=Menzel |first3=Richard J. |last3=Mayer |first4=Florence |last4=Fillion |first5=Michael T. |last5=Futrell |first6=Paula S. |last6=deWitte |first7=Madhavi |last7=Lingineni |date=September 21, 1994 |url=http://www.idef.com/pdf/Idef5.pdf |format=PDF |title=IDEF5 Method Report |publisher=Knowledge Based Systems, Inc.}}</ref>

During the second half of the 20th century, philosophers extensively debated the possible methods or approaches to building ontologies without actually ''building'' any very elaborate ontologies themselves. By contrast, computer scientists were building some large and robust ontologies, such as [[WordNet]] and [[Cyc]], with comparatively little debate over ''how'' they were built.

Since the mid-1970s, researchers in the field of [[artificial intelligence]] (AI) have recognized that capturing knowledge is the key to building large and powerful AI systems.  AI researchers argued that they could create new ontologies as [[computational model]]s that enable certain kinds of [[automated reasoning]]. In the 1980s, the AI community began to use the term ''ontology'' to refer to both a theory of a modeled world and a component of knowledge systems. Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.<ref name="TG08">{{cite book |first=T. |last=Gruber |authorlink=Tom Gruber |year=2008 |url=http://tomgruber.org/writing/ontology-definition-2007.htm  |title=Ontology |work=Encyclopedia of Database Systems |editor1-first=Ling |editor1-last=Liu |editor2-first=M. Tamer |editor2-last=Özsu |publisher=Springer-Verlag|isbn=978-0-387-49616-0}}</ref>

In the early 1990s, the widely cited Web page and paper "Toward Principles for the Design of Ontologies Used for Knowledge Sharing" by [[Tom Gruber]]<ref name="TRG95">{{cite journal |first=T. |last=Gruber |authorlink=Tom Gruber |title=Toward Principles for the Design of Ontologies Used for Knowledge Sharing |journal=[[International Journal of Human-Computer Studies]] |volume=43 |issue=5-6 |pages=907–928 |year=1995 |doi=10.1006/ijhc.1995.1081}}</ref> is credited with a deliberate definition of ''ontology'' as a technical term in [[computer science]]. Gruber introduced the term to mean a specification of a conceptualization: <blockquote>An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.<ref name="TRG01">{{cite web |first=T. |last=Gruber |authorlink=Tom Gruber |year=2001 |url=http://www-ksl.stanford.edu/kst/what-is-an-ontology.html |title=What is an Ontology? |publisher=[[Stanford University]] |accessdate=2009-11-09}}</ref></blockquote>

According to Gruber (1993): <blockquote>Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to [[Conservative extension|conservative definitions]]&nbsp;&mdash; that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.<ref>{{cite book
|first=H. B. |last=Enderton |authorlink=Herbert Enderton |date=1972-05-12 |title=A Mathematical Introduction to Logic |location=San Diego, CA |publisher=Academic Press |edition=1 |page=295 |isbn=978-0-12-238450-9 |postscript=&nbsp;2nd edition; January 5, 2001, ISBN 978-0-12-238452-3}}</ref> To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.<ref name="TRG93"/></blockquote>

== Components ==
{{Main article|Ontology components}}
Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed.  As mentioned above, most ontologies describe individuals (instances), classes (concepts), attributes, and relations.  In this section each of these components is discussed in turn.

Common components of ontologies include:
; Individuals
: Instances or objects (the basic or "ground level" objects)
; [[Class (set theory)|Class]]es<!-- This deliberately links to the disambiguation page -->
: Sets, collections, concepts, [[Class (computer science)|classes in programming]], [[Class (philosophy)|types of objects]], or kinds of things
; [[Attribute (computing)|Attribute]]s
: Aspects, properties, features, characteristics, or parameters that objects (and classes) can have
; [[Relation (mathematics)|Relations]]
: Ways in which classes and individuals can be related to one another
; Function terms
: Complex structures formed from certain relations that can be used in place of an individual term in a statement
; Restrictions
: Formally stated descriptions of what must be true in order for some assertion to be accepted as input
; Rules
: Statements in the form of an if-then (antecedent-consequent) sentence that describe the logical inferences that can be drawn from an assertion in a particular form
; Axioms
: Assertions (including rules) in a [[logical form]] that together comprise the overall theory that the ontology describes in its domain of application.  This definition differs from that of "axioms" in [[generative grammar]] and [[formal logic]].  In those disciplines, axioms include only statements asserted as ''a priori'' knowledge.  As used here, "axioms" also include the theory derived from axiomatic statements
; [[Event (philosophy)|Events]]<!-- this links  to the philosophy sense of 'Events' as that is currently the only article describing the issues around defining events in the ontology community-->
: The changing of attributes or relations

Ontologies are commonly encoded using [[ontology language]]s.

== Types ==

=== Domain ontology<!--linked from 'Domain ontology'--> ===
A domain ontology (or domain-specific ontology) represents concepts which belong to part of the world. Particular meanings of terms applied to that domain are provided by domain ontology. For example, the word ''[[:wikt:card|card]]'' has many different meanings. An ontology about the domain of [[poker]] would model the "[[playing card]]" meaning of the word, while an ontology about the domain of [[computer hardware]] would model the "[[punched card]]" and "[[video card]]" meanings.

Since domain ontologies represent concepts in very specific and often eclectic ways, they are often incompatible. As systems that rely on domain ontologies expand, they often need to merge domain ontologies into a more general representation.  This presents a challenge to the ontology designer. Different ontologies in the same domain arise due to different languages, different intended usage of the ontologies, and different perceptions of the domain (based on cultural background, education, ideology, etc.).

At present, merging ontologies that are not developed from a common foundation ontology is a largely manual process and therefore time-consuming and expensive. Domain ontologies that use the same foundation ontology to provide a set of basic elements with which to specify the meanings of the domain ontology elements can be merged automatically. There are studies on generalized techniques for merging ontologies,<ref name="Dynamic Ontology Repair">{{cite web |url=http://dream.inf.ed.ac.uk/projects/dor/ |title=Project: Dynamic Ontology Repair |publisher= University of Edinburgh Department of Informatics|accessdate=2 January 2012}}</ref> but this area of research is still largely theoretical.

=== Upper ontology ===
{{Main article|Upper ontology}}

An [[Upper ontology (computer science)|upper ontology]] (or foundation ontology) is a model of the common objects that are generally applicable across a wide range of domain ontologies. It usually employs a [[core glossary]] that contains the terms and associated object descriptions as they are used in various relevant domain sets.

There are several standardized upper ontologies available for use, including [[Basic Formal Ontology|BFO]], [[BORO method]], [[Dublin Core]], [[General Formal Ontology|GFO]], [[OpenCyc]]/[[ResearchCyc]], [[Suggested Upper Merged Ontology|SUMO]], the Unified Foundational Ontology (UFO),<ref>{{cite web|last=Giancarlo Guizzardi & Gerd Wagner|url=http://ceur-ws.org/Vol-125/paper2.pdf|accessdate=31 March 2014|title=A Unified Foundational Ontology and some Applications of it in Business Modeling}}</ref> and [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]].<ref name="DOLCE">{{cite web |url=http://www.loa-cnr.it/DOLCE.html |title=Laboratory for Applied Ontology - DOLCE |publisher=Laboratory for Applied Ontology (LOA)|accessdate=10 February 2011}}</ref><ref name="DOLCE-OWL">{{cite web |url=http://www.ontologydesignpatterns.org/ont/dul/DUL.owl |title=OWL version of DOLCE+DnS  |publisher=Semantic Technology Lab|accessdate=21 February 2013}}</ref> [[WordNet]], while considered an upper ontology by some, is not strictly an ontology. However, it has been employed as a linguistic tool for learning domain ontologies.<ref>{{cite journal |first1=Roberto |last1=Navigli |authorlink1=Roberto Navigli |first2=Paola |last2=Velardi |authorlink2=Paola Velardi |year=2004 |url=http://www.mitpressjournals.org/doi/pdf/10.1162/089120104323093276 |format=PDF |title=Learning Domain Ontologies from Document Warehouses and Dedicated Web Sites |journal=[[Computational Linguistics (journal)|Computational Linguistics]] |volume=30 |issue=2 |publisher=MIT Press |pages=151–179 |doi=10.1162/089120104323093276}}</ref>

=== Hybrid ontology ===

The [[Gellish]] ontology is an example of a combination of an upper and a domain ontology.

== Visualization ==
A survey of ontology visualization techniques is presented by Katifori et al.<ref>{{cite journal |last1=Katifori |first1=A. |last2=Halatsis |first2=C. |last3=Lepouras |first3=G. |last4=Vassilakis |first4=C. |last5=Giannopoulou |first5=E. |title=Ontology Visualization Methods - A Survey |journal=ACM Computing Surveys |volume=39 |issue=4 |page=10 |date=2007 |url=http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-url=http://web.archive.org/web/20160304203317/http://entrezneuron.googlecode.com/svn-history/r2/trunk/references/12-onto-vis-survey-final.pdf |archive-date=4 March 2016 |format=PDF }}</ref> An evaluation of two most established ontology visualization techniques: indented tree and graph is discussed in.<ref>{{cite conference |first1=Bo |last1=Fu |first2=Natalya F. |last2=Noy |first3=Margaret-Anne |last3=Storey |title=Indented Tree or Graph? A Usability Study of Ontology Visualization Techniques in the Context of Class Mapping Evaluation |book-title=The Semantic Web – ISWC 2013: 12th International Semantic Web Conference, Sydney, NSW, Australia, October 21–25, 2013, Proceedings, Part I |series=Lecture Notes in Computer Science |volume=8218 |pages=117–134 |url=http://link.springer.com/chapter/10.1007/978-3-642-41335-3_8 |doi=10.1007/978-3-642-41335-3_8 |isbn=978-3-642-41335-3 |publisher=Springer |location=Berlin |date=2013 |via=SpringerLink }}</ref> A visual language for ontologies represented in [[Web Ontology Language|OWL]] is specified by the ''Visual Notation for OWL Ontologies (VOWL)''.<ref>{{cite web |last1=Negru |first1=Stefan |last2=Lohmann |first2=Steffen |last3=Haag |first3=Florian |date=7 April 2014 |title=VOWL: Visual Notation for OWL Ontologies: Specification of Version 2.0 |website=Visual Data Web |url=http://vowl.visualdataweb.org/v2/ }}</ref>

== Engineering ==
{{Main article|Ontology engineering}}
[[Ontology engineering]] (or ontology building) is a subfield of [[knowledge engineering]]. It studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tool suites and languages that support them.<ref name="PFC04">{{cite book |first1=Ascunion |last1=Gómez-Pérez |authorlink1=Ascunion Gómez-Pérez |first2=Mariano |last2=Fernández-López |authorlink2=Mariano Fernández-López |first3=Oscar |last3=Corcho |authorlink3=Oscar Corcho |year=2004 |title=Ontological Engineering: With Examples from the Areas of Knowledge Management, E-commerce and the Semantic Web |publisher=Springer |isbn=978-1-85233-551-9 |page=403 |edition=1 }}</ref><ref name="DMN">{{cite journal |first1=Antonio |last1=De Nicola |authorlink1=Antonio De Nicola |first2=Michele |last2=Missikoff |authorlink2=Michele Missikoff |first3=Roberto |last3=Navigli |authorlink3=Roberto Navigli |year=2009 |url=http://www.dsi.uniroma1.it/~navigli/pubs/De_Nicola_Missikoff_Navigli_2009.pdf |format=PDF |title=A Software Engineering Approach to Ontology Building |journal=[[Information Systems (journal)|Information Systems]] |volume=34 |issue=2 |publisher=Elsevier |pages=258–275 | doi = 10.1016/j.is.2008.07.002 }}</ref>

Ontology engineering aims to make explicit the knowledge contained within software applications, and within enterprises and business procedures for a particular domain. Ontology engineering offers a direction towards solving the interoperability problems brought about by semantic obstacles, such as the obstacles related to the definitions of business terms and software classes. Ontology engineering is a set of tasks related to the development of ontologies for a particular domain.<ref name="PIS00">{{cite journal |first1=Line  |last1=Pouchard |authorlink1=Line Pouchard |first2=Nenad |last2=Ivezic |authorlink2=Nenad Ivezic |first3=Craig |last3=Schlenoff |authorlink3=Craig Schlenoff |date=March 2000 |url=http://www.mel.nist.gov/msidlibrary/doc/AISfinal2.pdf |format=PDF |title=Ontology Engineering for Distributed Collaboration in Manufacturing |work=Proceedings of the AIS2000 conference }}</ref>

Known challenges with ontology engineering include:
# Ensuring the ontology is ''current'' with domain knowledge and term use
# Providing ''sufficient specificity and concept coverage'' for the domain of interest, thus minimizing the [[content completeness problem]]
# Ensuring the ontology can support its use cases

=== Editor ===
'''Ontology editors''' are applications designed to assist in the creation or  manipulation of [[ontology (computer science)|ontologies]]. They often express ontologies in one of many [[ontology language (computer science)|ontology languages]]. Some provide export to other ontology languages however.

Among the most relevant criteria for choosing an ontology editor are the degree to which the editor abstracts from the actual [[Ontology language (computer science)|ontology representation language]] used for [[persistence (computer science)|persistence]] and the visual navigation possibilities within the [[knowledge model]]. Next come built-in [[inference engine]]s and [[information extraction]] facilities, and the support of meta-ontologies such as [[OWL-S]], [[Dublin Core]], etc. Another important feature is the ability to import & export foreign [[knowledge representation]] languages for [[ontology matching]]. Ontologies are developed for a specific purpose and application.

*a.k.a. software (Ontology, taxonomy and thesaurus management software available from The Synercon Group)
*Anzo for Excel (Includes an RDFS and OWL ontology editor within Excel; generates ontologies from Excel spreadsheets)
*Chimaera (Other web service by Stanford)
*CmapTools Ontology Editor (COE) (Java based ontology editor from the Florida Institute for Human and Machine Cognition. Supports numerous formats)
*[[dot15926]] Editor (Open source ontology editor for data compliant to engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] scripting and pattern-based data analysis. Supports extensions.)
*EMFText OWL2 Manchester Editor, Eclipse-based, open-source, Pellet integration
* Enterprise Architect, along with [[Unified Modeling Language|UML]] modeling, supports [[Object Management Group|OMG's]] [[Ontology Definition MetaModel]] which includes [[Web Ontology Language|OWL]] and [[Resource Description Framework|RDF]].
*[[Fluent Editor]], a comprehensive ontology editor for OWL and SWRL with Controlled Natural Language (Controlled English). Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[Description Logic|DL]] and Functional rendering, unlimited imports and built-in reasoning services.
*[[HOZO]] (Java-based graphical editor especially created to produce heavy-weight and well thought out ontologies, from [[Osaka University]] and Enegate Co, ltd.)
*Java Ontology Editor (JOE)  (1998)
*[[KAON]] (single user and server based solutions possible, open source, from FZI/AIFB Karlsruhe)
*KMgen (Ontology editor for the KM language.&nbsp;km: The Knowledge Machine)
*Knoodl (Free web application/service that is an ontology editor, [[wiki]], and [[Digital repository|ontology registry]].  Supports creation of communities where members can collaboratively import, create, discuss, document and publish ontologies.  Supports [[Web Ontology Language|OWL]], [[Resource Description Framework|RDF]], [[RDF Schema|RDFS]], and [[SPARQL]] queries.  Available since early Nov 2006 from Revelytix, Inc..)
*Model Futures IDEAS AddIn (free) A plug-in for Sparx Systems [[Enterprise Architect (software)|Enterprise Architect]] that allows [[IDEAS Group]] [[4D ontology|4D ontologies]] to be developed using a [[Unified Modeling Language|UML]] profile
*Model Futures OWL Editor (Free) Able to work with very large OWL files (e.g. [[Cyc]]) and has extensive import and export capabilities (inc. [[Unified Modeling Language|UML]], Thesaurus Descriptor, [[MS Word]], CA ERwin Data Modeler, CSV, etc.)
*myWeb (Java-based, mySQL connection, bundled with applet that allows online browsing of ontologies (including OBO))
*Neologism (Web-based, open source, supports RDFS and a subset of OWL, built on [[Drupal]])
*[[NeOn Toolkit (software)|NeOn Toolkit]]  (Eclipse-based, open source, OWL support, several import mechanisms, support for reuse and management of networked ontologies, visualization, etc.…from NeOn Project)
*OBO-Edit (Java-based, downloadable, open source, developed by the [[Gene Ontology Consortium]] for editing biological ontologies)
*OntoStudio (Eclipse-based, downloadable, support for RDF(S), OWL and F-Logic, graphical rule editor, visualizations, from ontoprise)
*Ontolingua (Web service offered by Stanford University)
* [[Open Semantic Framework]] (OSF), an integrated software stack using semantic technologies for knowledge management, which includes an ontology editor
*OWLGrEd (A graphical ontology editor, easy-to-use)
*PoolParty Thesaurus Server (Commercial ontology, taxonomy and thesaurus management software available from Semantic Web Company, fully based on standards like RDFS, SKOS and SPARQL, integrated with [[Virtuoso Universal Server]])
*[[Protégé (software)|Protégé]] (Java-based, downloadable, Supports OWL, open source, many sample ontologies, from Stanford University)
*ScholOnto (net-centric representations of research)
*Semantic Turkey (Firefox extension - also based on Java - for managing ontologies and acquiring new knowledge from the Web; developed at University of Rome, Tor Vergata )
*[[Sigma knowledge engineering environment]] is a system primarily for development of the [[Suggested Upper Merged Ontology]]
*Swoop (Java-based, downloadable, open source, OWL Ontology browser and editor from the University of Maryland)
*Semaphore Ontology Manager  (Commercial ontology, taxonomy and thesaurus management software available from [[Smartlogic Semaphore Limited]]. Intuitive tool to manage the entire "build - enhance - review - maintain" ontology lifecycle.)
*Synaptica  (Ontology, taxonomy and thesaurus management software available from Synaptica, LLC. Web based, supports [[Web Ontology Language|OWL]] and [[SKOS]].)
*TopBraid Composer  (Eclipse-based, downloadable, full support for RDFS and OWL, built-in inference engine, SWRL editor and SPARQL queries, visualization, import of XML and UML, from TopQuadrant)
*Transinsight (The editor is especially designed for creating text mining ontologies and part of GoPubMed.org)
*WebODE (Web service offered by the Technical University of Madrid)
*TwoUse Toolkit (Eclipse-based, open source, model-driven ontology editing environment especially designed for software engineers)
*Be Informed Suite (Commercial tool for building large ontology based applications. Includes visual editors, inference engines, export to standard formats)
*Thesaurus Master (Manages creation and use of ontologies for use in data management and semantic enrichment by enterprise, government, and scholarly publishers.)
*[[Tool for Ontology Development and Editing (TODE)|TODE]] (A Dot Net-based Tool for Ontology Development and Editing)
*VocBench (Collaborative Web Application for SKOS/SKOS-XL Thesauri Management - developed on a joint effort between University of Rome, Tor Vergata and the Food and Agriculture Organization of the United Nations: FAO )
*OBIS (Web based user interface that allows to input ontology instances in a user friendly way that can be accessed via SPARQL endpoint)
*[[Menthor Editor]] (An ontology engineering tool for dealing with [[OntoUML]]. It also includes OntoUML syntax validation, [[Alloy Analyzer|Alloy]] simulation, [[Anti-pattern|Anti-Pattern]] verification, and transformations from [[OntoUML]] to [[Web Ontology Language|OWL]], [[Semantics of Business Vocabulary and Business Rules|SBVR]] and [[Brazilian Portuguese|Natural Language (Brazilian Portuguese)]])

=== Learning ===
{{Main article|Ontology learning}}

Ontology learning is the automatic or semi-automatic creation of ontologies, including extracting a domain's terms from natural language text. As building ontologies manually is extremely labor-intensive and time consuming, there is great motivation to automate the process. 
Information extraction and text mining methods have been explored to automatically link ontologies to documents, e.g. in the context of the BioCreative challenges.<ref>{{Cite journal
 | pmid = 22438567
| year = 2012
| author1 = Krallinger
| first1 = M
| title = How to link ontologies and protein-protein interactions to literature: Text-mining approaches and the Bio ''Creative'' experience
| journal = Database
| volume = 2012
| pages = bas017
| last2 = Leitner
| first2 = F
| last3 = Vazquez
| first3 = M
| last4 = Salgado
| first4 = D
| last5 = Marcelle
| first5 = C
| last6 = Tyers
| first6 = M
| last7 = Valencia
| first7 = A
| last8 = Chatr-Aryamontri
| first8 = A
| doi = 10.1093/database/bas017
| pmc = 3309177
}}</ref>

== Languages ==
{{Main article|Ontology language}}
An [[ontology language]] is a [[formal language]] used to encode the ontology. There are a number of such languages for ontologies, both proprietary and standards-based:
* [[Common Algebraic Specification Language]] is a general logic-based specification language developed within the IFIP working group 1.3 "Foundations of System Specifications" and functions as a de facto standard in the area of software specifications. It is now being applied to ontology specifications in order to provide modularity and structuring mechanisms.
* [[Common logic]] is ISO standard 24707, a specification for a family of ontology languages that can be accurately translated into each other.
* The [[Cyc]] project has its own ontology language called [[CycL]], based on [[first-order predicate calculus]] with some higher-order extensions.
* [[DOGMA]] (Developing Ontology-Grounded Methods and Applications) adopts the fact-oriented modeling approach to provide a higher level of semantic stability.
* The [[Gellish]] language includes rules for its own extension and thus integrates an ontology with an ontology language.
* [[IDEF5]] is a [[software engineering]] method to develop and maintain usable, accurate, domain ontologies.
* [[Knowledge Interchange Format|KIF]] is a syntax for [[first-order logic]] that is based on [[S-expression]]s.  SUO-KIF is a derivative version supporting the [[Suggested Upper Merged Ontology]].
* [[Meta-Object Facility|MOF]] and [[Unified Modeling Language|UML]] are standards of the [[Object Management Group|OMG]]
* [[Olog]] is a [[Category theory|category theoretic]] approach to ontologies, emphasizing translations between ontologies using [[functor]]s. 
* [[Open Biomedical Ontologies|OBO]], a language used for biological and biomedical ontologies.
* [[OntoUML]] is an ontologically well-founded profile of UML for conceptual modeling of domain ontologies.
* [[Web Ontology Language|OWL]] is a language for making ontological statements, developed as a follow-on from [[Resource Description Framework|RDF]] and [[RDFS]], as well as earlier ontology language projects including [[Ontology Inference Layer|OIL]], [[DARPA Agent Markup Language|DAML]], and [[DAMLplusOIL|DAML+OIL]]. OWL is intended to be used over the [[World Wide Web]], and all its elements (classes, properties and individuals) are defined as RDF [[web resource|resource]]s, and identified by [[Uniform Resource Identifier|URI]]s.
* [[Rule Interchange Format]] (RIF) and [[F-Logic]] combine ontologies and rules.
* [[Semantic Application Design Language]] (SADL)<ref>{{cite web |url=http://sadl.sourceforge.net/sadl.html |title=SADL |work=[[Sourceforge]] |accessdate=10 February 2011}}</ref> captures a subset of the expressiveness of [[Web Ontology Language|OWL]], using an English-like language entered via an [[Eclipse (software)|Eclipse]] Plug-in.
* [[SBVR]] (Semantics of Business Vocabularies and Rules) is an OMG standard adopted in industry to build ontologies.
* [[TOVE Project]], TOronto Virtual Enterprise project

== Published examples ==
* AURUM - Information Security Ontology,<ref>{{cite web |url=http://www.securityontology.com |title=AURUM - Information Security Ontology |accessdate=29 January 2016}}</ref> An ontology for information security knowledge sharing, enabling users to collaboratively understand and extend the domain knowledge body. It may serve as a basis for automated information security risk and compliance management.
* [[BabelNet]], a very large multilingual semantic network and ontology, lexicalized in many languages
* Basic Formal Ontology,<ref>{{cite web |url=http://www.ifomis.org/bfo/ |title=Basic Formal Ontology (BFO)
|publisher=[[Institute for Formal Ontology and Medical Information Science]] (IFOMIS) |accessdate=}}</ref> a formal upper ontology designed to support scientific research
* BioPAX,<ref>{{cite web |url=http://biopax.org |title=BioPAX |accessdate=10 February 2011}}</ref> an ontology for the exchange and interoperability of biological pathway (cellular processes) data
* BMO,<ref>{{cite journal |first1=Alexander |last1=Osterwalder |first2=Yves |last2=Pigneur | author-link2= Yves Pigneur | url=http://129.3.20.41/eps/io/papers/0202/0202004.pdf |title=An e-Business Model Ontology for Modeling e-Business |location=[[Bled eConference|15th Bled eConference]], [[Slovenia]] |date=June 17–19, 2002}}</ref> an e-Business Model Ontology based on a review of enterprise ontologies and business model literature
* SSBMO,<ref>{{cite journal |first1=Antony |last1=Upward |first2=Peter |last2=Jones |url=https://www.academia.edu/14461116 |title=An Ontology for Strongly Sustainable Business Models: Defining an Enterprise Framework Compatible with Natural and Social Science |journal=Organization & Environment |volume=29 |issue=1 |pages=97-123 |doi=10.1177/1086026615592933}}</ref> a Strongly Sustainable Business Model Ontology based on a review of the systems based natural and social science literature (including business).  Includes critique of and significant extensions to the Business Model Ontology (BMO).
* CCO and GexKB,<ref>{{cite web|title=About CCO and GexKB|url=http://www.semantic-systems-biology.org/apo/|publisher=Semantic Systems Biology}}</ref> Application Ontologies (APO) that integrate diverse types of knowledge with the Cell Cycle Ontology (CCO) and the Gene Expression Knowledge Base (GexKB)
* CContology (Customer Complaint Ontology),<ref>{{cite web |url=http://www.jarrar.info/CContology/ |title=CContology |accessdate=10 February 2011}}</ref> an e-business ontology to support online customer complaint management
* [[CIDOC Conceptual Reference Model]], an ontology for [[cultural heritage]]<ref>{{cite web |url=http://www.cidoc-crm.org/ |title=The CIDOC Conceptual Reference Model (CRM) |accessdate=10 February 2011}}</ref>
* COSMO,<ref>{{cite web |url=http://micra.com/COSMO/ |title=COSMO |publisher=MICRA Inc.|accessdate=10 February 2011}}</ref> a Foundation Ontology (current version in OWL) that is designed to contain representations of all of the primitive concepts needed to logically specify the meanings of any domain entity.  It is intended to serve as a basic ontology that can be used to translate among the representations in other ontologies or databases.  It started as a merger of the basic elements of the OpenCyc and SUMO ontologies, and has been supplemented with other ontology elements (types, relations) so as to include representations of all of the words in the [[Longman Dictionary of Contemporary English|Longman dictionary]] [[defining vocabulary]].
* [[Cyc]], a large Foundation Ontology for formal representation of the universe of discourse
* [[Disease Ontology]],<ref>{{cite journal |pmid=19594883}}</ref> designed to facilitate the mapping of diseases and associated conditions to particular medical codes
* [[Upper ontology (computer science)#DOLCE and DnS|DOLCE]], a Descriptive Ontology for Linguistic and Cognitive Engineering<ref name="DOLCE"/><ref name="DOLCE-OWL"/>
* [[Drammar]], ontology of drama
* [[Dublin Core]], a simple ontology for documents and publishing
* Foundational, Core and Linguistic Ontologies<ref>{{cite web |url=http://www.loa-cnr.it/Ontologies.html
|title=Foundational, Core and Linguistic Ontologies |accessdate=10 February 2011}}</ref>
* [[Foundational Model of Anatomy]],<ref>{{cite web |url=http://sig.biostr.washington.edu/projects/fm/AboutFM.html |title=Foundational Model of Anatomy |accessdate=10 February 2011}}</ref> an ontology for human anatomy
* [[FOAF (software)|Friend of a Friend]], an ontology for describing persons, their activities and their relations to other people and objects
* [[Gene Ontology]] for [[genomics]]
* [[Gellish English dictionary]], an ontology that includes a dictionary and taxonomy that includes an upper ontology and a lower ontology that focusses on industrial and business applications in engineering, technology and procurement.
* [[Geopolitical ontology]], an ontology describing geopolitical information created by [[Food and Agriculture Organization]](FAO). The geopolitical ontology includes names in multiple languages (English, French, Spanish, Arabic, Chinese, Russian and Italian); maps standard coding systems (UN, ISO, FAOSTAT, AGROVOC, etc.); provides relations among territories (land borders, group membership, etc.); and tracks historical changes. In addition, FAO provides web services of geopolitical ontology and a module maker to download modules of the geopolitical ontology into different formats (RDF, XML, and EXCEL). See more information at [[FAO Country Profiles]].
* GOLD,<ref>{{cite web |url=http://www.linguistics-ontology.org/gold.html |title=GOLD |accessdate=10 February 2011}}</ref> General Ontology for [[descriptive linguistics|Linguistic Description]]
* GUM (Generalized Upper Model),<ref>{{cite web |url=http://www.fb10.uni-bremen.de/anglistik/langpro/webspace/jb/gum/index.htm |title=Generalized Upper Model |accessdate=10 February 2011}}</ref> a linguistically motivated ontology for mediating between clients systems and natural language technology
* [[IDEAS Group]],<ref>{{cite web |url=http://www.ideasgroup.org |title=The IDEAS Group Website |accessdate=10 February 2011}}</ref> a formal ontology for enterprise architecture being developed by the Australian, Canadian, UK and U.S. Defence Depts.
* Linkbase,<ref>{{cite web |url=http://www.landcglobal.com/pages/linkbase.php |title=Linkbase |accessdate=10 February 2011}}</ref> a formal representation of the biomedical domain, founded upon [http://www.ifomis.org/bfo/ Basic Formal Ontology].
* LPL, Lawson Pattern Language
* NCBO Bioportal,<ref>{{cite web|title=Bioportal|url=http://www.bioontology.org/tools/portal/bioportal.html|publisher=National Center for Biological Ontology (NCBO)}}</ref> biological and biomedical ontologies and associated tools to search, browse and visualise
* [[NIFSTD]] Ontologies from the [[Neuroscience Information Framework]]: a modular set of ontologies for the neuroscience domain.
* OBO-Edit,<ref>{{cite web|title=Ontology browser for most of the Open Biological and Biomedical Ontologies|url=http://oboedit.org/?page=index|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}</ref> an ontology browser for most of the Open Biological and Biomedical Ontologies
* [[OBO Foundry]],<ref>{{cite web|title=The Open Biological and Biomedical Ontologies|url=http://www.obofoundry.org/|publisher=Berkeley Bioinformatics Open Source Project (BBOP)}}</ref> a suite of interoperable reference ontologies in biology and biomedicine
* OMNIBUS Ontology,<ref>{{cite web |url=http://edont.qee.jp/omnibus/ |title=OMNIBUS Ontology |accessdate=10 February 2011}}</ref> an ontology of learning, instruction, and instructional design
* [[Ontology for Biomedical Investigations]], an open access, integrated ontology for the description of biological and clinical investigations
* ONSTR,<ref>{{cite web |url= https://nbsdc.org/onstr.php |title=ONSTR |accessdate=16 April 2014}}</ref> Ontology for Newborn Screening Follow-up and Translational Research, Newborn Screening Follow-up Data Integration Collaborative, Emory University, Atlanta.
* Plant Ontology<ref name="Plant Ontology">{{cite web |url=http://www.plantontology.org/ |title=Plant Ontology |accessdate=10 February 2011}}</ref> for plant structures and growth/development stages, etc.
* POPE, Purdue Ontology for Pharmaceutical Engineering
* PRO,<ref>{{cite web |url=http://pir.georgetown.edu/pro/ |title=PRO |accessdate=10 February 2011}}</ref> the Protein Ontology of the Protein Information Resource, Georgetown University
* Program abstraction taxonomy
* Protein Ontology<ref>{{cite web |url=http://pir.georgetown.edu/pro/ |title=Protein Ontology |accessdate=10 February 2011}}</ref> for [[proteomics]]
* [[RXNO Ontology]], for [[name reaction]]s in chemistry
* [[Sequence Ontology]],<ref>{{cite journal |vauthors= Eilbeck K, Lewis SE, Mungall CJ, Yandell M, Stein L, Durbin R, Ashburner M |title= The Sequence Ontology: a tool for the unification of genome annotations |journal= Genome Biology |volume= 6 |issue= 5 |pages= R44 |year= 2005 |pmid= 15892872 |pmc= 1175956 |doi= 10.1186/gb-2005-6-5-r44 |authorlink5= Lincoln Stein |authorlink6= Richard M. Durbin |authorlink7= Michael Ashburner |authorlink2= Suzanna Lewis}}</ref> for representing genomic feature types found on [[Sequence (biology)|biological sequences]]
* [[SNOMED CT]] (Systematized Nomenclature of Medicine—Clinical Terms)
* [[Suggested Upper Merged Ontology]], a formal upper ontology
* [[Systems Biology Ontology]] (SBO), for computational models in biology
* SWEET,<ref>{{cite web |url=http://sweet.jpl.nasa.gov/ |title=SWEET |accessdate=10 February 2011}}</ref> Semantic Web for Earth and Environmental Terminology
* [[ThoughtTreasure]] ontology
* [[TIME-ITEM]], Topics for Indexing Medical Education
* [[Uberon]],<ref>{{cite journal|pmid=22293552}}</ref> representing [[metazoa|animal]] anatomical structures
* [[UMBEL]], a lightweight reference structure of 20,000 subject concept classes and their relationships derived from [[Opencyc|OpenCyc]]
* [[WordNet]], a lexical reference system
* YAMATO,<ref>{{cite web |url=http://www.ei.sanken.osaka-u.ac.jp/hozo/onto_library/upperOnto.htm |title=YAMATO |accessdate=10 February 2011}}</ref> Yet Another More Advanced Top-level Ontology

The W3C [[Linked data#Linking Open Data community project|Linking Open Data community project]] coordinates attempts to converge different ontologies into worldwide [[Semantic Web]].

== Libraries ==
The development of ontologies for the Web has led to the emergence of services providing lists or directories of ontologies with search facility. Such directories have been called ontology libraries.

The following are libraries of human-selected ontologies.
* COLORE<ref>{{cite web |url=http://stl.mie.utoronto.ca/colore/ |title=COLORE |accessdate=4 May 2011}}</ref> is an open repository of first-order ontologies in [[Common Logic]] with formal links between ontologies in the repository.
* DAML Ontology Library<ref>{{cite web |url=http://www.daml.org/ontologies/ |title=DAML Ontology Library |accessdate=10 February 2011}}</ref> maintains a legacy of ontologies in DAML.
* Ontology Design Patterns portal<ref>{{cite web |url=http://www.ontologydesignpatterns.org |title=ODP Library |accessdate=21 February 2013}}</ref> is a wiki repository of reusable components and practices for ontology design, and also maintains a list of ''exemplary ontologies''. 
* Protégé Ontology Library<ref>{{cite web
|url=http://protegewiki.stanford.edu/index.php/Protege_Ontology_Library |title=Protege Ontology Library |accessdate=10 February 2011}}</ref> contains a set of OWL, Frame-based and other format ontologies.
* SchemaWeb<ref>{{cite web |url=http://www.schemaweb.info/ |title=SchemaWeb |accessdate=10 February 2011}}</ref> is a directory of RDF schemata expressed in RDFS, OWL and DAML+OIL.

The following are both directories and search engines. They include crawlers searching the Web for well-formed ontologies.
* [[OBO Foundry]] is a suite of interoperable reference ontologies in biology and biomedicine.<ref>{{cite web |url=http://www.obofoundry.org/ |title=OBO Foundry|accessdate=10 February 2011}}</ref><ref name="pmid17989687">{{Cite journal 
| last1 = Smith | first1 = B. 
| authorlink1 = Barry Smith (ontologist)
| last2 = Ashburner | first2 = M. 
| authorlink2 = Michael Ashburner
| last3 = Rosse | first3 = C. 
| last4 = Bard | first4 = J. 
| last5 = Bug | first5 = W. 
| last6 = Ceusters | first6 = W. 
| last7 = Goldberg | first7 = L. J. 
| last8 = Eilbeck | first8 = K. 
| last9 = Ireland | first9 = A. 
| last10 = Mungall 
| doi = 10.1038/nbt1346 | first10 = C. J. 
| last11 = Leontis | first11 = N. 
| last12 = Rocca-Serra | first12 = P. 
| last13 = Ruttenberg | first13 = A. 
| last14 = Sansone | first14 = S. A. 
| last15 = Scheuermann | first15 = R. H. 
| last16 = Shah | first16 = N. 
| last17 = Whetzel | first17 = P. L. 
| last18 = Lewis | first18 = S. | authorlink18 = Suzanna Lewis
| title = The OBO Foundry: Coordinated evolution of ontologies to support biomedical data integration 
| journal = [[Nature Biotechnology]] 
| volume = 25 
| issue = 11 
| pages = 1251–1255 
| year = 2007 
| pmid = 17989687 
| pmc =2814061 
}} {{open access}}</ref>
* Bioportal (ontology repository of NCBO)
* OntoSelect<ref>{{cite web |url=http://olp.dfki.de/OntoSelect/ |title=OntoSelect |accessdate=10 February 2011}}</ref> Ontology Library offers similar services for RDF/S, DAML and OWL ontologies.
* Ontaria<ref>{{cite web |url=http://www.w3.org/2004/ontaria/ |title=Ontaria |accessdate=10 February 2011}}</ref> is a "searchable and browsable directory of semantic web data" with a focus on RDF vocabularies with OWL ontologies. (NB Project "on hold" since 2004).
* [[Swoogle]] is a directory and search engine for all RDF resources available on the Web, including ontologies.
* Open Ontology Repository initiative
* ROMULUS is a foundational ontology repository aimed at improving semantic interoperability. Currently there are three foundational ontologies in the repository: DOLCE, BFO and GFO.

== Examples of applications ==
In general, ontologies can be used beneficially in 
* enterprise applications.<ref>{{cite journal |first=Daniel |last=Oberle |title=How ontologies benefit enterprise applications |journal=Semantic Web Journal |volume=5 |issue=6 |pages=473–491 |publisher=IOS Press |date=2014 |doi=10.3233/SW-130114 |url=http://www.semantic-web-journal.net/system/files/swj212_2.pdf |format=PDF }}</ref> A more concrete example is [[SAPPHIRE (Health care)]] or ''Situational Awareness and Preparedness for Public Health Incidences and Reasoning Engines'' which is a [[semantics]]-based [[health information system]] capable of tracking and evaluating situations and occurrences that may affect [[public health]].
* [[geographic information systems]] bring together data from different sources and benefit therefore from ontological metadata which helps to connect the semantics of the data.<ref>{{cite journal|first=Andrew U. |last=Frank|title=Tiers of ontology and consistency constraints in geographical information systems|journal=International Journal of Geographical Information Science|volume=15|issue=7|year=2001|pages=667–678|doi=10.1080/13658810110061144}}</ref>

== See also ==
{{div col||25em}}
* [[Commonsense knowledge bases]]
* [[Controlled vocabulary]]
* [[Folksonomy]]
* [[Formal concept analysis]]
* [[Formal ontology]]
* [[Gene Ontology]]
* [[General formal ontology]]
* [[Lattice (order)|Lattice]]
* [[Ontology]]
* [[Ontology alignment]]
* [[Ontology chart]]
* [[Open Biomedical Ontologies]]
* [[Open Semantic Framework]]
* [[Soft ontology]]
* [[Terminology extraction]]
* [[Weak ontology]]
* [[Web Ontology Language]]
{{div col end}}

;Related philosophical concepts
* [[Alphabet of human thought]]
* [[Characteristica universalis]]
* [[Interoperability]]
* [[Metalanguage]]
* [[Natural semantic metalanguage]]

==References==
{{Reflist|2}}

==Further reading==
* Oberle, D., Guarino, N., & Staab, S. (2009) [http://userpages.uni-koblenz.de/~staab/Research/Publications/2009/handbookEdition2/what-is-an-ontology.pdf What is an ontology?]. In: "Handbook on Ontologies". Springer, 2nd edition, 2009.
* Fensel, D., van Harmelen, F., Horrocks, I., McGuinness, D. L., & Patel-Schneider, P. F. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920598 "OIL: an ontology infrastructure for the Semantic Web"]. In: ''Intelligent Systems''. IEEE, 16(2): 38&ndash;45.
* Gangemi A., Presutti V. (2009). [http://hem.hj.se/~blev/HandbookChapter_ODPs.pdf Ontology Design Patterns].{{dead link|date=September 2016}} In Staab S. et al. (eds.): Handbook on Ontologies (2nd edition), Springer, 2009.
* Maria Golemati, Akrivi Katifori, Costas Vassilakis, George Lepouras, Constantin Halatsis (2007). [http://oceanis.mm.di.uoa.gr/pened/papers/11-onto-user-final.pdf "Creating an Ontology for the User Profile: Method and Applications"]. In: ''Proceedings of the First IEEE International Conference on Research Challenges in Information Science (RCIS)'', Morocco 2007.
* Mizoguchi, R. (2004). [http://www.ei.sanken.osaka-u.ac.jp/pub/miz/Part3V3.pdf "Tutorial on ontological engineering: part 3: Advanced course of ontological engineering"]. In: ''New Generation Computing''. Ohmsha & Springer-Verlag, 22(2):198-220.
* [[Tom Gruber|Gruber, T. R.]] 1993. [http://tomgruber.org/writing/ontolingua-kaj-1993.pdf "A translation approach to portable ontology specifications"]. In: ''Knowledge Acquisition''. 5: 199&ndash;199.
* Maedche, A. & Staab, S. (2001). [http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=920602 "Ontology learning for the Semantic Web"]. In: ''Intelligent Systems''. IEEE, 16(2): 72&ndash;79.
* Natalya F. Noy and [[Deborah McGuinness|Deborah L. McGuinness]]. [http://www-ksl.stanford.edu/people/dlm/papers/ontology-tutorial-noy-mcguinness-abstract.html Ontology Development 101: A Guide to Creating Your First Ontology]. Stanford Knowledge Systems Laboratory Technical Report KSL-01-05 and Stanford Medical Informatics Technical Report SMI-2001-0880, March 2001.
* Prabath Chaminda Abeysiriwardana, Saluka R Kodituwakku, [http://www.ijorcs.org/manuscript/id/51/prabath-chaminda-abeysiriwardana-saluka-r-kodituwakku/ontology-based-information-extraction-ford-disease-intelligence "Ontology Based Information Extraction for Disease Intelligence"]. International Journal of Research in Computer Science, 2 (6): pp.&nbsp;7–19, November 2012. doi:10.7815/ijorcs.26.2012.051
* Razmerita, L., Angehrn, A., & Maedche, A. 2003. [http://www.springerlink.com/index/THW9RMVMVKLX9HAC.pdf "Ontology-Based User Modeling for Knowledge Management Systems"]. In: ''Lecture Notes in Computer Science'': 213&ndash;17.
* Soylu, A., De Causmaecker, Patrick. 2009.[http://dx.doi.org/10.1109/ISCIS.2009.5291915 Merging model driven and ontology driven system development approaches pervasive computing perspective]. in Proc 24th Intl Symposium on Computer and Information Sciences. pp 730–735.
* Smith, B. [http://precedings.nature.com/documents/2027/version/2 Ontology (Science)], in C. Eschenbach and [[Michael Gruninger|M. Gruninger]] (eds.), Formal Ontology in Information Systems. Proceedings of FOIS 2008, Amsterdam/New York: ISO Press, 21&ndash;35.
* [[Uschold, Mike]] & [[Michael Gruninger|Gruninger, M.]] (1996). [http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.111.5903&rep=rep1&type=pdf Ontologies: Principles, Methods and Applications]. Knowledge Engineering Review, 11(2).
* W. Pidcock, [http://infogrid.org/wiki/Reference/PidcockArticle ''What are the differences between a vocabulary, a taxonomy, a thesaurus, an ontology, and a meta-model?'']
* Yudelson, M., Gavrilova, T., & Brusilovsky, P. 2005. [http://www.springerlink.com/index/3n0ekp8dgm4v3pr2.pdf Towards User Modeling Meta-ontology]. Lecture Notes in Computer Science, 3538: 448.
* Movshovitz-Attias, Dana and Cohen, William W. (2012) [http://www.cs.cmu.edu/~dmovshov/papers/dma_bioNELL_bioNLP2012.pdf Bootstrapping Biomedical Ontologies for Scientific Text using NELL]. BioNLP in NAACL, Association for Computational Linguistics, 2012.

==External links==
{{Commons category|Ontology}}
* [http://www.dmoz.org/Reference/Knowledge_Management/Knowledge_Representation/ Knowledge Representation] at Open Directory Project
* [http://protegewiki.stanford.edu/wiki/Protege_Ontology_Library Library of ontologies]
* [http://www.GoPubMed.com GoPubMed] using Ontologies for searching
* [http://ontolog.cim3.net/wiki ONTOLOG] (a.k.a. "[http://ontolog.cim3.net/forum/ontolog-forum/ Ontolog Forum]") - an Open, International, Virtual Community of Practice on Ontology, Ontological Engineering and Semantic Technology
* [http://trimc-nlp.blogspot.com/2013/08/nlp-driven-ontology-modeling-for.html Use of Ontologies in Natural Language Processing]
* [http://ontolog.cim3.net/cgi-bin/wiki.pl?OntologySummit Ontology Summit] - an annual series of events (first started in 2006) that involves the ontology community and communities related to each year's theme chosen for the summit.
<!--
***********************************************************************************************

      This section shouldn't contain external links to specific ontologies,
      or to specific new subjects.

***********************************************************************************************
-->
* [http://kore-nordmann.de/talks/09_04_standardization_of_ontologies_paper.pdf Standardization of Ontologies]

{{Semantic Web}}
{{Software engineering}}
{{computable knowledge}}

{{Authority control}}

{{DEFAULTSORT:Ontology (Information Science)}}
[[Category:Knowledge engineering]]
[[Category:Technical communication]]
[[Category:Information science]]
[[Category:Semantic Web]]
[[Category:Ontology (information science)| ]]
[[Category:Knowledge representation]]
[[Category:Knowledge bases]]
<=====doc_Id=====>:796
<=====title=====>:
Basic Formal Ontology
<=====text=====>:
The '''Basic Formal Ontology''' ([http://www.ifomis.org/bfo BFO]) is a formal ontological framework developed by [[Barry Smith (ontologist)|Barry Smith]] and his associates that consists in a series of sub-ontologies at different levels of granularity. The ontologies are divided into two varieties: '''continuant''' (or snapshot) ontologies, comprehending continuant entities such as three-dimensional enduring objects, and '''occurrent''' ontologies, comprehending processes conceived as extended through (or as spanning) time. BFO thus incorporates both three-dimensionalist and four-dimensionalist perspectives on reality within a single framework. Interrelations are defined between the two types of ontologies in a way which gives BFO the facility to deal with both static/spatial and dynamic/temporal features of reality. Each continuant ontology is an inventory of all entities existing at a time. Each occurrent ontology is an inventory (processory) of all the processes unfolding through a given interval of time. Both types of ontology serve as basis for a series of sub-ontologies, each of which can be conceived as a window on a certain portion of reality at a given level of granularity.

==Applications of BFO==

BFO has been adopted as a foundational ontology by many [http://www.ifomis.org/bfo/users projects], principally in the areas of biomedical ontology and security and defense (intelligence) ontology. An example application of BFO can be seen in the [[Ontology for Biomedical Investigations]] (OBI).

==References==

*Arp, R., Smith, B., and Spear, A. D. ''[http://mitpress.mit.edu/building-ontologies Building Ontologies with Basic Formal Ontology]'', Cambridge, MA: MIT Press, August 2015, xxiv + 220pp.
* Grenon, P. and Smith, B. (2004) [http://ontology.buffalo.edu/smith/articles/SNAP_SPAN.pdf “SNAP and SPAN: Towards Dynamic Spatial Ontology”], Spatial Cognition and Computation, 4:1, 69-103.
* Smith, B. and Grenon, P. (2004) [http://ontology.buffalo.edu/smith/articles/cornucopia.pdf “The Cornucopia of Formal-Ontological Relations”], Dialectica, 58:3, 279-296.
*http://www.ifomis.uni-saarland.de/bfo/
*https://github.com/bfo-ontology/BFO/wiki
*http://ncorwiki.buffalo.edu/index.php/Basic_Formal_Ontology_2.0
==See also==

* [[Formal Ontology]]
* [[Upper ontology]]

==External links==

* [http://www.ifomis.org/bfo Basic Formal Ontology at IFOMIS]
*Katherine Munn, Barry Smith (Eds.): [http://www.ontosverlag.com/index.php?page=shop.product_details&flypage=flypage.tpl&product_id=108&category_id=13&option=com_virtuemart&Itemid=1&lang=en ''Applied Ontology: An Introduction''], Ontos Verlag.
*Ludger Jansen: "[http://ontology.buffalo.edu/bfo/Tendencies.pdf Tendencies and other Realizables in Medical Information Sciences]"
*Fabian Neuhaus, Pierre Grenon, Barry Smith: "[http://ontology.buffalo.edu/bfo/SQU.pdf A Formal Theory of Substances, Qualities, and Universals]"
*Luc Schneider: "[http://www.ifomis.org/bfo/documents/schneider-fois2010.pdf Revisiting the Ontological Square]"
*Lars Vogt: "[http://www.biomedcentral.com/1471-2105/11/289 Spatio-structural granularity of biological material entities]"
*Barry Smith, Werner Ceusters, Bert Klagges, Jacob Köhler, Anand Kumar, Jane Lomax, Chris Mungall, Fabian Neuhaus, Alan Rector and Cornelius Rosse: "[http://genomebiology.com/2005/6/5/R46 Relations in Biomedical Ontologies]", Genome Biology (2005), 6 (5), R46
*Thomas Bittner, Maureen Donnelly and Barry Smith: "[http://www.acsu.buffalo.edu/~bittner3/Publications_files/Bittner-NA-2006-28.pdf A Spatio-Temporal Ontology for Geographic Information Integration]", International Journal for Geographical Information Science, 23 (6), 2009, 765-798
* [http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0012258 Realism in Biology]
* Smith, B. and Ceusters, W. (2010) “[http://iospress.metapress.com/content/1551884412214u67/fulltext.pdf Ontological Realism as a Methodology for Coordinated Evolution of Scientific Ontologies]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}”, Applied Ontology, 5 (2010), 139–188.


[[Category:Knowledge representation]]
[[Category:Information science]]
[[Category:Ontology]]
[[Category:Ontology (information science)]]
<=====doc_Id=====>:799
<=====title=====>:
Cognitive map
<=====text=====>:
A '''cognitive map''' (sometimes called a [[mental map]] or [[mental model]]) is a type of [[mental representation]] which serves an individual to acquire, code, store, recall, and decode information about the relative locations and attributes of phenomena in their everyday or metaphorical spatial environment. The concept was introduced by [[Edward C. Tolman|Edward Tolman]] in 1948.<ref name="pmid18870876">{{cite journal |last=Tolman |first=Edward C. |authorlink=Edward C. Tolman |title=Cognitive maps in rats and men |journal=[[Psychological Review]] |volume=55 |issue=4 |pages=189–208 |date=July 1948 |pmid=18870876| doi=10.1037/h0061626}}</ref> The term was later generalized by some researchers, especially in the field of [[operations research]], to refer to a kind of [[semantic network]] representing an individual's personal knowledge or [[Schema (psychology)|schemas]].<ref>{{cite journal |last=Eden |first=Colin |date=July 1988 |title=Cognitive mapping |journal=[[European Journal of Operational Research]] |volume=36 |issue=1 |pages=1–13 |doi=10.1016/0377-2217(88)90002-1 |quote=In the practical setting of work in with a team of busy managers cognitive mapping is a tool for building interest from all team members in the problem solving activity. [...] The cycle of ''problem construction'', ''making sense'', ''defining the problem'', and declaring a ''portfolio of solutions'', which I have discussed elsewhere (Eden, 1982) is the framework that guides the process of working with teams. Thus building and working with the cognitive maps of each individual is primarily aimed at helping each team member reflectively 'construct' and 'make sense' of the situation they believe the team is facing. (pp. 7–8)}}</ref><ref>{{cite journal |last1=Fiol |first1=C. Marlene |last2=Huff |first2=Anne Sigismund |date=May 1992 |title=Maps for managers: Where are we? Where do we go from here? |journal=[[Journal of Management Studies]] |volume=29 |issue=3 |pages=267–285 |doi=10.1111/j.1467-6486.1992.tb00665.x |quote=For geographers, a map is a means of depicting the world so that people understand where they are and where they can go. For cognitive researchers, who often use the idea of a 'map' as an analogy, the basic idea is the same. Cognitive maps are graphic representations that locate people in relation to their information environments. Maps provide a frame of reference for what is known and believed. They highlight some information and fail to include other information, either because it is deemed less important, or because it is not known. (p. 267)}}</ref><ref>{{cite book |last1=Ambrosini |first1=Véronique |last2=Bowman |first2=Cliff |date=2002 |chapter=Mapping successful organizational routines |editor1-last=Huff |editor1-first=Anne Sigismund |editor2-last=Jenkins |editor2-first=Mark |title=Mapping strategic knowledge |location=London; Thousand Oaks, CA |publisher=[[Sage Publications]] |pages=19–45 |isbn=0761969497 |oclc=47900801 |quote=We shall not explain here what cognitive maps are about as this has been done extensively elsewhere (Huff, 1990). Let us just say that cognitive maps are the representation of an individual's personal knowledge, of an individual's own experience (Weick and Bougon, 1986), and they are ways of representing individuals' views of reality (Eden et al., 1981). There are various types of cognitive maps (Huff, 1990). (pp. [//books.google.com/books?id=LE95fcRz_IcC&pg=PA21 21–22])}}</ref>

== Overview ==

Cognitive maps have been studied in various fields, such as psychology, education, archaeology, planning, geography, cartography, architecture, landscape architecture, urban planning, management and history.<ref>{{cite book |title=Conspiracy nation: the politics of paranoia in Postwar America |last=Knight |first=Peter |year=2002 |publisher=[[New York University Press]] |location=New York and London |isbn=0814747353}}</ref>{{Page needed|date=August 2016}} As a consequence, these mental models are often referred to, variously, as cognitive  maps, [[mental map]]s, [[Behavioral script|script]]s, [[Schema (psychology)|schemata]], and [[Frame of reference|frames of reference]].

Cognitive maps serve the construction and accumulation of spatial knowledge, allowing the "[[mind's eye]]" to visualize images in order to reduce [[cognitive load]], enhance [[recollection|recall]] and [[learning]] of information. This type of spatial thinking can also be used as a metaphor for non-spatial tasks, where people performing non-spatial tasks involving [[memory]] and imaging use spatial knowledge to aid in processing the task.<ref>{{cite journal |last=Kitchin |first=Robert M. |title=Cognitive maps: what are they and why study them? |journal=[[Journal of Environmental Psychology]] |year=1994 |volume=14 |issue=1 |pages=1–19 |doi=10.1016/S0272-4944(05)80194-X}}</ref>

The [[neural correlate]]s of a cognitive map have been speculated to be the [[place cell]] system in the [[hippocampus]]<ref name="O'Keefe">{{cite book |last1=O'Keefe |first1=John |authorlink1=John O'Keefe (neuroscientist) |last2=Nadel |first2=Lynn |authorlink2=Lynn Nadel |date=1978 |title=The hippocampus as a cognitive map |location=Oxford; New York |publisher=[[Clarendon Press]]; [[Oxford University Press]] |isbn=0198572069 |oclc=4430731 |url=http://www.cognitivemap.net/}}</ref> and the recently discovered [[grid cells]] in the [[entorhinal cortex]].<ref name="pmid16675704">{{cite journal |last1=Sargolini |first1=Francesca |last2=Fyhn |first2=Marianne |last3=Hafting |first3=Torkel |last4=McNaughton |first4=Bruce L. |last5=Witter |first5=Menno P. |last6=Moser |first6=May-Britt |last7=Moser |first7=Edvard I. |title=Conjunctive representation of position, direction, and velocity in entorhinal cortex |journal=[[Science (journal)|Science]] |volume=312 |issue=5774 |pages=758–762 |date=May 2006 |pmid=16675704 |doi=10.1126/science.1125572 |bibcode=2006Sci...312..758S }}</ref>

== Neurological basis ==

Cognitive mapping is believed to largely be a function of the hippocampus. The hippocampus is connected to the rest of the brain in such a way that it is ideal for integrating both spatial and nonspatial information. Connections from the [[postrhinal cortex]] and the medial entorhinal cortex provide spatial information to the hippocampus. Connections from the [[perirhinal cortex]] and lateral entorhinal cortex provide nonspatial information. The integration of this information in the hippocampus makes the hippocampus a practical location for cognitive mapping, which necessarily involves combining information about an object's location and its other features.<ref name="Manns">{{cite journal |last1=Manns |first1=Joseph R. |last2=Eichenbaum |first2=Howard |authorlink2=Howard Eichenbaum |date=October 2009 |title=A cognitive map for object memory in the hippocampus |journal=[[Learning & Memory]] |volume=16 |issue=10 |pages=616–624 |doi=10.1101/lm.1484509 |pmc=2769165 |pmid=19794187 }}</ref>

O'Keefe and Nadel were the first to outline a relationship between the hippocampus and cognitive mapping.<ref name="O'Keefe" /> Many additional studies have shown additional evidence that supports this conclusion.<ref name=Moser>{{cite journal |last1=Moser |first1=Edvard I. |authorlink1=Edvard Moser |last2=Kropff |first2=Emilio |last3=Moser |first3=May-Britt |authorlink3=May-Britt Moser |date=2008 |title=Place cells, grid cells, and the brain's spatial representation system |journal=[[Annual Review of Neuroscience]] |volume=31 |pages=69–89 |doi=10.1146/annurev.neuro.31.061307.090723 |pmid=18284371 |url=http://www.annualreviews.org/eprint/7t2VcSrTYa8V8yACMweG/full/10.1146/annurev.neuro.31.061307.090723}}</ref> Specifically, [[pyramidal cells]] ([[place cells]], [[boundary cell]]s, and [[grid cells]]) have been implicated as the neuronal basis for cognitive maps within the hippocampal system.

Numerous studies by O'Keefe have implicated the involvement of place cells. Individual place cells within the hippocampus correspond to separate locations in the environment with the sum of all cells contributing to a single map of an entire environment. The strength of the connections between the cells represents the distances between them in the actual environment. The same cells can be used for constructing several environments, though individual cells' relationships to each other may differ on a map by map basis.<ref name="O'Keefe" /> The possible involvement of place cells in cognitive mapping has been seen in a number of mammalian species, including rats and macaque monkeys.<ref name=Moser /> Additionally, in a study of rats by Manns and Eichenbaum, pyramidal cells from within the hippocampus were also involved in representing object location and object identity, indicating their involvement in the creation of cognitive maps.<ref name=Manns /> However, there has been some dispute as to whether such studies of mammalian species indicate the presence of a cognitive map and not another, simpler method of determining one's environment.<ref name=Bennet />

While not located in the hippocampus, grid cells from within the medial entorhinal cortex have also been implicated in the process of [[path integration]], actually playing the role of the path integrator while place cells display the output of the information gained through path integration.<ref name=McNaughton>{{cite journal |last1=McNaughton |first1=Bruce L. |last2=Battaglia |first2=Francesco P. |last3=Jensen |first3=Ole |last4=Moser |first4=Edvard I. |authorlink4=Edvard Moser |last5=Moser |first5=May-Britt |authorlink5=May-Britt Moser |date=August 2006 |title=Path integration and the neural basis of the 'cognitive map' |journal=[[Nature Reviews Neuroscience]] |volume=7 |issue=8 |pages=663–678 |doi=10.1038/nrn1932 |pmid=16858394 }}</ref> The results of path integration are then later used by the hippocampus to generate the cognitive map.<ref name=Jacobs /> The cognitive map likely exists on a circuit involving much more than just the hippocampus, even if it is primarily based there. Other than the medial entorhinal cortex, the presubiculum and parietal cortex have also been implicated in the generation of cognitive maps.<ref name=Moser />

=== Parallel map theory ===

There has been some evidence for the idea that the cognitive map is represented in the [[hippocampus]] by two separate maps. The first is the bearing map, which represents the environment through self-movement cues and [[gradient]] cues. The use of these [[vector (mathematics)|vector]]-based cues creates a rough, 2D map of the environment. The second map would be the sketch map that works off of positional cues. The second map integrates specific objects, or [[landmark]]s, and their relative locations to create a 2D map of the environment. The cognitive map is thus obtained by the integration of these two separate maps.<ref name=Jacobs />

==Generation==

The cognitive map is generated from a number of sources, both from the [[visual system]] and elsewhere. Much of the cognitive map is created through self-generated movement [[sensory cue|cues]]. Inputs from senses like vision, [[proprioception]], olfaction, and hearing are all used to deduce a person's location within their environment as they move through it. This allows for path integration, the creation of a vector that represents one's position and direction within one's environment, specifically in comparison to an earlier reference point. This resulting vector can be passed along to the hippocampal place cells where it is interpreted to provide more information about the environment and one's location within the context of the cognitive map.<ref name=Jacobs>{{cite journal |last1=Jacobs |first1=Lucia F. |last2=Schenk |first2=Françoise |date=April 2003 |title=Unpacking the cognitive map: the parallel map theory of hippocampal function |journal=[[Psychological Review]] |volume=110 |issue=2 |pages=285–315 |doi=10.1037/0033-295X.110.2.285 |pmid=12747525}}</ref>

Directional cues and positional landmarks are also used to create the cognitive map. Within directional cues, both explicit cues, like markings on a compass, as well as gradients, like shading or magnetic fields, are used as inputs to create the cognitive map. Directional cues can be used both statically, when a person does not move within his environment while interpreting it, and dynamically, when movement through a gradient is used to provide information about the nature of the surrounding environment. Positional landmarks provide information about the environment by comparing the relative position of specific objects, whereas directional cues give information about the shape of the environment itself. These landmarks are processed by the hippocampus together to provide a graph of the environment through relative locations.<ref name=Jacobs />

== History ==

The idea of a cognitive map was first developed by [[Edward C. Tolman]]. Tolman, one of the early cognitive psychologists, introduced this idea when doing an experiment involving rats and mazes. In Tolman's experiment, a rat was placed in a cross shaped maze and allowed to explore it. After this initial exploration, the rat was placed at one arm of the cross and food was placed at the next arm to the immediate right. The rat was conditioned to this layout and learned to turn right at the intersection in order to get to the food. When placed at different arms of the cross maze however, the rat still went in the correct direction to obtain the food because of the initial cognitive map it had created of the maze. Rather than just deciding to turn right at the intersection no matter what, the rat was able to determine the correct way to the food no matter where in the maze it was placed.<ref>{{cite book |last=Goldstein |first=E. Bruce |date=2011 |title=Cognitive psychology: connecting mind, research, and everyday experience |edition=3rd |location=Belmont, CA |publisher=[[Wadsworth Cengage Learning]] |isbn=9780840033550 |oclc=658234658 |pages=11–12}}</ref>

== Criticism ==

In a review, Andrew T.D. Bennett argued that there are no clear evidence for cognitive maps in non-human animals (i.e. cognitive map according to Tolman's definition).<ref name=Bennet>{{cite journal |last=Bennett |first=Andrew T. D. |date=January 1996 |title=Do animals have cognitive maps? |journal=[[The Journal of Experimental Biology]] |volume=199 |issue=Pt 1 |pages=219–224 |pmid=8576693}}</ref> This argument is based on analyses of studies where it has been found that simpler explanations can account for experimental results. Bennett highlights three simpler alternatives that cannot be ruled out in tests of cognitive maps in non-human animals "These alternatives are (1) that the apparently novel short-cut is not truly novel; (2) that path integration is being used; and (3) that familiar landmarks are being recognised from a new angle, followed by movement towards them."

== Related term ==
{{Refimprove section|date=August 2016}}

A cognitive map is a spatial representation of the outside world that is kept within the mind, until an actual manifestation (usually, a drawing) of this perceived knowledge is generated, a mental map. Cognitive mapping is the implicit, mental mapping the explicit part of the same process. In most cases, a cognitive map exists independently of a mental map, an article covering just cognitive maps would remain limited to theoretical considerations.

In some uses, mental map refers to a practice done by urban theorists by having city dwellers draw a map, from memory, of their city or the place they live. This allows the theorist to get a sense of which parts of the city or dwelling are more substantial or imaginable. This, in turn, lends itself to a decisive idea of how well urban planning has been conducted.

==See also==
* [[Cognitive geography]]
* [[Fuzzy cognitive map]]
* [[Motion perception]]
* [[Repertory grid]]

==References==
{{Reflist|30em}}

== External links ==
* {{commonscat-inline|Cognitive maps}}

{{DEFAULTSORT:Cognitive Map}}
[[Category:Cognitive science]]
[[Category:Mnemonics]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:802
<=====title=====>:
Vivid knowledge
<=====text=====>:
'''Vivid knowledge''' refers to a specific kind of [[knowledge representation]].

The idea of a '''vivid knowledge base''' is to get an interpretation mostly straightforward out of it &ndash; it implies the interpretation. Thus, any query to such a [[knowledge base]] can be reduced to a [[database]]-like query.

== Propositional knowledge base ==

A [[Propositional logic|propositional]] [[knowledge base]] KB is '''vivid''' ''iff'' KB is a [[Completeness (knowledge bases)|complete]] and [[consistency (knowledge bases)|consistent]] set of [[Literal (mathematical logic)|literals]] (over some vocabulary).<ref>Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337</ref>

Such a knowledge base has the property that it as exactly one interpretation, i.e. the interpretation is unique. A check for entailment of a sentence can simply be broken down into its literals and those can be answered by a simple database-like check of KB.

== First-order knowledge base ==

A [[First-order logic|first-order]] knowledge base KB is '''vivid''' ''iff'' for some finite set of positive function-free ground literals KB<sup>+</sup>,

: KB = KB<sup>+</sup> ∪ Negations ∪ DomainClosure ∪ UniqueNames,

whereby

: Negations ≔ { ¬p | p is atomic and KB ⊭ p },
: DomainClosure ≔ { (c<sub>i</sub> ≠ c<sub>j</sub>) | c<sub>i</sub>, c<sub>j</sub> are distinct constants },
: UniqueNames ≔ { ∀x: (x = c<sub>1</sub>) ∨ (x = c<sub>2</sub>) ∨ ..., where the c<sub>i</sub> are all the constants in KB<sup>+</sup> }.

<ref>Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 337</ref>

All interpretations of a vivid first-order knowledge base are isomorphic.<ref>Knowledge Representation and Reasoning / Ronald J. Brachman, Hector J. Levesque / page 339</ref>

== See also ==
* [[Closed world assumption]]

{{computable knowledge}}

== References ==

<references/>

[[Category:Knowledge representation]]


{{logic-stub}}
{{database-stub}}
<=====doc_Id=====>:805
<=====title=====>:
Ontology alignment
<=====text=====>:
'''Ontology alignment''', or '''ontology matching''', is the process of determining correspondences between [[concept]]s in [[ontologies]]. A set of correspondences is also called an alignment. The phrase takes on a slightly different meaning, in [[computer science]], [[cognitive science]] or [[philosophy]].

==Computer Science==

For [[computer scientist]]s, concepts are expressed as labels for data.  Historically, the need for ontology alignment arose out of the need to [[data integration|integrate]] heterogeneous [[database]]s, ones developed independently and thus each having their own data vocabulary.  In the [[Semantic Web]] context involving many actors providing their own [[ontology (information science)|ontologies]], ontology matching has taken a critical place for helping heterogeneous resources to interoperate. Ontology alignment tools find classes of data that are "[[semantic equivalence|semantically equivalent]]," for example, "Truck" and "Lorry."  The classes are not necessarily logically identical.  According to Euzenat and Shvaiko (2007),<ref name="Euzenat Shvaiko">Jérôme Euzenat and Pavel Shvaiko. 2007. [http://book.ontologymatching.org Ontology matching], Springer-Verlag, 978-3-540-49611-3.</ref> there are three major dimensions for similarity: syntactic, external, and semantic.  Coincidentally, they roughly correspond to the dimensions identified by Cognitive Scientists below.  A number of tools and frameworks have been developed for aligning ontologies, some with inspiration from Cognitive Science and some independently.

Ontology alignment tools have generally been developed to operate on [[database schema]]s,<ref>J. Berlin and A. Motro. 2002. [http://www.dit.unitn.it/~accord/RelatedWork/Matching/Berlin_caise02.pdf Database Schema Matching Using Machine Learning with Feature Selection]. Proc. of the 14th International Conference on Advanced Information Systems Engineering, pp. 452-466</ref> [[XML schema]]s,<ref name="coma">D. Aumueller, H. Do, S. Massmann, E. Rahm. 2005. [http://www.dit.unitn.it/~p2p/RelatedWork/Matching/COMA++-SIGMOD05.pdf Schema and ontology matching with COMA++]. Proc. of the 2005 International Conference on Management of Data, pp. 906-908</ref> [[Taxonomy (general)|taxonomies]],<ref>S. Ponzetto, R. Navigli. 2009. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf "Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia"]. Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009), Pasadena, California, pp. 2083-2088.</ref> [[formal language]]s, [[entity-relationship model]]s,<ref>A. H. Doan, A. Y. Halevy. [http://pages.cs.wisc.edu/~anhai/papers/si-survey-db-community.pdf Semantic integration research in the database community: A brief survey]. AI magazine, 26(1), 2005</ref> [[dictionary|dictionaries]], and other label frameworks. They are usually converted to a graph representation before being matched. 
Since the emergence of the Semantic Web, such graphs can be represented in the [[Resource Description Framework]] line of languages by triples of the form <subject, predicate, object>, as illustrated in the [[Notation 3]] syntax.
In this context, aligning ontologies is sometimes referred to as "ontology matching".

The problem of Ontology Alignment has been tackled recently by trying to compute matching first and mapping (based on the matching) in an automatic fashion. Systems like [[DSSim]], X-SOM<ref name="curino-xsom2007">{{cite journal|author=Carlo A. Curino and Giorgio Orsi and Letizia Tanca |title=X-SOM: A Flexible Ontology Mapper |url=http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |journal=International Workshop on Semantic Web Architectures for Enterprises (SWAE'07) in conjunction with the 18th International Conference on Database and Expert Systems Applications (DEXA'07) |year=2007 |format= |deadurl=yes |archiveurl=https://web.archive.org/web/20120213104823/http://www.polibear.net/blog/wp-content/uploads/2008/01/orsi-xsomflexmap.pdf |archivedate=February 13, 2012 }}</ref> or COMA++ obtained at the moment very high precision and recall.<ref name="coma" /> The [http://oaei.ontologymatching.org Ontology Alignment Evaluation Initiative] aims to evaluate, compare and improve the different approaches.

More recently, a technique useful to minimize the effort in mapping validation and visualization has been presented which is based on [[Minimal mappings|Minimal Mappings]]. Minimal mappings are high quality mappings such that i) all the other mappings can be computed from them in time linear in the size of the input graphs, and ii) none of them can be dropped without losing property i).

=== Formal Definition ===
Given two ontologies <math>i=\langle C_{i}, R_{i}, I_{i}, A_{i}\rangle</math> and <math>j=\langle C_{j}, R_{j}, I_{j}, A_{j}\rangle</math>{{clarify|Give a (link to a) formal definition of 'ontology' first, such that the meaning of C, R, I, and A is explained.|date=January 2017}} we can define different type of (inter-ontology) relationships among their terms. 
Such relationships will be called, all together, alignments and can be categorized among different dimensions:

* similarity vs logic: this is the difference between matchings (predicating about the [[semantic similarity|similarity]] of ontology terms), and mappings ([[logical axiom]]s, typically expressing [[logical equivalence]] or inclusion among ontology terms)
* atomic vs complex: whether the alignments we considered are [[bijection|one-to-one]], or can involve more terms in a query-like formulation (e.g., [[data integration|LAV/GAV]] mapping)
* homogeneous vs heterogeneous: do the alignments predicate on terms of the same type (e.g., classes are related only to classes, individuals to individuals, etc.) or we allow heterogeneity in the relationship?
* type of alignment: the semantics associated to an alignment. It can be [[Hierarchy#Subsumptive containment hierarchy|subsumption]], [[logical equivalence|equivalence]], [[disjointness]], [[part-of]] or any user-specified relationship.

Subsumption, atomic, homogeneous alignments are the building blocks to obtain richer alignments, and have a well defined semantics in every Description Logic. 
Let's now introduce more formally ontology matching and mapping.

An atomic homogeneous '''matching''' is an alignment that carries a similarity degree <math>s\in [0,1]</math>, describing the similarity of two terms of the input ontologies <math>i</math> and <math>j</math>.
Matching can be either ''computed'', by means of heuristic algorithms, or ''[[inference|inferred]]'' from other matchings.

Formally we can say that, a matching is a quadruple <math>m=\langle id, t_{i}, t_{j}, s\rangle</math>, where <math>t_{i}</math> and <math>t_{j}</math> are homogeneous ontology terms, <math>s</math> is the similarity degree of <math>m</math>. 
A (subsumption, homogeneous, atomic) mapping is defined as a pair <math>\mu=\langle t_{i}, t_{j}\rangle</math>, where <math>t_{i}</math> and <math>t_{j}</math> are homogeneous ontology terms.

==Cognitive Science==

For [[cognitive scientist]]s interested in ontology alignment, the "concepts" are nodes in a [[semantic network]] that reside in brains as "conceptual systems."  The focal question is: if everyone has unique experiences and thus different semantic networks, then how can we ever understand each other?  This question has been addressed by a model called ABSURDIST (Aligning Between Systems Using Relations Derived Inside Systems for Translation). Three major dimensions have been identified for similarity as equations for "internal similarity, external similarity, and mutual inhibition."<ref>R. Goldstone and B. Rogosky. 2002. [http://courses.media.mit.edu/2003spring/mas963/goldstone.pdf Using relations within conceptual systems to translate across conceptual systems]. Cognition 84, pp. 295–320.</ref>

Ontology alignment is closely related to [[analogy formation]], where "concepts" are variables in logic expressions.

==Ontology alignment methods==
Two sub research fields have emerged in recent years in ontology mapping, namely monolingual ontology mapping and cross-lingual ontology mapping. The former refers to the mapping of ontologies in the same natural language, whereas the latter refers to "the process of establishing relationships among ontological resources from two or more independent ontologies where each ontology is labelled in a different natural language".<ref>Bo Fu, Rob Brennan, Declan O'Sullivan, A Configurable Translation-Based Cross-Lingual Ontology Mapping System to adjust Mapping Outcomes. Journal of Web Semantics, Volume 15, 15-36, ISSN 1570-8268, 2012 [http://www.sciencedirect.com/science/article/pii/S1570826812000704].</ref> Existing matching methods in monolingual ontology mapping are discussed in Euzenat and Shvaiko (2007).<ref name="Euzenat Shvaiko"/> Current approaches to cross-lingual ontology mapping are presented in Fu et al. (2011).<ref>Fu B., Brennan R., O'Sullivan D., Using Pseudo Feedback to Improve Cross-Lingual Ontology Mapping [http://www.springerlink.com/content/a214858426kgm750/]. In Proceedings of the 8th Extended Semantic Web Conference (ESWC 2011), LNCS 6643, pp.336-351, Heraklion, Greece, May 2011.</ref>

==Philosophy==

For philosophers, much like cognitive scientists, the interest is in the nature of "understanding."  The roots of discourse, however, may be traced to [[radical interpretation]].

==Visualization Tools (links obsolete)==
*[http://www.mondeca.com/content/download/718/6964/file/ITM_ALIGN_en.pdf ITM Align: semi-automated ontology alignment]
*[http://cs.uga.edu/~uthayasa/Optima/Optima.html Optima: A Visual Ontology Alignment Tool]
*[http://www.stanford.edu/~sfalc/cogz/cogz.html CogZ: Cognitive Support and Visualization for Human-Guided Mapping Systems]
*[http://agreementmaker.org AgreementMaker: Efficient Matching for Large Real-World Schemas and Ontologies]
*[http://bio-mixer.appspot.com/ Biomixer]: A Web-based Collaborative Ontology Visualization Tool.
*[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6882028 SDI(Semantic Data Integration) Tool]: A Semantic Mapping Representation and Generation Tool Using UML for System Engineers

==See also==
* [[Ontology (computer science)]]
* [[Rule Interchange Format]] (RIF)
* [[Data conversion]]
* [[Semantic Integration]]
* [[Semantic matching]]
* [[Minimal mappings|Minimal Mappings]]
* [[Semantics|Interpretation]] "An interpretation can be the part of a presentation or portrayal of information altered in order to conform to a specific set of symbols."
* [[Graph isomorphism]]
* [[Unification (computer science)]] (as [[Semantic unification]])
* [[Semantic integration]]

==References==
{{Reflist|colwidth=35em}}

== Further reading ==
*[http://www.ontologymatching.org/publications.html Collection of surveys and research papers related to ontology mapping, matching, and alignment]
* [http://www.atl.external.lmco.com/projects/ontology/ The Ontology Alignment Source]
* [http://cognitrn.psych.indiana.edu/rgoldsto/pdfs/cogsci2002.pdf ABSURDIST]
* [http://ontologymatching.org/publications.html Ontologymatching.org: Surveys, Approaches, and Themes]
* [http://knoesis.wright.edu/library/publications/iswc10_paper218.pdf Ontology Alignment for Linked Open Data]
* [http://sites.google.com/site/bschopman/master-thesis/master.pdf Instance-based Ontology Matching by Instance Enrichment]
* Noy, N. F. (2004). "Semantic integration: a survey of ontology-based approaches." SIGMOD Rec. 33(4): 65-70.

[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:808
<=====title=====>:
Semantic data model
<=====text=====>:
[[File:A2 4 Semantic Data Models.svg|thumb|320px|Semantic data models.<ref name ="FIPS184">[http://www.itl.nist.gov/fipspubs/idef1x.doc FIPS Publication 184] released of IDEF1X by the Computer Systems Laboratory of the National Institute of Standards and Technology (NIST). 21 December 1993.</ref>]]
A '''semantic data model''' in [[software engineering]] has various meanings: 
# It is a [[conceptual data model]] in which semantic information is included. This means that the model describes the meaning of its instances. Such a semantic [[data model]] is an abstraction that defines how the stored [[symbol]]s (the instance data) relate to the real world.<ref name ="FIPS184"/>
# It is a [[conceptual data model]] that includes the capability to express information that enables parties to the information exchange to interpret meaning (semantics) from the instances, without the need to know the meta-model. Such semantic models are fact oriented (as opposed to object oriented). Facts are typically expressed by [[binary relations]] between [[data]] elements, whereas higher order relations are expressed as collections of binary relations. Typically binary relations have the form of triples: Object-RelationType-Object. For example: the Eiffel Tower <is located in> Paris.
Typically the instance data of semantic data models explicitly include the kinds of relationships between the various data elements, such as <is located in>. To interpret the meaning of the facts from the instances it is required that the meaning of the kinds of relations (relation types) be known. Therefore, semantic data models typically standardise such relation types. This means that the second kind of semantic data models enable that the instances express facts that include their own meaning. 
The second kind of semantic data models are usually meant to create semantic databases. The ability to include meaning in semantic databases facilitates building [[distributed database]]s that enable applications to interpret the meaning from the content. This implies that semantic databases can be integrated when they use the same (standard) relation types. This also implies that in general they have a wider applicability than relational or object oriented databases.

== Overview ==
The logical data structure of a [[database management system]] (DBMS), whether [[Hierarchical model|hierarchical]], [[Network model|network]], or [[Relational model|relational]], cannot totally satisfy the [[Requirements analysis|requirements]] for a conceptual definition of data, because it is limited in scope and biased toward the implementation strategy employed by the DBMS. Therefore, the need to define data from a [[Three schema approach|conceptual view]] has led to the development of semantic data modeling techniques. That is, techniques to define the meaning of data within the context of its interrelationships with other data. As illustrated in the figure. The real world, in terms of resources, ideas, events, etc., are symbolically defined within physical data stores. A semantic data model is an abstraction which defines how the stored symbols relate to the real world. Thus, the model must be a true representation of the real world.<ref name ="FIPS184"/>

According to Klas and Schrefl (1995), the "overall goal of semantic data models is to capture more meaning of data by integrating relational concepts with more powerful abstraction concepts known from the [[Artificial Intelligence]] field. The idea is to provide high level modeling primitives as integral part of a data model in order to facilitate the representation of real world situations".<ref>Wolfgang Klas, Michael Schrefl (1995). "Semantic data modeling" In: ''Metaclasses and Their Application''. Book Series Lecture Notes in Computer Science. Publisher Springer Berlin / Heidelberg. Volume Volume 943/1995.</ref>

== History ==
The need for semantic data models was first recognized by the U.S. Air Force in the mid-1970s as a result of the [[Integrated Computer-Aided Manufacturing]] (ICAM) Program. The objective of this program was to increase manufacturing productivity through the systematic application of computer technology. The ICAM Program identified a need for better analysis and communication techniques for people involved in improving manufacturing productivity. As a result, the ICAM Program developed a series of techniques known as the IDEF (ICAM Definition) Methods which included the following:<ref name ="FIPS184"/>
* [[IDEF0]] used to produce a “function model” which is a structured representation of the activities or processes within the environment or system.
* [[IDEF1]] used to produce an “information model” which represents the structure and semantics of information within the environment or system.
** [[IDEF1X]] is a semantic data modeling technique. It is used to produce a graphical information model which represents the structure and semantics of information within an environment or system. Use of this standard permits the construction of semantic data models which may serve to support the management of data as a resource, the integration of information systems, and the building of computer databases.
* [[IDEF2]] used to produce a “dynamics model” which represents the time varying behavioral characteristics of the environment or system.

During the 1990s the application of semantic modelling techniques resulted in the semantic data models of the second kind. An example of such is the semantic data model that is standardised as [[ISO 15926]]-2 (2002), which is further developed into the semantic modelling language [[Gellish]] (2005). The definition of the Gellish language is documented in the form of a semantic data model. Gellish itself is a semantic modelling language, that can be used to create other semantic models. Those semantic models can be stored in Gellish Databases, being semantic databases.

== Applications ==
A semantic data model can be used to serve many purposes. Some key objectives include:<ref name ="FIPS184"/>
* Planning of Data Resources: A preliminary data model can be used to provide an overall view of the data required to run an enterprise. The model can then be analyzed to identify and scope projects to build shared data resources.
* Building of Shareable Databases: A fully developed model can be used to define an application independent view of data which can be validated by users and then transformed into a physical database design for any of the various DBMS technologies. In addition to generating databases which are consistent and shareable, development costs can be drastically reduced through data modeling.
* Evaluation of Vendor Software: Since a data model actually represents the infrastructure of an organization, vendor software can be evaluated against a company’s data model in order to identify possible inconsistencies between the infrastructure implied by the software and the way the company actually does business.
* Integration of Existing Databases: By defining the contents of existing databases with semantic data models, an integrated data definition can be derived. With the proper technology, the resulting conceptual schema can be used to control transaction processing in a distributed database environment. The U.S. Air Force Integrated Information Support System (I2S2) is an experimental development and demonstration of this type of technology applied to a heterogeneous DBMS environment.

== See also ==
* [[Conceptual schema]]
* [[Object-role modeling]]
* [[Entity-relationship model]]
* [[Information model]]
* [[Relational Model/Tasmania]]
* [[Three schema approach]]
* [[QuakeSim]]

== References ==
{{NIST-PD}}
{{reflist}}

== Further reading ==
* [http://hpdrc.cs.fiu.edu/library/books/datades-book/ Database Design - The Semantic Modelling Approach]
* Johan ter Bekke (1992). ''Semantic Data Modeling''. Prentice Hall.
* Alfonso F. Cardenas and Dennis McLeod (1990). ''Research Foundations in Object-Oriented and Semantic Database Systems''. Prentice Hall.
* Peter Gray, Krishnarao G. Kulkarni and, Norman W. Paton (1992). ''Object-Oriented Databases: A Semantic Data Model Approach''. Prentice-Hall International Series in Computer Science.
* Michael Hammer and Dennis McLeod (1978). "The Semantic Data Model: a Modeling Mechanism for Data Base Applications." In: ''Proc. ACM SIGMOD Int’l. Conf. on Management of Data''. Austin, Texas, May 31 - June 2, 1978, pp.&nbsp;26–36.

== External links ==
* [http://www.jhterbekke.net/SemanticDataModeling.html Semantic Data Modeling] Johan ter Bekke tribute site.

{{Data model}}

{{DEFAULTSORT:Semantic Data Model}}
[[Category:Data modeling]]
[[Category:Systems analysis]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:811
<=====title=====>:
Procedural reasoning system
<=====text=====>:
In [[artificial intelligence]], a '''procedural reasoning system''' ('''PRS''') is a framework for constructing real-time [[Reasoning system|reasoning systems]] that can perform complex tasks in dynamic environments. It is based on the notion of a [[rational agent]] or [[intelligent agent]] using the [[belief–desire–intention software model]].

A user application is predominately defined, and provided to a PRS system is a set of ''knowledge areas''.  Each knowledge area is a piece of [[procedural knowledge]] that specifies how to do something, e.g., how to navigate down a corridor, or how to plan a path (in contrast with [[robotic architectures]] where the [[computer programmer|programmer]] just provides a model of what the states of the world are and how the agent's primitive actions affect them).  Such a program, together with a PRS [[interpreter (computing)|interpreter]], is used to control the agent.

The interpreter is responsible for maintaining beliefs about the world state, choosing which goals to attempt to achieve next, and choosing which knowledge area to apply in the current situation.  How exactly these operations are performed might depend on domain-specific [[metaknowledge|meta-level]] knowledge areas.  Unlike traditional [[computer planning|AI planning]] systems that generate a complete plan at the beginning, and replan if unexpected things happen, PRS interleaves planning and doing actions in the world.  At any point, the system might only have a partially specified plan for the future.

PRS is based on the [[BDI software agent|BDI]] or belief–desire–intention framework for intelligent agents.  Beliefs consist of what the agent believes to be true about the current state of the world, desires consist of the agent's goals, and intentions consist of the agent's current plans for achieving those goals.  Furthermore, each of these three components is typically ''explicitly'' represented somewhere within the memory of the PRS agent at runtime, which is in contrast to purely reactive systems, such as the [[subsumption architecture]].

== History ==
The PRS concept was developed by the [[Artificial Intelligence Center]] at [[SRI International]] during the 1980s, by many workers including [[Michael Georgeff]], [[Amy L. Lansky]], and [[François Félix Ingrand]]. Their framework was responsible for exploiting and popularizing the BDI model in software for control of an [[intelligent agent]]. The seminal application of the framework was a fault detection system for the reaction control system of the [[NASA]] [[Space Shuttle Discovery]]. Development on this PRS continued at the [[Australian Artificial Intelligence Institute]] through to the late 1990s, which lead to the development of a [[C++]] implementation and extension called [[distributed multi-agent reasoning system|dMARS]].

== Architecture ==
[[Image:PRS.gif|thumb|Depiction of the PRS architecture]]
The system architecture of SRI's PRS includes the following components:
* '''Database''' for beliefs about the world, represented using first order predicate calculus.
* '''Goals''' to be realized by the system as conditions over an interval of time on internal and external state descriptions (desires).
* '''Knowledge areas''' (KAs) or plans that define sequences of low-level actions toward achieving a goal in specific situations.
* '''Intentions''' that include those KAs that have been selected for current and eventual execution.
* '''Interpreter''' or inference mechanism that manages the system.

== Features ==
SRI's PRS was developed for embedded application in dynamic and real-time environments. As such it specifically addressed the limitations of other contemporary control and reasoning architectures like [[expert system]]s and the [[blackboard system]]. The following define the general requirements for the development of their PRS:<ref>
{{cite journal
 | doi = 10.1109/64.180407
 | last = Ingrand
 | first = F.
 |author2=M. Georgeff |author3=A Rao
  | title = An architecture for real-time reasoning and system control
 | journal = IEEE Expert: Intelligent Systems and Their Applications
 | volume = 7
 | issue = 6
 | year = 1992
 | pages = 34–44 
 | publisher = IEEE Press
 | url = http://portal.acm.org/citation.cfm?id=629535.629890 }}
</ref>

* asynchronous event handling
* guaranteed reaction and response types
* procedural representation of knowledge
* handling of multiple problems
* reactive and goal-directed behavior
* focus of attention
* reflective reasoning capabilities
* continuous embedded operation
* handling of incomplete or inaccurate data
* handling of transients
* modeling delayed feedback
* operator control

== Applications ==
The seminal application of SRI's PRS was a monitoring and fault detection system for the reaction control system (RCS) on the NASA space shuttle.<ref>
{{cite conference
  | last = Georgeff
  | first = M. P.
  |author2=F. F. Ingrand
  | title = Real-time reasoning: the monitoring and control of spacecraft systems
  | booktitle = Proceedings of the sixth conference on Artificial intelligence applications
  | year = 1990
  | pages = 198–204
  | url = http://portal.acm.org/citation.cfm?id=96782 }}
</ref> The RCS provides propulsive forces from a collection of jet thrusters and controls altitude of the space shuttle. A PRS-based fault diagnostic system was developed and tested using a simulator. It included over 100 KAs and over 25 meta level KAs. RCS specific KAs were written by space shuttle mission controllers. It was implemented on the [[Symbolics]] 3600 Series [[LISP]] machine and used multiple communicating instances of PRS. The system maintained over 1000 facts about the RCS, over 650 facts for the forward RCS alone and half of which are updated continuously during the mission. A version of the PRS was used to monitor the reaction control system on the [[NASA]] [[Space Shuttle Discovery]].

PRS was tested on [[Shakey the robot]] including navigational and simulated jet malfunction scenarios based on the space shuttle.<ref>
{{cite conference
  | last = Georgeff
  | first = M. P.
  |author2=A. L. Lansky
  | title = Reactive reasoning and planning
  | booktitle = Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI-87)
  | year = 1987
  | pages = 198–204
  | url = http://www.ai.sri.com/pubs/files/1364.pdf 
  | work = [[Artificial Intelligence Center]]
  | publisher = [[SRI International]] }}
</ref> Later applications included a network management monitor called the Interactive Real-time Telecommunications Network Management System (IRTNMS) for [[Telecom Australia]].<ref>
{{cite conference
  | last = Rao
  | first = Anand S.
  |author2=Michael P. Georgeff 
  | title = Intelligent Real-Time Network Management
  | booktitle = Australian Artificial Intelligence Institute, Technical Note 15
  | year = 1991
  | citeseerx = 10.1.1.48.3297 }}
</ref>

== Extensions ==
The following list the major implementations and extensions of the PRS architecture.<ref>
{{cite conference
  | last = Wobcke
  | first = W. R.
  | title = Reasoning about BDI Agents from a Programming Languages Perspective
  | booktitle = Proceedings of the AAAI 2007 Spring Symposium on Intentions in Intelligent Systems
  | year = 2007
  | url = http://www.cse.unsw.edu.au/~wobcke/papers/ss.07.pdf }}
</ref>
* UM-PRS <ref>[http://www.marcush.net/IRS/irs_downloads.html]</ref>
* OpenPRS (formerly C-PRS and Propice) <ref>[http://www.laas.fr/~felix/PRS]</ref> <ref>[https://softs.laas.fr/openrobots/wiki/openprs]</ref>
* [[AgentSpeak]]
* [[Distributed Multi-Agent Reasoning System]] (dMARS)
* JAM <ref>[http://www.marcush.net/IRS/irs_downloads.html]</ref>
* [[JACK Intelligent Agents]]
* SRI Procedural Agent Realization Kit (SPARK) <ref>[http://www.ai.sri.com/~spark/]</ref>
* PRS-CL <ref>[http://www.ai.sri.com/~prs/]</ref>

== See also ==
* [[Distributed multi-agent reasoning system]]
* [[JACK Intelligent Agents]]
* [[Belief-desire-intention software model]]
* [[Intelligent agent]]

== References ==
{{reflist}}

==Further reading==
* M.P. Georgeff and A.L. Lansky. "A system for reasoning in dynamic domains: Fault diagnosis on the space shuttle" Technical Note 375, Artificial Intelligence Center, SRI International, 1986.
* Michael P. Georgeff, Amy L. Lansky, Marcel J. Schoppers. "[http://www.ai.sri.com/pubs/files/579.pdf Reasoning and Planning in Dynamic Domains: An Experiment with a Mobile Robot]" Technical Note 380, Artificial Intelligence Center, SRI International, 1987.
* M. Georgeff, and A. L. Lansky (1987). [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1457907 Procedural knowledge].  Proceedings of the IEEE 74(10):1383–1398, IEEE Press.
* Georgeff, Michael P.; Ingrand, Francois Felix. "[http://ntrs.nasa.gov/search.jsp?R=124384&id=4&as=false&or=false&qs=Ns%3DArchiveName%257c0%26N%3D4294823185 Research on procedural reasoning systems]" Final Report – Phase 1, Artificial Intelligence Center, SRI International, 1988.
* Michael P. Georgeff and François Félix Ingrand "[http://www.laas.fr/~felix/download.php/ijcai89.pdf Decision-Making in an Embedded Reasoning System]" Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, Detroit (Michigan), August 1989.
* K. L. Myers, [http://www.ai.sri.com/~prs/prs-manual.pdf User Guide for the Procedural Reasoning System] Technical Report, Artificial Intelligence Center, Technical Report, SRI International, Menlo Park, CA, 1997
* [http://www.sti.nasa.gov/tto/Spinoff2006/ch_2.html A Match Made in Space] Spinoff, NASA, 2006

== External links ==
* [http://www.ai.sri.com/~prs/ PRS-CL: A Procedural Reasoning System] An extension to PRS maintained by SRI International

[[Category:Knowledge representation]]
[[Category:Cognitive architecture]]
[[Category:Agent-based software]]
[[Category:Multi-agent systems]]
[[Category:Agent-oriented programming languages]]
[[Category:Agent-based programming languages]]
[[Category:SRI International software]]
<=====doc_Id=====>:814
<=====title=====>:
Figurative system of human knowledge
<=====text=====>:
[[File:ENC SYSTEME FIGURE.jpeg|right|200px|thumb|[[Classification chart]] with the original "figurative system of human knowledge" tree, in French.]]

The '''"figurative system of human knowledge"''', sometimes known as '''the tree of Diderot and d'Alembert''', was a tree developed to represent the structure of [[knowledge]] itself, produced for the ''[[Encyclopédie]]'' by [[Jean le Rond d'Alembert]] and [[Denis Diderot]].

The tree was a [[Taxonomy (general)|taxonomy]] of human knowledge, inspired by [[Francis Bacon]]'s ''[[The Advancement of Learning]]''. The three main branches of knowledge in the tree are: "Memory"/[[History]], "Reason"/[[Philosophy]], and "Imagination"/[[Poetry]].

Notable is the fact that [[theology]] is ordered under 'Philosophy'. The historian [[Robert Darnton]] has argued that this categorization of [[religion]] as being subject to human reason, and not a source of knowledge in and of itself ([[revelation]]), was a significant factor in the controversy surrounding the work.<ref>Robert Darnton, "Philosophers Trim the Tree of Knowledge: The Epistemological Strategy of the ''Encyclopedie''," ''The Great Cat Massacre and Other Episodes in French Cultural History'' (New York: Basic Books, Inc., 1984), 191-213.</ref>  Additionally notice that 'Knowledge of God' is only a few nodes away from 'Divination' and 'Black Magic'.

The original version, in [[French (language)|French]], can be seen in the graphic on the right. An [http://www.hti.umich.edu/d/did/tree.html image of the diagram with English translations superimposed over the French text] is available. Another example of English translation of the tree is available in literature (see the reference by Schwab). Below is a version of it rendered in [[English (language)|English]] as a bulleted outline.

== ''The Tree of Diderot and d'Alembert'' ==
'''"Detailed System of Human Knowledge"'''
from the [[Encyclopédie]].
* [[Understanding]]
:* [[Memory]].
::* [[History]].
:::* [[Sacred history|Sacred]] (History of [[Prophet]]s).
:::* [[History of Christianity|Ecclesiastical]].
:::* [[Civilization#History|Civil]], [[Ancient history|Ancient]] and [[Modern history|Modern]].
::::* [[Civilization#History|Civil History]], properly said. ''(See also: [[Civil society#History|History of civil society]])''
::::* [[Literary History]].
:::::* [[Memoirs]].
:::::* [[Antiquities]]. ''(See also: [[Classical antiquity]])''
:::::* Complete Histories.
:::* [[Natural history|Natural]].
::::* Uniformity of Nature. ''(See: [[Uniformitarianism]])''
:::::* [[Cosmology|Celestial History]].
:::::* History...
::::::* of [[Meteoroid#History|Meteors]].
::::::* of the [[History of the Earth|Earth]] and the [[World Ocean|Sea]] ''(See also: [[Origin of water on Earth]])''
::::::* of [[Minerals]]. ''(See also:  [[Geological history of Earth]])''
::::::* of [[Vegetable]]s. ''(See also: [[History of agriculture]])''
::::::* of [[Animal]]s.  ''(See also: [[Evolutionary history of life]])''
::::::* of the [[Chemical element|Elements]]. ''(See also: [[Classical element]], [[History of alchemy]], and [[History of chemistry]])''
::::* Deviations of Nature.
:::::* [[Celestial object|Celestial Wonders]].
:::::* [[Meteoroid#Frequency of large meteors|Large Meteors]]. ''(See also: [[Asteroid]]s)''
:::::* Wonders of Land and Sea. ''(See: [[Wonders of the World]])''
:::::* [[Mineral#Other properties|Monstrous Mineral]]s.
:::::* Monstrous Vegetables. ''(See: [[Largest organisms#Plants|Largest plants]], [[Poisonous plant]]s, and [[Carnivorous plant]]s)''
:::::* Monstrous Animals. (See: ''[[Largest organisms#Animals|Largest animals]] and [[Predator]]s)''
:::::* Wonders of the Elements. ''(See: [[Natural disaster]]s)''
::::* Uses of Nature (See ''[[Technology]] and [[Applied science]]s)''
:::::* Arts, [[Craft]]s, Manufactures.
::::::* Work and Uses of [[Gold]] and [[Silver]].
:::::::* [[Mint (coin)|Minting]].
:::::::* [[Goldsmith]].
:::::::* Gold Spinning.
:::::::* Gold Drawing.
:::::::* [[Silversmith]]
:::::::* [[Planishing|Planisher]], etc.
::::::* Work and Uses of Precious Stones.
:::::::* [[Lapidary]].
:::::::* [[Diamond cutting]].
:::::::* [[Jewellery|Jeweler]], etc.
::::::* Work and Uses of [[Iron]].
:::::::* Large [[Forge|Forges]].
:::::::* [[Locksmithing|Locksmith]].
:::::::* Tool Making.
:::::::* Armorer.
:::::::* Gun Making, etc.
::::::* Work and Uses of [[Glass]].
:::::::* [[Glass|Glassmaking]].
:::::::* [[Plate-Glass|Plate-Glassmaking]].
:::::::* [[Mirror#Manufacture|Mirror Making]].
:::::::* [[Optician]].
:::::::* [[Glazier]], etc.
::::::* Work and Uses of Skin.
:::::::* [[Tanner (occupation)|Tanner]].
:::::::* [[Chamois leather|Chamois Maker]].
:::::::* Leather Merchant.
:::::::* [[Glove]] Making, etc.
::::::* Work and Uses of [[Stonemasonry|Stone]], [[Plaster#Uses|Plaster]], [[Slate#Uses|Slate]], etc.
:::::::* Practical [[Architecture]].
:::::::* Practical [[Sculpture]].
:::::::* [[Masonry|Mason]].
:::::::* [[Tiler]], etc.
::::::* Work and Uses of [[Silk#Uses|Silk]].
:::::::* Spinning.
:::::::* Milling.
:::::::* Work like.
:::::::* [[Velvet]].
:::::::* Brocaded Fabrics, etc.
::::::* Work and Uses of [[Wool]].
:::::::* Cloth-Making.
:::::::* Bonnet-Making, etc.
::::::* Working and Uses, etc.
:* [[Reason]]
::* [[Philosophy]]
:::* General [[Metaphysics]], or [[Ontology]], or Science of Being in General, of Possibility, of [[Existence]], of Duration, etc.
:::* Science of [[God]].
::::* [[Natural Theology]].
::::* Revealed [[Theology]].
::::* Science of Good and Evil Spirits.
:::::* [[Divination]].
:::::* [[Black Magic]].
:::* Science of Man.
::::* [[Pneumatology]] or Science of the [[Soul]].
:::::* Reasonable.
:::::* Sensible.
::::* [[Logic]].
:::::* Art of [[Outline of thought|Thinking]].
::::::* [[Apprehension (understanding)|Apprehension]].
:::::::* Science of [[Idea]]s
::::::* [[Judgement]].
:::::::* Science of [[Proposition]]s.
::::::* [[Reasoning]].
:::::::* [[Inductive reasoning|Induction]].
::::::* [[Reasoning#Logical_reasoning_methods_and_argumentation|Method]].
:::::::* Demonstration.
::::::::* [[Analysis]].
::::::::* [[:wikt:-synthesis|Synthesis]].
:::::* Art of Remembering.
::::::* [[Memory]].
:::::::* Natural.
:::::::* [[Art of memory|Artificial]].
::::::::* Prenotion.
::::::::* Emblem.
::::::* Supplement to Memory.
:::::::* [[Writing]].
:::::::* [[Printing]].
::::::::* [[Alphabet]].
::::::::* Cipher.
:::::::::* Arts of [[Writing]], Printing, [[Reading (process)|Reading]], Deciphering.
::::::::::* [[Orthography]].
:::::* Art of [[Communication]]
::::::* Science of the Instrument of [[Discourse]].
:::::::* [[Grammar]].
::::::::* [[Sign]]s.
:::::::::* [[Gesture]].
::::::::::* [[Mime|Pantomime]].
::::::::::* Declamation.
:::::::::* Characters.
::::::::::* [[Ideogram]]s.
::::::::::* [[Hieroglyphics]].
::::::::::* [[Heraldry]] or Blazonry.
::::::::* [[Prosody (linguistics)|Prosody]].
::::::::* Construction.
::::::::* [[Syntax]].
::::::::* [[Philology]].
::::::::* Critique.
:::::::* [[Pedagogy]].
::::::::* [[Curriculum|Choice of Studies]].
::::::::* [[Teaching method|Manner of Teaching]].
::::::* Science of Qualities of [[Discourse]].
:::::::* [[Rhetoric]].
:::::::* Mechanics of [[Poetry]].
::::* [[Outline of ethics|Ethics]].
:::::* [[Contemporary ethics|General]].
::::::* General Science of [[Good and evil|Good and Evil]], of duties in general, of [[Virtue]], of the necessity of being Virtuous, etc.
:::::* [[Outline of ethics#Branches of ethics|Particular]].
::::::* Science of [[Law]]s or [[Jurisprudence]].
:::::::* [[Natural law|Natural]].
:::::::* [[Economic forces|Economic]]. ''(See also [[commercial law]])''
:::::::* [[Politics|Political]]. ''(See also [[political law]])''
::::::::* [[Domestic politics|Internal]] and [[International politics|External]]. ''(See also [[foreign policy]])''
::::::::* [[Commerce]] on Land and [[Maritime industry|Sea]].
:::* [[Natural science|Science of Nature]]
::::* [[Metaphysics]] of Bodies or, General Physics, of Extent, of Impenetrability, of Movement, of Word, etc.
::::* [[Outline of mathematics|Mathematics]].
:::::* [[Pure mathematics|Pure]].
::::::* [[Outline of arithmetic|Arithmetic]].
:::::::* [[Number|Numeric]].
:::::::* [[Algebra]].
::::::::* [[Elementary algebra|Elementary]].
::::::::* [[Infinitesimal]].
:::::::::* [[Differential algebra|Differential]].
:::::::::* [[Integral]].
::::::* [[Outline of geometry|Geometry]].
:::::::* Elementary (Military Architecture, Tactics).
:::::::* Transcendental (Theory of Courses).
:::::* Mixed.
::::::* [[Mechanics]].
::::::::* [[Statics]].
:::::::::* Statics, properly said.
:::::::::* [[Hydrostatics]].
::::::::* [[Dynamics (mechanics)|Dynamics]].
:::::::::* Dynamics, properly said.
:::::::::* [[Ballistics]].
:::::::::* [[Hydrodynamics]].
::::::::::* [[Hydraulics]].
::::::::::* [[Navigation]], Naval Architecture.
::::::* Geometric [[Astronomy]].
:::::::* [[Cosmography]].
::::::::* [[Celestial cartography|Uranography]].
::::::::* [[Geography]].
::::::::* [[Hydrography]].
:::::::* [[Chronology]].
:::::::* [[Gnomon]]ics.
::::::* [[Optics]].
:::::::* Optics, properly said.
:::::::* [[Dioptrics]], Perspective.
:::::::* [[Catoptrics]].
::::::* [[Acoustics]].
::::::* [[Pneumatics]].
::::::* Art of Conjecture. [[probability|Analysis of Chance]].
:::::* Physicomathematics.
::::* Particular Physics.
:::::* [[Outline of zoology|Zoology]].
::::::* [[Anatomy]].
:::::::* Simple.
:::::::* [[Comparative anatomy|Comparative]].
::::::* [[Physiology]].
::::::* [[Outline of medicine|Medicine]].
:::::::* Hygiene.
::::::::* [[Hygiene]], properly said.
::::::::* Cosmetics (Orthopedics).
::::::::* Athletics (Gymnastics).
:::::::* Pathology.
:::::::* Semiotics.
:::::::* Treatment.
::::::::* Diete.
::::::::* [[Surgery]].
::::::::* Pharmacy.
::::::* [[Veterinary medicine|Veterinary Medicine]].
::::::* [[Horse care|Horse Management]].
::::::* [[Hunting]].
::::::* [[Outline of fishing|Fishing]].
::::::* [[Falconry]].
:::::* Physical [[Astronomy]].
::::::* [[Astrology]].
:::::::* Judiciary Astrology.
:::::::* Physical Astrology.
:::::* [[Meteorology]].
:::::* [[Cosmology]].
::::::* Uranology.
::::::* [[Aerology]].
::::::* [[Geology]].
::::::* [[Hydrology]].
:::::* [[Botany]].
::::::* [[Agriculture]].
::::::* [[Gardening]].
:::::* [[Mineralogy]].
:::::* [[Chemistry]].
::::::* Chemistry, properly said, ([[Pyrotechnics]], Dyeing, etc.).
::::::* [[Metallurgy]].
::::::* [[Alchemy]].
::::::* Natural Magic.
:* Imagination.
::* [[Poetry]].
:::* Sacred, Profane.
::::* Narrative.
:::::* [[Epic poetry|Epic Poem]]
:::::* [[Madrigal (poetry)|Madrigal]]
:::::* [[Epigram]]
:::::* [[Novel]], etc.
::::* [[Drama|Dramatic]]
:::::* [[Tragedy]]
:::::* [[Comedy]]
:::::* [[Pastoral]], etc.
::::* Parable
:::::* [[Allegory]]
(NOTE: THIS NEXT BRANCH SEEMS TO BELONG TO BOTH THE NARRATIVE AND DRAMATIC TREE AS DEPICTED BY THE LINE DRAWN CONNECTING THE TWO.)
::::* [[Outline of music|Music]]
:::::* [[Music theory|Theoretical]]
:::::* Practical ''(see also [[musical technique]])''
:::::** [[Instrumental]]
:::::** [[vocal music|Vocal]]
::::* [[Outline of painting|Painting]]
::::* [[Outline of sculpture|Sculpture]]
::::* [[Engraving]]

== See also ==
* [[Classification chart]]
* [[Instauratio magna]]
* [[Propædia]]
* [[Pierre Mouchon]]

== References ==
{{reflist}}

== Further reading ==
* Robert Darnton, "Epistemological angst: From encyclopedism to advertising," in Tore Frängsmyr, ed., ''The structure of knowledge: classifications of science and learning since the Renaissance'' (Berkeley, CA: Office for the History of Science and Technology, University of California, Berkeley, 2001).
* Adams, David (2006) 'The Système figuré des Connaissances humaines and the structure of Knowledge in the Encyclopédie',  in Ordering the World, ed. Diana Donald and Frank O'Gorman, London: Macmillan, p.&nbsp;190-215. 
* ''Preliminary discourse to the Encyclopedia of Diderot'', Jean Le Rond d'Alembert, translated by Richard N. Schwab, 1995. ISBN 0-226-13476-8

==External links==

* [http://quod.lib.umich.edu/d/did/tree.html The ''Tree'' translated into English]
* [http://artfl.uchicago.edu/cactus/ ESSAI D'UNE DISTRIBUTION GÉNÉALOGIQUE DES SCIENCES ET DES ARTS PRINCIPAUX, published as a fold-out frontispiece in volume 1 of Pierre Mouchon, ''Table analytique et raisonnée des matieres contenues dans les XXXIII volumes in-folio du Dictionnaire des sciences, des arts et des métiers, et dans son supplément'', Paris, Panckoucke 1780.]

{{DEFAULTSORT:Figurative System Of Human Knowledge}}
[[Category:Taxonomy]]
[[Category:Age of Enlightenment]]
[[Category:Trees (data structures)]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:817
<=====title=====>:
FAO Country Profiles
<=====text=====>:
{{ infobox software
| name                   = FAO Country Profiles 
| logo                   = [[File:FAO countryprofiles logo.jpg]]
| caption                = 
| developer              = [[FAO]] of the [[United Nations]]
| latest_release_version = 2012
| latest_release_date    = 2001
| operating_system       = 
| genre                  = [[Knowledge Representation]], [[Ontology]] Editor
| website                = [http://www.fao.org/countryprofiles/ FAO Country Profiles]
}}

The [[FAO]] [[Country]] Profiles is a [[multilingual]]<ref>Arabic, Chinese, English, French, Russian and Spanish are the languages of the Organization. See FAO's Basic texts http://www.fao.org/docrep/010/k1713e/k1713e02b.htm#47. The FAO Country Profiles system provides information in Arabic, Chinese, English, French and Spanish. Russian is in preparation.</ref> [[web portal]] which repackages the [[Food and Agriculture Organization]] of the United Nations (FAO) vast archive of information on its global activities in [[agriculture]] and [[food security]] in a single area and catalogues it exclusively by [[country]] and thematic areas.

The portal's purpose is to offer decision-makers, [[researchers]] and project formulators around the world a fast and reliable way to access country-specific information on national [[food security]] situations without the need to search individual [[databases]] and [[systems]]. It gives added-value to [[FAO]]'s wealth of information by providing an easy-to-use [[User interface|interface]] containing [[interactive]] [[maps]] and [[charts]].<ref>For reviews of the [[FAO]] Country profiles initiatives, please see the [http://news.eoportal.org/didyouknow/080226_did2.html Sharing Earth and Observations Resources portal], [http://www.sciencecentral.com/category/962945 Science Central], [http://www.scinet.cc/dir/Science/Agriculture/ SciNet Science & Technology Search, News, Articles], etc.</ref>

== Background ==

[[FAO]] has always highlighted [[information]] and [[Knowledge sharing]] as priority areas in fighting [[hunger]] and achieving [[food security]].<ref>See ARTICLE I of FAO Constitution: The Organization shall collect, analyze, interpret, and disseminate information relating to nutrition, food and agriculture. http://www.fao.org/docrep/x5584E/x5584e0i.htm</ref> In this context, [[FAO]] identified that [[countries]] could improve their national programmes on [[agriculture]] and [[food security]] if they could access [[FAO]]'s information through a cross-sectoral (or [[interdisciplinary]]) country-based approach.<ref>Programme of Work and Budget 2002–2003:  http://www.fao.org/docrep/meeting/003/y1194e/y1194e06b.htm#P11324_311453</ref><ref>Programme of Work and Budget 2004–2005: http://www.fao.org/DOCREP/MEETING/006/y9859e/Y9859e07a.htm#P10820_371793</ref> However, despite the existence of a large number of country-based [[information systems]] in FAO, the information managed by the various systems lacked [[System integration|integration]]. Information tended to be generated and used in a circumscribed manner and tailored to a specific system, department or [[sector (economic)|sector]].

The [http://www.fao.org/countryprofiles/ FAO Country Profiles] portal, initially called FAO Country Profiles and Mapping Information System, was launched in 2002 responding to the Organization’s need to provide [[FAO]] web site’s users an easy to use mechanism to find FAO country-specific [[information]] without the need to [[Search engine technology|search]] individual [[FAO]] [[web sites]], [[databases]] or [[systems]]. The system was designed to integrate [[Scientific modelling|analytical]] and [[multilingual]] information with thematic databases and [[Digital data|digital]] [[map]] [[Disciplinary repository|repositories]] and to facilitate access to information on multiple factors contributing to national [[food insecurity]].

Since its launch, the system has grown by incorporating more and more [[FAO Country Profiles#Data sources|data sources]]. This was achieved thanks to a [[corporate]] effort to reduce [[information silo]]s and the adoption of [[international standards]] for country-based [[information management]] throughout the Organization.

== Country Profiles ==

The methodology behind the [[FAO]] Country Profiles is rather simple; it links, reuses and repackages data and information from most relevant existing [[FAO]] [[databases]] and [[systems]].

The [[FAO]] Country Profiles covers current FAO Members and Associated Nations.<ref>FAO membership as the 17 November 2007: http://www.fao.org/Legal/member-e.htm</ref> Once a country is selected, the portal presents to the user [[documents]], news feeds, [[statistical data]], project details and [[maps]] from relevant [[FAO Country Profiles#Data sources|FAO databases and systems]] for the selected [[country]] and categorized according to thematic areas.

The thematic areas are grouped in two categories:

* FAO Core Activities: these correspond to  [[FAO]]'s main areas of expertise, such as, [[natural resources]], [[economics]], [[agriculture]], [[forestry]], [[fisheries]] and technical cooperation. This grouping is based on the work of the corresponding [[FAO]] departments.<ref>For a list of FAO departments and divisions, please see http://www.fao.org/about/depart/en/</ref>
* Global issues: these are themes that [[FAO]] identified as priority areas for action, and include [[biodiversity]], [[biotechnology]], [[climate change]], [[diseases]] and [[Pest (organism)|pests]], [[emergency]] and aid, [[food security]] and [[food safety|safety]], [[trade]] and [[prices]], [[water management]]. These priority areas correspond to [[FAO]]'s strategic response to a fast-changing world where issues ranging from [[biotechnology]] to [[climate change]] and [[trade]] present new challenges and choices to governments and the general public.

===Data sources ===

Country pages provide access to or integrate the following thematic profiles and systems.<ref name="Inventory of resources">[http://www.fao.org/countryprofiles/resources.asp Inventory of data sources used in the FAO country profiles]</ref>

==== FAO data sources ====
* [http://www.fao.org/nr/water/aquastat/countries/index.stm Aquastat Country Profiles]: The AQUASTAT country profiles describe the state of [[water resources]] and [[agricultural]] [[water use]] in the respective country. Special attention is given to [[water resource]], [[irrigation]], and [[drainage]] sub-sectors.
* [http://www.fao.org/biotech/inventory_admin/dep/country_rep_search.asp?lang=en Biotechnology Country Profiles]: The objective of the profiles is to provide a platform on which [[developing country]] [[biotechnology]]-related [[policies]], [[regulations]] and activities can be readily accessed, directing the user to key, updated sources of information.
* [http://www.fao.org/biotech/inventory_admin/dep/default.asp?lang=en BIODEC Biotechnologies in Developing Countries]: FAO-BioDeC is a database meant to gather, store, organize and disseminate, updated baseline information on the state-of-the-art of [[crop]] [[biotechnology]] [[Product (business)|products]] and [[Scientific technique|techniques]], which are in use, or in the pipeline in [[developing countries]]. The database includes about 2000 entries from 70 [[developing countries]], including countries with [[economies in transition]].
* [http://www.fao.org/ag/AGP/AGPC/doc/Counprof/regions/index.htm Country Pasture/Forage Resource Profiles]: The Country [[Pasture]]/[[Forage]] Resource Profile provides a broad overview of relevant general, [[topographical]], [[climatic]] and [[agro-ecological]] information with focus on [[livestock]] production systems and the [[pasture]]/[[forage]] resources.
* [http://www.fao.org/documents FAO Corporate Document Repository]: The FAO Corporate Document Repository houses FAO documents and publications, as well as selected non-FAO publications, in electronic format.
* [http://www.fao.org/tc/tcom/index_en.htm FAO Projects in the country]: From the Field Programme Management Information System.
* [http://www.fao.org/faoterm/ FAO Terminology - Names of Countries]: In order to standardize and harmonize the vast quantity of terms used in FAO documents and publications, the Organization developed the [[terminology]] database [[FAOTERM]]. The Corporate NAMES OF COUNTRIES database also aims at facilitating the consultation and harmonization of country names throughout the Organization.
* [http://www.fao.org/fishery/countryprofiles/search/en Fisheries and Aquaculture Country Profiles]: FAO's [[Fisheries]] and [[Aquaculture]] Department prepares and publishes Fishery and Aquaculture Country Profiles. Each profile summarizes the Department's assessment of activities and trends in fisheries and aquaculture for the country concerned.  Economic and [[demographic data]] are based on [[UN]] or [[World Bank]] sources; data on fisheries are generally those published by the FAO Fisheries and Aquaculture Department.
* [http://www.fao.org/forestry/country/en/ Forestry Country Profiles]: The [[forestry]] country profiles provide detailed information on [[forests]] and the forest sector: [[forest cover]] (types, extent and change), [[forest management]], policies, products and trade, and more - in all some 30 pages for each country in the world.
* [http://www.fao.org/geonetwork/srv/en/main.home FAO-GeoNetwork]: FAO-GeoNetwork is a web-based Geographic Data and Information Management System. It enables easy access to local and distributed [[geospatial information]] catalogues and makes available data, graphics, documents for immediate download. FAO-GeoNetwork holds approximately 5000 standardized [[metadata]] records for digital and paper maps, most of them at the global, continent and national level.
* [http://www.fao.org/giews/english/index.htm Global Information and Early Warning System on Food and Agriculture (GIEWS)]: The System aims to provide policy-makers and policy-analysts with the most up-to-date information available on all aspects of [[food supply]] and demand, warning of imminent [[food crises]], so that timely interventions can be planned.
* [http://www.fao.org/ag/againfo/resources/en/pubs_sap.html Livestock Sector Briefs]: The purpose of the [[Livestock]] Sector Briefs is to provide a concise overview of livestock production in the selected countries through tables, maps and graphs.
* [http://www.fao.org/ag/agn/nutrition/profiles_en.stm Nutrition Country Profiles]: The [[Nutrition]] Country Profiles (NCP) provide concise analytical summaries describing the food and nutrition situation in individual countries.

==== Partnerships data sources ====
* [http://www.agrifeeds.org/ AgriFeeds]: AgriFeeds is a service that allows users to search and filter news and events from several agricultural information sources. It harvests, stores and re-aggregates news and events from feeds published by agricultural organizations and information services.
* [http://www.ipfsaph.org/En/default.jsp International Portal on Food Safety, Animal & Plant Health (IPFSAPH)]: IPFSAPH facilitates trade in food and agriculture by providing a single access point to authorized official international and national information across the sectors of food safety, animal and plant health.  It has been developed by FAO in association with the organizations responsible for international standard setting in sanitary and phytosanitary matters.

==== Non-FAO data sources ====
* [http://earthtrends.wri.org/gsearch.php?kw=country&action=results Earthtrends], [[World Resources Institute]]: EarthTrends is a comprehensive online database, maintained by the World Resources Institute, that focuses on the environmental, social, and economic trends that shape the world. The Earthtrends country profiles present environmental information about key variables for different topic areas.
* International Fund for Agricultural Development ([[IFAD]]): Rural poverty country profiles are produced by IFAD.

== Standards ==

[[File:Geopolitical Ontology in Country Profiles August 12 2009 v 1.png||thumb|200px|right| Geopolitical information section in the FAO Country Profiles.]]

There are various [[international standards]] and [[coding systems]] to manage country information. Historically, systems dealing with different types of data used different coding systems that were tailored to specific data type requirements. For example, [[statistical systems]] in the [[United Nations]] commonly use the M-49 classification and pigmentation<ref>Standard Country or Area Codes for Statistical Use http://unstats.un.org/unsd/methods/m49/m49.htm</ref> (also known as [[UN]] code) or the [[FAOSTAT]] area classification;<ref>FAOSTAT standardized list of country/territories and groupings: http://faostat.fao.org/site/441/default.aspx</ref> mapping systems could use [[geographic coordinates]] or [[Global Administrative Unit Layers (GAUL)|GAUL]] codes; textual systems (document repositories or web sites) could use [[ISO 3166-1 alpha-2]], [[ISO 3166-1 alpha-3]] or [[AGROVOC]] keywords; etc.

The FAO Country Profiles provide access to systems managing [[statistics]], [[documents]], [[maps]], [[news feeds]], etc., therefore one of its key aspects to succeed was the mapping of all these [[country codes]].

For this purpose a [[geopolitical ontology]] was developed.<ref>For linking country-based heterogeneous data at [[FAO]], please see:[http://www.semanticuniverse.com/articles-integrating-country-based-heterogeneous-data-united-nations-fao%E2%80%99s-geopolitical-ontology-and Integrating Country-based heterogeneous data at the United Nations: FAO's geopolitical ontology and services.]</ref> This ontology, among other features, maps [[ISO 3166-1 alpha-2|ISO2]], [[ISO 3166-1 alpha-3|ISO3]], [[AGROVOC]], [[Food and Agriculture Organization Corporate Statistical Database|FAOSTAT]], [http://www.fao.org/faoterm/index.asp?lang=EN FAOTERM], [[Global Administrative Unit Layers (GAUL)|GAUL]], [[UN]], and [[UNDP]] codes for all countries.

== Global Resources ==

Besides the profiles for each country the portal also provides access to other important global resources, such as:

=== Low-Income Food Deficit Countries (LIFDC) ===
The FAO Country Profiles keeps updated for the public the list of [[LIFDC]] countries. This list is revised every year according to the methodology explained below. The new list of the LIFDC,<ref>For an updated list of Low-Income Food Deficit Countries, please check this page: http://www.fao.org/countryprofiles/lifdc/en/</ref> stands at 62 countries, four less than in the (2012) list. These are: [[Georgia (country)|Georgia]], [[Syrian Arab Republic]], [[Timor-Leste]], [[Republic of Moldova]]. While [[Moldova]] graduated from the list on the basis of net food-exporter criterion, the other graduated based on income criterion.

==== LIFDC methodology====

The classification of a country as low-income food-deficit used for analytical purposes by [[FAO]] is traditionally determined by three criteria:

# A country should have a [[per capita income]] below the "historical" ceiling used by the [[World Bank]]<ref>For operational and analytical purposes, the World Bank’s main criterion for classifying economies is gross national income (GNI) per capita. Classifications are set each year on 1 July. These official analytical classifications are fixed during the World Bank's fiscal year (ending on 30 June), thus countries remain in the categories in which they are classified irrespective of any revisions to their per capita income data. (Source: [[The World Bank]])</ref> to determine eligibility for [[International Development Association|IDA]] assistance and for 20-year [[IBRD]] terms, applied to countries included in the World Bank categories I and II.<ref>Several important distinctions among member countries are commonly used at the World Bank Group. Countries choose whether they are part of Part I or Part II primarily on the basis of their economic standing. Part I are almost all industrial countries and donors to IDA and they pay their contributions in freely convertible currency. Part II countries are almost all developing countries, some of which are donors to IDA. Part II countries are entitled to pay most of their contribution to IDA in local currency. Please see: "A Guide to the World Bank Group", The World Bank, 2003</ref> For instance, the historical ceiling of per capita [[gross national income]] (GNI) for 2006, based on the World Bank Atlas method,<ref>Please see: [http://web.worldbank.org/WBSITE/EXTERNAL/DATASTATISTICS/0,,contentMDK:20452009~menuPK:64133156~pagePK:64133150~piPK:64133175~theSitePK:239419~isCURL:Y~isCURL:Y,00.html The World Bank Atlas Method]</ref> was US$1,735, i.e. higher than the level established for 2005 ($1,675).
# The net food [[trade]]<ref>Net food trade refers to the gross imports less gross exports of food</ref> position of a country averaged over the preceding three years for which statistics are available, in this case from 2003 to 2005. Trade volumes for a broad basket of basic foodstuffs ([[cereals]], [[root]]s and [[tubers]], [[pulses]], [[oilseeds]] and oils other than tree crop oils, [[meat]] and [[dairy products]]) are converted and aggregated by the [[calorie]] content of individual [[commodities]].
# A self-exclusion criterion is applied when countries that meet the above two criteria specifically request FAO to be excluded from the LIFDC category.

In order to avoid countries changing their LIFDC status too frequently - typically due to short-term, [[exogenous]] shocks - an additional factor was introduced in 2001. This factor, called "persistence of position", would postpone the "exit" of a LIFDC from the list, despite the country not meeting the LIFDC [[income]] criterion or the [[food-deficit]] criterion, until the change in its status is verified for three consecutive years.<ref>For a list of countries and economies sorted by their gross domestic product (GDP) at purchasing power parity (PPP) per capita, please see [[List of countries by GDP (PPP) per capita]]</ref>

=== FAO Member Countries and Flags ===

The FAO Country Profiles is FAO's source for dissemination of [[FAO]]'s Member Nations and Associated Nations<ref>The list of FAO member countries and date of entry is available at: http://www.fao.org/Legal/member-e.htm</ref> official flags.<ref>The list of FAO member countries and flags is available at http://www.fao.org/countryprofiles/flags/</ref> The update of any [[country flag]] is coordinated with the other [[United Nations]] agencies. All flags are made available in a standardized manner which also aims to help web site owners to ensure that they always display the official country flag.

The standard URL for any given country flag would be composed by: the generic URL: "http://www.fao.org/countryprofiles/flags/" to which the [[ISO 3166-1 alpha-2|ISO 3166-1 Alpha-2]] code for the country is added, plus the image format suffix ".gif". For instance, the URL for the [[Argentine flag|Argentina flag]] would be: http://www.fao.org/countryprofiles/flags/AR.gif, with AR being the [[ISO 3166-1 alpha-2]] code of [[Argentina]].<ref>One of several international coding systems (some of the others being: [[ISO2]], [[ISO3]], [[AGROVOC]], [[FAOSTAT]], [[FAOTERM]], [[GAUL]], [[UN]], and [[UNDP]]) for territories and groups.</ref>

== Criticism ==

Early criticism of the [[FAO]] Country Profiles was that, in its inception phase, it only contained very few resources. Since 2002, the number of available resources has increased to cover country-based information and data, directly linked from [[FAO]]'s web pages or [[FAO]]'s digital repositories.<ref name="Inventory of resources"/> Over the last years, another identified area for improvement was the simplicity of the system methodology, being the resources only linked from country pages and thus, lacking real integration. This need was addressed by starting to integrate additional data, such as, the fisheries charts or the news and events items taken from [[AgriFeeds]]. In addition, in order to provide more complete country profiles, the system started to link or integrate  non-[[FAO]] resources.

== See also ==
* [[Agricultural Information Management Standards]]
* [[AGROVOC]]
* [[Country codes]]
* [[Food and Agriculture Organization]]
* [[Forestry Information Centre]]
* [[Geopolitical ontology]]

== References ==
{{reflist|33em}}

==External links==
* [http://www.fao.org/countryprofiles/default.asp?lang=en FAO Country Profiles]
* [http://www.fao.org/Legal/member-e.htm FAO membership]
* [http://www.fao.org/countryprofiles/lifdc.asp?lang=en Low-Income Food-Deficit Countries (LIFDC)]
* [http://www.fao.org/sids/index_en.asp Small Island Developing States (SIDS)]

{{DEFAULTSORT:Fao Country Profiles}}
[[Category:Agriculture]]
[[Category:Agriculture by country| FAO]]
[[Category:Knowledge representation]]
[[Category:Information systems]]
[[Category:Food and Agriculture Organization]]
[[Category:Country codes]]
<=====doc_Id=====>:820
<=====title=====>:
VoID
<=====text=====>:
{{Other uses|Void (disambiguation)}}
{{Multiple issues|
{{Expert-subject|Internet|date=November 2008}}
{{Technical|date=November 2008}}
{{Unreferenced|date=December 2008}}
{{Orphan|date=February 2009}}
{{Underlinked|date=November 2013}}
}}

The '''Vocabulary of Interlinked Datasets''' ('''VoID''') is an [[Resource Description Framework|RDF]] vocabulary, and a set of instructions, that enables the discovery and usage of [[Linked Data|linked data]] sets. A linked dataset is a collection of data, published and maintained by a single provider, available as RDF on the Web, where at least some of the resources in the dataset are identified by dereferencable URIs.

==References==
{{Reflist}}

==External links==
* [http://www.w3.org/TR/void/ Describing Linked Datasets with the VoID Vocabulary, W3C TR]
* [http://semanticweb.org/wiki/VoID VoID at Semantic Web Wiki]

{{Semantic Web}}

{{DEFAULTSORT:Void}}
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Knowledge representation]]
[[Category:XML-based standards]]
<=====doc_Id=====>:823
<=====title=====>:
Integrated Operations in the High North
<=====text=====>:
{{Infobox Organization
|name         = Integrated Operations in the High North
|image        = IOHN logo small.gif
|size         = 200
|alt          = Logo for Integrated Operations in the High North.
|caption      = Logo for Integrated Operations in the High North.
|abbreviation = IOHN or IO High North
|formation    = 2008-05-06
|status       = Project at [[Det Norske Veritas|Det Norske Veritas (DNV)]]
|purpose      = Designing, implementing and testing a Digital Platform for the next generation of [[Integrated Operations]]
|location     = Bærum, Norway
|region_served = Worldwide
|membership   = 22
|language     = English
|leader_title = Project Manager
|leader_name  = [http://www.linkedin.com/in/fredericverhelst Frédéric Verhelst]
|main_organ   = Steering Committee
|affiliations = <!-- if any -->
|num_staff    = 
|num_volunteers =
|budget       = 
|website      = http://www.IOHN.org/
}}
'''Integrated Operations in the High North''' ('''IOHN''', '''IO High North or IO in the High North''') is a unique collaboration project that during a four-year period starting May 2008 is working on designing, implementing and testing a Digital Platform for what in the [[Upstream (oil industry)|Upstream Oil and Gas Industry]] is called the next or second generation of [[Integrated Operations]].<ref>
{{cite web 
|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf 
|title=Integrated Operations and the Oil and Gas Ontology 
|author=The [[Norwegian Oil Industry Association]] (OLF) and POSC Caesar Association (PCA) 
|accessdate=2009-05-06 
|date=2007-09-19
}}</ref> 
The work on the Digital platform is focussed on capture, transfer and integration of [[Real-time data]] from the remote production installations to the decision makers. A risk evaluation across the whole chain is also included. The platform is based on [[open standards]] and enables a higher degree of [[interoperability]]. Requirements for the digital platform come from use cases defined within the [[Oil_and_gas_well_drilling#Drilling|Drilling]] and [[Oil_and_gas_well_drilling#Completion|Completion]], Reservoir and Production and Operations and Maintenance domains. The platform will subsequently be demonstrated through pilots within these three domains.<ref name="IOHNsite">
{{cite web 
|url=http://trac.posccaesar.org/wiki/IOHN
|title=Short introduction to the Integrated Operations in the High North (IOHN) project
|author=Integrated Operations in the High North (IOHN) 
|accessdate=2009-05-07 
}}</ref> 

This new platform is considered an important enabler for safe and sustainable operations in remote, vulnerable and hazardous areas such as the [[Arctic|High North]],<ref>
{{cite web 
|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html 
|title=Norway takes a leading role in next generation Integrated Operations 
|author=The [[Norwegian Oil Industry Association]] (OLF) 
|accessdate=2009-05-07 
|date=2008-08-26
}}</ref><ref name="Rigzone">
{{cite web 
|url=http://www.rigzone.com/news/article.asp?a_id=65883 
|title=Norway Takes Reign to Provide Next Generation Integrated Operations 
|author=Rigzone E&P News
|accessdate=2009-05-08 
|date=2008-08-26
}}</ref><ref name="DEJ">
{{cite web 
|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7
|title=Norway developing next generation Integrated Operations 
|author=Digital Energy Journal
|accessdate=2009-05-08 
|date=2008-08-27
}}</ref><ref name="EPMag">
{{cite web 
|url=http://www.epmag.com/Magazine/2008/12/item24047.php 
|title=Offshore R&D pushes the limits 
|author=E&P Magazine
|accessdate=2009-05-08 
|date=2008-12-02
}}</ref> but the technology is clearly also applicable in more general applications.

The IOHN project consortium consists of 23 participants,<ref name="IOHNmembers">
{{cite web 
|url=http://www.posccaesar.org/wiki/IOHN/participants 
|title=List of participating companies in the IOHN project 
|author=[[IOHN]] 
|accessdate=2010-03-10 
|date=2010-03-10
}}</ref> including operators, service providers, software vendors, technology providers, research institutions and universities. In addition, the [[Norwegian Defence Force]] is working with the project to resolve common infrastructural and [[interoperability]] challenges.<ref name="IOHNsite"/>

The project is managed by [[DNV|Det Norske Veritas (DNV)]].<ref>
{{cite web 
|url=http://www.dnv.com/news_events/news/2008/dnvleadsintegratedoperationsdevelopment.asp
|title=DNV leads Integrated Operations development 
|author=[[Det Norske Veritas]] (DNV) 
|accessdate=2009-05-07 
|date=2008-08-26
}}</ref> Nils Sandsmark was the project manager during the initiation and start-up phase. Frédéric Verhelst took over as project manager from the beginning of 2009.<ref>
{{cite web 
|url=http://www.linkedin.com/in/fredericverhelst
|title=Profile of Frédéric Verhelst 
|author=[[LinkedIn]]
|accessdate=2009-09-28 
}}</ref>

Financing comes from the participants and the [[Research Council of Norway]] (RCN) for parts of the project (GOICT<ref>
{{cite web 
|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&cid=1207296035860&pagename=verdikt/Hovedsidemal&p=1226993814962 
|title=Dependable ICT for the Energy Sector (GOICT, RCN proj.no. 183235, VERDIKT-programme)
|author=The [[Research Council of Norway]] (RCN) 
|accessdate=2009-05-07 
}}</ref>
and AutoConRig<ref>
{{cite web 
|url=http://www.forskningsradet.no/servlet/Satellite?c=Prosjekt&cid=1198060412649&pagename=ForskningsradetNorsk/Hovedsidemal&p=1181730334233 
|title=Semi-autonomous control system for unmanned drilling rigs (AutoConRig, RCN proj.no. 187473, PETROMAKS-programme)
|author=The [[Research Council of Norway]] (RCN) 
|accessdate=2009-05-07 
}}</ref><ref>
{{cite web 
|url=http://www.posccaesar.org/wiki/IOHN/AutoConRig 
|title=RCN/NFR project "AutoConRig"
|author=Jens Ornæs (NOV)
|accessdate=2009-07-02 
}}</ref>).

== Participants ==
The consortium consists of the following 22 participants<ref name="IOHNmembers"/> (in alphabetical order):<br />
{| class="wikitable"
|-
| [[ABB Group|ABB]]
| [http://www.abelia.no Abelia]
| [[Baker Hughes]]
| [[Cisco]]
| [http://www.computas.no Computas]
|-
| [[Det Norske Veritas]]
| [[Eni|ENI]]
| [http://www.epsis.no Epsis]
| [[FMC Technologies]]
| [http://www.fsi.no FSI]
|-
| [http://www.ntnu.no/iocenter IO Center]
| [http://www.iris.no IRIS]
| [[National Oilwell Varco]]
| [[Norwegian University of Science and Technology|NTNU]]
| [http://www.olf.no OLF]
|-
| [[POSC Caesar Association]]
| [http://www.ptil.no Petroleum Safety Authority Norway]
| [[Siemens]]
| [[Statoil]]
| [[Norwegian Defence]]
|-
| [[University of Oslo]]
| [[University of Stavanger]]
|}

== See also ==
* [[Integrated Operations]]
* [[Semantic Web]]
* [[ISO 15926]] aka [[Oil and Gas Ontology]], an enabler for the next or second generation of [[Integrated Operations]] by integrating data across disciplines and business domains.
* [[Petroleum exploration in the Arctic]]
* [[POSC Caesar Association]], the custodian of [[ISO 15926]], the [[Oil and Gas Ontology]].

== References ==
{{reflist|2}}

== External links ==
* [http://www.IOHN.org/ Integrated Operations in the High North] website
* [[W3C]] workshop on [http://www.w3.org/2008/12/ogws-report.html Semantic Web in Oil and Gas industry], Houston, December 9–10, 2008. [http://www.w3.org/2008/12/ogws-report#papers Position papers] from several participants in IOHN.
* [http://www.posccaesar.org/wiki/PCA/SemanticDays2009/AboutSemanticDays Semantic Days 2009] conference, Stavanger, May 18–20, 2009. One [http://www.posccaesar.org/wiki/PCA/SemanticDays2009#Session6:SemantictechnologyforIOGeneration2 session] is devoted to IOHN.
* [http://www.ioconf.no/2009/ IO 09 Science and Practice] conference, Trondheim, September 29–30, 2009. One [http://ioconf.no/2009/parallel6 session] is devoted to IOHN.
* [http://www.oilit.com/2journal/2article/1003_16.htm#IOHN Integrated Operations in the High North—mid term report], ''Oil IT Journal'', March 2010.

{{DEFAULTSORT:Integrated Operations In The High North}}
[[Category:Petroleum organizations]]
[[Category:Petroleum engineering]]
[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:826
<=====title=====>:
Logico-linguistic modeling
<=====text=====>:
'''Logico-linguistic modeling''' is a method for building knowledge-based systems with a learning capability using [[Conceptual model|Conceptual Models]] from [[Soft systems methods]], modal predicate logic and the Prolog artificial intelligence language.

== Overview==
Logico-linguistic modeling is a six stage method developed primarily for building [[knowledge-based systems]] (KBS), but it also has application in manual decision support systems and information source analysis. Logico-linguistic models have a superficial similarity to Sowa's<ref>Sowa, John F. (1984), ''Conceptual Structures:  Information Processing in Mind and Machine'', Addison-Wesley, Reading, MA, USA.</ref> [[Conceptual Graphs]], both use bubble style diagrams, both are concerned with concepts, both can be expressed in logic and both can be used in artificial intelligence. However, logico-linguistic models are very different in both logical form and in their method of construction.
 
Logico-linguistic modeling was developed in order to solve theoretical problems found in the Soft Systems method for information system design. The main thrust of the research into has been to show how [[Soft Systems Methodology]] (SSM), a method of systems analysis, can be extended into artificial intelligence.

== Background ==

SSM employs three modeling devices i.e. rich pictures, root definitions, and Conceptual Models of human activity systems. The root definitions and conceptual models are built by stakeholders themselves in an iterative debate organized by a facilitator. The strengths of this method lie, firstly, in its flexibility, the fact that it can address any problem situation, and, secondly, in the fact that the solution belongs to the people in the organization and is not imposed by an outside analyst.<ref name =  "source">Gregory, Frank Hutson and Lau, Sui Pong (1999) [http://logicalgregory.jimdo.com/publications/logical-ssm-for-isa/ Logical Soft Systems Modelling for Information Source Analysis - The Case of Hong Kong Telecom], Journal of the Operational Research Society, vol. 50 (2).</ref>

Information Requirements Analysis (IRA)<ref name="Wilson">Wilson, Brian ''Systems: Concepts, Methodologies and Applications'', John Wiley & Sons Ltd. 1984, 1990. ISBN 0-471-92716-3</ref>  took the basic SSM method a stage further and showed how the Conceptual Models could be developed into a detailed information system design. IRA calls for the addition of two modeling devices: "Information Categories" which show the required information inputs and outputs from the activities identified in an expanded conceptual model; and the "Maltese Cross" a matrix which shows the inputs and outputs from the information categories and shows where new information processing procedures are required. A completed Maltese Cross is sufficient for the detailed design of a transaction processing system.

The initial impetus to the development of logico-linguistic modeling was a concern with the theoretical problem of how an information system can have a connection to the physical world.<ref>Gregory, Frank Hutson (1995) [[s:Mapping Information Systems onto the Real World|Mapping Information Systems onto the Real World]]. Working Paper Series No. WP95/01. Dept. of Information Systems, City University of Hong Kong.</ref> This is a problem in both IRA and more established methods (such as [[SSADM]]) because none base their information system design on models of the physical world. IRA designs are based on a notional conceptual model and SSADM is based on models of the movement of documents.

The solution to these problems provided a formula that was not limited to the design of transaction processing systems but could be used for the design of KBS with learning capability.<ref name="know">Gregory, Frank Hutson (1993) SSM for Knowledge Elicitation & Representation, Warwick Business School Research Paper No. 98 ({{ISSN|0265-5976}}). Later published as Soft Systems Models for Knowledge Elicitation and Representation in Journal of the Operational Research Society (1995) 46, 562-578.</ref>

== The Six Stages of logico-linguistic modeling==
[[File:Fig 1. SSM model abstracted from Wilson.jpg|thumb|Fig 1. SSM Conceptual Model]]
The logico-linguistic modeling method comprises six stages.<ref name="know"/>

=== 1. Systems Analysis ===

In the first stage logico-linguistic modeling uses SSM for [[systems analysis]]. This stage seeks to structure the problem in the client organization by identifying stakeholders, modelling organizational objectives and discussing possible solutions. At this stage it not assumed that a KBS will be a solution and logico-linguistic modeling often produces solutions that do not require a computerized KBS.

[[Expert systems]] tend to capture the expertise, of individuals in different organizations, on the same topic. By contrast a KBS, produced by logico-linguistic modeling, seeks to capture the expertise of individuals in the same organization on different topics. The emphasis is on the [[elicitation technique|elicitation]] of organizational or group knowledge rather than individual experts. In logico-linguistic modeling the stakeholders become the experts.

The end point of this stage is an SSM style conceptual models such as figure 1.

=== 2. Language Creation ===
[[File:Fig 2. Logico-linguistic Model.jpeg|thumb|Fig 2. Logico-linguistic Model]]

According to the theory behind logico-linguistic modeling the SSM conceptual model building process is a Wittgensteinian [[language-game]] in which the stakeholders build a language to describe the problem situation.<ref>Gregory, Frank Hutson (1992) [[s:SSM to Information Systems: A Wittengsteinian Approach|SSM to Information Systems: A Wittengsteinian Approach. Warwick Business School Research Paper No. 65.]] With revisions and additions this paper was published in Journal of Information Systems (1993) 3, pp.&nbsp;149–168.</ref> The logico-linguistic model expresses this language as a set of definitions, see figure 2.

=== 3. Knowledge Elicitation===
After the model of the language has been built putative knowledge about the real world can be added by the stakeholders. Traditional SSM conceptual models contain only one logical connective (a necessary condition). In order to represent causal sequences, “[[sufficient condition]]” and “[[necessary and sufficient condition|necessary & sufficient conditions]]” are also required.<ref name="cause2">Gregory, Frank Hutson (1992) [[s:Cause, Effect, Efficiency & Soft Systems Models|Cause, Effect, Efficiency & Soft Systems Models. Warwick Business School Research Paper No. 42]]. Later published in Journal of the Operational Research Society (1993) 44 (4), pp 149-168</ref> In logico-linguistic modeling this deficiency is remedied by two addition types of connective. The outcome of stage three is an empirical model, see figure 3.

=== 4. Knowledge Representation ===
[[File:Fig 3. Empirical Model.jpeg|thumb|Fig 3. Empirical Model]]

Modal predicate logic (a combination of [[modal logic]] and [[predicate logic]]) is used as the formal method of knowledge representation. The connectives from the language model are logically true (indicated by the “''L''” modal operator) and connective added at the knowledge elicitation stage are possibility true (indicated by the “''M''” modal operator). Before proceeding to stage 5, the models are expressed in logical formulae.

=== 5. Computer code ===

Formulae in predicate logic translate easily into the [[Prolog]] artificial intelligence language. The modality is expressed by two different types of Prolog rules. Rules taken from the language creation stage of  model building process are treated as incorrigible. While rules from the knowledge elicitation stage are marked as hypothetical rules. The system is not confined to decision support but has a built in learning capability.

=== 6. Verification ===

A knowledge based system built using this method verifies itself. [[Verification and Validation (software)|Verification]] takes place when the KBS is used by the clients. It is an ongoing process that continues throughout the life of the system. If the stakeholder beliefs about the real world are mistaken this will be brought out by the addition of Prolog facts that conflict with  the hypothetical rules. It operates in accordance to the classic principle of [[falsifiability]] found in the philosophy of science<ref>Gregory, Frank Hutson (1996) "The need for "Scientific" Information Systems" Proceedings of the Americas Conference on Information Systems, Aug 1996, Association for Information Systems, 1996. pp. 534-536.</ref>

== Applications ==
* '''Knowledge-based computer systems'''
Logico-linguistic modeling has been used to produce fully operational computerized knowledge based systems, such as one for the management of diabetes patients in a hospital out-patients department.<ref>Choi, Mei Yee Sarah (1997) Logico-linguistic Modelling for building a Diabetes Mellitus Patient Management Knowledge Based System. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.</ref>

*'''Manual decision support'''
In other projects the need to move into Prolog was considered unnecessary because the printed logico-linguistic models provided an easy to use guide to decision making. For example, a system for mortgage loan approval<ref>Lee, Kam Shing Clive (1997) The Development of a Knowledge Based System on Mortgage Loan Approval. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.</ref>

*'''Information source analysis'''
In some cases a KBS could not be built because the organization did not have all the knowledge needed to support all their activities. In these cases logico-linguistic modeling showed shortcomings in the supply of information and where more was needed. For example, a planning department in a telecoms company<ref name =  "source"/>

== Criticism ==
While logico-linguistic modeling overcomes the problems found in SSM's transition from conceptual model to computer code, it does so at the expense of increased stakeholder constructed model complexity. The benefits of this complexity are questionable<ref>Klein, J. H. (1994) Cognitive processes and operational research: a human information processing perspective. Journal of the Operational Research Society. Vol. 45, No. 8.</ref>
and this modeling method may be much harder to use than other methods.<ref>Klein, J. H. (1995) Over-simplistic cognitive science: A response.  Journal of the Operational Research Society. Vol. 46, No. 4. pp. 275-6.</ref>

This contention has been exemplified by subsequent research. An attempt by researchers to model buying decisions across twelve companies using logico-linguistic modeling required simplification of the models and removal of the modal elements.<ref>Nakswasdi, Suravut (2004) [http://arrow.unisa.edu.au:8080/vital/access/manager/Repository/unisa:44235 Logical Soft Systems for Modeling Industrial Machinery Buying Decisions in Thailand]. Doctor of Business Administration thesis, University of South Australia.</ref>

== References ==
{{Reflist}}

== Further reading ==
{{commons category}}
* Gregory, Frank Hutson  (1993) "[http://wrap.warwick.ac.uk/2888/ A logical analysis of soft systems modelling: implications for information system design and knowledge based system design]''. PhD thesis, University of Warwick.

[[Category:Knowledge representation]]
[[Category:Systems analysis]]
[[Category:Modal logic]]
<=====doc_Id=====>:829
<=====title=====>:
Visual hierarchy
<=====text=====>:
'''Visual hierarchy''' refers to the arrangement or presentation of elements in a way that implies importance.<ref>{{cite web|url = http://support.esri.com/en/knowledgebase/GISDictionary/term/visual%20hierarchy|title = GIS Dictionary|accessdate = 2014-08-13|publisher=ESRI}}</ref> In other words, visual hierarchy influences the order in which the human eye perceives what it sees. This order is created by the visual [[Contrast (vision)|contrast]] between forms in a field of perception. Objects with highest contrast to their surroundings are recognized first by the human mind. The term visual hierarchy is used most frequently in the discourse of the visual arts fields, notably so within the field of [[graphic design]].

==Theory==
The concept of visual hierarchy is based in [[Gestalt psychology|Gestalt psychological theory]], an early 20th-century German theory that proposes that the human brain has innate organizing tendencies that “structure individual elements, shapes or forms into a coherent, organized whole.” <ref>Jackson, Ian. “Gestalt—A  Learning Theory for Graphic Design Education.” ''International Journal of Art and Design Education''. Volume 27. Issue 1 (2008): 63-69. Digital.</ref> The German word Gestalt translates into “form,” “pattern,” or “shape” in English.<ref>Pettersson, Rune. “Information Design—Principles and Guidelines.” ''Journal of Visual Literacy''. Volume 29. Issue 2 (2010): 167-182. Digital.</ref> When an element in a visual field disconnects from the ‘whole’ created by the brain’s perceptual organization, it “stands out” to the viewer. The shapes that disconnect most severely from their surroundings stand out the most.

==Physical characteristics==
The brain disassociates objects from one another based upon the differences between their physical characteristics. These characteristics fall into four categories: color, size, alignment, and character. The category of color encompasses the [[hue]], [[Colorfulness|saturation]], [[Lightness (color)|value]], and perceived [[Texture (visual arts)|texture]] of forms. Size describes the surface area of a form. Alignment is the arrangement of forms with respect to their direction, orientation, or pattern.<ref>Feldsted, CJ. ''Design Fundamentals''. New York: Pittman Publishing Corporation, 1950.</ref> Character is the [[Rectilinear polygon|rectilinearity]] and [[Curvilinear coordinates|curvilinearity]] of forms. Forms that have differences in these characteristics contrast each other.

==Application==
{{Unreferenced section|date=February 2015}}
Visual hierarchy is an important concept in the field of [[graphic design]], a field that specializes in visual organization. Designers attempt to control visual hierarchy to guide the eye to information in a specific order for a specific purpose. One could compare visual hierarchy in graphic design to grammatical structure in writing in terms of the importance of each principle to these fields.

===Examples===
[[Fluorescence|Fluorescent]] color contrasts highly against most naturally occurring colors. Fluorescent substances achieve this contrast by emitting light. Forms of this type of color are almost always high in visual hierarchy. [[Tennis ball]]s are fluorescent green for the perceptual ease of players, match officials, and spectators.

[[Camouflage]] patterns diminish the contrast between themselves and their surroundings. Camouflage describes a form that mimics the physical characteristics of its environment. These patterns are difficult and sometimes impossible to perceive. Certain animals and military forces have both developed their own camouflaged patterns as mechanisms of defense.

==See also==
*[[Bauhaus]]
*[[Cognitive psychology]]
*[[Pattern recognition]]

==References==
{{Reflist}}

[[Category:Page layout]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:832
<=====title=====>:
Linear belief function
<=====text=====>:
{{Notability|date=January 2010}}

'''Linear belief function''' is an extension of the [[Dempster–Shafer theory]] of [[belief functions]] to the case when variables of interest are [[Continuous function|continuous]]. Examples of such variables include financial asset prices, portfolio performance, and other antecedent and consequent variables. The theory was originally proposed by [[Arthur P. Dempster]]<ref>A. P. Dempster, "Normal belief functions and the [[Kalman filter]]," in ''Data Analysis from Statistical Foundations'', A. K. M. E. Saleh, Ed.: Nova Science Publishers, 2001, pp. 65–84.</ref> in the context of Kalman Filters and later was reelaborated, refined, and applied to knowledge representation in artificial intelligence and decision making in finance and accounting by Liping Liu.<ref>Liu, Liping, Catherine Shenoy, and Prakash P. Shenoy, “Knowledge Representation and Integration for Portfolio Evaluation Using Linear Belief Functions,” IEEE Transactions on Systems, Man, and Cybernetics, Series A, vol. 36 (4), 2006, pp. 774–785.</ref>

== Concept ==

A linear belief function intends to represent our belief regarding the location of the true value as follows: We are certain that the truth is on a so-called certainty [[hyperplane]] but we do not know its exact location; along some dimensions of the certainty hyperplane, we believe the true value could be anywhere from –∞ to +∞ and the probability of being at a particular location is described by a [[normal distribution]]; along other dimensions, our knowledge is [[vacuous]], i.e., the true value is somewhere from –∞ to +∞ but the associated probability is unknown. A [[belief function]] in general is defined by a [[mass function]] over a class of [[focal elements]], which may have nonempty intersections. A linear belief function is a special type of [[belief function]] in the sense that its [[focal elements]] are exclusive, parallel sub-hyperplanes over the certainty hyperplane and its [[mass function]] is a [[normal distribution]] across the sub-hyperplanes.

Based on the above geometrical description, Shafer<ref>G. Shafer, "A note on Dempster's Gaussian belief functions," School of Business, University of Kansas, Lawrence, KS, Technical Report 1992.</ref> and Liu<ref>L. Liu, "A theory of Gaussian belief functions," ''International Journal of Approximate Reasoning'', vol. 14, pp. 95–126, 1996</ref>  propose two mathematical representations of a LBF: a wide-sense inner product and a linear functional in the variable space, and as their duals over a hyperplane in the sample space. Monney <ref>P. A. Monney, ''A Mathematical Theory of Arguments for Statistical Evidence''. New York, NY: Springer, 2003.</ref> proposes still another structure called Gaussian hints. Although these representations are mathematically neat, they tend to be unsuitable for knowledge representation in expert systems.

== Knowledge representation ==

A linear belief function can represent both logical and probabilistic knowledge for three types of variables: deterministic such as an observable or controllable, random whose distribution is normal, and vacuous on which no knowledge bears. Logical knowledge is represented by linear equations, or geometrically, a certainty hyperplane. Probabilistic knowledge is represented by a normal distribution across all parallel focal elements.

In general, assume X is a vector of multiple normal variables with mean μ and covariance Σ. Then, the multivariate normal distribution can be equivalently represented as a moment matrix:

: <math>
M(X) = \left( \begin{array}{*{20}c}
   \mu \\
   \Sigma
\end{array} \right).
</math>

If the distribution is non-degenerate, i.e., Σ has a full rank and its inverse exists, the moment matrix can be fully swept:

: <math> 
M(\vec X) = \left( \begin{array}{*{20}c}
   \mu \Sigma^{-1} \\
   -\Sigma^{-1}
\end{array} \right)
</math>

Except for normalization constant, the above equation completely determines the normal density function for ''X''. Therefore,  <math>M(\vec X)</math> represents the probability distribution of ''X'' in the potential form.

These two simple matrices allow us to represent three special cases of linear belief functions. First, for an ordinary normal probability distribution M(X) represents it. Second, suppose one makes a direct observation on X and obtains a value μ. In this case, since there is no uncertainty, both variance and covariance vanish, i.e., Σ = 0. Thus, a direct observation can be represented as:

: <math>M(X) = \left( \begin{array}{*{20}c}
   \mu \\
   0
\end{array} \right)
</math>

Third, suppose one is completely ignorant about X. This is a very thorny case in Bayesian statistics since the density function does not exist. By using the fully swept moment matrix, we represent the vacuous linear belief functions as a zero matrix in the swept form follows:

: <math>M(\vec X) = \left[ \begin{array}{*{20}c}
   0 \\
   0
\end{array} \right]
</math>

One way to understand the representation is to imagine complete ignorance as the limiting case when the variance of X approaches to ∞, where one can show that Σ<sup>−1</sup> = 0 and hence <math>M(\vec X)</math> vanishes. However, the above equation is not the same as an improper prior or normal distribution with infinite variance. In fact, it does not correspond to any unique probability distribution. For this reason, a better way is to understand the vacuous linear belief functions as the neutral element for combination (see later).

To represent the remaining three special cases, we need the concept of partial sweeping. Unlike a full sweeping, a partial sweeping is a transformation on a subset of variables. Suppose X and Y are two vectors of normal variables with the joint moment matrix:

: <math>M(X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   \mu_1 \\
   \Sigma _{11} \\
   \Sigma_{21}
\end{array} & \begin{array}{*{20}c}
   \mu_2 \\
   \Sigma _{12} \\
   \Sigma_{22}
\end{array}
\end{array} \right]</math>

Then M(X, Y) may be partially swept. For example, we can define the partial sweeping on X as follows:

: <math> M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   \mu _1 (\Sigma_{11})^{-1} \\
   -(\Sigma_{11})^{-1} \\
   \Sigma_{21} (\Sigma_{11})^{-1}
\end{array} & \begin{array}{*{20}c}
   \mu_2  - \mu_1 (\Sigma_{11} )^{-1} \Sigma_{12} \\
   (\Sigma_{11} )^{-1} \Sigma_{12} \\
   \Sigma_{22} - \Sigma_{21} (\Sigma_{11})^{-1} \Sigma_{12}
\end{array}
\end{array} \right]
</math>

If ''X'' is one-dimensional, a partial sweeping replaces the variance of ''X'' by its negative inverse and multiplies the inverse with other elements. If ''X'' is multidimensional, the operation involves the inverse of the covariance matrix of ''X'' and other multiplications. A swept matrix obtained from a partial sweeping on a subset of variables can be equivalently obtained by a sequence of partial sweepings on each individual variable in the subset and the order of the sequence does not matter. Similarly, a fully swept matrix is the result of partial sweepings on all variables.

We can make two observations. First, after the partial sweeping on&nbsp;''X'', the mean vector and covariance matrix of ''X'' are respectively <math> \mu_1 (\Sigma _{11} )^{-1} </math> and <math> -(\Sigma_{11} )^{-1} </math>, which are the same as that of a full sweeping of the marginal moment matrix of&nbsp;''X''. Thus, the elements corresponding to X in the above partial sweeping equation represent the marginal distribution of X in potential form. Second, according to statistics, <math> \mu_2  - \mu_1 (\Sigma_{11} )^{-1} \Sigma_{12} </math>is the conditional mean of ''Y'' given ''X''&nbsp;=&nbsp;0; <math> \Sigma_{22} - \Sigma_{21} (\Sigma_{11} )^{-1} \Sigma_{12} </math>  is the conditional covariance matrix of ''Y'' given ''X''nbsp;=&nbsp;0; and  <math>(\Sigma_{11} )^{-1} \Sigma_{12} </math> is the slope of the regression model of ''Y'' on&nbsp;''X''. Therefore, the elements corresponding to Y indices and the intersection of ''X'' and ''Y'' in <math> M(\vec X,Y)</math>represents the conditional distribution of ''Y'' given&nbsp;''X''&nbsp;=&nbsp;0.

These semantics render the partial sweeping operation a useful method for manipulating multivariate normal distributions. They also form the basis of the moment matrix representations for the three remaining important cases of linear belief functions, including proper belief functions, linear equations, and linear regression models.

=== Proper linear belief functions ===
For variables ''X'' and ''Y'', assume there exists a piece of evidence justifying a normal distribution for variables ''Y'' while bearing no opinions for variables&nbsp;''X''. Also, assume that ''X'' and ''Y'' are not perfectly linearly related, i.e., their correlation is less than&nbsp;1. This case involves a mix of an ordinary normal distribution for Y and a vacuous belief function for&nbsp;''X''. Thus, we represent it using a partially swept matrix  as follows:

: <math>M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   0
\end{array} & \begin{array}{*{20}c}
   \mu_2 \\
   0  \\
   \Sigma_{22} \\
\end{array}
\end{array} \right]
</math>

This is how we could understand the representation. Since we are ignorant on&nbsp;''X'', we use its swept form and set  <math> \mu_1 (\Sigma_{11})^{-1} = 0</math> and <math> - (\Sigma_{11})^{-1} = 0</math>. Since the correlation between ''X'' and ''Y'' is less than&nbsp;1, the regression coefficient of ''X'' on ''Y'' approaches to 0 when the variance of ''X'' approaches to&nbsp;∞. Therefore,  <math>(\Sigma_{11})^{-1} \Sigma_{12} = 0</math>. Similarly, one can prove that <math>\mu_1 (\Sigma_{11})^{-1} \Sigma_{12}  = 0</math> and  <math> \Sigma_{21} (\Sigma_{11})^{-1} \Sigma_{12} = 0</math>.

=== Linear equations ===
Suppose X and Y are two row vectors, and Y = XA + b, where A and b are the coefficient matrices. We represent the equation using a partially swept matrix as follows:

: <math>M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   A^T
\end{array} & \begin{array}{*{20}c}
   b  \\
   A  \\
   0
\end{array}
\end{array} \right]
</math>

We can understand the representation based on the fact that a linear equation contains two pieces of knowledge: (1) complete ignorance about all variables; and (2) a degenerate conditional distribution of dependent variables given independent variables. Since X is an independent vector in the equation, we are completely ignorant about it. Thus, <math> \mu_1 (\Sigma _{11})^{-1} = 0</math> and <math> -(\Sigma_{11})^{-1} = 0</math>. Given ''X'' = 0, ''Y'' is completely determined to be b. Thus, the conditional mean of Y is b and the conditional variance is 0. Also, the regression coefficient matrix is A.

Note that the knowledge to be represented in linear equations is very close to that in a proper linear belief functions, except that the former assumes a perfect correlation between X and Y while the latter does not. This observation is interesting; it characterizes the difference between partial ignorance and linear equations in one parameter — correlation.

=== Linear regression models ===
A linear regression model is a more general and interesting case than previous ones. Suppose X and Y are two vectors and Y = XA + b + E, where A and b are the appropriate coefficient matrices and E is an independent white noise satisfying E ~ N(0, Σ). We represent the model as the following partially swept matrix:

: <math>
M(\vec X,Y) = \left[ \begin{array}{*{20}c}
   \begin{array}{*{20}c}
   0  \\
   0  \\
   A^T
\end{array} & \begin{array}{*{20}c}
   b  \\
   A \\
   \Sigma
\end{array}
\end{array} \right]
</math>

This linear regression model may be considered as the combination of two pieces of knowledge (see later), one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, Σ). Alternatively, one may consider it similar to a linear equation, except that, given X = 0, Y is not completely determined to be b. Instead, the conditional mean of Y is b while the conditional variance is Σ. Note that, in this alternative interpretation, a linear regression model forms a basic building block for knowledge representation and is encoded as one moment matrix. Besides, the noise term E does not appear in the representation. Therefore, it makes the representation more efficient.

From representing the six special cases, we see a clear advantage of the moment matrix representation, i.e., it allows a unified representation for seemingly diverse types of knowledge, including linear equations, joint and conditional distributions, and ignorance. The unification is significant not only for knowledge representation in artificial intelligence but also for statistical analysis and engineering computation. For example, the representation treats the typical logical and probabilistic components in statistics — observations, distributions, improper priors (for Bayesian statistics), and linear equation models — not as separate concepts, but as manifestations of a single concept. It allows one to see the inner connections between these concepts or manifestations and to interplay them for computational purposes.

== Knowledge operations ==

There are two basic operations for making inferences in [[expert system]]s using linear belief functions: combination and marginalization. Combination corresponds to the integration of knowledge whereas marginalization corresponds to the coarsening of knowledge. Making an inference involves combining relevant knowledge into a full body of knowledge and then projecting the full body of knowledge to a partial domain, in which an inference question is to be answered.

=== Marginalization ===
Marginalization projects a linear belief function into one with fewer variables. Expressed as a moment matrix, it is simply the restriction of a nonswept moment matrix to a submatrix corresponding to the remaining variables. For example, for the joint distribution M(X, Y), its marginal to Y is:

: <math>
M^{\downarrow Y} (X,Y) = \left[ {\begin{array}{*{20}c}
   \mu_2  \\
   \Sigma_{22}
\end{array}} \right]
</math>

When removing a variable, it is important that the variable has not been swept on in the corresponding moment matrix, i.e., it does not have an arrow sign above the variable. For example, projecting the matrix <math>M(\vec X,Y)</math> to Y produces:

: <math> M^{ \downarrow Y} (\vec X,Y) = \left[ {\begin{array}{*{20}c}
   \mu _2  - \mu _1 (\Sigma _{11} )^{-1} \Sigma _{12} \\
   \Sigma_{22} - \Sigma_{21} (\Sigma _{11})^{-1} \Sigma_{12}
\end{array}} \right]
</math>

which is not the same linear belief function of Y. However, it is easy to see that removing any or all variables in Y from the partially swept matrix will still produce the correct result — a matrix representing the same function for the remaining variables.

To remove a variable that has been already swept on, we have to reverse the sweeping using partial or full reverse sweepings. Assume  <math>M(\vec X)</math> is a fully swept moment matrix,

: <math>
M(\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu }  \\
   {\bar \Sigma }  \\
\end{array}} \right)
</math>

Then a full reverse sweeping of <math>M(\vec X)</math> will recover the moment matrix M(X) as follows:

: <math>
M(X) = \left( {\begin{array}{*{20}c}
   { - \bar \mu \bar \Sigma ^{ - 1} }  \\
   { - \bar \Sigma ^{ - 1} }  \\
\end{array}} \right)
</math>

If a moment matrix is in a partially swept form, say

: <math>
M(\vec X,Y) = \left[ {\begin{array}{*{20}c}
   {\begin{array}{*{20}c}
   {\bar \mu _1 }  \\
   {\bar \Sigma _{11} }  \\
   {\bar \Sigma _{21} }  \\
\end{array}} & {\begin{array}{*{20}c}
   {\bar \mu _2 }  \\
   {\bar \Sigma _{12} }  \\
   {\bar \Sigma _{22} }  \\
\end{array}}  \\
\end{array}} \right]
</math>

its partially reverse sweeping on X is defined as follows:

: <math> M(X,Y) = \left[ {\begin{array}{*{20}c}
   {\begin{array}{*{20}c}
   { - \bar \mu _1 (\bar \Sigma _{11} )^{ - 1} }  \\
   { - (\bar \Sigma _{11} )^{ - 1} }  \\
   { - \bar \Sigma _{21} (\bar \Sigma _{11} )^{ - 1} }  \\
\end{array}} & {\begin{array}{*{20}c}
   {\bar \mu _2  - \bar \mu _1 (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
   { - (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
   {\bar \Sigma _{22}  - \bar \Sigma _{21} (\bar \Sigma _{11} )^{ - 1} \bar \Sigma _{12} }  \\
\end{array}}  \\
\end{array}} \right]
</math>

Reverse sweepings are similar to those of forward ones, except for a sign difference for some multiplications. However, forward and reverse sweepings are opposite operations. It can be easily shown that applying the fully reverse sweeping to <math>M(\vec X)</math>   will recover the initial moment matrix  M(X). It can also be proved that applying a partial reverse sweeping on X to the matrix  <math> M(\vec X,Y)</math> will recover the moment matrix M(X,Y). As a matter of fact, Liu<ref>L. Liu, "Local Computation of Gaussian Belief Functions," ''International Journal of Approximate Reasoning'', vol. 22, pp. 217–248, 1999</ref> proves that a moment matrix will be recovered through a reverse sweeping after a forward sweeping on the same set of variables. It can be also recovered through a forward sweeping after a reverse sweeping. Intuitively, a partial forward sweeping factorizes a joint into a marginal and a conditional, whereas a partial reverse sweeping multiplies them into a joint.

=== Combination ===
According to [[Dempster’s rule]], the combination of belief functions may be expressed as the intersection of focal elements and the multiplication of probability density functions. [[Liping Liu]] applies the rule to linear belief functions in particular and obtains a formula of combination in terms of density functions. Later he proves a claim by [[Arthur P. Dempster]] and reexpresses the formula as the sum of two fully swept matrices. Mathematically, assume <math>M_1 (\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _1 }  \\
   {\bar \Sigma _1 }  \\
\end{array}} \right)
</math>  and <math> M_2 (\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _2 }  \\
   {\bar \Sigma _2 }  \\
\end{array}} \right)
</math>  are two LBFs for the same vector of variables X. Then their combination is a fully swept matrix:

: <math> M(\vec X) = \left( {\begin{array}{*{20}c}
   {\bar \mu _1  + \bar \mu _2 }  \\
   {\bar \Sigma _1  + \bar \Sigma _2 }  \\
\end{array}} \right)
</math>

This above equation is often used for multiplying two normal distributions. Here we use it to define the combination of two linear belief functions, which include normal distributions as a special case. Also, note that a vacuous linear belief function (0 swept matrix) is the neutral element for combination. When applying the equation, we need to consider two special cases. First, if two matrices to be combined have different dimensions, then one or both matrices must be vacuously extended, i.e., assuming ignorance on the variables that are no present in each matrix. For example, if M<sub>1</sub>(X,Y)  and M<sub>2</sub>(X,Z)  are to be combined, we will first extend them into <math> M_1 (X,Y,\vec Z)</math>  and <math> M_2 (X,\vec Y,Z)</math>  respectively such that  <math> M_1 (X,Y,\vec Z)</math> is ignorant about Z and  <math> M_2 (X,\vec Y,Z)</math> is ignorant about Y. The vacuous extension was initially proposed by Kong <ref>A. Kong, "Multivariate belief functions and graphical models," in Department of Statistics. Cambridge, MA: Harvard University, 1986</ref> for discrete belief functions. Second, if a variable has zero variance, it will not permit a sweeping operation. In this case, we can pretend the variance to be an extremely small number, say ε, and perform the desired sweeping and combination. We can then apply a reverse sweeping to the combined matrix on the same variable and let ε approaches 0. Since zero variance means complete certainty about a variable, this ε-procedure will vanish ε terms in the final result.

In general, to combine two linear belief functions, their moment matrices must be fully swept. However, one may combine a fully swept matrix with a partially swept one directly if the variables of the former matrix have been all swept on in the later. We can use the linear regression model — Y = XA + b + E — to illustrate the property. As we mentioned, the regression model may be considered as the combination of two pieces of knowledge: one is specified by the linear equation involving three variables X, Y, and E, and the other is a simple normal distribution of E, i.e., E ~ N(0, Σ). Let  <math>M_1 (\vec X,\vec {\rm E},Y) = \left[ {\begin{array}{*{20}c}
   0 & 0 & b  \\
   0 & 0 & A  \\
   0 & 0 & I  \\
   {A^T } & I & 0  \\
\end{array}} \right]
</math> and <math> M_2 (\vec {\rm E}) = \left[ {\begin{array}{*{20}c}
   0  \\
   { - \Sigma ^{ - 1} }  \\
\end{array}} \right]
</math>  be their moment matrices respectively. Then the two matrices can be combined directly without sweeping <math> M_1 (\vec X,\vec {\rm E},Y)
</math>  on Y first. The result of the combination is a partially swept matrix as follows:

: <math> M(\vec X,\vec {\rm E},Y) = \left[ {\begin{array}{*{20}c}
   0 & 0 & b  \\
   0 & 0 & A  \\
   0 & { - \Sigma ^{ - 1} } & I  \\
   {A^T } & I & 0  \\
\end{array}} \right]
</math>

If we apply a reverse sweeping on E and then remove E from the matrix, we will obtain the same representation of the regression model.

== Applications ==

We may use an audit problem to illustrate the three types of variables as follows. Suppose we want to audit the ending balance of accounts receivable (''E''). As we saw earlier, ''E'' is equal to the beginning balance (''B'') plus the sales (''S'') for the period minus the cash receipts (''C'') on the sales plus a residual (''R'') that represents insignificant sales returns and cash discounts. Thus, we can represent the logical relation as a linear equation:

: <math>E=B+S-C+R</math>
	
Furthermore, if the auditor believes ''E'' and ''B'' are 100 thousand dollars on the average with a standard deviation 5 and the covariance 15, we can represent the belief as a multivariate normal distribution. If historical data indicate that the residual R is zero on the average with a standard deviation of 0.5 thousand dollars, we can summarize the historical data by normal distribution ''R''&nbsp;~&nbsp;N(0,&nbsp;0.5<sup>2</sup>).  If there is a direct observation on cash receipts, we can represent the evidence as an equation say, C = 50 (thousand dollars). If the auditor knows nothing about the beginning balance of accounts receivable, we can represent his or her ignorance by a vacuous LBF. Finally, if historical data suggests that, given cash receipts&nbsp;''C'', the sales ''S'' is on the average 8''C''&nbsp;+&nbsp;4 and has a standard deviation 4 thousand dollars, we can represent the knowledge as a linear regression model ''S''&nbsp;~&nbsp;N(4&nbsp;+&nbsp;8''C'',&nbsp;16).

==References==
<references/>

{{DEFAULTSORT:Linear Belief Function}}
[[Category:Knowledge representation]]
<=====doc_Id=====>:835
<=====title=====>:
Folksonomy
<=====text=====>:
A '''folksonomy''' is a system in which users apply public [[Tag (metadata)|tags]]  to online items, typically to aid them in re-finding those items. This can give rise to a classification system based on those tags and their frequencies, in contrast to a [[Taxonomy (general)|taxonomic]] classification specified by the owners of the content when it is published.<ref>{{cite news
 | title = Folksonomies. Indexing and Retrieval in Web 2.0.
 | url = https://books.google.com/books?id=Aeib_wy18gkC&printsec=frontcover&dq=folksonomies.+Indexing+and+Retrieval+in+Web+2.0#v=onepage&q&f=false
 | first = Isabella
 | last = Peters
 | work = Berlin: De Gruyter Saur
 | year = 2009
 }}</ref><ref>{{cite news
 | title = Folksonomy
 | first = Daniel H.
 | last = Pink
 | authorlink =
 | url = http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html
 | work = New York Times
 | date = 11 December 2005
 | accessdate = 14 July 2009
 }}</ref> This practice is also known as '''collaborative tagging''',<ref>Lambiotte, R, and M Ausloos. 2005. Collaborative tagging as a tripartite network. http://arxiv.org/abs/cs.DS/0512090.</ref><ref>{{cite web|last1=Borne|first1=Kirk|title=Collaborative Annotation for Scientific Data Discovery and Reuse|url=http://www.asis.org/Bulletin/Apr-13/AprMay13_RDAP_Borne.html|website=Bulletin of Association for Information Science and Technology|publisher=ASIS&T|accessdate=26 May 2016}}</ref> '''social classification''', '''social indexing''', and '''social tagging'''. However, these terms have slightly different meanings than folksonomy. Folksonomy was originally “the result of personal free tagging of information [...] for one’s own retrieval.”.<ref>Vander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from <nowiki>http://www.vanderwal.net/folksonomy.html</nowiki></ref> '''Social tagging''' is the application of tags in an open online environment where the tags of other users are available to others. '''Collaborative tagging''' (also known as group tagging) is tagging performed by a group of users. This type of folksonomy is commonly used in cooperative and collaborative projects such as research, content repositories, and social bookmarking.

The term was coined by [[Thomas Vander Wal]] in 2004<ref>{{cite news
 | title = Folksonomy Coinage and Definition
 | url = http://www.vanderwal.net/folksonomy.html
 | first = Tomas
 | last = Vander Wal
 | date = 11 December 2005
 }}</ref><ref>Vander Wal, T. (2005). "[http://www.vanderwal.net/random/category.php?cat=153 Off the Top: Folksonomy Entries]." Visited November 5, 2005. See also: Smith, Gene. "[https://web.archive.org/web/20040828035712/http://atomiq.org/archives/2004/08/folksonomy_social_classification.html Atomiq: Folksonomy: social classification]." Aug 3, 2004. Retrieved January 1, 2007.</ref><ref>http://vanderwal.net/folksonomy.html Origin of the term</ref> as a [[portmanteau]] of ''[[Volk (German word)|folk]]'' and ''[[Taxonomy (general)|taxonomy]]''. Folksonomies became popular as part of [[social software]] applications such as [[social bookmarking]] and photograph annotation that enable users to collectively classify and find information via shared tags. Some websites include [[tag cloud]]s as a way to visualize tags in a folksonomy.<ref>{{Cite journal
 | last1 = Lamere | first1 = Paul
 | title = Social Tagging And Music Information Retrieval
 | journal = Journal of New Music Research
 | volume = 37
 | issue = 2
 | pages = 101–114
 | date = June 2008
 | url = http://www.informaworld.com/smpp/content~db=all~content=a906001732
 | doi = 10.1080/09298210802479284 }}</ref>

Folksonomies can be used for K-12 education, business, and higher education. More specifically, folksonomies may be implemented for social bookmarking, teacher resource repositories, e-learning systems, collaborative learning, collaborative research, and professional development.

==Benefits and disadvantages==
Folksonomies are a trade-off between traditional centralized classification and no classification at all,<ref>Gupta, M., et al., ''An Overview of Social Tagging and Applications, in Social Network Data Analytics'', C.C. Aggarwal, Editor. 2011, Springer. p. 447-497.</ref> and have several advantages:<ref>Quintarelli, E., ''Folksonomies: power to the people''. 2005.</ref><ref>Mathes, A., ''Folksonomies - Cooperative Classification and Communication Through Shared Metadata''. 2004.</ref><ref>Wal, T.V. ''Folksonomy''. 2007</ref>
* tagging is easy to understand and do, even without training and previous knowledge in classification or indexing
* the vocabulary in a folksonomy directly reflects the user’s vocabulary
* folksonomies are flexible, in the sense that the user can add or remove tags
* tags consist of both popular content and long-tail content, enabling users to browse and discover new content even in narrow topics
* tags reflect the user’s conceptual model without cultural, social, or political bias
* enable the creation of communities, in the sense that users who apply the same tag have a common interest
* folksonomies are multi-dimensional, in the sense that users can assign any number and combination of tags to express a concept

There are several disadvantages with the use of tags and folksonomies as well,<ref>Kipp, M. and D.G. Campbell, ''Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices''. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.</ref> and some of the advantages (see above) can lead to problems. For example, the simplicity in tagging can result in poorly applied tags.<ref>Hayman, S., ''Folksonomies and Tagging: New developments in social bookmarking'', in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.</ref> Further, while controlled vocabularies are exclusionary by nature,<ref>Kroski, E., The Hive Mind: ''Folksonomies and User-Based Tagging. 2005''</ref> tags are often ambiguous and overly personalized.<ref>Guy, M. and E. Tonkin, ''Folksonomies: Tidying up Tags?'' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.</ref> Users apply tags to documents in many different ways and tagging systems also often lack mechanisms for handling synonyms, acronyms and homonyms, and they also often lack mechanisms for handling spelling variations such as misspellings, singular/plural form, conjugated and compound words. Some tagging systems do not support tags consisting of multiple words, resulting in tags like “viewfrommywindow”. Sometimes users choose specialized tags or tags without meaning to others.

==Elements and types==
A folksonomy emerges when users tag content or information, such web pages, photos, videos, podcasts, tweets, scientific papers and others. Strohmaier et al.<ref>Strohmaier, M., C. Körner, and R. Kern, ''Understanding why users tag: A survey of tagging motivation literature and results from an empirical study''. Web Semantics: Science, Services and Agents on the World Wide Web, 2012. 17: p. 1-11.</ref> elaborate the concept: the term “tagging” refers to a "voluntary activity of users who are annotating resources with term-so-called 'tags' – freely chosen from an unbounded and uncontrolled vocabulary". Others explain tags as an unstructured textual label <ref>Ames, M.N.M., ''Why We Tag: Motivations for Annotation in Mobile and Online Media'', in SIGCHI conference on Human factors in computing systems. 2007, ACM Press: New York, NY, USA. p. 971-980.</ref> or keywords,<ref>Guy, M. and E. Tonkin, Folksonomies: ''Tidying up Tags?'' D-Lib Magazine, 2006. 12(Number 1): p. 1-15.</ref> and that they appear as a simple form of metadata.<ref>Brooks, C.H. and N. Montanez, ''Improved annotation of the blogosphere via autotagging and hierarchical clustering'', in WWW '06: Proceedings of the 15th international conference on World Wide Web. 2006, ACM Press: New York, NY, USA. p. 625-632.</ref>

Folksonomies consist of three basic entities: users, tags, and resource. Users create tags to mark resources such as: web pages, photos, videos, and podcasts. These tags are used to manage, categorize and summarize online content. This collaborative tagging system also uses these tags as a way to index information, facilitate searches and navigate resources. Folksonomy also includes a set of URLs that are used to identify resources that have been referred to by users of different websites. These systems also include category schemes that have the ability to organize tags at different levels of granularity.<ref name="Berlin, B. 1992">Berlin, B. (1992). Ethnobiological Classification. Princeton: Princeton University Press.</ref>

Vander Wal identifies two types of folksonomy: broad and narrow.<ref name="Vander Wal">{{cite web |title=Explaining and Showing Broad and Narrow Folksonomies |url=http://www.vanderwal.net/random/entrysel.php?blog=1635 | last = Vander Wal | first=Thomas |accessdate= 2013-03-05}}</ref>  A broad folksonomy arises when multiple users can apply the same tag to an item, providing information about which tags are the most popular. A narrow folksonomy occurs when users, typically fewer in number and often including the item's creator, tag an item with tags that can each be applied only once.  While both broad and narrow folksonomies enable the searchability of content by adding an associated word or phrase to an object, a broad folksonomy allows for sorting based on the popularity of each tag, as well as the tracking of emerging trends in tag usage and developing vocabularies.<ref name="Vander Wal"/>

An example of a broad folksonomy is [[Delicious (website)|del.icio.us]],  a website where users can tag any online resource they find relevant with their own personal tags. The photo-sharing website [[Flickr]] is an oft-cited example of a narrow folksonomy.

==Folksonomy vs. taxonomy==
'Taxonomy' refers to a hierarchical categorization in which relatively well-defined classes are nested under broader categories. A ''folksonomy'' establishes categories (each tag is a category) without stipulating or necessarily deriving a hierarchical structure of parent-child relations among different tags. (Work has been done on techniques for deriving at least loose hierarchies from clusters of tags.<ref>{{cite journal|last1=Laniado|first1=David|title=Using WordNet to turn a folksonomy into a hierarchy of concepts|journal=CEUR Workshop Proceedings|volume=314|issue=51|url=http://ceur-ws.org/Vol-314/51.pdf|accessdate=7 August 2015}}</ref>)

Supporters of folksonomies claim that they are often preferable to taxonomies because folksonomies democratize the way information is organized, they are more useful to users because they reflect current ways of thinking about domains, and they express more information about domains.<ref>{{cite web|last1=Weinberger|first1=David|title=Folksonomy as Symbol|url=http://www.hyperorg.com/blogger/?p=6254|website=Joho the Blog|accessdate=7 August 2015}}</ref> Critics claim that folksonomies are messy and thus harder to use, and can reflect transient trends that may misrepresent what is known about a field.

An empirical analysis of the complex dynamics of tagging systems, published in 2007,<ref name="WWW07-ref" >Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proc. International Conference on World Wide Web, ACM Press, 2007.</ref> has shown that consensus around stable distributions and shared vocabularies does emerge, even in the absence of a central [[controlled vocabulary]]. For content to be searchable, it should be categorized and grouped. While this was believed to require commonly agreed on sets of content describing tags (much like keywords of a journal article), some research has found that in large folksonomies common structures also emerge on the level of categorizations.<ref name="TWEB-ref" >V. Robu, H. Halpin, H. Shepherd [http://portal.acm.org/citation.cfm?id=1594173.1594176 Emergence of consensus and shared vocabularies in collaborative tagging systems], ACM Transactions on the Web (TWEB), Vol. 3(4), art. 14, 2009.</ref>
Accordingly, it is possible to devise mathematical [[models of collaborative tagging]] that allow for translating from personal tag vocabularies (personomies) to the vocabulary shared by most users.<ref>Robert Wetzker, Carsten Zimmermann, Christian Bauckhage, and Sahin Albayrak [http://portal.acm.org/citation.cfm?id=1718487.1718497 I tag, you tag: translating tags for advanced user models], Proc. International Conference on Web Search and Data Mining, ACM Press, 2010.</ref>

Folksonomy is unrelated to [[folk taxonomy]], a cultural practice that has been widely documented in anthropological and [[folkloristics|folkloristic]] work. Folk taxonomies are culturally supplied, intergenerationally transmitted, and relatively stable classification systems that people in a given culture use to make sense of the entire world around them (not just the [[Internet]]).<ref name="Berlin, B. 1992"/>

The study of the structuring or classification of folksonomy is termed ''folksontology''.<ref>{{cite web | url=http://www.heppnetz.de/files/vandammeheppsiorpaes-folksontology-semnet2007-crc.pdf | title=FolksOntology: An Integrated Approach for Turning Folksonomies into Ontologies | accessdate=April 20, 2012 | author=Van Damme, Céline|display-authors=etal}}</ref> This branch of [[ontology (information science)|ontology]] deals with the intersection between highly structured taxonomies or hierarchies and loosely structured folksonomy, asking what best features can be taken by both for a system of classification. The strength of flat-tagging schemes is their ability to relate one item to others like it. Folksonomy allows large disparate groups of users to collaboratively label massive, dynamic information systems. The strength of taxonomies are their browsability: users can easily start from more generalized knowledge and target their queries towards more specific and detailed knowledge.<ref>Trattner, C., Körner, C., Helic, D.: [http://www.christophtrattner.info/pubs/iknow2011.pdf Enhancing the Navigability of Social Tagging Systems with Tag Taxonomies]. In Proceedings of the 11th International Conference on Knowledge Management and Knowledge Technologies, ACM, New York, NY, USA, 2011</ref> Folksonomy looks to categorize tags and thus create browsable spaces of information that are easy to maintain and expand.

== Social tagging for knowledge acquisition ==
Social tagging for knowledge acquisition is the specific use of tagging for finding and re-finding specific content for an individual or group. Social tagging systems differ from traditional taxonomies in that they are community-based systems lacking the traditional hierarchy of taxonomies. Rather than a top-down approach, social tagging relies on users to create the folksonomy from the bottom up.<ref name=":0">Held, C., & Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.</ref>

Common uses of social tagging for knowledge acquisition include personal development for individual use and collaborative projects. Social tagging is used for knowledge acquisition in secondary, post-secondary, and graduate education as well as personal and business research. The benefits of finding/re-finding source information are applicable to a wide spectrum of users. Tagged resources are located through search queries rather than searching through a more traditional file folder system.<ref>Fu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229–238. ACM, New York.</ref> The social aspect of tagging also allows users to take advantage of metadata from thousands of other users.<ref name=":0" />

Users choose individual tags for stored resources. These tags reflect personal associations, categories, and concepts. All of which are individual representations based on meaning and relevance to that individual. The tags, or keywords, are designated by users. Consequently, tags represent a user’s associations corresponding to the resource. Commonly tagged resources include videos, photos, articles, websites, and email.<ref name=":1">Kimmerle, J., Cress, U., & Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research & Practice, 8(1), 33-44.</ref> Tags are beneficial for a couple of reasons. First, they help to structure and organize large amounts of digital resources in a manner that makes them easily accessible when users attempt to locate the resource at a later time. The second aspect is social in nature, that is to say that users may search for new resources and content based on the tags of other users. Even the act of browsing through common tags may lead to further resources for knowledge acquisition.<ref name=":0" />

Tags that occur more frequently with specific resources are said to be more strongly connected. Furthermore, tags may be connected to each other. This may be seen in the frequency in which they co-occur. The more often they co-occur, the stronger the connection. Tag clouds are often utilized to visualize connectivity between resources and tags. Font size increases as the strength of association increases.<ref name=":1" />

Tags show interconnections of concepts that were formerly unknown to a user. Therefore, a user’s current cognitive constructs may be modified or augmented by the metadata information found in aggregated social tags. This process promotes knowledge acquisition through cognitive irritation and equilibration. This theoretical framework is known as the co-evolution model of individual and collective knowledge.<ref name=":1" />

The co-evolution model focuses on cognitive conflict in which a learner’s prior knowledge and the information received from the environment are dissimilar to some degree.<ref name=":0" /><ref name=":1" /> When this incongruence occurs, the learner must work through a process cognitive equilibration in order to make personal cognitive constructs and outside information congruent. According to the coevolution model, this may require the learner to modify existing constructs or simply add to them.<ref name=":0" /> The additional cognitive effort promotes information processing which in turn allows individual learning to occur.<ref name=":1" />

A Canadian university study of instructors' use of folksonomy tools in a learning objects repository identified critical success factors, and affirmed the applicability of Zipf's [[Principle of least effort|Principle of Least Effort]], concluding that a major benefit of "the folksonomical approach to knowledge management... is the fact that it is developed and maintained by the users of that body of knowledge," fostering "the dual outcome of creating a more viable knowledge management tool while strengthening the bonds of the user community."<ref>{{Cite book|url=http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-368-5.ch045|title=Critical Success Factors in the Development of Folksonomy-Based Knowledge Management Tools|last=Owen|first=Kenneth|last2=Willis|first2=Robert|date=2010|publisher=IGI Global|year=|isbn=|location=|pages=509–518|language=English|doi=10.4018/978-1-60566-368-5.ch045|hdl=http://hdl.handle.net/10613/3176|quote=|via=VIUSpace|hdl-access=free}}</ref>

==Examples of folksonomies==

* [[Twitter]] [[hashtag]]s
* Many libraries' online catalogs<ref>Steele, T. (2009).  The new cooperative cataloging.  Library Hi Tech, 27 (1), 68-77</ref><ref>Corey A. Harper and [[Barbara B. Tillett]], [https://scholarsbank.uoregon.edu/dspace/bitstream/1794/3269/1/ccq_s Library of Congress controlled vocabularies and their application to the Semantic Web]</ref>
* [[Delicious (website)|del.icio.us]]: public tagging service
* [[Flickr]]: shared photos
* [[Steam (software)|Steam]] video game store
* [[Mendeley]]: social reference management software
* [[StumbleUpon]]: content discovery engine
* [[Diigo]]: [[social bookmarking]] website
*  The [[World Wide Web Consortium]]'s [[Annotea]] project with user-generated tags in 2002.
* [[Instagram]]: online photo-sharing and social networking service
* [[WordPress]]: blogging tool and Content Management System
* [[Pinterest]]: photosharing and publishing website

==See also==
{{div col|colwidth=30em}}
* [[Collective intelligence]]
* [[Enterprise bookmarking]]
* [[Faceted classification]]
* [[Semantic similarity]]
* [[Thesaurus]]
* [[Weak ontology]]
* [[Wiki]]
{{div col end}}

==References==
{{Reflist|30em|refs = Bateman, S., Brooks, C., McCalla, G., & Brusilovsky, P. (2007, May). Applying collaborative tagging to e-learning. In Proceedings of the 16th International World Wide Web Conference (WWW2007).

Civan, A., Jones, W., Klasnja, P., & Bruce, H. (2008). Better to organize personal information by folders or by tags?: The devil is in the details.Proceedings of the American Society for Information Science and Technology,45(1), 1-13.
 
Fu, W. (2008). The microstructures of social tagging: a rational model. In: Proceedings of the ACM 2008 Conference on Computer Supported Cooperative Work, pp. 229–238. ACM, New York.

Guy, M. and E. Tonkin, Folksonomies: Tidying up Tags? D-Lib Magazine, 2006. 12(Number 1): p. 1-15.

Gupta, M., et al., An Overview of Social Tagging and Applications, in Social Network Data Analytics, C.C. Aggarwal, Editor. 2011, Springer. p. 447-497
 
Held, C., & Cress, U. (2009). Learning by Foraging: The impact of social tags on knowledge acquisition. In Learning in the synergy of multiple disciplines (pp. 254-266). Springer Berlin Heidelberg.

Hayman, S., Folksonomies and Tagging: New developments in social bookmarking, in Proceedings of Ark Group Conference: Developing and Improving Classification Schemes, 2007, Sydney. 2007: Sydney.
 
Kimmerle, J., Cress, U., & Held, C. (2010). The interplay between individual and collective knowledge: technologies for organisational learning and knowledge building. Knowledge Management Research & Practice, 8(1), 33-44.

Kipp, M. and D.G. Campbell, Patterns and Inconsistencies in Collaborative Tagging Systems: An Examination of Tagging Practices. Proceedings Annual General Meeting of the American Society for Information Science and Technology, 2006.

Kroski, E., The Hive Mind: Folksonomies and User-Based Tagging. 2005.
 
Lavoué, É. (2011). Social tagging to enhance collaborative learning. In Advances in Web-Based Learning-ICWL 2011 (pp. 92-101). Springer Berlin Heidelberg.

Mathes, A., Folksonomies - Cooperative Classification and Communication Through Shared Metadata. 2004.

Quintarelli, E., Folksonomies: power to the people. 2005.

Vander Wal, Thomas. "Folksonomy Coinage and Definition". Retrieved October 25, 2015 from http://www.vanderwal.net/folksonomy.html

Weinberger, D. (2007). Everything is miscellaneous: The power of the new digital disorder. Times Books, New York.
 
}}

==Additional references==
* [http://www.nytimes.com/2005/12/11/magazine/11ideas1-21.html "Folksonomy"], [[The New York Times]], 2005-12-11
* [http://www.wired.com/science/discoveries/news/2005/02/66456 "Folksonomies Tap People Power"], [[Wired News]], 2005-02-01
* {{cite journal | journal = [[Information Services & Use]] | title = Folksonomy and science communication | issue = 27 | year = 2007 | pages = 97–103 | url = http://wwwalt.phil-fak.uni-duesseldorf.de/infowiss/admin/public_dateien/files/1/1194272247inf_servic.pdf }}{{spaced ndash}} Folksonomies as a tool for professional scientific databases
* [http://www.hyperorg.com/blogger/misc/taxonomies_and_tags.html "The Three Orders"]: 2005 explanation of tagging and folksonomies

==External links==
* [http://www.socialtagging.org/ SocialTagging.org] provides short definitions of key terms related to tagging and folksonomies
* [http://www.vanderwal.net/folksonomy.html Vanderwal's definition of folksonomy]
* [http://www.vanderwal.net/random/entrysel.php?blog=1750 Vanderwal's take on Wikipedia's definition of folksonomy]
*[http://er.educause.edu/articles/2011/9/classroom-collaboration-using-social-bookmarking-service-diigo Classroom Collaboration Using Social Bookmarking Service Diigo]

{{Web syndication}}
{{Semantic Web}}

[[Category:Collective intelligence]]
[[Category:Folksonomy| ]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Social bookmarking]]
[[Category:Taxonomy]]
[[Category:Web 2.0 neologisms]]
[[Category:Sociology of knowledge]]
<=====doc_Id=====>:838
<=====title=====>:
IMARK
<=====text=====>:
{{Infobox website
|name=Information Management Resource Kit (IMARK)
|logo = 
|url ={{URL|http://www.imarkgroup.org}}
|type = [[Capacity building]]
|commercial      = No
|registration    = Optional
|language        = English, French, Spanish, Arabic, Chinese
|launch date = 2001
|current status = Online
|screenshot      = }}

The Information Management Resource Kit ('''IMARK''') is a partnership-based [[e-learning]] initiative developed by the [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] and partner organizations to support individuals, institutions and networks world-wide in the effective management of information and agricultural development. IMARK consists of a suite of [[distance learning]] resources and tools on [[information management]].<ref>{{cite web|url=http://www.imarkgroup.org/index_en.asp?m=0 |title=IMARK - Information Management Resource Kit |publisher=Imarkgroup.org |date= |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://usain.org/links2.html |title=Links for Agricultural Librarians |publisher=USAIN |date=2010-05-19 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://knowledge.cta.int/index.php/en/Dossiers/S-T-Issues-in-Perspective/ICT-for-transforming-research-for-agricultural-and-rural-development/Links/IMARK-FAO |title=IMARK (FAO) / Links / ICT for transforming research for agricultural and rural development / S&T Issues in Perspective / Dossiers / Home - Knowledge for Development |publisher=Knowledge.cta.int |date= |accessdate=2011-06-07}}</ref>

== About IMARK ==
The [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]] initiated a partnership-based [[e-learning]] programme in 2001 to support [[information management]].<ref>http://www.fao.org/rdd/doc/IMARK%20General%20Sheet%20EN%2011-05.pdf</ref> IMARK is targeted at information professionals in developing countries. Each IMARK curriculum is designed through a consultative process with [[Subject-matter expert|subject matter experts]], field practitioners and representatives from the target audience from around the world. The IMARK initiative is a response to demand for enhanced information and [[knowledge management]] in the effort to achieve the [[Millennium Development Goals|Millennium Development Goals (MDGs)]], especially those related to [[hunger]] and the information society, in the context of bridging the digital divide.<ref>{{cite web|url=http://www.fao.org/rdd/doc/FAO%20Approach%20to%20WSIS2005.pdf |title=FAO's strategies towards the WSIS 2005 |format=PDF |date= |accessdate=2011-06-07}}</ref>  The development goal of IMARK is to improve the capabilities of people concerned with [[information management]] and [[knowledge sharing]].<ref>{{cite web|url=http://editlib.org/noaccesspresent/26495 |title=Ed/ITLib Digital Library → No Access |publisher=Editlib.org |date=2007-10-15 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.infodev.org/en/Publication.183.html |title=Quick Guide: ICT and Rural Livelihood Resources at FAO |publisher=infoDev.org |date=2006-09-28 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.icimod.org/index.php?page=23 |title=Knowledge Management |publisher=Icimod.org |date=2010-12-16 |accessdate=2011-06-07}}</ref>

=== Objectives and Scope ===
The development goal of IMARK is to improve the overall effectiveness of programmes in agricultural development and [[food security]] by enhancing access to information by key stakeholders.<ref>http://eprints.rclis.org/bitstream/10760/15682/1/FAO%E2%80%99s%20Capacity-Building%20Initiatives%20in%20Accessing,%20Documenting,%20Communicating%20and%20Managing%20Agricultural%20Information.pdf</ref>

=== Steering Committee ===
IMARK has over 30 partners and collaborating institutions since its inception in 2001, and its activities are coordinated through a Steering Committee whose members include [[Association for Progressive Communications|The Association for Progressive Communications (APC)]], [[Food and Agriculture Organization|Food and Agriculture Organization (FAO)]] of the [[United Nations]], the [[Agence universitaire de la Francophonie|Agence Universitaire de la Francophonie (AUF)]], [[Commonwealth of Learning|Commonwealth of Learning (COL)]], [[Groupe de Recherches et d'Echanges Technologiques|Groupe de Recherches et d'Echanges Technologiques (GRET)]], [[Bibliotheca Alexandrina]] and [[UNESCO]].<ref>{{cite web|url=http://portal.unesco.org/ci/en/ev.php-URL_ID=21458&URL_DO=DO_TOPIC&URL_SECTION=201.html |title=IMARK launches new e-learning module |publisher=Portal.unesco.org |date=2008-01-24 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.apc.org/en/news/development/world/e-learning-module-developing-electronic-communitie |title=e-Learning module developing electronic communities &#124; Association for Progressive Communications |publisher=Apc.org |date=2006-05-08 |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://webapp.ciat.cgiar.org/ccc/imark.htm |title=CCC: E-Learning Initiatives - IMARK-FAO |publisher=Webapp.ciat.cgiar.org |date= |accessdate=2011-06-07}}</ref><ref>{{cite web|url=http://www.iaald.org/docs/iisast2_report.pdf |title=2nd Expert Consultation IISAST |format=PDF |date= |accessdate=2011-06-07}}</ref>

== See also ==
* [[E-Learning|E-learning]]
* [[FAO|Food and Agriculture Organization of the United Nations (FAO)]]
* [[Agricultural Information Management Standards|Agricultural Information Management Standards (AIMS)]]

== References ==
{{Reflist|2}}

== External links ==
* [http://www.imarkgroup.org/ Official IMARK Website ]
* [http://www.fao.org Food and Agriculture Organization of the United Nations]

{{DEFAULTSORT:Imark}}
[[Category:Information technology]]
[[Category:Distance education]]
[[Category:Education]]
[[Category:Virtual learning environments]]
[[Category:Learning methods]]
[[Category:Educational technology projects]]
[[Category:Rural development]]
[[Category:Non-profit technology]]
[[Category:Food and Agriculture Organization]]
[[Category:Information technology management]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:841
<=====title=====>:
LEAP (programming language)
<=====text=====>:
{{Unreferenced|date=November 2007}}
'''LEAP''' is an extension to the [[ALGOL 60]] '''[[programming language]]''' which provides an associative memory of triples. The three items in a triple denote the association that an Attribute of an Object has a specific Value.  LEAP was created by Jerome Feldman (University of California Berkeley) and Paul Rovner (MIT Lincoln Lab) in 1967.  LEAP was also implemented in [[SAIL (programming language)|SAIL]].

==References==
* Feldman, Jerome A. and Paul D. Rovner (Jan, 1968).  "The LEAP language and data structure", MIT Lincoln Laboratory, Lexington, MA.  In abbreviated form, ''Proc. 1968 IFIP Congress'', Brandon Systems Press, Princeton, NJ.
* Feldman, Jerome A. and Paul D. Rovner (Aug, 1969).  [http://portal.acm.org/citation.cfm?doid=363196.363204 "An ALGOL-based associative language"], ''Communications of the ACM'', 12:8, pp 439 - 449.
* Rovner, Paul D (Dec, 1968).  "The LEAP users manual", MIT Lincoln Laboratory, Lexington, MA.
* [ftp://reports.stanford.edu/pub/cstr/reports/cs/tr/73/373/CS-TR-73-373.pdf VanLehn, Kurt A. (Jul, 1973).  "SAIL User Manual", Stanford Artificial Intelligence Laboratory, Stanford, CA.]

[[Category:Structured programming languages]]
[[Category:Procedural programming languages]]
[[Category:Programming languages created in 1967]]
[[Category:Knowledge representation]]


{{compu-lang-stub}}
<=====doc_Id=====>:844
<=====title=====>:
POSC Caesar
<=====text=====>:
{{advert|date=September 2011}}

{{Infobox Organization
|name         = POSC Caesar Association
|image        = POSC Caesar logo.gif
|size         = 200
|alt          = Logo for POSC Caesar Association.
|caption      = Logo for POSC Caesar Association.
|abbreviation = PCA
|formation    = 1997-11-14
|status       = Association
|purpose      = Promote the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters
|location     = Bærum, Norway
|region_served = Worldwide
|membership   = 36
|language     = English
|leader_title = General Manager
|leader_name  = Nils Sandsmark
|main_organ   = Board of Directors
|affiliations = <!-- if any -->
|num_staff    =
|num_volunteers =
|budget       =
|website      = http://www.posccaesar.org
}}
'''POSC Caesar Association''' (PCA) is an international, open, [[Non-profit organization|not-for-profit]], member organization that promotes the development of open specifications to be used as standards for enabling the interoperability of data, software and related matters.

PCA is the initiator of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities" and is committed to its maintenance and enhancement.

Nils Sandsmark has been the General Manager of POSC Caesar Association since 1999<ref name="BioSandsmarkDaraTech">
{{cite web
|url=http://www.daratech.com/plant2007/bios/sandsmark_nils.html
|title=Biography Dr. Ing. Nils Sandsmark
|author=Daratech
|accessdate=2009-08-11
}}</ref> and Thore Langeland, [[Norwegian Oil Industry Association]] ({{lang-no|Oljeindustriens Landsforening}}, OLF), is the Chairman of the Board.

== History ==

=== Caesar Offshore ===
The first predecessor of POSC Caesar Association, the '''Caesar Offshore program''', started in 1993.<ref name="PCAHis">
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/History
|title=History of POSC Caesar
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-05
}}</ref><ref name=POSCStatus98>{{cite web
  | title = POSC/CAESAR project - Information and Status - January 1998
  | publisher = Petrotechnical Open Software Corporation (POSC)
  | date = 1998-03-17
  | url = http://posc.org/caesar/caesar_info.html
  | accessdate = 2009-08-06}}</ref><ref name=Pein>{{cite book
  | last = Pein
  | first = Martin
  | title = On data models, their transformations and consistency preserving programming interfaces
  | publisher = Books on Demand GmbH
  | year = 2002
  | location = Norderstedt, Germany
  | pages = 228
  | url = http://www.amazon.com/transformations-consistency-preserving-programming-interfaces/dp/3831139288/ref=sr_1_1?ie=UTF8&s=books&qid=1249538783&sr=1-1
  | isbn = 3-8311-3928-8}}</ref><ref>{{cite journal
  | title = Data warehouse manages offshore project information
  | journal = Oil & Gas Journal
  | volume = 96
  | issue = 18
  | pages = 94
  | url = http://www.ogj.com/index/current-issue/s-oil-gas-journal/s-volume-96/s-issue-18.html
  | publisher = Pennwell Corporation
  | location = Tulsa, OK
  | date = 1998-05-04
  | issn = 0030-1388
}}</ref><ref name=Stanley>{{cite web
  | last = Port
  | first = Stanley
  | title = Plant Information Management at Statoil Norway
  | date = 1998-04-13
  | url = http://www.hydrocarbononline.com/article.mvc/Plant-Information-Management-at-Statoil-Norwa-0001
  | accessdate = }}</ref>
The original focus was on standardizing technical data definitions for capital intensive projects at the handover from the [[EPC (contract)|EPC]] contractor to the owner/operators of onshore and offshore oil and gas production facilities. The program was sponsored by [[The Research Council of Norway]], two [[EPC (contract)|EPC]] contractors ([[Aker Maritime]] and [[Kværner]]), three owners/operators ([[Norsk Hydro]], [[Saga Petroleum]] and [[Statoil]]) and [[DNV]] as service provider and project owner.

=== POSC Caesar project ===
During the period 1994-96, Caesar Offshore Program was defined as a project of [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]), and changed its name to the '''POSC Caesar Project'''.<ref name="PCAHis"/><ref name=POSCStatus98/><ref name="Pein"/><ref name="Stanley"/>

In 1995 the project was joined by [[BP]], [[Brown and Root]] and [[Elf Aquitaine]] and in 1997 by [[Intergraph]], [[IBM]], [[Oracle Corporation|Oracle]], [[Lloyd's]], [[Royal Dutch Shell|Shell]], [[ABB Group|ABB]] and [http://www.umoe.no/ UMOE Technologies].<ref name=POSCStatus98/>

During that time, POSC Caesar also became a member of European Process Industries STEP Technical Liaison Executive (EPISTLE) where it collaborates with PISTEP (UK), and USPI-NL (The Netherlands) on the development of [[ISO 10303]], also known as "Standard for the Exchange of Product model data (STEP)."

=== POSC Caesar Association ===
In 1997, POSC Caesar Association was founded as an independent, global, non-profit, member organization. POSC Caesar Association serves an international membership and collaborates with other international organizations. It has its main office in Norway.

Albeit the name of POSC Caesar Association still hints to its past as a project within the [[POSC|Petrotechnical Open Software Corporation (POSC)]] (now [[Energistics]]<ref name="POSCEnergistics">
{{cite web
|url=http://www.energistics.org/posc/NewsBot.asp?MODE=VIEW&ID=606&SnID=2
|title=POSC Rebrands as Energistics - Press release
|author=Energistics
|accessdate=2009-09-01
|date=2006-11-09
}}</ref>), from 1997 onwards, the organization has been independent. Energistics and POSC Caesar Association do collaborate, and are formally member in each other's organization.<ref name="EnergisticsMembers">
{{cite web
|url=http://www.energistics.org//assnfe/companydirectory.asp?MODE=FINDRESULTS&SEARCH_TYPE=1&COMPANY_TYPE=2,3,6&SnID=1941691677
|title=Energistics Member Directory
|author=Energistics
|accessdate=2009-08-11
}}</ref><ref name="PCAMembers">
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Membership
|title=POSC Caesar Association Membership
|author=POSC Caesar Association
|accessdate=2009-08-11
}}</ref>

== Membership ==
POSC Caesar Association has with its current 36 members<ref name="PCAMem">
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Membership
|title=POSC Caesar Association - Membership
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-13
}}</ref> from around the world established an international footprint (with a strong membership in Norway) that includes a wide range from academia, solution providers to engineering contractors and owners/operators. The members are (subdivided by organization type):
* Associations: Energistics (USA) and [[Norwegian Oil Industry Association|The Norwegian Oil Industry Association (OLF, Norway)]],
* [[Universities]] and [[Research institute|Research Institutes]]: International Research Institute of Stavanger (IRIS, Norway), [[Norwegian University of Science and Technology|Norwegian University of Science and Technology (NTNU, Norway)]], [[KAIST|Korea Advanced Institute of Science and Technology (KAIST, Korea)]], [[SINTEF|SINTEF (Norway)]], [[University of Bergen|University of Bergen (Norway)]], [[University of Oslo|University of Oslo (Norway)]], [[University of Stavanger|University of Stavanger (Norway)]], [[University of Tromsø|University of Tromsø (Norway)]] and Western Norway Research Institute (Norway),
* [[Petroleum Industry|Oil and Gas Companies]]: [[BP|BP (UK)]], [[Petronas|Petronas (Malaysia)]] and [[Statoil|Statoil (Norway)]],
* Engineering contractors and consultants:  Akvaplan-niva (Norway), [[Aker Solutions|Aker Solutions (Norway)]], Asset Life Cycle Information Management (ALCIM, Malaysia), CAESAR systems (USA), [[Bechtel|Bechtel (USA)]], [[Det Norske Veritas|Det Norske Veritas (DNV, Norway)]], Information Logic (USA) and iXIT Engineering Technology (Germany), Phusion IM Ltd (UK).<ref>https://www.posccaesar.org/wiki/PCA/Membership</ref>
* Solution providers: [[Aveva|Aveva (UK)]], [[Bentley Systems|Bentley Systems (USA)]], Jotne EPM Technology (Norway), Epsis (Norway), Eurostep (Sweden), [[IBM|International Business Machines Corporation (IBM, USA)]], Siemens - Comos Industry Solutions (before Innotec) (Germany), [[Intergraph|Intergraph (USA)]], Invenia (Norway), Keel Solution (Denmark), Noumenon (UK), NRX (Canada), Octaga (Norway) and Tektonisk (Norway).

In general, the organization holds three membership meetings a year;<ref name="PCAAgenda">{{cite web|url=http://www.posccaesar.org/wiki/PCA/Agenda |title=POSC Caesar Association - Agenda |author=POSC Caesar Association (PCA) |accessdate=2009-08-13 }}{{dead link|date=June 2016|bot=medic}}{{cbignore|bot=medic}}</ref> one in January / February in North-America (typically USA), one in April / May in Europe (typically Norway) and one in October in Asia (typically Malaysia).

== Activities and services ==

=== Initiator and custodian of ISO 15926 ===

In consultation with the other EPISTLE members and the [[International Organization for Standardization|International Organization for Standardization (ISO)]], it was decided in 2003 (some say already in 1997{{Citation needed|date=August 2009}}) that for modeling-technical reasons it was better to discontinue the development of [[ISO 10303]]<ref name="SC4Legacy">
{{cite web
|url=http://www.tc184-sc4.org/SC4_Open/SC4%20Legacy%20Products%20(2001-08)/STEP_(10303)/
|title=ISO - SC4- Legacy products - STEP (ISO 10303)
|author=ISO
|accessdate=2009-08-05
}}</ref> and to initiate the development of [[ISO 15926]] "Integration of life-cycle data for process plants including oil and gas production facilities."

Over the years, the scope of the standard has increased from the initial capital-intensive projects in the [[Upstream (oil industry)|upstream oil and gas industry]], to include also relevant terminology for [[Downstream (oil industry)|downstream oil and gas industry]] applications and to deal with real-time data related to the actual oil and gas production.

[[ISO 15926]] has also over the years evolved from a dictionary (a list of terms with definitions), over a [[Taxonomy (general)|taxonomy]] (added hierarchy) to an [[Ontology (information science)|ontology]] (a formal representation of a set of concepts within a domain and the relationships between those concepts). [[ISO 15926]] is therefore sometimes nicknamed the "Oil and Gas Ontology.",<ref name="OLFIOOntology">
{{cite web
|url=http://www.olf.no/getfile.php/zKonvertert/www.olf.no/Rapporter/Dokumenter/070919%20IO%20and%20Ontology%20-%20Brosjyre.pdf
|title=Integrated Operations and the Oil & Gas Ontology
|author=[[Norwegian Oil Industry Association]] (OLF)
|accessdate=2009-08-05
}}</ref> for some considered to be an essential prerequisite together with [[Semantic Web]] technologies<ref name="W3C">
{{cite web
|url=http://www.w3.org/2008/12/ogws-report
|title=W3C workshop on Semantic Web in Oil and Gas Industry - Report
|author=[[W3C]]
|date=2009-01-13
|accessdate=2009-08-05
}}</ref>
to get to better interoperability, an optimal use of all available data across boundaries and an increase in efficiency. This is what some call the next generation of [[Integrated Operations]].<ref name="OLFIOOntology"/>

=== Reference data services ===
Placeholders:
* Flow scheme of WIP - RDS - ISO and role of SIGs
* RDS
* Standards in database pilot (ISO)

=== Special interest groups ===
Placeholders:
* Overview of SIGs
* Drilling and Completion
* Reservoir and Production
* Operations and Maintenance

== Projects ==
There are a number of projects (co-)organized by POSC Caesar Association working on the extension of the [[ISO 15926]] standard in different application areas.

=== Capital intensive projects application domain ===
The following projects are running at the moment (August 2009):

* The ADI Project of FIATECH, to build the tools (which will then be made available in the public domain)
* The IDS Project of POSC Caesar Association, to define product models required for data sheets
* A joint collaboration project between FIATECH POSC Caesar Association is the ADI-IDS project is the [[ISO 15926 WIP]]

=== Upstream oil and gas industry application domain ===
The following projects are currently running (August 2009):

* The [[Integrated Operations in the High North|Integrated Operations in the High North (IOHN)]] project is working on extending ISO 15926 to handle real-time data transmission and (pre-)processing to enable the next generation of [[Integrated operations|Integrated Operations]].<ref>
{{cite web
|url=http://www.olf.no/news/norway-takes-a-leading-role-in-next-generation-integrated-operations-article18586-291.html
|title=Norway takes a leading role in next generation Integrated Operations
|author=The [[Norwegian Oil Industry Association]] (OLF)
|accessdate=2009-08-05
|date=2008-08-26
}}</ref><ref name="Rigzone">
{{cite web
|url=http://www.rigzone.com/news/article.asp?a_id=65883
|title=Norway Takes Reign to Provide Next Generation Integrated Operations
|author=Rigzone E&P News
|accessdate=2009-08-05
|date=2008-08-26
}}</ref><ref name="DEJ">
{{cite web
|url=http://www.digitalenergyjournal.com/displaynews.php?NewsID=758&PHPSESSID=9hhp6qe4qqpi7qnbgffgqhv1h7
|title=Norway developing next generation Integrated Operations
|author=Digital Energy Journal
|accessdate=2009-08-05
|date=2008-08-27
}}</ref><ref name="EPMag">
{{cite web
|url=http://www.epmag.com/Magazine/2008/12/item24047.php
|title=Offshore R&D pushes the limits
|author=E&P Magazine
|accessdate=2009-08-05
|date=2008-12-02
}}</ref>
* The Environment Web project to include environmental reporting terms and definitions as used in EPIM's EnvironmentWeb in ISO 15926.

Finalised projects include:

* The Integrated Information Platform (IIP) project working on establishing a real-time information pipeline based on open standards. It worked among others on:
** Daily Drilling Report (DDR) to including all terms and definitions in ISO 15926. This standard became mandatory on February 1, 2008<ref>{{cite web |url=http://www.npd.no/English/Produkter+og+tjenester/Skjemaer/CDRS_reporting_oct_2007.htm |title=Drilling Reporting to the authorities |author=Norwegian Petroleum Directorate |accessdate=2009-08-05 }}</ref> for reporting on the [[Norwegian Continental Shelf]] by the [[Norwegian Petroleum Directorate|Norwegian Petroleum Directorate (NPD)]] and Safety Authority Norway (PSA). NPD says that the quality of the reports has improved considerably since.
** Daily Production Report (DPR) to including all terms and definitions in ISO 15926. This standard was tested successfully on the [[Valhall oil field|Valhall]] ([[BP]]-operated) and Åsgard ([[StatoilHydro]]-operated) fields offshore Norway. The terminology and XML schemata developed have also been included in [http://www.Energistics.org Energistics]’ [[PRODML]] standard.

== Conferences and events ==

=== Semantic Days ===
{{Empty section|date=January 2011}}

=== Sogndal academic network meeting ===
{{Empty section|date=January 2011}}

== Collaborations ==

POSC Caesar is collaborating with a number of standardization bodies,<ref name="PCAColl">
{{cite web
|url=http://www.posccaesar.org/wiki/PCA/Collaboration
|title=POSC Caesar - Collaboration
|author=POSC Caesar Association (PCA)
|accessdate=2009-08-05
}}</ref> including:
* Mimosa: collaboration on open information standards for Operations and Maintenance mainly for the [[Downstream (oil industry)|downstream oil and gas industry]],
* FIATECH: collaboration on open information standards for life cycle data of capital projects<ref name="IDSADI">
{{cite web
|url=http://www.posccaesar.org/wiki/IdsAdiProject
|title=POSC Caesar / FIATECH IDS-ADI Projects
|author=FIATECH & POSC Caesar Association
|accessdate=2009-08-05
}}</ref><ref name="iRing">
{{cite press release
|url=http://fiatech.org/press-releases/364-iring-version100.html
|title=iRING Version 1.0.0 Available Now
|publisher=FIATECH & POSC Caesar Association
|accessdate=2009-08-05
|date=2009-06-05
}}</ref>
* Energistics: collaboration on information standards for the [[Upstream (oil industry)|upstream oil and gas industry]], including [[WITSML]] and [[PRODML]]
* OASIS: collaboration on e-business standards,
* [[ISO TC 184/SC 4|ISO TC184/SC4]]: the host of the ISO 15926 standard.

== See also ==
* [[ISO 15926]]

== References ==
{{Reflist|30em}}

== External links ==
* [http://www.posccaesar.org/ POSC Caesar Association] website

[[Category:Semantic Web]]
[[Category:Knowledge engineering]]
[[Category:Information science]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Standards organizations]]
<=====doc_Id=====>:847
<=====title=====>:
Category:Belief revision
<=====text=====>:
{{Cat main}}
{{see also|WP:WikiProject Philosophy/Resources}}

[[Category:Belief]]
[[Category:Formal epistemology]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Logic programming]]
<=====doc_Id=====>:850
<=====title=====>:
BabelNet
<=====text=====>:
{{Infobox software
 |name = BabelNet
 |logo = [[File:BabelNet_logo.svg|140px|BabelNet logo.]]
 |screenshot =
 |caption = Wikipedia Extraction
 |developer = 
 |released = 
 |latest_release_version = BabelNet 3.7
 |latest_release_date = August 2016
 |operating_system = {{Flatlist|
* [[Virtuoso Universal Server]]
* [[Lucene]]
}}
 |genre = {{Flatlist|
* [[Machine-readable dictionary|Multilingual encyclopedic dictionary]]
* [[Linked data]]
}}
 |programming language = 
 |license = [[Attribution-NonCommercial-ShareAlike 3.0 Unported]]
 |website = {{URL|babelnet.org}}
 |alexa   = 
}}

'''BabelNet''' is a [[Multilinguality|multilingual]] lexicalized [[semantic network]] and [[Ontology (information science)|ontology]] developed at the Linguistic Computing Laboratory in the Department of Computer Science of the [[Sapienza University of Rome]].<ref name="NavigliPonzetto12">R. Navigli and S. P Ponzetto. 2012. [http://dx.doi.org/10.1016/j.artint.2012.07.001 BabelNet: The Automatic Construction, Evaluation and Application of a Wide-Coverage Multilingual Semantic Network]. Artificial Intelligence, 193, Elsevier, pp. 217-250.</ref><ref>R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11–16, 2010, pp. 216–225.</ref> BabelNet was automatically created by linking Wikipedia, to the most popular computational [[Dictionary|lexicon]] of the [[English language]], [[WordNet]]. The integration is performed by means of an automatic mapping and by filling in lexical gaps in resource-poor [[language]]s with the aid of [[statistical machine translation]]. The result is an "encyclopedic dictionary" 
that provides [[concept]]s and [[Named entity|named entities]] [[Lexicalization|lexicalized]] in many languages and connected with large amounts of [[Semantic relation#Relationships|semantic relations]]. Additional lexicalizations and definitions are added by linking to free-license wordnets, [[OmegaWiki]], the English [[Wiktionary]], [[Wikidata]], [[FrameNet]], [[VerbNet]] and others. Similarly to WordNet, BabelNet groups [[word]]s in different languages into sets of [[synonyms]], called ''Babel [[synsets]]''. For each Babel synset, BabelNet provides short definitions (called [[Definition|glosses]]) in many languages harvested from both WordNet and Wikipedia.

[[File:The BabelNet structure.png|thumb|600px|BabelNet is a multilingual semantic network obtained as an integration of WordNet and Wikipedia.]]

==Statistics of BabelNet==

{{As of|2016|08}}, BabelNet (version 3.7) covers 271 [[language]]s, including all European languages, most [[Asian language]]s, and [[Latin]]. BabelNet 3.7 contains almost 14 million synsets and about 746 million [[word sense]]s (regardless of their language). Each Babel synset contains 2 synonyms per language, i.e., word senses, on average. The semantic network includes all the lexico-semantic relations from WordNet ([[Hyponymy|hypernymy and hyponymy]], [[meronymy]] and [[holonymy]], [[antonymy]] and [[synonymy]], etc., totaling around 364,000 relation edges) as well as an underspecified relatedness relation from Wikipedia (totaling around 380 million relation edges).<ref name="NavigliPonzetto12" /> Version 3.7 also associates about 11 million images with Babel synsets and provides a Lemon [[Resource Description Framework|RDF]] encoding of the resource,<ref>M. Ehrmann, F. Cecconi, D. Vannella, J. McCrae, P. Cimiano, R. Navigli. [http://www.lrec-conf.org/proceedings/lrec2014/pdf/810_Paper.pdf Representing Multilingual Data as Linked Data: the Case of BabelNet 2.0]. Proc. of the 9th Language Resources and Evaluation Conference (LREC 2014), Reykjavik, Iceland, 26–31 May 2014.</ref> available via a [[SPARQL endpoint]]. 2.67 million synsets are assigned domain labels.

==Applications==

BabelNet has been shown to enable multilingual [[Natural Language Processing]] applications. The lexicalized [[knowledge]] available in BabelNet has been shown to obtain state-of-the-art results in:

* [[semantic relatedness]]<ref>R. Navigli and S. Ponzetto. 2012. [http://www.aaai.org/ocs/index.php/AAAI/AAAI12/paper/download/5112/5126 BabelRelate! A Joint Multilingual Approach to Computing Semantic Relatedness]. Proc. of the 26th AAAI Conference on Artificial Intelligence (AAAI 2012), Toronto, Canada, pp. 108-114.</ref><ref>J. Camacho-Collados, M. T. Pilehvar and R. Navigli. [http://aclweb.org/anthology/N/N15/N15-1059.pdf NASARI: a Novel Approach to a Semantically-Aware Representation of Items]. Proc. of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2015), Denver, Colorado (US), 31 May-5 June 2015, pp. 567-577.</ref>
* multilingual [[Word Sense Disambiguation]]<ref>R. Navigli and S. Ponzetto. [http://www.aclweb.org/anthology/D12-1128 Joining Forces Pays Off: Multilingual Joint Word Sense Disambiguation]. Proc. of the 2012 Conference on Empirical Methods in Natural Language Processing (EMNLP 2012), Jeju, Korea, July 12–14, 2012, pp. 1399-1410.</ref>
* multilingual Word Sense Disambiguation and [[Entity Linking]] with the [[Babelfy]] system<ref>A. Moro, A. Raganato, R. Navigli. [http://www.transacl.org/wp-content/uploads/2014/05/54.pdf Entity Linking meets Word Sense Disambiguation: a Unified Approach]. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 231-244, 2014.</ref>
* [[game with a purpose|video games with a purpose]]<ref>D. Jurgens, R. Navigli. {{webarchive|url=http://web.archive.org/web/20150103085712/http://www.transacl.org/wp-content/uploads/2014/10/421-camera-ready.pdf |date=January 3, 2015 |title=It's All Fun and Games until Someone Annotates: Video Games with a Purpose for Linguistic Annotation}}. Transactions of the Association for Computational Linguistics (TACL), 2, pp. 449-464, 2014.</ref>

==Prizes and acknowledgments==
BabelNet received the [http://www.meta-net.eu/meta-prize META prize] 2015 for "groundbreaking work in overcoming language barriers through a multilingual lexicalised semantic network and ontology making use of heterogeneous data sources". 

BabelNet featured prominently in a [[TIME magazine]]'s article<ref>Katy Steinmetz. [http://time.com/4327440/redefining-the-modern-dictionary/ Redefining the modern dictionary], TIME magazine, vol. 187, 23 maggio 2016, pp. 20-21.</ref> about the new age of innovative and up-to-date lexical knowledge resources available on the Web. The article describes in some detail how BabelNet is playing a leading role in the 21st century scenario.

==See also==
* [[Babelfy]]
* [[EuroWordNet]]
* [[Knowledge acquisition]]
* [[Linguistic Linked Open Data]]
* [[OmegaWiki]]
* [[Semantic network]]
* [[Semantic relatedness]]
* [[Wikidata]]
* [[Wiktionary]]
* [[Word sense disambiguation]]
* [[Word sense induction]]
* [[UBY]]

== References ==
{{reflist}}

== External links ==
* {{Official website|http://babelnet.org}}
* [http://babelnet.org/sparql SPARQL endpoint]
* [http://www.meta-net.eu/meta-prize META prize]

[[Category:Lexical databases]]
[[Category:Knowledge bases]]
[[Category:Ontology (information science)]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Artificial intelligence]]
[[Category:Online dictionaries]]
[[Category:Multilingualism]]
<=====doc_Id=====>:853
<=====title=====>:
Protégé (software)
<=====text=====>:
{{Infobox software
| name                   = Protégé
| logo                   = <!-- Image name is enough -->
| logo alt               = 
| screenshot             = <!-- Image name is enough -->
| caption                = 
| screenshot alt         = 
| collapsible            = 
| author                 = 
| developer              = [[Stanford University School of Medicine|Stanford]] Center for Biomedical Informatics Research
| released               = {{Start date and age|1999|11|11|df=yes}}<ref name=versions>{{cite web |title=Protege Desktop Older Versions |website=Protege Wiki |date=24 May 2016 |url=http://protegewiki.stanford.edu/wiki/Protege_Desktop_Old_Versions }}</ref>
| discontinued           = 
| latest release version = 5.0.0
| latest release date    = {{Start date and age|2016|05|24|df=yes}}<ref name=versions/>
| latest preview version = 
| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| status                 = Active
| programming language   = [[Java (programming language)|Java]]
| operating system       = Linux, Mac OS X & Windows<ref name=install5>{{cite web |title=Protege Desktop 5.0 Installation Instructions |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Install_Protege5 }}</ref>
| platform               = Java {{abbr|VM|Virtual Machine}}<ref>{{cite web |title= Protege Desktop Frequently Asked Questions  |website=Protege Wiki |url=http://protegewiki.stanford.edu/wiki/Protege-OWL_4_FAQ }}</ref>
| size                   = 
| language               = 
| language count         = <!-- Number only -->
| language footnote      = 
| genre                  = [[Ontology (information science)#Editor|Ontology editor]]
| license                = [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause]]
| alexa                  = 
| website                = {{URL|protege.stanford.edu}}
| repo                   = {{URL|https://github.com/protegeproject/protege}}
| standard               = 
| AsOf                   = 
}}

'''Protégé''' is a free, open source [[Ontology (computer science)|ontology]] editor and a [[knowledge management]] system. Protégé provides a graphic user interface to define ontologies. It also includes [[deductive classifier]]s to validate that models are consistent and to infer new information based on the analysis of an ontology. Like [[Eclipse (software)|Eclipse]], Protégé is a framework for which various other projects suggest plugins. This application is written in [[Java (programming language)|Java]] and heavily uses [[Swing (Java)|Swing]] to create the user interface. Protégé recently has over 300,000 registered users.<ref>[http://protege.stanford.edu/community.php Protégé Community]</ref> According to a 2009 book it is "the leading ontological engineering tool".<ref name="SelicGaševic2009">{{cite book|author1=Dragan Gašević|author2=Dragan Djurić|author3=Vladan Devedžić|title=Model Driven Engineering and Ontology Development|url=https://books.google.com/books?id=s-9yu7ubSykC&pg=PA194|year= 2009|publisher=Springer|isbn=978-3-642-00282-3|pages=194|edition=2nd}}</ref>

Protégé is being developed at [[Stanford University]] and is made available under the [[BSD licenses#2-clause license ("Simplified BSD License" or "FreeBSD License")|BSD 2-clause license]].<ref>{{cite web |title=protege/license.txt |website=GitHub |url=https://github.com/protegeproject/protege/blob/master/license.txt }}</ref> Earlier versions of the tool were developed in collaboration with the [[University of Manchester]].

== References ==
<references/>

== External links ==
* {{Official website|http://protege.stanford.edu/}}
* [http://protegewiki.stanford.edu/wiki/Main_Page Protégé wiki]

{{DEFAULTSORT:Protege (software)}}
[[Category:Knowledge representation]]
[[Category:Free integrated development environments]]
[[Category:Ontology (information science)]]
[[Category:Ontology editors]]
[[Category:Free software programmed in Java (programming language)]]


{{programming-software-stub}}
<=====doc_Id=====>:856
<=====title=====>:
Conceptualization (information science)
<=====text=====>:
[[File:Ontological commitments.png|thumb|200px|Chart showing the relation between a conceptualization in information science, its various ontologies (each with its own specialized language), and their shared ontological commitment.<ref name=CZ>
This figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&pg=PA7&lpg=PA7#v=onepage&q&f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].
</ref>]]
In [[information science]] a '''conceptualization''' is an abstract simplified view of some selected part of the world, containing the objects, concepts, and other entities that are presumed of interest for some particular purpose and the relationships between them.<ref name=Gruber/><ref name=Smith/> An explicit specification of a conceptualization is an [[ontology (information science)|ontology]], and it may occur that a conceptualization can be realized by several distinct ontologies.<ref name=Gruber/>  An ''[[ontological commitment]]'' in describing ontological comparisons is taken to refer to that subset of elements of an ontology shared with all the others.<ref name=Audi/><ref name=Ceccaroni1/>  "An ontology is ''language-dependent''", its objects and interrelations described within the language it uses,  while a conceptualization is always the same, more general, its concepts existing "independently of the language used to describe it".<ref name=Guarino/> The relation between these terms is shown in the figure to the right.

Not all workers in [[knowledge engineering]] use the term ‘conceptualization’, but instead refer to the conceptualization itself, or to the ontological commitment of all its realizations, as an overarching ontology.<ref name=Ceccaroni/>

==Purpose and implementation==
As a higher level abstraction, a conceptualization facilitates the discussion and comparison of its various ontologies, facilitating knowledge sharing and reuse.<ref name=Ceccaroni/><ref name=Harmelen/> Each ontology based upon the same overarching conceptualization maps the conceptualization into specific elements and their relationships.

The question then arises as to how to describe the 'conceptualization' in terms that can encompass multiple ontologies. This issue has been called the '[[Tower of Babel]]' problem, that is, how can persons used to one ontology talk with others using a different ontology?<ref name=Smith/><ref name=Harmelen/> This problem is easily grasped, but a general resolution is not at hand. It can be a 'bottom-up' or a 'top-down' approach, or something in between.<ref name=Alignment/>

However, in more artificial situations, such as information systems, the idea of a 'conceptualization' and the 'ontological commitment' of various ontologies that realize the 'conceptualization' is possible.<ref name=Guarino/><ref name=Guarino1/> The formation of a conceptualization and its ontologies involves these steps:<ref name=Hadzic/>
* specification of the conceptualization
* ontology concepts: every definition involves the definitions of other terms
* relationships between the concepts: this step maps conceptual relationships onto the ontology structure
* groups of concepts: this step may lead to the creation of sub-ontologies
* formal description of ontology commitments, for example, to make them computer readable

An example of moving conception into a language leading to a variety of ontologies is the expression of a process in [[pseudocode]] (a strictly structured form of ordinary language) leading to implementation in several different formal computer languages like [[Lisp (programming language)|Lisp]] or [[Fortran]]. The pseudocode makes it easier to understand the instructions and compare implementations, but the formal languages make possible the compilation of the ideas as computer instructions. {{citation needed|date=August 2013}}

Another example is mathematics, where a very general formulation (the analog of a conceptualization) is illustrated with 'applications' that are more specialized examples. For instance, aspects of a [[function space]] can be illustrated using a [[vector space]] or a [[topological space]] that introduce interpretations of the 'elements' of the conceptualization and additional relationships between them but preserve the connections required in the [[function space]]. {{citation needed|date=August 2013}}

==See also==
*[[Knowledge representation and reasoning]]
*[[Ontology alignment]]
*[[Ontology (computer science)]]
*[[Semantic integration]]
*[[Semantic matching]]
*[[Semantic translation]]

==References==
{{reflist |30em|refs=
<ref name=Alignment>
In information science, one approach to finding a conceptualization (or avoiding it and using an automated comparison) is called 'ontology alignment' or 'ontology matching'. See for example, {{cite book |title=Ontology Matching |url=https://books.google.com/books?id=qYVpA2t2EtQC&printsec=frontcover  |author1=Jérôme. Euzenat |author2=Pavel Shvaiko |isbn=3540496122 |year=2007 |publisher=Springer}}
</ref>

<ref name=Audi>
{{cite book |title=The Cambridge Dictionary of Philosophy |edition=Paperback 2nd |page= 631 |chapter=Ontological commitment |isbn=0521637228 |author= Roger F. Gibson |editor=Robert Audi  |year=1999 |url=https://books.google.com/books?id=kQQNBTW_hoAC&pg=PT1537}} A shortened version of that definition is as follows:
:The ''ontological commitments'' of a theory are those things which occur in all the ''ontologies'' of that theory. To explain further, the [[ontology]] of a theory consists of the objects the theory makes use of. A dependence of a theory upon an object is indicated if the theory fails when the object is omitted. However, the ontology of a theory is not necessarily unique. A theory is ''ontologically committed'' to an object only if that object occurs in ''all'' the ontologies of that theory. A theory also can be ''ontologically committed'' to a class of objects if that class is populated (not necessarily by the same objects) in all its ontologies. [italics added]
</ref>

<!-- Unused ref <ref name=Aydede>
{{cite web |title=The language of thought hypothesis |first=Murat|last=Aydede |work= The Stanford Encyclopedia of Philosophy (Fall 2010 Edition) |editor=Edward N. Zalta |url= http://plato.stanford.edu/archives/fall2010/entries/language-thought/  |date=September 17, 2010}}
</ref> -->

<ref name=Ceccaroni>
For example, see {{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf |journal=Proceedings of the workshop AAMAS |year=2002}}
</ref>

<ref name=Ceccaroni1>
{{cite journal |title= Modeling utility ontologies in agentcities with a collaborative approach |author1=Luigi Ceccaroni |author2=Myriam Ribiere |url=http://ceur-ws.org/Vol-66/oas02-13.pdf  |journal=Proceedings of the workshop AAMAS |year=2002}} A quotation follows:
:“Researchers...come from different areas of study and have different perspectives on modeling, but significantly they pledged to adopt the same ''ontological commitment''. That is, they agree to adopt common, predefined ontologies...to express general categories, even if they do not completely agree on the modeling behind the ontological representations. Where ontological commitment is lacking, it is difficult to converse clearly about a domain and to benefit from knowledge representations developed by others... Ontological commitment is thus an integral aspect of ontological engineering.” [italics added]
</ref>

<!--ref name=CZ>
This figure has similarities with Figure 1 in [http://books.google.ca/books?id=Wf5p3_fUxacC&pg=PA7&lpg=PA7#v=onepage&q&f=false Guarino] and to slide 7 in the talk by [http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf  van Harmelen]. These sources are among the references to this article. The figure is imported from [http://en.citizendium.org/wiki/File:Ontological_commitments.png  Citizendium].
</ref-->

<ref name=Guarino>
{{cite book |title=Formal Ontology in Information Systems (Proceedings of FOIS '98, Trento, Italy) |first=Nicola|last=Guarino |pages=3 ''ff'' |chapter=Formal Ontology in Information Systems |editor=Nicola Guarino |isbn=978-90-5199-399-8 |year=1998 |publisher=IOS Press |url=http://books.google.ca/books?id=Wf5p3_fUxacC&pg=PA7&lpg=PA7}}
</ref>

<ref name=Gruber>
{{cite journal |first=Thomas R. |last=Gruber |authorlink=Tom Gruber |date=June 1993 |url=http://tomgruber.org/writing/ontolingua-kaj-1993.pdf |format=PDF |title=A translation approach to portable ontology specifications |journal=[[Knowledge Acquisition]] |volume=5 |issue=2 |pages=199–220 |doi=10.1006/knac.1993.1008}}
</ref>

<ref name=Guarino1>
{{cite journal |title=Formalizing ontological commitments |author1=Nicola Guarino |author2=Massimiliano Carrara |author3=Pierdaniele Giaretta |journal=AAAI |volume=94 |pages=560–567 |year=1994 |url=http://www.mit.bme.hu/system/files/oktatas/targyak/7412/Formalizing_Ontological_Commitments.pdf}} 
</ref>

<ref name=Hadzic>
{{cite book |title=Ontology-Based Multi-Agent Systems |chapter=Chapter 7: Design methodology for integrated systems - Part I (Ontology design) |pages=111 ''ff'' |author1=Maja Hadzic |author2=Pornpit Wongthongtham |author3=Elizabeth Chang |author4=Tharam Dillon |isbn=364201903X |year=2009 |publisher=Springer |url=https://books.google.com/books?id=kRoA_vxUwvQC&pg=PA111}}
</ref>

<ref name=Harmelen>
{{cite web |title=Ontology mapping: a way out of the medical tower of babel |author=Frank van Harmelen |url=http://www.csd.abdn.ac.uk/aime05/presentations/Ontology%20Mapping%20A%20Way%20out%20of%20the%20Medical%20Tower%20of%20Babel.pdf}}
</ref>

<ref name=Smith>
{{cite book |chapter= Chapter 11: Ontology |first=Barry|last=Smith |url=http://ontology.buffalo.edu/smith/articles/ontology_PIC.pdf |title=Blackwell Guide to the Philosophy of Computing and Information |publisher=Blackwell |year=2003 |pages=155–166 |editor=Luciano Floridi |isbn=0631229183 }}
</ref>

}}

==Further reading==

*{{cite book |chapter=On Ontology, ontologies, conceptualizations, modeling languages and (meta)models |first=G|last=Guizzardi |title=Frontiers in artificial intelligence and applications, databases and information systems IV |editor=Olegas Vaselicas |editor2=Johan Edler |editor3=Albertas Caplinskas, eds |isbn=978-1-58603-715-4|publisher=IOS Press |year=2007 |url=http://www.loa.istc.cnr.it/Guizzardi/FAIA.pdf }}
*{{cite book |title=Applied ontology: an introduction |editor1=Katherine Munn |editor2=Barry Smith |url=https://books.google.com/books?id=vuYLID7IfqEC&printsec=frontcover |isbn=3938793988 |publisher=Ontos Verlag |year=2008}}

==External links==
*{{cite web |url=http://www.obitko.com/tutorials/ontologies-semantic-web/specification-of-conceptualization.html |title=Specification of conceptualization |work=Ontologies and the semantic web |first=Marek|last=Obitko |year=2006–2007}}
{{Citizendium|Ontological commitment#Conceptualization}}

[[Category:Information science]]
[[Category:Ontology]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
[[Category:Ontology (information science)| ]]
[[Category:Semantic Web]]
[[Category:Technical communication]]
<=====doc_Id=====>:859
<=====title=====>:
Type–token distinction
<=====text=====>:
[[File:Flock of birds at Rome.ogg|right|300px|thumb|Although this flock is made of the same ''type'' of bird, each individual bird is a different ''token''. (50 MB video of a [[Flock (birds)|flock of birds]] in Rome)|thumbtime=6]]
The '''type–token distinction''' is used in disciplines such as [[logic]], [[linguistics]], [[metalogic]], [[typography]], and [[computer programming]] to clarify what words mean.

The sentence "''they drive the same car''" is ambiguous. Do they drive the same ''type'' of car (the same model) or the same instance of a car type (a single vehicle)? Clarity requires us to distinguish words that represent abstract types from words that represent objects that embody or exemplify types. The type–token distinction separates types (representing abstract descriptive concepts) from tokens (representing objects that instantiate concepts).

For example: "bicycle" is a type that represents the concept of a bicycle; whereas "my bicycle" is a token that represents an object that instantiates that type. In the sentence "the bicycle is becoming more popular" the word "bicycle" is a type representing a concept; whereas in the sentence "the bicycle is in the garage" the word "bicycle" is a token representing a particular object.

(The distinction in [[computer programming]] between [[class (software)|classes]] and [[object (computer science)|objects]] is related, though in this context, "class" sometimes refers to a set of objects (with class-level attribute or operations) rather than a description of an object in the set.)

The words type, concept, property, quality, feature and attribute (all used in describing things) tend to be used with different verbs. E.g. Suppose a rose bush is defined as a plant that is "thorny", "flowering" and "bushy". You might say a rose bush ''instantiates'' these three types, or ''embodies'' these three concepts, or ''exhibits'' these three properties, or ''possesses'' these three qualities, features or attributes.

Property types (e.g "height in metres" or "thorny") are often understood [[ontology|ontologically]] as concepts. Property instances (e.g. height = 1.74) are sometimes understood as measured values, and sometimes understood as sensations or observations of reality.

Some say types exist in descriptions of objects, but not as tangible [[physical object]]s. They say one can show someone a particular bicycle, but cannot show someone the type "bicycle", as in "''the bicycle'' is popular.". However types do exist in sense that they appear in mental and documented models.

Some say tokens represent objects that are tangible, exist in space and time as physical matter and/or energy. However, tokens can represent intangible objects of types such as "thought", "tennis match", "government" and "act of kindness".

== Occurrences ==
There is a related distinction very closely connected with the type-token distinction. This distinction is the distinction between an object, or type of object, and an '''''occurrence''''' of it. In this sense, an occurrence is not necessarily a token. Considering the sentence: "[[A rose is a rose is a rose]]". We may equally correctly state that there are eight or three words in the sentence. There are, in fact, three word types in the sentence: "rose", "is" and "a". There are eight word tokens in a token copy of the line. The line itself is a type. There are not eight word types in the line. It contains (as stated) only the three word types, 'a', 'is' and 'rose', each of which is unique. So what do we call what there are eight of? They are occurrences of words. There are three occurrences of the word type 'a', two of 'is' and three of 'rose'.

The need to distinguish tokens of types from occurrences of types arises, not just in linguistics, but whenever types of things have other types of things occurring in them.<ref>Stanford Encyclopedia of Philosophy, ''[http://plato.stanford.edu/entries/types-tokens Types and Tokens]''</ref> Reflection on the simple case of occurrences of [[numeral]]s is often helpful.{{citation needed|date=January 2017|reason=Claim made with no support or citation, term used without definition.}}

== Typography ==
In [[typography]], the type–token distinction is used to determine the presence of a text printed by [[movable type]]:<ref>[[Herbert E. Brekle|Brekle, Herbert E.]]: ''Die Prüfeninger Weiheinschrift von 1119. Eine paläographisch-typographische Untersuchung'', Scriptorium Verlag für Kultur und Wissenschaft, Regensburg 2005, ISBN 3-937527-06-0, p.&nbsp;23</ref>

{{quote|The defining criteria which a typographic print has to fulfill is that of the type identity of the various [[letter form]]s which make up the printed text. In other words: each letter form which appears in the text has to be shown as a particular instance ("token") of one and the same type which contains a reverse image of the printed [[Letter (alphabet)|letter]].}}

== Charles Sanders Peirce ==
:''There are only 26 letters in the [[English alphabet]] and yet there are more than 26 letters in this [[sentence (linguistics)|sentence]]. Moreover, every time a child writes the alphabet 26 new letters have been created.''

The word 'letters' was used three times in the above paragraph, each time in a different meaning. The word 'letters' is one of many words having "type–token ambiguity". This section disambiguates 'letters' by separating the three senses using terminology standard in logic today. The key distinctions were first made by the American logician-philosopher [[Charles Sanders Peirce]] in 1906 using terminology that he established.<ref>Charles Sanders Peirce, Prolegomena to an apology for pragmaticism, Monist, vol.16 (1906), pp. 492–546.</ref>

The letters that are created by writing are physical objects that can be destroyed by various means: these are letter TOKENS or letter INSCRIPTIONS. The 26 letters of the alphabet are letter TYPES or letter FORMS.

'''Peirce's type–token distinction''', also applies to words, sentences, paragraphs, and so on: to anything in a universe of discourse of character-string theory, or [[concatenation theory]]. There is only one word type spelled el-ee-tee-tee-ee-ar,<ref>Using a variant of [[Alfred Tarski]]'s structural-descriptive naming found in [[John Corcoran (logician)|John Corcoran]] , Schemata: the Concept of Schema in the History of Logic, Bulletin of Symbolic Logic, vol. 12 (2006), pp. 219–40.</ref> namely, 'letter'; but every time that word type is written, a new word token has been created.

Some logicians consider a word type to be the class of its tokens. Other logicians counter that the word type has a permanence and constancy not found in the class of its tokens. The type remains the same while the class of its tokens is continually gaining new members and losing old members.

The word type 'letter' uses only four letter types: el, ee, tee, and ar. Nevertheless, it uses ee twice and tee twice. In standard terminology, the word type 'letter' has six letter OCCURRENCES and the letter type ee OCCURS twice in the word type 'letter'. Whenever a word type is inscribed, the number of letter tokens created equals the number of letter occurrences in the word type.

Peirce's original words are the following.
"A common mode of estimating the amount of matter in a ... printed book is to count the number of words. There will ordinarily be about twenty 'thes' on a page, and, of course, they count as twenty words. In another sense of the word 'word,' however, there is but one word 'the' in the English language; and it is impossible that this word should lie visibly on a page, or be heard in any voice .... Such a ... Form, I propose to term a Type. A Single ... Object ... such as this or that word on a single line of a single page of a single copy of a book, I will venture to call a Token. .... In order that a Type may be used, it has to be embodied in a Token which shall be a sign of the Type, and thereby of the object the Type signifies." – Peirce 1906, Ogden-Richards, 1923, 280-1.

These distinctions are subtle but solid and easy to master. This section ends using the new terminology to disambiguate the first paragraph.

:''There are 26 letter types in the English alphabet and yet there are more than 26 letter occurrences in this sentence type. Moreover, every time a child writes the alphabet 26 new letter tokens have been created.''

== See also ==
* [[Formalism (philosophy)]]
* [[Is-a]]
* [[Class (philosophy)]]
* [[Type theory]]
* [[Type physicalism]]
* [[Mental model]]
* [[Map–territory relation]]
* [[Problem of universals#Peirce]]

==References==
{{reflist}}

===Sources===
*Baggin J., and Fosl, P. (2003) ''The Philosopher's Toolkit''. Blackwell: 171-73. ISBN 978-0-631-22874-5.
*Peper F., Lee J., Adachi S.,Isokawa T. (2004) ''Token-Based Computing on Nanometer Scales'', Proceeding of the ToBaCo 2004 Workshop on Token Based Computing, Vol.1 pp.&nbsp;1–18.

== External links ==
*[[The Stanford Encyclopedia of Philosophy]]: "[http://plato.stanford.edu/entries/types-tokens/ Types and Tokens]" by Linda Wetzel.

{{Metalogic}}
{{Metaphysics}}

{{DEFAULTSORT:Type-token distinction}}
[[Category:Metalogic]]
[[Category:Conceptual distinctions]]
[[Category:Knowledge representation]]
[[Category:Abstraction]]
[[Category:Concepts in metaphysics]]
[[Category:Articles containing video clips]]
[[Category:Philosophy of logic]]
[[Category:Philosophy of language]]
[[Category:Linguistics]]
<=====doc_Id=====>:862
<=====title=====>:
Category:Dewey Decimal Classification
<=====text=====>:
{{catmore}}

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:OCLC]]
<=====doc_Id=====>:865
<=====title=====>:
Knowledge representation and reasoning
<=====text=====>:
'''Knowledge representation and reasoning''' ('''KR''') is the field of [[artificial intelligence]] (AI) dedicated to representing information about the world in a form that a computer system can utilize to solve complex tasks such as diagnosing a medical condition or having a dialog in a [[natural language]]. Knowledge representation incorporates findings from psychology{{citation needed|date=February 2016}} about how humans solve problems and represent knowledge in order to design [[Formalism (mathematics)|formalisms]] that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from [[logic]] to automate various kinds of ''reasoning'', such as the application of rules or the relations of [[Set theory|sets]] and [[subset]]s.

Examples of knowledge representation formalisms include [[Semantic network|semantic nets]], [[systems architecture]], [[Frame (artificial intelligence)|Frames]], Rules, and [[Ontology (information science)|ontologies]]. Examples of [[automated reasoning]] engines include [[inference engine]]s, [[automated theorem proving|theorem prover]]s, and classifiers.

== History ==

The earliest work in computerized knowledge representation was focused on general problem solvers such as the [[General Problem Solver]] (GPS) system developed by [[Allen Newell]] and [[Herbert A. Simon]] in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.

In these early days of AI, general search algorithms such as [[A*]] were also developed.  However, the amorphous problem definitions for systems such as GPS meant that they worked only for very constrained toy domains (e.g. the "[[blocks world]]"). In order to tackle non-toy problems, AI researchers such as [[Ed Feigenbaum]] and [[Rick Hayes-Roth|Frederick Hayes-Roth]] realized that it was necessary to focus systems on more constrained problems.

It was the failure of these efforts that led to the [[cognitive revolution]] in psychology and to the phase of AI focused on knowledge representation that resulted in [[expert systems]] in the 1970s and 80s, [[Production system (computer science)|production systems]], [[frame language]]s, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.

Expert systems gave us the terminology still in use today where AI systems are divided into a Knowledge Base with facts about the world and rules and an inference engine that applies the rules to the [[knowledge base]] in order to answer questions and solve problems. In these early systems the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.<ref>{{cite book|last=Hayes-Roth|first=Frederick|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}</ref>

In addition to expert systems, other researchers developed the concept of [[Frame language|Frame based languages]] in the mid 1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. [[natural language understanding|understanding natural language]] and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.

It wasn't long before the frame communities and the rule-based researchers realized that there was synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined Frames and Rules. One of the most powerful and well known was the 1983 [[Knowledge Engineering Environment]] (KEE) from [[IntelliCorp (software)|Intellicorp]]. KEE had a complete rule engine with [[forward chaining|forward]] and [[backward chaining]]. It also had a complete frame based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from [[Symbolics]], [[Xerox]], and [[Texas Instruments]].<ref>{{cite journal|last=Mettrey|first=William|title=An Assessment of Tools for Building Large Knowledge-Based Systems|journal=AI Magazine|year=1987|volume= 8| issue = 4|url=http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625}}</ref>

The integration of Frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time as this was occurring, there was another strain of research which was less commercially focused and was driven by mathematical logic and automated theorem proving.  One of the most influential languages in this research was the [[KL-ONE]] language of the mid 80's. KL-ONE was a [[frame language]] that had a rigorous semantics, formal definitions for concepts such as an [[Is-a|Is-A relation]].<ref>{{cite journal|last=Brachman|first=Ron|title=A Structural Paradigm for Representing Knowledge|journal=Bolt, Beranek, and Neumann Technical Report|year=1978|issue=3605}}</ref> KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref>

Another area of knowledge representation research was the problem of common sense reasoning.  One of the first realizations from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent.  Basic principles of common sense physics, causality, intentions, etc. An example is the [[Frame problem]], that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world it is essential to represent this kind of knowledge. One of the most ambitious programs to tackle this problem was Doug Lenat's [[Cyc]] project. Cyc established its own Frame language and had large numbers of analysts document various areas of common sense reasoning in that language. The knowledge recorded in Cyc included common sense models of time, causality, physics, intentions, and many others.<ref>{{cite book|last=Lenat|first=Doug|title=Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project|publisher=Addison-Wesley|isbn=978-0201517521|author2=R. V. Guha |date=January 1990}}</ref>

The starting point for knowledge representation is the ''knowledge representation hypothesis'' first formalized by [[Brian Cantwell Smith|Brian C. Smith]] in 1985:<ref>{{cite book|last=Smith|first=Brian C.|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=31–40|editor=Ronald Brachman and Hector J. Levesque|chapter=Prologue to Reflections and Semantics in a Procedural Language}}</ref>

<blockquote>''Any mechanically embodied intelligent process will be {{sic|comprised |hide=y|of}} structural ingredients that a) we as external observers naturally take to represent a propositional account of the knowledge that the overall process exhibits, and b) independent of such external semantic attribution, play a formal but causal and essential role in engendering the behavior that manifests that knowledge.''</blockquote>

Currently one of the most active areas of knowledge representation research are projects associated with the [[Semantic web]]. The semantic web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the semantic web creates large [[ontologies]] of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future semantic web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.

Recent projects funded primarily by the [[Defense Advanced Research Projects Agency]] (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The [[Resource Description Framework]] (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The [[Web Ontology Language]] (OWL) provides additional levels of semantics and enables integration with classification engines.<ref>{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}</ref><ref>{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}</ref>

== Overview ==
Knowledge-representation is the field of artificial intelligence that focuses on designing computer representations that capture information about the world that can be used to solve complex problems. The justification for knowledge representation is that conventional [[procedural code]] is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in [[expert systems]].

For example, talking to experts in terms of business rules rather than code lessens the semantic gap between users and developers and makes development of complex systems more practical.

Knowledge representation goes hand in hand with [[automated reasoning]] because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all [[knowledge representation language]]s have a reasoning or inference engine as part of the system.<ref>{{cite book|last=Hayes-Roth|first=Frederick|pages=6–7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|author2=Donald Waterman |author3=Douglas Lenat }}</ref>

A key trade-off in the design of a knowledge representation formalism is that between expressivity and practicality. The ultimate knowledge representation formalism in terms of expressive power and compactness is First Order Logic (FOL).  There is no more powerful formalism than that used by mathematicians to define general propositions about the world. However, FOL has two drawbacks as a knowledge representation formalism:  ease of use and practicality of implementation.  First order logic can be intimidating even for many software developers. Languages which do not have the complete formal power of FOL can still provide close to the same expressive power with a user interface that is more practical for the average developer to understand. The issue of practicality of implementation is that FOL in some ways is too expressive. With FOL it is possible to create statements (e.g. quantification over infinite sets) that would cause a system to never terminate if it attempted to verify them.

Thus, a subset of FOL can be both easier to use and more practical to implement. This was a driving motivation behind rule-based expert systems. IF-THEN rules provide a subset of FOL but a very useful one that is also very intuitive.  The history of most of the early AI knowledge representation formalisms; from databases to semantic nets to theorem provers and production systems can be viewed as various design decisions on whether to emphasize expressive power or computability and efficiency.<ref>{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}</ref>

In a key 1993 paper on the topic, Randall Davis of [[Massachusetts Institute of Technology|MIT]] outlined five distinct roles to analyze a knowledge representation framework:<ref>{{cite journal|last=Davis|first=Randall|author2=Howard Shrobe |author3=Peter Szolovits |title=What Is a Knowledge Representation?|journal=AI Magazine|date=Spring 1993|volume=14|issue=1|pages=17–33|url=http://www.aaai.org/ojs/index.php/aimagazine/article/view/1029/947}}</ref> 
* A knowledge representation (KR) is most fundamentally a surrogate, a substitute for the thing itself, used to enable an entity to determine consequences by thinking rather than acting, i.e., by reasoning about the world rather than taking action in it.
* It is a set of ontological commitments, i.e., an answer to the question: In what terms should I think about the world?
* It is a fragmentary theory of intelligent reasoning, expressed in terms of three components: (i) the representation's fundamental conception of intelligent reasoning; (ii) the set of inferences the representation sanctions; and (iii) the set of inferences it recommends.
* It is a medium for pragmatically efficient computation, i.e., the computational environment in which thinking is accomplished. One contribution to this pragmatic efficiency is supplied by the guidance a representation provides for organizing information so as to facilitate making the recommended inferences.
* It is a medium of human expression, i.e., a language in which we say things about the world."

Knowledge representation and reasoning are a key enabling technology for the [[Semantic web]]. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today it will be possible to define logical queries and find pages that map to those queries.<ref>{{cite journal|last=Berners-Lee|first=Tim |author2=James Hendler |author3=Ora Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}</ref> The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the [[Subsumption relation|subsumption]] relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever changing and evolving information space of the Internet.<ref>{{cite web|last=Macgregor|first=Robert|title=Retrospective on Loom|url=http://www.isi.edu/isd/LOOM/papers/macgregor/Loom_Retrospective.html|work=isi.edu|publisher=Information Sciences Institute|accessdate=10 December 2013|date=August 13, 1999}}</ref>

The Semantic web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The [[Resource Description Framework]] (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The [[Web Ontology Language]] (OWL) adds additional semantics and integrates with automatic classification reasoners.<ref>{{cite web|url=http://www.w3.org/2001/sw/BestPractices/SE/ODSD/|title=A Semantic Web Primer for Object-Oriented Software Developers|last1=Knublauch|first1=Holger|last2=Oberle|first2=Daniel|last3=Tetlow|first3=Phil|last4=Wallace|first4=Evan|publisher=[[W3C]]|date=2006-03-09|accessdate=2008-07-30}}</ref>

== Characteristics ==
In 1985, Ron Brachman categorized the core issues for knowledge representation as follows:<ref>{{cite book|last=Brachman|first=Ron|title=Readings in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages=XVI-XVII|editor=Ronald Brachman and Hector J. Levesque|chapter=Introduction}}</ref> 
*Primitives. What is the underlying framework used to represent knowledge? [[Semantic network]]s were one of the first knowledge representation primitives. Also, data structures and algorithms for general fast search. In this area there is a strong overlap with research in data structures and algorithms in computer science. In early systems the Lisp programming language which was modeled after the [[lambda calculus]] was often used as a form of functional knowledge representation. Frames and Rules were the next kind of primitive. Frame languages had various mechanisms for expressing and enforcing constraints on frame data. All data in frames are stored in slots. Slots are analogous to relations in entity-relation modeling and to object properties in object-oriented modeling. Another technique for primitives is to define languages that are modeled after [[First Order Logic]] (FOL). The most well known example is Prolog but there are also many special purpose theorem proving environments. These environments can validate logical models and can deduce new theories from existing models. Essentially they automate the process a logician would go through in analyzing a model. Theorem proving technology had some specific practical applications in the areas of software engineering. For example, it is possible to prove that a software program rigidly adheres to a formal logical specification.
*Meta-Representation.  This is also known as the issue of [[Reflection (computer programming)|reflection]] in computer science. It refers to the capability of a formalism to have access to information about its own state. An example would be the meta-object protocol in [[Smalltalk]] and [[CLOS]] that gives developers run time access to the class objects and enables them to dynamically redefine the structure of the knowledge base even at run time. Meta-representation means the knowledge representation language is itself expressed in that language. For example, in most Frame based environments all frames would be instances of a frame class. That class object can be inspected at run time so that the object can understand and even change its internal structure or the structure of other parts of the model. In rule-based environments the rules were also usually instances of rule classes. Part of the meta protocol for rules were the meta rules that prioritized rule firing. 
*[[Completeness (logic)|Incompleteness]]. Traditional logic requires additional axioms and constraints to deal with the real world as opposed to the world of mathematics. Also, it is often useful to associate degrees of confidence with a statement. I.e., not simply say "Socrates is Human" but rather "Socrates is Human with confidence 50%". This was one of the early innovations from [[expert system]]s research which migrated to some commercial tools, the ability to associate certainty factors with rules and conclusions. Later research in this area is known as [[Fuzzy Logic]].<ref>{{cite journal|last=Bih|first=Joseph|title=Paradigm Shift: An Introduction to Fuzzy Logic|journal=IEEE POTENTIALS|year=2006|url=http://www.cse.unr.edu/~bebis/CS365/Papers/FuzzyLogic.pdf|accessdate=24 December 2013}}</ref> 
*Definitions and [[Universals]] vs. facts and defaults.  Universals are general statements about the world such as "All humans are mortal". Facts are specific examples of universals such as "Socrates is a human and therefore mortal". In logical terms definitions and universals are about universal quantification while facts and defaults are about existential quantifications. All forms of knowledge representation must deal with this aspect and most do so with some variant of set theory, modeling universals as sets and subsets and definitions as elements in those sets. 
*[[Non-monotonic logic|Non-Monotonic reasoning]]. Non-monotonic reasoning allows various kinds of hypothetical reasoning. The system associates facts asserted with the rules and facts used to justify them and as those facts change updates the dependent knowledge as well. In rule based systems this capability is known as a [[truth maintenance system]].<ref>{{cite journal|last=Zlatarva|first=Nellie|title=Truth Maintenance Systems and their Application for Verifying Expert System Knowledge Bases|journal=Artificial Intelligence Review|year=1992|volume=6|pages=67–110|url=http://link.springer.com/article/10.1007%2FBF00155580#page-2|accessdate=25 December 2013|doi=10.1007/bf00155580}}</ref> 
*[[Functional completeness|Expressive Adequacy]]. The standard that Brachman and most AI researchers use to measure expressive adequacy is usually First Order Logic (FOL). Theoretical limitations mean that a full implementation of FOL is not practical. Researchers should be clear about how expressive (how much of full FOL expressive power) they intend their representation to be.<ref>{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|pages = 41–70|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning}}</ref>
*Reasoning Efficiency. This refers to the run time efficiency of the system. The ability of the knowledge base to be updated and the reasoner to develop new inferences in a reasonable period of time. In some ways this is the flip side of expressive adequacy. In general the more powerful a representation, the more it has expressive adequacy, the less efficient its [[automated reasoning]] engine will be. Efficiency was often an issue, especially for early applications of knowledge representation technology. They were usually implemented in interpreted environments such as Lisp which were slow compared to more traditional platforms of the time.

== Ontology engineering ==
{{main article|Ontology engineering|Ontology language}}

In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic but medical diagnosis of certain kinds of diseases.

As knowledge-based technology scaled up the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common sense knowledge. In designing an artificial intelligence agent it was soon realized that representing common sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as [[CycL]].

After CycL, a number of [[ontology language]]s have been developed.  Most are [[declarative language]]s, and are either [[frame language]]s, or are based on [[first-order logic]]. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, "Every ontology is a treaty- a social agreement among people with common motive in sharing." There are always many competing and differing views that make any general purpose ontology impossible. A general purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.<ref>Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (3rd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, p. 437-439</ref>

There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids,<ref>Hayes P, Naive physics I: Ontology for liquids. University of Essex report, 1978, Essex, UK.</ref> the lumped element model widely used in representing electronic circuits (e.g.,<ref>Davis R, Shrobe H E, Representing Structure and Behavior of Digital Hardware, IEEE Computer, Special Issue on Knowledge Representation, 16(10):75-82.</ref>), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.
The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.

Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.

The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., [[MYCIN]]) looks substantially different from the same task viewed in terms of frames (e.g., [[INTERNIST]]). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.

== See also ==
* [[Chunking (psychology)]]
* [[Commonsense knowledge base]]
* [[Personal knowledge base]]
* [[Valuation-based system]]
* [[Conceptual Graph]]

== References ==

<references/>

== Further reading ==
* [[Ronald J. Brachman]]; [http://citeseer.nj.nec.com/context/177306/0 What IS-A is and isn't. An Analysis of Taxonomic Links in Semantic Networks]; IEEE Computer, 16 (10); October 1983
* [[Ronald J. Brachman]], [[Hector J. Levesque]] ''Knowledge Representation and Reasoning'', Morgan Kaufmann, 2004 ISBN 978-1-55860-932-7
* [[Ronald J. Brachman]], [[Hector J. Levesque]] (eds) ''Readings in Knowledge Representation'', Morgan Kaufmann, 1985, ISBN 0-934613-01-X
* Chein, M., Mugnier, M.-L. (2009),''[http://www.lirmm.fr/gbkrbook/ Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs]'', Springer, 2009,ISBN 978-1-84800-285-2.
* Randall Davis, Howard Shrobe, and Peter Szolovits; [http://citeseer.ist.psu.edu/davis93what.html What Is a Knowledge Representation?] AI Magazine, 14(1):17-33,1993
* [[Ronald Fagin]], [[Joseph Y. Halpern]], [[Yoram Moses]], [[Moshe Y. Vardi]] ''Reasoning About Knowledge'', MIT Press, 1995, ISBN 0-262-06162-7
* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57
* Hermann Helbig: ''Knowledge Representation and the Semantics of Natural Language'', Springer, Berlin, Heidelberg, New York 2006
* Arthur B. Markman: ''Knowledge Representation''  Lawrence Erlbaum Associates, 1998
* [[John F. Sowa]]: ''Knowledge Representation'': Logical, Philosophical, and Computational Foundations. Brooks/Cole: New York, 2000
* Adrian Walker, Michael McCord, [[John F. Sowa]], and Walter G. Wilson: ''Knowledge Systems and Prolog'', Second Edition, Addison-Wesley, 1990

== External links ==
* [http://medg.lcs.mit.edu/ftp/psz/k-rep.html What is a Knowledge Representation?] by Randall Davis and others
* [http://www.makhfi.com/KCM_intro.htm Introduction to Knowledge Modeling] by Pejman Makhfi
* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics course] by Enrico Franconi, Faculty of Computer Science, Free University of Bolzano, Italy
* [http://www.ccl.kuleuven.ac.be/LKR/html/datr.html DATR Lexical knowledge representation language]
* [http://www.isi.edu/isd/LOOM/LOOM-HOME.html Loom Project Home Page]
* [http://www.research.att.com/sw/tools/classic/tm/ijcai-95-with-scenario.html Description Logic in Practice: A CLASSIC Application]
* [http://www.dfki.uni-kl.de/ruleml/ The Rule Markup Initiative]
* [http://nelements.org Nelements KOS] - a non-free 3d knowledge representation system

{{Computer science}}
{{computable knowledge}}
{{Commons category|Knowledge representation}}

{{DEFAULTSORT:Knowledge Representation And Reasoning}}
[[Category:Knowledge representation| ]]
[[Category:Scientific modeling]]
[[Category:Programming paradigms]]
[[Category:Reasoning]]
<=====doc_Id=====>:868
<=====title=====>:
East Pole–West Pole divide
<=====text=====>:
{{redirect|West Pole|the album by The Gathering|The West Pole|the location in Texas|The West Pole, Texas}}

The '''East Pole–West Pole divide''' in the fields of [[cognitive psychology]] and [[cognitive neuroscience]] is an intellectual schism between researchers subscribing to the [[psychological nativism|nativist]] and [[empiricism|empiricist]] schools of thought.  The term arose from the fact that much of the theory and research supporting [[psychological nativism|nativism]], [[modularity of mind]], and [[computational theory of mind]] originated at several universities located on the East Coast, including [[Harvard University]], the [[University of Michigan]], [[Massachusetts Institute of Technology]], and [[Tufts University]]. Conversely, much of the research and theory supporting [[empiricism]], [[emergentism]], and [[embodied cognition]] originated at several universities located on the West Coast, including the [[University of California, Berkeley]], the [[Salk Institute]], and, most notably, the [[University of California, San Diego]].  In reality, the divide is not so clear, with many universities and scholars on both coasts (as well as the Midwest and around the world) supporting each position, as well as more moderate positions in between the two extremes.  The phrase was coined by [[Jerry Fodor]] at an [[MIT]] conference on [[cognition]], at which he referred to another researcher as a "West Coast theorist," apparently unaware that the researcher worked at [[Yale University]].<ref>{{cite book |title=The Blank Slate:The Modern Denial of Human Nature |last=Pinker |first=Steven |authorlink=Steven Pinker |year=2003 |publisher=Penguin |location=New York |isbn=978-0-14-200334-3 }}</ref>

Very few researchers adhere strictly to the extreme positions highlighted by the East Pole–West Pole debate.  That is, there are very few empiricists who believe in the [[John Locke|Lockean]] ideal of the ''[[tabula rasa]]'' (namely, that children are born with no innate knowledge or constraints), and there are very few nativists who agree with [[Jerry Fodor|Fodor's]] assertion that all concepts that are learned over the course of life are present in the mind prior to birth.  Nevertheless, most scholars within the fields of [[cognitive science]] and [[developmental psychology]] affiliate themselves with one of the two positions through the means of their research.

The two books best known for espousing the empiricist and nativist positions within the context of cognitive psychology are ''[[Rethinking Innateness]]'' by [[Jeffrey Elman]] et al. and ''[[The Modularity of Mind]]'' by [[Jerry Fodor]], respectively.  Incidentally, the authors are affiliated with the two institutions on which the East Pole–West Pole metaphor is based, [[UCSD]] and [[MIT]], affirming the relevance and pervasiveness of this moniker for the intellectual divide.

==Notable scholars with affiliations==

{{Col-begin}}
{{Col-break}}
'''Nativists'''
*[[Jerry Fodor]], [[Massachusetts Institute of Technology]]
*[[Steven Pinker]], [[Harvard University]]
*[[Lila R. Gleitman]], [[University of Pennsylvania]]
*[[Leda Cosmides]], [[University of California, Santa Barbara]]
*[[Elizabeth Spelke]], [[Harvard University]]
*[[Thomas Bever]], [[University of Arizona]]
*[[Daniel Dennett]], [[Tufts University]]
*[[Nancy Kanwisher]], [[Massachusetts Institute of Technology]]

{{Col-break}}
'''Empiricists'''
*[[Elizabeth Bates]], [[University of California, San Diego]]
*[[George Lakoff]], [[University of California, Berkeley]]
*[[Brian MacWhinney]], [[Carnegie Mellon University]]
*[[Jeffrey Elman]], [[University of California, San Diego]]
*[[Ronald Langacker]], [[University of California, San Diego]]
*[[Dan Slobin]], [[University of California, Berkeley]]
*[[David Rumelhart]], [[Stanford University]]
*[[James McClelland (psychologist)|James McClelland]], [[Stanford University]]

{{col-end}}

==See also==
*[[Nature and nurture]]
*[[Empiricism]]
*[[Psychological nativism]]
*[[Computational theory of mind]]
*[[Embodied cognition]]
*[[Reductionism]]
*[[Emergentism]]

==References==
{{reflist}}

==External links==
*[http://query.nytimes.com/gst/fullpage.html?res=9B05E4DA1230F937A35752C1A961958260&sec=&spon=&pagewanted=all Recipe for a Brain: Cups of genes or a dash of experience? NY Times article]
*[http://www.edge.org/3rd_culture/lakoff/lakoff_p4.html George Lakoff's discussion of the philosophical roots of embodied cognition]

{{DEFAULTSORT:East Pole-West Pole divide}}
[[Category:Cognition]]
[[Category:Cognitive science]]
[[Category:Knowledge representation]]
[[Category:Arguments in philosophy of mind]]
<=====doc_Id=====>:871
<=====title=====>:
Universal Decimal Classification
<=====text=====>:
The '''Universal Decimal Classification''' ('''UDC''') is a bibliographic and [[library classification]] developed by the [[Belgium|Belgian]] bibliographers [[Paul Otlet]] and [[Henri La Fontaine]] at the end of the 19th century. They worked with numerous subject specialists, for example, [[Herbert Haviland Field]] at the [[Concilium Bibliographicum]] for Zoology. UDC provides a systematic arrangement of all branches of human knowledge organized as a coherent system in which knowledge fields are related and inter-linked.<ref name="UDC Fact Sheet">[http://www.udcc.org/index.php/site/page?view=factsheet UDC Fact Sheet], UDC Consortium website</ref><ref>[McIlwaine, I. C. "Universal Decimal Classification: a guide to its use. Revised ed. The Hague: UDC Consortium, 2007]</ref><ref>[http://www.tandfonline.com/doi/abs/10.1081/E-ELIS3-120043532 McIlwaine, I. C. (2010) Universal Decimal Classification (UDC). In: Encyclopedia of Library and Information Sciences. 3rd ed. New York: Taylor & Francis, 2010. Vol. 1:1, pp. 5432-5439. DOI: 10.1081/E-ELIS3-120043532]</ref><ref>Broughton, V: Universal Decimal Classification - chapters 18 and 19. IN: Essential Classification. London: Facet Publishing, 2004, pp. 207-256</ref>

Originally based on the [[Dewey Decimal Classification]], the UDC was developed as a new analytico-synthetic classification system with a significantly larger vocabulary and syntax that enables very detailed content indexing and information retrieval in large collections.<ref name="UDC History">[http://www.udcc.org/index.php/site/page?view=about_history UDC History], "About UDC" - UDC Consortium website</ref><ref>[http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199704%2948:4%3C331::AID-ASI6%3E3.0.CO;2-X/abstract McIlwaine, I. C. (1997) The Universal Decimal Classification: Some factors concerning its origins, development, and influence. Journal of the American Society for Information Science, 48 (4), pp. 331–339]</ref> In its first edition in 1905, the UDC already included many features that were revolutionary in the context of knowledge classifications: tables of generally applicable (aspect-free) concepts—called common auxiliary tables; a series of special auxiliary tables with specific but re-usable attributes in a particular field of knowledge; an expressive notational system with connecting symbols and syntax rules to enable coordination of subjects and the creation of a documentation language proper. Albeit originally designed as an indexing and retrieval system, due to its logical structure and scalability, UDC has become one of the most widely used knowledge organization systems in libraries, where it is used for either shelf arrangement, content indexing or both.<ref>[http://hdl.handle.net/10150/105685 Slavic, A. (2004) UDC implementation: from library shelves to a structured indexing language. International Cataloguing and Bibliographic Control , 33 3(2004), 60-65.]</ref> UDC codes can describe any type of document or object to any desired level of detail. These can include textual documents and other media such as [[film]]s, [[video]] and [[sound]] recordings, [[illustration]]s, [[map]]s as well as [[realia (library science)|realia]] such as [[museum]] objects.

Since the first edition in French "Manuel du Répertoire bibliographique universel" (1905), UDC has been translated and published in various editions in 40 languages.<ref>[http://www.udcc.org/index.php/site/page?view=editions UDC Editions], UDC Consortium website</ref><ref>[http://hdl.handle.net/10150/106363 Slavic, A. (2004) UDC Translations: a 2004 Survey Report and Bibliography. Extensions & Corrections to the UDC, 26 (2004): 58-80. ]</ref> UDC Summary, an abridged Web version of the scheme is available in over 50 languages.<ref name="UDCS" /> The classification has been modified and extended over the years to cope with increasing output in all areas of human knowledge, and is still under continuous review to take account of new developments.<ref>[http://www.udcc.org/index.php/site/page?view=major_revisions Major Revisions of the UDC 1993-2013], UDC Consortium website</ref><ref>[http://hdl.handle.net/10150/105220 Slavic, A., Cordeiro, M. I. & Riesthuis, G. (2008) Maintenance of the Universal Decimal Classification: overview of the past and preparations for the future. International Cataloguing and Bibliographic Control, 37 (2), 23-29.]</ref>

== The application of UDC ==

UDC is used in around 150,000 libraries in 130 countries and in many bibliographical services which require detailed content indexing. In a number of countries it is the main classification system for information exchange and is used in all type of libraries: public, school, academic and special libraries.<ref>[http://hdl.handle.net/10150/105579 Slavic, A. (2008) Use of the Universal Decimal Classification: a worldwide survey. Journal of Documentation, 64 (2), 2008: 211-228. ]</ref><ref name="UDC Users Worldwide">[http://www.udcc.org/index.php/site/page?view=users_worldwide  UDC Users Worldwide], UDC Consortium website</ref><ref name="UDC Countries">[http://www.udcc.org/countries.htm UDC Countries], UDC Consortium website</ref>

UDC is also used in national bibliographies of around 30 countries. Examples of large databases indexed by UDC include:<ref name="Large collections">[http://www.udcc.org/index.php/site/page?view=collections Collections indexed by UDC], UDC Consortium website</ref> 
: NEBIS (The Network of Libraries and Information Centers in Switzerland) – 2.6 million records
: COBIB.SI (Slovenian National Union Catalogue) – 3.5 million records
: Hungarian National Union Catalogue (MOKKA) – 2.9 million records
: [[VINITI RAS]] database (All-Russian Scientific and Technical Information Institute of Russian Academy of Science) with 28 million records
: Meteorological & Geoastrophysical Abstracts (MGA) with 600 journal titles
: PORBASE (Portuguese National Bibliography) with 1.5 million records

UDC has traditionally been used for the indexing of scientific articles which was an important source of information of scientific output in the period predating electronic publishing. Collections of research articles in many countries covering decades of scientific output contain UDC codes.  Examples of journal articles indexed by UDC:
:UDC code '''663.12:57.06''' in the article "Yeast Systematics: from Phenotype to Genotype" in the  journal ''Food Technology and Biotechnology'' ({{ISSN|1330-9862}})<ref>[http://www.ftb.pbf.hr/index.php/ftb/article/viewFile/243/241 Example: Journal article indexed by UDC] ({{ISSN|1330-9862}})</ref>
:UDC code '''37.037:796.56''', provided in the article "The game method as means of interface of technical-tactical and psychological preparation in sports orienteering" in the Russian journal "''Pedagogico-psychological and medico-biological problems of the physical culture and sport''"  ({{ISSN|2070-4798}}).<ref>[http://www.kamgifk.ru/magazin/20_%283%29_2011/20_%283%29_2011_16.pdf Example: Journal article indexed by UDC] ({{ISSN|2070-4798}})</ref>
:UDC code '''621.715:621.924:539.3''' in the article Residual Stress in Shot-Peened Sheets of AIMg4.5Mn Alloy - in the journal ''Materials and technology'' ({{ISSN|1580-2949}}).<ref>[http://www.docstoc.com/docs/5320753/UDK-Pregledni-znanstveni-lanek-ISSN-MTAEC-M-MI-OVI Example: Journal article indexed by UDC] ({{ISSN|1580-2949}})</ref>
:
The design of UDC lends itself to machine readability, and the system has been used both with early automatic mechanical sorting devices, and modern library [[OPAC]]s.<ref>[http://hdl.handle.net/10150/105346 Slavic, A. (2006) The level of exploitation of Universal Decimal Classification in library OPACs: a pilot study. Vjesnik bibliotekara Hrvatske, 49(3-4):155-182]</ref><ref>[http://hdl.handle.net/10150/105276 Slavic, A. (2006) UDC in subject gateways: experiment or opportunity? Knowledge Organization, 33 2, 67-85.]</ref> From 1993, a standard version of UDC is maintained and is distributed in a [[database]] format: UDC Master Reference File (UDC MRF) which is updated and released annually.<ref name="UDC MRF">[http://www.udcc.org/index.php/site/page?view=mrf UDC Master Reference File], UDC Consortium website</ref> The 2011 version of the MRF (released in 2012) contains over 70,000 classes.<ref name="UDC Fact Sheet"/> In the past full printed editions used to have around 220,000 subdivisions.<ref name="UDCS">{{cite web
| url         = http://www.udcc.org/udcsummary/php/index.php
| title       = Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088)
| year        = 2012
| work        = Multilingual Universal Decimal Classification Summary
| publisher   = UDC Consortium
| accessdate  = 2012-03-25
| quote = ''Multilingual UDC Summary (2012). Multilingual Universal Decimal Classification Summary. Web resource, v. 1.1. The Hague: UDC Consortium (UDCC Publication No. 088). Available at: http://www.udcc.org/udcsummary/php/index.php ''
}}</ref>

== UDC structure ==

=== Notation ===
A notation is a code commonly used in classification schemes to represent a class, i.e. a subject and its position in the hierarchy, to enable mechanical sorting and filing of subjects. UDC uses [[Arabic numerals]] arranged decimally. Every number is thought of as a decimal fraction with the initial decimal point omitted, which determines the filing order. An advantage of decimal notational systems is that they are infinitely extensible, and when new subdivisions are introduced, they need not disturb the existing allocation of numbers. For ease of reading, a UDC notation is usually punctuated after every third digit:
{| 
|- 
| style="width:18%; font-weight:bold;" | Notation || style="font-weight:bold;" | Caption (Class description)
|-
| 539.120 ||Theoretical problems of elementary particles physics. Theories and models of fundamental interactions
|-
| 539.120.2 || Symmetries of quantum physics
|-
|539.120.22 ||  Conservation laws
|-
| 539.120.222 ||   Translations. Rotations
|-
| 539.120.224 ||   Reflection in time and space
|-
|539.120.226 ||   Space-time symmetries
|-
| 539.120.23 ||  Internal symmetries
|-
| 539.120.3 || Currents
|-
| 539.120.4 || Unified field theories
|-
|539.120.5 || Strings
|}

In UDC the notation has two features that make the scheme easier to browse and work with: 
* '''hierarchically expressive''' – the longer the notation, the more specific the class: removing the final digit automatically produces a broader class code.
* '''syntactically expressive''' – when UDC codes are combined, the sequence of digits is interrupted by a precise type of punctuation sign which indicates that the expression is a combination of classes rather than a simple class e.g. the colon in 34:32 indicates that there are two distinct notational elements: 34 Law. Jurisprudence and 32 Politics; the closing and opening parentheses and double quotes in the following code 913(574.22)"19"(084.3) indicate four separate notational elements: 913 Regional geography, (574.22) North Kazakhstan (Soltüstik Qazaqstan); "19" 20th century and (084.3) Maps (document form)

=== Basic features and syntax ===
UDC is an analytico-synthetic and [[faceted classification]]. It allows an unlimited combination of attributes of a subject and relationships between subjects to be expressed. UDC codes from different tables can be combined to present various aspects of document content and form, e.g. 94(410)"19"(075) History ''(main subject)'' of United Kingdom ''(place)'' in 20th century ''(time)'', a textbook ''(document form)''. Or: 37:2 Relationship between Education and Religion. Complex UDC expressions can be accurately parsed into constituent elements.

UDC is also a disciplinary classification covering the entire universe of knowledge.<ref name="UDC Subject Coverage">[http://www.udcc.org/index.php/site/page?view=subject_coverage UDC Subject Coverage], UDC Consortium website</ref> This type of classification can also be described as ''aspect'' or ''perspective'', which means that concepts are subsumed and placed under the field in which they are studied. Thus, the same concept can appear in different fields of knowledge. This particular feature is usually implemented in UDC by re-using the same concept in various combinations with the main subject, e.g. a code for language in common auxiliaries of language is used to derive numbers for ethnic grouping, individual languages in linguistics and individual literatures. Or, a code from the auxiliaries of place, e.g. ''(410) United Kingdom'', uniquely representing the concept of United Kingdom can be used to express ''911(410) Regional geography of United Kingdom'' and ''94(410) History of United Kingdom''.

=== Organization of classes ===

Concepts are organized in two kinds of tables in UDC:<ref name="UDC Structure">[http://www.udcc.org/index.php/site/page?view=about_structure UDC Structure and Tables], UDC Consortium website</ref>

*'''Common auxiliary tables''' (including certain auxiliary signs). These tables contain facets of concepts representing, general recurrent characteristics, applicable over a range of subjects throughout the main tables, including notions such as place, language of the text and physical form of the document, which may occur in almost any subject. UDC numbers from these tables, called common auxiliaries are simply added at the end of the number for the subject taken from the main tables. There are over 15,000 of common auxiliaries in UDC.
*'''The main tables or main schedules''' containing the various disciplines and branches of knowledge, arranged in 9 main classes,  numbered from 0 to 9 (with class 4 being vacant). At the beginning of each class there are also series of special auxiliaries, which express aspects that are recurrent within this specific class. Main tables in UDC contain more than 60,000 subdivisions.

==== Main classes ====
*0 [[Science]] and [[Knowledge]]. [[Organization]]. [[Computer Science]]. [[Information Science]]. [[Documentation]]. [[Librarianship]]. [[Institutions]]. [[Publications]]
*1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]
*2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]
*3 [[Outline of social science|Social Sciences]]
*4 ''vacant''
*5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural Sciences]]
*6 [[Outline of applied science|Applied Sciences]]. [[Outline of medicine|Medicine]], [[Outline of technology|Technology]]
*7 [[The arts|The Arts]]. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]
*8 [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]
*9 [[Outline of geography|Geography]]. [[Outline of history|History]]

The vacant class 4 is the result of a planned schedule expansion. This class was freed by moving linguistics into class 8 in the 1960s to make space for future developments in the rapidly expanding fields of knowledge; primarily natural sciences and technology.

==== Common auxiliary tables ====
''Common auxiliaries'' are aspect-free concepts that can be used in combination with any other UDC code from the main classes or with other common auxiliaries. They have unique notational representations that makes them stand out in complex expressions. Common auxiliary numbers always begin with a certain symbol known as a facet indicator, e.g. <nowiki>=</nowiki> (equal sign) always introduces concepts representing the language of a document; (0...) numbers enclosed in parentheses starting with zero always represent a concept designating document form. Thus (075) Textbook and =111 English can be combined to express, e.g.(075)=111 Textbooks in English, and when combined with numbers from the main UDC tables they can be used as follows: 2(075)=111 Religion textbooks in English, 51(075)=111 Mathematics textbooks in English etc.

*=...	Common auxiliaries of language. Table 1c
*(0...)	Common auxiliaries of form. Table 1d
*(1/9)	Common auxiliaries of place. Table 1e
*(=...)	Common auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f
*"..."	Common auxiliaries of time. Table 1g helps to make minute division of time e.g.:  "1993-1996''
*-0...	Common auxiliaries of general characteristics: Properties, Materials, Relations/Processes and Persons. Table 1k.
*-02	Common auxiliaries of properties. Table 1k
*-03	Common auxiliaries of materials. Table 1k
*-04	Common auxiliaries of relations, processes and operations. Table 1k
*-05	Common auxiliaries of persons and personal characteristics. Table 1k this table is repeated

==== Connecting signs ====
In order to preserve the precise meaning and enable accurate parsing of complex UDC expressions, a number of connecting symbols are made available to relate and extend UDC numbers. These are:
{| class="wikitable"
!Symbol !! Symbol name !! Meaning !! Example
|-
|<nowiki>+</nowiki> || [[Plus and minus signs|plus]] || coordination, addition || e.g. 59+636 [[zoology]] and [[animal breeding]]
|-
|<nowiki>/</nowiki> || [[Slash (punctuation)|stroke]] || consecutive extension || e.g. 592/599 Systematic zoology (everything from 592 to 599 inclusive)
|-
|<nowiki>:</nowiki> || [[Colon (punctuation)|colon]] || relation || e.g. 17:7  Relation of [[ethics]] to [[art]]
|-
|<nowiki>[ ]</nowiki> || square [[bracket]]s || subgrouping || e.g. 311:[622+669](485) [[statistics]] of [[mining]] and [[metallurgy]] in [[Sweden]] (the auxiliary qualifiers 622+669 considered as a unit)
|-
|<nowiki>*</nowiki> || asterisk || Introduces non-UDC notation  || e.g. 523.4*433 Planetology, minor planet Eros (IAU authorized number after the asterisk)
|-
|<nowiki>A/Z</nowiki> || alphabetical extension || Direct alphabetical specification  || e.g. 821.133.1MOL French literature, works of Molière
|}

== UDC outline ==

<small>UDC classes in this outline are taken from the Multilingual Universal Decimal Classification Summary (UDCC Publication No. 088) released by the UDC Consortium under the Creative Commons Attribution Share Alike 3.0 license (first release 2009, subsequent update 2012).<ref name="UDCS" /></small>

=== Main tables ===

====0 [[Outline of science|Science]] and [[Outline of knowledge|knowledge]]. Organization. [[Outline of computer science|Computer science]]. Information. Documentation. Librarianship. Institution. Publications====

  00          Prolegomena. Fundamentals of knowledge and culture. Propaedeutics
  001         [[Outline of science|Science]] and [[Outline of knowledge|knowledge]] in general. Organization of intellectual work
  002         Documentation. Books. Writings. Authorship
  003         Writing systems and scripts
  004         [[Outline of computer science|Computer science]] and technology. Computing
  004.2       Computer architecture
  004.3       Computer hardware
  004.4       [[Software]]
  004.5       Human-computer interaction
  004.6       Data
  004.7       Computer communication
  004.8       [[Outline of artificial intelligence|Artificial intelligence]]
  004.9       Application-oriented computer-based techniques
  005         [[Outline of business management|Management]]
  005.1       Management Theory
  005.2       Management agents. Mechanisms. Measures
  005.3       Management activities
  005.5       Management operations. Direction
  005.6       Quality management. Total quality management (TQM)
  005.7       Organizational management (OM)
  005.9       Fields of management
  005.92      Records management
  005.93      Plant management. Physical resources management
  005.94      Knowledge management
  005.95/.96  Personnel management. Human Resources management
  006         Standardization of products, operations, weights, measures and time
  007         Activity and organizing. Information. Communication and control theory generally (cybernetics)
  008         Civilization. [[Outline of culture|Culture]]. Progress   
  01          Bibliography and bibliographies. Catalogues
  02          Librarianship
  030         General reference works (as subject)
  050         Serial publications, periodicals (as subject)
  06          Organizations of a general nature
  069         Museums
  070         Newspapers (as subject). The Press. Outline of [[journalism]]
  08          Polygraphies. Collective works (as subject)
  09          Manuscripts. Rare and remarkable works (as subject)

====1 [[Outline of philosophy|Philosophy]]. [[Outline of psychology|Psychology]]====
  101        Nature and role of philosophy
  11         Metaphysics
  111        General metaphysics. Ontology
  122/129    Special Metaphysics
  13         Philosophy of mind and spirit. Metaphysics of spiritual life
  14         Philosophical systems and points of view
  159.9      [[Outline of psychology|Psychology]]
  159.91     Psychophysiology (physiological psychology). Mental physiology
  159.92     Mental development and capacity. Comparative psychology
  159.93     Sensation. Sensory perception
  159.94     Executive functions
  159.95     Higher mental processes
  159.96     Special mental states and processes
  159.97     Abnormal psychology
  159.98     Applied psychology (psychotechnology) in general
  16         [[Outline of logic|Logic]]. [[Outline of epistemology|Epistemology]]. Theory of knowledge. Methodology of logic
  17         Moral philosophy. [[Outline of ethics|Ethics]]. Practical philosophy

====2 [[Outline of religion|Religion]]. [[Outline of theology|Theology]]====

<small>The UDC tables for religion are fully faceted. Indicated in italics below, are special auxiliary numbers that can be used to express attributes (facets) of any specific faith. Any special number can be combined with any religion e.g.  ''-5 Worship'' can be used to express e.g. ''26-5 Worship in Judaism'', ''27-5 Worship in Christianity'', ''24-5 Worship in Buddhism''. The complete special auxiliary tables contain around 2000 subdivisions of various attributes that can be attached to express various aspects of individual faiths to a great level of specificity allowing equal level of detail for every religion.</small>
  ''2-1/-9	Special auxiliary subdivision for religion''
  ''2-1	Theory and philosophy of religion. Nature of religion. Phenomenon of religion''
  ''2-2	Evidences of religion''
  ''2-3	Persons in religion''
  ''2-4	Religious activities. Religious practice''
  ''2-5	Worship broadly. Cult. Rites and ceremonies''
  ''2-6	Processes in religion''
  ''2-7	Religious organization and administration''
  ''2-8	Religions characterised by various properties''
  ''2-9	History of the faith, religion, denomination or church''
  21/29	Religious systems. Religions and faiths
  21	Prehistoric and primitive religions
  22	Religions originating in the Far East
  23	Religions originating in Indian sub-continent. Hindu religion in the broad sense
  24	[[Outline of Buddhism|Buddhism]]
  25	Religions of antiquity. Minor cults and religions
  26	[[Outline of Judaism|Judaism]]
  27	[[Outline of Christianity|Christianity]]
  28	[[Outline of Islam|Islam]]
  29	Modern spiritual movements

====3 [[Outline of social science|Social sciences]]====
  303   Methods of the social sciences
  304	Social questions. Social practice. Cultural practice. Way of life (Lebensweise)
  305	Gender studies
  308	Sociography. Descriptive studies of society (both qualitative and quantitative)
  311	[[Outline of statistics|Statistics]] as a science. Statistical theory
  314/316 [[Outline of society|Society]]
  314	Demography. Population studies
  316	[[Outline of sociology|Sociology]]
  32	[[Outline of politics|Politics]]
  33	[[Outline of economics|Economics]]. Economic science
  34	[[Outline of law|Law]]. Jurisprudence
  35	Public administration. Government. Military affairs
  36	Safeguarding the mental and material necessities of life
  37	[[Outline of education|Education]]
  39	Cultural anthropology. Ethnography. Customs. Manners. Traditions. Way of life

====4 Vacant====

This section is currently vacant.

====5 [[Outline of mathematics|Mathematics]]. [[Outline of natural science|Natural sciences]]====
  502/504  Environmental science. Conservation of natural resources. Threats to the environment and protection against them
  502	The environment and its protection
  504	Threats to the environment
  51	[[Outline of mathematics|Mathematics]]
  510	Fundamental and general considerations of mathematics
  511	Number theory
  512	[[Outline of algebra|Algebra]]
  514	[[Outline of geometry|Geometry]]
  517	Analysis
  519.1	Combinatorial analysis. Graph theory
  519.2	[[Outline of probability|Probability]]. Mathematical statistics
  519.6	Computational mathematics. [[Outline of numerical analysis|Numerical analysis]]
  519.7	Mathematical cybernetics
  519.8	Operational research (OR): mathematical theories and methods
  52	[[Outline of astronomy|Astronomy]]. Astrophysics. [[Outline of space exploration|Space research]]. Geodesy
  53	[[Outline of physics|Physics]]
  531/534  Mechanics
  535	Optics
  536	Heat. Thermodynamics. Statistical physics
  537	Electricity. Magnetism. Electromagnetism
  538.9	Condensed matter physics. Solid state physics
  539	Physical nature of matter
  54	[[Outline of chemistry|Chemistry]]. Crystallography. Mineralogy
  542	Practical laboratory chemistry. Preparative and experimental chemistry
  543	Analytical chemistry
  544	Physical chemistry
  546	Inorganic chemistry
  547	[[Outline of organic chemistry|Organic chemistry]]
  548/549 Mineralogical sciences. Crystallography. Mineralogy
  55	[[Outline of earth science|Earth sciences]]. Geological sciences
  56	Paleontology
  57	Biological sciences in general
  58	[[Outline of botany|Botany]]
  59	[[Outline of zoology|Zoology]]

====6 [[Outline of applied science|Applied sciences]]. [[Outline of medicine|Medicine]]. [[Outline of technology|Technology]]====

<small>Class 6 occupies the largest proportion of UDC schedules. It contains over 44,000 subdivisions. Each specific field of technology or industry usually contains more than one special auxiliary table with concepts needed to express operations, processes, materials and products. As a result, UDC codes are often created through the combination of various attributes. Equally, some parts of this class enumerate concepts to a great level of detail e.g.  ''621.882.212 Hexagon screws with additional shapes. Including: Flank screws. Collar screws. Cap screws''
</small>

  60    [[Outline of biotechnology|Biotechnology]]
  61	Medical sciences
  611/612 Human biology
  613	Hygiene generally. Personal health and hygiene
  614	Public health and hygiene. Accident prevention
  615	Pharmacology. Therapeutics. Toxicology
  616	Pathology. Clinical medicine
  617	Surgery. Orthopaedics. Ophthalmology
  618	Gynaecology. Obstetrics
  62	[[Outline of engineering|Engineering]]. [[Outline of technology|Technology]] in general
  620	Materials testing. Commercial materials. Power stations. Economics of energy
  621	Mechanical engineering in general. Nuclear technology. Electrical engineering. Machinery
  622	[[Outline of mining|Mining]]
  623	Military engineering
  624	Civil and structural engineering in general
  625	Civil engineering of land transport. Railway engineering. Highway engineering
  626/627  Hydraulic engineering and construction. Water (aquatic) structures
  629	Transport vehicle engineering
  63	[[Outline of agriculture|Agriculture]] and related sciences and techniques. Forestry. Farming. Wildlife exploitation
  630	Forestry
  631/635	Farm management. Agronomy. Horticulture
  633/635	Horticulture in general. Specific crops
  636	Animal husbandry and breeding in general. Livestock rearing. Breeding of domestic animals
  64	Home economics. Domestic science. Housekeeping
  65	Communication and transport industries. Accountancy. Business management. Public relations
  654	Telecommunication and telecontrol (organization, services)
  655	Graphic industries. Printing. Publishing. Book trade
  656	Transport and postal services. Traffic organization and control
  657	Accountancy
  658	[[Outline of business management|Business management]], administration. Commercial organization
  659	Publicity. Information work. [[Outline of public relations|Public relations]]
  66	Chemical technology. Chemical and related industries
  67	Various industries, trades and crafts
  68	Industries, crafts and trades for finished or assembled articles
  69	Building ([[Outline of construction|construction]]) trade. Building materials. Building practice and procedure

====7 The arts. Recreation. [[Outline of entertainment|Entertainment]]. [[Outline of sports|Sport]]====
  ''7.01/.09	Special auxiliary subdivision for the arts''
  ''7.01	Theory and philosophy of art. Principles of design, proportion, optical effect''
  ''7.02	Art technique. Craftsmanship''
  ''7.03	Artistic periods and phases. Schools, styles, influences''
  ''7.04	Subjects for artistic representation. Iconography. Iconology''
  ''7.05	Applications of art (in industry, trade, the home, everyday life)''
  ''7.06	Various questions concerning art''
  ''7.07	Occupations and activities associated with the arts and entertainment''
  ''7.08	Characteristic features, forms, combinations etc. (in art, entertainment and sport)''
  ''7.091	Performance, presentation (in original medium)''
  71	Physical planning. Regional, town and country planning. Landscapes, parks, gardens
  72	[[Outline of architecture|Architecture]]
  73	Plastic arts
  74	[[Outline of drawing and drawings|Drawing]]. [[Outline of design|Design]]. [[Outline of crafts|Applied arts and crafts]]
  745/749	Industrial and domestic arts and crafts. Applied arts
  75	[[Outline of painting|Painting]]
  76	Graphic art, printmaking. Graphics
  77	[[Outline of photography|Photography]] and similar processes
  78	[[Outline of music|Music]]
  79	Recreation. [[Outline of entertainment|Entertainment]]. [[Outline of games|Games]]. [[Outline of sports|Sport]]
  791	Cinema. [[Outline of film|Films]] (motion pictures)
  792	[[Outline of theatre|Theatre]]. [[Outline of stagecraft|Stagecraft]]. Dramatic performances
  793	Social entertainments and recreations. Art of movement. [[Outline of dance|Dance]]
  794	Board and table games (of thought, skill and chance)
  796	[[Outline of sports|Sport]]. [[Outline of games|Games]]. [[Outline of exercise|Physical exercises]]
  797	Water sports. Aerial sports
  798	Riding and driving. Horse and other animal sports
  799	Sport fishing. Sport hunting. Shooting and target sports

====8 Language. [[Outline of linguistics|Linguistics]]. [[Outline of literature|Literature]]====

<small>Tables for class 8 are fully faceted and details are expressed through combination with common auxiliaries of language (Table 1c) and a series of special auxiliary tables to indicate other facets or attributes in Linguistics or Literature. As a result, this class allows for great specificity in indexing although the schedules themselves occupy very little space in UDC. The subdivisions of e.g. ''811 Languages'' or ''821 Literature'' are derived from common auxiliaries of language =1/=9 (Table 1c) by substituting a point for the equals sign, e.g. 811.111 English language (as a subject of a linguistic study) and ''821.111 English literature'' derives from ''=111 English language''. Common auxiliaries of place and time are also frequently used in this class to express place and time facets of Linguistics or Literature, e.g. ''821.111(71)"18" English literature of Canada in 19th century''
</small>
  80	General questions relating to both linguistics and literature. Philology
  801	Prosody. Auxiliary sciences and sources of philology
  808	Rhetoric. The effective use of language  
  '''81	[[Outline of linguistics|Linguistics]] and languages'''
  ''81`1/`4	Special auxiliary subdivision for subject fields and facets of linguistics and languages''
 '' 81`1	General linguistics''
  ''81`2	[[Outline of semiotics|Theory of signs]]. Theory of translation. Standardization. Usage. Geographical linguistics''
  ''81`3	Mathematical and applied linguistics. Phonetics. Graphemics. Grammar. Semantics. Stylistics''
  ''81`4	Text linguistics, Discourse analysis. Typological linguistics''
  ''81`42	Text linguistics. Discourse analysis''
  ''81`44	Typological linguistics''
  811	Languages
        <small>Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix ''811.'' e.g. ''=111 English'' becomes ''811.111 Linguistics of English language''</small>
  811.1/.9	All languages natural or artificial
  811.1/.8	Individual natural languages
  811.1/.2	Indo-European languages
  811.21/.22	Indo-Iranian languages
  811.3	Dead languages of unknown affiliation. Caucasian languages
  811.4	Afro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages
  811.5	Ural-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu
  811.6	Austro-Asiatic languages. Austronesian languages
  811.7	Indo-Pacific (non-Austronesian) languages. Australian languages
  811.8	American indigenous languages
  811.9	Artificial languages
  '''82	[[Outline of literature|Literature]]'''
  ''82-1/-9	Special auxiliary subdivision for literary forms, genres''
  ''82-1	[[Outline of poetry|Poetry]]. Poems. Verse''
  ''82-2	Drama. Plays''
  ''82-3	[[Outline of fiction|Fiction]]. Prose narrative''
  ''82-31	Novels. Full-length stories''
  ''82-32	Short stories. Novellas''
  ''82-4	Essays''
  ''82-5	Oratory. Speeches''
  ''82-6	Letters. Art of letter-writing. Correspondence. Genuine letters''
  ''82-7	Prose satire. Humour, epigram, parody''
  ''82-8	Miscellanea. Polygraphies. Selections''
  ''82-9	Various other literary forms''
  ''82-92	Periodical literature. Writings in serials, journals, reviews''
  ''82-94	History as literary genre. Historical writing. Historiography. Chronicles. Annals. Memoirs''
  ''82.02/.09	Special auxiliary subdivision for theory, study and technique of literature''
  ''82.02	Literary schools, trends and movements''
  ''82.09	Literary criticism. Literary studies''
  ''82.091	Comparative literary studies. Comparative literature''
  821	Literatures of individual languages and language families
        <small>Derived from the common auxiliaries of language =1/=9 (Table 1c) by replacing the equal sign = with prefix ''821.'' e.g. ''=111 English'' becomes ''821.111 English literature''</small>

====9 [[Outline of geography|Geography]]. Biography. [[Outline of history|History]]====

<small>Tables for Geography and History in UDC are fully faceted and place, time and ethnic grouping facets are expressed through combination with common auxiliaries of place (Table 1d), ethnic grouping (Table 1f) and time (Table 1g)
</small>

  902/908	Archaeology. Prehistory. Cultural remains. Area studies
  902	[[Outline of archaeology|Archaeology]]
  903	Prehistory. Prehistoric remains, artefacts, antiquities
  904	Cultural remains of historical times
  908	Area studies. Study of a locality
  91	[[Outline of geography|Geography]]. Exploration of the Earth and of individual countries. Travel. [[Outline of geography#Regional geography|Regional geography]]
  910	General questions. Geography as a science. Exploration. Travel
  911	General geography. Science of geographical factors (systematic geography). Theoretical geography
  911.2	[[Outline of geography#Physical geography|Physical geography]]
  911.3	[[Outline of geography#Human geography|Human geography]] (cultural geography). Geography of cultural factors
  911.5/.9	Theoretical geography
  912	Nonliterary, nontextual representations of a region
  913	[[Outline of geography#Regional geography|Regional geography]]
  92	Biographical studies. Genealogy. Heraldry. Flags
  929	Biographical studies
  929.5	Genealogy
  929.6	Heraldry
  929.7	Nobility. Titles. Peerage
  929.9	Flags. Standards. Banners
  93/94	[[Outline of history|History]]
  930	Science of history. Historiography
  930.1	History as a science
  930.2	Methodology of history. Ancillary historical sciences
  930.25	Archivistics. Archives (including public and other records)
  930.85	History of civilization. Cultural history
  94	General

=== Common auxiliary tables ===

====Common auxiliaries of language. Table 1c====
  =1/=9	Languages (natural and artificial)
  =1/=8	Natural languages
  =1/=2	Indo-European languages
  =1	Indo-European languages of Europe
  =11	Germanic languages
  =12	Italic languages
  =13	Romance languages
  =14	Greek (Hellenic)
  =15	Celtic languages
  =16	Slavic languages
  =17	Baltic languages
  =18	Albanian
  =19	Armenian
  =2	Indo-Iranian, Nuristani (Kafiri) and dead Indo-European languages
  =21/=22	Indo-Iranian languages
  =21	Indic languages
  =22	Iranian languages
  =29	Dead Indo-European languages (not listed elsewhere)
  =3	Dead languages of unknown affiliation. Caucasian languages
  =34	Dead languages of unknown affiliation, spoken in the Mediterranean and Near East (except Semitic)
  =35	Caucasian languages
  =4	Afro-Asiatic, Nilo-Saharan, Congo-Kordofanian, Khoisan languages
  =41	Afro-Asiatic (Hamito-Semitic) languages
  =42	Nilo-Saharan languages
  =43	Congo-Kordofanian (Niger-Kordofanian) languages
  =45	Khoisan languages
  =5	Ural-Altaic, Palaeo-Siberian, Eskimo-Aleut, Dravidian and Sino-Tibetan languages. Japanese. Korean. Ainu
  =51	Ural-Altaic languages
  =521	Japanese
  =531	Korean
  =541	Ainu
  =55	Palaeo-Siberian languages
  =56	Eskimo-Aleut languages
  =58	Sino-Tibetan languages
  =6	Austro-Asiatic languages. Austronesian languages
  =61	Austro-Asiatic languages
  =62	Austronesian languages
  =7	Indo-Pacific (non-Austronesian) languages. Australian languages
  =71	Indo-Pacific (non-Austronesian) languages
  =72	Australian languages
  =8	American indigenous languages
  =81	Indigenous languages of Canada, USA and Northern-Central Mexico
  =82	Indigenous languages of western North American Coast, Mexico and Yucatán
  =84/=88	Central and South American indigenous languages
  =84	Ge-Pano-Carib languages. Macro-Chibchan languages
  =85	Andean languages. Equatorial languages
  =86	Chaco languages. Patagonian and Fuegian languages
  =88	Isolated, unclassified Central and South American indigenous languages
  =9	Artificial languages
  =92	Artificial languages for use among human beings. International auxiliary languages (interlanguages)
  =93	Artificial languages used to instruct machines. Programming languages. Computer languages

====(0...) Common auxiliaries of form. Table 1d====
  ''(0.02/.08)	Special auxiliary subdivision for document form''
  ''(0.02)	Documents according to physical, external form''
  ''(0.03)	Documents according to method of production''
  ''(0.032)	Handwritten documents (autograph, holograph copies). Manuscripts. Pictorial documents (drawings, paintings)''
  ''(0.034)	Machine-readable documents''
  ''(0.04)	Documents according to stage of production''
  ''(0.05)	Documents for particular kinds of user''
  ''(0.06)	Documents according to level of presentation and availability''
  ''(0.07)	Supplementary matter issued with a document''
  ''(0.08)	Separately issued supplements or parts of documents''
  (01)	Bibliographies
  (02)	Books in general
  (03)	Reference works
  (04)	Non-serial separates. Separata
  (041)	Pamphlets. Brochures
  (042)	Addresses. Lectures. Speeches
  (043)	Theses. Dissertations
  (044)	Personal documents. Correspondence. Letters. Circulars
  (045)	Articles in serials, collections etc. Contributions
  (046)	Newspaper articles
  (047)	Reports. Notices. Bulletins
  (048)	Bibliographic descriptions. Abstracts. Summaries. Surveys
  (049)	Other non-serial separates
  (05)	Serial publications. Periodicals
  (06)	Documents relating to societies, associations, organizations
  (07)	Documents for instruction, teaching, study, training
  (08)	Collected and polygraphic works. Forms. Lists. Illustrations. Business publications
  (09)	Presentation in historical form. Legal and historical sources
  (091)	Presentation in chronological, historical form. Historical presentation in the strict sense
  (092)	Biographical presentation
  (093)	Historical sources
  (094)	Legal sources. Legal documents

====(1/9) Common auxiliaries of place. Table 1e====
  (1)	Place and space in general. Localization. Orientation
  ''(1-0/-9)	Special auxiliary subdivision for boundaries and spatial forms of various kinds''
  ''(1-0)	Zones''
  ''(1-1)	Orientation. Points of the compass. Relative position''
  ''(1-11)	East. Eastern''
  ''(1-13)	South. Southern''
  ''(1-14)	South-west. South-western''
  ''(1-15)	West. Western''
  ''(1-17)	North. Northern''
  ''(1-19)	Relative location, direction and orientation''
  ''(1-2)	Lowest administrative units. Localities''
  ''(1-5)	Dependent or semi-dependent territories''
  ''(1-6)	States or groupings of states from various points of view''
  ''(1-7)	Places and areas according to privacy, publicness and other special features''
  ''(1-8)	Location. Source. Transit. Destination''
  ''(1-9)	Regionalization according to specialized points of view''
  (100)	Universal as to place. International. All countries in general
  (2)	Physiographic designation
  (20)	Ecosphere
  (21)	Surface of the Earth in general. Land areas in particular. Natural zones and regions
  (23)	Above sea level. Surface relief. Above ground generally. Mountains
  (24)	Below sea level. Underground. Subterranean
  (25)	Natural flat ground (at, above or below sea level). The ground in its natural condition, cultivated or inhabited
  (26)	Oceans, seas and interconnections
  (28)	Inland waters
  (29)	The world according to physiographic features
  (3)	Places of the ancient and mediaeval world
  (31)	Ancient China and Japan
  (32)	[[Outline of ancient Egypt|Ancient Egypt]]
  (33)	Ancient Roman Province of Judaea. The Holy Land. Region of the Israelites
  (34)	[[Outline of ancient India|Ancient India]]
  (35)	Medo-Persia
  (36)	Regions of the so-called barbarians
  (37)	Italia. [[Outline of ancient Rome|Ancient Rome]] and Italy
  (38)	[[Outline of ancient Greece|Ancient Greece]]
  (399)	Other regions. Ancient geographical divisions other than those of classical antiquity
  (4/9)	Countries and places of the modern world
  (4)	[[Outline of Europe|Europe]]
  (5)	[[Outline of Asia|Asia]]
  (6)	[[Outline of Africa|Africa]]
  (7)	[[Outline of North America|North]] and Central America
  (8)	[[Outline of South America|South America]]
  (9)	States and regions of the South Pacific and [[Outline of Australia|Australia]]. Arctic. Antarctic

====(=...) Common auxiliaries of human ancestry, ethnic grouping and nationality. Table 1f====

<small>''They are derived mainly from the common auxiliaries of language =... (Table 1c) and so may also usefully distinguish linguistic-cultural groups, e.g. =111 English is used to represent (=111) English speaking peoples''</small>

  (=01)	Human ancestry groups
  (=011)	European Continental Ancestry Group
  (=012)	Asian Continental Ancestry Group
  (=013)	African Continental Ancestry Group
  (=014)	Oceanic Ancestry Group
  (=017)	American Native Continental Ancestry Group
  (=1/=8)	Linguistic-cultural groups, ethnic groups, peoples ['''derived from Table 1c''']
  (=1:1/9)	Peoples associated with particular places
                ''e.g. (=111:71) Anglophone population of Canada''

===="..." Common auxiliaries of time. Table 1g====
  "0/2"	Dates and ranges of time (CE or AD) in conventional Christian (Gregorian) reckoning
  "0"	First millennium CE
  "1"	Second millennium CE
  "2"	Third millennium CE
  "3/7"	Time divisions other than dates in Christian (Gregorian) reckoning
  "3"	Conventional time divisions and subdivisions: numbered, named, etc.
  "4"	Duration. Time-span. Period. Term. Ages and age-groups
  "5"	Periodicity. Frequency. Recurrence at specified intervals.
  "6"	Geological, archaeological and cultural time divisions
  "61/62" Geological time division
  "63"	Archaeological, prehistoric, protohistoric periods and ages
  "67/69" Time reckonings: universal, secular, non-Christian religious
  "67"	Universal time reckoning. Before Present
  "68"	Secular time reckonings other than universal and the Christian (Gregorian) calendar
  "69"	Dates and time units in non-Christian (non-Gregorian) religious time reckonings
  "7"	Phenomena in time. Phenomenology of time

====-0 Common auxiliaries of general characteristics. Table 1k====
  '''-02	Common auxiliaries of properties'''
  -021	Properties of existence
  -022	Properties of magnitude, degree, quantity, number, temporal values, dimension, size
  -023	Properties of shape
  -024	Properties of structure. Properties of position
  -025	Properties of arrangement
  -026	Properties of action and movement
  -027	Operational properties
  -028	Properties of style and presentation
  -029	Properties derived from other main classes
  '''-03 Common auxiliaries of materials'''
  -032	Naturally occurring mineral materials
  -033	Manufactured mineral-based materials
  -034	Metals
  -035	Materials of mainly organic origin
  -036	Macromolecular materials. Rubbers and plastics
  -037	Textiles. Fibres. Yarns. Fabrics. Cloth
  -039	Other materials
  '''-04 Common auxiliaries of relations, processes and operations'''
  -042	Phase relations
  -043	General processes
  -043.8/.9 Processes of existence
  -045	Processes related to position, arrangement, movement, physical properties, states of matter
  -047/-049	General operations and activities
  '''-05 Common auxiliaries of persons and personal characteristics'''
  -051	Persons as agents, doers, practitioners (studying, making, serving etc.)
  -052	Persons as targets, clients, users (studied, served etc.)
  -053	Persons according to age or age-groups
  -054	Persons according to ethnic characteristics, nationality, citizenship etc.
  -055	Persons according to gender and kinship
  -056	Persons according to constitution, health, disposition, hereditary or other traits
  -057	Persons according to occupation, work, livelihood, education
  -058	Persons according to social class, civil status

==See also==
'''Special classifications based on or used in combination with UDC'''
*[http://www.spri.cam.ac.uk/library/overview.html#classification Universal Decimal Classification for Use in Polar Libraries - Scott Polar Research Institute, Cambridge] 
*[[Lonclass|BBC LonClass]]
*[http://iufro.forintek.ca/GFDCDefault.aspx Global Forest Decimal Classification]

'''Other faceted classifications:'''
*[[Bliss bibliographic classification]]
*[[Colon classification]]
*[http://www.ucl.ac.uk/fatks/bso/ Broad System of Ordering]

'''Other general bibliographic classifications'''
*[[Dewey Decimal Classification]]
*[[Library of Congress Classification]]
* Russian Library-Bibliographical Classification (BBK) 
*[[Chinese Library Classification]]
*[[Harvard-Yenching Classification]]

==References==
{{reflist}}

==External links==
{{wikidata property|P1190}}
*[http://www.udcc.org/ Universal Decimal Classification Consortium]
**[http://www.udcc.org/about.htm About Universal Decimal Classification]
**[http://www.udcc.org/udcsummary/php/index.php Multilingual UDC Summary]
**[http://udcdata.info/ UDC Linked Data]

{{Library classification systems}}

{{Authority control}}

[[Category:Classification systems]]
[[Category:Library cataloging and classification]]
[[Category:Controlled vocabularies]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:874
<=====title=====>:
Enactive interfaces
<=====text=====>:
[[File:Enactive Human Machine Interface.png|thumb|300px|Enactive human-machine interface translating the aspects of a knowledge base into modalities of perception for a human operator. The auditory, visual, and tactile presentations by the system respond to tactile input from the operator, which user input in turn depends upon the auditory, visual, and tactile feedback from the system.<ref name=Bordegoni/><ref name=Fukuda/>]]
'''Enactive interfaces''' are interactive systems that allow organization and transmission of knowledge obtained through action. Examples are interfaces that couple a human with a machine to do things usually done unaided, such as shaping a three-dimensional object using multiple modality interactions with a data base,<ref name=Fukuda/> or using interactive video to allow a student to visually engage with mathematics concepts.<ref name=Held/> Enactive interface design can be approached through the idea of raising awareness of [[affordances]], that is, optimization of the awareness of possible actions available to someone using the enactive interface.<ref name=Stoffregen/> This optimization involves visibility, affordance, and feedback.<ref name=Stone/><ref name=Zudilova/>

The enactive interface in the figure interprets manual input and provides a response in perceptual terms in the form of images, sounds, and haptic (tactile) feedback. The system is called enactive because of the feedback loop in which the system response is decided by the user input, and the user input is driven by the perceived system responses.<ref name=Bordegoni/>

Enactive interfaces are new types of [[Human–computer interaction|human-computer interface]] that express and transmit the enactive knowledge by integrating different sensory aspects. The driving concept of enactive interfaces is then the fundamental role of motor action for storing and acquiring knowledge (action driven interfaces). Enactive interfaces are then capable of conveying and understanding gestures of the user, in order to provide an adequate response in perceptual terms. Enactive interfaces can be considered a new step in the development of the human-computer interaction because they are characterized by a closed loop between the natural gestures of the user (efferent component of the system) and the perceptual modalities activated (afferent component). Enactive interfaces can be conceived to exploit this direct loop and the capability of recognizing complex gestures.

The development of such interfaces requires the creation of a common vision between different research areas like [[computer vision]], [[Haptic perception|haptic]] and sound processing, giving more attention on the motor action aspect of interaction. An example of prototypical systems that are able to introduce enactive interfaces are reactive robots, robots that are always in contact with the human hand (like current play console controllers, [[Wii Remote]]) and are capable of interpreting the human movements and guiding the human for the completion of a manipulation task.

==Enactive knowledge==
Enactive knowledge is information gained through perception–action interaction in the environment. In many aspects the enactive knowledge is more natural than the other forms both in terms of the learning process and in the way it is applied in the world. Such knowledge is inherently [[multimodal interaction|multimodal]] because it requires the co-ordination of the various senses. Two key characteristics of enactive knowledge are that it is ''experential'': it relates to doing and depends on the user's experience, and it is ''cultural'': the way of doing is itself dependent upon social aspects, attitudes, values, practices, and legacy.<ref name=Bordegoni/>

Enactive interfaces are related to a fundamental interaction concept that often is not exploited by existing [[Human–computer interaction|human-computer interface]] technologies. As stated by cognitive psychologist [[Jerome Bruner]], the traditional interaction with the information mediated by a computer is mostly based on symbolic or iconic knowledge, and not on enactive knowledge.<ref name=Slee/> While in the symbolic way of learning knowledge is stored as words, mathematical symbols or other symbol systems, in the iconic stage knowledge is stored in the form of visual images, such as diagrams and illustrations that can accompany verbal information. On the other hand, enactive knowledge is a form of knowledge based on active participation, knowing by doing, by living rather than thinking.<ref name=Slee2/>
:"Any domain of knowledge (or any problem within that domain of knowledge) can be represented in three ways: by a set of actions appropriate for achieving a certain result (enactive representation); by a set of summary images or graphics that stand for a concept without defining it fully (iconic representation); and by a set of symbolic or logical propositions drawn from a symbolic system that is governed by rules or laws for forming and transforming propositions (symbolic representation)"<ref name=Bruner/>

A particular form of knowledge is a ''[[skill]]'', juggling being a simple example, and the acquisition of a skill is one area where enactive knowledge is evident. The sensorimotor and cognitive activities involved in acquiring skills are tabulated by the SKILLS FP6 European skills project.<ref name=Bardy>
{{cite journal |title=An enactive approach to perception-action and skill acquisition in virtual reality environments |author1=B Bardy |author2=D Delignières |author3=J Lagarde |author4=D Mottet |author5=G Zelic |journal= Third International Conference on Applied Human Factors and Ergonomics |location=Miami |date=July 2010 |url=http://didier.delignieres.perso.sfr.fr/Colloques-docs/Bardy%20et%20al.%20%282010%29%20Skills%20apprentissage.pdf }}
</ref>

==Multimodal interfaces==
Multimodal interfaces are a good candidate for the creation of ''Enactive interfaces'' because of their coordinated use of [[Haptic perception|haptic]], sound and vision. Such research is the main objective of the ENACTIVE [[Framework Programmes for Research and Technological Development|Network of Excellence]], a European consortium of more than 20 research laboratories that are joining their research effort for the definition, development and exploitation of enactive interfaces.

==ENACTIVE Network of Excellence==
The research on enactive knowledge and enactive interfaces is the objective of the ENACTIVE Network of Excellence. A Network of Excellence is a [[European Economic Community|European Community]] research instrument that provides fundings for the integration of the research activities of different research laboratories and institutions. The ENACTIVE NoE started in 2004 with more than 20 partners with the objective of ''the creation of a multidisciplinary research community with the aim of structuring the research on a new generation of human-computer interfaces called Enactive Interfaces.''. The aim of this NoE is not only the research on enactive interfaces by itself, but also the integration of the partners through a Virtual Laboratory and the spreading of the expertise and knowledge of the Network.

Since 2004, the partners, coordinated by the PERCRO laboratory, have improved both the theoretical aspects of enaction, through seminars and the creation of a [[lexicon]], and the technological aspects necessary for the creation of enactive interfaces. Every year the status of the ENACTIVE NoE is presented through an international conference.<ref name=PERCRO/>

== See also ==
* [[Enactivism]]

==References==
{{reflist|refs=

<ref name=Bordegoni>
{{cite book |title=Emotional Engineering: Service Development |editor=Shuichi Fukuda |url=https://books.google.com/books?id=ow-UFDj15rUC&pg=PA76 |page=76 |chapter=§4.4.2: PDP [Product Development Process] scenario based on user-centered design |author=Monica Bordegoni |isbn=9781849964234 |publisher=Springer |year=2010}}
</ref>

<ref name=Bruner>
{{cite book |title=Toward a Theory of Instruction |author=Jerome Seymour Bruner |url=http://h.uib.no/examplewiki/en/images/5/5a/Bruner_1966_Theory_of_Instruction.pdf |isbn=9780674897014 |publisher=Harvard University Press |year=1966 |page=44}}. Quoted in {{cite book |title=Fundamental Constructs in Mathematics Education |author=J Bruner |editor1=John Mason |editor2=Sue Johnston-Wilder |url=https://books.google.com/books?id=EA3LtKYTa7YC&pg=PA260 |page=260 |chapter=Chapter 10: Sustaining mathematical activity |year=2004 |publisher=Taylor & Francis |isbn= 0415326982 |edition=Paperback}}
</ref>

<ref name=Fukuda>
{{cite book |title=Emotional Engineering: Service Development |chapter=§4.5.2 Design tools based upon enactive interfaces |url=https://books.google.com/books?id=ow-UFDj15rUC&pg=PA78 |pages=78 ''ff'' |isbn=9781849964234 |year=2010 |publisher=Springer |author=Monica Bordegoni |editor=Shuichi Fukuda}}
</ref>

<ref name=Held>
{{cite book |title=Research on Technology and the Teaching and Learning of Mathematics  |editor1=Mary Kathleen Heid |editor2=Glendon W. Blume |url=https://books.google.com/books?id=RGqFJ9inaQQC&pg=PA213 |pages=213 ''ff'' |chapter=Enactive control |authors=D Tall, D Smith, C Piez |isbn=9781931576192 |year=2008 |publisher=Information Age Publishing Inc }}
</ref>

<ref name=PERCRO>
{{cite web |title=Research on haptic interfaces and virtual environments |url=http://www.percro.org/node/24 |publisher=PERCRO Perceptual Robotics Laboratory |accessdate=April 30, 2014}}
</ref>

<ref name=Slee>
Bruner's list of six characteristics of iconic knowledge is found in {{cite book |chapter=Iconic representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}
</ref>

<ref name=Slee2>
{{cite book |chapter=Enactive representation |title=Child, Adolescent and Family Development |author1=Phillip T. Slee |author2=Marilyn Campbell |author3=Barbara Spears |url=https://books.google.com/books?id=iLd7XILh7QkC&pg=PA176 |page=176 |isbn=9781107402164 |year=2012 |publisher=Cambridge University Press}}
</ref>

<ref name=Stoffregen>
{{cite journal |url=http://link.springer.com/article/10.1007/s10055-006-0025-7 |title=Affordances in the design of enactive systems |author1=TA Stoffregen |author2=BG Bardy |author3=B Mantel |journal=Virtual Reality |volume=10 |issue=1 |year=2006 |pages=4–10 |doi=10.1007/s10055-006-0025-7}}
</ref>

<ref name=Stone>
{{cite book |author1=Debbie Stone |author2=Caroline Jarrett |author3=Mark Woodroffe |author4=Shailey Minocha Morgan Kaufmann |year=2005 |title=User Interface Design and Evaluation |publisher=Morgan Kaufmann |isbn=9780080520322 |url=https://books.google.com/books?id=VvSoyqPBPbMC&pg=PA97 |pages=97 ''ff'' |chapter=Chapter 5; §3: Three principles from experience: visibility, affordance, and feedback}}
</ref>

<ref name=Zudilova>
{{cite book |title=Trends in Interactive Visualization: State-of-the-Art Survey |pages=166 ''ff'' |chapter=Perceptual and design principles for effective interactive visualizations |author1=Elena Zudilova-Seinstra |author2=Tony Adriaansen |author3=Robert van Liere |year=2008 |publisher=Springer |isbn=9781848002692 |url=https://books.google.com/books?id=mFtS7uN8ybsC&pg=PA166}}
</ref>

}}

==External links==
* [http://vimeo.com/79179138 Vimeo], video of a three-dimensional dynamic interactive graphical display allowing a human operator to visualize and manipulate data.

==Additional reading==
*{{cite book |title=Orchestrating Human-Centered Design |author=Guy Boy |url=https://books.google.com/books?id=I5gCTZCIL3AC&pg=PA118&lpg=PA118 |isbn=9781447143383 |year=2012 |publisher=Springer |page=118}} "The organization producing the system can itself be defined as an autopoietic system in Maturana and Varela's sense. An autopoietic system is producer and product at the same time. HCD [Human Centered Design] is both the process of design and the design itself." 
*{{cite journal |title=The systemics of dialogism: On the prevalence of the self in HCI design |author=Colin T Schmidt |journal=Journal of the American society for information science |volume=48 |issue=11 |pages=1073–1081 |year=1997 |url=http://www.researchgate.net/publication/220433804_The_Systemics_of_Dialogism_On_the_Prevalence_of_the_Self_in_HCI_Design |doi=10.1002/(sici)1097-4571(199711)48:11<1073::aid-asi9>3.0.co;2-t}} Autopoiesic systems.
*{{cite journal |title=An autopoietic approach for knowledge management systems in manufacturing enterprises |author1=Markus Thannhuber |author2=Mitchell M Tseng |author3=Hans-Jörg Bullinger |url=http://www.researchgate.net/publication/223035600_An_Autopoietic_Approach_for_Building_Knowledge_Management_Systems_in_Manufacturing_Enterprises/file/50463525a5a320287e.pdf%26sa%3DX%26scisig%3DAAGBfm3GtB0hiqz1jul4MXuCQxnRzPbcHQ%26oi%3Dscholarr&rct=j&q=&esrc=s&sa=X&ei=N-h_U6HtHIiEogSy_oHAAw&ved=0CCcQgAMoADAA&usg=AFQjCNEt_M1NOffumXQSxrJIVuZI48XRGQ&cad=rja |journal=Annals of the CIRP-Manufacturing Technology |volume=50 |issue=1 |year=2001 |pages=313 ''ff'' |doi=10.1016/s0007-8506(07)62129-5}}

[[Category:Enactive cognition]]
[[Category:Knowledge representation]]
[[Category:Educational psychology]]
[[Category:Motor cognition]]
[[Category:User interface techniques]]
<=====doc_Id=====>:877
<=====title=====>:
Knowledge Engineering Environment
<=====text=====>:
'''KEE''' (Knowledge Engineering Environment) is a [[Frame language|frame-based]] development tool for [[Expert system|Expert Systems]].<ref>[http://portal.acm.org/citation.cfm?id=62001 An evaluation of expert system development tools]</ref> KEE was developed and sold by [[IntelliCorp (Software)|IntelliCorp]]. It was first released in 1983 and ran on [[Lisp Machine]]s. KEE was later ported to Lucid [[Common Lisp]] with [[CLX (Common Lisp)|CLX]] (X11 interface for Common Lisp). This version was available on various Workstations.

On top of KEE several extensions were offered:

* Simkit,<ref>[http://doi.acm.org/10.1145/76738.76766 The SimKit system: knowledge-based simulation and modeling tools in KEE]</ref><ref>[http://portal.acm.org/citation.cfm?id=62001 SimKit: a model-building simulation toolkit]</ref> a frame-based simulation library
* KEEconnection,<ref>[http://portal.acm.org/citation.cfm?id=62001 KEEConnection: a bridge between databases and knowledge bases]</ref> [[database connection]] between the frame system and relational databases.

Frames are called ''Units'' in KEE. Units are used for both individual instances and classes. Frames have ''slots'' and slots have ''facets''. Facets for example describe the expected values of a slot, the inheritance rule for the slot or the value of a slot. Slots can have multiple values. Behavior can be implemented using the message-passing paradigm.

KEE provides an extensive graphical user interface to create, browse and manipulate frames.

KEE also includes a frame-based [[Production system (computer science)|rule system]]. Rules themselves are frames in the KEE knowledge base. Both forward and backward chaining inference is available.

KEE supports non-monotonic reasoning through the concepts of ''worlds''. Worlds allow provide alternative slot-values of frames. Through an assumption-based [[Truth maintenance system]] inconsistencies can be detected and analyzed.<ref>[http://portal.acm.org/citation.cfm?id=62001 Reasoning with worlds and truth maintenance]</ref>

''ActiveImages'' allows graphical displays to be attached to slots of Units. Typical examples are buttons, dials, graphs and histograms. The graphics are also implemented as Units via ''KEEPictures'' - a frame-based graphics library.

==See also==
* [[Expert system]]
* [[Frame language]]
* [[Inference engine]]
* [[IntelliCorp (software)|IntelliCorp]]
* [[Knowledge base]]
* [[Knowledge-based system]]
* [[Knowledge representation]]

==References==
<references/>

==External links==
* [http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/625 An Assessment of Tools for Building Large Knowledge-Based Systems]

[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
[[Category:Common Lisp software]]
<=====doc_Id=====>:880
<=====title=====>:
Deductive classifier
<=====text=====>:
A '''deductive classifier''' is a type of [[artificial intelligence]] [[inference engine]]. It takes as input a set of declarations in a [[frame language]] about a domain such as medical research or molecular biology. For example, the names of [[Class hierarchy|classes, sub-classes]], properties, and restrictions on allowable values.  The classifier determines if the various declarations are logically consistent and if not will highlight the specific inconsistent declarations and the inconsistencies among them. If the declarations are consistent the classifier can then assert additional information based on the input. For example, it can add information about existing classes, create additional classes, etc. This differs from traditional inference engines that trigger off of IF-THEN conditions in rules. Classifiers are also similar to [[Automated theorem proving|theorem provers]] in that they take as input and produce output via [[First Order Logic]]. Classifiers originated with [[KL-ONE]] [[Frame language]]s. They are increasingly significant now that they form a part in the enabling technology of the [[Semantic Web]]. Modern classifiers leverage the [[Web Ontology Language]]. The models they analyze and generate are called [[Ontologies (computer science)|ontologies]].<ref>{{cite journal|last=Berners-Lee|first=Tim|first2=James|last2=Hendler|first3=Ora|last3=Lassila|title=The Semantic Web A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities|journal=Scientific American|date=May 17, 2001|url=http://www.cs.umd.edu/~golbeck/LBSC690/SemanticWeb.html|doi=10.1038/scientificamerican0501-34|volume=284|pages=34–43}}</ref>

== History ==
A classic problem in [[knowledge representation]] for artificial intelligence is the trade off between the expressive power and the computational efficiency of the knowledge representation system. The most powerful form of knowledge representation is First Order Logic (FOL). However, it is not possible to implement knowledge representation that provides the complete expressive power of first order logic. Such a representation will include the capability to represent concepts such as the set of all integers which are impossible to iterate through. Implementing an assertion quantified for an infinite set by definition results in an undecidable non-terminating program. However, the problem is deeper than not being able to implement infinite sets. As Levesque demonstrated, the closer a knowledge representation mechanism comes to FOL, the more likely it is to result in expressions that require infinite or unacceptably large resources to compute.<ref>{{cite book|last=Levesque|first=Hector|title=Reading in Knowledge Representation|year=1985|publisher=Morgan Kaufmann|isbn=0-934613-01-X|page=49|author2=Ronald Brachman |editor=Ronald Brachman and Hector J. Levesque|chapter=A Fundamental Tradeoff in Knowledge Representation and Reasoning|quote=The good news in reducing KR service to theorem proving is that we now have a very clear, very specific notion of what the KR system should do; the bad new is that it is also clear that the services can not be provided... deciding whether or not a sentence in FOL is a theorem... is unsolvable.}}</ref>

As a result of this trade-off, a great deal of early work on knowledge representation for artificial intelligence involved experimenting with various compromises that provide a subset of FOL with acceptable computation speeds. One of the first and most successful compromises was to develop languages based predominately on [[modus ponens]], i.e. IF-THEN rules. [[Rule-based systems]] were the predominate knowledge representation mechanism for virtually all early [[expert systems]]. Rule-based systems provided acceptable computational efficiency while still providing powerful knowledge representation. Also, rules were highly intuitive to knowledge workers. Indeed, one of the data points that encouraged researchers to develop rule-based knowledge representation was psychological research that humans often represented complex logic via rules.<ref>{{cite book|last=Hayes-Roth|first=Frederick|pages=6–7|title=Building Expert Systems|year=1983|publisher=Addison-Wesley|isbn=0-201-10686-8|first2=Donald|last2=Waterman|first3=Douglas|last3=Lenat}}</ref>

However, after the early success of rule-based systems there arose more pervasive use of frame languages instead of or more often combined with rules. Frames provided a more natural way to represent certain types of concepts, especially concepts in subpart or subclass hierarchies. This led to development of a new kind of inference engine known as a classifier. A classifier could analyze a class hierarchy (also known as an [[Ontology (computer science)|ontology]]) and determine if it was valid. If the hierarchy was invalid the classifier would highlight the inconsistent declarations. For a language to utilize a classifier it required a formal foundation. The first language to successfully demonstrate a classifier was the KL-ONE family of languages. The [[Loom (ontology)|LOOM language]] from ISI was heavily influenced by KL-ONE.  LOOM also was influenced by the rising popularity of object-oriented tools and environments. Loom provided a true object-oriented capability (e.g. message passing) in addition to Frame language capabilities. Classifiers play a significant role in the vision for the next generation Internet known as the Semantic Web. The Web Ontology Language provides a formalism that can be validated and reasoned on via classifiers such as Hermit and Fact++.<ref>{{cite journal|last1=MacGregor|first1=Robert|title=A Descriptive Classifier for the Predicate Calculus|journal=AAAI - 94 Proceedings|date=1994|url=http://www.aaai.org/Papers/AAAI/1994/AAAI94-033.pdf|accessdate=17 July 2014}}</ref>

== Implementations ==
[[File:Protégé 3.4.3.png|500px|thumbnail|right|Protege Ontology Editor]]
The earliest versions of classifiers were [[Automated theorem proving|logic theorem provers]]. The first classifier to work with a [[Frame language]] was the [[KL-ONE]] classifier.<ref>{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers & Mathematics with Applications | volume = 23 | issue = 2–5 | pages = 133–177 | year = 1992 | pmid =  | pmc = }}</ref><ref>{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171–216 | year = 1985 | pmid =  | pmc = }}</ref> A later system built on common lisp was LOOM from the Information Sciences Institute. LOOM provided true object-oriented capabilities leveraging the Common Lisp Object System, along with a frame language.<ref>{{cite journal|last=MacGregor|first=Robert|title=Using a description classifier to enhance knowledge representation|journal=IEEE Expert|date=June 1991|volume=6|issue=3|url=http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=87683&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D87683|accessdate=10 November 2013|doi=10.1109/64.87683|pages=41–46}}</ref> In the Semantic Web the [[Protégé (software)|Protege]] tool from [[Stanford University|Stanford]] provides classifiers (also known as reasonsers) as part of the default environment.<ref>{{cite web|title=Protege Wiki: Reasoners that integrate with Protege|url=http://protegewiki.stanford.edu/wiki/Reasoning|publisher=Stanford University|accessdate=19 July 2014}}</ref>

== External links ==
* [http://owl.man.ac.uk/factplusplus/ Fact++ Reasoner]
* [http://hermit-reasoner.com/ HermiT Reasoner]
* [http://protege.stanford.edu/ Protege Ontology Editor]

==References==
{{Reflist}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Ontology languages]]
[[Category:Classification algorithms]]
<=====doc_Id=====>:883
<=====title=====>:
User profile
<=====text=====>:
{{pp-move-indef|small=yes}}
{{pp-semi-indef|small=yes}}
{{for|Wikipedia's guideline on its own user pages|WP:USERPAGE}}

{{unreferenced|date=September 2014}}
[[File:User Profile Info Model.png|thumb|User Profile Info Model]]

A '''user profile''' is a visual display of [[personal data]] associated with a specific [[User (computing)|user]], or a [[customized]] [[desktop environment]]. A profile refers therefore to the explicit digital representation of a person's [[Online identity|identity]]. A user profile can also be considered as the computer representation of a [[user modeling|user model]].

A profile can be used to store the description of the characteristics of person. This information can be exploited by systems taking into account the persons' characteristics and preferences.

[[Profiling (information science)|Profiling]] is the process that refers to construction of a profile via the extraction from a set of data.

User profiles can be found on [[operating system]]s, [[computer program]]s, [[recommender system]]s, or [[Website|dynamic websites]] (such as [[Social network service|online social networking]] sites or [[bulletin board]]s).

==See also==
*[[Online identity]]
*[[Online identity management]]
*[[Personally identifiable information]]
*[[Web mining]]
*[[Internet privacy]]

{{Online social networking}}
{{Social networking}}

[[Category:Identity management]]
[[Category:Knowledge representation]]
[[Category:Software features]]


{{compu-stub}}
<=====doc_Id=====>:886
<=====title=====>:
VisiRule
<=====text=====>:
{{COI|date=September 2014}}

'''VisiRule''' is a graphical tool for non-programmers to develop and deliver rule-based and expert systems simply by drawing their decision logic.

VisiRule is designed for building regulatory compliance systems, financial and legal decision-making systems, and machine
diagnostic and medical systems.

VisiRule generates executable rules in the form of the [[Flex expert system|Flex expert system toolkit]] which was developed by [[Logic Programming Associates|LPA]] in 1989.

LPA set up a dedicated website for VisiRule in 2015 at http://www.visirule.co.uk

VisiRule is used as part of expert systems and decision support courses in Universities such as Uniten.<ref name = "VisiRule Lab">{{citation |url=http://metalab.uniten.edu.my/~zaihisma/dss/lab/CISB434-module1.ppt | title=VisiRule Slides from Uniten}}</ref>

==Academic Uses==
In RSA-Expert, VisiRule is used as a decision support tool, in which the rules are basically and precisely presented using a Logic Programming model. RSA-Expert aims to assist researchers in making a decision about utilizing suitable statistical data analysis in research.<ref name = "RSA-Expert">{{citation |url=http://iiste.org/Journals/index.php/IKM/article/view/4740 | title=Use of a Rule Tool in Data Analysis Decision Making}}</ref>

TPA-EXPERT is a legal expert system which deals with transfer of property act of Indian legal domain. TPA-EXPERT has a simple representation structure which combine time tested rule based and case based approach.<ref name = "TPA-EXPERT">{{citation |url=http://www.ijcaonline.org/archives/volume33/number9/4045-5494 | title=TPA-EXPERT: A Hybrid Legal Knowledge Based System for Indian Legal domain}}</ref>

==External links==
* [http://www.lpa.co.uk/vsr.htm VisiRule Overview, LPA website]
* [http://www.visirule.co.uk/ VisiRule website]
* [http://www.pcai.com/18.3_sample_issue/18.3%20sample%20PDF/PCAI_LPA-pg.29-30-Sample_Issue.pdf "The Visual Development of Rule-Based Systems", PCAI Magazine]
* [http://www.academicpub.com/map/items/2999654.html "Drawing on your Knowledge with VisiRule", C.Spenser, IEEE]

==See also==
* [[Expert system]]
* [[Inference engine]]
* [[Knowledge base]]
* [[Knowledge-based system]]
* [[Knowledge representation]]

== References ==
{{Reflist}}

[[Category:Artificial intelligence stubs]]
[[Category:Expert systems]]
[[Category:Rule engines]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:889
<=====title=====>:
PowToon
<=====text=====>:
{{Underlinked|date=March 2014}}
{{Infobox company
| name = PowToon
| type = [[Private company|Private]]
| foundation = {{Start date and age|2012}}
| location_city =  28 Church Rd<br />[[London, UK]]
| location_country = [[United Kingdom]]
| key_people = Ilya Spitalnik (Co-Founder and CEO), Daniel Zaturansky (Co-Founder and COO), Sven Hoffman (Co-Founder and CTO)
| industry = [[Internet Marketing]]
| products = PowToon Web-based animation software
| homepage = {{URL|www.powtoon.com}}
}}
'''PowToon''' is a company which sells cloud-based software [[Software as a service|(SaaS)]] for creating animated presentations and animated explainer videos.<ref>Perez, Sarah. [http://techcrunch.com/2012/06/26/now-everyone-can-make-marketing-videos-powtoon-launches-diy-presentation-tool/ TechCrunch], June 26th, 2012, "Now Everyone Can Make Marketing Videos: PowToon Launches DIY Presentation Tool"</ref>

== History ==
PowToon was founded in January 2012. The company released a [[Software_release_life_cycle#BETA|beta]] version in August 2012 and has seen fast subscriber growth since.<ref name="powtoon">[http://www.powtoon.com Powtoon Website]</ref> In December 2012 PowToon secured $600,000 investment from LA based Venture Capital firm Startup Minds.<ref>Perez, Sarah, [http://techcrunch.com/2012/12/14/diy-animation-platform-powtoon-grabs-600k-for-its-video-creation-software/ TechCrunch] , Dec 14, 2012, "DIY Animation Platform PowToon Grabs $600K For Its Video Creation Software"</ref> In February 2013 PowToon introduced a free account option allowing users to create animated videos that can be exported to [[YouTube]]. The free videos include the PowToon branding.

== Product ==
PowToon is Web-based animation software that allows users to create animated presentations  by manipulating pre-created objects, imported images, provided music and user created voice-overs.<ref>{{cite web|last=Mersand |first=Shannon |title=Product Review: PowToon|url=http://www.techlearning.com/product-reviews/0072/product-review-powtoon-/54971|publisher=''Tech and Learning''|accessdate=12 May 2014|date=May 2014}}</ref> 
Powtoon uses an [[Apache Flex]] engine to generate an XML file that can be played in the Powtoon online viewer, exported to YouTube or downloaded as an MP4 file.<ref name="powtoon" />

PowToon is also available on the Google Chrome Store<ref>{{citation |title=PowToon - Chrome Web Store|url=https://chrome.google.com/webstore/detail/powtoon/aomfhbjiekjcbeefclbidjgnikfbooem?hl=en|accessdate=25 February 2015|date=Feb 2015}}</ref>  and has an application on Edmodo.com.<ref>{{citation |title=PowToon by PowToon Ltd|url=https://www.edmodo.com/store/app/powtoon-1|accessdate=25 February 2015|date=Feb 2015}}</ref>

== References ==
{{reflist}}

== External links ==
* {{official website|http://www.powtoon.com/}}

[[Category:Animation software]]
[[Category:Companies established in 2012]]
[[Category:Cloud applications]]
[[Category:Computer animation]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]

{{animation-stub}}
<=====doc_Id=====>:892
<=====title=====>:
Semantic network
<=====text=====>:
{{Network Science}}

A '''semantic network''', or '''frame network''',  is a network that represents [[Semantics|semantic]] relations between [[concept]]s. This is often used as a form of [[Knowledge representation and reasoning|knowledge representation]]. It is a [[directed graph|directed]] or [[undirected graph]] consisting of [[vertex (graph theory)|vertices]], which represent [[concept]]s, and [[graph theory|edges]], which represent semantic relations between concepts.<ref name = 'Sowa'/>

Typical standardized semantic networks are expressed as [[semantic triple]]s.

== History ==
[[Image:Semantic Net.svg|thumb|320px|Example of a semantic network]]
"Semantic Nets" were first invented for [[computers]] by [[Richard H. Richens]] of the Cambridge Language Research Unit in 1956 as an "[[Pivot language|interlingua]]" for [[machine translation]] of [[natural language]]s.{{citation needed|date=October 2013}}

They were independently developed by Robert F. Simmons,<ref name='Simmons1963'>{{cite journal | title=Synthetic language behavior | journal=Data Processing Management | year=1963 | last=Robert F. Simmons |volume=5 |issue=12 |pages=11–18}}</ref> Sheldon Klein, Karen McConologue, M. Ross Quillian<ref name='Quillian1963'>Quillian, R. A notation for representing conceptual information: An application to semantics and mechanical English para- phrasing. SP-1395, System Development Corporation, Santa Monica, 1963.</ref> and others at [[System Development Corporation]] in the early 1960s as part of the SYNTHEX project. It later featured prominently in the work of [[Allan M. Collins]] and Quillian (e.g., Collins and Quillian;<ref name='Collins1969'>{{cite journal | title=Retrieval time from semantic memory | journal=Journal of verbal learning and verbal behavior | year=1969 | last1=Allan M. Collins |author2= M. R. Quillian |volume=8 |issue=2 |pages=240–247 |doi=10.1016/S0022-5371(69)80069-1  }}</ref><ref name='Collins1970'>{{cite journal |title=Does category size affect categorization time? |journal=Journal of verbal learning and verbal behavior |year=1970 |first= |last=Allan M. Collins
|author2=M. Ross Quillian  |volume=9 |issue=4 |pages=432–438 |doi=10.1016/S0022-5371(70)80084-6 }}</ref> Collins and Loftus<ref name='Collins1975'>{{cite journal |title=A spreading-activation theory of semantic processing |journal=Psychological Review |year=1975 |last=Allan M. Collins |author2=Elizabeth F. Loftus |volume=82 | doi = 10.1037/0033-295x.82.6.407 |pages=407–428}}</ref> Quillian<ref>Quillian, M. R. (1967). Word concepts: A theory and simulation of some basic semantic capabilities. Behavioral Science, 12(5), 410-430.</ref><ref>Quillian, M. R. (1968). Semantic memory. Semantic information processing, 227–270.</ref><ref>{{cite journal | last1 = Quillian | first1 = M. R. | year = 1969 | title = The teachable language comprehender: a simulation program and theory of language | url = | journal = Communications of the ACM | volume = 12 | issue = 8| pages = 459–476 | doi=10.1145/363196.363214}}</ref><ref>Quillian, R. Semantic Memory. Unpublished doctoral dissertation, Carnegie Institute of Technology, 1966.</ref>)

In the late 1980s, two [[Netherlands]] universities, [[University of Groningen|Groningen]] and [[University of Twente|Twente]], jointly began a project called ''Knowledge Graphs'', which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations, to facilitate algebras on the graph.<ref>{{cite book |last=Van de Riet |first=R. P. |date=1992 |title=Linguistic Instruments in Knowledge Engineering |url=http://www.stokman.org/artikel/92Jame.KnowGraphs.LIKE.pdf |publisher=Elsevier Science Publishers |page=98 |isbn=0444883940}}</ref> In the subsequent decades, the distinction between semantic networks and knowledge graphs was blurred.<ref>{{cite conference |url=https://books.google.com/books?id=15PDCgAAQBAJ&pg=PA444 |title=Path-Based Semantic Relatedness on Linked Data and Its Use to Word and Entity Disambiguation |last1=Hulpus |first1=Ioana |last2=Prangnawarat |first2=Narumol |date=2015 |publisher=Springer International Publishing |book-title=The Semantic Web - ISWC 2015: 14th International Semantic Web Conference, Bethlehem, PA, USA, October 11-15, 2015, Proceedings, Part 1 |pages=444 |conference=[[International Semantic Web Conference]] 2015}}</ref><ref>{{cite web |url=https://www.authorea.com/users/6341/articles/107281 |title=What is a Knowledge Graph? |last1=McCusker |first1=James P. |last2=Chastain |first2=Katherine |date=April 2016 |website=authorea.com |access-date=15 June 2016 |quote="usage [of the term 'knowledge graph'] has evolved"}}</ref> In 2012, [[Google]] gave their knowledge graph the name [[Knowledge Graph]].

== Basics of semantic networks ==
A semantic network is used when one has knowledge that is best understood as a set of concepts that are related to one another.

Most semantic networks are cognitively based. They also consist of arcs and nodes which can be organized into a taxonomic hierarchy. Semantic networks contributed ideas of [[spreading activation]], [[inheritance]], and nodes as proto-objects.

== Examples ==

=== Semantic Net in [[Lisp (programming language)|Lisp]] ===
Using an association list.
<source lang="lisp">
(defun *database* ()
'((canary  (is-a bird)
           (color yellow)
           (size small))
  (penguin (is-a bird)
           (movement swim))
  (bird    (is-a vertebrate)
           (has-part wings)
           (reproduction egg-laying))))
</source>

You would use the "assoc" function with a key of "canary" to extract all the information about the "canary" type.<ref>{{cite web|last=Swigger|first=Kathleen|title=Semantic.ppt|url=http://zeus.csci.unt.edu/swigger/csci3210/semantic.ppt|accessdate=23 March 2011}}</ref>

=== WordNet ===
{{Main|WordNet}}
An example of a semantic network is [[WordNet]], a [[lexicon|lexical]] database of [[English language|English]]. It groups English words into sets of synonyms called [[synsets]], provides short, general definitions, and records the various semantic relations between these synonym sets. Some of the most common semantic relations defined are [[meronymy]] (A is part of B, i.e. B has A as a part of itself), [[holonymy]] (B is part of A, i.e. A has B as a part of itself), [[hyponym]]y (or [[troponymy]])  (A is subordinate of B; A is kind of B), [[hypernym]]y (A is superordinate of B), [[synonym]]y (A denotes the same as B) and [[antonym]]y (A denotes the opposite of B).

WordNet properties have been studied from a [[Graph theory|network theory]] perspective and compared to other semantic networks created from [[Roget's Thesaurus]] and [[word association]] tasks.  From this perspective the three of them are a [[Small-world network|small world structure]].<ref name=Steyvers2005>{{cite journal
 | author = Steyvers, M.
 |author2=Tenenbaum, J.B.
  | year = 2005
 | title = The Large-Scale Structure of Semantic Networks: Statistical Analyses and a Model of Semantic Growth
 | journal = Cognitive Science
 | volume = 29
 | issue = 1
 | pages = 41–78
 | doi = 10.1207/s15516709cog2901_3
}}</ref>

=== Other examples ===
It is also possible to represent logical descriptions using semantic networks such as the [[existential graph]]s of [[Charles Sanders Peirce]] or the related [[conceptual graph]]s of [[John F. Sowa]].<ref name='Sowa'>{{cite encyclopedia
|author=John F. Sowa
|editor=Stuart C Shapiro
|encyclopedia=Encyclopedia of Artificial Intelligence
|title=Semantic Networks
|url=http://www.jfsowa.com/pubs/semnet.htm
|accessdate=2008-04-29
|year=1987
|authorlink=John F. Sowa}}</ref> These have expressive power equal to or exceeding standard [[first-order predicate calculus|first-order predicate logic]].  Unlike WordNet or other lexical or browsing networks, semantic networks using these representations can be used for reliable automated logical deduction.  Some automated reasoners exploit the graph-theoretic features of the networks during processing.

Other examples of semantic networks are [[Gellish]] models. [[Gellish English]] with its [[Gellish English dictionary]], is a [[formal language]] that is defined as a network of relations between concepts and names of concepts. Gellish English is a formal subset of natural English, just as Gellish Dutch is a formal subset of Dutch, whereas multiple languages share the same concepts. Other Gellish networks consist of knowledge models and information models that are expressed in the Gellish language. A Gellish network is a network of (binary) relations between things. Each relation in the network is an expression of a fact that is classified by a relation type. Each relation type itself is a concept that is defined in the Gellish language dictionary. Each related thing is either a concept or an individual thing that is classified by a concept. The definitions of concepts are created in the form of definition models (definition networks) that together form a Gellish Dictionary. A Gellish network can be documented in a Gellish database and is computer interpretable.

[[SciCrunch]] is a collaboratively edited knowledge base for scientific resources. It provides unambiguous identifiers (Research Resource IDentifiers or RRIDs) for software, lab tools etc. and it also provides options to create links between RRIDs and from communities.

Another example of semantic networks, based on [[category theory]], is [[olog]]s. Here each type is an object, representing a set of things, and each arrow is a morphism, representing a function. [[Commutative diagrams]] also are prescribed to constrain the semantics.

In the social sciences people sometimes use the term semantic network to refer to [[co-occurrence networks]].<ref name='Atteveldt'>{{cite book
|author=Wouter Van Atteveldt
|title=Semantic Network Analysis: Techniques for Extracting, Representing, and Querying Media Content
|publisher=BookSurge Publishing
|year=2008}}</ref> The basic idea is that words that co-occur in a unit of text, e.g. a sentence, are semantically related to one another. Ties based on co-occurrence can then be used to construct semantic networks.

== Software tools ==
There are also elaborate types of semantic networks connected with corresponding sets of software tools used for [[Lexicon|lexical]] [[knowledge engineering]], like the Semantic Network Processing System ([[SNePS]]) of Stuart C. Shapiro<ref>[http://www.cse.buffalo.edu/~shapiro/ Stuart C. Shapiro]</ref> or the [[MultiNet]] paradigm of Hermann Helbig,<ref>[http://pi7.fernuni-hagen.de/helbig/index_en.html Hermann Helbig]</ref> especially suited for the semantic representation of natural language expressions and used in several [[Natural language processing|NLP]] applications.

Semantic networks are used in specialized information retrieval tasks, such as [[plagiarism]] detection. They provide information on hierarchical relations in order to employ [[semantic compression]] to reduce language diversity and enable the system to match word meanings, independently from sets of words used.

== See also ==
{{Div col}}
* [[Abstract semantic graph]]
* [[Chunking (psychology)]]
* [[Network diagram]]
* [[Ontology (information science)]]
* [[Repertory grid]]
* [[Semantic lexicon]]
* [[Semantic neural network]]
* [[SemEval]] - an ongoing series of evaluations of [[Semantic analysis (computational)|computational semantic analysis]] systems
* [[Sparse distributed memory]]
* [[Taxonomy (general)]]
* [[Unified Medical Language System]] (UMLS)
* [[Word-sense disambiguation]] (WSD)
{{Div col end}}

=== Other examples ===
* [[Cognition Network Technology]]
* [[Lexipedia]]
* [[Open Mind Common Sense]] (OMCS)
* [[Schema.org]]
* [[SNOMED CT]]
* [[Universal Networking Language]] (UNL)
* [[Wikidata]]

== References ==
{{reflist|30em}}

== Further reading ==
* Allen, J. and A. Frisch (1982). "What's in a Semantic Network". In: ''Proceedings of the 20th. annual meeting of ACL'', Toronto, pp.&nbsp;19–27.
* John F. Sowa, Alexander Borgida (1991). ''Principles of Semantic Networks: Explorations in the Representation of Knowledge''.

== External links ==
{{Commons category|Semantic networks}}
* [http://www.jfsowa.com/pubs/semnet.htm "Semantic Networks"] by John F. Sowa
* [http://www.knowledgegrid.net/~H.Zhuge/SLN.htm "Semantic Link Network" ] by Hai Zhuge

{{Semantic Web}}
{{Use dmy dates|date=August 2011}}

{{Authority control}}

[[Category:Knowledge representation]]
[[Category:Networks]]
<=====doc_Id=====>:895
<=====title=====>:
WordNet
<=====text=====>:
'''WordNet''' is a [[lexical database]] for the [[English language]].<ref>G. A. Miller, R. Beckwith, C. D. Fellbaum, D. Gross, K. Miller. 1990. WordNet: An online lexical database. Int. J. Lexicograph. 3, 4, pp. 235–244.</ref> It groups English [[word]]s into sets of [[synonyms]] called ''[[synsets]]'', provides short definitions and usage examples, and records a number of relations among  these synonym sets or their members. WordNet can thus be seen as a combination of [[dictionary]] and [[thesaurus]]. While it is  accessible to human users via a [[web browser]],<ref name="WordNet Search">{{cite web|url=http://wordnetweb.princeton.edu/perl/webwn|title=WordNet Search - 3.1}}</ref> its primary use is in automatic [[natural language processing|text analysis]] and [[artificial intelligence]] applications. The [[database]] and [[software]] tools have been released under a [[BSD License|BSD style license]] and are freely available for download from the WordNet website. Both the lexicographic data (''lexicographer files'') and the compiler (''called grind'') for producing the distributed database are available.
[[File:WordNet.PNG|thumb|This is a snapshot of WordNet's definition of itself.]]

== History and team members ==
WordNet was created in the [[Cognitive Science]] Laboratory of [[Princeton University]] under the direction of  [[psychology]] [[professor]] [[George Armitage Miller]] starting in 1985 and has been directed in recent years by [[Christiane Fellbaum]]. The project received funding from government agencies including the [[National Science Foundation]], [[DARPA]], the [[Disruptive Technology Office]] (formerly the Advanced Research and Development Activity), and REFLEX. George Miller and Christiane Fellbaum were awarded the 2006 [[European Language Resources Association#Antonio Zampolli Prize|Antonio Zampolli Prize]] for their work with WordNet.

== Database contents ==
[[File:Hamburger WordNet.png|thumb|Example entry "Hamburger" in WordNet]]

As of November 2012 WordNet's latest Online-version is 3.1.<ref>{{cite web|url=http://wordnet.princeton.edu/wordnet/download/current-version/ |title=Current WordNet version |publisher=Wordnet.princeton.edu |date=2012-11-09 |accessdate=2014-03-11}}</ref> The  database contains 155,287 words organized in 117,659 [[synsets]] for a total of 206,941 word-sense pairs; in [[data compression|compressed]] form, it is about 12 [[megabyte]]s in size.<ref>{{cite web|url=http://wordnet.princeton.edu/wordnet/man/wnstats.7WN.html |title=WordNet Statistics |publisher=Wordnet.princeton.edu |date= |accessdate=2014-03-11}}</ref>

WordNet includes the lexical categories [[noun]]s, [[verb]]s, [[adjective]]s and [[adverb]]s but ignores [[preposition]]s, [[determiner (linguistics)|determiner]]s and other function words.

Words from the same lexical category that are roughly synonymous are grouped into [[synsets]]. Synsets include simplex words as well as [[collocation]]s like "eat out" and "car pool." The different senses of a [[polysemous]] word form are assigned to different synsets. The meaning of a synset is further clarified with a short defining ''gloss'' and one or more usage examples. An example adjective synset is:

: good, right, ripe – (most suitable or right for a particular purpose; "a good time to plant tomatoes"; "the right time to act"; "the time is ripe for great sociological changes")

All synsets are connected to other synsets by means of semantic relations. These relations, which are not all shared by all lexical categories, include:
* [[Noun]]s
**''[[hypernym]]s'': ''Y'' is a hypernym of ''X'' if every ''X'' is a (kind of) ''Y'' (''canine'' is a hypernym of ''[[dog]]'')
**''[[hyponym]]s'': ''Y'' is a hyponym of ''X'' if every ''Y'' is a (kind of) ''X'' (''dog'' is a hyponym of ''canine'')
**''coordinate terms'': ''Y'' is a coordinate term of ''X'' if ''X'' and ''Y'' share a hypernym (''wolf'' is a coordinate term of ''dog'', and ''dog'' is a coordinate term of ''wolf'')
**''[[meronymy|meronym]]'': ''Y'' is a meronym of ''X'' if ''Y'' is a part of ''X'' (''window'' is a meronym of ''building'')
**''[[holonymy|holonym]]'': ''Y'' is a holonym of ''X'' if ''X'' is a part of ''Y'' (''building'' is a holonym of ''window'')
* [[Verb]]s
**''hypernym'': the verb ''Y'' is a hypernym of the verb ''X'' if the activity ''X'' is a (kind of) ''Y'' (''to perceive'' is an hypernym of ''to listen'')
**''[[troponym]]'': the verb ''Y'' is a troponym of the verb ''X'' if the activity ''Y'' is doing ''X'' in some manner (''to lisp'' is a troponym of ''to talk'')
**''[[entailment]]'': the verb ''Y'' is entailed by ''X'' if by doing ''X'' you must be doing ''Y'' (''to sleep'' is entailed by ''to snore'')
**''coordinate terms'': those verbs sharing a common hypernym (''to lisp'' and ''to yell'')

These semantic relations hold among all members of the linked synsets. Individual synset members (words) can also be connected with  lexical relations. For example, (one sense of) the noun "director" is linked to (one sense of) the verb "direct" from which it is derived via a "morphosemantic" link.

The morphology functions of the software distributed with the database try to deduce the [[Lemma (morphology)|lemma]] or [[stem (linguistics)|stem]] form of a [[word]] from the user's input. Irregular forms are stored in a list, and looking up "ate" will return "eat," for example.

== Knowledge structure ==
Both nouns and verbs are organized into hierarchies, defined by [[hypernym]] or ''[[is-a|IS A]]'' relationships. For instance, one sense of the word ''dog'' is found following hypernym hierarchy; the words at the same level represent synset members.  Each set of synonyms has a unique index.
{{tree list}}
* {{Tree list/final branch}}dog, domestic dog, Canis familiaris
** {{Tree list/final branch}}canine, canid
*** {{Tree list/final branch}}carnivore
**** {{Tree list/final branch}}placental, placental mammal, eutherian, eutherian mammal
***** {{Tree list/final branch}}mammal
****** {{Tree list/final branch}}vertebrate, craniate
******* {{Tree list/final branch}}chordate
******** {{Tree list/final branch}}animal, animate being, beast, brute, creature, fauna
********* {{Tree list/final branch}}...
{{tree list/end}}
At the top level, these hierarchies are organized into 25 beginner "trees" for nouns and 15 for verbs (called ''lexicographic files'' at a maintenance level).  All are linked to a unique beginner synset, "entity."
Noun hierarchies are far deeper than verb hierarchies

Adjectives are not organized into hierarchical trees. Instead, two "central" antonyms such as "hot" and "cold" form binary poles, while 'satellite' synonyms such as "steaming" and "chilly" connect to their respective poles via a "similarity" relations. The adjectives can be visualized in this way as "dumbbells" rather than as "trees."

== Psycholinguistic aspects of WordNet ==

The initial goal of the WordNet project was to build a lexical database that would be consistent with theories of human semantic memory developed in the late 1960s.  Psychological experiments indicated that speakers organized their knowledge of concepts in an economic, hierarchical fashion. Retrieval time required to access conceptual knowledge seemed to be directly related to the number of hierarchies the speaker needed to "traverse" to access the knowledge. Thus, speakers could more quickly verify that ''canaries can sing'' because a canary is a songbird ("sing" is a property stored on the same level as "canary"), but required slightly more time to verify that ''canaries can fly'' (where they had to access the concept "bird" on the superordinate level) and even more time to verify ''canaries have skin'' (requiring look-up across multiple levels of hyponymy, up to "animal").<ref>Collins A., Quillian M. R. 1972. Experiments on Semantic Memory and Language Comprehension. In ''Cognition in Learning and Memory''. Wiley, New York.</ref>
While such experiments and the underlying theories have been subject to criticism, some of WordNet's organization is consistent with experimental evidence. For example, [[anomic aphasia]] selectively affects speakers' ability to produce words from a specific semantic category, a WordNet hierarchy. Antonymous adjectives (WordNet's central adjectives in the dumbbell structure) are found to co-occur far more frequently than chance, a fact that has been found to hold for many languages.

== WordNet as a lexical ontology ==

WordNet is sometimes called an ontology, a persistent claim that its creators do not make. The hypernym/hyponym relationships among the noun synsets can be interpreted as specialization relations among conceptual categories. In other words, WordNet can be interpreted and used as a lexical [[ontology (computer science)|ontology]] in the [[computer science]] sense. However, such an ontology should normally be corrected before being used since it contains hundreds of basic semantic inconsistencies such as (i) the existence of common specializations for exclusive categories and (ii) redundancies in the specialization hierarchy. Furthermore, transforming WordNet into a lexical ontology usable for knowledge representation should normally also involve (i)&nbsp;distinguishing the specialization relations into ''subtypeOf'' and ''instanceOf'' relations, and (ii)&nbsp;associating intuitive unique identifiers to each category. Although such corrections and transformations have been performed and documented as part of the integration of WordNet&nbsp;1.7 into the cooperatively updatable knowledge base of WebKB-2,<ref>{{cite web|author=http://www.phmartin.info |url=http://www.webkb.org/doc/wn/ |title=Integration of WordNet 1.7 in WebKB-2|publisher=Webkb.org |date= |accessdate=2014-03-11}}</ref> most projects claiming to re-use WordNet for knowledge-based applications (typically, knowledge-oriented information retrieval) simply re-use it directly.

WordNet has also been converted to a formal specification, by means of a hybrid bottom-up top-down methodology to automatically extract association relations from WordNet, and interpret these associations in terms of a set of conceptual relations, formally defined in the [[Upper ontology (computer science)#DOLCE and DnS|DOLCE foundational ontology]].<ref>{{cite book |first1=A. |last1=Gangemi |first2=R. |last2=Navigli |first3=P. |last3=Velardi |url=http://www.w3.org/2001/sw/BestPractices/WNET/ODBASE-OWN.pdf |format=PDF |title=The OntoWordNet Project: Extension and Axiomatization of Conceptual Relations in WordNet |work= Proc. of International Conference on Ontologies, Databases and Applications of SEmantics (ODBASE 2003) |location=Catania, Sicily (Italy) |year=2003 |pages= 820–838}}</ref>

In most works that claim to have integrated WordNet into ontologies, the content of WordNet has not simply been corrected when it seemed necessary; instead, WordNet has been heavily re-interpreted and updated whenever suitable. This was the case when, for example, the top-level ontology of WordNet was re-structured<ref>{{cite conference | first1 = A. | last1 = Oltramari | first2 = A. | last2 = Gangemi | first3 = N. | last3 = Guarino | first4 = C. | last4 = Masolo | date = 2002 | title = Restructuring WordNet's Top-Level: The OntoClean approach | citeseerx = 10.1.1.19.6574 | conference = OntoLex'2 Workshop, Ontologies and Lexical Knowledge Bases (LREC 2002) | location = Las Palmas, Spain | pages = 17–26 }}</ref> according to the [[OntoClean]] based approach or when WordNet was used as a primary source for constructing the lower classes of the SENSUS ontology.

== Limitations ==

WordNet does not include information about the [[etymology]] or the pronunciation of words and it contains only limited information about usage.
WordNet aims to cover most of everyday English and does not include much domain-specific terminology.

WordNet is the most commonly used computational lexicon of English for [[word sense disambiguation]] (WSD), a task aimed to assigning the context-appropriate meanings (i.e. synset members) to words in a text.<ref>R. Navigli. [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ''ACM Computing Surveys'', 41(2), 2009, pp. 1–69</ref> However, it has been argued that WordNet encodes sense distinctions that are too fine-grained. This issue prevents WSD systems from achieving a level of performance comparable to that of humans, who do not always agree when confronted with the task of selecting a sense from a dictionary that matches a word in a context. The granularity issue has been tackled by proposing [[cluster analysis|clustering]] methods that automatically group together similar senses of the same word.<ref>E. Agirre, O. Lopez. 2003.
Clustering WordNet Word Senses. In ''Proc. of the Conference on Recent Advances on Natural Language (RANLP’03)'', Borovetz, Bulgaria, pp. 121–130.</ref><ref>R. Navigli. [http://acl.ldc.upenn.edu/P/P06/P06-1014.pdf Meaningful Clustering of Senses Helps Boost Word Sense Disambiguation Performance], In ''Proc. of the 44th Annual Meeting of the Association for Computational Linguistics joint with the 21st International Conference on Computational Linguistics (COLING-ACL 2006)'', Sydney, Australia, July 17-21st, 2006, pp. 105–112.</ref><ref>R. Snow, S. Prakash, D. Jurafsky, A. Y. Ng. 2007. [http://www.aclweb.org/anthology/D/D07/D07-1107.pdf Learning to Merge Word Senses], ''In Proc. of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)'', Prague, Czech Republic, pp. 1005–1014.</ref>

=== Licensed vs. Open WordNets ===
Some wordnets were subsequently created for other languages. A 2012 survey lists the wordnets and their availability.<ref>Francis Bond and Kyonghee Paik 2012a. [http://web.mysites.ntu.edu.sg/fcbond/open/pubs/2012-gwc-wn-license.pdf A survey of wordnets and their licenses]. In Proceedings of the 6th Global WordNet Conference (GWC 2012). Matsue. 64–71</ref> In an effort to propagate the usage of WordNets, the Global WordNet community had been slowly re-licensing their WordNets to an open domain where researchers and developers can easily access and use WordNets as language resources to provide [[ontology|ontological]] and [[lexicon|lexical]] knowledge in [[Natural Language Processing]] tasks.

The Open Multilingual WordNet<ref>http://compling.hss.ntu.edu.sg/omw/</ref> provides access to [[Open-source license|open licensed]] wordnets in a variety of languages, all linked to the Princeton Wordnet of English (PWN). The goal is to make it easy to use wordnets in multiple languages.

== Applications ==

WordNet has been used for a number of different purposes in information systems, including [[word-sense disambiguation]], [[information retrieval]], [[Document classification|automatic text classification]], [[Automatic summarization|automatic text summarization]], [[machine translation]] and even automatic crossword puzzle generation.

A common use of WordNet is to determine the [[semantic similarity|similarity]] between words. Various algorithms have been proposed, and these include measuring the distance among the words and synsets in WordNet's graph structure, such as by counting the number of edges among synsets. The intuition is that the closer two words or synsets are, the closer their meaning. A number of WordNet-based word similarity algorithms are implemented in a [[Perl]] package called WordNet::Similarity,<ref>{{cite web|url=http://www.d.umn.edu/~tpederse/similarity.html |title=Ted Pedersen - WordNet::Similarity |publisher=D.umn.edu |date=2008-06-16 |accessdate=2014-03-11}}</ref> and in a [[Python (programming language)|Python]] package called [[NLTK]].
Other more sophisticated WordNet-based similarity techniques include ADW,<ref>M. T. Pilehvar, D. Jurgens and R. Navigli.  [http://wwwusers.di.uniroma1.it/~navigli/pubs/ACL_2013_Pilehvar_Jurgens_Navigli.pdf Align, Disambiguate and Walk: A Unified Approach for Measuring Semantic Similarity.]. Proc. of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), Sofia, Bulgaria, August 4–9, 2013, pp. 1341-1351.</ref> whose implementation is available in [[Java (programming language)|Java]]. WordNet can also be used to inter-link other vocabularies.<ref>{{cite journal  |vauthors=Ballatore A, etal |volume=20|issue=2| arxiv=1404.5372| journal=Annals of GIS |title=Linking geographic vocabularies through WordNet |publisher= |date=2014}}</ref>

== Interfaces ==
Princeton maintains a list of related projects<ref>{{cite web|url=http://wordnet.princeton.edu/wordnet/related-projects/ |title=Related projects - WordNet - Related projects |publisher=Wordnet.princeton.edu |date=2014-01-06 |accessdate=2014-03-11}}</ref> that includes links to some of the widely used [[application programming interface]]s available for accessing WordNet using various programming languages and environments.

== Related projects and extensions ==
WordNet is connected to several databases of the [[Semantic Web]]. WordNet is also commonly re-used via mappings between the WordNet synsets and the categories from ontologies. Most often, only the top-level categories of WordNet are mapped.

===Global WordNet Association===
The Global WordNet Association (GWA)<ref>{{cite web|author=The Global WordNet Association |url=http://www.globalwordnet.org/ |title=globalwordnet.org |publisher=globalwordnet.org |date=2010-02-04 |accessdate=2014-03-11}}</ref> is a public and non-commercial organization that provides a platform for discussing, sharing and connecting wordnets for all languages in the world. The GWA also promotes the standardization of wordnets across different languages to ensure its uniformity in enumerating the different synsets in human languages. The GWA keeps a list of wordnets developed around the world.<ref>{{cite web|title=Wordnets in the World|url=http://www.globalwordnet.org/gwa/wordnet_table.html|archiveurl=https://web.archive.org/web/20111021114613/http://www.globalwordnet.org/gwa/wordnet_table.html |archivedate=2011-10-21}}</ref>

===Other languages===
* [[Arabic WordNet]]:<ref>Black W., Elkateb S., Rodriguez H., Alkhalifa M., Vossen P., Pease A., Bertran M., Fellbaum C., (2006) The Arabic WordNet Project, Proceedings of LREC 2006</ref><ref>Lahsen Abouenour, Karim Bouzoubaa, Paolo Rosso (2013) On the evaluation and improvement of Arabic WordNet coverage and usability, Language Resources and Evaluation 47(3) pp 891–917</ref> WordNet for Arabic language.
* [[Malayalam WordNet]] , developed by [[Cochin University of Science and Technology|Cochin University Of Science and Technology]]
* CWN (Chinese Wordnet or 中文詞彙網路) supported by [[National Taiwan University]].<ref>[http://lope.linguistics.ntu.edu.tw/cwn/ Chinese Wordnet (中文詞彙網路) official page] at National Taiwan University</ref>
* WOLF (WordNet Libre du Français), a French version of WordNet.<ref>S. Benoît, F. Darja. 2008. [http://alpage.inria.fr/~sagot/pub/Ontolex08.pdf Building a free French wordnet from multilingual resources]. In ''Proc. of Ontolex 2008'', Marrakech, Maroc.</ref>
* JAWS (Just Another WordNet Subset), another French version of WordNet<ref>C. Mouton, G. de Chalendar. 2010.[http://www.iro.umontreal.ca/~felipe/TALN2010/Xml/Papers/all/taln2010_submission_71.pdf JAWS : Just Another WordNet Subset]. In ''Proc. of TALN 2010''.</ref> built using the Wiktionary and semantic spaces
* The [[IndoWordNet]]<ref name="PushpakBhattacharyya">Pushpak Bhattacharyya, IndoWordNet, Lexical Resources Engineering Conference 2010 (LREC 2010), Malta, May, 2010.</ref> is a linked lexical knowledge base of wordnets of 18 scheduled languages of India.
* The MultiWordNet project,<ref>E. Pianta, L. Bentivogli, C. Girardi. 2002. [http://multiwordnet.itc.it/paper/MWN-India-published.pdf MultiWordNet: Developing an aligned multilingual database]. In ''Proc. of the 1st International Conference on Global WordNet'', Mysore, India, pp. 21–25.</ref> a multilingual WordNet aimed at producing an Italian WordNet strongly aligned with the Princeton WordNet.
* The [[EuroWordNet]] project<ref>P. Vossen, Ed. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer, Dordrecht, The Netherlands.</ref> has produced WordNets for several European languages and linked them together; these are not freely available however. The Global Wordnet project attempts to coordinate the production and linking of "wordnets" for all languages.<ref>{{cite web|url=http://www.globalwordnet.org/ |title=The Global WordNet Association |publisher=Globalwordnet.org |date=2010-02-04 |accessdate=2014-01-05}}</ref> [[Oxford University Press]], the publisher of the [[Oxford English Dictionary]], has voiced plans to produce their own online competitor to WordNet.{{Citation needed|date=May 2009}}
* The BalkaNet project<ref>D. Tufis, D. Cristea, S. Stamou. 2004. [http://www.racai.ro/~tufis/papers/Tufis-CS-ROMJIST2004.pdf Balkanet: Aims, methods, results and perspectives. A general overview]. ''Romanian J. Sci. Tech. Inform. (Special Issue on Balkanet)'', 7(1-2), pp. 9–43.</ref> has produced WordNets for six European languages (Bulgarian, Czech, Greek, Romanian, Turkish and Serbian). For this project, a freely available XML-based WordNet editor was developed. This editor – VisDic – is not in active development anymore, but is still used for the creation of various WordNets. Its successor, DEBVisDic, is client-server application and is currently used for the editing of several WordNets (Dutch in Cornetto project, Polish, Hungarian, several African languages, Chinese).
* UWN is an automatically constructed multilingual lexical knowledge base extending WordNet to cover over a million words in many different languages.<ref>{{cite web|url=http://www.mpi-inf.mpg.de/yago-naga/uwn |title=UWN: Towards a Universal Multilingual Wordnet - D5: Databases and Information Systems (Max-Planck-Institut für Informatik) |publisher=Mpi-inf.mpg.de |date=2011-08-14 |accessdate=2014-01-05}}</ref>
* Such projects as BalkaNet and EuroWordNet made it feasible to create standalone wordnets linked to the original one. One of such projects is Russian WordNet patronized by [[Petersburg State University of Means of Communication]]<ref>{{cite web|url=http://www.pgups.ru/abitur/inostrancam/inter/ruwordnet/ |title=Русский WordNet |publisher=Pgups.ru |date= |accessdate=2014-01-05}}</ref> or Russnet<ref>{{cite web|url=http://project.phil.spbu.ru/RussNet/index_ru.shtml |title=RussNet: Главная страница |publisher=Project.phil.spbu.ru |date= |accessdate=2014-03-11}}</ref> by [[Saint Petersburg State University]]
* FinnWordNet is a Finnish version of the WordNet where all entries of the original English WordNet were translated.<ref>{{cite web|url=http://www.ling.helsinki.fi/en/lt/research/finnwordnet/ |title=FinnWordNet – The Finnish WordNet - Department of General Linguistics |publisher=Ling.helsinki.fi |date= |accessdate=2014-01-05}}</ref>
* [[GermaNet]] is a German version of the WordNet developed by the University of Tübingen.<ref>{{cite web|url=http://www.sfs.uni-tuebingen.de/lsd/index.shtml |title=GermaNet |publisher=Sfs.uni-tuebingen.de |date= |accessdate=2014-03-11}}</ref>
* OpenWN-PT  is a Brazilian Portuguese version of the original WordNet freely available for download under CC-BY-SA license.<ref>{{cite web|url=https://github.com/arademaker/openWordnet-PT |title=arademaker/openWordnet-PT — GitHub |publisher=Github.com |date= |accessdate=2014-01-05}}</ref>
* [[plWordNet]]<ref>http://plwordnet.pwr.wroc.pl/wordnet/ official webpage</ref> is a Polish-language version of WordNet developed by [[Wrocław University of Technology]].
* PolNet<ref>http://www.ltc.amu.edu.pl/polnet/ official webpage</ref> is a Polish-language version of WordNet developed by [[Adam Mickiewicz University in Poznań]] (distributed under CC BY-NC-ND 3.0 license).
* [[BulNet]] is a Bulgarian version of the WordNet developed at the Department of Computational Linguistics of the [[Institute for Bulgarian Language]], Bulgarian Academy of Sciences.<ref>{{cite web|url=http://dcl.bas.bg/BulNet/general_en.html |title=BulNet |publisher=dcl.bas.bg |date= |accessdate=2015-05-07}}</ref>

===Linked data===

* [[BabelNet]],<ref>R. Navigli, S. P. Ponzetto. [http://www.aclweb.org/anthology/P/P10/P10-1023.pdf BabelNet: Building a Very Large Multilingual Semantic Network]. Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL 2010), Uppsala, Sweden, July 11–16, 2010, pp. 216–225.</ref> a very large multilingual [[semantic network]] with millions of concepts obtained from an integration of WordNet and Wikipedia based on an automatic mapping algorithm.
* The [[Suggested Upper Merged Ontology|SUMO]] ontology<ref>A. Pease, I. Niles, J. Li. 2002. [https://www.aaai.org/Papers/Workshops/2002/WS-02-11/WS02-11-011.pdf The suggested upper merged ontology: A large ontology for the Semantic Web and its applications]. In ''Proc. of the AAAI-2002 Workshop on Ontologies and the Semantic Web'', Edmonton, Canada.</ref> has produced a mapping between all of the WordNet synsets, (including nouns, verbs, adjectives and adverbs), and [[SUMO class]]es.  The most recent addition of the mappings provides links to all of the more specific terms in the MId-Level Ontology (MILO), which extends SUMO.
* [[OpenCyc]],<ref>S. Reed and D. Lenat. 2002. [http://www.cyc.com/doc/white_papers/mapping-ontologies-into-cyc_v31.pdf Mapping Ontologies into Cyc]. In ''Proc. of AAAI 2002 Conference Workshop on Ontologies For The Semantic Web'', Edmonton, Canada, 2002</ref> an open [[ontology (information science)|ontology]] and [[knowledge base]] of everyday common sense knowledge, has 12,000 terms linked to WordNet synonym sets.
* [[Descriptive Ontology for Linguistic and Cognitive Engineering|DOLCE]],<ref>Masolo, C., Borgo, S., Gangemi, A., Guarino, N., Oltramari, A., Schneider, L.S. 2002. [http://www.loa-cnr.it/Papers/WonderWebD17V2.0.pdf WonderWeb Deliverable D17. The WonderWeb Library of Foundational Ontologies and the DOLCE ontology]. Report (ver. 2.0, 15-08-2002)</ref> is the first module of the WonderWeb Foundational Ontologies Library (WFOL). This upper-ontology has been developed in light of rigorous ontological principles inspired by the philosophical tradition, with a clear orientation toward language and cognition. OntoWordNet<ref>Gangemi, A., Guarino, N., Masolo, C., Oltramari, A. 2003 [http://www.loa-cnr.it/Papers/AIMag24-03-003.pdf Sweetening WordNet with DOLCE]. In AI Magazine 24(3): Fall 2003, pp. 13–24</ref> is the result of an experimental effort to align WordNet's upper level with DOLCE. It is suggested that such alignment could lead to an "ontologically sweetened" WordNet, meant to be conceptually more rigorous, cognitively transparent, and efficiently exploitable in several applications.
* [[DBpedia]],<ref>C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, S. Hellmann, [http://www.wiwiss.fu-berlin.de/en/institute/pwo/bizer/research/publications/Bizer-etal-DBpedia-CrystallizationPoint-JWS-Preprint.pdf DBpedia – A crystallization point for the Web of Data]. Web Semantics, 7(3), 2009, pp. 154–165</ref> a database of structured information, is also linked to WordNet.
* The [[eXtended WordNet]]<ref>S. M. Harabagiu, G. A. Miller, D. I. Moldovan. 1999. [http://www.ldc.upenn.edu/acl/W/W99/W99-0501.pdf WordNet 2 – A Morphologically and Semantically Enhanced Resource]. In ''Proc. of the ACL SIGLEX Workshop: Standardizing Lexical Resources'', pp. 1–8.</ref> is a project at the [[University of Texas at Dallas]] which aims to improve WordNet by semantically parsing the glosses, thus making the information contained in these definitions available for automatic knowledge processing systems. It is also freely available under a license similar to WordNet's.
* The [[GCIDE]] project produced a dictionary by combining a [[public domain]] ''[[Webster's Dictionary]]'' from 1913 with some WordNet definitions and material provided by volunteers. It was released under the [[copyleft]] license [[GNU General Public License|GPL]].
* [[ImageNet]] is an image database organized according to the WordNet hierarchy (currently only the nouns), in which each node of the hierarchy is depicted by hundreds and thousands of images.<ref>J. Deng, W. Dong, R. Socher, L. Li, K. Li, L. Fei-Fei. [https://nlpainter.googlecode.com/svn-history/r16/trunk/papers/ImageNet__cvpr09.pdf ImageNet: A Large-Scale Hierarchical Image Database]. In ''Proc. of 2009 IEEE Conference on Computer Vision and Pattern Recognition''</ref> Currently it has an average of over five hundred images per node.
* BioWordnet, a biomedical extension of wordnet was abandoned due to issues about stability over versions.<ref>M. Poprat, E. Beisswanger, U. Hahn. 2008. [http://www.aclweb.org/anthology/W/W08/W08-0507.pdf Building a BIOWORDNET by Using WORDNET’s Data Formats and WORDNET’s Software Infrastructure – A Failure Story]. In ''Proc. of the Software Engineering, Testing, and Quality Assurance for Natural Language Processing Workshop'', pp. 31–39.</ref>
* WikiTax2WordNet, a mapping between WordNet synsets and [[Wikipedia:Categorization|Wikipedia categories]].<ref>S. Ponzetto, R. Navigli. [http://ijcai.org/papers09/Papers/IJCAI09-343.pdf Large-Scale Taxonomy Mapping for Restructuring and Integrating Wikipedia], In ''Proc. of the 21st International Joint Conference on Artificial Intelligence (IJCAI 2009)'', Pasadena, California, July 14-17th, 2009, pp. 2083–2088.</ref>
* WordNet++, a resource including over millions of semantic edges harvested from Wikipedia and connecting pairs of WordNet synsets.<ref>S. P. Ponzetto, R. Navigli. [http://aclweb.org/anthology-new/P/P10/P10-1154.pdf Knowledge-rich Word Sense Disambiguation rivaling supervised systems]. In Proc. of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), 2010, pp. 1522–1531.</ref>
* SentiWordNet, a resource for supporting opinion mining applications obtained by tagging all the WordNet 3.0 synsets according to their estimated degrees of positivity, negativity, and neutrality.<ref>S. Baccianella, A. Esuli and F. Sebastiani. [http://nemis.isti.cnr.it/sebastiani/Publications/LREC10.pdf SentiWordNet 3.0: An Enhanced Lexical Resource for Sentiment Analysis and Opinion Mining]. In Proceedings of the 7th Conference on Language Resources and Evaluation (LREC'10), Valletta, MT, 2010, pp. 2200–2204.</ref>
* ColorDict, is an Android application to mobiles phones that use Wordnet database and others, like Wikipedia.
* [[UBY-LMF]] a database of 10 resources including WordNet.

===Related projects===
* [[FrameNet]] is a lexical database that shares some similarities with, and refers to, WordNet. 
* [[Lexical markup framework]] (LMF) is an ISO standard specified within [[ISO/TC37]] in order to define a common standardized framework for the construction of lexicons, including WordNet. The subset of LMF for Wordnet is called Wordnet-LMF. An instantiation has been made within the KYOTO project.<ref>Piek Vossen, Claudia Soria, Monica Monachini: Wordnet-LMF: a standard representation for multilingual wordnets, in ''LMF Lexical Markup Framework'', edited by Gil Francopoulo ISTE / Wiley 2013 (ISBN 978-1-84821-430-9)</ref>
* [[Universal Networking Language|UNL Programme]] is a project under the auspices of [[United Nations|UNO]] aimed to consolidate lexicosemantic data of many languages to be used in machine translation and information extraction systems.

==Distributions==
* [[Babylon (software)|Babylon]]<ref>{{cite web|url=http://www.babylon.com/free-dictionaries/reference/encyclopedias/WordNet-2.0/42406.html |title=Babylon WordNet |publisher=Babylon.com |date= |accessdate=2014-03-11}}</ref>
WordNet Database is distributed as a dictionary package (usually a single file) for the following software:
* [[GoldenDict]]<ref>{{cite web|url=http://sourceforge.net/projects/goldendict/files/dictionaries |title=GoldenDict - Browse /dictionaries at Sourceforge.net |publisher=Sourceforge.net |date=2010-12-01 |accessdate=2014-01-05}}</ref>
* [[Lingoes (program)|Lingoes]]<ref>{{cite web|url=http://www.lingoes.net/en/dictionary/dict_down.php?id=12D98EC3940843498672A92149455292 |title=Lingoes WordNet |publisher=Lingoes.net |date=2007-11-16 |accessdate=2014-03-11}}</ref>

== See also ==
* [[Lexical Markup Framework]]
* [[Machine-readable dictionary]]
* [[Synonym Ring]]
* [[Taxonomy (general)|Taxonomy]]
* [[ThoughtTreasure]]

== References ==
{{Reflist|2}}

== External links ==
* {{Official website|http://wordnet.princeton.edu/ }}
{{Lexicography}}

{{Authority control}}

{{DEFAULTSORT:Wordnet}}
[[Category:English dictionaries]]
[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Online dictionaries]]
[[Category:Open data]]
[[Category:Thesauri]]
<=====doc_Id=====>:898
<=====title=====>:
Flail space model
<=====text=====>:
The '''flail space model (FSM)''' is a [[Physical model|model]] of how a [[passenger|car passenger]] moves in a [[vehicle]] that collides with a roadside feature such as a [[Traffic barrier|guardrail]] or a [[crash cushion]]. Its principal purpose is to assess the potential risk of harm to the hypothetical occupant as he or she impacts the interior of the passenger compartment and, ultimately, the efficacy of an experimental roadside feature undergoing full-scale vehicle crash testing.

The FSM eliminates the complexity and expense of using instrumented [[Crash test dummy|anthropometric dummies]] during the crash test experiments. Furthermore, while crash test dummies were developed to model collisions between vehicles, they are not accurate when used for the sorts of collision angles that occur when a vehicle collides with a roadside feature; by contrast, the FSM was designed for such collisions.<ref name="gabauer">Gabauer, Douglas, "A methodology to evaluate the flail space model using event data recorder technology", Department of Mechanical Engineering, Rowan University, Glassboro, NJ, 2004.</ref>

== History ==
The FSM is based on research performed at [[Southwest Research Institute]] in 1980<ref>Michie, J. D., "Development of improved criteria for evaluating safety performance of highway appurtenances", Final Report of  Internal Research Project No. 03-9254, Southwest Research Institute, San Antonio, Texas, June 1980.</ref> and published in 1981 in the paper entitled "Collision Risk Assessment Based on Occupant Flail-Space Model" by Jarvis D. Michie.<ref name=":0">Michie, J. D., "Collision risk assessment based on occupant flail space model," in Transportation Research Record 796, 1981, pp. 1–9.</ref> The FSM (coined by Michie) was accepted by the highway community and published as a key part of the "Recommended Procedures for the Safety Evaluation of Highway Appurtenances" published in 1981 in [[National Cooperative Highway Research Program]] (NCHRP) Report 230.<ref>Michie, J. D.  National Cooperative Highway Research Program Report 230: Recommended Procedures for the Safety Performance Evaluation of Highway Appurtenances.  NCHRP Transportation Research Board, Washington, DC, March 1981.</ref> In 1993, the NCHRP Report was updated and presented as NCHRP Report 350;<ref>Ross, H. E., Jr. et al.  National Cooperative Highway Research Program Report 350:  Recommended Procedures for the Safety Evaluation of Highway Features.  NCHRP Transportation Research Board, Washington, DC, 1993.</ref> in this research effort performed by the [[Texas A&M Transportation Institute|Texas Transportation Research Institute]], the FSM was reexamined and was unmodified in the new publication. In 2004, Douglas Gabauer further examined the efficacy of the FSM in his [[PhD thesis]].<ref name="gabauer" /> The [[American Association of State Highway and Transportation Officials]] (AASHTO) retained the FSM as the method of assessing the risk of harm to vehicle occupants in the 2009 "Manual for Assessing Safety Hardware" that replaced NCHRP Report 350, stating that the FSM had "served its intended purpose well".<ref>Manual for Assessing Safety Hardware.  American Association of State Highway and Transportation Officials, Washington, DC,  2009.</ref>

== Details ==
The FSM hypothesis divides the collision into two stages.  In stage one, the unrestrained occupant is propelled forward and sideways in the compartment space due to vehicle collision [[Acceleration|accelerations]] and then impacts one or more surfaces (including the steering wheel) with velocity "V". According to the model, the vehicle (instead of the occupant) is the object that is accelerating. The occupant experiences no injury-producing force prior to contact with the compartment surfaces.<ref name=":0" />

In stage two, the occupant is assumed to remain in contact with the compartment surface and experiences the same accelerations as the vehicle for the rest of the collision.  The occupant may sustain [[Blunt trauma|injury]] at the end of stage one based on the velocity of impact with the compartment surfaces and due to vehicle accelerations during stage two.  The occupant impact velocity and acceleration are computed from the vehicle collision acceleration history and the compartment geometry.  Finally, the hypothetical occupant impact velocity and acceleration are then compared to threshold values of [[Engineering tolerance|human tolerance]] to these forces.<ref name=":0" />

==References==
{{reflist|colwidth=30em}}

[[Category:Articles created via the Article Wizard]]
[[Category:Scientific modeling]]
[[Category:Applied mathematics]]
[[Category:Knowledge representation]]
[[Category:Mathematical modeling]]
[[Category:Transport safety]]
<=====doc_Id=====>:901
<=====title=====>:
Script theory
<=====text=====>:
'''Script theory''' is a [[Psychology|psychological]] theory which posits that [[human behaviour]] largely falls into patterns called "scripts" because they function analogously to the way a written script does, by providing a program for action. [[Silvan Tomkins]] created script theory as a further development of his [[affect theory]], which regards human beings' emotional responses to stimuli as falling into categories called "[[Affect (psychology)|affects]]": he noticed that the purely biological response of affect may be followed by awareness and by what we [[Cognition|cognitively]] do in terms of acting on that affect so that more was needed to produce a complete explanation of what he called "human being theory".

In script theory, the basic unit of analysis is called a "scene", defined as a sequence of events linked by the affects triggered during the experience of those events. Tomkins recognized that our affective experiences fall into patterns that we may group together according to criteria such as the types of persons and places involved and the degree of intensity of the effect experienced, the patterns of which constitute scripts that inform our behavior in an effort to maximize positive affect and to minimize negative affect.

== In artificial intelligence ==
[[Roger Schank]], [[Robert P. Abelson]] and their research group, extended Tomkins' scripts and used them in early artificial intelligence work as a method of representing [[procedural knowledge]]. In their work, scripts are very much like [[frame (artificial intelligence)|frames]], except the values that fill the slots must be ordered. A script is a structured representation describing a stereotyped sequence of events in a particular context. Scripts are used in natural language understanding systems to organize a knowledge base in terms of the situations that the system should understand.

The classic example of a script involves the typical sequence of events that occur when a person drinks in a restaurant:  ''finding a seat, reading the menu, ordering drinks from the waitstaff...''  In the script form, these would be decomposed into [[conceptual dependency theory|conceptual transitions]], such as '''MTRANS''' and '''PTRANS''', which refer to ''mental transitions [of information]'' and ''physical transitions [of things]''.

Schank, Abelson and their colleagues tackled some of the most difficult problems in [[artificial intelligence]] (i.e., [[story understanding]]), but ultimately their line of work ended without tangible success. This type of work received little attention after the 1980s, but it is very influential in later [[knowledge representation]] techniques, such as [[case-based reasoning]].

Scripts can be inflexible. To deal with inflexibility, smaller modules called [[memory organization packet]]s (MOP) can be combined in a way that is appropriate for the situation.{{citation needed|date=February 2012}}

== References ==
{{More footnotes|date=November 2014}}
* Nathanson, Donald L. ''Shame and Pride: Affect, Sex, and the Birth of the Self''. London: W.W. Norton, 1992 
* [[Eve Kosofsky Sedgwick|Sedgwick, Eve Kosofsky]] and Adam Frank, eds. 1995. ''Shame and Its Sisters: A Silvan Tomkins Reader''. Durham and London: Duke University Press.
* Tomkins, Silvan. "Script Theory". ''The Emergence of Personality''. Eds. Joel Arnoff, A. I. Rabin, and Robert A. Zucker. New York: Springer Publishing Company, 1987. 147–216. 
* Tomkins, Silvan. "Script Theory: Differential Magnification of Affects". Nebraska Symposium On Motivation 1978. Ed. Richard A. Deinstbier. Lincoln, NE: [[University of Nebraska Press]], 1979. 201–236.

[[Category:History of artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Psychological theories]]
<=====doc_Id=====>:904
<=====title=====>:
Parallel Tree Contraction
<=====text=====>:
{{Orphan|date=December 2015}}

In [[computer science]], '''parallel tree contraction''' is a broadly applicable technique for the parallel solution of a large number of [[tree]] problems, and is used as an algorithm design technique for the design of a large number of parallel [[graph (discrete mathematics)|graph]] algorithms.  Parallel tree contraction was introduced by [[Gary L. Miller]] and [[John H. Reif]],<ref name="Miller89book">[[Gary L. Miller]] and [[John H. Reif]], Parallel Tree Contraction--Part I: Fundamentals., 1989</ref>  and has subsequently been modified to improve efficiency by X. He and Y. Yesha,<ref>X. He and Y. Yesha, "Binary tree algebraic computation and parallel algorithms for simple graphs.", Journal of Algorithms, 1988, pp 92-113</ref> Hillel Gazit, Gary L. Miller and Shang-Hua Teng<ref>Hillel Gazit, Gary L. Miller and Shang-Hua Teng, Optimal tree contraction in the EREW model, Springer, 1988</ref> and many others.<ref>Karl Abrahamson and et al., "A simple parallel tree contraction algorithm.", Journal of Algorithms, 1989, pp 287-302</ref>

Tree contraction has been used in designing many efficient [[parallel algorithms]], including [[Expression (mathematics)|expression]] evaluation, finding [[lowest common ancestors]], tree isomorphism, [[graph isomorphism]], [[maximal subtree isomorphism]], [[common subexpression elimination]], computing the 3-connected components of a graph, and finding an explicit planar embedding of a [[planar graph]]<ref name="Reif94dynamic">John H. Reif and Stephen R. Tate, Dynamic parallel tree contraction, Proceedings of the sixth annual ACM symposium on Parallel algorithms and architectures (ACM), 1994</ref>

Based on the research and work on parallel tree contraction, various algorithms have been proposed targeting to improve the efficiency or simplicity of this topic. This article hereby focuses on a particular solution, which is a variant of the algorithm by Miller and Reif, and its application.

==Introduction==
Over the past several decades there has been significant research on deriving new parallel algorithms for a variety of problems, with the goal of designing highly parallel ([[polylogarithmic depth]]), work-efficient (linear in the sequential running time) algorithms.<ref name="Miller89book" /> For some problems, tree turns out to be a nice solution. Addressing these problems, we can sometimes get more parallelism simply by representing our problem as a tree.

Considering a generic definition of a tree, there is a root vertex, and several child vertices attached to the root.<ref>[[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&nbsp;214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&nbsp;253–320.</ref> And the child vertices might have children themselves, and so on so forth. Eventually, the paths come down to leaves, which are defined to be the terminal of a tree. Then based on this generic tree, we can further come up with some special cases: (1) [[balanced binary tree]]; (2) [[linked list]].<ref>[[Donald Knuth]]. ''[[The Art of Computer Programming]]: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&nbsp;308–423.</ref> A balanced binary tree has exactly two branches for each vertex except for leaves. This gives a O(log n) bound on the depth of the tree.<ref>{{citation
 | last1 = Nešetřil | first1 = Jaroslav | author1-link = Jaroslav Nešetřil
 | last2 = Ossona de Mendez | first2 = Patrice | author2-link = Patrice Ossona de Mendez
 | contribution = Chapter 6. Bounded height trees and tree-depth
 | doi = 10.1007/978-3-642-27875-4
 | isbn = 978-3-642-27874-7
 | location = Heidelberg
 | mr = 2920058
 | pages = 115–144
 | publisher = Springer
 | series = Algorithms and Combinatorics
 | title = Sparsity: Graphs, Structures, and Algorithms
 | volume = 28
 | year = 2012}}.</ref> A linked list is also a tree where every vertex has only one child. We can also achieve O(log n) depth using [[symmetry breaking]].<ref>Andrew Goldberg, Serge Plotkin, and Gregory Shannon, Parallel symmetry-breaking in sparse graphs, Proceedings of the nineteenth annual ACM symposium on Theory of computing (ACM), 1987</ref>

Given the general case of a tree, we would like to keep the bound at O(log n) no matter it is unbalanced or list-like or a mix of both. To address this problem, we make use of an algorithm called [[prefix sum]] by using the [[Euler tour technique]].<ref>[http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf Euler tour trees] - in Lecture Notes in Advanced Data Structures. Prof. Erik Demaine; Scribe: Katherine Lai.</ref> With the Euler tour technique, a tree could be represented in a flat style, and thus prefix sum could be applied to an arbitrary tree in this format. In fact, prefix sum can be used on any set of values and binary operation which form a group: the binary operation must be associative, every value must have an inverse, and there exists an identity value.

With a bit of thought, we can find some exceptional cases where prefix sum becomes incapable or inefficient. Consider the example of multiplication when the set of values includes 0. Or there are some commonly desired operations are max() and min() which do not have [[inverses]]. The goal is to seek an algorithm which works on all trees, in expected O(n) work and O(log n) depth. In the following sections, a Rake/Compress algorithm will be proposed to fulfill this goal.<ref name="Miller85app">Gary L. Miller and John H. Reif, Parallel tree contraction and its application, Defense Technical Information Center, 1985</ref>

==Definitions==

[[File:Rake-1.png|480*360px|thumbnail|right|Fig. 1: Rake Operation]]
[[File:Compress-1.png|480*360px|thumbnail|right|Fig. 2: Compress Operation]]
Before going into the algorithm itself, we first look at a few terminologies that will be used later.

* '''Rake'''<ref name="cmutrees">[https://www.cs.cmu.edu/afs/cs/academic/class/15499-s09/www/scribe/lec11/lec11.pdf Parallel Algorithms: Tree Operations], Guy Blelloch, Carnegie Mellon University, 2009</ref> – Rake step joins every left leaf of binary nodes to the parent. By join, we mean that it undergoes a functional process which achieves the operation we want to make. An example of rake is given in Figure 1.
* '''Compress'''<ref name="cmutrees" /> – Compress step is actually a sequence of several events: (1) Find an independent set of unary nodes. (Independence here is defined such that no two are neighbors, meaning no parent to child relation) (2) Join each node in independent set with its child (Note that independent set is not unique). An example of compress is given in Figure 2.

And in order to solve actual problems using tree contraction, the algorithm has a structure:

<pre>
Repeat until tree becomes a unary node
{
    Rake;
    Compress;
}
</pre>


==Analysis==
For the moment, let us assume that all nodes have less than three children, namely binary. Generally speaking, as long as the degree is bounded, the bounds will hold.<ref>MORIHATA, Akimasa, and Kiminori MATSUZAKI, A Parallel Tree Contraction Algorithm on Non-Binary Trees, MATHEMATICAL ENGINEERING
TECHNICAL REPORTS, 2008</ref> But we will analyze the binary case for simplicity. In the two “degenerate” cases listed above, the rake is the best tool for dealing with balanced binary trees, and compress is the best for linked lists. However, arbitrary trees will have to require a combination of these operations. By this combination, we claim a theorem that
* '''Theorem''': After O(log n) expected rake and compress steps, a tree is reduced to a single node.
Now rephrase the tree contraction algorithm as follows:
* Input: A binary tree rooted at r
* Output: A single node
* Operation:  A sequence of contraction steps, each consisting of a rake operation and a compress operation (in any order). The rake operation removes all the leaf nodes in parallel. The compress operation finds an [[Independent set (graph theory)|independent set]] of unary nodes and splice out the selected nodes.
To approach the theorem, we first take a look at a property of a binary tree. Given a binary tree T, we can partition the nodes of T into 3 groups: {{tmath|T_0}} contains all leaf nodes, {{tmath|T_1}} contains all nodes with 1 child, and {{tmath|T_2}} contains all nodes with 2 children. It is easy to see that: <math>V(T) = T_0  \cup T_1 \cup T_2</math>. Now we propose:
* Claim: <math>|T_0| = |T_2|  + 1</math>
This claim can be proved by strong induction on the number of nodes. It is easy to see that the base case of n=1 trivially holds. And we further assume the claim also holds for any tree with at most n nodes. Then given a tree with n+1 nodes rooted at r, there appears to be two cases:
# If r has only one subtree, consider the subtree of r. We know that the subtree has the same number of binary nodes and the same number of leaf nodes as the whole tree itself. This is true since the root is a unary node. And based the previous assumption, a unary node does not change either {{tmath|T_0}} or {{tmath|T_2}}.
# If r has two subtrees, we define {{tmath|T_0^L, T_2^L}} to be the leaf nodes and binary nodes in the left subtree, respectively. Similarly, we define the same {{tmath|T_0^R, T_2^R}} for the right subtree. From previous, there is <math>|T_0^L| = |T_2^L| + 1</math> and <math>|T_0^R| = |T_2^R| + 1</math>. Also we know that T has <math>|T_0^L| + |T_0^R|</math> leaf nodes and <math>|T_2^L| + |T_2^R| + 1</math> binary nodes. Thus, we can derive:

:<math>|T_0^L| + |T_0^R| = |T_2^L| + 1 + |T_2^R| + 1 = (|T_2^L| + |T_2^R| + 1) + 1</math>

which proves the claim.

Following the claim, we then prove a lemma, which leads us to the theorem.
* Lemma: The number of nodes of after a contraction step is reduced by a constant factor in expectation.
Assume the number of nodes before the contraction to be m, and m' after the contraction. By definition, the rake operation deletes all {{tmath|T_0}} and the compress operation deletes at least 1/4 of {{tmath|T_1}} in expectation. All {{tmath|T_2}} remains. Therefore, we can see:

:<math>E[m'] \leq |T_2| + \tfrac{3}{4}*|T_1| \leq \tfrac{3}{4} + \tfrac{3}{4}*|T_1| + \tfrac{3}{2}*|T_2| = \tfrac{3}{4}(1 + |T_1| + 2*|T_2|) = \tfrac{3}{4}(|T_0| + |T_1| + |T_2|) = \tfrac{3}{4}m</math>

Finally, based on this lemma, we can conclude that if the nodes are reduced by a constant factor in each iteration, after {{tmath|O(\log n)}}, there will be only one node left.<ref>[https://www.cs.cmu.edu/afs/cs/academic/class/15492-f07/www/scribe/lec9/lecture9.pdf Parallel Algorithms: Analyzing Parallel Tree Contraction], Guy Blelloch, 2007</ref>

==Applications==

===Expression Evaluation===
To evaluate an expression given as a binary tree (this problem also known as [[binary expression tree]]),<ref>S Buss, Algorithms for boolean formula evaluation and for tree contraction, Arithmetic, Proof Theory, and Computational Complexity, 1993, pp. 96-115</ref> consider that:
An arithmetic expression is a tree where the leaves have values from some domain and each internal vertex has two children and a label from {+, x, %}. And further assume that these binary operations can be performed in constant time.

We now show the evaluation can be done with parallel tree contraction.<ref>Bader, David A., Sukanya Sreshta, and Nina R. Weisse-Bernstein, Evaluating arithmetic expressions using tree contraction: A fast and scalable parallel implementation for symmetric multiprocessors (SMPs), High Performance Computing—HiPC 2002. Springer Berlin Heidelberg, 2002, pp. 63-75.</ref>
* Step 1. Assign expressions to every node. The expression of a leaf is simply the value that it contains. Write L + R, L − R, or L × R for the operators, where L and R are the values of the expressions in the left and right subtrees, respectively.
* Step 2. When a left (right) child with 0 children is merged into an operator, replace L (R) with the value of the child.
* Step 3. When a node has 1 child, it has an expression that is a function of one variable. When a left (right) child with 1 child is merged into an operator, replace L (R) with the expression and change the variable in the expression to L (R) if appropriate.

In a node with 2 children, the operands in the expression are f(L) and g(R), where f and g are linear functions, and in a node with 1 child, the expression is h(x), where h is a linear function and x is either L or R. We prove this invariant by induction. At the beginning, the invariant is clearly satisfied. There are three types of merges that result in a not fully evaluated expression. (1) A 1-child node is merged into a 2-children node. (2) A leaf is merged into a 2-children node. (3) A 1-child node is merged into a 1-child node. All three types of merges do not change the invariant. Therefore, every merge simply evaluates or composes linear functions, which takes constant time <ref>[http://math.mit.edu/~rpeng/18434/applicationsParallelTreeContraction.pdf Applications of Parallel Tree Contraction], Samuel Yeom, 2015</ref>

==References==
{{Reflist}}
{{refbegin}}
{{refend}}

==External links==
{{Commons category|Tree structures}}
* [http://courses.csail.mit.edu/6.851/spring07/scribe/lec05.pdf 6.851: Advanced Data Structures] by Prof. Erik Demaine

{{CS-Trees}}

[[Category:Knowledge representation]]

[[de:Baum (Graphentheorie)]]
<=====doc_Id=====>:907
<=====title=====>:
Group concept mapping
<=====text=====>:
'''Group concept mapping''' is a structured methodology for organizing the ideas of a group on any topic of interest and representing those ideas visually in a series of interrelated maps.<ref name="ref1">Kane M, Trochim WM (2007). Concept mapping for planning and evaluation. Thousand Oaks, CA: Sage Publications.</ref><ref name="ref2">Trochim W (1989). An introduction to concept mapping for evaluation and planning. Evaluation and Program Planning, 12(1), 1-16.</ref>  It is a type of integrative mixed method,<ref>Caracelli VW, Greene JC (1993). Data analysis strategies for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 15(2), 195-207.</ref><ref>Greene JC, Caracelli VJ, Graham WF (1989). Toward a conceptual framework for mixed-method evaluation designs. Educational Evaluation and Policy Analysis, 11, 255-274.</ref> combining qualitative and quantitative approaches to [[data collection]] and [[analysis]].  Group concept mapping allows for a collaborative group process with groups of any size, including a broad and diverse array of participants.<ref name="ref1" />  Since its development in the late 1980s by William M.K. Trochim at [[Cornell University]], it has been applied to various fields and contexts, including community and public health,<ref>Rao JK, Alongi J, Anderson LA, Jenkins L, Stokes GA, Kane M (2005). Development of public health priorities for end-of-life initiatives. American Journal of Preventive Medicine, 29(5), 453-460.</ref><ref>Risisky D, Hogan VK, Kane M, Burt B, Dove C, Payton M (2008). Concept mapping as a tool to engage a community in health disparity identification. Ethnicity & Disease, 18, 77-83.</ref><ref>Trochim W, Milstein B, Wood B, Jackson S, Pressler V (2004). Setting objectives for community and systems change: an application of concept mapping for planning a statewide health improvement initiative. Health Promotion Practice, 5, 8–19.</ref><ref>Trochim WM, Cabrera DA, Milstein B, Gallagher RS, Leischow SJ (2006). Practical challenges of systems thinking and modeling in public health. American Journal of Public Health, 96(3), 538-546.</ref> social work,<ref>Petrucci C, Quinlan KM (2007). Bridging the research-practice gap: concept mapping as a mixed-methods strategy in practice-based research and evaluation. Journal of Social Science Research, 34(2), 25-42.</ref><ref>Ridings JW, Powell DM, Johnson JE, Pullie CJ, Jones CM, Jones RL, Terrell KJ (2008). Using concept mapping to promote community building: the African American Initiative at Roseland. Journal of Community Practice, 16(1), 39-63.</ref> health care,<ref>Trochim WM, Kane M (2005). Concept mapping: an introduction to structured conceptualization in health care. International Journal for Quality in Health Care, 7(3),187-191.</ref> human services,<ref>Pammer W, Haney M, Wood BM, Brooks RG, Morse K, Hicks P, Handler EG, Rogers H, Jennett P (2001). Use of telehealth technology to extend child protection team services. Pediatrics, 108(3), 584-590.</ref><ref>Paulson BL, Truscott D, Stuart J (1999). Clients’ perceptions of helpful experiences in counseling. Journal of Counseling Psychology, 46(3), 317-324.</ref> and biomedical research and evaluation.<ref>Kagan JM, Kane M, Quinlan KM, Rosas S, Trochim WMK (2009). Developing a conceptual framework for an evaluation system for the NIAID HIV/AIDS clinical trials networks. Health Research Policy and Systems, 7, 12.</ref><ref>Robinson JM, Trochim WMK (2007). An examination of community members’, researchers’ and health professionals’ perceptions of barriers to minority participation in medical research: an application of concept mapping. Ethnicity & Health, 12(5), 521-539.</ref><ref>Trochim WM, Markus SE, Masse LC, Moser RP, and Weld PC (2008). The evaluation of large research initiatives: a participatory integrative mixed-methods approach. American Journal of Evaluation, 29(1), 8–28.</ref>

==Overview==
Group concept mapping integrates qualitative group processes with [[multivariate analysis]] to help a group organize and visually represent its ideas on any topic of interest through a series of related maps.<ref name="ref1" /><ref name="ref2" />  It combines the ideas of diverse participants to show what the group thinks and values in relation to the specific topic of interest. It is a type of structured conceptualization used by groups to develop a conceptual framework, often to help guide evaluation and planning efforts.<ref name="ref2" />  Group concept mapping is participatory in nature, allowing participants to have an equal voice and to contribute through various methods.<ref name="ref1" /> A group concept map visually represents all the ideas of a group and how they relate to each other, and depending on the scale, which ideas are more relevant, important, or feasible.

==Process==

Group concept mapping involves a structured multi-step process, including [[brainstorming]], sorting and rating, [[multidimensional scaling]] and [[cluster analysis]], and the generation and interpretation of multiple maps.<ref name="ref1" /><ref name="ref2" />  The first step requires participants to brainstorm a large set of statements relevant to the topic of interest, usually in response to a focus prompt.  Participants are then asked to individually sort those statements into categories based on their perceived similarity and rate each statement on one or more scales, such as importance or feasibility.

The data is then analyzed using The Concept System® software, which creates a series of interrelated maps using [[multidimensional scaling]] (MDS) of the sort data, [[hierarchical clustering]] of the MDS coordinates applying [[Ward's method|Ward’s method]], and the computation of average ratings for each statement and cluster of statements.<ref name="ref3">Rosas SR, Camphausen (2007).  The use of concept mapping for scale development and validation in evaluation.  Evaluation and Program Planning, 30, 125-135.</ref>  The resulting maps display the individual statements in two-dimensional space with more similar statements located closer to each other, and grouped into clusters that partition the space on the map.  The Concept System® software also creates other maps that show the statements in each cluster rated on one or more scales, and absolute or relative cluster ratings between two cluster sets.  As a last step in the process, participants are led through a structured interpretation session to better understand and label all the maps.

==History==
Group concept mapping was developed as a methodology in the late 1980s by William M.K. Trochim at [[Cornell University]].  Trochim is considered to be a leading evaluation expert, and he has taught evaluation and research methods at Cornell since 1980.<ref name="ref4">Cornell University, College of Human Ecology (2012).  William Trochim biographical statement. http://www.human.cornell.edu/bio.cfm?netid=wmt1.</ref>  Originally called "concept mapping", the methodology has evolved since its inception with the maturation of the field and the continued advancement of the software, which is now a Web application.

==Uses==
Group concept mapping can be used with any group for any topic of interest.  It is often used by government agencies, academic institutions, national associations, not-for-profit and community-based organizations, and private businesses to help turn the ideas of the group into measurable actions.  This includes in the areas of organizational development, strategic planning, needs assessment, curriculum development, research, and evaluation.<ref name="ref1" />  Group concept mapping is well-documented, well-established methodology, and it has been used in hundreds of published papers.

==Group concept mapping versus concept mapping and mind mapping==
Concept mapping is any process used for visually representing relationships between ideas in pictures or maps.<ref name="ref1" />  The technique was originally developed in the 1970s by [[Joseph D. Novak]] at [[Cornell University]]. <ref name="ref5">Novak JD, Gowin DB (1984). Learning How to Learn. Cambridge: Cambridge University Press.</ref>  A [[concept map]] is typically a diagram of multiple ideas, often represented as boxes or circles, linked in a hierarchical structure through arrows and words where each idea is connected to each other and linked back to the original idea.<ref name="ref5" />  Concept mapping tends to be more free form, and may involve an individual or group.  Unlike other forms of concept mapping, group concept mapping is purposefully designed to work with groups and has a more structured process for organizing and visually representing the ideas of a group through a series of specific steps.<ref name="ref1" />

A [[mind map]] is a diagram used to visually represent information, centering on one word or idea with categories and sub-categories radiating off of it.<ref>Buzan T (2010).  The Mind Map Book: Unlock Your Creativity, Boost Your Memory, Change Your Life. Essex: British Broadcasting Company.</ref>  Popularized by [[Tony Buzan]] in the 1970s, mind mapping is often a spontaneous exercise done by an individual or group to gather information about what they think around a single topic.  In contrast to mind mapping, group concept mapping represents multiple ideas and it has a less flexible and more structured process.  Group concept mapping is also specific to groups.

==See also==
* [[Knowledge representation and reasoning]]
* [[List of concept- and mind-mapping software]]

==References==
{{reflist}}

==External links==
* [http://www.socialresearchmethods.net/mapping/mapping.htm Concept mapping research guide]

[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Survey methodology]]
<=====doc_Id=====>:910
<=====title=====>:
Moovly
<=====text=====>:
{{Orphan|date=May 2016}}
{{Infobox company
| name             = Moovly
| type             = [[Public company|Public]]
| key_people       = Brendon Grunewald (Co-Founder and CEO), Geert Coppens (Co-Founder and CTO)
| industry         = [[Internet Marketing]]
| products         = Moovly web-based animation software video creation and generation platform.
| foundation       = {{Start date and age|2012}}
| location_city    = [[Vancouver]]
| location_country = [[Canada]]
| homepage         = {{URL|www.moovly.com}}
}}

'''Moovly''' is a company that provides a cloud-based platform [[Software as a service|(SaaS)]] that enables users to create and generate multimedia content: animated videos, video presentations, animated info graphics and any other video content that includes a mix of animation and motion graphics.<ref>Flanders Today, [http://www.flanderstoday.eu/business/moovly-offers-new-online-tool-develop-multimedia-designs Flanders Today], September 13th, 2013, "Moovly offers new online tool to develop multimedia designs"</ref>

== History ==
Moovly was originally founded in Belgium in November 2012 by Brendon Grunewald and Geert Coppens with the vision of "becoming the number one platform for engaging customisable multimedia content creation". The company's mission is to "Enable everyone to create engaging multimedia content by making it affordable, Intuitive and Simple".  The company has seen steady subscriber growth from multinational corporations, small and medium business as well as educational institutions.. The company initially secured several rounds of external investment to fund its growth, and in July 2016, listed on the Toronto Venture Stock Exchange as Moovly Media Inc under the symbol MVY (TSX.V: MVY).<ref>Levak, Rachel, [https://www.crunchbase.com/organization/moovly/funding-rounds Crunchbase] , Apr 09, 2015, "Moovly - Funding Rounds |  CrunchBase"</ref>

== Product ==
Moovly is a cloud based digital media and content creation software platform. Content can be created via various interfaces, including the editor as well as simple, custom-made video generation interfaces.

Using a combination of uploaded images, videos and sounds, as well as a pre-defined library of objects, users are able to quickly assemble new animated content. The final videos or presentations can be downloaded as an [[MP4]] for example, or published on a variety of video platforms.

Moovly provides a feature-rich free license allowing users to create animated videos<ref>Wilson, Liévano, [http://jsk.stanford.edu/news-notes/2014/10-things-you-need-to-know-when-producing-a-data-animation-for-a-newsroom/ John S. Knight Journalism Fellowships at Stanford], Sep 04th, 2014, "10 things you need to know when producing a data animation for a newsroom"</ref> that can be exported to [[Facebook]] and [[YouTube]], as well as premium licenses for advanced and professional use. The free videos include the Moovly branding. As an educational tool<ref>Hart, Jane, [http://c4lpt.co.uk/top100tools/moovly/ Centre for Learning & Performance Technologies], Sep 21, 2015, " Top 100 Tools for Learning | Centre for Learning & Performance Technologies"</ref> and for educational purposes,<ref>Janssens, Mieke, [http://www.klascement.eu/sites/60764/ KlasCement Educational Resources Network], Sep 22, 2015, "Moovly: Create animated videos and presentations | KlasCement Educational Resources Network"</ref> Moovly offers specific licenses.<ref>[https://www.moovly.com/education-solutions Moovly Website]</ref>

Companies and brands can use their own library of animated graphics, their own fonts and standard color set.<ref>Waldron, John, [http://www.markitwrite.com/moovly/ markITwrite] , Nov, 2014, "Moovly: Video Animation for Everyone |  markITwrite"</ref>

== References ==
{{reflist}}

== External links ==
* {{official website|https://www.moovly.com/}}
* [http://web.tmxmoney.com/company.php?qm_symbol=MVY&locale=EN Toronto Venture Exchange Moovly Page]
* [http://www.bloomberg.com/profiles/companies/1291746D:BB-moovly-nv Bloomberg overview]

[[Category:Animation software]]
[[Category:Companies established in 2012]]
[[Category:Cloud applications]]
[[Category:Computer animation]]
[[Category:Presentation software]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:913
<=====title=====>:
Semantic triple
<=====text=====>:
A '''semantic triple''', or simply '''triple''', is the atomic data entity in the [[Resource Description Framework]] (RDF) data model.<ref>http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework (RDF) Model and Syntax Specification"</ref> As its name indicates, a triple is a [[tuple|set of three entities]] that codifies a [[statement (programming)|statement]] about [[Data model|semantic data]] in the form of subject–predicate–object expressions (e.g. "Bob is 35", or "Bob knows John").

This format enables [[Knowledge representation and reasoning|knowledge to be represented]] in a machine-readable way. Particularly, every part of an RDF triple is individually addressable via unique [[Uniform Resource Identifier|URIs]] — for example, the second statement above might be represented in RDF as <code><nowiki>http://example.name#BobSmith12 http://xmlns.com/foaf/0.1/knows http://example.name#JohnDoe34</nowiki></code>.
Given this precise representation, semantic data can be unambiguously [[Semantic query|queried]] and [[Semantic reasoner|reasoned]] about.

The components of a triple, such as the statement "The sky has the color blue", consist of a [[Subject (grammar)|subject]] ("the sky"), a [[Predicate (grammar)|predicate]] ("has the color"), and an [[Object (grammar)|object]] ("blue"). This is similar to the classical notation of an [[entity–attribute–value model]] within [[object-oriented design]], where this example would be expressed as an entity (sky), an attribute (color) and a value (blue). From this basic structure, triples can be composed into [[Semantic network|more complex models]], by using triples as objects or subjects of other triples — for example, <code>Mike → said → (triples → can be → objects)</code>.

Given their particular, consistent structure, a collection of triples is often stored in purpose-built databases called [[Triplestore]]s.

== See also ==
* [[Named graph#Named graphs and quads]], an extension to semantic triples to also include a context node as a fourth element.

== References ==
{{reflist}}

== External links ==
* {{cite web |url = https://www.w3.org/TR/rdf11-primer/#section-triple |title = RDF 1.1 Primer § Triples |publisher = [[World Wide Web Consortium|W3C]] }}

{{Semantic Web}}

[[Category:Semantic Web]]
[[Category:Data modeling]]
[[Category:Resource Description Framework]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:916
<=====title=====>:
Category:Computational fields of study
<=====text=====>:
Computational fields of study are areas of research in an existing field using the power of computation, and which are usually named for that. Computational fields of study as a group are sometimes also be referred to as [[Computational X]]<ref>[http://blog.stephenwolfram.com/2016/09/how-to-teach-computational-thinking/ How to Teach Computational Thinking] by [[Stephen Wolfram]], Stephen Wolfram Blog, September 7, 2016.</ref>.

[[Category:Computer science]]
[[Category:Knowledge representation]]
[[Category:Applied mathematics]]
[[Category:Big data]]
[[Category:Systems theory]]
[[Category:Computing and society]]
[[Category:Systems thinking]]
[[Category:Futurology]]
[[Category:Theories of deduction]]
