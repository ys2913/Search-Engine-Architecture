<=====doc_Id=====>:2
<=====title=====>:
Category:Electronic documents
<=====text=====>:
[[Category:Documents]]
[[Category:Digital media]]
[[Category:Information retrieval]]
[[Category:Electronic publishing]]
<=====doc_Id=====>:5
<=====title=====>:
Information retrieval
<=====text=====>:
{{Information science}}

'''Information retrieval''' ('''IR''') is the activity of obtaining [[information]] resources relevant to an information need from a collection of information resources.  Searches can be based on [[Full text search|full-text]] or other content-based indexing.

Automated information retrieval systems are used to reduce what has been called "[[information overload]]". Many [[University|universities]] and [[public library|public libraries]] use IR systems to provide access to books, journals and other documents. [[Web search engine]]s are the most visible [[Information retrieval applications|IR applications]].

== Overview ==

An information retrieval process begins when a user enters a [[query string|query]] into the system. Queries are formal statements of [[information need]]s, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of [[Relevance (information retrieval)|relevancy]].

An object is an entity that is represented by information in a content collection or [[database]]. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.<ref>Jansen, B. J. and Rieh, S. (2010) [https://faculty.ist.psu.edu/jjansen/academic/jansen_theoretical_constructs.pdf The Seventeen Theoretical Constructs of Information Searching and Information Retrieval]. Journal of the American Society for Information Sciences and Technology. 61(8), 1517-1534.</ref>

Depending on the [[Information retrieval applications|application]] the data objects may be, for example, text documents, images,<ref name=goodron2000>{{cite journal |first=Abby A. |last=Goodrum |title=Image Information Retrieval: An Overview of Current Research |journal=Informing Science |volume=3 |number=2 |year=2000 }}</ref> audio,<ref name=Foote99>{{cite journal |first=Jonathan |last=Foote |title=An overview of audio information retrieval |journal=Multimedia Systems |year=1999 |publisher=Springer }}</ref> [[mind maps]]<ref name=Beel2009>{{cite conference|first=Jöran |last=Beel |first2=Bela |last2=Gipp |first3=Jan-Olaf |last3=Stiller |title=Information Retrieval On Mind Maps - What Could It Be Good For? |url=http://www.sciplore.org/publications_en.php |conference=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) |year=2009 |publisher=IEEE |place=Washington, DC }}</ref> or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or [[metadata]].

Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.<ref name="Frakes1992">{{cite book |last=Frakes |first=William B. |title=Information Retrieval Data Structures & Algorithms |publisher=Prentice-Hall, Inc. |year=1992 |isbn=0-13-463837-9 |url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes }}</ref>

== History ==
{{Rquote|right|there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, ca be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute| J. E. Holmstrom, 1948}}
The idea of using computers to search for relevant pieces of information was popularized in the article ''[[As We May Think]]'' by [[Vannevar Bush]] in 1945.<ref name="Singhal2001">{{cite journal |last=Singhal |first=Amit |title=Modern Information Retrieval: A Brief Overview |journal=Bulletin of the IEEE Computer Society Technical Committee on Data Engineering|volume=24 |issue=4 |pages=35–43 |year =2001 |url=http://singhal.info/ieee2001.pdf }}</ref> It would appear that Bush was inspired by patents for a 'statistical machine' - filed by [[Emanuel Goldberg]] in the 1920s and '30s - that searched for documents stored on film.<ref name="Sanderson2012">{{cite journal |author=Mark Sanderson & W. Bruce Croft |title=The History of Information Retrieval Research |journal=Proceedings of the IEEE |volume=100 |pages=1444–1451 |year =2012 |url=http://dx.doi.org/10.1109/JPROC.2012.2189916 |doi=10.1109/jproc.2012.2189916}}</ref> The first description of a computer searching for information was described by Holmstrom in 1948,<ref name="Holmstrom1948">{{cite journal |author=JE Holmstrom |title=‘Section III. Opening Plenary Session |journal=The Royal Society Scientific Information Conference, 21 June-2 July 1948: report and papers submitted |pages=85|year =1948|url=https://books.google.com.au/books?ei=44VxVZrkGYqU8QX4wYPoBA&id=M34lAAAAMAAJ&dq=%E2%80%98Section+III.+Opening+Plenary+Session%22.+The+Royal+Society+Scientific+Information+Conference&focus=searchwithinvolume&q=univac}}</ref> detailing an early mention of the [[Univac]] computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, [[Desk Set]]. In the 1960s, the first large information retrieval research group was formed by [[Gerard Salton]] at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small [[text corpora]] such as the Cranfield collection (several thousand documents).<ref name="Singhal2001" /> Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.

In 1992, the US Department of Defense along with the [[National Institute of Standards and Technology]] (NIST), cosponsored the [[Text Retrieval Conference]] (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that [[scalability|scale]] to huge corpora. The introduction of [[web search engine]]s has boosted the need for very large scale retrieval systems even further.

== Model types ==
[[File:Information-Retrieval-Models.png|thumb|500px|Categorization of IR-models (translated from [[:de:Informationsrückgewinnung#Klassifikation von Modellen zur Repräsentation natürlichsprachlicher Dokumente|German entry]], original source [http://www.logos-verlag.de/cgi-bin/engbuchmid?isbn=0514&lng=eng&id= Dominik Kuropka]).]]
For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.

=== First dimension: mathematical basis ===
* ''Set-theoretic'' models represent documents as sets of words or phrases. Similarities are usually derived from set-theoretic operations on those sets. Common models are:
** [[Standard Boolean model]]
** [[Extended Boolean model]]
** [[Fuzzy retrieval]]
* ''Algebraic models'' represent documents and queries usually as vectors, matrices, or tuples. The similarity of the query vector and document vector is represented as a scalar value.
** [[Vector space model]]
** [[Generalized vector space model]]
** [[Topic-based vector space model|(Enhanced) Topic-based Vector Space Model]]
** [[Extended Boolean model]]
** [[Latent semantic indexing]] a.k.a. [[latent semantic analysis]]
* ''Probabilistic models'' treat the process of document retrieval as a probabilistic inference. Similarities are computed as probabilities that a document is relevant for a given query. Probabilistic theorems like the [[Bayes' theorem]] are often used in these models.
** [[Binary Independence Model]]
** [[Probabilistic relevance model]] on which is based the [[Probabilistic relevance model (BM25)|okapi (BM25)]] relevance function
** [[Uncertain inference]]
** [[Language model]]s
** [[Divergence-from-randomness model]]
** [[Latent Dirichlet allocation]]
* ''Feature-based retrieval models'' view documents as vectors of values of ''feature functions'' (or just ''features'') and seek the best way to combine these features into a single relevance score, typically by [[learning to rank]] methods. Feature functions are arbitrary functions of document and query, and as such can easily incorporate almost any other retrieval model as just another feature.

=== Second dimension: properties of the model ===
* ''Models without term-interdependencies'' treat different terms/words as independent. This fact is usually represented in vector space models by the [[orthogonality]] assumption of term vectors or in probabilistic models by an [[Independence (mathematical logic)|independency]] assumption for term variables.
* ''Models with immanent term interdependencies'' allow a representation of interdependencies between terms. However the degree of the interdependency between two terms is defined by the model itself. It is usually directly or indirectly derived (e.g. by [[dimension reduction|dimensional reduction]]) from the [[co-occurrence]] of those terms in the whole set of documents.
* ''Models with transcendent term interdependencies'' allow a representation of interdependencies between terms, but they do not allege how the interdependency between two terms is defined. They rely an external source for the degree of interdependency between two terms. (For example, a human or sophisticated algorithms.)

== Performance and correctness measures ==
{{further|Evaluation measures (information retrieval)}}

The '''evaluation of an information retrieval system''' is the process of assessing how well a system meets the information needs of its users. Traditional evaluation metrics, designed for [[Standard Boolean model|Boolean retrieval]] or top-k retrieval, include [[precision and recall]].  Many more measures for evaluating the performance of information retrieval systems have also been proposed. In general, measurement considers a collection of documents to be searched and a search query. All common measures described here assume a [[ground truth]] notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be [[ill-posed]] and there may be different shades of relevancy.

Virtually all modern evaluation metrics (e.g., [[Information retrieval#Mean average precision|mean average precision]], [[Information retrieval#Discounted cumulative gain|discounted cumulative gain]]) are designed for '''ranked retrieval''' without any explicit rank cutoff, taking into account the relative order of the documents retrieved by the search engines and giving more weight to documents returned at higher ranks.{{citation needed|date=June 2015}}

The mathematical symbols used in the formulas below mean:
* <math>X \cap Y</math> - [[Intersection (set theory)|Intersection]] - in this case, specifying the documents in ''both'' sets X and Y
* <math>| X |</math> - [[Cardinality]] - in this case, the number of documents in set X
* <math>\int</math> - [[Integral]]
* <math>\sum</math> - [[Summation]]
* <math>\Delta</math> - [[Symmetric difference]]

=== Precision ===
{{main|Precision and recall}}

Precision is the fraction of the documents retrieved that are [[Relevance (information retrieval)|relevant]] to the user's information need.

:<math> \mbox{precision}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{retrieved documents}\}|} </math>

In [[binary classification]], precision is analogous to [[positive predictive value]]. Precision takes all retrieved documents into account. It can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and [[statistics]].

=== Recall ===
{{main|Precision and recall}}

Recall is the fraction of the documents that are relevant to the query that are successfully retrieved.

:<math>\mbox{recall}=\frac{|\{\mbox{relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{relevant documents}\}|} </math>

In binary classification, recall is often called [[sensitivity and specificity|sensitivity]]. So it can be looked at as ''the probability that a relevant document is retrieved by the query''.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

=== Fall-out ===
The proportion of non-relevant documents that are retrieved, out of all non-relevant documents available:

:<math> \mbox{fall-out}=\frac{|\{\mbox{non-relevant documents}\}\cap\{\mbox{retrieved documents}\}|}{|\{\mbox{non-relevant documents}\}|} </math>

In binary classification, fall-out is closely related to [[sensitivity and specificity|specificity]] and is equal to <math>(1-\mbox{specificity})</math>. It can be looked at as ''the probability that a non-relevant document is retrieved by the query''.

It is trivial to achieve fall-out of 0% by returning zero documents in response to any query.

=== F-score / F-measure ===
{{main|F-score}}
The weighted [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score is:

:<math>F = \frac{2 \cdot \mathrm{precision} \cdot \mathrm{recall}}{(\mathrm{precision} + \mathrm{recall})}</math>

This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.

The general formula for non-negative real <math>\beta</math> is:
:<math>F_\beta = \frac{(1 + \beta^2) \cdot (\mathrm{precision} \cdot \mathrm{recall})}{(\beta^2 \cdot \mathrm{precision} + \mathrm{recall})}\,</math>

Two other commonly used F measures are the <math>F_{2}</math> measure, which weights recall twice as much as precision, and the <math>F_{0.5}</math> measure, which weights precision twice as much as recall.

The F-measure was derived by van Rijsbergen (1979) so that <math>F_\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}</math>.  Their relationship is:
:<math>F_\beta = 1 - E</math> where <math>\alpha=\frac{1}{1 + \beta^2}</math>

F-measure can be a better single metric when compared to precision and recall; both precision and recall give different information that can complement each other when combined. If one of them excels more than the other, F-measure will reflect it.{{citation needed|date=June 2015}}

=== Average precision ===
<!-- [[Average precision]] redirects here -->
Precision and recall are single-value metrics based on the whole list of documents returned by the system. For systems that return a ranked sequence of documents, it is desirable to also consider the order in which the returned documents are presented. By computing a precision and recall at every position in the ranked sequence of documents, one can plot a precision-recall curve, plotting precision <math>p(r)</math> as a function of recall <math>r</math>. Average precision computes the average value of <math>p(r)</math> over the interval from <math>r=0</math> to <math>r=1</math>:<ref name="zhu2004">{{cite journal |first=Mu |last=Zhu |title=Recall, Precision and Average Precision |url=http://sas.uwaterloo.ca/stats_navigation/techreports/04WorkingPapers/2004-09.pdf |year=2004 }}</ref>
:<math>\operatorname{AveP} = \int_0^1 p(r)dr</math>
That is the area under the precision-recall curve.
This integral is in practice replaced with a finite sum over every position in the ranked sequence of documents:
:<math>\operatorname{AveP} = \sum_{k=1}^n P(k) \Delta r(k)</math>
where <math>k</math> is the rank in the sequence of retrieved documents, <math>n</math> is the number of retrieved documents, <math>P(k)</math> is the precision at cut-off <math>k</math> in the list, and <math>\Delta r(k)</math> is the change in recall from items <math>k-1</math> to <math>k</math>.<ref name="zhu2004" />

This finite sum is equivalent to:
:<math> \operatorname{AveP} = \frac{\sum_{k=1}^n (P(k) \times \operatorname{rel}(k))}{\mbox{number of relevant documents}} \!</math>
where <math>\operatorname{rel}(k)</math> is an indicator function equaling 1 if the item at rank <math>k</math> is a relevant document, zero otherwise.<ref name="Turpin2006">{{cite journal |last=Turpin |first=Andrew |last2=Scholer |first2=Falk |title=User performance versus precision measures for simple search tasks |journal=Proceedings of the 29th Annual international ACM SIGIR Conference on Research and Development in information Retrieval (Seattle, WA, August 06–11, 2006) |publisher=ACM |location=New York, NY |pages=11–18 |doi=10.1145/1148170.1148176 |year=2006 |isbn=1-59593-369-7 }}</ref> Note that the average is over all relevant documents and the relevant documents not retrieved get a precision score of zero.

Some authors choose to interpolate the <math>p(r)</math> function to reduce the impact of "wiggles" in the curve.<ref name=voc2010>{{cite journal |last=Everingham |first=Mark |last2=Van Gool |first2=Luc |last3=Williams |first3=Christopher K. I. |last4=Winn |first4=John |last5=Zisserman |first5=Andrew |title=The PASCAL Visual Object Classes (VOC) Challenge |journal=International Journal of Computer Vision |volume=88 |issue=2 |pages=303–338 |publisher=Springer |date=June 2010 |url=http://pascallin.ecs.soton.ac.uk/challenges/VOC/pubs/everingham10.pdf |accessdate=2011-08-29 |doi=10.1007/s11263-009-0275-4 }}</ref><ref name="nlpbook">{{cite book |last=Manning |first=Christopher D. |last2=Raghavan |first2=Prabhakar |last3=Schütze |first3=Hinrich |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008 |url=http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-ranked-retrieval-results-1.html }}</ref> For example, the PASCAL Visual Object Classes challenge (a benchmark for computer vision object detection) computes average precision by averaging the precision over a set of evenly spaced recall levels {0, 0.1, 0.2, ... 1.0}:<ref name="voc2010" /><ref name="nlpbook" />
:<math>\operatorname{AveP} = \frac{1}{11} \sum_{r \in \{0, 0.1, \ldots, 1.0\}} p_{\operatorname{interp}}(r)</math>
where <math>p_{\operatorname{interp}}(r)</math> is an interpolated precision that takes the maximum precision over all recalls greater than <math>r</math>:
:<math>p_{\operatorname{interp}}(r) = \operatorname{max}_{\tilde{r}:\tilde{r} \geq r} p(\tilde{r})</math>.

An alternative is to derive an analytical <math>p(r)</math> function by assuming a particular parametric distribution for the underlying decision values. For example, a ''binormal precision-recall curve'' can be obtained by assuming decision values in both classes to follow a Gaussian distribution.<ref>K.H. Brodersen, C.S. Ong, K.E. Stephan, J.M. Buhmann (2010). [http://icpr2010.org/pdfs/icpr2010_ThBCT8.28.pdf The binormal assumption on precision-recall curves]. ''Proceedings of the 20th International Conference on Pattern Recognition'', 4263-4266.</ref>

=== Precision at K ===

For modern (Web-scale) information retrieval, recall is no longer a meaningful metric, as many queries have thousands of relevant documents, and few users will be interested in reading all of them. [[Precision and recall#Precision|Precision]] at k documents (P@k) is still a useful metric (e.g., P@10 or "Precision at 10" corresponds to the number of relevant results on the first search results page), but fails to take into account the positions of the relevant documents among the top k.{{citation needed|date=June 2015}}  Another shortcoming is that on a query with fewer relevant results than k, even a perfect system will have a score less than 1.<ref name="stanford" />  It is easier to score manually since only the top k results need to be examined to determine if they are relevant or not.

=== R-Precision ===

R-precision requires knowing all documents that are relevant to a query.  The number of relevant documents, <math>R</math>, is used as the cutoff for calculation, and this varies from query to query.  For example, if there are 15 documents relevant to "red" in a corpus (R=15), R-precision for "red" looks at the top 15 documents returned, counts the number that are relevant <math>r</math> turns that into a relevancy fraction: <math>r/R = r/15</math>.<ref name="trec15"/>

Precision is equal to recall at the '''R'''-th position.<ref name="stanford">{{cite web|url=http://nlp.stanford.edu/IR-book/pdf/08eval.pdf|title=Chapter 8: Evaluation in information retrieval|accessdate=2015-06-14|date=2009|authors=Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze}}  Part of ''Introduction to Information Retrieval'' [http://nlp.stanford.edu/IR-book/]</ref>

Empirically, this measure is often highly correlated to mean average precision.<ref name="stanford" />

=== Mean average precision ===
<!-- [[Mean average precision]] redirects here -->
Mean average precision for a set of queries is the mean of the average precision scores for each query.
:<math> \operatorname{MAP} = \frac{\sum_{q=1}^Q \operatorname{AveP(q)}}{Q} \!</math>
where ''Q'' is the number of queries.

=== Discounted cumulative gain ===
{{main|Discounted cumulative gain}}
DCG uses a graded relevance scale of documents from the result set to evaluate the usefulness, or gain, of a document based on its position in the result list. The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.

The DCG accumulated at a particular rank position <math>p</math> is defined as:

:<math> \mathrm{DCG_{p}} = rel_{1} + \sum_{i=2}^{p} \frac{rel_{i}}{\log_{2}i}. </math>

Since result set may vary in size among different queries or systems, to compare performances the normalised version of DCG uses an ideal DCG. To this end, it sorts documents of a result list by relevance, producing an ideal DCG at position p (<math>IDCG_p</math>), which normalizes the score:

:<math> \mathrm{nDCG_{p}} = \frac{DCG_{p}}{IDCG{p}}. </math>

The nDCG values for all queries can be averaged to obtain a measure of the average performance of a ranking algorithm. Note that in a perfect ranking algorithm, the <math>DCG_p</math> will be the same as the <math>IDCG_p</math> producing an nDCG of 1.0. All nDCG calculations are then relative values on the interval 0.0 to 1.0 and so are cross-query comparable.

=== Other measures ===
{{Confusion matrix terms}}
* [[Mean reciprocal rank]]
* [[Spearman's rank correlation coefficient]]
* bpref - a summation-based measure of how many relevant documents are ranked before irrelevant documents<ref name="trec15">http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf</ref>
* GMAP - geometric mean of (per-topic) average precision<ref name="trec15" />
* Measures based on marginal relevance and document diversity - see {{section link|Relevance (information retrieval)|Problems and alternatives}}

===Visualization===

Visualizations of information retrieval performance include:
* Graphs which chart precision on one axis and recall on the other<ref name="trec15" />
* Histograms of average precision over various topics<ref name="trec15" />
* [[Receiver operating characteristic]] (ROC curve)
* [[Confusion matrix]]

== Timeline ==

* Before the '''1900s'''
*: '''1801''': [[Joseph Marie Jacquard]] invents the [[Jacquard loom]], the first machine to use punched cards to control a sequence of operations.
*: '''1880s''': [[Herman Hollerith]] invents an electro-mechanical data tabulator using punch cards as a machine readable medium.
*: '''1890''' Hollerith [[Punched cards|cards]], [[keypunch]]es and [[Tabulating machine|tabulators]] used to process the [[1890 US Census]] data.
* '''1920s-1930s'''
*: [[Emanuel Goldberg]] submits patents for his "Statistical Machine” a document search engine that used photoelectric cells and pattern recognition to search the metadata on rolls of microfilmed documents.
* '''1940s–1950s'''
*: '''late 1940s''': The US military confronted problems of indexing and retrieval of wartime scientific research documents captured from Germans.
*:: '''1945''': [[Vannevar Bush]]'s ''[[As We May Think]]'' appeared in ''[[Atlantic Monthly]]''.
*:: '''1947''': [[Hans Peter Luhn]] (research engineer at IBM since 1941) began work on a mechanized punch card-based system for searching chemical compounds.
*: '''1950s''': Growing concern in the US for a "science gap" with the USSR motivated, encouraged funding and provided a backdrop for mechanized literature searching systems ([[Allen Kent]] ''et al.'') and the invention of citation indexing ([[Eugene Garfield]]).
*: '''1950''': The term "information retrieval" was coined by [[Calvin Mooers]].<ref>Mooers, Calvin N.; ''[https://babel.hathitrust.org/cgi/pt?id=mdp.39015034570591;view=1up;seq=3 The Theory of Digital Handling of Non-numerical Information and its Implications to Machine Economics]'' (Zator Technical Bulletin No. 48), cited in {{cite journal|last1=Fairthorne|first1=R. A.|title=Automatic Retrieval of Recorded Information|journal=The Computer Journal|date=1958|volume=1|issue=1|page=37|doi=10.1093/comjnl/1.1.36|url=http://comjnl.oxfordjournals.org/content/1/1/36.short}}</ref>
*: '''1951''': Philip Bagley conducted the earliest experiment in computerized document retrieval in a master thesis at [[MIT]].<ref name="Doyle1975">{{cite book |last=Doyle |first=Lauren |last2=Becker |first2=Joseph |title=Information Retrieval and Processing |publisher=Melville |year=1975 |pages=410 pp. |isbn=0-471-22151-1 }}</ref>
*: '''1955''': Allen Kent joined [[Case Western Reserve University]], and eventually became associate director of the Center for Documentation and Communications Research. That same year, Kent and colleagues published a paper in American Documentation describing the precision and recall measures as well as detailing a proposed "framework" for evaluating an IR system which included statistical sampling methods for determining the number of relevant documents not retrieved.<ref>{{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |doi=10.1002/asi.5090060411}}</ref>
*: '''1958''': International Conference on Scientific Information Washington DC included consideration of IR systems as a solution to problems identified. See: ''Proceedings of the International Conference on Scientific Information, 1958'' (National Academy of Sciences, Washington, DC, 1959)
*: '''1959''': [[Hans Peter Luhn]] published "Auto-encoding of documents for information retrieval."
* '''1960s''':
*: '''early 1960s''': [[Gerard Salton]] began work on IR at Harvard, later moved to Cornell.
*: '''1960''': [[Melvin Earl Maron]] and John Lary<!-- sic --> Kuhns<ref name="Maron2008">{{cite journal |title=An Historical Note on the Origins of Probabilistic Indexing |last=Maron | first=Melvin E. |journal=Information Processing and Management |volume=44 |year=2008 |pages=971–972 |url=http://yunus.hacettepe.edu.tr/~tonta/courses/spring2008/bby703/maron-on-probabilistic%20indexing-2008.pdf |doi=10.1016/j.ipm.2007.02.012 |issue=2 }}</ref> published "On relevance, probabilistic indexing, and information retrieval" in the Journal of the ACM 7(3):216–244, July 1960.
*: '''1962''':
*:* [[Cyril W. Cleverdon]] published early findings of the Cranfield studies, developing a model for IR system evaluation. See: Cyril W. Cleverdon, "Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems". Cranfield Collection of Aeronautics, Cranfield, England, 1962.
*:* Kent published ''Information Analysis and Retrieval''.
*: '''1963''':
*:* Weinberg report "Science, Government and Information" gave a full articulation of the idea of a "crisis of scientific information."  The report was named after Dr. [[Alvin Weinberg]].
*:* Joseph Becker and [[Robert M. Hayes]] published text on information retrieval. Becker, Joseph; Hayes, Robert Mayo. ''Information storage and retrieval: tools, elements, theories''. New York, Wiley (1963).
*: '''1964''':
*:* [[Karen Spärck Jones]] finished her thesis at Cambridge, ''Synonymy and Semantic Classification'', and continued work on [[computational linguistics]] as it applies to IR.
*:* The [[National Bureau of Standards]] sponsored a symposium titled "Statistical Association Methods for Mechanized Documentation." Several highly significant papers, including G. Salton's first published reference (we believe) to the [[SMART Information Retrieval System|SMART]] system.
*:'''mid-1960s''':
*::* National Library of Medicine developed [[MEDLARS]] Medical Literature Analysis and Retrieval System, the first major machine-readable database and batch-retrieval system.
*::* Project Intrex at MIT.
*:: '''1965''': [[J. C. R. Licklider]] published ''Libraries of the Future''.
*:: '''1966''': [[Don Swanson]] was involved in studies at University of Chicago on Requirements for Future Catalogs.
*: '''late 1960s''': [[F. Wilfrid Lancaster]] completed evaluation studies of the MEDLARS system and published the first edition of his text on information retrieval.
*:: '''1968''':
*:* Gerard Salton published ''Automatic Information Organization and Retrieval''.
*:* John W. Sammon, Jr.'s RADC Tech report "Some Mathematics of Information Storage and Retrieval..." outlined the vector model.
*:: '''1969''': Sammon's "A nonlinear mapping for data structure analysis" (IEEE Transactions on Computers) was the first proposal for visualization interface to an IR system.
* '''1970s'''
*: '''early 1970s''':
*::* First online systems—NLM's AIM-TWX, MEDLINE; Lockheed's Dialog; SDC's ORBIT.
*::* [[Theodor Nelson]] promoting concept of [[hypertext]], published ''Computer Lib/Dream Machines''.
*: '''1971''': [[Nicholas Jardine]] and [[Cornelis J. van Rijsbergen]] published "The use of hierarchic clustering in information retrieval", which articulated the "cluster hypothesis."<ref>{{cite journal|author=N. Jardine, C.J. van Rijsbergen|title=The use of hierarchic clustering in information retrieval|journal=Information Storage and Retrieval|volume=7|issue=5|pages=217–240|date=December 1971|doi=10.1016/0020-0271(71)90051-9}}</ref>
*: '''1975''': Three highly influential publications by Salton fully articulated his vector processing framework and term discrimination model:
*::* ''A Theory of Indexing'' (Society for Industrial and Applied Mathematics)
*::* ''A Theory of Term Importance in Automatic Text Analysis'' ([[JASIS]] v. 26)
*::* ''A Vector Space Model for Automatic Indexing'' ([[Communications of the ACM|CACM]] 18:11)
*: '''1978''': The First [[Association for Computing Machinery|ACM]] [[Special Interest Group on Information Retrieval|SIGIR]] conference.
*: '''1979''': C. J. van Rijsbergen published ''Information Retrieval'' (Butterworths). Heavy emphasis on probabilistic models.
*: '''1979''': Tamas Doszkocs implemented the CITE natural language user interface for MEDLINE at the National Library of Medicine. The CITE system supported free form query input, ranked output and relevance feedback.<ref>Doszkocs, T.E. & Rapp, B.A. (1979). "Searching MEDLINE in English: a Prototype User Inter-face with Natural Language Query, Ranked Output, and relevance feedback," In: Proceedings of the ASIS Annual Meeting, 16: 131-139.</ref>
* '''1980s'''
*: '''1980''': First international ACM SIGIR conference, joint with British Computer Society IR group in Cambridge.
*: '''1982''': [[Nicholas J. Belkin]], Robert N. Oddy, and Helen M. Brooks proposed the ASK (Anomalous State of Knowledge) viewpoint for information retrieval. This was an important concept, though their automated analysis tool proved ultimately disappointing.
*: '''1983''': Salton (and Michael J. McGill) published ''Introduction to Modern Information Retrieval'' (McGraw-Hill), with heavy emphasis on vector models.
*: '''1985''': David Blair and [[Bill Maron]] publish: An Evaluation of Retrieval Effectiveness for a Full-Text Document-Retrieval System
*: '''mid-1980s''': Efforts to develop end-user versions of commercial IR systems.
*:: '''1985–1993''': Key papers on and experimental systems for visualization interfaces.
*:: Work by [[Donald B. Crouch]], [[Robert R. Korfhage]], Matthew Chalmers, Anselm Spoerri and others.
*: '''1989''': First [[World Wide Web]] proposals by [[Tim Berners-Lee]] at [[CERN]].
* '''1990s'''
*: '''1992''': First [[Text Retrieval Conference|TREC]] conference.
*: '''1997''': Publication of [[Robert R. Korfhage|Korfhage]]'s ''Information Storage and Retrieval''<ref name="Korfhage1997">{{cite book |last=Korfhage |first=Robert R. |title=Information Storage and Retrieval |publisher=Wiley |year=1997 |pages=368 pp. |isbn=978-0-471-14338-3 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471143383,descCd-authorInfo.html }}</ref> with emphasis on visualization and multi-reference point systems.
*: '''late 1990s''': [[Web search engine]]s implementation of many features formerly found only in experimental IR systems. Search engines become the most common and maybe best instantiation of IR models.

== Awards in the field ==

* [[Tony Kent Strix award]]
* [[Gerard Salton Award]]

== Leading IR Research Groups ==
* [[Center for Intelligent Information Retrieval]] (CIIR) at the University of Massachusetts Amherst <ref>{{Cite web|url=http://ciir.cs.umass.edu|title=Center for Intelligent Information Retrieval {{!}} UMass Amherst|website=ciir.cs.umass.edu|access-date=2016-07-29}}</ref>
* Information Retrieval Group at the University of Glasgow <ref>{{Cite web|url=http://www.gla.ac.uk/schools/computing/research/researchoverview/informationretrieval/|title=University of Glasgow - Schools - School of Computing Science - Research - Research overview - Information Retrieval|website=www.gla.ac.uk|access-date=2016-07-29}}</ref>
* Information and Language Processing Systems (ILPS) at the University of Amsterdam <ref>{{Cite web|url=http://ilps.science.uva.nl/|title=ILPS - information and language processing systems|website=ILPS|language=en-US|access-date=2016-07-29}}</ref>
* Language Technologies Institutes (LTI) at the Carnegie Mellon University
* Text Information Management and Analysis Group (TIMAN) at  the University of Illinois at Urbana-Champaign

==See also==

{{div col}}

* [[Adversarial information retrieval]]
* [[Collaborative information seeking]]
* [[Controlled vocabulary]]
* [[Cross-language information retrieval]]
* [[Data mining]]
* [[European Summer School in Information Retrieval]]
* [[Human–computer information retrieval]] (HCIR)
* [[Information extraction]]
* [[Information Retrieval Facility]]
* [[Knowledge visualization]]
* [[Multimedia information retrieval]]
* [[Personal information management]]
* [[Relevance (Information Retrieval)]]
* [[Relevance feedback]]
* [[Rocchio Classification]]
* [[Index (search engine)|Search index]]
* [[Social information seeking]]
* [[Special Interest Group on Information Retrieval]]
* [[Subject indexing]]
* [[Temporal information retrieval]]
* [[tf-idf]]
* [[XML-Retrieval]]

{{div col end}}

== References ==
{{reflist}}

==Further reading==
* Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch&uuml;tze. [http://www-csli.stanford.edu/~hinrich/information-retrieval-book.html Introduction to Information Retrieval]. Cambridge University Press, 2008.
*Stefan B&uuml;ttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, Cambridge, Mass., 2010.

==External links==
{{wikiquote}}
* [http://www.acm.org/sigir/ ACM SIGIR: Information Retrieval Special Interest Group]
* [http://irsg.bcs.org/ BCS IRSG: British Computer Society - Information Retrieval Specialist Group]
* [http://trec.nist.gov Text Retrieval Conference (TREC)]
* [http://www.isical.ac.in/~fire Forum for Information Retrieval Evaluation (FIRE)]
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval] (online book) by [[C. J. van Rijsbergen]]
* [http://ir.dcs.gla.ac.uk/wiki/ Information Retrieval Wiki]
* [http://ir-facility.org/ Information Retrieval Facility]
* [http://www.nonrelevant.net Information Retrieval @ DUTH]
* [http://trec.nist.gov/pubs/trec15/appendices/CE.MEASURES06.pdf TREC report on information retrieval evaluation techniques]
* [http://www.ebaytechblog.com/2010/11/10/measuring-search-relevance/ How eBay measures search relevance]
* [http://retrieval.ceti.gr Information retrieval performance evaluation tool @ Athena Research Centre]

{{Authority control}}

{{DEFAULTSORT:Information Retrieval}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Information retrieval| ]]
[[Category:Natural language processing]]
<=====doc_Id=====>:8
<=====title=====>:
Category:Internet search
<=====text=====>:
{{Cat main|Internet search}}

[[Category:Web services]]
[[Category:Information retrieval]]
[[Category:World Wide Web|Search]] <!-- searching is a web function. Note that [[Internet search]] redirects to [[Web search engine]] -->
<=====doc_Id=====>:11
<=====title=====>:
Category:Information retrieval evaluation
<=====text=====>:
The main overview for this category is at {{section link|Information retrieval|Performance and correctness_measures}}.

[[Category:Information retrieval]]
<=====doc_Id=====>:14
<=====title=====>:
Category:Information retrieval researchers
<=====text=====>:
[[Category:Computer scientists by field of research|Information retrieval]]
[[Category:Information retrieval]]
<=====doc_Id=====>:17
<=====title=====>:
Category:Knowledge representation
<=====text=====>:
<!--''Main article : [[Knowledge representation and reasoning]]''-->
{{Cat main|Knowledge representation}}

Significant articles:
* [[Library classification]]
* [[Ontology (computer science)]]
* [[Semantic network]]
{{Category TOC}}

{{Commons cat|Knowledge representation}}

[[Category:Artificial intelligence]]
[[Category:Information science]]
[[Category:Knowledge engineering]]
[[Category:Programming paradigms]]
[[Category:Reasoning]]
[[Category:Scientific modeling]]
[[Category:Information retrieval]]
<=====doc_Id=====>:20
<=====title=====>:
Thomas write rule
<=====text=====>:
In [[computer science]], particularly the field of [[database]]s, the '''Thomas write rule''' is a rule in [[timestamp-based concurrency control]].  It can be summarized as ''ignore outdated writes''.

It states that, if a more recent transaction has already written the value of an object, then a less recent transaction does not need perform its own write since it will eventually be overwritten by the more recent one.  

The Thomas write rule is applied in situations where a predefined '''logical''' order is assigned to transactions when they start.  For example a transaction might be assigned a monotonically increasing timestamp when it is created.  The rule prevents changes in the order in which the transactions are executed from creating different outputs: The outputs will always be consistent with the predefined logical order.

For example consider a database with 3 variables (A, B, C), and two atomic operations C := A (T1), and C := B (T2).  Each transaction involves a read (A or B), and a write (C).  The only conflict between these transactions is the write on C.  The following is one possible schedule for the operations of these transactions:

:<math>\begin{bmatrix}
T_1 & T_2 \\
& Read(A) \\
Read(B) &   \\
 &Write(C)   \\
Write(C) &  \\
Commit & \\
& Commit \end{bmatrix} \Longleftrightarrow
\begin{bmatrix}
T_1 & T_2 \\
& Read(A) \\
Read(B) & \\
& Write(C) \\
 & \\
Commit & \\
& Commit\\
\end{bmatrix}
</math>

If (when the transactions are created) T1 is assigned a timestamp that precedes T2 (i.e., according to the logical order, T1 comes first), then only T2's write should be visible.  If, however, T1's write is executed after T2's write, then we need a way to detect this and discard the write.

One practical approach to this is to label each value with a write timestamp (WTS) that indicates the timestamp of the last transaction to modify the value.  Enforcing the Thomas write rule only requires checking to see if the write timestamp of the object is greater than the time stamp of the transaction performing a write.  If so, the write is discarded   

In the example above, if we call TS(T) the timestamp of transaction T, and WTS(O) the write timestamp of object O, then T2's write sets WTS(C) to TS(T2).  When T1 tries to write C, it sees that TS(T1) < WTS(C), and discards the write.  If a third transaction T3 (with TS(T3) > TS(T2)) were to then write to C, it would get TS(T3) > WTS(C), and the write would be allowed.

==References==
*{{Cite journal | author=Robert H. Thomas | title=A majority consensus approach to concurrency control for multiple copy databases | journal=ACM Transactions on Database Systems | year=1979 | volume=4 | issue=2 | pages= 180–209 | doi=10.1145/320071.320076 }}

[[Category:Data management]]
[[Category:Transaction processing]]


{{compu-sci-stub}}
<=====doc_Id=====>:23
<=====title=====>:
Rollback (data management)
<=====text=====>:
{{Other uses|Rollback (disambiguation)}}
{{Selfref|For the Wikipedia tool, see [[Wikipedia:Rollback feature]].}}
{{no footnotes|date=June 2009}}

In [[database]] technologies, a '''rollback''' is an operation which returns the database to some previous state. Rollbacks are important for database [[data integrity|integrity]], because they mean that the database can be restored to a clean copy even after erroneous operations are performed. They are crucial for recovering from database server crashes; by rolling back any [[Database transaction|transaction]] which was active at the time of the crash, the database is restored to a consistent state.

The rollback feature is usually implemented with a [[Database log|transaction log]], but can also be implemented via [[multiversion concurrency control]].

==Cascading rollback==
A ''cascading rollback'' occurs in database systems when a transaction (T1) causes a failure and a rollback must be performed. Other transactions dependent on T1's actions must also be rollbacked due to T1's failure, thus causing a cascading effect. That is, one transaction's failure causes many to fail.

Practical database recovery techniques guarantee cascadeless rollback, therefore a cascading rollback is not a desirable result.

==SQL==
In [[SQL]], <code>ROLLBACK</code> is a command that causes all data changes since the last <code>[[Begin work (SQL)|BEGIN WORK]]</code>, or <code>[[Start transaction (SQL)|START TRANSACTION]]</code> to be discarded by the [[relational database management systems]] (RDBMS), so that the state of the data is "rolled back" to the way it was before those changes were made.

A <code>ROLLBACK</code> statement will also release any existing [[savepoint]]s that may be in use.

In most SQL dialects, <code>ROLLBACK</code>s are connection specific.  This means that if two connections are made to the same database, a <code>ROLLBACK</code> made in one connection will not affect any other connections.  This is vital for proper [[Concurrent programming|concurrency]].

==See also==
*[[Savepoint]]
*[[Commit (data management)|Commit]]
*[[Undo]]
*[[Schema migration]]

==References==
*{{cite book |author = [[Ramez Elmasri]] |title= Fundamentals of Database Systems |publisher= [[Pearson Addison Wesley]] |year= 2007|isbn= 0-321-36957-2 }}
*[http://msdn2.microsoft.com/en-us/library/ms181299.aspx "ROLLBACK Transaction"], Microsoft SQL Server.
*[http://www.pantz.org/software/mysql/mysqlcommands.html "Sql Commands"], MySQL.

{{Databases}}
{{Web syndication}}

[[Category:Data management]]
[[Category:Database theory]]
[[Category:SQL]]
[[Category:Transaction processing]]
[[Category:Reversible computing]]


{{compu-prog-stub}}
<=====doc_Id=====>:26
<=====title=====>:
Schedule (computer science)
<=====text=====>:
{{refimprove|date=November 2012}}

In the fields of [[database]]s and [[transaction processing]] (transaction management), a '''schedule''' (or '''history''') of a system is an abstract model to describe execution of transactions running in the system. Often it is a ''list'' of operations (actions) ordered by time, performed by a set of [[Database transaction|transactions]] that are executed together in the system. If order in time between certain operations is not determined by the system, then a ''[[partial order]]'' is used. Examples of such operations are requesting a read operation, reading, writing, aborting, committing, requesting lock, locking, etc. Not all transaction operation types should be included in a schedule, and typically only selected operation types (e.g., data access operations) are included, as needed to reason about and describe certain phenomena. Schedules and schedule properties are fundamental concepts in database [[concurrency control]] theory.

==Formal description==

The following is an example of a schedule:

:<math>D = \begin{bmatrix}
T1 & T2 & T3 \\
R(X) &  &  \\
W(X) &  &  \\
Com. &  &  \\
 & R(Y) & \\
 & W(Y) & \\
 & Com. & \\
 && R(Z) \\
 && W(Z) \\
 && Com. \end{bmatrix}</math>

In this example, the horizontal axis represents the different transactions in the schedule D. The vertical axis represents time order of operations. Schedule D consists of three transactions T1, T2, T3.  The schedule describes the actions of the transactions as seen by the [[DBMS]].
First T1 Reads and Writes to object X, and then Commits. Then T2 Reads and Writes to object Y and Commits, and finally T3 Reads and Writes to object Z and Commits.  This is an example of a ''serial'' schedule, i.e., sequential with no overlap in time, because the actions of in all three transactions are sequential, and the transactions are not interleaved in time.

Representing the schedule D above by a table (rather than a list) is just for the convenience of identifying each transaction's operations in a glance. This notation is used throughout the article below. A more common way in the technical literature for representing such schedule is by a list:

:::D = R1(X) W1(X) Com1 R2(Y) W2(Y) Com2 R3(Z) W3(Z) Com3

Usually, for the purpose of reasoning about concurrency control in databases, an operation is modeled as ''[[Atomic operation|atomic]]'', occurring at a point in time, without duration. When this is not satisfactory start and end time-points and possibly other point events are specified (rarely). Real executed operations always have some duration and specified respective times of occurrence of events within them (e.g., "exact" times of beginning and completion), but for concurrency control reasoning usually only the precedence in time of the whole operations (without looking into the quite complex details of each operation) matters, i.e., which operation is before, or after another operation. Furthermore, in many cases the before/after relationships between two specific operations do not matter and should not be specified, while being specified for other pairs of operations.

In general operations of transactions in a schedule can interleave (i.e., transactions can be executed concurrently), while time orders between operations in each transaction remain unchanged as implied by the transaction's program. Since not always time orders between all operations of all transactions matter and need to be specified, a schedule is, in general, a ''[[partial order]]'' between operations rather than a ''[[total order]]'' (where order for each pair is determined, as in a list of operations). Also in the general case each transaction may consist of several processes, and itself be properly represented by a partial order of operations, rather than a total order. Thus in general a schedule is a partial order of operations, containing ([[embedding]]) the partial orders of all its transactions.

Time-order between two operations can be represented by an ''[[ordered pair]]'' of these operations (e.g., the existence of a pair (OP1,OP2) means that OP1 is always before OP2), and a schedule in the general case is a [[set (mathematics)|set]] of such ordered pairs. Such a set, a schedule, is a [[partial order]] which can be represented by an ''[[acyclic directed graph]]'' (or ''directed acyclic graph'', DAG) with operations as nodes and time-order as a [[directed edge]] (no cycles are allowed since a cycle means that a first (any) operation on a cycle can be both before and after (any) another second operation on the cycle, which contradicts our perception of [[Time]]). In many cases a graphical representation of such graph is used to demonstrate a schedule.

'''Comment:''' Since a list of operations (and the table notation used in this article) always represents a total order between operations, schedules that are not a total order cannot be represented by a list (but always can be represented by a DAG).

==Types of schedule==

===Serial===

The transactions are executed non-interleaved (see example above)
i.e., a serial schedule is one in which no transaction starts until a running transaction has ended.

===Serializable===<!-- This section is linked from [[Concurrency control]] -->

A schedule that is equivalent (in its outcome) to a serial schedule has the [[serializability]] property.

In schedule E, the order in which the actions of the transactions are executed is not the same as in D, but in the end, E gives the same result as D.
:<math>E = \begin{bmatrix}
T1 & T2 & T3 \\
R(X) &  &  \\
   & R(Y) & \\
 && R(Z) \\

W(X) &  &  \\
 & W(Y) & \\
 && W(Z) \\
Com. & Com. & Com. \end{bmatrix}</math>

====Conflicting actions====

Two actions are said to be in conflict (conflicting pair) if: 

# The actions belong to different transactions.
# At least one of the actions is a write operation.
# The actions access the same object (read or write).

The following set of actions is conflicting: 
* R1(X), W2(X), W3(X) (3 conflicting pairs)

While the following sets of actions are not: 
* R1(X), R2(X), R3(X)
* R1(X), W2(Y), R3(X)

====Conflict equivalence====

The schedules S1 and S2 are said to be conflict-equivalent if the following two conditions are satisfied: 

# Both schedules S1 and S2 involve the same set of transactions (including ordering of actions within each transaction).
# Both schedules have same set of conflicting operations.

====Conflict-serializable====

A schedule is said to be conflict-serializable when the schedule is conflict-equivalent to one or more serial schedules. 

Another definition for conflict-serializability is that a schedule is conflict-serializable if and only if its [[precedence graph]]/serializability graph, when only committed transactions are considered, is acyclic (if the graph is defined to include also uncommitted transactions, then cycles involving uncommitted transactions may occur without conflict serializability violation).

:<math>G = \begin{bmatrix}
T1 & T2 \\
R(A) &   \\
 & R(A) \\
W(B) & \\
Com. & \\
 & W(A) \\
 & Com. \\
 &\end{bmatrix}</math>

Which is conflict-equivalent to the serial schedule <T1,T2>, but not <T2,T1>.

====Commitment-ordered====
{{POV-section|Commitment ordering|date=November 2011}}
A schedule is said to be commitment-ordered (commit-ordered), or commitment-order-serializable, if it obeys the [[Commitment ordering]] (CO; also commit-ordering or commit-order-serializability) schedule property. This means that the order in time of transactions' commitment events is compatible with the precedence (partial) order of the respective transactions, as induced by their schedule's acyclic precedence graph (serializability graph, conflict graph). This implies that it is also conflict-serializable. The CO property is especially effective for achieving [[Global serializability]] in distributed systems.

'''Comment:''' [[Commitment ordering]], which was discovered in 1990, is obviously not mentioned in ([[#Bern1987|Bernstein et al. 1987]]). Its correct definition appears in ([[#Weikum2001|Weikum and Vossen 2001]]), however the description there of its related techniques and theory is partial, inaccurate, and misleading.{{Says who|date=December 2011}} For an extensive coverage of commitment ordering and its sources see ''[[Commitment ordering]]'' and ''[[The History of Commitment Ordering]]''.

====View equivalence====

Two schedules S1 and S2 are said to be view-equivalent when the following conditions are satisfied:

# If the transaction <math>T_i</math> in S1 reads an initial value for object X, so does the transaction <math>T_i</math> in S2. 
# If the transaction <math>T_i</math> in S1 reads the value written by transaction <math>T_j</math> in S1 for object X, so does the transaction <math>T_i</math> in S2.
# If the transaction <math>T_i</math> in S1 is the final transaction to write the value for an object X, so is the transaction <math>T_i</math> in S2.

====View-serializable====

A schedule is said to be view-serializable if it is view-equivalent to some serial schedule. 
Note that by definition, all conflict-serializable schedules are view-serializable. 

:<math>G = \begin{bmatrix}
T1 & T2 \\
R(A) &   \\
 & R(A) \\
W(B) & \\
 \end{bmatrix}</math>

Notice that the above example (which is the same as the example in the discussion of conflict-serializable) is both view-serializable and conflict-serializable at the same time.) There are however view-serializable schedules that are not conflict-serializable: those schedules with a transaction performing a [[blind write]]:
 
:<math>H = \begin{bmatrix}
T1 & T2 & T3 \\
R(A) & & \\
 & W(A) & \\
 & Com. & \\
W(A) & & \\
Com. & & \\
 & & W(A) \\
 & & Com. \\
 & & \end{bmatrix}</math>

The above example is not conflict-serializable, but it is view-serializable since it has a view-equivalent serial schedule <T1,&nbsp;T2,&nbsp;T3>. 

Since determining whether a schedule is view-serializable is [[NP-complete]], view-serializability has little practical interest.{{citation needed|date=April 2015}}

===Recoverable===<!-- This section is linked from [[Concurrency control]] -->

Transactions commit only after all transactions whose changes they read, commit.

:<math>F = \begin{bmatrix}
T1 & T2 \\
R(A) &   \\
W(A) &   \\
 & R(A) \\
 & W(A) \\
Com. & \\
 & Com.\\
 &\end{bmatrix} 
F2 = \begin{bmatrix}
T1 & T2 \\
R(A) &   \\
W(A) &   \\
 & R(A) \\
 & W(A) \\
Abort &  \\
& Abort \\
 &\end{bmatrix}</math>

These schedules are recoverable.  F is recoverable because T1 commits before T2, that makes the value read by T2 correct.  Then T2 can commit itself.  In F2, if T1 aborted, T2 has to abort because the value of A it read is incorrect.  In both cases, the database is left in a consistent state.

====Unrecoverable====

If a transaction T1 aborts, and a transaction T2 commits, but T2 relied on T1, we have an unrecoverable schedule.

:<math>G = \begin{bmatrix}
T1 & T2 \\
R(A) &   \\
W(A) &   \\
 & R(A) \\
 & W(A) \\
 & Com. \\
Abort & \\
 &\end{bmatrix}</math>

In this example, G is unrecoverable, because T2 read the value of A written by T1, and committed.  T1 later aborted, therefore the value read by T2 is wrong, but since T2 committed, this schedule is unrecoverable.

====Avoids cascading aborts / rollbacks (ACA)====

Also named cascadeless. Avoids that a single transaction abort leads to a series of transaction rollbacks. A strategy to prevent cascading aborts is to disallow a transaction from reading uncommitted changes from another transaction in the same schedule. 

The following examples are the same as the ones in the discussion on recoverable: 

:<math>F = \begin{bmatrix}
T1 & T2 \\
R(A) &   \\
W(A) &   \\
 & R(A) \\
 & W(A) \\
Com. & \\
 & Com.\\
 &\end{bmatrix} 
F2 = \begin{bmatrix}
T1 & T2 \\
R(A) &   \\
W(A) &   \\
 & R(A) \\
 & W(A) \\
Abort &  \\
& Abort \\
 &\end{bmatrix}</math>

In this example, although F2 is recoverable, it does not avoid 
cascading aborts. It can be seen that if T1 aborts, T2 will have to 
be aborted too in order to maintain the correctness of the schedule 
as T2 has already read the uncommitted value written by T1. 

The following is a recoverable schedule which avoids cascading abort. Note, however, that the update of A by T1 is always lost (since T1 is aborted).

:<math>F3 = \begin{bmatrix}
T1 & T2 \\
 & R(A) \\
R(A) &   \\
W(A) &   \\
 & W(A) \\
Abort &  \\
& Commit \\
 &\end{bmatrix}</math>
Note that this Schedule would not be serializable if T1 would be committed.
Cascading aborts avoidance is sufficient but not necessary for a schedule to be recoverable.

====Strict====

A schedule is strict - has the strictness property - if for any two transactions T1, T2, if a write operation of T1 precedes a ''conflicting'' operation of T2 (either read or write), then the commit or abort event of T1 also precedes that conflicting operation of T2.

Any strict schedule is cascadeless, but not the converse. Strictness allows efficient recovery of databases from failure.

==Hierarchical relationship between serializability classes==

The following expressions illustrate the hierarachical (containment) relationships between [[serializability]] and [[Serializability#Correctness - recoverability|recoverability]] classes: 

* Serial &sub; commitment-ordered &sub; conflict-serializable &sub; view-serializable &sub; all schedules
* Serial &sub; strict &sub; avoids cascading aborts &sub; recoverable &sub; all schedules

The [[Venn diagram]] (below) illustrates the above clauses graphically. 

[[File:Schedule-serializability.png|frame|none|Venn diagram for serializability and recoverability classes]]

==Practical implementations==

In practice, most general purpose database systems employ conflict-serializable and recoverable (primarily strict) schedules.

==See also==
* [[schedule (project management)]]

==References==

*<cite id=Bern1987>[[Phil Bernstein|Philip A. Bernstein]], Vassos Hadzilacos, Nathan Goodman: [http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx  ''Concurrency Control and Recovery in Database Systems''], Addison Wesley Publishing Company, 1987, ISBN 0-201-10715-5</cite>
*<cite id=Weikum2001>[[Gerhard Weikum]], Gottfried Vossen: [http://www.elsevier.com/wps/find/bookdescription.cws_home/677937/description#description  ''Transactional Information Systems''], Elsevier, 2001, ISBN 1-55860-508-8</cite>

[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:29
<=====title=====>:
Nested transaction
<=====text=====>:
A '''nested transaction''' is a [[database transaction]] that is started by an instruction within the scope of an already started transaction.

Nested transactions are implemented differently in different databases. However, they have in common that the changes are not made visible to any unrelated transactions until the outermost transaction has committed. This means that a commit in an inner transaction does not necessarily persist updates to the database.

In some databases, changes made by the nested transaction are not seen by the 'host' transaction until the nested transaction is committed. According to some,{{Who|date=November 2009}} this follows from the isolation property of transactions.

The capability to handle nested transactions properly is a prerequisite for true component-based application architectures. In a component-based encapsulated architecture, nested transactions can occur without the programmer knowing it. A component function may or may not contain a database transaction (this is the encapsulated secret of the component. See [[Information hiding]]). If a call to such a component function is made inside a BEGIN - COMMIT bracket, nested transactions occur. Since popular databases like [[MySQL]]<ref>
{{cite web
 |url=http://dev.mysql.com/doc/refman/4.1/en/implicit-commit.html
 |title=Statements That Cause an Implicit Commit
 |author=
 |work=MySQL 4.1 Reference Manual
 |publisher=Oracle
 |accessdate=5 December 2010
}}
</ref> do not allow nesting BEGIN - COMMIT brackets, a framework or a transaction monitor is needed to handle this. When we speak about nested transactions, it should be made clear that this feature is DBMS dependent and is not available for all databases.

Theory for nested transactions is similar to the theory for flat transactions.<ref>{{Cite journal
  | last = Resende | first = R.F. | last2 = El Abbadi | first2 = A.
  | title = On the serializability theorem for nested transactions
  | journal = Information Processing Letters | volume = 50 | issue = 4
  | pages = 177–183 | date = 1994-05-25 
  | doi = 10.1016/0020-0190(94)00033-6 }}</ref>

The banking industry usually processes financial transactions using ''open nested transactions'',{{Citation needed|date=August 2015}} which is a looser variant of the nested transaction model that provides higher performance while accepting the accompanying trade-offs of inconsistency.<ref>{{cite journal
  | last = Weikum | first = Gerhard |author2=Hans-J. Schek
  | title = Concepts and Applications of Multilevel Transactions and Open Nested Transactions
  | journal = Database Transaction Models for Advanced Applications
  | pages = 515–553 | publisher = Morgan Kaufmann | year = 1992
  | url = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7962 | isbn = 1-55860-214-3
  | accessdate = 2007-11-13 }}</ref>

==Further reading==
* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8

==References==
{{reflist}}

[[Category:Data management]]
[[Category:Transaction processing]]

{{database-stub}}
<=====doc_Id=====>:32
<=====title=====>:
Match report
<=====text=====>:
{{Unreferenced|date=May 2009}}
In [[metadata]], a '''match report''' is a report that compares two distinct [[data dictionary|data dictionaries]] and creates a list of the [[data element]]s that have been identified as [[Semantic equivalence|semantically equivalent]].

== Use of match reports ==

Match reports are critical for systems that wish to automatically exchange data such as intelligent software agents.  If one computer system is requesting a report from a remote system that uses a distinct data dictionary and all of the data elements on the report manifest are included in the '''match report''' the report request can be executed.

Match reports are useful if data dictionaries use a metadata tagging system such as the [[UDEF]].

==See also==
*[[Data dictionary]]
*[[Data warehouse]]
*[[Metadata]]
*[[Semantic equivalence]]
*[[Universal Data Element Framework]]

[[Category:Knowledge representation]]
[[Category:Data management]]
[[Category:Technical communication]]
[[Category:Metadata]]
{{compu-stub}}
<=====doc_Id=====>:35
<=====title=====>:
VMDS
<=====text=====>:
{{COI|date=July 2016}}
{{unreferenced|date=July 2016}}
'''VMDS''' abbreviates the relational database technology called '''Version Managed Data Store''' provided by [[GE Energy]] as part of its [[Smallworld]] technology platform and was designed from the outset to store and analyse the highly complex spatial and topological networks typically used by enterprise utilities such as power distribution and telecommunications.

VMDS was originally introduced in 1990 as has been improved and updated over the years. Its current version is 6.0.

VMDS has been designed as a [[spatial database]]. This gives VMDS a number of distinctive characteristics when compared to conventional attribute only relational databases.

==Distributed server processing==
VMDS is composed of two parts: a simple, highly scalable data block server called '''SWMFS''' (Smallworld Master File Server) and an intelligent client [[Application programming interface|API]] written in [[C (programming language)|C]] and [[Magik programming language|Magik]]. Spatial and attribute data are stored in data blocks that reside in special files called data store files on the server. When the client application requests data it has sufficient intelligence to work out the optimum set of data blocks that are required. This request is then made to SWMFS which returns the data to the client via the network for processing.

This approach is particularly efficient and scalable when dealing with spatial and topological data which tends to flow in larger volumes and require more processing then plain attribute data (for example during a map redraw operation). This approach makes VMDS well suited to enterprise deployment that might involve hundreds or even thousands of concurrent clients.

==Support for long transactions==
Relational databases support [[Database transaction|short transactions]] in which changes to data are relatively small and are brief in terms in duration (the maximum period between the start and the end of a transaction is typically a few seconds or less).

VMDS supports long transactions in which the volume of data involved in the transaction can be substantial and the duration of the transaction can be significant (days, weeks or even months). These types of transaction are common in advanced network applications used by, for example, power distribution utilities.

Due to the time span of a long transaction in this context the amount of change can be significant (not only within the scope of the transaction, but also within the context of the database as a whole). Accordingly, it is likely that the same record might be changed more than once. To cope with this scenario VMDS has inbuilt support for automatically managing such conflicts and allows applications to review changes and accept only those edits that are correct.

==Spatial and topological capabilities==
As well as conventional relational database features such as attribute querying, join fields, triggers and calculated fields, VMDS has numerous spatial and topological capabilities. This allows spatial data such as points, texts, polylines, polygons and raster data to be stored and analysed.

Spatial functions include: find all features within a polygon, calculate the [[Voronoi polygon]]s of a set of sites and perform a [[cluster analysis]] on a set of points.

Vector spatial data such as points, polylines and polygons can be given topological attributes that allow complex networks to be modelled. Network analysis engines are provided to answer questions such as find the shortest path between two nodes or how to optimize a delivery route (the [[travelling salesman problem]]). A topology engine can be configured with a set of rules that define how topological entities interact with each other when new data is added or existing data edited.

==Data abstraction==
In VMDS all data is presented to the application as objects. This is different from many relational databases that present the data as rows from a table or query result using say [[JDBC]]. VMDS provides a data modelling tool and underlying infrastructure as part of the [[Smallworld]] technology platform that allows administrators to associate a table in the database with a Magik exemplar (or class). Magik get and set methods for the Magik exemplar can be automatically generated that expose a table's field (or column). Each VMDS ''row'' manifests itself to the application as an instance of a [[Magik programming language|Magik]] object and is known as an '''RWO''' (or real world object). Tables are known as collections in Smallworld parlance.

  # all_rwos hold all the rwos in the database and is heterogeneous
  all_rwos << my_application.rwo_set()
 
  # valve_collection holds the valve collection
  valves << all_rwos.select(:collection, {:valve})
  number_of_valves << valves.size

Queries are built up using predicate objects:

  # find 'open' valves.
  open_valves << valves.select(predicate.eq(:operating_status, "open"))
  number_of_open_valves << open_valves.size

  _for valve _over open_valves.elements()
  _loop
    write(valve.id)
  _endloop

Joins are implemented as methods on the parent RWO. For example a manager might have several employees who report to him:

  # get the employee collection.
  employees << my_application.database.collection(:gis, :employees)

  # find a manager called 'Steve' and get the first matching element
  steve << employees.select(predicate.eq(:name, "Steve").and(predicate.eq(:role, "manager")).an_element()

  # display the names of his direct reports. name is a field (or column)
  # on the employee collection (or table)
  _for employee _over steve.direct_reports.elements()
  _loop
     write(employee.name)
  _endloop

Performing a transaction:

  # each key in the hash table corresponds to the name of the field (or column) in
  # the collection (or table)
  valve_data << hash_table.new_with(
    :asset_id, 57648576,
    :material, "Iron")

  # get the valve collection directly
  valve_collection << my_application.database.collection(:gis, :valve)

  # create an insert transaction to insert a new valve record into the collection a
  # comment can be provide that describes the transaction
  transaction << record_transaction.new_insert(valve_collection, valve_data, "Inserted a new valve")
  transaction.run()

==See also==
* [[List of relational database management systems]]
* [[List of object-relational database management systems]]
* [[Spatial database]]
* [[Multiversion concurrency control]]

[[Category:Data management]]
[[Category:GIS software]]
<=====doc_Id=====>:38
<=====title=====>:
Distributed database
<=====text=====>:
{{multiple issues|
{{refimprove|date=August 2010}}
{{Cleanup|date=June 2009}}
}}

A '''distributed database''' is a [[database]]  in which [[computer data storage|storage devices]] are not all attached to a common [[Processor (computing)|processor]].<ref>http://www.its.bldrdoc.gov/fs-1037/dir-012/_1750.htm</ref> It may be stored in multiple [[computers]], located in the same physical location; or may be dispersed over a [[computer network|network]] of interconnected computers. Unlike [[Parallel computing|parallel systems]], in which the processors are tightly coupled and constitute a single database system, a distributed database system consists of loosely coupled sites that share no physical components.

System administrators can distribute collections of data (e.g. in a database) across multiple physical locations. A distributed database can reside on organized [[network servers]] or [[blockchain (database)|decentralized independent computers]] on the [[Internet]], on corporate [[intranets]] or [[extranets]], or on other organization [[Computer network|networks]]. Because they store data across multiple computers, distributed databases may improve performance at [[end-user]] worksites by allowing transactions to be processed on many machines, instead of being limited to one.<ref name="obrien">
O'Brien, J. & Marakas, G.M.(2008) Management Information Systems (pp. 185-189). New York, NY: McGraw-Hill Irwin</ref>

Two processes ensure that the distributed databases remain up-to-date and current: [[Replication (computing)|replication]] and [[Data transmission|duplication]].

# Replication involves using specialized software that looks for changes in the distributive database. Once the changes have been identified, the replication process makes all the databases look the same. The replication process can be complex and time-consuming depending on the size and number of the distributed databases. This process can also require a lot of time and computer resources.  
# Duplication, on the other hand, has less complexity. It basically identifies one database as a [[master-slave (technology)|master]] and then duplicates that database. The duplication process is normally done at a set time after hours.  This is to ensure that each distributed location has the same data.  In the duplication process, users may change only the master database. This ensures that local data will not be overwritten.

Both replication and duplication can keep the data current in all distributive locations.<ref name="obrien" />

Besides distributed database replication and fragmentation, there are many other distributed database design technologies. For example, local autonomy, synchronous and asynchronous distributed database technologies. These technologies' implementations can and do depend on the needs of the business and the sensitivity/[[confidentiality]] of the data stored in the database, and the price the business is willing to spend on ensuring [[data security]], [[data consistency|consistency]] and [[data integrity|integrity]].

When discussing access to distributed databases, [[Microsoft]] favors the term '''distributed query''', which it defines in protocol-specific manner as "[a]ny SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources".<ref>
{{cite web
 |url          = http://technet.microsoft.com/en-us/library/cc966484.aspx
 |title        = TechNet Glossary
 |publisher    = Microsoft
 |accessdate   = 2013-07-16
 |quote        = distributed query[:] Any SELECT, INSERT, UPDATE, or DELETE statement that references tables and rowsets from one or more external OLE DB data sources.
}}
</ref>
[[Oracle Database|Oracle]] provides a more language-centric view in which distributed queries and [[distributed transaction]]s form part of '''distributed SQL'''.<ref>
{{cite web
 |url          = http://docs.oracle.com/cd/E11882_01/server.112/e25789/toc.htm
 |title        = Oracle Database Concepts, 11g Release 2 (11.2)
 |last1        = Ashdown
 |first1       = Lance
 |last2        = Kyte
 |first2       = Tom
 |date=September 2011
 |publisher    = Oracle Corporation
 |accessdate   = 2013-07-17
 |quote        = Distributed SQL synchronously accesses and updates data distributed among multiple databases. [...] Distributed SQL includes distributed queries and distributed transactions. 
}}
</ref>

Today the distributed [[DBMS]] market is evolving dramatically, with new, innovative entrants and incumbents supporting the growing use of unstructured data and [[NoSQL]] DBMS engines, as well as [[XML database]]s and [[NewSQL|NewSQL databases]]. These databases are increasingly supporting distributed database architecture that provides [[high availability]] and [[fault tolerance]] through [[replication (computing)|replication]] and scale out ability.  Some examples are [[Aerospike database|Aerospike]],<ref>{{cite web|title=Aerospike distributed database|url=http://www.aerospike.com|website=Aerospike}}</ref> [[Apache Cassandra|Cassandra]],<ref>{{cite web |url=http://cassandra.apache.org/ |title=Apache Cassandra database menagement system |publisher=Apache.org}}</ref> [[Clusterpoint]],<ref>{{cite web |url=http://www.clusterpoint.com |title=Clusterpoint XML distributed database |publisher=Clusterpoint}}</ref> [[Clustrix|ClustrixDB]],<ref>{{cite web|title=Frequently Asked Questions about ClustrixDB - Clustrix Documentation|url=http://docs.clustrix.com/display/CLXDOC/Frequently+Asked+Questions+about+ClustrixDB#FrequentlyAskedQuestionsaboutClustrixDB-WhatisClustrixDB?|publisher=Clustrix, Inc.}}</ref> [[Couchbase Server|Couchbase]],<ref>{{cite web|title=Couchbase distributed database|url=http://www.couchbase.com|website=Couchbase}}</ref> [[Druid (open-source data store)]],<ref>{{cite web |url=http://druid.io |title=Druid distributed datastore/database |publisher=The Druid Community}}</ref> [[FoundationDB]],<ref>
{{cite web |url=https://foundationdb.com |title=FoundationDB database |publisher=FoundationDB}}</ref> [[NuoDB]],<ref>Clark, Jack. [http://www.theregister.co.uk/2014/02/26/nuodb_funding/ "NuoDB slurps European cash for database expansion"] The Register. Feb. 26, 2014</ref> [[Riak]]<ref>
{{cite web |url=http://www.basho.com |title=Basho Riak Distributed database |publisher=Basho}}</ref> and [[OrientDB]].<ref>
{{cite web |url=http://www.orientdb.com |title=OrientDB database |publisher=OrientDB}}</ref> The [[Blockchain (database)|blockchain]] technology popularised by [[bitcoin]] is an implementation of a distributed database.<ref>{{cite news|last1=Margaret|first1=Alyson|title=How Bitcoin and the blockchain are a transformative technology |url=http://blog.blockchain.com/2015/06/23/how-bitcoin-and-the-block-chain-are-a-transformative-technology/|accessdate=23 July 2015|date=23 June 2015}}</ref>

== Architecture ==
A database user accesses the distributed database through:
;Local applications
:applications which do not require data from other sites.
;Global applications
:applications which do require data from other sites.

A '''homogeneous distributed database''' has identical software and hardware running all databases instances, and may appear through a single interface as if it were a single database.  A '''heterogeneous distributed database''' may have different hardware, operating systems, database management systems, and even data models for different databases.

===Homogeneous Distributed Databases Management System===
In homogeneous distributed database, all sites have identical software and are aware of each other and agree to cooperate in processing user requests. Each site surrenders part of its autonomy in terms of right to change schema or software. A homogeneous DBMS appears to the user as a single system. The homogeneous system is much easier to design and manage. The following conditions must be satisfied for homogeneous database:
*The operating system used at each location must be same or compatible.{{According to whom|date=March 2013}}{{Elucidate|date=March 2013}}
*The data structures used at each location must be same or compatible.
*The database application (or DBMS) used at each location must be same or compatible.

===Heterogeneous DDBMS===
{{See also|Heterogeneous database system}}
In a heterogeneous distributed database, different sites may use different schema and software. Difference in schema is a major problem for query processing and transaction processing. Sites may not be aware of each other and may provide only limited facilities for cooperation in transaction processing. In heterogeneous systems, different nodes may have different hardware & software and data structures at various nodes or locations are also incompatible. Different computers and operating systems, database applications or data models may be used at each of the locations. For example, one location may have the latest relational database management technology, while another location may store data using conventional files or old version of database management system. Similarly, one location may have the Windows 10 operating system, while another may have UNIX. Heterogeneous systems are usually used when individual sites use their own hardware and software. On heterogeneous system, translations are required to allow communication between different sites (or DBMS). In this system, the users must be able to make requests in a database language at their local sites. Usually the SQL database language is used for this purpose. If the hardware is different, then the translation is straightforward, in which computer codes and word-length is changed. The heterogeneous system is often not technically or economically feasible. In this system, a user at one location may be able to read but not update the data at another location.

== Important considerations ==
Care with a distributed database must be taken to ensure the following:
* The distribution is transparent — users must be able to interact with the system as if it were one logical system.  This applies to the system's performance, and methods of access among other things.
* [[Database transaction|Transaction]]s are transparent — each transaction must maintain [[database integrity]] across multiple databases.  Transactions must also be divided into sub-transactions, each sub-transaction affecting one database system.

There are two principal approaches to store a relation r in a distributed database system:

:A) [[database replication|Replication]]
:B) Fragmentation/[[Partition (database)|Partitioning]]

A) Replication: In replication, the system maintains several identical replicas of the same relation r in different sites.
:* Data is more available in this scheme.
:* Parallelism is increased when read request is served.
:* Increases overhead on update operations as each site containing the replica needed to be updated in order to maintain consistency.
:* Multi-datacenter replication provides geographical diversity, like in [[Clusterpoint]]<ref>
{{cite web |url=http://www.clusterpoint.com/solutions/distributed-storage |title=Clusterpoint database distributed storage multi-datacenter replication|publisher=Clusterpoint}}</ref> or [[Riak]].<ref>
{{cite web |url=http://basho.com/tag/multi-datacenter-replication/ |title=Riak database multi-datacenter replication|publisher=Basho}}</ref>

B) Fragmentation: The relation r is fragmented into several relations r<sub>1</sub>, r<sub>2</sub>, r<sub>3</sub>....r<sub>n</sub> in such a way that the actual relation could be reconstructed from the fragments and then the fragments are scattered to different locations. There are basically two schemes of fragmentation:

:* Horizontal fragmentation - splits the relation by assigning each tuple of r to one or more fragments.
:* Vertical fragmentation - splits the relation by decomposing the schema R of relation r.

A distributed database can be run by independent or even competing parties as, for example, in [[bitcoin]] or [[Hasq]].

== Advantages ==
* Management of distributed data with different levels of transparency like network transparency, fragmentation transparency, replication transparency, etc.
* Increase reliability and availability
* Easier expansion
* Reflects organizational structure — database fragments potentially stored within the departments they relate to
* Local autonomy or site autonomy — a department can control the data about them (as they are the ones familiar with it)
* Protection of valuable data — if there were ever a catastrophic event such as a fire, all of the data would not be in one place, but distributed in multiple locations
* Improved performance — data is located near the site of greatest demand, and the database systems themselves are parallelized, allowing load on the databases to be balanced among servers.  (A high load on one module of the database won't affect other modules of the database in a distributed database)
* Economics — it may cost less to create a network of smaller computers with the power of a single large computer
* Modularity — systems can be modified, added and removed from the distributed database without affecting other modules (systems)
* Reliable transactions - due to replication of the database
* Hardware, operating-system, network, fragmentation, DBMS, replication and location independence
* Continuous operation, even if some nodes go offline (depending on design)
* Distributed query processing can improve performance
* Single-site failure does not affect performance of system.
* For those systems that support full distributed transactions, operations enjoy the [[ACID]] properties:
** A-atomicity, the transaction takes place as a whole or not at all
** C-consistency, maps one consistent DB state to another
** I-isolation, each transaction sees a consistent DB
** D-durability, the results of a transaction must survive system failures

The Merge Replication Method is popularly used to consolidate the data between databases.{{citation needed|date=July 2013}}

== Disadvantages ==
* Complexity — [[Database administrator|DBAs]] may have to do extra work to ensure that the distributed nature of the system is transparent.  Extra work must also be done to maintain multiple [[disparate system]]s, instead of one big one.  Extra database design work must also be done to account for the disconnected nature of the database — for example, joins become prohibitively expensive when performed across multiple systems.
* Economics — increased complexity and a more extensive infrastructure means extra labour costs
* Security — remote database fragments must be secured, and they are not centralized so the remote sites must be secured as well.  The infrastructure must also be secured (for example, by encrypting the network links between remote sites).
* Difficult to maintain integrity — but in a distributed database, enforcing integrity over a network may require too much of the network's resources to be feasible
* Inexperience — distributed databases are difficult to work with, and in such a young field there is not much readily available experience in "proper" practice
* Lack of standards — there are no tools or methodologies yet to help users convert a centralized DBMS into a distributed DBMS{{citation needed|date=July 2013}}
* Database design more complex — In addition to traditional database design challenges, the design of a distributed database has to consider fragmentation of data, allocation of fragments to specific sites and data replication
* Additional software is required
* Operating system should support distributed environment
* [[Concurrency control]] poses a major issue. It can be solved by [[Lock (database)|locking]] and [[timestamp]]ing.
* Distributed access to data
* Analysis of distributed data

==See also==
*[[Centralized database]]
*[[Data grid]]
*[[Distributed data store]]
*[[Distributed cache]]
*[[Routing protocol]]
*[[Distributed hash table]]

==References==
{{Reflist|30em}}
{{more footnotes|date=April 2013}}
*M. T. Özsu and P. Valduriez, ''Principles of Distributed Databases'' (3rd edition) (2011), Springer, ISBN 978-1-4419-8833-1
*Elmasri and Navathe, ''Fundamentals of database systems'' (3rd edition), Addison-Wesley Longman, ISBN 0-201-54263-3
*''Oracle Database Administrator's Guide 10g'' (Release 1), http://docs.oracle.com/cd/B14117_01/server.101/b10739/ds_concepts.htm

{{Databases}}

[[Category:Data management]]
[[Category:Types of databases]]
[[Category:Distributed computing architecture]]
[[Category:Applications of distributed computing]]
[[Category:Database management systems]]
<=====doc_Id=====>:41
<=====title=====>:
Enterprise information integration
<=====text=====>:
{{multiple issues|{{refimprove|date=February 2015}}
{{POV|date=February 2011}}}}
'''Enterprise information integration''' ('''EII''') is the ability to support a unified view of data and information for an entire organization.  In a [[data virtualization]] application of EII, a process of [[information integration]], using [[data abstraction]] to provide a unified interface (known as [[uniform data access]]) for viewing all the data within an organization, and a single set of structures and naming conventions (known as [[uniform information representation]]) to represent this data; the goal of EII is to get a large set of [[heterogeneous]] data sources to appear to a user or system as a single, homogeneous data source.

== Overview ==
[[Data]] within an [[Enterprise architecture|enterprise]] can be stored in heterogeneous formats, including [[relational database]]s (which themselves come in a large number of varieties), text files, [[XML]] files, [[spreadsheet]]s and a variety of proprietary [[data storage device|storage]] methods, each with their own [[index (database)|index]]ing and [[data access]] methods.

Standardized data access [[application programming interface|API]]s have emerged, that offer a specific set of commands to retrieve and modify data from a generic data source. Many applications exist that implement these APIs' commands across various data sources, most notably relational databases. Such APIs include [[ODBC]], [[JDBC]], [[XQJ]], [[OLE DB]], and more recently [[ADO.NET]].

There are also standard formats for representing data within a file, that are very important to information integration. The best-known of these is XML, which has emerged as a standard universal representation format. There are also more specific XML "grammars" defined for specific types of data, such as [[Geography Markup Language]] for expressing geographical features, and [[Directory Service Markup Language]], for holding directory-style information. In addition, non-XML standard formats exist, such as [[iCalendar]], for representing calendar information, and [[vCard]], for [[business card]] information.

Enterprise Information Integration (EII) applies [[data integration]] commercially.  Despite the theoretical problems described above, the private sector shows more concern with the problems of data integration as a viable product.<ref name="refthree">{{cite conference | author=Alon Y. Halevy | title=Enterprise information integration: successes, challenges and controversies | booktitle=SIGMOD 2005 | year=2005 | pages=778–787 | url=http://www.cs.washington.edu/homes/alon/files/eiisigmod05.pdf|display-authors=etal}}</ref>
EII emphasizes neither on correctness nor tractability, but speed and simplicity. An EII industry has emerged, but many professionals{{Who|date=June 2009}} believe it does not perform to its full potential.  Practitioners cite the following major issues which  EII must address for the industry to become mature:{{Citation needed|date=June 2009}}

; Combining disparate data sets : Each data source is disparate and as such is not designed to support EII.  Therefore, data virtualization as well as [[Federated database system|data federation]] depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.

:  One solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.
; Simplicity of understanding : Answering queries with views arouses interest from a theoretical standpoint, but difficulties in understanding how to incorporate it as an "enterprise solution".{{Citation needed|date=June 2009}}  Some developers{{Who|date=June 2009}} believe it should be merged with [[Enterprise application integration|EAI]].  Others{{Who|date=June 2009}} believe it should be incorporated with ETL systems, citing customers' confusion over the differences between the two services.{{Citation needed|date=June 2009}}
; Simplicity of deployment : Even if recognized as a solution to a problem, EII {{as of | 2009 | lc = on}} currently takes time to apply and offers complexities in deployment.  People have proposed a variety of schema-less solutions such as "Lean Middleware",<ref name="reffour">{{cite conference | author=David A. Maluf | title=Lean middleware | booktitle=SIGMOD 2005 | year=2005 | pages=788–791 | url=http://portal.acm.org/citation.cfm?id=1066157.1066247&coll=portal&dl=ACM&type=series&idx=1066157&part=Proceedings&WantType=Proceedings&title=International%20Conference%20on%20Management%20of%20Data&CFID=15151515&CFTOKEN=6184618|display-authors=etal}}</ref> but ease-of-use and speed of employment appear inversely proportional to the generality of such systems.{{Citation needed|date=June 2009}}  Others{{Who|date=June 2009}} cite the need for standard data interfaces to speed and simplify the integration process in practice.
; Handling higher-order information : Analysts experience difficulty—even with a functioning information integration system—in determining whether the sources in the database will  satisfy a given application.  Answering these kinds of questions about a set of repositories requires semantic information like [[metadata]] and/or ontologies.  The few commercial tools{{Which|date=June 2009}} that leverage this information remain in their infancy.

== Applications ==
EII products enable [[loose coupling]] between [[wiktionary:Homogeneous|homogeneous]]-data consuming client applications and services and heterogeneous-data stores.  Such client applications and services include Desktop Productivity Tools (spreadsheets, [[word processor]]s, presentation software, etc.), [[Integrated development environment|development environment]]s and [[Software framework|framework]]s ([[Java EE]], [[Microsoft .NET|.NET]], [[Mono (software)|Mono]], [[SOAP]] or [[Representational State Transfer|REST]]ful [[Web service]]s, etc.), [[business intelligence]] (BI), [[business activity monitoring]] (BAM) software, [[enterprise resource planning]] (ERP), [[Customer relationship management]] (CRM), [[business process management]] (BPM and/or BPEL) Software, and [[web content management]] (CMS).

==  Example technology vendors ==
*[[Capsenta]]
*[[Composite Software]]
*[[Denodo]]
*[[MetaMatrix]]
*[[XAware]]

== Data access technologies ==
*[[XQuery]] and [[XQuery API for Java]]
*[[Service Data Objects]] (SDO) for Java, C++ and .Net clients and any type of data source

== See also ==
{{div col|3}}
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Data warehouse]]
* [[Disparate system]]
* [[Enterprise integration]]
* [[Federated database system]]
* [[Resource Description Framework]]
* [[Semantic heterogeneity]]
* [[Semantic integration]]
* [[Semantic Web]]
* [[Web 2.0]]
* [[Web services]]
{{div col end}}

==References==

<references/>

[[Category:Data management]]
<=====doc_Id=====>:44
<=====title=====>:
IMS VDEX
<=====text=====>:
{{Multiple issues|
{{refimprove|date=December 2010}}
{{primary sources|date=December 2010}}
{{notability|date=December 2010}}
{{context|date=December 2010}}
}}

'''IMS VDEX''', which stands for '''IMS Vocabulary Definition Exchange''', is a mark-up language – or grammar – for [[Controlled vocabulary|controlled vocabularies]] developed by IMS Global as an open specification, with the Final Specification being approved in February 2004.

IMS VDEX allows the exchange and expression of simple machine-readable lists of human language terms, along with information that may assist a human in understanding the meaning of the various terms, i.e. a flat list of values, a hierarchical tree of values, a thesaurus, a taxonomy, a glossary or a dictionary.

Structural a vocabulary has an identifier, title and a list of terms. Each term has a unique key, titles and (optional) descriptions. A term may have nested terms, thus a hierarchical structure can be created. It is possible to define relationships between terms and add custom metadata to terms.

IMS VDEX support multilinguality. All values supposed to be read by a human, i.e. titles, can be defined in one or more languages.

== Purposes ==
VDEX was designed to supplement other IMS specifications and the IEEE LOM standard by giving additional semantic control to tool developers. IMS VDEX could be used for the following purposes. It is used in practice for other purposes as well.

* ''Interfaces providing pre-defined choices'' – providing radio buttons and drop-down menus for interfaces such as metadata editors or a repository browse tool, based on the vocabulary allowed in the metadata profile used
* ''Distributing vocabularies among many users'' – achieved by simple XML file sharing, or possibly a searchable [[Repository Open Service Interface Definition|repository]] or registry of vocabularies
* ''XML stylesheets used to select and generate different views'' – selecting an overview of an entire vocabulary as an [[HTML]] or [[PDF]] file, for example; providing scope notes for catalogues; or storing a glossary of terms which are called upon by hyperlinks within a document
* ''Validation of metadata instances'' – validated against an application profile, by comparison of the vocabulary terms used in certain metadata elements with those of the machine readable version of the vocabularies specified by the application profile.
* ''Controlled terms for other IMS specifications and IEEE LOM'' – both may contain elements where controlled terms should be used. These elements are often specified as being of a vocabulary data type, and a definition of the permitted terms and their usage may be expressed using VDEX.

== Technical details ==
[[Image:VDEX model.PNG|350px|right|thumb|simplified VDEX data model]]
The VDEX Information Model is represented in the diagram. A VDEX file describing a vocabulary comprises a number of information elements, most of which are relatively simple, such as a string representation of the default (human) language or a [[URI]] identifying the value domain (or vocabulary). Some of the elements are ‘containers’ – such as a ''term'' – that contain additional elements.

Elements may be required or optional, and in some cases, repeatable. Within a term, for example, a ''description'' and ''caption'' may be defined. Multiple language definitions can be used inside a description, by using a ''langstring'' element, where the description is paired with the language to be used. Additional elements within a term include ''media descriptors'', which are one or more media files to supplement a term’s description; and ''metadata'', which is used to describe the vocabulary further.

The ''relationship'' container defines a relationship between terms by identifying the two terms and the specifying type or relationship, such as a term being broader or narrower than another. The term used to specify the type of relationship may conform to the ISO standards for thesauri.

''Vocabulary identifiers'' are unique, persistent URIs, whereas term or relationship identifiers are locally unique strings. VDEX also allows for a ''default language'' and ''vocabulary name'' to be given, and for whether the ordering of terms within the vocabulary is significant (''order significance'') to be specified.

A ''profile type'' is specified to describe the type of vocabulary being expressed; different features of the VDEX model are permitted depending on the profile type, providing a common grammar for several classes of vocabulary. For example, it is possible, in some profile types, for terms to be contained within one another and be nested, which is suited to the expression of hierarchical vocabularies. Five profile types exist: ''lax'', ''thesaurus'', ''hierarchicalTokenTerms'', ‘glossaryOrDictionary’ and ''flatTokenTerms''. The lax profile is the least restrictive and offers the full VDEX model, whereas the flatTokenTerms profile is the most restrictive and lightweight.

VDEX also offers some scope for complex vocabularies, assuming the existence of a well-defined application profile (for exchange interoperability). Some examples are:
* ''Faceted schemes'' – faceted vocabularies are possible with the definition of appropriate relationships
* ''Multi-lingual thesauri'' – metadata could be used within a relationship to achieve multilingual thesauri
* ''Polyhierarchical taxonomies'' – can be expressed using the source/target value pairs in the relationship.

Identifiers in VDEX data should be persistent, unique, resolvable, transportable and URI-compliant. Specifically, vocabulary identifiers should be unique URIs, whereas term and relationship identifiers should be locally unique strings.

== Implementations ==
* [http://aloha2.netera.ca/ ALOHA Metadata Tagging Tool] — Java-based software project that can read IMS VDEX files.
* [http://www.ivimeds.org/news/demonstrator.html IVIMEDS 1G v1.0] – from The International Virtual Medical School – includes VDEX instances in curriculum maps. Partners can create their own maps in VDEX format and use these to help students search the repository.
* [http://www.elframework.org/projects/spws/view Skills Profiling Web Service] — project implemented and demonstrated use of a skills profiling web service using open standards in a medical context. IMS VDEX files were used in the representation of the SPWS hierarchy skills framework.
* [http://www.scottishdoctor.org/ Scottish Doctors] — project used VDEX as a format for expressing curricular outcome systems.
* [http://prs.heacademy.ac.uk/technical/vdex_scripts.html VDEX XSLT scripts] — developed by The Higher Education Academy Centre for Philosophical and Religious Studies to convert VDEX to XHTML and PostgreSQL .
* [http://www.icbl.hw.ac.uk/vdex VDEX Implementation Project] — carried out by the Institute for Computer Based Learning at Heriot-Watt University, with a primary objective of creating a tool for editing vocabularies in VDEX format. The project, which ended in January 2004, was based on the Public Draft (not the current Final Specification).
* [http://sourceforge.net/projects/vdex-j/ VDEX Java Binding] — implementation neutral Java interface for VDEX, as well as providing a default implementation of that interface, and XML marshalling functionality.
* [https://pypi.python.org/pypi/imsvdex imsvdex Python egg] —  API for VDEX XML-files. It is free software written in [[Python (programming language)|Python]].
* [http://plone.org/products/atvocabularymanager ATVocabularyManager] — addon for [[Plone]] CMS uses VDEX as a possible format to define vocabularies.
* [https://pypi.python.org/pypi/collective.vdexvocabulary collective.vdexvocabulary] — implements IMS VDEX as standard [[Zope]] vocabulary which can also be used in [[Plone]] CMS, written in [[Python (programming language)|Python]].
* [https://pypi.python.org/pypi/vdexcsv/ vdexcsv] — offers a commandline converter from [[Comma-separated values|CSV]] to VDEX. It is written in [[Python (programming language)|Python]].

== See also ==
*IMS Global
*[[Learning object metadata]]

==References==
#{{note|coillie}} Marc van Coillie [http://www.eife-l.org/publications/standards/interop/europasscv/europassCV-IMS-AP/usingvdex Using IMS VDEX for the EDS AP - EIfEL]
#{{note|sarasa}} Antonio Sarasa, Jose Manuel Canabal, Juan Carlos Sacristan, Raquel Jimenez [http://online-journals.org/i-jet/article/view/806 Using IMS VDEX in Agrega]

== External links ==
* [http://www.imsglobal.org/vdex IMS VDEX] — official resources by IMS global
* [http://wiki.cetis.ac.uk/What_is_IMS_VDEX What is IMS VDEX] — JISC CETIS
* [http://metadata.cetis.ac.uk/ CETIS Metadata and Digital Repository Special Interest Group (SIG)] — mailing list for those in UK Higher and Further Education  interested in creating, storing and serving educational metadata.

[[Category:Data management]]
[[Category:Educational technology standards]]
[[Category:Knowledge representation]]
[[Category:Library science]]
[[Category:Metadata]]
[[Category:Standards]]
[[Category:Standards organizations]]
[[Category:Technical communication]]
<=====doc_Id=====>:47
<=====title=====>:
Content format
<=====text=====>:
[[File:Pcm.svg|200px|thumb|right|Graphical representations of electrical data: analog audio content format (red), 4-bit digital pulse code modulated content format (blue).]][[File:Mi Fu-On Calligraphy.jpg|200px|thumb|right|Chinese calligraphy written in a language content format by [[Song Dynasty]] (A.D. 1051-1108) poet [[Mi Fu]].]][[File:12345678901-2-23456 barcode UPC(A).svg|200px|thumb|A series of numbers encoded in a Universal Product Code digital numeric content format.]]A '''content format''' is an [[encoding|encoded]] format for converting a specific type of [[data]] to displayable [[information]]. [[Content (media and publishing)|Content]] formats are used in [[recording]] and [[Telecommunication|transmission]] to prepare data for [[Information processing|observation]] or [[interpreting|interpretation]].<ref>Bob Boiko, ''Content Management Bible,'' Nov 2004 pp:79, 240, 830</ref><ref>[[Ann Rockley]], ''Managing Enterprise Content: A Unified Content Strategy,'' Oct 2002 pp:269, 320, 516</ref> This includes both [[Analog signal|analog]] and [[digitizing|digitized]] content. Content formats may be recorded and read by either natural or manufactured tools and mechanisms.

In addition to converting data to information, a content format may include the [[encryption]] and/or [[Scrambler|scrambling]] of that information.<ref>Jessica Keyes, ''Technology Trendlines,'' Jul  1995 pp:201</ref> Multiple content formats may be contained within a single section of a [[storage medium]] (e.g. [[Multitrack recording|track]], [[disk sector]], [[computer file]], [[document]], [[page (paper)|page]], [[Column (typography)|column]]) or transmitted via a single [[Channel (communications)|channel]] (e.g. [[wire]], [[carrier wave]]) of a [[transmission medium]]. With [[multimedia]], multiple tracks containing multiple content formats are presented simultaneously. Content formats may either be recorded in secondary signal processing methods such as a software container format (e.g. [[digital audio]], [[digital video]]) or recorded in the primary format (e.g. [[spectrogram]], [[pictogram]]). 

Observable data is often known as [[raw data]], or raw content.<ref>Oge Marques and Borko Furht, ''Content-Based Image and Video Retrieval,'' April 2002 pp:15</ref> A primary raw content format may be directly [[information processing|observable]] (e.g. [[image]], [[sound]], [[Motion (physics)|motion]], [[Odor|smell]], [[Haptic perception|sensation]]) or [[physics|physical]] data which only requires hardware to display it, such as a [[phonograph]]ic [[Gramophone needle|needle]] and [[diaphragm (acoustics)|diaphragm]] or a [[Image projector|projector]] [[List of light sources|lamp]] and [[magnifying glass]].

There has been a countless number of content formats throughout history. The following are examples of some common content formats and content format categories (covering: sensory experience, model, and language used for encoding information):
{| width="65%"
|- valign=top
|width="50%"|
*Audio data encoding<ref>David Austerberry, ''The Technology of Video and Audio Streaming, Second Edition,'' Sep 2004 pp: 328</ref>
**[[Audio coding format]]
**[[Analog signal|Analog]] [[Audio frequency|audio]] data
**[[Stereophonic sound]] formats
**[[Digital audio]] data
**[[Synthesizer]] [[Music sequencer|sequences]]
*Visual data encoding
**[[Art techniques and materials|Hand rendering materials]]
**[[Film speed]] formats
**[[Pixel]] [[coordinates]] data
**[[Color space]] data
**[[Vector graphics|Vector graphic]] [[coordinates]]/[[dimensions]]
**[[Texture mapping]] formats
**[[3D display]] formats
**[[Holographic]] formats
**[[Display resolution]] formatting
*[[Motion graphics]] encoding
**[[Video coding format]]
**[[Frame rate]] data
**[[Video]] data<ref>M. Ghanbari, ''Standard Codecs: Image Compression to Advanced Video Coding,'' Jun 2003 pp:364</ref>
**[[Computer animation]] formats
*Instruction encoding
**[[Musical notation]]
**[[Computer language]]
**[[Traffic signals]]
|width="50%"|
*[[Language|Natural languages formats]]
**[[Writing system]]s
**[[Phonetic]]
**[[Sign language]]s
*[[Signal (electronics)|Communication signaling formats]]
*[[Code]] formats
*Expert language formats
**[[Graphic organizer]]
**[[Statistical model]]
**[[Table of elements#Standard periodic table|Table of elements]]
**[[DNA sequence]]
**[[Human anatomy]]
**[[Biometrics|Biometric data]]
**[[Chemical formula]]s
**[[Aroma compound]]
**[[Psychoactive drug#Psychoactive drug chart|Drug chart]]
**[[Electromagnetic spectrum]]
**[[Time standard]]
**[[Numerical weather prediction]]
**[[Capital asset pricing model]]
**[[Measures of national income and output|National income and output]]
**[[Celestial coordinate system]]
**[[APP-6a|Military mapping]]
**[[Geographic information system]]
**[[Interstate Highway System]]
|}

==See also==
*[[Communication]]
*[[Representation (arts)]]
*[[Modulation|Content carrier signals]]
*[[Multiplexing|Content multiplexing format]]
*[[Transmission (telecommunications)|Content transmission]]
*[[Wireless|Wireless content transmission]]
*Data storage device
*[[Recording format]]
*[[Encoder]]
*[[Analog television]]: [[NTSC]], [[PAL]] and [[SECAM]]
*[[Information mapping]]

==References==
{{reflist}}

[[Category:Communication]]
[[Category:Media technology]]
[[Category:Data management]]
[[Category:Recording]]
[[Category:Film and video technology]]
[[Category:Sound production technology]]


{{library-stub}}
<=====doc_Id=====>:50
<=====title=====>:
Content management
<=====text=====>:
{{distinguish|Information management|Knowledge management}}
{{Refimprove|date=July 2007}}

{{Business administration}}
'''Content management''' ('''CM'''), is a set of processes and technologies that supports the collection, managing, and publishing of information in any form or medium.  When stored and accessed via computers, this information may be more specifically referred to as [[digital content]], or simply as [[Content (media and publishing)|content]].  Digital content may take the form of text (such as [[electronic document]]s), multimedia files (such as audio or video files), or any other file type that follows a content lifecycle requiring [[Product lifecycle management|management]]. The process is complex enough to manage that several large and small commercial software vendors such as [[Interwoven]] and [[Microsoft]] offer [[Content management system|content management software]] to control and automate significant aspects of the content lifecycle.

==The process of content management==
Content management practices and goals vary by mission and by organizational governance structure.
News organizations, [[e-commerce]] websites, and educational institutions all use content management, but in different ways. This leads to differences in terminology and in the names and number of steps in the process.

For example, some digital content is created by one or more authors. Over time that content may be edited. One or more individuals may provide some editorial oversight, approving the content for publication. Publishing may take many forms: it may be the act of "pushing" content out to others, or simply granting digital access rights to certain content to one or more individuals.  Later that content may be superseded by another version of the content and thus retired or removed from use (as when this wiki page is modified).

Content management is an inherently collaborative process. It often consists of the following basic roles and responsibilities:

* '''Creator''' – responsible for creating and editing content.
* '''Editor''' – responsible for tuning the content message and the style of delivery, including translation and localization.
* '''Publisher''' – responsible for releasing the content for use.
* '''Administrator''' – responsible for managing access permissions to folders and files, usually accomplished by assigning access rights to user groups or roles. Admins may also assist and support users in various ways.
* '''Consumer, viewer or guest''' – the person who reads or otherwise takes in content after it is published or shared.

A critical aspect of content management is the ability to manage versions of content as it evolves (''see also'' [[version control]]). Authors and editors often need to restore older versions of edited products due to a process failure or an undesirable series of edits.

Another equally important aspect of content management involves the creation, maintenance, and application of review standards. Each member of the content creation and review process has a unique role and set of responsibilities in the development or publication of the content. Each review team member requires clear and concise review standards. These must be maintained on an ongoing basis to ensure the long-term consistency and health of the [[knowledge base]].

A content management system is a set of automated processes that may support the following features:

* Import and creation of documents and multimedia material
* Identification of all key users and their roles
* The ability to assign roles and responsibilities to different instances of content categories or types
* Definition of workflow tasks often coupled with messaging so that content managers are alerted to changes in content
* The ability to track and manage multiple versions of a single instance of content
* The ability to publish the content to a repository to support access
* The ability to personalize content based on a set of rules
* 
Increasingly, the repository is an inherent part of the system, and incorporates [[enterprise search]] and retrieval. Content management systems take the following forms:

* [[Web content management system]]—[[software]] for [[web site]] management (often what ''content management'' implicitly means)
* Output of a [[newspaper]] editorial staff organization
* [[Workflow]] for [[essay|article]] publication
* [[Document management system]]
* [[Single source publishing|Single source]] content management system—content stored in chunks within a relational database
* Variant management system—where personnel tag source content (usually text and graphics) to represent variants stored as single source "master" content modules, resolved to the desired variant at publication (for example: automobile owners manual content for 12 model years stored as single master content files and "called" by model year as needed)—often used in concert with database chunk storage (see above) for large content objects

==Governance structures==
Content management expert Marc Feldman defines three primary content management governance structures: localized, centralized, and federated—each having its unique strengths and weaknesses.<ref>http://www.clickz.com/clickz/column/1715089/governance-issues-content-management</ref>

===Localized governance===
By putting control in the hands of those closest to the content, the context experts, localized governance models empower and unleash creativity.  These benefits come, however, at the cost of a partial-to-total loss of managerial control and oversight.

===Centralized governance===
When the levers of control are strongly centralized, content management systems are capable of delivering an exceptionally clear and unified brand message.  Moreover, centralized content management governance structures allow for a large number of cost-savings opportunities in large enterprises, realized, for example, (1) the avoidance of duplicated efforts in creating, editing, formatting, repurposing and archiving content, (2) through process management and the streamlining of all content related labor, and/or (3) through an orderly deployment or updating of the content management system.

===Federated governance===
Federated governance models potentially realize the benefits of both localized and centralized control while avoiding the weaknesses of both. While content management software systems are inherently structured to enable federated governance models, realizing these benefits can be difficult because it requires, for example, negotiating the boundaries of control with local managers and content creators.  In the case of larger enterprises, in particular, the failure to fully implement or realize a federated governance structure equates to a failure to realize the full return on investment and cost savings that content management systems enable.

==Implementation==
Content management implementations must be able to manage content distributions and digital rights in content life cycle.<ref>{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture & Television Engineers |volume=113 |issue=4 |pages=110–120 |doi= |access-date=July 1, 2016}}</ref><ref>{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}</ref><ref>{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution & DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}</ref><ref>{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}</ref> Content management systems are usually involved with [[digital rights management]] in order to control user access and digital rights. In this step, the read-only structures of [[digital rights management]] systems force some limitations on content management, as they do not allow authors to change protected content in their life cycle. Creating new content using managed (protected) content is also an issue that gets protected contents out of management controlling systems. A few content management implementations cover all these issues.<ref>{{cite journal |last=White |first=Blake |date=April 2004 |title=A New Era for Content: Protection, Potential, and Profit in the Digital World |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7262562 |journal=SMPTE Motion Imaging Journal |publisher=Society of Motion Picture & Television Engineers |volume=113 |issue=4 |pages=110–120 |doi= |access-date=July 1, 2016}}</ref><ref>{{cite web | url=http://www.giantstepsmts.com/CM-DRMwhitepaper.pdf  | title=Integrating Content Management with Digital Rights Management   | publisher=GiantSteps  | accessdate=2016-07-01}}</ref><ref>{{cite web | url=http://www.locklizard.com/content-distribution-drm/  | title=Content Distribution & DRM - Managing Content Distribution with DRM | publisher=Locklizard.com  | accessdate=2016-07-01}}</ref><ref>{{cite book  | title= The World Beyond Digital Rights Management  |url=https://www.amazon.com/World-Beyond-Digital-Rights-Management-ebook/dp/B004GEAEZM  | location = |last=Umeh   |first =Jude | date =October 2007  | publisher = British Computer Society  |page=320 |isbn= 978-1902505879 }}</ref>

==See also==
{{Div col|colwidth=22em}}
* [[Content delivery]]
* [[Content engineering]]
* [[Content Management Interoperability Services]]
* [[Content management system]]
* [[Digital asset management]]
* [[Enterprise content management]]
* [[Enterprise information management]]
* [[Information architecture]]
* [[List of content management systems]]
* [[Single source publishing]]
* [[Snippet management]]
* [[Web content lifecycle]]
* [[Web design]]
* [[Website architecture]]
* [[Website governance]]
{{Div col end}}

==References==
{{reflist|colwidth=30em}}

==External links==
* {{cite book | last = Boiko | first = Bob | authorlink = | title = Content Management Bible | publisher = Wiley | date = 2004-11-26 | location = | pages = 1176 | url = | doi = | id = | isbn = 0-7645-7371-3 }}
* {{cite book | last = Rockley | first = Ann | authorlink = | title = Managing Enterprise Content: A Unified Content Strategy | publisher = New Riders Press | date = 2002-10-27 | location = | pages = 592 | url = | doi = | id = | isbn = 0-7357-1306-5 }}
* {{cite book | last = Hackos | first = JoAnn T. | authorlink = | title = Content Management for Dynamic Web Delivery | publisher = Wiley | date = 2002-02-14 | location = | pages = 432 | url = | doi = | id = | isbn = 0-471-08586-3 }}
* {{cite book | last = Glushko| first = Robert J. | authorlink = |author2=Tim McGrath| title = Document Engineering: Analyzing and Designing Documents for Business Informatics and Web Services | publisher = MIT Press| year = 2005 | location = | pages = 728| url = | doi = | id = | isbn = 0-262-57245-1 }}
* {{cite book | last = Ferran | first = Núria | authorlink = | author2=Julià Minguillón | title = Content Management for E-Learning | publisher = Springer | date = 2011 | location = | pages = 215 | url = http://www.springer.com/us/book/9781441969583 | doi = | id = | isbn = 978-1-4419-6958-3 }}

{{Content management systems}}
{{WebManTools}}

[[Category:Technical communication]]
[[Category:Data management]]
[[Category:Content management systems]]

[[fr:Système de gestion de contenu]]
<=====doc_Id=====>:53
<=====title=====>:
Distributed data store
<=====text=====>:
{{Essay-like|date=May 2012}}

A '''distributed data store''' is a [[computer network]] where information is stored on more than one [[Node (networking)|node]], often in a [[Replication (computing)|replicated]] fashion.<ref>{{Citation
|author = Yaniv Pessach
|url = http://openlibrary.org/books/OL25423189M/Distributed_Storage_Concepts_Algorithms_and_Implementations
|title = Distributed Storage
|edition = Distributed Storage: Concepts, Algorithms, and Implementations
}}</ref> It is usually specifically used to refer to either a [[distributed database]] where users store information on a ''number of nodes'', or a [[computer network]] in which users store information on a ''number of peer network nodes''.

==Distributed databases==
[[Distributed database]]s are usually [[non-relational database]]s that make a quick access to data over a large number of nodes possible. Some distributed databases expose rich query abilities while others are limited to a [[key-value store]] semantics. Examples of limited distributed databases are [[Google]]'s [[BigTable]], which is much more than a [[distributed file system]] or a [[peer-to-peer network]],<ref>{{cite web
| accessdate = 2011-04-05
| location = http://the-paper-trail.org/
| publisher = Paper Trail
| title = BigTable: Google's Distributed Data Store
| quote = Although GFS provides Google with reliable, scalable distributed file storage, it does not provide any facility for structuring the data contained in the files beyond a hierarchical directory structure and meaningful file names. It’s well known that more expressive solutions are required for large data sets. Google’s terabytes upon terabytes of data that they retrieve from web crawlers, amongst many other sources, need organising, so that client applications can quickly perform lookups and updates at a finer granularity than the file level. [...] The very first thing you need to know about BigTable is that it isn’t a relational database. This should come as no surprise: one persistent theme through all of these large scale distributed data store papers is that RDBMSs are hard to do with good performance. There is no hard, fixed schema in a BigTable, no referential integrity between tables (so no foreign keys) and therefore little support for optimised joins.
| url = http://the-paper-trail.org/blog/?p=86}}</ref> [[Amazon.com|Amazon]]'s [[Dynamo (storage system)|Dynamo]]<ref>{{cite web
| accessdate = 2011-04-05
| author = Sarah Pidcock
| date = 2011-01-31
| location = http://www.cs.uwaterloo.ca/
| page = 2/22
| publisher = WATERLOO – CHERITON SCHOOL OF COMPUTER SCIENCE
| title = Dynamo: Amazon’s Highly Available Key-value Store
| quote = Dynamo: a highly available and scalable distributed data store
| url = http://www.cs.uwaterloo.ca/~kdaudjee/courses/cs848/slides/sarah1.pdf}}</ref>
and [[Azure Services Platform|Windows Azure Storage]].<ref>{{cite web 
| url= http://www.microsoft.com/windowsazure/features/storage/ |title= Windows Azure Storage |author= |date=2011-09-16 |work= |publisher= |accessdate=6 November 2011}}</ref>

As the ability of arbitrary querying is not as important as the [[availability]], designers of distributed data stores have increased the latter at an expense of consistency. But the high-speed read/write access results in reduced consistency, as it is not possible to have both [[Consistency (database systems)|consistency]], availability, and partition tolerance of the network, as it has been proven by the [[CAP theorem]].

==Peer network node data stores==
In peer network data stores, the user can usually reciprocate and allow other users to use their computer as a storage node as well. Information may or may not be accessible to other users depending on the design of the network.

Most [[peer-to-peer]] networks do not have distributed data stores in that the user's data is only available when their node is on the network. However, this distinction is somewhat blurred in a system such as [[BitTorrent (protocol)|BitTorrent]], where it is possible for the originating node to go offline but the content to continue to be served. Still, this is only the case for individual files requested by the redistributors, as contrasted with a network such as [[Freenet]] where all computers are made available to serve all files.

Distributed data stores typically use an [[error detection and correction]] technique.
Some distributed data stores (such as [[Parchive]] over NNTP) use [[forward error correction]] techniques to recover the original file when parts of that file are damaged or unavailable.
Others try again to download that file from a different mirror.

==Examples==

===Distributed non-relational databases===
* [[Aerospike database|Aerospike]]
* [[Apache Cassandra]], former data store of [[Facebook]]
* [[BigTable]], the data store of [[Google]]
* [[CrateIO]]
* [[Druid (open-source data store)]], used by [[Netflix]], [[Yahoo]] and others
* [[Dynamo (storage system)|Dynamo]] of [[Amazon.com|Amazon]]
* [[Hazelcast]]
* [[HBase]], current data store of Facebook's Messaging Platform
* [[Couchbase]], data store used by [[LinkedIn]], [[Paypal]], [[Ebay]] and others.
* [[MongoDB]]
* [[Riak]]
* [[Hypertable]], from [[Baidu]]
* [[Voldemort (distributed data store)|Voldemort]], data store used by [[LinkedIn]]

===Peer network node data stores===
* [[BitTorrent (protocol)|BitTorrent]]
* [[Blockchain (database)]]
* [[Chord project]]
* [[GNUnet]]
* [[Freenet]]
* Unity, of the software [[Perfect Dark (P2P)|Perfect Dark]]
* [[Mnet (Computer program)|Mnet]]
* [[Network News Transfer Protocol|NNTP]] (the distributed data storage protocol used for [[Usenet]] news)
* [[Storage@home]]
* [[Tahoe-LAFS]]

==See also==
{{Portal|Computer Science}}
* [[Data store]]
* [[Distributed file system]]
* [[Keyspace (distributed data store)|Keyspace]], the DDS [[Schema (database)|schema]]
* [[Peer-to-peer]]
* [[Distributed hash table]]
* [[Distributed cache]]

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Distributed data storage| ]]
[[Category:Distributed data stores| ]]

[[ja:分散ファイルシステム#分散データストア]]
<=====doc_Id=====>:56
<=====title=====>:
Database administration and automation
<=====text=====>:
{{More footnotes|date=March 2011}}
'''Database administration''' is the function of managing and maintaining [[database management system]]s (DBMS) software. Mainstream DBMS software such as [[Oracle database|Oracle]], [[IBM DB2]] and [[Microsoft SQL Server]] need ongoing management. As such, corporations that use DBMS software often hire specialized IT ([[Information technology|Information Technology]]) personnel called [[Database administrator|Database Administrators]] or DBAs.

==DBA Responsibilities==

* Installation, configuration and upgrading of Database server software and related products. 
* Evaluate Database features and Database related products.
* Establish and maintain sound backup and recovery policies and procedures. 
* Take care of the [[Database design]] and implementation. 
* Implement and maintain database security (create and maintain users and roles, assign privileges). 
* [[Database tuning]] and performance monitoring. 
* Application tuning and performance monitoring. 
* Setup and maintain documentation and standards. 
* Plan growth and changes (capacity planning). 
* Work as part of a team and provide 24x7 support when required. 
* Do general technical troubleshooting and give cons.
* Database recovery.

== Types of database administration ==
There are three types of DBAs:

#Systems DBAs (also referred to as Physical DBAs, Operations DBAs or Production Support DBAs): focus on the physical aspects of database administration such as DBMS installation, configuration, patching, upgrades, backups, restores, refreshes, performance optimization, maintenance and disaster recovery.
#Development DBAs: focus on the logical and development aspects of database administration such as [[data model]] design and maintenance, DDL ([[Data Definition Language|data definition language]]) generation, SQL writing and tuning, coding [[stored procedure]]s, collaborating with developers to help choose the most appropriate DBMS feature/functionality and other pre-production activities. 
#Application DBAs: usually found in organizations that have purchased [[Third-party developer|3rd party]] [[application software]] such as ERP ([[enterprise resource planning]]) and CRM ([[customer relationship management]]) systems. Examples of such application software includes [[Oracle Applications]], Siebel and [[PeopleSoft]] (both now part of Oracle Corp.) and SAP. Application DBAs straddle the fence between the DBMS and the application software and are responsible for ensuring that the application is fully optimized for the database and vice versa. They usually manage all the [[Software componentry|application components]] that interact with the database and carry out activities such as application installation and patching, application upgrades, database cloning, building and running data cleanup routines, data load [[process management]], etc.

While individuals usually specialize in one type of database administration, in smaller organizations, it is not uncommon to find a single individual or group performing more than one type of database administration.

== Nature of database administration ==
The degree to which the administration of a database is automated dictates the skills and personnel required to manage databases.  On one end of the spectrum, a system with minimal automation will require significant experienced resources to manage; perhaps 5-10 databases per DBA.  Alternatively an organization might choose to automate a significant amount of the work that could be done manually therefore reducing the skills required to perform tasks.  As automation increases, the personnel needs of the organization splits into highly [[skilled worker]]s to create and manage the automation and a group of lower skilled "line" DBAs who simply execute the automation.

Database administration work is complex, repetitive, time-consuming and requires significant training. Since databases hold valuable and mission-critical data, companies usually look for candidates with multiple years of experience. Database administration often requires DBAs to put in work during off-hours (for example, for planned after hours downtime, in the event of a database-related outage or if performance has been severely degraded). DBAs are commonly well compensated for the long hours

One key skill required and often overlooked when selecting a DBA is database recovery (under disaster recovery).  It is not a case of “if” but a case of “when” a database suffers a failure, ranging from a simple failure to a full catastrophic failure.  The failure may be data corruption, media failure, or user induced errors.  In either situation the DBA must have the skills to recover the database to a given point in time to prevent a loss of data.  A highly skilled DBA can spend a few minutes or exceedingly long hours to get the database back to the operational point.

== Database administration tools ==
Often, the DBMS software comes with certain tools to help DBAs manage the DBMS. Such tools are called native tools. For example, Microsoft SQL Server comes with SQL Server Management Studio and Oracle has tools such as [[SQL*Plus]] and Oracle Enterprise Manager/Grid Control. In addition, 3rd parties such as BMC, [[Quest Software]], [[Embarcadero Technologies]], [[EMS Database Management Solutions]] and SQL Maestro Group offer GUI tools to monitor the DBMS and help DBAs carry out certain functions inside the database more easily.

Another kind of database software exists to manage the provisioning of new databases and the management of existing databases and their related resources.  The process of creating a new database can consist of hundreds or thousands of unique steps from satisfying prerequisites to configuring backups where each step must be successful before the next can start.  A human cannot be expected to complete this procedure in the same exact way time after time - exactly the goal when multiple databases exist.  As the number of DBAs grows, without automation the number of unique configurations frequently grows to be costly/difficult to support.  All of these complicated procedures can be modeled by the best DBAs into database automation software and executed by the standard DBAs.  Software has been created specifically to improve the reliability and repeatability of these procedures such as [[Stratavia]]'s [[Data Palette]] and [[GridApp Systems]] Clarity.

== The impact of IT automation on database administration ==
Recently, automation has begun to impact this area significantly. Newer technologies such as [[Stratavia]]'s [[Data Palette]] suite and [[GridApp Systems]] Clarity have begun to increase the automation of databases causing the reduction of database related tasks. However at best this only reduces the amount of mundane, repetitive activities and does not eliminate the need for DBAs. The intention of DBA automation is to enable DBAs to focus on more proactive activities around database architecture, deployment, performance and service level management.

''Every database requires a database owner account that can perform all schema management operations. This account is specific to the database and cannot log in to Data Director. You can add database owner accounts after database creation. Data Director users must log in with their database-specific credentials to view the database, its entities, and its data or to perform database management tasks.
Database administrators and application developers can manage databases only if they have appropriate permissions and roles granted to them by the organization administrator. The permissions and roles must be granted on the database group or on the database, and they only apply within the organization in which they are granted.''

== Learning database administration ==
There are several education institutes that offer professional courses, including late-night programs, to allow candidates to learn database administration. Also, DBMS vendors such as Oracle, Microsoft and IBM offer certification programs to help companies to hire qualified DBA practitioners.  College degree in Computer Science or related field is helpful but not necessarily a prerequisite.

==See also==

*[[Column-oriented DBMS]]
*[[Data warehouse]]
*[[Directory service]]
*[[Distributed database management system]]
*[[Hierarchical model]]
*[[Navigational database]]
*[[Network model]]
*[[Object model]]
*[[Object database]] (OODBMS)
*[[Object-relational database]] (ORDBMS)
*[[Run Book Automation]] (RBA)
*[[Relational model]] (RDBMS)
*[[Comparison of relational database management systems]]
*[[Comparison of database tools]]
*[[SQL]] is a language for database management

== External links ==
* {{cite journal | publisher = ACM [[Special Interest Group on Information Retrieval]] | work = SIGIR Forum | volume = 7 | issue = 4 | date = Winter 1972 | pages = 45–55 | url = http://portal.acm.org/citation.cfm?id=1095495.1095500 | title = A set theoretic data structure and retrieval language }}
* {{cite journal | publisher = [[SIGMOD|ACM Special Interest Group on Management of Data]] | work = SIGMOD Record | volume = 35 | issue = 2 | date = June 2006 | url = http://www.tomandmaria.com/tom/Writing/VeritableBucketOfFactsSIGMOD.pdf | title = Origins of the Data Base Management System | format=PDF | author = Thomas Haigh }}

{{Databases}}
{{FOLDOC}}

[[Category:Database management systems]]
[[Category:Data management]]
<=====doc_Id=====>:59
<=====title=====>:
Versomatic
<=====text=====>:
'''Versomatic''' installs as a file system service where it tracks file changes and preemptively archives a copy of a file before it is modified. Archiving copies pre-emptively obviates the need to archive a reference copy of the files beforehand, as would be the case if the files were archived after being edited.

Starting from the moment Versomatic is installed, the last version of a file remains where the user expects it to be, and prior versions, if any, reside in a separate archive. Without this capability, we would have to scan your entire hard drive beforehand and make a duplicate copy of every file in order to create a baseline for subsequent revisions.

Files can be moved, renamed and copied without losing connection to their revision histories. Depending on user preferences, files can be monitored on local, removable and network drives. File revision histories are stored in a central database. Thus, for example, a user may insert a USB drive into his computer and edit a file on the USB drive. The edited file remains on the USB drive but a copy of the original unedited version is copied to the archive.

When a file is deleted, the deleted file is added to the repository together with any previous versions of the deleted file. This provides excellent level of protection against inadvertent file deletion.

Access to prior versions of a file is easy. A user merely selects a file and clicks the right mouse button to display a contextual pop-up menu listing the X most recent previous versions of the file, if any.

Upon user selection of one of the entries the appropriate previous version is opened with read-only privileges using the same application that created the file. Versomatic can also retrieve a copy of a previous version and move it into the same directory where the current version resides. The previous version will have a date & time stamp added to its file name in order to distinguish it from the current version. If a previous version is exported from Versomatic’s archive and changes are made thereto, a new revision history is created for the new file.

==History==

Versomatic is the first product from the long time team of [[Joaquin de Soto]], Jorge Miranda and Manny Menendez released under their new company [[Acertant]]. The team has a long list of hit products to their credit: MacLightning, [[ACD Canvas|Canvas]], Spelling Coach, BigThesaurus, Comment, UltraPaint, Artworks, [[ACDSee]], DenebaCAD, etc.

==External links==
*[http://www.acertant.com Company Web Page]

[[Category:Data management]]
<=====doc_Id=====>:62
<=====title=====>:
Holos
<=====text=====>:
{{multiple issues|
{{notability|date=May 2010}}
{{Unreferenced|date=May 2010}}
{{Peacock|date=September 2012}}
}}

'''Holos''' is an influential [[OLAP]] (Online Analytical Processing) product of the 1990s. Developed by Holistic Systems in 1987, the product remained in use until around 2004.

==Conception==
The Holos product succeeded an older generation of mainframe products such as [[System-W]]. It was the first to use an industry standard [[SQL]] database (as opposed to a proprietary one), and also the first to use the new GUI PC for the user interface.{{citation needed|date=September 2012}} In physically separating the ''number crunching'' from the user interface, the product was immediately client/server, although the term didn't come into use until some time later. In fact the process was described as cooperative processing at the time as client/server was not a current term at that time. The client/server model used for Holos was initially for a very "light" client as it was not clear at that time (1986/7) that PCs were going to be so commonplace and most were still running MS-DOS.

In fact it was technically possible to run the system using "dumb" terminal with reduced functionality in early versions although save for in Holistic's test environment this was rarely if ever done. In time due to the increased popularly of PCs and their increased power and the available of a stable and more functional version of Microsoft Windows additional functionality was added to the client end mostly in the form of development aids. In addition to data services, the Holos Server supplied business logic and calculation services. It also provided complementary services to the Holos Client which meant the internal processing associated with the report writer, worksheet, etc., was distributed between the two components.

==Architecture==
The core of the Holos Server was a [[business intelligence]] (BI) [[virtual machine]]. The Holos Language (HL) was compiled into a soft instruction code, and executed in this virtual machine (similar in concept to Java in more modern systems). The virtual machine was fully fault-tolerant, using structured [[exception handling]] internally, and provided a debugger interface. The debugger was machine-level until quite late on, after which it also supported source-level access.

OLAP data was handled as a core data type of HL, with specific syntax to accommodate multidimensional data concepts, and complete programmatic freedom to explore and utilise the data. This made it very different from the industry trend of query-based OLAP and SQL engines. On the upside, it allowed amazing flexibility in the applications to which it could be applied. On the downside, it mean that 3-tier configurations were never successfully implemented since the processing had to be close to the data itself. This hindered large-scale deployment to many clients, and the use of OLAP data from other vendors. In reality, its own data access times were probably some of the fastest around—at the individual cell level; they had to be in order to be practical. However, when fetching back bulk data for non-cooperating clients, or data from other vendors, the queries could not be optimised as a whole. Its own data access used a machine-wide shared memory cache.

==Language==
The Holos Language was a very broad language in that it covered a wide range of statements and concepts, including the reporting system, business rules, OLAP data, SQL data (using the Embedded SQL syntax within the hosting HL), device properties, analysis, forecasting, and data mining. It even supported elements to enable self-documentation and self-verification. Placing all these areas on a common footing, and allowing them to co-operate by sharing data, events, etc., was key to the number of possibilities that resulted. For instance, the report writer supported input as well as output, plus interactive graphics, and a comprehensive event mechanism to pass back information about the viewed data to event handlers. Also, reports and data were separate entities, thus allowing the same report to be applied to different data as long as it was described by similar meta-data. This meant that when terms like [[Enterprise Information System|EIS]] and [[Management information systems|MIS]] were first coined, the industry norm was "slideshows", i.e. pre-programmed transitions between views, whereas Holos provided data-driven drill-down, i.e. no pre-programmed views or links. The transitions could be made dependent upon the data values and trends, in conjunction with the available business logic.

==OLAP Storage==
Holos Server provided an array of different, but compatible, storage mechanisms for its multi-cube architecture: memory, disk, SQL. It was therefore the first product to provide "hybrid OLAP" ([[HOLAP]]). It provided a very versatile mechanism for joining cubes, irrespective of their storage technology, dimensionality, or meta-data, and this was eventually given a [[US patent]] (called COA—Compound OLAP Architecture {{US patent|6289352}}{{US patent|6490593}}). One novel aspect of this was a 'stack' feature that allowed read/write cubes to be stacked over read-only cubes. Read operations to the overall virtual cube then visited both 'racks' (top first, and then the bottom), whereas write operations only affected the top. The resulting valve-like mechanism found many applications in data sharing, what-if forecasting, and aggregation of slow SQL-based data. Since the overhead of the joining was small, it was not uncommon to have stacks 7 levels deep, and joining terabytes of real OLAP data. Around about V8.5, Holos Server implemented a hierarchical lock manager, allowing nesting of fine and coarse-grain OLAP locks, and full transaction control.

==Business Rules==
The business logic supported full cross-dimensional calculations, automatic ordering of rules using static data-flow analysis, and the identification and solution of simultaneous equations. The rules treated all dimensions in an orthogonal fashion. The aggregation process did not distinguish between simple summation or average calculations, and more complex non-commutative calculations. Both could be applied to any dimension member. The process allowed aggregation levels (i.e. those calculation levels starting with base data (level 0) and proceeding up to the overall grand total) to be individually pre-stored or left to be calculated on demand.

==Holos Client==
The Holos Client was both a design and delivery vehicle, and this made it quite large. Around about 2000, the Holos Language was made object-oriented (HL++) with a view to allowing the replacement of the Holos Client with a custom Java or VB product. However, the company were never sold on this, and so the project was abandoned.

One of the biggest failures was not to provide a thin-client interface to the Holos Server, and this must have contributed to the product's demise. Although an [[HTML]] toolkit was sold, it was clumsy and restricted. By the time a real thin-client mechanism was developed, it was far too late and it never got to market.

==Deployment==
Before its demise, the Holos Server product ran under Windows NT (Intel and Alpha), VMS (VAX and Alpha), plus about 10 flavours of UNIX, and accessed over half-a-dozen different SQL databases. It was also ported to several different locales, including Japanese.

==Company==
{{main|Crystal Decisions}}
Holistic Systems was purchased by the hardware company [[Seagate Technology]] in 1996. Along with other companies such as [[Crystal Services]], it was used to create a new subsidiary company called [[Seagate Software]]. Only Holistic and Crystal remained, and Seagate Software was renamed to [[Crystal Decisions]]. Holistic and Crystal had very different sales models. The average sale for the Holos Product in the United States was in excess of $250,000 and was sold primarily to Fortune 500 companies by a direct sales force. The Crystal sales model was based upon a "shrink wrapped" product Crystal Reports sold primarily through resellers. As Crystal was acquired prior to Holistic the senior management in the sales and marketing arena were mostly drawn from that organisation. They felt that all the product range should be sold through third parties and over a period of time dismantled the direct sales force culmination in a significant drop in sales for the Holos Product. Subsequently after some in-fighting and argument over product strategy, the main Holos development team finally started to leave around 2000, and Crystal Decisions was finally taken over by [[Business Objects (company)|Business Objects]] in 2004. Following the takeover, support for Holos was outsourced to [[Raspberry Software]], which was set up by former employees of Crystal Decisions.

[[Category:Data management]]
[[Category:Online analytical processing]]
<=====doc_Id=====>:65
<=====title=====>:
Data classification (data management)
<=====text=====>:
In the field of [[data management]], '''data classification''' as a part of [[Information Lifecycle Management]] (ILM) process can be defined as a tool for categorization of data to enable/help organization to effectively answer following questions:
*What [[data type]]s are available?
*Where are certain data located?
*What [[access level]]s are implemented?
*What protection level is implemented and does it adhere to [[compliance (regulation)|compliance]] regulations?
When implemented it provides a bridge between IT professionals and process or application owners. IT staff is informed about the data value and on the other hand management (usually application owners) understands better to what segment of data centre has to be invested to keep operations running effectively.  This can be of particular importance in risk management, legal discovery, and compliance with government regulations.
Data classification is typically a manual process; however, there are many tools from different vendors that can help gather information about the data.

==How to start process of data classification==
''Note that this classification structure is written from a Data Management perspective and therefore has a focus for text and text convertible binary data sources. Images, videos, and audio files are highly structured formats built for industry standard API's and do not readily fit within the classification scheme outlined below.''

First step is to evaluate and divide the various applications and data into their respective category as follows:
*Relational or Tabular data (around 15% of non audio/video data) 
**Generally describes  proprietary data which can be accessible only through application or [[application programming interfaces]] (API)
**Applications that produce structured data are usually database applications.
**This type of data usually brings complex procedures of data evaluation and migration between the storage tiers.
**To ensure adequate quality standards, the classification process has to be monitored by subject matter experts.
*Semi-structured or Poly-structured data (all other non audio/video data that does not conform to a system or platform defined Relational or Tabular form).
**Generally describes data files that have a dynamic or non-relational semantic structure (e.g. documents,XML,JSON,Device or System Log output,Sensor Output).
**Relatively simple process of data classification is criteria assignment.
**Simple process of [[data migration]] between assigned segments of predefined storage tiers.

Types of data classification - ''note that this designation is entirely orthogonal to the application centric designation outlined above. Regardless of structure inherited from application, data may be of the types below''

1. Geographical : i.e. according to area (supposing the rice production of a state or country etc.)
2. Chronological: i.e. according to time (sale of last 3 months)
3. Qualitative  : i.e. according to distinct categories. (E.g.: population on the basis of poor and rich)
4. Quantitative : i.e. according to magnitude(a) discrete and b)continuous

==Basic criteria for semi-structured or poly-structured data classification==
*Time criteria is the simplest and most commonly used where different type of data is evaluated by time of creation, time of access, time of update, etc.
*Metadata criteria as type, name, owner, location and so on can be used to create more advanced classification policy
*Content criteria which involve usage of advanced content classification algorithms are most advanced forms of unstructured data classification

''Note that any of these criteria may also apply to Tabular or Relational data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.''.

==Basic criteria for relational or Tabular data classification==
These criteria are usually initiated by application requirements such as:
*Disaster recovery and Business Continuity rules
*Data centre resources optimization and consolidation
*Hardware performance limitations and possible improvements by reorganization

''Note that any of these criteria may also apply to semi/poly structured data as "Basic Criteria". These criteria are application specific, rather than inherent aspects of the form in which the data is presented.''

==Benefits of data classification==
Benefits of effective implementation of appropriate data classification can significantly improve ILM process and save data centre storage resources. If implemented systemically it can generate improvements in data centre performance and utilization. Data classification can also reduce costs and administration overhead. "Good enough" data classification can produce these results:
*Data compliance and easier [[risk management]]. Data are located where expected on predefined storage tier and "point in time"
*Simplification of data encryption because all data need not be encrypted. This saves valuable processor cycles and all related consecutiveness. 
*Data indexing to improve user access times
*Data protection is redefined where RTO ([[Recovery Time Objective]]) is improved.

==See also==
*[[Data classification (business intelligence)]]

==References==

* Josh Judd and Dan Kruger (2005), Principles of SAN Design. Infinity Publishing
* Stephen J. Bigelown (November 2005), SearchStorage.com, http://searchstorage.techtarget.com/news/article/0,289142,sid5_gci1139240,00.html

[[Category:Data management]]
[[Category:Information technology]]
[[Category:Regulations]]
<=====doc_Id=====>:68
<=====title=====>:
Category:Semantic desktop
<=====text=====>:
{{Cat main|Semantic desktop}}

[[Category:Knowledge representation]]
[[Category:Data management]]
<=====doc_Id=====>:71
<=====title=====>:
Ontology merging
<=====text=====>:
{{Unreferenced|date=March 2013}}

'''Ontology merging''' defines the act of bringing together two conceptually divergent [[ontology (computer science)|ontologies]] or the instance data associated to two ontologies. This is similar to work in database merging ([[schema matching]]). This merging process can be performed in a number of ways, manually, semi automatically, or automatically. Manual ontology merging although ideal is extremely labour-intensive and current research attempts to find semi or entirely automated techniques to merge ontologies. These techniques are statistically driven often taking into account similarity of concepts and raw similarity of instances through textual [[string metrics]] and semantic knowledge. These techniques are similar to those used in [[information integration]] employing [[string metrics]] from [[open source]] similarity libraries.

==See also==
*[[ontology mapping]]
*[[data integration]]

[[Category:Ontology (information science)]]
[[Category:Data management]]
<=====doc_Id=====>:74
<=====title=====>:
Data governance
<=====text=====>:
{{Governance}}

'''Data governance''' is a [[Control (management)|control]] that ensures that the [[data]] entry by an operations team member or by automated processes meets precise standards, such as a business rule, a data definition and data integrity constraints in the data model. The data governor uses data quality monitoring against production data to communicate errors in data back to operational team members, or to the technical support team, for corrective action.  Data governance is used by organizations to exercise control over processes and methods used by their [[data stewards]] and [[data custodian]]s in order to improve data quality.

Data governance is a set of processes that ensures that important data assets are formally managed throughout the enterprise. Data governance ensures that data can be trusted and that people can be made accountable for any adverse event that happens because of low data quality. It is about putting people in charge of fixing and preventing issues with data so that the enterprise can become more efficient. Data governance also describes an evolutionary process for a company, altering the company’s way of thinking and setting up the processes to handle information so that it may be utilized by the entire organization. It’s about using technology when necessary in many forms to help aid the process. When companies desire, or are required, to gain control of their data, they empower their people, set up processes and get help from technology to do it.<ref name="sarsfield">Sarsfield, Steve (2009). "The Data Governance Imperative", IT Governance.</ref>

According to one vendor, data governance is a [[quality control]] discipline for assessing, managing, using, improving, monitoring, maintaining, and protecting organizational information. It is a system of decision rights and accountabilities for information-related processes, executed according to agreed-upon models which describe who can take what actions with what information, and when, under what circumstances, using what methods.<ref name="The DGI Data Governance Framework">{{cite web|url=http://www.datagovernance.com/wp-content/uploads/2014/11/dgi_framework.pdf|title=The DGI Data Governance Framework}}</ref>

== Overview ==
Data governance encompasses the people, processes, and [[information technology]] required to create a consistent and proper handling of an organization's data across the business enterprise.  Goals may be defined at all levels of the enterprise and doing so may aid in acceptance of processes by those who will use them.  Some goals include

* Increasing consistency and confidence in [[decision making]]
* Decreasing the risk of regulatory fines
* Improving [[information security|data security]], also defining and verifying the requirements for data distribution policies<ref>Gianni, D., (2015, Jan). Data Policy Definition and Verification for System of Systems Governance, in Modeling and Simulation Support for System of Systems Engineering [http://onlinelibrary.wiley.com/doi/10.1002/9781118501757.ch5/summary]</ref> 
* Maximizing the income generation potential of data
* Designating accountability for information quality
* Enable better planning by supervisory staff
* Minimizing or eliminating re-work
* Optimize staff effectiveness
* Establish process performance baselines to enable improvement efforts
* Acknowledge and hold all gain

These goals are realized by the implementation of Data governance programs, or initiatives using Change Management techniques

==Data governance drivers==
While data governance initiatives can be driven by a desire to improve data quality, they are more often driven by C-Level leaders responding to external regulations. Examples of these regulations include [[Sarbanes-Oxley]], [[Basel I]], [[Basel II]], [[HIPAA]], [[General Data Protection Regulation|GDPR]] and a number of data privacy regulations. To achieve compliance with these regulations, business processes and controls require formal management processes to govern the data subject to these regulations.<ref>[http://www.rimes.com/rimes-data-governance-handbook 'Rimes Data Governance Handbook'] [[RIMES]]</ref> Successful programs identify drivers meaningful to both supervisory and executive leadership.

Common themes among the external regulations center on the need to manage risk. The risks can be financial misstatement, inadvertent release of sensitive data, or poor data quality for key decisions. Methods to manage these risks vary from industry to industry. Examples of commonly referenced best practices and guidelines include [[COBIT]], [[ISO/IEC 38500]], and others. The proliferation of regulations and standards creates challenges for data governance professionals, particularly when multiple regulations overlap the data being managed. Organizations often launch data governance initiatives to address these challenges.

== Data governance initiatives (Dimensions)==
Data governance initiatives improve [[data quality]] by assigning a team responsible for data's accuracy, accessibility, consistency, and completeness, among other metrics.  This team usually consists of executive leadership, [[project management]], [[line function|line-of-business managers]], and [[data steward]]s. The team usually employs some form of methodology for tracking and improving enterprise data, such as [[Six Sigma]], and tools for [[data mapping]], [[data profiling|profiling]], cleansing, and monitoring data.

Data governance initiatives may be aimed at achieving a number of objectives including offering better visibility to internal and external customers (such as [[supply chain]] management), compliance with [[compliance (regulation)|regulatory law]], improving operations after rapid company growth or [[mergers and acquisitions|corporate mergers]], or to aid the efficiency of enterprise [[knowledge worker]]s by reducing confusion and error and increasing their scope of knowledge. Many data governance initiatives are also inspired by past attempts to fix information quality at the departmental level, leading to incongruent and redundant data quality processes. Most large companies have many applications and databases that can't easily share information. Therefore, knowledge workers within large organizations often don't have access to the information they need to best do their jobs. When they do have access to the data, the [[data quality]] may be poor. By setting up a data governance practice or [[corporate data|Corporate Data]] Authority, these problems can be mitigated.

The structure of a data governance initiative will vary not only with the size of the organization, but with the desired objectives or the 'focus areas' <ref name="focus areas">{{cite web|url=http://datagovernance.com/fc_focus_areas_for_data_governance.html |title=Data Governance Focus Areas |deadurl=yes |archiveurl=https://web.archive.org/web/20081006152845/http://www.datagovernance.com/fc_focus_areas_for_data_governance.html |archivedate=2008-10-06 |df= }}</ref> of the effort.

== Implementation ==
Implementation of a Data Governance initiative may vary in scope as well as origin. Sometimes, an executive mandate will arise to initiate an enterprise wide effort, sometimes the mandate will be to create a pilot project or projects, limited in scope and objectives, aimed at either resolving existing issues or demonstrating value. Sometimes an initiative will originate lower down in the organization’s hierarchy, and will be deployed in a limited scope to demonstrate value to potential sponsors higher up in the organization.  The initial scope of an implementation can vary greatly as well, from review of a one-off IT system, to a cross-organization initiative.

== Data governance tools ==
Leaders of successful data governance programs declared in December 2006 at the Data Governance Conference in Orlando, Fl, that data governance is between 80 and 95 percent communication."<ref>{{cite web
 |url=http://www.dmreview.com/issues/2007_48/10001356-1.html 
 |title=Data Governance: One Size Does Not Fit All 
 |last=Hopwood 
 |first=Peter 
 |authorlink=Peter Hopwood 
 |publisher=[[DM Review Magazine]] 
 |date=June 2008 
 |accessdate=2008-10-02 
 |archiveurl=http://www.webcitation.org/5bGHaz1gA?url=http://www.dmreview.com/issues/2007_48/10001356-1.html 
 |archivedate=2008-10-02 
 |quote=At the inaugural Data Governance Conference in Orlando, Florida, in December 2006, leaders of successful data governance programs declared that in their experience, data governance is between 80 and 95 percent communication. Clearly, data governance is not a typical IT project. 
 |deadurl=yes 
 |df= 
}}</ref> That stated, it is a given that many of the objectives of a Data Governance program must be accomplished with appropriate tools. Many vendors are now positioning their products as Data Governance tools; due to the different focus areas of various data governance initiatives, any given tool may or may not be appropriate, in addition, many tools that are not marketed as governance tools address governance needs.<ref>{{cite web
 |url=http://www.datagovernancesoftware.com 
 |title=DataGovernanceSoftware.com 
 |publisher=[[The Data Governance Institute]] 
 |accessdate=2008-10-02 
 |archiveurl=http://www.webcitation.org/5bGI3dfHV?url=http://www.datagovernancesoftware.com/ 
 |archivedate=2008-10-02 
 |quote= 
 |deadurl=yes 
 |df= 
}}</ref>

== Data governance organizations ==
;DAMA International<ref>[http://www.dama.org/i4a/pages/index.cfm?pageid=1 DAMA International]</ref>
:[[DAMA]] (the Data Management Association) is a not-for-profit, vendor-independent, international association of technical and business professionals dedicated to advancing the concepts and practices of information resource management (IRM) and data resource management (DRM).

;Data Governance Professionals Organization (DGPO)<ref>
[http://www.dgpo.org/ Data Governance Professionals Organization<!-- Bot generated title -->]</ref>
:The Data Governance Professionals Organization (DGPO) is a non-profit, vendor neutral, association of business, IT and data professionals dedicated to advancing the discipline of data governance.  The objective of the DGPO is to provide a forum that fosters discussion and networking for members and to encourage, develop and advance the skills of members working in the data governance discipline.

;The Data Governance Society <ref>[http://www.datagovernancesociety.org Data Governance Society]</ref>
:The Data Governance Society, Inc. is dedicated to fostering a new paradigm for the effective use and protection of information in which Data is governed and leveraged as a unique corporate asset.

;The Data Governance Council <ref>[https://www-935.ibm.com/services/uk/cio/pdf/leverage_wp_data_gov_council_maturity_model.pdf Data Governance Council]</ref> 
:The Data Governance Council is an organization formed by IBM consisting of companies, institutions and technology solution providers with the stated objective to build consistency and quality control in governance, which will help companies better protect critical data."

;IQ International -- the International Association for Information and Data Quality<ref>[http://iaidq.org/ IQ International, the International Association for Information and Data Quality]</ref>
:IQ International is a not-for-profit, vendor neutral, professional association formed in 2004, dedicated to building the information and data quality profession.

== Data governance conferences ==
A number of major conferences relevant to data governance are held annually:
;Data Governance and Information Quality Conference<ref>[http://dgiq-conference.com/ Data Governance and Information Quality Conference<!-- Bot generated title -->]</ref> 
:Commercial conferences held each year in the USA

;Data Governance Conference Europe,<ref>[http://www.irmuk.co.uk/ Data Governance Conference Europe<!-- Bot generated title -->]</ref>
:Commercial conferences held annually in London, England .
;Information and Data Quality Conference<ref>[http://idq-conference.com/ Information and Data Quality Conference]</ref>
:Not for profit conference run by IQ International in the USA
;Master Data Management & Data Governance Conferences<ref>[http://www.tcdii.com/events/cdimdmsummitseries.html MDM SUMMIT Conference<!-- Bot generated title -->]</ref> 
:Six major conferences are run annually by the MDM Institute in London, San Francisco, Sydney, Toronto, Madrid, Frankfurt, and New York City.
;Financial Information Summit series of conferences<ref>[http://www.financialinformationsummit.com<!-- Bot generated title -->]</ref> 
;Hosted by Inside Reference Data magazine in New York, London, Hong Kong, Toronto, Chicago, Frankfurt, Paris and Tokyo.

==See also==
* [[Information Architecture]]
* [[Information technology governance]]
* [[Semantics of Business Vocabulary and Business Rules]]
* [[Master data management]]
* [[COBIT]]
* [[ISO/IEC 38500]]
* [[ISO/TC 215]]
* [[Operational risk management]]
* [[Basel II Accord]]
* [[HIPAA]]
* [[Sarbanes-Oxley Act]]
* [[Information technology controls]]
* [[Data Protection Directive]] (EU)
* [[Universal Data Element Framework]]
* [[Asset Description Metadata Schema]]

==References==
<!--to cite a web resource, use this template
<ref>{{cite web
  | url = MANDATORY
  | title = MANDATORY
  | last =
  | first =
  | authorlink =
  | coauthors =
  | work =
  | publisher =
  | date =
  | format =
  | language=
  | doi =
  | accessdate =  
  | archiveurl = SHOULD BE USED ON PAGES ALLOWING ARCHIVING - USE A SERVICE LIKE webcitation.org or archive.org
  | archivedate = MANDATORY IF archiveurl
  | quote = 
 }}</ref>
-->
{{reflist}}

[[Category:Information technology governance]]
[[Category:Data management]]
<=====doc_Id=====>:77
<=====title=====>:
Ontology-based data integration
<=====text=====>:
'''Ontology-based data integration''' involves the use of [[ontology (computer science)|ontology]](s) to effectively combine data or information from multiple heterogeneous sources.<ref name="wache">{{cite conference |author1=H. Wache |author2=T. Vögele |author3=U. Visser |author4=H. Stuckenschmidt |author5=G. Schuster |author6=H. Neumann |author7=S. Hübner | title=Ontology-Based Integration of Information A Survey of Existing Approaches | year=2001 | citeseerx = 10.1.1.142.4390 }}</ref> It is one of the multiple [[data integration]] approaches and may be classified as Global-As-View (GAV).<ref name="refone">{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | year=2002 | pages=243–246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf }}</ref> The effectiveness of ontology based data integration is closely tied to the consistency and expressivity of the ontology used in the integration process.

==Background==
Data from multiple sources are characterized by multiple types of heterogeneity. The following hierarchy is often used:<ref name="sheth">{{cite book | author=A.P. Sheth | title = Changing Focus on Interoperability in Information Systems: From System, Syntax, Structure to Semantics | booktitle=Interoperating Geographic Information Systems. M. F. Goodchild, M. J. Egenhofer, R. Fegeas, and C. A. Kottman (eds.), Kluwer Academic Publishers | year=1999 | pages=5–30 | url=http://lsdis.cs.uga.edu/library/download/S98-changing.pdf}}</ref><ref>[http://daks.ucdavis.edu/~ludaesch/Paper/AHM02/tutorial5.html AHM02 Tutorial 5: Data Integration and Mediation; Contributors: B. Ludaescher, I. Altintas, A. Gupta, M. Martone, R. Marciano, X. Qian]</ref>
* [[Syntactic heterogeneity]]: is a result of differences in representation format of data
* Schematic or [[structural heterogeneity]]: the native model or structure to store data differ in data sources leading to structural heterogeneity. Schematic heterogeneity that particularly appears in structured databases is also an aspect of structural heterogeneity.<ref name="sheth"/>
* [[Semantic heterogeneity]]: differences in interpretation of the 'meaning' of data are source of semantic heterogeneity
* [[System heterogeneity]]: use of different [[operating system]], hardware platforms lead to system heterogeneity

[[ontology (computer science)|Ontologies]], as formal models of representation with explicitly defined concepts and named relationships linking them, are used to address the issue of [[semantic heterogeneity]] in data sources. In domains like [[bioinformatics]] and [[biomedicine]], the rapid development, adoption and public availability of ontologies [http://www.bioontology.org/repositories.html#obo] has made it possible for the [[data integration]] community to leverage them for [[semantic integration]] of data and information.

==The role of ontologies==

Ontologies enable the unambiguous identification of entities in heterogeneous information systems and assertion of applicable named relationships that connect these entities together. Specifically, ontologies play the following roles:

* Content Explication<ref name="wache"/>
The ontology enables accurate interpretation of data from multiple sources through the explicit definition of terms and relationships in the ontology.

* Query Model<ref name="wache"/>
In some systems like SIMS,<ref name="arens">{{cite conference |author1=Y. Arens |author2=C. Hsu |author3=C.A. Knoblock | title=Query Processing in sims information mediator | year=1996 | url=http://www.isi.edu/integration/papers/arens98-agents.pdf}}</ref> the query is formulated using the ontology as a global query schema.

* Verification<ref name="wache"/>
The ontology verifies the mappings used to integrate data from multiple sources. These mappings may either be user specified or generated by a system.

===Approaches using ontologies for data integration===
There are three main architectures that are implemented in ontology-based data integration applications,<ref name="wache"/> namely, 
;Single ontology approach: A single ontology is used as a global reference model in the system. This is the simplest approach as it can be simulated by other approaches.<ref name="wache"/> SIMS<ref name="arens"/>  a prominent example of this approach.  The Structured Knowledge Source Integration component of [[Cyc|Research Cyc]] is another prominent example of this approach.<ref>http://www.cyc.com/content/semantic-knowledge-source-integration</ref><ref>http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/2299</ref> (Title = Harnessing Cyc to Answer Clinical Researchers' Ad Hoc Queries)

;Multiple ontologies: Multiple ontologies, each modeling an individual data source, are used in combination for integration. Though, this approach is more flexible than the single ontology approach, it requires creation of mappings between the multiple ontologies. Ontology mapping is a challenging issue and is focus of large number of research efforts in [[computer science]] [http://www.ontologymatching.org/]. The OBSERVER system<ref name="mena">{{cite conference |author1=E. Mena |author2=V. Kashyap |author3=A. Sheth |author4=A. Illarramendi | title=OBSERVER: An Approach for Query Processing in Global Information Systems based on Interoperation across Pre-existing Ontologies | year=1996 | url=http://dit.unitn.it/~p2p/RelatedWork/Matching/MKSI96.pdf}}</ref> is an example of this approach.

;Hybrid approaches: The hybrid approach involves the use of multiple ontologies that subscribe to a common, top-level vocabulary.<ref name="goh">{{cite conference | author=Cheng Hian Goh | title=Representing and Reasoning about Semantic Conflicts in Heterogeneous Information Systems | year=1997 | url=http://context2.mit.edu/coin/publications/goh-thesis/goh-thesis.pdf}}</ref>  The top-level vocabulary defines the basic terms of the domain. Thus, the hybrid approach makes it easier to use multiple ontologies for integration in presence of the common vocabulary.

==See also==
* [[Data mapping]]
* [[Enterprise application integration]]
* [[Enterprise information integration]]
* [[Ontology mapping]]
* [[Schema matching]]

==References==

<references/>

==External links==
*[http://sid.cps.unizar.es/OBSERVER/ OBSERVER home page]
*[http://www.cyc.com/content/semantic-knowledge-source-integration Cyc Semantic Knowledge Source Integration (SKSI)]

[[Category:Ontology (information science)]]
[[Category:Data management]]
<=====doc_Id=====>:80
<=====title=====>:
XLDB
<=====text=====>:
'''XLDB''' refers to '''eXtremely Large [[Database|Data Bases]]'''.  The definition of ''extremely large'' refers to data sets that are too big in terms of volume (too much), and/or velocity (too fast), and/of variety (too many places, too many formats) to be handled using conventional solutions.

== History ==

In October 2007 the XLDB experts gathered at [[SLAC]] for the [https://web.archive.org/web/20080417002612/http://www-conf.slac.stanford.edu/xldb07/ First Workshop on Extremely Large Databases]. As a result, the XLDB research community was formed. to meet rapidly growing demands, in addition to the original invitational workshop, an open conference, tutorials, and annual satellite events on different continents were added. The main event, held annually at Stanford gathers over 300 technically savvy attendees. XLDB is one of the premier database events catered towards both academic and industrial communities.

== Goals ==

The main goals of this community include:<ref>{{ cite web | url=http://www-conf.slac.stanford.edu/xldb09/docs/xldb09_welcomeTalk.ppt | year=2009 | last=Becla| first=Jacek | title=XLDB 3 Welcome | accessdate=2009-08-29 }}</ref>

* Identify trends, commonalities and major roadblocks related to building extremely large databases
* Bridge the gap between users trying to build extremely large databases and database solution providers worldwide
* Facilitate development and growth of practical technologies for extremely large data stores

== XLDB Community ==
As of 2013, the community consisted of about a thousand members including:
# Scientists who develop, use, or plan to develop or use XLDB for their research, from laboratories.
# Commercial users of XLDB.
# Providers of database products, including commercial vendors and representatives from open source database communities.
# Academic database researchers.

== XLDB Conferences, Workshops and Tutorials ==
The community meets annually at [[Stanford]] where the main event is held each fall, usually in September. These who live too far from California to attend have the opportunity to attend satellite events, organized annually around May/June either in [[Asia]] or in [[Europe]].

A detailed report is produced after each workshop.

{| class="wikitable"
|-
! Year
! Place
! Link
! Report
! Comments
|-
| 2015
| [[Stanford]]
| [https://web.archive.org/web/20150521105100/http://www-conf.slac.stanford.edu/xldb2015/]
|
| 8th XLDB Conference
|-
| 2014
| [http://www.on.br/ Observatório Nacional], [[Rio_de_Janeiro]]
| [https://web.archive.org/web/20150219081443/http://xldb-rio2014.linea.gov.br/]
|
| Satellite XLDB Workshop in South America
|-
| 2014
| [[Stony_Brook_University]]
| [https://web.archive.org/web/20150521052839/http://www3.cs.stonybrook.edu/~xldb/]
|
| XLDB-Healthcare Workshop
|-
| 2013
| [[Stanford]]
| [https://conf-slac.stanford.edu/xldb-2013/]
|
| 7th XLDB Conference
|-
| 2013
| [[CERN]], [[Geneva]]/[[Switzerland]]
| [http://xldb-europe-workshop-2013.web.cern.ch/]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
|
| Satellite XLDB Workshop in Europe
|-
| 2012
| [[Stanford]]
| [http://www-conf.slac.stanford.edu/xldb2012/]
| [http://www.jstage.jst.go.jp/article/dsj/12/0/12_12_023/_pdf]
| 6th XLDB Conference, Workshop & Tutorials
|-
| 2012
| [[Beijing]], [[China]]
| [https://web.archive.org/web/20120708164351/http://idke.ruc.edu.cn/xldb/www.xldb-asia.org/home.html]
| [http://www.xldb.org/wp-content/uploads/2012/09/XLDBAsia2012Report.pdf]
| Satellite XLDB Conference in Asia
|-
| 2011
| [[SLAC]]
| [https://web.archive.org/web/20110426125951/http://www-conf.slac.stanford.edu/xldb2011/]
| [http://www.jstage.jst.go.jp/article/dsj/11/0/37/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 5th XLDB Conference and Workshop
|-
| 2011
| [[Edinburgh]], [[UK]]
| [https://web.archive.org/web/20160303221547/http://xldb.eu/xldb_europe_2011/]
| not available
| Satellite XLDB Workshop in Europe
|-
| 2010
| [[SLAC]]
| [https://web.archive.org/web/20110727234052/http://www-conf.slac.stanford.edu/xldb2010/]
| [http://www.jstage.jst.go.jp/article/dsj/9/0/9_MR1/_article]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 4th XLDB Conference and Workshop
|-
| 2009
| [[Lyon]], [[France]]
| [https://web.archive.org/web/20110727234623/http://www-conf.slac.stanford.edu/xldb2009/]
| [http://www.jstage.jst.go.jp/article/dsj/8/0/MR1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 3rd XLDB Workshop
|-
| 2008
| [[SLAC]]
| [https://web.archive.org/web/20110727234818/http://www-conf.slac.stanford.edu/xldb2008/]
| [http://www.jstage.jst.go.jp/article/dsj/7/0/196/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 2nd XLDB Workshop
|-
| 2007
| [[SLAC]]
| [https://web.archive.org/web/20110727235121/http://www-conf.slac.stanford.edu/xldb2007/]
| [http://www.jstage.jst.go.jp/article/dsj/7/0/1/_pdf]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
| 1st XLDB Workshop
|}

== Tangible results ==

The XLDB events led to initiating the effort of building a new open source, science database, [https://web.archive.org/web/20090220121225/http://scidb.org/ SciDB].<ref>{{ cite web | url=http://www.jstage.jst.go.jp/article/dsj/7/0/88/_pdf  | year=2008 | last=Becla| first=Jacek | title=Report from the SciDB Workshop |accessdate=2008-09-29}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>

The XLDB organizers started defining a [http://www.xldb.org/science-benchmark/ science benchmark] for scientific data management systems called SS-DB.

At [http://xldb.org/2012|XLDB 2012]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} the XLDB organizers announced that two major databases that support arrays as first-class objects ([[MonetDB]] SciQL and [[SciDB]]) have formed a working group in conjunction with XLDB. This working group is proposing a common syntax (provisionally named “ArrayQL”) for manipulating arrays, including array creation and query.

== References ==
{{reflist}}

== Further reading ==
* Pavlo A., Paulson E., Rasin A., Abadi D. J., Dewitt D. J., Madden S., and Stonebraker M., ''A Comparison of Approaches to Large-Scale Data Analysis," Proceedings of the 2009 ACM SIGMOD, http://web.archive.org/web/20090611174944/http://database.cs.brown.edu:80/sigmod09/benchmarks-sigmod09.pdf
* Becla, J., et al. 2006, ''Designing a multi-petabyte database for LSST,'' http://arxiv.org/abs/cs/0604112
* Becla, J., & Wang, D. L. 2005, ''Lessons Learned from Managing a Petabyte'', downloaded from http://web.archive.org/web/20110604223735/http://www.slac.stanford.edu/pubs/slacpubs/10750/slac-pub-10963.pdf on 2007-11-25.
* Bell, G., Gray, J., & Szalay, A. 2005, ''Petascale computations systems: Balanced cyberinfrastructure in a data-centric world,'' http://arxiv.org/abs/cs/0701165
* Duellmann, D. 1999, ''Petabyte Databases'', ACM SIGMOD Record, vol. 28, p. 506, http://web.archive.org/web/20071012015357/http://www.sigmod.org/sigmod/record/issues/9906/index.html#TutorialSessions.
* Hanushevsky, A., & Nowak, M. 1999, ''Pursuit of a Scalable High Performance Multi-Petabyte Database'', 16th IEEE Symposium on Mass Storage Systems, pp. 169–175, http://citeseer.ist.psu.edu/217883.html.
* Shiers, J., ''Building Very Large, Distributed Object Databases'', downloaded from http://web.archive.org/web/20070915101842/http://wwwasd.web.cern.ch:80/wwwasd/cernlib/rd45/papers/dbprog.html on 2007-11-25.

[[Category:Types of databases]]
[[Category:Data management]]
<=====doc_Id=====>:83
<=====title=====>:
Category:Data synchronization
<=====text=====>:
[[Category:Synchronization]]
[[Category:Data management|Synchronization]]
[[Category:Distributed computing problems]]
[[Category:Fault-tolerant computer systems]]
[[Category:Data quality]]
<=====doc_Id=====>:86
<=====title=====>:
Core data integration
<=====text=====>:
{{Citations missing|date=December 2007}}
'''Core data integration''' is the use of [[data integration]] technology for a significant, centrally planned and managed IT initiative within a company. Examples of core data integration initiatives could include:

* ETL ([[Extract, transform, load]]) implementations
* EAI ([[Enterprise Application Integration]]) implementations
* SOA ([[Service-Oriented Architecture]]) implementations
* ESB ([[Enterprise Service Bus]]) implementations

Core data integrations are often designed to be enterprise-wide integration solutions. They may be designed to provide a data abstraction layer, which in turn will be used by individual core data integration implementations, such as ETL servers or applications integrated through EAI.

Because it is difficult to promptly roll out a centrally managed data integration solution that anticipates and meets all data integration requirements across an organization, IT engineers and even business users create [[edge data integration]], using technology that may be incompatible with that used at the core. In contrast to a core data integration, an edge data integration is not centrally planned and is generally completed with a smaller budget and a tighter deadline. 

==See also==
* [[data integration]]
* [[edge data integration]]

== References ==
* http://searchsoa.techtarget.com/tip/0,289483,sid26_gci1171085,00.html* 
* 

[[Category:Data management]]
<=====doc_Id=====>:89
<=====title=====>:
Edge data integration
<=====text=====>:
{{Refimprove|date=January 2008}}
An '''edge data integration''' is an implementation of [[data integration]] technology undertaken in an ad hoc or tactical fashion. This is also sometimes referred to as point-to-point integration because it connects two types of data directly to serve a narrow purpose. Many edge integrations, and actually the vast majority of all data integration, involves hand-coded scripts. Some may take the form of [[Business Mashups]] (web application hybrids), [[Rich Internet application]]s, or other browser-based models that take advantage of [[Web 2.0]] technologies to combine data in a Web browser.

Examples of edge data integration projects might be:

* extracting a list of customers from a host [[Sales Force Automation]] application and writing the results to an [[Microsoft Excel|Excel]] spreadsheet
* creating a script-driven framework for managing [[RSS]] feeds
* combining data from a weather Web site, a shipping company's Web site, and a company's internal logistics database to track shipments and estimated arrival times of packages

It has been claimed that edge data integration do not typically require large budgets and centrally managed technologies, which is in contrast to a [[core data integration]].

== See also ==
* [[core data integration]]
* [[Business Mashups]]
* [[Rich Internet application]]
* [[Web 2.0]]
* [[Yahoo! Pipes]]
* [[Microsoft Popfly]]
*[[IBM Mashup Center]]

[[Category:Data management]]
<=====doc_Id=====>:92
<=====title=====>:
Reference data
<=====text=====>:
{{For|use in finance|Reference data (financial markets)}}
'''Reference data''' are [[data]] that define the set of permissible values to be used by other [[data field]]s. Reference data gain in value when they are widely re-used and widely referenced. Typically, they do not change overly much in terms of definition, apart from occasional revisions. Reference data are often defined by standards organizations, such as country codes as defined in [[ISO 3166-1]].<ref>{{Cite web|title = IBM Redbooks {{!}} Reference Data Management|url = http://www.redbooks.ibm.com/abstracts/tips1016.html|website = www.redbooks.ibm.com|date = 2013-05-16|accessdate = 2015-12-09|language = en}}</ref><ref>{{Cite web|title = Reference Data Management and Master Data: Are they Related ?   (Oracle Master Data Management)|url = https://blogs.oracle.com/mdm/entry/reference_data_management_and_master|website = blogs.oracle.com|accessdate = 2015-12-09}}</ref>

Examples of reference data include:
* [[Units of measurement]]
* [[Country code]]s
* Corporate codes
* [[conversion of units|Fixed conversion rates]] e.g., [[weight]], [[temperature]], and [[length]]
* [[Calendar]] structure and constraints

==Differences with master data==
Reference data should be distinguished from [[master data]], which represent key business entities such as customers and materials in all their necessary detail (e.g., for customers: number, name, address, and date of account creation). In contrast, reference data usually consist only of a list of permissible values and attached textual descriptions. A further difference between reference data and master data is that a change to the reference data values may require an associated change in business process to support the change; a change in master data will always be managed as part of existing business processes. For example, adding a new customer or sales product is part of the standard business process. However, adding a new product classification (e.g. restricted sales item) or a new customer type (e.g. gold level customer) will result in a modification to the business processes to manage those items.

==References==
<references />

==Further reading==
* {{Book reference|title = Managing Reference Data in Enterprise Databases|last = Chisholm|first = Malcolm|publisher = Morgan Kaufmann Publishers|year = 2001|isbn = 1558606971|location = |pages = }}
* {{Book reference|title = Master Data Management for SaaS Applications|last = Whei-Jen|first = Chen|publisher = IBM Redbooks|year = 2014|isbn = 978-0738440040|location = |pages = }}
* {{Book reference|title = Master Data Management and Data Governance|last = Berson|first = Alex|publisher = McGraw-Hill Osborne Media|year = 2011|isbn = 978-0071744584|location = |pages = }}

==See also==
* [[Master Data]]
* [[Data modeling]]
* [[Master Data Management]]
* [[Enterprise bookmarking]]
* [[Data architecture]]
* [[Transaction data]]
* [[Code_(metadata)]]

== External links ==
* [https://msdn.microsoft.com/en-us/library/hh213066.aspx Microsoft MSDN, Reference Data Services in DQS, 2012]

[[Category:Data management]]
<=====doc_Id=====>:95
<=====title=====>:
Cognos ReportNet
<=====text=====>:
{{Infobox Software
| name = Cognos ReportNet
| logo = 
| screenshot = 
| caption = 
| author = [[Cognos]], an [[IBM]] Company
| developer = 
| released = September, 2003
| latest release version = Cognos ReportNet 1.3
| latest release date = 
| latest preview version = 
| latest preview date = 
| operating system = Multiple
| platform = Multiple
| language = Multi-lingual
| status = '''Inactive'''<ref>[http://www-01.ibm.com/software/analytics/cognos/products/reportnet/ IBM.com, ''Cognos ReportNet : now part of Cognos Enterprise'', consulté le 12 février 2014]</ref>
| genre = [[Business Intelligence]]
| license = 
| website = [http://www-01.ibm.com/software/data/cognos/products/reportnet/ IBM.com]
}}

'''Cognos ReportNet (CRN)''' is a web-based [[software]] product for creating and managing [[ad hoc]] and custom-made reports. ReportNet is developed by the [[Ottawa]]-based  company [[Cognos]] (formerly Cognos Incorporated), an [[IBM]] company. The web-based reporting tool was launched in September 2003. Since IBM's acquisition of Cognos, ReportNet has been renamed ''IBM Cognos ReportNet'' like all other Cognos products.

ReportNet uses web services standards such as [[XML]] and [[Simple Object Access Protocol]] and also supports dynamic [[HTML]] and [[Java (programming language)|Java]].<ref>[https://web.archive.org/web/20080312025955/http://www.vnunet.com/vnunet/news/2123232/bear-sterns-chooses-cognos-reportnet Cognos ReportNet in news]</ref> ReportNet is compatible with multiple databases including [[Oracle Database|Oracle]], [[SAP AG|SAP]], [[Teradata]], [[Microsoft SQL server]], [[IBM DB2|DB2]] and [[Sybase]].<ref>[http://www.cognos.com/solutions/data/ibm/advantages.html Data sources]</ref><ref>[http://support.cognos.com/en/support/products/crn101_software_environments.html CRN Environment details]</ref> The product provides interface in over 10 languages,<ref>[http://www.cognos.com/products/business_intelligence/reporting/features.html CRN Features]</ref> has Web Services architecture to meet the needs of multi-national, diversified enterprises and helps reduce total cost of ownership. Multiple versions of Cognos ReportNet have since been released by the company. Cognos ReportNet was awarded the [[Software and Information Industry Association]] (SIIA) 2005 [[Codie Awards]] for the "Best Business Intelligence or Knowledge Management Solution" category.<ref>[http://www.mywire.com/pubs/PRNewswire/2005/06/08/885642?extID=10051 Cognos ReportNet wins award]</ref> CRN's capabilities have been further used in [[IBM Cognos 8 Business Intelligence|IBM Cognos 8 BI (2005)]], the latest reporting tool.<ref>[http://www.cognos.com/products/cognos8businessintelligence Cognos 8 BI]</ref> CRN comes with its own [[software development kit]] (SDK).

==Launch==
Early adopters of Cognos ReportNet for their corporate reporting needs included [[Bear Stearns]], [[BMW]] and [[Alfred Publishing]]. Around this same time of launch, Cognos competitor [[Business Objects]] released version 6.1 of its enterprise reporting tool. Cognos ReportNet has been successful since its launch, raising revenues in 2004 from licensing fees.<ref>[http://www.highbeam.com/doc/1G1-131525446.html Cognos ReportNet delivers $30 Million in License Revenue in one Quarter]</ref> Subsequently, other major corporations like [[McDonald's]] adopted Cognos ReportNet.<ref>[http://www.ebizq.net/news/5538.html ReportNet and fries]</ref>

==Controversy==
Cognos rival [[Business Objects (company)|Business Objects]] announced in 2005 that BusinessObjects XI significantly outperformed Cognos ReportNet in benchmark tests conducted by VeriTest, an independent software testing firm. The tests performed showed Cognos ReportNet performed poorly when processing styled reports, complex business reports and combination of both.<ref>[http://www.crm2day.com/news/crm/114773.php BO XI Vs Cognos ReportNet]</ref> The tests reported a massive 21 times higher report throughput for BusinessObjects XI than Cognos ReportNet at capacity loads.<ref>[http://goliath.ecnext.com/coms2/summary_0199-4404821_ITM BO XI outperforms Cognos ReportNet]</ref> Cognos soon dismissed the claims by stating Business Objects dictated the environment and testing criteria and Cognos did not provide the software to participate in benchmark test.<ref>[http://www.cognos.com/news/releases/2005/0624_3.html Cognos dismisses the Test results]</ref> Cognos later performed their own test to demonstrate Cognos ReportNet capabilities.<ref>[http://www.cognos.com/pdfs/whitepapers/wp_cognos_reportnet_scalability_benchmakrs_ms_windows.pdf Cognos scalability results]</ref>

==Components==
* Cognos Report Studio – A Web-based product for creating complex professional looking reports.<ref>[http://web.princeton.edu/sites/datamall/documents/ug_cr_rptstd.pdf Refer definition in introduction page]</ref>
* Cognos Query Studio - A Web-based product for creating ad-hoc reports.<ref>[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer Introduction page]</ref>
* Cognos Framework Manager – A [[metadata modeling]] tool to create BI metadata for reporting and dashboard applications.<ref>[http://www.cognos.com/products/framework_services Framework Manager Services] {{webarchive |url=https://web.archive.org/web/20080417030129/http://www.cognos.com/products/framework_services |date=April 17, 2008 }}</ref>
* Cognos Connection – Main [[Enterprise portal|portal]] used to access reports, schedule reports and perform administrator activities.<ref>[http://web.princeton.edu/sites/datamall/documents/ug_cr_qstd.pdf Refer page9]</ref>

==Versions==
* Cognos ReportNet 1.1 – [[Java EE]]-style professional web-based authoring tool. (base version)
* Cognos ReportNet IBM Special Edition – comes with an embedded version of [[IBM WebSphere]] as its application server and [[IBM DB2]] as its data store.
* Cognos Linux – for Intel-based [[Linux]] platforms.<ref>[http://www.ebizq.net/news/5688.html ReportNet on Linux]</ref>

==See also==
*[[IBM Cognos Business Intelligence]]

==References==
{{reflist|30em}}

[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:IBM software]]
<=====doc_Id=====>:98
<=====title=====>:
Holistic Data Management
<=====text=====>:
'''Holistic Data Management''' (HDM) framework is AHISDATA indigenous standard for implementing software implementations within an organization network. This framework extends the existing data management solutions such as [[data quality]], [[data governance]], [[data integration]], [[data processing]], [[master data management]] and [[data validation]] solutions.

The HDM framework specifies that:
*All data objects must exist as a child data object or a parent data object.
*Only one unique parent data object must exist within a data network scope (DNS).
*All child data objects must have a data-mapping link defined within a data network scope.
*A data object relationship must exist at least in one of the following four data management modules:
''Data mapping'', ''data validation'', ''[[data integration]]'',''[[data processing]]''

== HDM framework ==
The following entities are specified in the HDM framework.

*''Data network scope (DNS)''
The data network scope (DNS) is the logical boundary that a software application database system of record (SOR) exists within an enterprise network. There can be multiple DNS within an enterprise network.

*''Data network domain (DND)''
The data network domain (DND) is the logical boundary representing a collection of multiple data network scope (DNS). There can be multiple DND within an enterprise network.

*''System of record (SOR)''
A system of record applies to the master or principal database system that a parent data objects resides on.
There can only be one SOR within a data network scope.

*''Parent data object (PDO)''
A parent data object (PDO) is the system of record schema object name. Only one unique parent data object must exist within a data network scope.

*''Child data object (CDO)''
A child data object (CDO) is a schema object name that derives its data from one or more parent data object(s).

*''Data-mapping link (DML)''
A data-mapping link (DML) is the data requirement specification applied to the relationship between multiple database schema objects where one data object derives its data from one or more data objects. DML is only applicable for a data-mapping data management module.

*''Data–object relationship (DOR)''
The data–object relationship (DOR) is the data requirement, business rule, program function that applies to one or multiple data objects. DOR can be applied on data-mapping links for each data management modules. Only one DOR can exist on a DML within a data management module.

*''Data management modules (DMM)''
Data management modules are the common user interface (UI) programs that defines and manage the data object relationship(s) within a data network scope.

There are four data management modules:

''Data mapping'' – This is the base data management user interface module. The data-mapping module provide the functionalities for managing data-mapping links and data object relationships for all database schemas within a data network scope. A data network scope must have at least one data-mapping design defined.

''Data validation'' – This user interface module provides the functionalities for defining and managing validation events on data object relationships. Validation events include auditing, reporting, scheduler, logger, triggers and DNS health check. Data validation events requires a data-mapping design defined within a data network scope.

''[[Data integration]]'' – This user interface module provides the functionalities for defining and managing interface configurations on data object relationships. The interface configurations include scheduler, transmission mode, listener, interface API and reporting. The interface APIs would allow third-party systems to transfer data using the data object relationship defined within a data network scope. Data Integration interface configuration requires a Data Mapping design defined within a data network scope.

''[[Data processing]]'' – This user interface module provides the functionalities for defining and managing interface configurations and batch runtime engines on data object relationships. The interface configurations include scheduler, transmission mode, multi-batch transmission, user-defined DOR API and reporting. Data Processing interface configuration requires a data-mapping design defined within a data network scope.

== Implementing the HDM framework ==
The HDM framework presents a standard for software implementations within an organization. The objective is to shed visibility, increase efficiency and centralized management of all other software implementations within an organization.

The HDM framework should be implemented as a major organization project that is supervised by the project management office. This would require a project charter developed and a project manager assigned for managing the implementation process. There are several phases involve in implementing the HDM framework:

*''Choose a data management module (DMM)'' – This exercise requires the acquisition of a data management module software application to be used for implementing the rest of the HDM framework. AHISDATA iNTEGRITY software is an integrated solution that provides DMM functionalities.
*''Scrub (inventory of existing applications and data sources)'' – This exercise identifies all applications within an organization and the data sources that they are connected to.
*''Formation (applications and data schema relation)'' – This exercise is to align all applications in relation to the data schemas within the data sources. The applications are grouped in the order of the data schemas that they access.
*''First axe (applications eligible for decommission)'' – This exercise is to identify all applications that are rogue, obsolete and completely redundant. These applications are eligible for removal.
*''Second axe (application eligible for consolidation)'' – This exercise is to identify all applications that have some functional similarities and some uniqueness in the data requirement. These applications are eligible for consolidation. The functionalities that are similar are left intact on one application and turned off or disabled on the other(s).
*''Define data network domain (DND)'' – This exercise is to define the data network domain for all the approved applications within the enterprise network.
*''Define Data Network Scope (DNS)'' – This exercise is to define the data network scope(s) required for each DND.
*''Define system of records (SOR)'' – This exercise is to define the SOR for each DNS.
*''Define parent data objects (PDO)'' – This exercise is to define all PDOs in each DNS.
*''Define child data objects (CDO)'' – This exercise is to define all CDOs in each DNS.
*''Define data mapping links (DML)'' – This exercise is to define all data-mapping links and object relationship in all DNS.
*''Define data object relationships (DOR)'' – This exercise is to define the DOR requirement for each data management module implemented.

==See also==
*[[Reference data]]
*[[Master data]]
*[[Customer data integration]]
*[[Product information management]]
*[[Identity resolution]]

==External links==
* [http://www.ahisdata.com/eHDMS/AHISDATA_HDM_WhitePaper_v35.pdf AHISDATA HDM WhitePaper]
* [http://msdn2.microsoft.com/en-us/library/bb190163.aspx#mdm04_topic4 The What, Why, and How of Master Data Management]

{{Data warehouse}}

[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing products]]
[[Category:Information technology management]]
<=====doc_Id=====>:101
<=====title=====>:
Sales intelligence
<=====text=====>:
'''Sales intelligence''' (SI) refers to technologies, applications and practices for the collection, integration, analysis, and presentation of information to help salespeople keep up to date with clients, [[Prospect research|prospect data]] and drive business. In addition to providing [[Performance metric|metric]]s for win-loss and sales confidence,<ref>[http://chapmanhq.com/solutions/strategic-account-management-sam/metrics-and-measurements Metrics for sales intelligence]</ref> SI can present contextually relevant customer and product information.

The 2008 survey of 300 companies by the [[Aberdeen Group]]<ref>[http://www.aberdeen.com/Aberdeen-Library/5379/RA-sales-intelligence-nirvana.aspx Sales Intelligence, Aberdeen Group study (2008)]</ref> show that the recent economic downturn has lengthened traditional sales cycles. As businesses have been forced to reduce spending, sales representatives have been challenged to meet [[Sales quota|quota]]s. Top performing companies have implemented sales intelligence programs to improve the quality and quantity of sales leads. SI contextualizes opportunities by providing relevant industry, corporate and personal information. Frequently SI's fact-based information is integrated or includes [[customer relationship management]] (CRM).

Although some aspects of sales intelligence overlaps business intelligence (BI), SI is specifically designed for the use of salespeople and sales managers. Unlike [[customer relationship management]] (CRM) and traditional [[business intelligence]] (BI) applications, SI provides real-time analysis of current sales data and assists with suggesting and delivering actionable, relevant information.

Sales intelligence solutions are predominantly designed for companies in the [[manufacturing]], [[distribution (business)|distribution]] and [[wholesale]] sectors. These are highly competitive markets, where volumes are high, [[Profit margin|margin]]s are low. (SI) solutions provide unique insight into customer [[buying pattern]]s. By automatically analysing and evaluating these patterns, Sales Intelligence pro-actively identifies and delivers [[up-sell]], [[cross-sell]] and switch-sell opportunities.

==See also==
* [[Analytics]]
* [[Augmented learning]]
* [[Business intelligence tools]]
* [[Dashboards (management information systems)]]
* [[Location intelligence]]
* [[Market intelligence]]
* [[Marketing intelligence]]
* [[Operational Intelligence]]
* [[OODA Loop]]
* [[Predictive analytics]]
* [[Business Intelligence 2.0]]
* [[Process mining]]
* [[Right-time marketing]]
* [[Integrated business planning]]

==References==
{{reflist}}

==External links==
* [http://blog.findable.me/post/52963306183/sales-intelligence-a-short-primer Sales Intelligence A Short Primer]

[[Category:Business intelligence]]
[[Category:Data management]]

[[da:Business intelligence]]
[[de:Business-Intelligence]]
[[es:Inteligencia empresarial]]
[[fr:Informatique décisionnelle]]
[[ko:경영 정보학]]
[[hr:Poslovna inteligencija]]
[[id:Intelijen bisnis]]
[[it:Business intelligence]]
[[lt:Verslo analitika]]
[[nl:Business intelligence]]
[[pl:Business intelligence]]
[[pt:Business intelligence]]
[[ru:Business Intelligence]]
[[fi:Business intelligence]]
[[sv:Business intelligence]]
<=====doc_Id=====>:104
<=====title=====>:
Retention period
<=====text=====>:
The '''retention period''' of information is an aspect of [[records management|records and information management]] (RIM) and the [[records life cycle]].  It identifies the duration of time for which the information should be maintained or "retained", irrespective of format (paper, electronic, or other).  Retention periods vary on different types of information, based on content and a variety of other factors including: internal organizational need, regulatory requirements for inspection or audit, legal statutes of limitation, involvement in litigation, taxation and financial reporting needs, as well as other factors as defined by local, regional, state, national and/or international governing entities.  

Once an applicable retention period has elapsed for a given type or series of information, and all holds/moratoriums have been released, the information is typically destroyed using an approved and effective destruction method, which renders the information completely and irreversibly unusable via any means.  Information with historical value beyond its "usable value" may be accessioned to the custody of an archive organization for permanent or extended long-term preservation.  

==Defensible retention==
''Defensible retention'' refers to the ability of an identified and applied retention period to effectively provide for the defense of the record, and its eventual destruction or accessioning when scrutinized within a court of law or by other review.

It is commonly advised by [[Records management|Records and Information Management]] (RIM) professionals that any and all retention periods applied to organizational information should be reviewed and approved for use by competent legal counsel, which represents the organization, and is familiar with the specific legal and regulatory requirements of the organization.

==Guidance and education organizations==
*[[ARMA International]]
*[[Information and Records Management Society]]

==See also==
*[[Retention schedule]]

==References==
<references/>

[[Category:Legal documents]]
[[Category:Data management]]
[[Category:Public records]]
[[Category:Records management]]
<=====doc_Id=====>:107
<=====title=====>:
Enterprise data management
<=====text=====>:
{{notability|date=July 2015}}
{{No footnotes|date=November 2009}}
'''Enterprise Data Management''' ('''EDM''') is:

#A concept – referring to the ability of an organization to precisely define, easily integrate and effectively retrieve data for both internal applications and external communication.
#A business objective – focused on the creation of accurate, consistent and transparent content.  EDM emphasizes data precision, granularity and meaning and is concerned with how the content is integrated into [[business application]]s as well as how it is passed along from one [[business process]] to another.

EDM arose to address circumstances where users within organizations independently source, model, manage and [[Computer data storage|store data]].  Uncoordinated approaches by various segments of the organization can result in [[data conflict]]s and quality inconsistencies – lowering the trustworthiness of the data as it is used for operations and [[Business reporting|reporting]]. 

The goal of EDM is trust and confidence in data assets.  Its components are:

==Strategy and governance==
EDM requires a strategic approach to choosing the right processes, technologies and resources (i.e. data owners, governance, stewardship, [[data analyst]]s and [[data architect]]s).  EDM is a challenge for organizations because it requires alignment among multiple stakeholders (including IT, operations, finance, strategy and [[end-user]]s) and relates to an area (creation and use of common data) that has not traditionally had a clear “owner.”  

The governance challenge can be a big obstacle to the implementation of an effective EDM because of the difficulties associated with providing a [[business case]] on the benefits of data management.  The core of the challenge is due to the fact that data quality has no intrinsic value.  It is an enabler of other processes and the true benefits of effective data management are systematic and intertwined with other processes.  This makes it hard to quantify all the downstream implications or upstream improvements.  

The difficulties associated with quantification of EDM benefits translate into challenges with the positioning of EDM as an organizational priority.  Achieving organizational alignment on the importance of data management (as well as managing data as an ongoing area of focus) is the domain of [[governance]].  In recent years the establishment of an EDM and the EDM governance practice has become commonplace despite these difficulties. 

==Program implementation==
Implementation of an EDM program encompasses many processes – all of which need to be coordinated throughout the organization and managed while maintaining operational continuity.  Below are some of the major components of EDM implementation that should be given consideration:

===Stakeholder requirements===
EDM requires alignment among multiple stakeholders (at the right level of authority) who all need to understand and support the EDM objectives.  EDM begins with a thorough understanding of the requirements of the end users and the organization as a whole.  Managing stakeholder requirements is a critical, and ongoing, process based in an understanding of [[workflow]], data dependencies and the tolerance of the organization for operational disruption.  Many organizations use formal processes such as [[service level agreement]]s to specify requirements and establish EDM program objectives.

===Policies and procedures===
Effective EDM usually includes the creation, documentation and enforcement of operating policies and procedures associated with [[change management]], (i.e. [[data model]], business [[glossary]], master data shared domains, [[data cleansing]] and [[data normalization|normalization]]), data [[stewardship]], security constraints and dependency rules.  In many cases, these policies and procedures are documented for the first time as part of the EDM initiative.

===Data definitions and tagging===

One of the core challenges associated with EDM is the ability to compare data that is obtained from multiple internal and external sources.  In many circumstances, these sources use inconsistent terms and definitions to describe the data content itself – making it hard to compare data, hard to automate business processes, hard to feed complex applications and hard to exchange data.  This frequently results in a difficult process of [[data mapping]] and cross-referencing.  Normalization of all the terms and definitions at the data attribute level is referred to as the [[metadata]] component of EDM and is an essential prerequisite for effective data management.

===Platform requirements===

Even though EDM is fundamentally a data content challenge, there is a core technology dimension that must be addressed.  Organizations need to have a functional storage platform, a comprehensive data model and a robust messaging infrastructure.  They must be able to integrate data into applications and deal with the challenges of the existing (i.e. legacy) technology infrastructure.  Building the platform or partnering with an established technology provider on how the data gets stored and integrated into business applications is an essential component of the EDM process.

Enterprise data management as an essential business requirement has emerged as a priority for many organizations.  The objective is confidence and trust in data as the glue that holds business strategy together.

==See also==
* [[Master data management]]
* [[Master Data]]

==References==
{{Reflist}}

;General
* Enterprise Data Management Council http://www.edmcouncil.org
* [http://www.thegoldensource.com/files/EDM_Finextra_Report_Final.pdf Issues in Enterprise Data Management: A Survey Report, 12/06]

[[Category:Data management]]
[[Category:Product lifecycle management]]
<=====doc_Id=====>:110
<=====title=====>:
Electronically stored information (Federal Rules of Civil Procedure)
<=====text=====>:
'''Electronically stored information''' ('''ESI'''), for the purpose of the [[Federal Rules of Civil Procedure]] (FRCP) is information created, manipulated, communicated, stored, and best utilized in digital form, requiring the use of computer hardware and software.<ref name="nwjtip">[http://www.law.northwestern.edu/journals/njtip/v4/n2/3 ''Electronically Stored Information: The December 2006 Amendments to the Federal Rules of Civil Procedure''], Kenneth J. Withers, Northwestern Journal of Technology and Intellectual Property, Vol.4 (2), 171</ref>

ESI has become a legally defined phrase as the [[Federal government of the United States|U.S. government]] determined for the purposes of the FRCP rules of 2006 that promulgating procedures for maintenance and discovery for electronically stored information was necessary.  References to “electronically stored information” in the Federal Rules of Civil Procedure (FRCP) invoke an expansive approach to what may be discovered during the fact-finding stage of civil litigation.<ref name="Federal Rules of Civil Procedure">{{cite web|title=Federal Rules of Civil Procedure (FRCP)|url=https://www.law.cornell.edu/rules/frcp/rule_34|website=Legal Information Institute [LII]|publisher=Cornell University Law School|accessdate=October 31, 2015|ref=Rule 34}}</ref>

Rule 34(a) enables a party in a civil lawsuit to request another party to produce and permit the requesting party or its representative to inspect, copy, test, or sample the following items in the responding party's possession, custody, or control:

<blockquote>any designated documents or electronically stored information—including writings, drawings, graphs, charts, photographs, sound recordings, images, and other data or data compilations—stored in any medium from which information can be obtained either directly or, if necessary, after translation by the responding party into a reasonably usable form...Rule 34(a)(1) is intended to be broad enough to cover all current types of computer-based information, and flexible enough to encompass future changes and developments.</blockquote>

==Types==

===Native files===
The term ''native files'' refers to user-created documents, which could be in [[Microsoft Office]] or [[Apache OpenOffice|Open Office]] document formats as well as other files stored on computer, but could include video surveillance footage saved on a computer hard drive, [[Computer-aided design]] files such as [[blueprint]]s or maps, [[digital photography|digital photographs]], scanned images, [[archive file]]s, e-mail, and [[digital audio]] files, among other data,

===Logical data===
A judge ruled that [[Random Access Memory|RAM]] is reasonably accessible and retainable for anticipation of litigation.{{Citation needed|date=November 2011}}

In Australia RAM, can be used in litigation post 1996.

==References==
{{reflist}}

==Further reading==
* {{cite book|chapter=Meet the New Rules|title=The Discovery Revolution|author1=George L. Paul |author2=Bruce H. Nearon |publisher=American Bar Association|year=2006|isbn=9781590316054}}
* {{cite book|title=Discovery of Electronically Stored Information|author=Ronald J. Hedges|publisher=BNA Books|year=2007|isbn=9781570186721}}
* {{cite book|title=The Sedona Principles 2007: Best Practices Recommendations &amp; Principles for Addressing Electronic Document Production|author=Jonathan M. Redgrave|publisher=BNA Books|year=2007|isbn=9781570186776}}
* {{cite book|title=Cyber Forensics|author1=Albert J. Marcella |author2=Albert J. Marcella Jr. |author3=Doug Menendez |chapter=Electronically stored information and cyber forensics|publisher=CRC Press|year=2007|isbn=9780849383281}}
* {{cite book|title=Litigating With Electronically Stored Information|author1= Marian K. Riedy |author2=Suman Beros |author3= Kim Sperduto |publisher=Artech House Telecommunications Library|year=2007|isbn=9781596932203}}

[[Category:Computer data]]
[[Category:Data management]]
[[Category:United States discovery law]]
[[Category:Records management]]


{{US-law-stub}}
<=====doc_Id=====>:113
<=====title=====>:
Locks with ordered sharing
<=====text=====>:
In [[databases]] and [[transaction processing]] the term '''Locks with ordered sharing''' comprises several variants of the ''[[Two phase locking]]'' (2PL) [[concurrency control]] protocol generated by changing the blocking semantics of locks upon [[Serializability#View and conflict serializability|conflicts]]. One variant is identical to [[Commitment ordering#Strict CO (SCO)|Strict commitment ordering (SCO)]].

==References==

*D. Agrawal, A. El Abbadi, A. E. Lang: [http://portal.acm.org/citation.cfm?id=627615   ''The Performance of Protocols Based on Locks with Ordered Sharing''], IEEE Transactions on Knowledge and Data Engineering, Volume 6, Issue 5, October 1994, PP. 805 – 818, {{ISSN|1041-4347}}

[[Category:Data management]]
[[Category:Databases]]
[[Category:Concurrency control]]
[[Category:Transaction processing]]


{{database-stub}}
<=====doc_Id=====>:116
<=====title=====>:
Data verification
<=====text=====>:
Data''' Verification''' is a process in which different types of data are checked for accuracy and [[data consistency|inconsistencies]] after [[data migration]] is done.<ref>http://www.datacap.com/products/features/verify/</ref>

It helps to determine whether data was accurately translated when data is [[data transfer|transferred]] from one source to another, is complete, and supports processes in the new system. During verification, there may be a need for a parallel run of both systems to identify areas of disparity and forestall erroneous [[data loss]].

A type of Data Verification is [[double entry]] and [[proofreading]] data. Proofreading data involves someone checking the data entered against the original document. This is also time consuming and costly.

==References==
{{reflist|2}}

==External links==
* [http://www.pcguide.com/care/bu/howVerification-c.html PC Guide article]

[[Category:Data management]]
[[Category:Data quality]]
<=====doc_Id=====>:119
<=====title=====>:
Consumer relationship system
<=====text=====>:
'''Consumer relationship systems''' ('''CRS''') are specialized [[customer relationship management]] (CRM) [[software]] applications that are used to handle a company's dealings with its customers.<ref name ="Insight44-50">[http://www.nxtbook.com/nxtbooks/cmp/cmi_200709/index.php ICMI Customer Management Insight Magazine, September 2007, pp 44–50], Retrieved 11 January 2012</ref>

Current consumer relationship systems integrate the [[software]] with telephone and call recording systems as well as with corporate systems for input and reporting. Customers can provide input from the company's website directly into the CRS. These systems are popular because they can deliver the 'voice of the consumer' that contributes to product quality improvement and that ultimately increases corporate profits.<ref name ="Insight44-50" />

Consumer relationship systems that provide automated support as well as advanced systems may have [[artificial intelligence]] (AI) interfaces that can extract and analyse [[data]] collected, or handle basic questions and complaints.<ref>{{cite web|last1=Smith| first1=S.E.|title= What is Consumer Relationship System? |date= |publisher= WiseGeek.net|url= http://www.wisegeek.net/what-is-consumer-relationship-system.htm|accessdate=1 February 2013}}</ref>

==History==
The first CRS was developed in the 1980s. In 1981 Michael Wilke and Robert Thornton founded Wilke/Thornton, Inc in [[Columbus, Ohio]], to develop new CRS software.<ref>[http://www.wilke-thornton.com/WTI/Pages/products.html Wilke/Thornton, Inc Products] Retrieved 11 January 2012</ref>

==See also==
* [[ECRM]]
* [[Business intelligence]]
* [[Customer experience]]
* [[Customer intelligence]]
* [[Customer service]] – contains ISO standards
* [[Customer value maximization]]
* [[Enterprise relationship management]] (ERM)
* [[Sales force management system]]
* [[Sales intelligence]]
* [[Sales process engineering]]

== References ==
{{reflist}}

[[Category:Business intelligence]]
[[Category:Data management]]
<=====doc_Id=====>:122
<=====title=====>:
Isolation (database systems)
<=====text=====>:
{{Refimprove|date=January 2009}}
In [[database]] systems, '''isolation''' determines how transaction integrity is visible to other users and systems. For example, when a user is creating a Purchase Order and has created the header, but not the Purchase Order lines, is the header available for other systems/users, carrying out [[Concurrency (computer science)|concurrent]] operations (such as a report on Purchase Orders), to see?

A lower isolation level increases the ability of many users to access data at the same time, but increases the number of concurrency effects (such as dirty reads or lost updates) users might encounter. Conversely, a higher isolation level reduces the types of concurrency effects that users may encounter, but requires more system resources and increases the chances that one transaction will block another.<ref>"Isolation Levels in the Database Engine", Technet, Microsoft, http://technet.microsoft.com/en-us/library/ms189122(v=SQL.105).aspx</ref>

Isolation is typically defined at database level as a property that defines how/when the changes made by one operation become visible to other. On older systems, it may be implemented systemically, for example through the use of temporary tables. In two-tier systems, a Transaction Processing (TP) manager is required to maintain isolation. In n-tier systems (such as multiple websites attempting to book the last seat on a flight), a combination of stored procedures and transaction management is required to commit the booking and send confirmation to the customer.<ref>"The Architecture of Transaction Processing Systems", Chapter 23, Evolution of Processing Systems, Department of Computer Science, Stony Brook University, retrieved 20 March 2014, http://www.cs.sunysb.edu/~liu/cse315/23.pdf</ref>

Isolation is one of the [[ACID]] (Atomicity, Consistency, Isolation, Durability) properties.

==Concurrency control==
[[Concurrency control]] comprises the underlying mechanisms in a [[DBMS]] which handles isolation and guarantees related correctness. It is heavily utilized by the database and storage engines (see above) both to guarantee the correct execution of concurrent transactions, and (different mechanisms) the correctness of other DBMS processes. The transaction-related mechanisms typically constrain the database data access operations' timing ([[Schedule (computer science)|transaction schedules]]) to certain orders characterized as the [[serializability]] and [[recoverability]] schedule properties. Constraining database access operation execution typically means reduced performance (rates of execution), and thus concurrency control mechanisms are typically designed to provide the best performance possible under the constraints. Often, when possible without harming correctness, the serializability property is compromised for better performance. However, recoverability cannot be compromised, since such typically results in a quick database integrity violation.

[[Two-phase locking]] is the most common transaction concurrency control method in DBMSs, used to provide both serializability and recoverability for correctness. In order to access a database object a transaction first needs to acquire a [[Lock (database)|lock]] for this object. Depending on the access operation type (e.g., reading or writing an object) and on the lock type, acquiring the lock may be blocked and postponed, if another transaction is holding a lock for that object.

==Isolation levels==
Of the four [[ACID]] properties in a [[Database management system|DBMS]] (Database Management System), the isolation property is the one most often relaxed.  When attempting to maintain the highest level of isolation, a DBMS usually acquires [[Lock (database)|locks]] on data or implements [[multiversion concurrency control]], which may result in a loss of [[concurrency (computer science)|concurrency]].  This requires adding logic for the [[software application|application]] to function correctly.

Most DBMSs offer a number of ''transaction isolation levels'', which control the degree of locking that occurs when selecting data. For many database applications, the majority of database transactions can be constructed to avoid requiring high isolation levels (e.g. SERIALIZABLE level), thus reducing the locking overhead for the system.  The programmer must carefully analyze database access code to ensure that any relaxation of isolation does not cause software bugs that are difficult to find. Conversely, if higher isolation levels are used, the possibility of [[deadlock]] is increased, which also requires careful analysis and programming techniques to avoid.

The isolation levels defined by the [[American National Standards Institute|ANSI]]/[[International Organization for Standardization|ISO]] [[SQL]] standard are listed as follows.

===Serializable===
This is the ''highest'' isolation level.

With a lock-based [[concurrency control]] DBMS implementation, [[serializability]] requires read and write locks (acquired on selected data) to be released at the end of the transaction.  Also ''range-locks'' must be acquired when a [[Select (SQL)|SELECT]] query uses a ranged ''WHERE'' clause, especially to avoid the ''phantom reads'' phenomenon (see below).

When using non-lock based concurrency control, no locks are acquired; however, if the system detects a ''write collision'' among several concurrent transactions, only one of them is allowed to commit.  See ''[[snapshot isolation]]'' for more details on this topic.

From : (Second Informal Review Draft) ISO/IEC 9075:1992, Database Language SQL- July 30, 1992:
''The execution of concurrent SQL-transactions at isolation level SERIALIZABLE is guaranteed to be serializable. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.''

===Repeatable reads===
In this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps read and write locks (acquired on selected data) until the end of the transaction.  However, ''range-locks'' are not managed, so '''''[[Isolation (database systems)#Phantom reads|phantom reads]]''''' can occur.

Also, write skew is possible when one transaction updates column to some color whereas competing transactions updates the same column to some other color(s). In serial execution of the transactions, you should end up with the whole column unicolored whereas repeatable read admits a mixture of colors.<ref>[https://wiki.postgresql.org/wiki/SSI#Simple_Write_Skew Postgresql wiki - SSI]</ref>

===Read committed===
In this isolation level, a lock-based [[concurrency control]] DBMS implementation keeps write locks (acquired on selected data) until the end of the transaction, but read locks are released as soon as the [[Select (SQL)|SELECT]] operation is performed (so the ''non-repeatable reads'' phenomenon can occur in this isolation level, as discussed below). As in the previous level, ''range-locks'' are not managed.

Putting it in simpler words, read committed is an isolation level that guarantees that any data read is committed at the moment it is read. It simply restricts the reader from seeing any intermediate, uncommitted, 'dirty' read. It makes no promise whatsoever that if the transaction re-issues the read, it will find the same data; data is free to change after it is read.

===Read uncommitted===
This is the ''lowest'' isolation level. In this level, '''''[[Isolation (database systems)#Dirty reads|dirty reads]]''''' are allowed, so one transaction may see ''not-yet-committed'' changes made by other transactions.

Since each isolation level is stronger than those below, in that no higher isolation level allows an action forbidden by a lower one, the standard permits a DBMS to run a transaction at an isolation level stronger than that requested (e.g., a "Read committed" transaction may actually be performed at a "Repeatable read" isolation level).

==Default isolation level==
The ''default isolation level'' of different [[Database management system|DBMS]]'s varies quite widely. Most databases that feature transactions allow the user to set any isolation level. Some DBMS's also require additional syntax when performing a SELECT statement to acquire locks (e.g. ''SELECT ... FOR UPDATE'' to acquire exclusive write locks on accessed rows).

However, the definitions above have been criticized as being ambiguous, and as not accurately reflecting the isolation provided by many databases:

:This paper shows a number of weaknesses in the anomaly approach to defining isolation levels. The three ANSI phenomena are ambiguous, and even in their loosest interpretations do not exclude some anomalous behavior ... This leads to some counter-intuitive results. In particular, lock-based isolation levels have different characteristics than their ANSI equivalents. This is disconcerting because commercial database systems typically use locking implementations. Additionally, the ANSI phenomena do not distinguish between a number of types of isolation level behavior that are popular in commercial systems.<ref name="sql-isolation">
{{cite web
| url = http://www.cs.umb.edu/~poneil/iso.pdf
| title = A Critique of ANSI SQL Isolation Levels
| accessdate = 29 July 2012 }}
</ref>

There are also other criticisms concerning ANSI SQL's isolation definition, in that it encourages implementors to do "bad things":

:... it relies in subtle ways on an assumption that a locking schema is used for concurrency control, as opposed to an optimistic or multi-version concurrency scheme. This implies that the proposed semantics are ''ill-defined''.<ref>{{cite web
| accessdate = 2010-03-09
| publisher = DataStax
| location = www.DataStax.com
| title = Customer testimonials (SimpleGeo, CLOUDSTOCK 2010)
| author = salesforce
| date = 2010-12-06
| url = https://www.youtube.com/v/7J61pPG9j90?version=3
| quote = (see above at about 13:30 minutes of the webcast!)}}</ref>

==Read phenomena==
The ANSI/ISO standard SQL 92 refers to three different ''read phenomena'' when Transaction 1 reads data that Transaction 2 might have changed.

In the following examples, two transactions take place. In the first, Query 1 is performed. Then, in the second transaction, Query 2 is performed and committed. Finally, in the first transaction, Query 1 is performed again.

The queries use the following data table:

{|class="wikitable"
|+ users
! id !! name !! age
|-
| 1  || Joe  || 20
|-
| 2  || Jill || 25
|}

===Dirty reads===
A ''dirty read'' (aka ''uncommitted dependency'') occurs when a transaction is allowed to read data from a row that has been modified by another running transaction and not yet committed.

Dirty reads work similarly to [[Isolation (database systems)#Non-repeatable reads|non-repeatable reads]]; however, the second transaction would not need to be committed for the first query to return a different result. The only thing that may be prevented in the READ UNCOMMITTED isolation level is updates appearing out of order in the results; that is, earlier updates will always appear in a result set before later updates.

In our example, Transaction 2 changes a row, but does not commit the changes.  Transaction 1 then reads the uncommitted data.  Now if Transaction 2 rolls back its changes (already read by Transaction 1) or updates different changes to the database, then the view of the data may be wrong in the records of Transaction 1.

{|style="font-size: 94%;"
|-
! Transaction 1
! Transaction 2
|-
|<source lang="sql">
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 20 */
</source>
|
|-
|
|<source lang="sql">
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
/* No commit here */
</source>
|-
|<source lang="sql">
/* Query 1 */
SELECT age FROM users WHERE id = 1;
/* will read 21 */
</source>
|
|-
|
|<source lang="sql">
ROLLBACK; /* lock-based DIRTY READ */
</source>
|}

But in this case no row exists that has an id of 1 and an age of 21.

===Non-repeatable reads===
A ''non-repeatable read'' occurs, when during the course of a transaction, a row is retrieved twice and the values within the row differ between reads.

''Non-repeatable reads'' phenomenon may occur in a lock-based concurrency control method when read locks are not acquired when performing a [[Select (SQL)|SELECT]], or when the acquired locks on affected rows are released as soon as the SELECT operation is performed.  Under the [[multiversion concurrency control]] method, ''non-repeatable reads'' may occur when the requirement that a transaction affected by a [[commit conflict]] must roll back is relaxed.

{|style="font-size: 94%;"
|-
! Transaction 1
! Transaction 2
|-
|<source lang="sql">
/* Query 1 */
SELECT * FROM users WHERE id = 1;
</source>
|
|-
|
|<source lang="sql">
/* Query 2 */
UPDATE users SET age = 21 WHERE id = 1;
COMMIT; /* in multiversion concurrency
   control, or lock-based READ COMMITTED */
</source>
|-
|<source lang="sql">
/* Query 1 */
SELECT * FROM users WHERE id = 1;
COMMIT; /* lock-based REPEATABLE READ */
</source>
|}

In this example, Transaction 2 commits successfully, which means that its changes to the row with id 1 should become visible. However, Transaction 1 has already seen a different value for ''age'' in that row. At the SERIALIZABLE and REPEATABLE READ isolation levels, the DBMS must return the old value for the second SELECT. At READ COMMITTED and READ UNCOMMITTED, the DBMS may return the updated value; this is a non-repeatable read.

There are two basic strategies used to prevent non-repeatable reads. The first is to delay the execution of Transaction 2 until Transaction 1 has committed or rolled back. This method is used when locking is used, and produces the serial [[Schedule (computer science)|schedule]] '''T1, T2'''. A serial schedule exhibits ''repeatable reads'' behaviour.

In the other strategy, as used in ''[[multiversion concurrency control]]'', Transaction 2 is permitted to commit first, which provides for better concurrency. However, Transaction 1, which commenced prior to Transaction 2, must continue to operate on a past version of the database&nbsp;— a snapshot of the moment it was started. When Transaction 1 eventually tries to commit, the DBMS checks if the result of committing Transaction 1 would be equivalent to the schedule '''T1, T2'''. If it is, then Transaction 1 can proceed. If it cannot be seen to be equivalent, however, Transaction 1 must roll back with a serialization failure.

Using a lock-based concurrency control method, at the REPEATABLE READ isolation mode, the row with ID = 1 would be locked, thus blocking Query 2 until the first transaction was committed or rolled back. In READ COMMITTED mode, the second time Query 1 was executed, the age would have changed.

Under multiversion concurrency control, at the SERIALIZABLE isolation level, both SELECT queries see a snapshot of the database taken at the start of Transaction 1. Therefore, they return the same data. However, if Transaction 1 then attempted to UPDATE that row as well, a serialization failure would occur and Transaction 1 would be forced to roll back.

At the READ COMMITTED isolation level, each query sees a snapshot of the database taken at the start of each query. Therefore, they each see different data for the updated row. No serialization failure is possible in this mode (because no promise of serializability is made), and Transaction 1 will not have to be retried.

===Phantom reads===
A ''phantom read'' occurs when, in the course of a transaction, two identical queries are executed, and the collection of rows returned by the second query is different from the first.

This can occur when ''[[range locks]]'' are not acquired on performing a ''[[Select (SQL)|SELECT]] ... WHERE'' operation.
The ''phantom reads'' anomaly is a special case of ''Non-repeatable reads'' when Transaction 1 repeats a ranged ''SELECT ... WHERE'' query and, between both operations, Transaction 2 creates (i.e. [[INSERT]]) new rows (in the target table) which fulfill that ''WHERE'' clause.

{|style="font-size: 95%;"
|-
! Transaction 1
! Transaction 2
|-
|<source lang="sql">
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
</source>
|
|-
|
|<source lang="sql">
/* Query 2 */
INSERT INTO users(id,name,age) VALUES ( 3, 'Bob', 27 );
COMMIT;
</source>
|-
|<source lang="sql">
/* Query 1 */
SELECT * FROM users
WHERE age BETWEEN 10 AND 30;
COMMIT;
</source>
|
|}

Note that Transaction 1 executed the same query twice. If the highest level of isolation were maintained, the same set of rows should be returned both times, and indeed that is what is mandated to occur in a database operating at the SQL SERIALIZABLE isolation level. However, at the lesser isolation levels, a different set of rows may be returned the second time.

In the SERIALIZABLE isolation mode, Query 1 would result in all records with age in the range 10 to 30 being locked, thus Query 2 would block until the first transaction was committed. In REPEATABLE READ mode, the range would not be locked, allowing the record to be inserted and the second execution of Query 1 to include the new row in its results.

==Isolation Levels, Read Phenomena and Locks==

===Isolation Levels vs Read Phenomena===
{|class="wikitable"
! Isolation level !! Dirty reads !! Non-repeatable reads !! Phantoms
|-
| Read Uncommitted  || may occur || may occur || may occur
|-
| Read Committed  || - || may occur || may occur
|-
| Repeatable Read || - || - || may occur
|-
| Serializable || - || - || -
|}

Anomaly Serializable is not the same as Serializable. That is, it is necessary, but not sufficient that a Serializable schedule should be free of all three phenomena types.<ref name="sql-isolation"/>

"may occur" means that the isolation level suffers that phenomenon, while "-" means that it does not suffer it.

===Isolation Levels vs Lock Duration ===
{{Citation needed|reason=This is a still a mess. Write operations always place locks until commit, whatever isolation level is used. Queries (selects/reads) never place any kind of lock under Read Uncommitted isolation level. Locks placed by read operations (the only operations that are affected by the isolation level used) should not be referred as Read and Range, but as Data Locks and Predicate Locks. Write operations also place Data and Predicate locks, always until commit, whatever isolation level is used.|date=January 2014}}

In lock-based concurrency control, isolation level determines the duration that locks are held.<br>  '''"C"''' - Denotes that locks are held until the transaction commits.<br>
'''"S"''' - Denotes that the locks are held only during the currently executing statement.  Note that if locks are released after a statement, the underlying data could be changed by another transaction before the current transaction commits, thus creating a violation.

{|class="wikitable"
! Isolation level !! Write Operation !! Read Operation !! Range Operation (...where...)
|-
| Read Uncommitted  || S || S || S
|-
| Read Committed  || C || S || S
|-
| Repeatable Read || C || C || S
|-
| Serializable || C || C || C
|}

==See also==
* [[Atomicity (database systems)|Atomicity]]
* [[Consistency (database systems)|Consistency]]
* [[Durability (database systems)|Durability]]
* [[Lock (database)]]
* [[Optimistic concurrency control]]
* [[Relational Database Management System]]
* [[Snapshot isolation]]

==References==
{{Reflist}}

==External links==
* [http://docs.oracle.com/cd/B12037_01/server.101/b10743/toc.htm  Oracle® Database Concepts], [http://docs.oracle.com/cd/B12037_01/server.101/b10743/consist.htm#sthref1919 chapter 13 Data Concurrency and Consistency, Preventable Phenomena and Transaction Isolation Levels]
* [http://docs.oracle.com/cd/B19306_01/server.102/b14200/toc.htm Oracle® Database SQL Reference], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10.htm#i2068385 chapter 19 SQL Statements: SAVEPOINT to UPDATE], [http://docs.oracle.com/cd/B19306_01/server.102/b14200/statements_10005.htm#i2067247 SET TRANSACTION]
<!-- representations in api: java -->
* in [[Java Database Connectivity|JDBC]]: [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#field_summary Connection constant fields], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#getTransactionIsolation() Connection.getTransactionIsolation()], [http://docs.oracle.com/javase/7/docs/api/java/sql/Connection.html#setTransactionIsolation(int) Connection.setTransactionIsolation(int)]
* in [[Spring Framework]]: [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Transactional.html @Transactional], [http://static.springsource.org/spring/docs/current/javadoc-api/org/springframework/transaction/annotation/Isolation.html Isolation]
<!-- representations in api: .NET_Framework -->
* [http://www.bailis.org/blog/when-is-acid-acid-rarely/ P.Bailis. When is "ACID" ACID? Rarely]

{{Authority control}}

{{DEFAULTSORT:Isolation (Database Systems)}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:125
<=====title=====>:
Semantic translation
<=====text=====>:
{{Unreferenced|date=December 2009}}
'''Semantic translation''' is the process of using [[semantic]] information to aid in the translation of data in one representation or [[data model]] to another representation or data model.  Semantic translation takes advantage of semantics that associate meaning with individual [[data element]]s in one [[data dictionary|dictionary]] to create an equivalent meaning in a second system.

An example of semantic translation is the conversion of [[XML]] data from one data model to a second data model using formal [[ontologies]] for each system such as the [[Web Ontology Language]] (OWL).  This is frequently required by [[intelligent agents]] that wish to perform searches on remote computer systems that use different data models to store their data elements.  The process of allowing a single user to search multiple systems with a single search request is also known as [[federated search]].

Semantic translation should be differentiated from [[data mapping]] tools that do simple one-to-one translation of data from one system to another without actually associating meaning with each data element.

Semantic translation requires that data elements in the source and destination systems have "semantic mappings" to a central registry or registries of data elements. The simplest mapping is of course where there is equivalence.
There are three types of [[Semantic equivalence]]:

* '''[[Class (computer science)|Class]] Equivalence'''{{Anchor|Class equivalence}} - indicating that class or "concepts" are equivalent.  For example: "Person" is the same as "Individual"
* '''[[Relation (mathematics)|Property]] Equivalence'''{{Anchor|Property equivalence}} - indicating that two properties are equivalent.  For example: "PersonGivenName" is the same as "FirstName"
* '''[[Instance (computer science)|Instance]] Equivalence'''{{Anchor|Instance equivalence}} - indicating that two individual instances of objects are equivalent.  For example: "Dan Smith" is the same person as "Daniel Smith"

Semantic translation is very difficult if the terms in a particular data model do not have direct one-to-one mappings to data elements in a foreign data model. In that situation an alternative approach must be used to find mappings from the original data to the foreign data elements.  This problem can be alleviated by centralized metadata registries that use the ISO-11179 standards such as the [[National Information Exchange Model]] (NIEM).

==See also==
* [[Data mapping]]
* [[Semantic heterogeneity]]
* [[Semantic mapper]]
* [[Federated search]]
* [[Intelligent agents]]
* [[ISO/IEC 11179]]
* [[National Information Exchange Model]]
* [[Semantic Web]]
* [[Vocabulary-based transformation]]
* [[Web Ontology Language]]

{{DEFAULTSORT:Semantic Translation}}
[[Category:Data management]]
[[Category:Enterprise application integration]]
<=====doc_Id=====>:128
<=====title=====>:
Flat file database
<=====text=====>:
{{distinguish|Flat file system}}
{{refimprove|date=March 2015}}
{{originalresearch|date=March 2015}}

[[Image:Flat File Model.svg|thumb|280px|Example of a flat file model<ref name="USDT01">[http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf Data Integration Glossary] {{webarchive |url=https://web.archive.org/web/20090320001015/http://knowledge.fhwa.dot.gov/tam/aashto.nsf/All+Documents/4825476B2B5C687285256B1F00544258/$FILE/DIGloss.pdf |date=March 20, 2009 }}, U.S. Department of Transportation, August 2001.</ref>]]

A '''flat file database''' is a [[database]] which is stored on its host computer system as an ordinary unstructured file called a "flat file". To access the structure of the data and manipulate it, the file must be read in its entirety into the computer's memory. Upon completion of the database operations, the file is again written out in its entirety to the host's file system. In this stored mode the database is said to be "flat", meaning that it has no structure for indexing and there are usually no structural relationships between the records. A flat file can be a [[plain text]] file or a [[binary file]].

The term has generally implied a small, simple database. As computer memory has become cheaper, more sophisticated databases can now be entirely held in memory for faster access. These newer databases would not generally be referred to as flat-file databases.

==Overview==

Plain text files usually contain one [[Record (computer science)|record]] per line,<ref>{{Citation
 | last = Fowler
 | first = Glenn
 | year = 1994
 | title = cql: Flat file database query language
 | periodical = WTEC'94: Proceedings of the USENIX Winter 1994 Technical Conference on USENIX Winter 1994 Technical Conference
 | url = http://www.research.att.com/~astopen/publications/cql-1994.pdf
}}</ref> There are different conventions for depicting data. In [[comma-separated values]] and [[delimiter-separated values]] files, [[field (computer science)|field]]s can be separated by [[delimiters]] such as [[Comma-separated values|comma]] or [[Tab separated values|tab]] characters. In other cases, each field may have a fixed length; short values may be padded with [[space character]]s. Extra formatting may be needed to avoid [[delimiter collision]]. More complex solutions are [[markup language]]s and [[programming language]]s.

Using delimiters incurs some [[Computational overhead|overhead]] in locating them every time they are processed (unlike fixed-width formatting), which may have [[Computer performance|performance]] implications. However, use of character delimiters (especially commas) is also a crude form of [[data compression]] which may assist overall performance by reducing data volumes&nbsp;— especially for [[data transmission]] purposes. Use of character delimiters which include a length component ([[String literal#Declarative notation|Declarative notation]]) is comparatively rare but vastly reduces the overhead associated with locating the extent of each field.

Typical examples of flat files are <code>[[/etc/passwd]]</code> and <code>[[/etc/group]]</code> on [[Unix-like]] operating systems. Another example of a flat file is a name-and-address list with the fields ''Name'', ''Address'', and ''Phone Number''.

A list of names, addresses, and phone numbers written by hand on a sheet of paper is a flat file database. This can also be done with any [[typewriter]] or [[word processor]]. A [[spreadsheet]] or [[text editor]] program may be used to implement a flat file database, which may then be printed or used [[online]] for improved search capabilities.

==History==
[[Herman Hollerith]] conceived the idea that data could be represented by holes punched in paper cards then tabulated by machine. He implemented this concept for the [[United States Census Bureau|US Census Bureau]]; thus the [[1890 United States Census]] processing created the first database—consisting of thousands of boxes full of [[punched card]]s.

Hollerith's enterprise grew into the computer giant [[IBM]], which dominated the data processing market for most of the 20th century. IBM's fixed-length field, 80-column punch cards became the ubiquitous means of inputting  electronic data until the 1970s.

In the 1980s, configurable flat-file database [[computer application]]s were popular on [[DOS]] and the [[Apple Macintosh|Macintosh]]. These programs were designed to make it easy for individuals to design and use their own databases, and were almost on par with [[word processors]] and [[spreadsheet]]s in popularity.{{citation needed|date=September 2011}} Examples of flat-file database products were early versions of [[FileMaker]] and the [[shareware]] [[PC-File]].  Some of these, like [[dBase II]], offered limited [[relational database|relational]] capabilities, allowing some data to be shared between files.

In the 2010s flat file databases were used in [[content management system]]s. Instead of using a database, web developers were able to change the content directly in the file system or at the command line.

===Contemporary implementations===
FairCom's [[c-tree]] is an example of a modern enterprise-level solution, and [[spreadsheet]] software and [[text editor]]s can be used for this purpose.  [[WebDNA]] is a scripting language designed for the World Wide Web, with a hybrid flat file in-memory database system making it easy to build resilient database-driven websites. With the in-memory concept, WebDNA searches and database updates are almost realtime while the data is stored as text files within the website itself. Otherwise, flat file database is implemented in [[Microsoft Works]] and [[Apple Works]]. Over time, products like [[Borland]]'s Paradox, and [[Microsoft]]'s [[Microsoft Access|Access]] started offering some relational capabilities, as well as built-in programming languages.  Database Management Systems ([[DBMS]]) like [[MySQL]] or [[Oracle database|Oracle]] generally require programmers to build applications.

Faceless flat file database engines are used internally by [[Mac OS X]], [[Firefox]], and other computer software to store configuration data. Programs to manage collections of books or appointments and [[address book]] are essentially single-purpose flat file database applications, allowing users to store and retrieve information from flat files using a predefined set of fields.

==Data transfer operations==
Flat files are used not only as data storage tools in DB and [[Content_management_system|CMS]] systems, but also as data transfer tools to remote servers (in which case they become known as information streams).

In recent years, this latter implementation has been replaced with [[XML]] files, which not only contain but also describe the data.  Those still using flat files to transfer information are mainframes employing specific procedures which are too expensive to modify.

One criticism often raised against the XML format as a way to perform mass data transfer operations is that file size is significantly larger than that of flat files, which is generally reduced to the bare minimum.  The solution to this problem consists in XML file compression (a solution that applies equally well to flat files), which has nowadays gained [[Efficient XML Interchange|EXI]] standards (i.e., Efficient XML Interchange, which is often used by mobile devices).

It is advisable that transfer data be performed via EXI rather than flat files because defining the compression method is not required, because libraries reading the file contents are readily available, and because there is no need for the two communicating systems to preliminarily establish a protocol describing data properties such as position, alignment, type, and format. However, in those circumstances where the sheer mass of data and/or the inadequacy of legacy systems becomes a problem, the only viable solution remains the use of flat files.  In order to successfully handle those problems connected with data communication, format, validation, control and much else (be it a flat file or an XML file data source), it is advisable to adopt a [[Data Quality Firewall]].

==Terminology==
"Flat file database" may be defined very narrowly, or more broadly. The narrower interpretation is correct in [[Database|database theory]]; the broader covers the term as generally used.

Strictly, a flat file database should consist of nothing but data and, if records vary in length, delimiters. More broadly, the term refers to any database which exists in a single file in the form of rows and columns, with no relationships or links between records and fields except the table structure.

Terms used to describe different aspects of a database and its tools differ from one implementation to the next, but the concepts remain the same. FileMaker uses the term "Find", while MySQL uses the term "Query"; but the concept is the same. FileMaker "files", in version 7 and above, are equivalent to MySQL "databases", and so forth. To avoid confusing the reader, one consistent set of terms is used throughout this article.

However, the basic terms "record" and "field" are used in nearly every flat file database implementation.

==Example database==
The following example illustrates the basic elements of a flat-file database. The [[data]] arrangement consists of a series of columns and rows organized into a [[table (information)|tabular format]]. This specific example uses only one table.

The columns include: ''name'' (a person's name, second column); ''team'' (the name of an athletic team supported by the person, third column); and a numeric ''unique ID'', (used to uniquely identify records, first column).

Here is an example textual representation of the described data:

 id    name    team
 1     Amy     Blues
 2     Bob     Reds
 3     Chuck   Blues
 4     Richard Blues
 5     Ethel   Reds
 6     Fred    Blues
 7     Gilly   Blues
 8     Hank    Reds
 9     Hank    Blues

This type of data representation is quite standard for a flat-file database, although there are some additional considerations that are not readily apparent from the text:
* '''Data types:''' each column in a database table such as the one above is ordinarily restricted to a specific [[data type]]. Such restrictions are usually established by convention, but not formally indicated unless the data is transferred to a [[relational database]] system.
* '''Separated columns:''' In the above example, individual columns are separated using [[Whitespace (computer science)|whitespace]] characters. This is also called indentation or "fixed-width" data formatting. Another common convention is to separate columns using one or more [[delimiter]] characters. More complex solutions are markup and programming languages.
* '''Relational algebra:''' Each row or record in the above table meets the standard definition of a [[tuple]] under [[relational algebra]] (the above example depicts a series of 3-tuples). Additionally, the first row specifies the [[Tuple#Field_names|field names]] that are associated with the values of each row.
* '''Database management system:''' Since the formal operations possible with a text file are usually more limited than desired, the text in the above example would ordinarily represent an intermediary state of the data prior to being transferred into a [[database management system]].

==References==
{{Commons category|Flat file models}}
{{reflist}}

{{Database models}}

{{DEFAULTSORT:Flat File Database}}
[[Category:Data management]]
[[Category:Computer file formats]]
[[Category:Database models]]

[[it:Flat file]]
<=====doc_Id=====>:131
<=====title=====>:
Modular serializability
<=====text=====>:
#REDIRECT [[Global serializability]]

[[Category:Data management]]
[[Category:Databases]]
[[Category:Transaction processing]]
[[Category:Concurrency control]]
<=====doc_Id=====>:134
<=====title=====>:
Data integration
<=====text=====>:
'''Data integration''' involves combining [[data]] residing in different sources and providing users with a unified view of these data.<ref name="refone">
{{cite conference | author=Maurizio Lenzerini | title=Data Integration:  A Theoretical Perspective | booktitle=PODS 2002 | year=2002 | pages=233–246 | url=http://www.dis.uniroma1.it/~lenzerin/homepagine/talks/TutorialPODS02.pdf}}</ref> This process becomes significant in a variety of situations, which include both commercial (when two similar companies need to merge their [[database]]s) and scientific (combining research results from different [[bioinformatics]] repositories, for example) domains.  Data integration appears with increasing frequency as the volume and the need to share existing data [[Information explosion|explodes]].<ref name="DataExplode">{{cite news | author=Frederick Lane | title=IDC: World Created 161 Billion Gigs of Data in 2006 | year=2006 | url=http://www.toptechnews.com/story.xhtml?story_id=01300000E3D0&full_skip=1 }}</ref>  It has become the focus of extensive theoretical work, and numerous open problems remain unsolved.

==History==
[[File:datawarehouse.png|thumb|right|Figure 1: Simple schematic for a data warehouse.  The [[Extract, transform, load|ETL]] process extracts information from the source databases, transforms it and then loads it into the data warehouse.]]

[[File:dataintegration.png|thumb|right|Figure 2: Simple schematic for a data-integration solution.  A system designer constructs a mediated schema against which users can run queries.  The [[virtual database]] interfaces with the source databases via [[Wrapper pattern|wrapper]] code if required.]]

Issues with combining [[heterogeneous]] data sources, often referred to as [[information silo]]s, under a single query interface have existed for some time. In the early 1980s, computer scientists began designing systems for interoperability of heterogeneous databases.<ref>{{cite news | author= John Miles Smith | title= Multibase: integrating heterogeneous distributed database systems | year=1982 | journal=AFIPS '81 Proceedings of the May 4–7, 1981, national computer conference  | pages= 487–499 |url=http://dl.acm.org/citation.cfm?id=1500483|display-authors=etal}}</ref> The first data integration system driven by structured metadata was designed at the [[University of Minnesota]] in 1991, for the [[IPUMS|Integrated Public Use Microdata Series (IPUMS)]]. IPUMS used a [[data warehousing]] approach, which [[Extract, transform, load|extracts, transforms, and loads]] data from heterogeneous sources into a single view [[logical schema|schema]] so data from different sources become compatible.<ref>{{cite news | author= [[Steven Ruggles]], J. David Hacker, and Matthew Sobek | title= Order out of Chaos: The Integrated Public Use Microdata Series | year=1995 | journal=Historical Methods |volume=28 | pages= 33–39}}</ref> By making thousands of population databases interoperable, IPUMS demonstrated the feasibility of large-scale data integration. The data warehouse approach  offers a [[Coupling (computer science)|tightly coupled]] architecture because the data are already physically reconciled in a single queryable repository, so it usually takes little time to resolve queries.<ref>{{cite news | author= Jennifer Widom | title= Research problems in data warehousing | year=1995 | journal=CIKM '95 Proceedings of the fourth international conference on information and knowledge management | pages= 25–30 | url=http://dl.acm.org/citation.cfm?id=221319}}</ref>

The data warehouse approach is less feasible for datasets that are frequently updated, requiring the [[Extract, transform, load|ETL]] process to be continuously re-executed for synchronization.  Difficulties also arise in constructing data warehouses when one has only a query interface to summary data sources and no access to the full data.  This problem frequently emerges when integrating several commercial query services like travel or classified advertisement web applications.

{{As of | 2009}} the trend in data integration favored loosening the coupling between data{{Citation needed|date=June 2009}} and providing a unified query-interface to access real time data over a [[data mediation|mediated]] schema (see figure 2), which allows information to be retrieved directly from original databases. This is consistent with the [[Service-oriented architecture|SOA]] approach popular in that era. This approach relies on mappings between the mediated schema and the schema of original sources, and transform a query into specialized queries to match the schema of the original databases.  Such mappings can be specified in 2 ways : as a mapping from entities in the mediated schema to entities in the original sources (the "[[Global As View]]" (GAV) approach), or as a mapping from entities in the original sources to the mediated schema (the "[[Local As View]]" (LAV) approach).  The latter approach requires more sophisticated inferences to resolve a query on the mediated schema, but makes it easier to add new data sources to a (stable) mediated schema.

{{As of | 2010}} some of the work in data integration research concerns the [[semantic integration]] problem.  This problem addresses not the structuring of the architecture of the integration, but how to resolve [[semantic]] conflicts between heterogeneous data sources.  For example, if two companies merge their databases, certain concepts and definitions in their respective schemas like "earnings" inevitably have different meanings.  In one database it may mean profits in dollars (a floating-point number), while in the other it might represent the number of sales (an integer).  A common strategy for the resolution of such problems involves the use of [[ontology (computer science)|ontologies]] which explicitly define schema terms and thus help to resolve semantic conflicts. This approach represents [[ontology-based data integration]]. On the other hand, the problem of combining research results from different bioinformatics repositories requires bench-marking of the similarities, computed from different data sources, on a single criterion such as positive predictive value. This enables the data sources to be directly comparable and can be integrated even when the natures of experiments are distinct.<ref>{{cite journal| url=http://shubhrasankar.tripod.com/cgi-bin/combiningMultisourceIEEE.pdf  | journal=IEEE Transactions on Biomedical Engineering | title=Combining Multi-Source Information through Functional Annotation based Weighting: Gene Function Prediction in Yeast| author=Shubhra S. Ray| volume = 56 | pages=229–236 | pmid=19272921 | year=2009| issue=2 | doi=10.1109/TBME.2008.2005955|display-authors=etal}}</ref>

{{As of | 2011}} it was determined that current [[data modeling]] methods were imparting data isolation into every [[data architecture]] in the form of islands of disparate data and information silos. This data isolation is an unintended artifact of the data modeling methodology that results in the development of disparate data models. Disparate data models, when instantiated as databases, form disparate databases. Enhanced data model methodologies have been developed to eliminate the data isolation artifact and to promote the development of integrated data models.<ref>{{cite news | author= Michael Mireku Kwakye | title= A Practical Approach To Merging Multidimensional Data Models | year=2011 | url=http://hdl.handle.net/10393/20457 }}</ref><ref>{{cite web | url=http://www.iri.com/pdf/RapidAce-Brochure.pdf  | title=Rapid Architectural Consolidation Engine&nbsp;– The enterprise solution for disparate data models. | year=2011 }}</ref> One enhanced data modeling method recasts data models by augmenting them with structural [[metadata]] in the form of standardized data entities.  As a result of recasting multiple data models, the set of recast data models will now share one or more commonality relationships that relate the structural metadata now common to these data models.  Commonality relationships are a peer-to-peer type of entity relationships that relate the standardized data entities of multiple data models.  Multiple data models that contain the same standard data entity may participate in the same commonality relationship.  When integrated data models are instantiated as databases and are properly populated from a common set of master data, then these databases are integrated.

Since 2011, [[data hub]] approaches have been of greater interest than fully structured (typically relational) Enterprise Data Warehouses. Since 2013, [[data lake]] approaches have risen to the level of Data Hubs. (See all three search terms popularity on Google Trends.<ref>{{cite web |title=Hub Lake and Warehouse search trends|url=https://www.google.com/trends/explore#q=enterprise%20data%20warehouse%2C%20%22data%20hub%22%2C%20%22data%20lake%22&cmpt=q&tz=Etc%2FGMT%2B5}}</ref>) These approaches combine unstructured or varied data into one location, but do not necessarily require an (often complex) master relational schema to structure and define all data in the Hub.

==Example==
Consider a [[web application]] where a user can query a variety of information about cities (such as crime statistics, weather, hotels, demographics, etc.).  Traditionally, the information must be stored in a single database with a single schema.  But any single enterprise would find information of this breadth somewhat difficult and expensive to collect.  Even if the resources exist to gather the data, it would likely duplicate data in existing crime databases, weather websites, and census data.

A data-integration solution may address this problem by considering these external resources as [[materialized view]]s over a [[Virtual database|virtual mediated schema]], resulting in "virtual data integration".  This means application-developers construct a virtual schema—the ''mediated schema''—to best model the kinds of answers their users want.  Next, they design "wrappers" or adapters for each data source, such as the crime database and weather website.  These adapters simply transform the local query results (those returned by the respective websites or databases) into an easily processed form for the data integration solution (see figure 2).  When an application-user queries the mediated schema, the data-integration solution transforms this query into appropriate queries over the respective data sources.  Finally, the virtual database combines the results of these queries into the answer to the user's query.

This solution offers the convenience of adding new sources by simply constructing an adapter or an application software blade for them.  It contrasts with [[Extract, transform, load|ETL]] systems or with a single database solution, which require manual integration of entire new dataset into the system. The virtual ETL solutions leverage [[Virtual database|virtual mediated schema]] to implement data harmonization; whereby the data are copied from the designated "master" source to the defined targets, field by field. Advanced [[data virtualization]] is also built on the concept of object-oriented modeling in order to construct virtual mediated schema or virtual metadata repository, using [[hub and spoke]] architecture.

Each data source is disparate and as such is not designed to support reliable joins between data sources.  Therefore, data virtualization as well as data federation depends upon accidental data commonality to support combining data and information from disparate data sets.  Because of this lack of data value commonality across data sources, the return set may be inaccurate, incomplete, and impossible to validate.

One solution is to recast disparate databases to integrate these databases without the need for [[Extract, transform, load|ETL]]. The recast databases support commonality constraints where referential integrity may be enforced between databases.  The recast databases provide designed data access paths with data value commonality across databases.

==Theory==
The theory of data integration<ref name="refone" /> forms a subset of database theory and formalizes the underlying concepts of the problem in [[first-order logic]].  Applying the theories gives indications as to the feasibility and difficulty of data integration.  While its definitions may appear abstract, they have sufficient generality to accommodate all manner of integration systems,<ref>{{cite web|url=http://link.springer.com/chapter/10.1007/3-540-46093-4_14 |title=A Model Theory for Generic Schema Management}}</ref> including those that include nested relational / XML databases <ref>{{cite web|url=http://www.vldb.org/conf/2006/p67-fuxman.pdf |title=Nested Mappings: Schema Mapping Reloaded }}</ref> and those that treat databases as programs.<ref>{{cite web|url=http://homepages.inf.ed.ac.uk/dts/pub/psi.pdf |title=The Common Framework Initiative for algebraic specification and development of software}}</ref>  Connections to particular databases systems such as Oracle or DB2 are provided by implementation-level technologies such as [[JDBC]] and are not studied at the theoretical level.

===Definitions===
Data integration systems are formally defined as a [[Triple (mathematics)|triple]] <math>\left \langle G,S,M\right \rangle</math> where <math>G</math> is the global (or mediated) schema, <math>S</math> is the heterogeneous set of source schemas, and <math>M</math> is the mapping that maps queries between the source and the global schemas.  Both <math>G</math> and <math>S</math> are expressed in [[formal language|languages]] over [[alphabet (computer science)|alphabets]] composed of symbols for each of their respective [[Relational database|relations]].  The [[Functional predicate|mapping]] <math>M</math> consists of assertions between queries over <math>G</math> and queries over <math>S</math>.  When users pose queries over the data integration system, they pose queries over <math>G</math> and the mapping then asserts connections between the elements in the global schema and the source schemas.

A database over a schema is defined as a set of sets, one for each relation (in a relational database).  The database corresponding to the source schema <math>S</math> would comprise the set of sets of tuples for each of the heterogeneous data sources and is called the ''source database''.  Note that this single source database may actually represent a collection of disconnected databases.  The database corresponding to the virtual mediated schema <math>G</math> is called the ''global database''.  The global database must satisfy the mapping <math>M</math> with respect to the source database.  The legality of this mapping depends on the nature of the correspondence between <math>G</math> and <math>S</math>.  Two popular ways to model this correspondence exist: ''Global as View'' or GAV and ''Local as View'' or LAV.

[[File:GAVLAV.png|thumb|right|Figure 3: Illustration of tuple space of the GAV and LAV mappings.<ref name="refseven">{{cite journal|author=Christoph Koch |title=Data Integration against Multiple Evolving Autonomous Schemata |year=2001 |url=http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |deadurl=yes |archiveurl=https://web.archive.org/web/20070926211342/http://www.csd.uoc.gr/~hy562/Papers/thesis_final.pdf |archivedate=2007-09-26 |df= }}</ref> In GAV, the system is constrained to the set of tuples mapped by the mediators while the set of tuples expressible over the sources may be much larger and richer. In LAV, the system is constrained to the set of tuples in the sources while the set of tuples expressible over the global schema can be much larger. Therefore, LAV systems must often deal with incomplete answers.]]

GAV systems model the global database as a set of [[view (database)|views]] over <math>S</math>.  In this case <math>M</math> associates to each element of <math>G</math> a query over <math>S</math>.  [[Query optimizer|Query processing]] becomes a straightforward operation due to the well-defined associations between <math>G</math> and <math>S</math>.  The burden of complexity falls on implementing mediator code instructing the data integration system exactly how to retrieve elements from the source databases.  If any new sources join the system, considerable effort may be necessary to update the mediator, thus the GAV approach appears preferable when the sources seem unlikely to change.

In a GAV approach to the example data integration system above, the system designer would first develop mediators for each of the city information sources and then design the global schema around these mediators.  For example, consider if one of the sources served a weather website.  The designer would likely then add a corresponding element for weather to the global schema.  Then the bulk of effort concentrates on writing the proper mediator code that will transform predicates on weather into a query over the weather website.  This effort can become complex if some other source also relates to weather, because the designer may need to write code to properly combine the results from the two sources.

On the other hand, in LAV, the source database is modeled as a set of [[view (database)|views]] over <math>G</math>.  In this case <math>M</math> associates to each element of <math>S</math> a query over <math>G</math>.  Here the exact associations between <math>G</math> and <math>S</math> are no longer well-defined.  As is illustrated in the next section, the burden of determining how to retrieve elements from the sources is placed on the query processor.  The benefit of an LAV modeling is that new sources can be added with far less work than in a GAV system, thus the LAV approach should be favored in cases where the mediated schema is less stable or likely to change.<ref name="refone" />

In an LAV approach to the example data integration system above, the system designer designs the global schema first and then simply inputs the schemas of the respective city information sources.  Consider again if one of the sources serves a weather website.  The designer would add corresponding elements for weather to the global schema only if none existed already.  Then programmers write an adapter or wrapper for the website and add a schema description of the website's results to the source schemas.  The complexity of adding the new source moves from the designer to the query processor.

===Query processing===
The theory of query processing in data integration systems is commonly expressed using conjunctive [[Database query language|queries]] and [[Datalog]], a purely declarative [[logic programming]] language.<ref name="reffive">{{cite conference | author=[[Jeffrey D. Ullman]] | title=Information Integration Using Logical Views | booktitle=ICDT 1997 | year=1997 | pages=19–40 | url=http://www-db.stanford.edu/pub/papers/integration-using-views.ps}}</ref>  One can loosely think of a [[conjunctive query]] as a logical function applied to the relations of a database such as "<math>f(A,B)</math> where <math>A < B</math>".  If a tuple or set of tuples is substituted into the rule and satisfies it (makes it true), then we consider that tuple as part of the set of answers in the query.  While formal languages like Datalog express these queries concisely and without ambiguity, common [[SQL]] queries count as conjunctive queries as well.

In terms of data integration, "query containment" represents an important property of conjunctive queries.  A query <math>A</math> contains another query <math>B</math> (denoted <math>A \supset B</math>) if the results of applying <math>B</math> are a subset of the results of applying <math>A</math> for any database.  The two queries are said to be equivalent if the resulting sets are equal for any database.  This is important because in both GAV and LAV systems, a user poses conjunctive queries over a ''virtual'' schema represented by a set of [[view (database)|views]], or "materialized" conjunctive queries.  Integration seeks to rewrite the queries represented by the views to make their results equivalent or maximally contained by our user's query.  This corresponds to the problem of answering queries using views ([[AQUV]]).<ref name="refsix">{{cite conference | author=[[Alon Y. Halevy]] | title=Answering queries using views: A survey | booktitle=The VLDB Journal | year=2001 | pages=270–294 | url=http://www.cs.uwaterloo.ca/~david/cs740/answering-queries-using-views.pdf}}
</ref>

In GAV systems, a system designer writes mediator code to define the query-rewriting.  Each element in the user's query corresponds to a substitution rule just as each element in the global schema corresponds to a query over the source.  Query processing simply expands the subgoals of the user's query according to the rule specified in the mediator and thus the resulting query is likely to be equivalent.  While the designer does the majority of the work beforehand, some GAV systems such as [http://www-db.stanford.edu/tsimmis/ Tsimmis] involve simplifying the mediator description process.

In LAV systems, queries undergo a more radical process of rewriting because no mediator exists to align the user's query with a simple expansion strategy.  The integration system must execute a search over the space of possible queries in order to find the best rewrite.  The resulting rewrite may not be an equivalent query but maximally contained, and the resulting tuples may be incomplete.  {{As of | 2009}} the MiniCon algorithm<ref name="refsix" /> is the leading query rewriting algorithm for LAV data integration systems.

In general, the complexity of query rewriting is [[NP-complete]].<ref name="refsix" />  If the space of rewrites is relatively small this does not pose a problem—even for integration systems with hundreds of sources.

==Tools==
* Alteryx
* Analytics Canvas
* [[Capsenta]]'s Ultrawrap Platform
*[[Cloud Elements]] API Integration
* DataWatch
* [[Denodo|Denodo Platform]]
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]<ref name="dca">M. Haghighat, M. Abdel-Mottaleb, &  W. Alhalabi (2016). [http://ieeexplore.ieee.org/document/7470527/ Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.</ref>
* [[elastic.io]] Integration Platform
* [http://www.hiperfabric.com HiperFabric]
* [[Lavastorm Analytics|Lavastorm]]
* [[Informatica]] Platform
* Oracle Data Integration Services
* ParseKit (enigma.io)
* Paxata
* [[RapidMiner]] Studio
* [[Red Hat]] JBoss Data Virtualization. Community project: teiid.
* [[Microsoft Azure|Azure]] Data Factory (ADF) 
* [[SQL Server Integration Services|SQL Server Integration Services (SSIS)]]
* [http://www.tmmdata.com TMMData]
* [http://www.dataladder.com Data Ladder]

==In the life sciences==
Large-scale questions in science, such as [[global warming]], [[invasive species]] spread, and [[resource depletion]], are increasingly requiring the collection of disparate data sets for [[meta-analysis]]. This type of data integration is especially challenging for ecological and environmental data because [[metadata standards]] are not agreed upon and there are many different data types produced in these fields. [[National Science Foundation]] initiatives such as [[Datanet]] are intended to make data integration easier for scientists by providing [[cyberinfrastructure]] and setting standards. The five funded [[Datanet]] initiatives are [[DataONE]],<ref>{{cite web|author=William Michener |url=https://www.dataone.org |title=DataONE: Observation Network for Earth |publisher=www.dataone.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by William Michener at the [[University of New Mexico]]; The Data Conservancy,<ref>{{cite web|author=Sayeed Choudhury |url=https://dataconservancy.org |title=Data Conservancy |publisher=dataconservancy.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Sayeed Choudhury of [[Johns Hopkins University]]; SEAD: Sustainable Environment through Actionable Data,<ref>{{cite web|author=[[Margaret Hedstrom]] |url=http://sead-data.net/ |title=SEAD Sustainable Environment - Actionable Data |publisher=sead-data.net | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Margaret Hedstrom]] of the [[University of Michigan]]; the DataNet Federation Consortium,<ref>{{cite web|author=[[Reagan Moore]] |url=http://datafed.org/ |title=DataNet Federation Consortium |publisher=datafed.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by Reagan Moore of the [[University of North Carolina]]; and ''Terra Populus'',<ref>{{cite web|author=[[Steven Ruggles]] |url=http://www.terrapop.org/ |title=Terra Populus: Integrated Data on Population and the Environment |publisher=terrapop.org | accessdate=2013-01-19|display-authors=etal}}</ref> led by [[Steven Ruggles]] of the [[University of Minnesota]].  The [[Research Data Alliance]],<ref>{{cite web|author=[[Bill Nichols]] |url=http://rd-alliance.org/ |title=Research Data Alliance |publisher=rd-alliance.org | accessdate=2014-10-01}}</ref> has more recently explored creating global data integration frameworks. The [[OpenPHACTS]] project, funded through the [[European Union]] [[Innovative Medicines Initiative]], built a drug discovery platform by linking datasets from providers such as [[European Bioinformatics Institute]], [[Royal Society of Chemistry]], [[UniProt]], [[WikiPathways]] and [[DrugBank]].

==See also==
{{div col||20em}}
* [[Business semantics management]]
* [[Core data integration]]
* [[Customer data integration]]
* [[Data curation]]
* [[Data fusion]]
* [[Data mapping]]
* [[Data wrangling]]
* [[Database model]]
* [[Dataspaces]]
* [[Edge data integration]]
* [[Enterprise application integration]]
* [[Enterprise architecture framework]]
* [[Enterprise information integration]] (EII)
* [[Enterprise integration]]
* [[Geodi]]: Geoscientific Data Integration
* [[Information integration]]
* [[Information server]]
* [[Information silo]]
* [[Integration Competency Center]]
* [[Integration Consortium]]
* [[JXTA]]
* [[Master data management]]
* [[Object-relational mapping]]
* [[Open Text]]
* [[Schema matching]]
* [[Three schema approach]]
* [[UDEF]]
* [[Web service]]
{{div col end}}

==External links==
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]<ref name="dca"></ref>

==References==
{{reflist|30em}}

{{data}}

{{DEFAULTSORT:Data Integration}}
[[Category:Data management]]
<=====doc_Id=====>:137
<=====title=====>:
Data center
<=====text=====>:
{{Refimprove|date=July 2015}}
[[File:NetworkOperations.jpg|thumb|right|An operation engineer overseeing a network operations control room of a data center]]

A '''data center''' is a facility used to house computer systems and associated components, such as [[telecommunication]]s and [[computer data storage|storage systems]]. It generally includes redundant or backup [[power supply|power supplies]], redundant data communications connections, environmental controls (e.g., air conditioning, fire suppression) and various security devices. Large data centers are industrial scale operations using as much electricity as a small town.<ref name=NYT92212>{{cite news|title=Power, Pollution and the Internet|url=http://www.nytimes.com/2012/09/23/technology/data-centers-waste-vast-amounts-of-energy-belying-industry-image.html|accessdate=2012-09-25|newspaper=The New York Times|date=September 22, 2012|author=James Glanz}}</ref><ref name="ReferenceDC2">"[http://www.academia.edu/6982393/Power_Management_Techniques_for_Data_Centers_A_Survey Power Management Techniques for Data Centers: A Survey]", 2014.</ref>

==History==
[[File:Indiana University Data Center - P1100134.JPG|thumb|[[Indiana University]] Data Center. [[Bloomington, Indiana]]]]
{{Copypaste|section|url=http://www.rackspace.com/blog/datacenter-evolution-1960-to-2000/|date=August 2014}}
{{Unreferenced section|date=August 2014}}
Data centers have their roots in the huge computer rooms of the early ages{{when|date=September 2015}} of the computing industry. Early computer systems, complex to operate and maintain, required a special environment in which to operate. Many cables were necessary to connect all the components, and methods to accommodate and organize these were devised such as standard [[19-inch rack|racks]] to mount equipment, [[raised floor]]s, and [[cable tray]]s (installed overhead or under the elevated floor). A single mainframe required a great deal of power, and had to be cooled to avoid overheating. Security became important&nbsp;– computers were expensive, and were often used for [[military]] purposes. Basic design-guidelines for controlling access to the computer room were therefore devised.

During the boom of the microcomputer industry, and especially during the 1980s, users started to deploy computers everywhere, in many cases with little or no care about operating requirements. However, as information technology (IT) operations started to grow in complexity, organizations grew aware of the need to control IT resources. The advent of [[Unix]] from the early 1970s led to the subsequent proliferation of freely available [[Linux]]-compatible [[personal computer|PC]] operating-systems during the 1990s. These were called "[[Server (computing)|servers]]", as [[timesharing]] operating systems like Unix rely heavily on the [[client-server model]] to facilitate sharing unique resources between multiple users. The availability of inexpensive [[Networking hardware|networking]] equipment, coupled with new standards for network [[structured cabling]], made it possible to use a hierarchical design that put the servers in a specific room inside the company. The use of the term "data center", as applied to specially designed computer rooms, started to gain popular recognition about this time.{{citation needed|date=September 2015}}

The boom of data centers came during the [[dot-com bubble]] of 1997–2000. [[Company|Companies]] needed fast Internet connectivity and non-stop operation to deploy systems and to establish a presence on the Internet. Installing such equipment was not viable for many smaller companies. Many companies started building very large facilities, called '''Internet data centers''' (IDCs), which provide [[customer|commercial client]]s with a range of solutions for systems deployment and operation. New technologies and practices were designed to handle the scale and the operational requirements of such large-scale operations. These practices eventually migrated toward the private data centers, and were adopted largely because of their practical results. Data centers for cloud computing are called '''cloud data centers''' (CDCs). But nowadays, the division of these terms has almost disappeared and they are being integrated into a term "data center".

With an increase in the uptake of [[cloud computing]], business and government organizations scrutinize data centers to a higher degree in areas such as security, availability, environmental impact and adherence to standards. Standards documents from accredited [[professional]] groups, such as the [[Telecommunications Industry Association]], specify the requirements for data-center design. Well-known operational metrics for [[data availability|data-center availability]] can serve to evaluate the [[Business Impact Analysis|commercial impact]] of a disruption. Development continues in operational practice, and also in environmentally-friendly data-center design. Data centers typically cost a lot to build and to maintain.{{citation needed|date=September 2015}}

==Requirements for modern data centers==
[[File:Datacenter-telecom.jpg|thumb|left|Racks of telecommunications equipment in part of a data center]]
{{Copypaste|section|url=https://global.ihs.com/doc_detail.cfm?&rid=TIA&input_doc_number=TIA-942&item_s_key=00414811&item_key_date=860905&input_doc_number=TIA-942&input_doc_title=#abstract|date=August 2014}}
IT operations are a crucial aspect of most organizational operations around the world. One of the main concerns is '''business continuity'''; companies rely on their information systems to run their operations. If a system becomes unavailable, company operations may be impaired or stopped completely. It is necessary to provide a reliable infrastructure for IT operations, in order to minimize any chance of disruption. Information security is also a concern, and for this reason a data center has to offer a secure environment which minimizes the chances of a security breach. A data center must therefore keep high standards for assuring the integrity and functionality of its hosted computer environment. This is accomplished through redundancy of mechanical cooling and power systems (including emergency backup power generators)serving the data center along with fiber optic cables.

The [[Telecommunications Industry Association]]'s Telecommunications Infrastructure Standard for Data Centers<ref>[http://www.tia-942.org TIA-942 Telecommunications Infrastructure Standard for Data Centers]</ref> specifies the minimum requirements for telecommunications infrastructure of data centers and computer rooms including single tenant enterprise data centers and multi-tenant Internet hosting data centers. The topology proposed in this document is intended to be applicable to any size data center.<ref>{{cite web|url=http://www.tiaonline.org/standards/ |title=Archived copy |accessdate=2011-11-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20111106042758/http://www.tiaonline.org/standards/ |archivedate=2011-11-06 |df= }}</ref>

Telcordia GR-3160, ''NEBS Requirements for Telecommunications Data Center Equipment and Spaces'',<ref>[http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&DOCUMENT=GR-3160& GR-3160, NEBS Requirements for Telecommunications Data Center Equipment and Spaces]</ref> provides guidelines for data center spaces within telecommunications networks, and environmental requirements for the equipment intended for installation in those spaces. These criteria were developed jointly by Telcordia and industry representatives. They may be applied to data center spaces housing data processing or Information Technology (IT) equipment. The equipment may be used to:
* Operate and manage a carrier's telecommunication network
* Provide data center based applications directly to the carrier's customers
* Provide hosted applications for a third party to provide services to their customers
* Provide a combination of these and similar data center applications

Effective data center operation requires a balanced investment in both the facility and the housed equipment. The first step is to establish a baseline facility environment suitable for equipment installation. Standardization and modularity can yield savings and efficiencies in the design and construction of telecommunications data centers.

Standardization means integrated building and equipment engineering. Modularity has the benefits of scalability and easier growth, even when planning forecasts are less than optimal. For these reasons, telecommunications data centers should be planned in repetitive building blocks of equipment, and associated power and support (conditioning) equipment when practical. The use of dedicated centralized systems requires more accurate forecasts of future needs to prevent expensive over construction, or perhaps worse&nbsp;— under construction that fails to meet future needs.

The "lights-out" data center, also known as a darkened or a [[dark data]] center, is a data center that, ideally, has all but eliminated the need for direct access by personnel, except under extraordinary circumstances. Because of the lack of need for staff to enter the data center, it can be operated without lighting. All of the devices are accessed and managed by remote systems, with automation programs used to perform unattended operations. In addition to the energy savings, reduction in staffing costs and the ability to locate the site further from population centers, implementing a lights-out data center reduces the threat of malicious attacks upon the infrastructure.<ref>{{cite book | first=Victor | last=Kasacavage | year=2002 | page=227 | title=Complete book of remote access: connectivity and security | series=The Auerbach Best Practices Series | publisher=CRC Press | isbn=0-8493-1253-1
}}</ref><ref>{{cite book |author1=Burkey, Roxanne E. |author2=Breakfield, Charles V. | year=2000 | title=Designing a total data solution: technology, implementation and deployment | page=24 | series=Auerbach Best Practices | publisher=CRC Press | isbn=0-8493-0893-3 }}</ref>

There is a trend to modernize data centers in order to take advantage of the performance and energy efficiency increases of newer IT equipment and capabilities, such as [[cloud computing]]. This process is also known as data center transformation.<ref name="mspmentor.net">Mukhar, Nicholas. "HP Updates Data Center Transformation Solutions," August 17, 2011 [http://www.mspmentor.net/2011/08/17/hp-updates-data-transformation-solutions/]</ref>

Organizations are experiencing rapid IT growth but their data centers are aging. Industry research company [[International Data Corporation]] (IDC) puts the average age of a data center at nine years old.<ref name="mspmentor.net"/> [[Gartner]], another research company says data centers older than seven years are obsolete.<ref>{{cite web|url=http://www.forbes.com/2010/03/12/cloud-computing-ibm-technology-cio-network-data-centers.html |title=Sperling, Ed. "Next-Generation Data Centers," Forbes, March 15. 2010 |publisher=Forbes.com |date= |accessdate=2013-08-30}}</ref>

In May 2011, data center research organization [[Uptime Institute]] reported that 36 percent of the large companies it surveyed expect to exhaust IT capacity within the next 18 months.<ref>Niccolai, James. "Data Centers Turn to Outsourcing to Meet Capacity Needs," CIO.com, May 10, 2011 [http://www.cio.com/article/681897/Data_Centers_Turn_to_Outsourcing_to_Meet_Capacity_Needs]</ref>

Data center transformation takes a step-by-step approach through integrated projects carried out over time. This differs from a traditional method of data center upgrades that takes a serial and siloed approach.<ref>Tang, Helen. "Three Signs it's time to transform your data center," August 3, 2010, Data Center Knowledge [http://www.datacenterknowledge.com/archives/2010/08/03/three-signs-it%E2%80%99s-time-to-transform-your-data-center/]</ref> The typical projects within a data center transformation initiative include standardization/consolidation, virtualization, [[automation]] and security.
* Standardization/consolidation: The purpose of this project is to reduce the number of data centers a large organization may have. This project also helps to reduce the number of hardware, software platforms, tools and processes within a data center. Organizations replace aging data center equipment with newer ones that provide increased capacity and performance. Computing, networking and management platforms are standardized so they are easier to manage.<ref name="datacenterknowledge.com">Miller, Rich. "Complexity: Growing Data Center Challenge," Data Center Knowledge, May 16, 2007
[http://www.datacenterknowledge.com/archives/2007/05/16/complexity-growing-data-center-challenge/]</ref>
* Virtualize: There is a trend to use IT virtualization technologies to replace or consolidate multiple data center equipment, such as servers. Virtualization helps to lower capital and operational expenses,<ref>Sims, David. "Carousel's Expert Walks Through Major Benefits of Virtualization," TMC Net, July 6, 2010
[http://virtualization.tmcnet.com/topics/virtualization/articles/193652-carousels-expert-walks-through-major-benefits-virtualization.htm]</ref> and reduce energy consumption.<ref>Delahunty, Stephen. "The New urgency for Server Virtualization," InformationWeek, August 15, 2011. [http://www.informationweek.com/news/government/enterprise-architecture/231300585]</ref> Virtualization technologies are also used to create virtual desktops, which can then be hosted in data centers and rented out on a subscription basis.<ref>{{cite web|title=HVD: the cloud's silver lining|url=http://www.intrinsictechnology.co.uk/FileUploads/HVD_Whitepaper.pdf|publisher=Intrinsic Technology|accessdate=2012-08-30}}</ref>  Data released by investment bank Lazard Capital Markets reports that 48 percent of enterprise operations will be virtualized by 2012. Gartner views virtualization as a catalyst for modernization.<ref>Miller, Rich. "Gartner: Virtualization Disrupts Server Vendors," Data Center Knowledge, December 2, 2008 [http://www.datacenterknowledge.com/archives/2008/12/02/gartner-virtualization-disrupts-server-vendors/]</ref>
* Automating: Data center automation involves automating tasks such as [[provisioning]], configuration, [[Patch (computing)|patching]], release management and compliance. As enterprises suffer from few skilled IT workers,<ref name="datacenterknowledge.com"/> automating tasks make data centers run more efficiently.
* Securing: In modern data centers, the security of data on virtual systems is integrated with existing security of physical infrastructures.<ref>Ritter, Ted. Nemertes Research, "Securing the Data-Center Transformation Aligning Security and Data-Center Dynamics," [http://lippisreport.com/2011/05/securing-the-data-center-transformation-aligning-security-and-data-center-dynamics/]</ref> The security of a modern data center must take into account physical security, network security, and data and user security.

==Carrier neutrality==
Today many data centers are run by [[Internet service provider]]s solely for the purpose of hosting their own and third party [[Server (computing)|servers]].

However traditionally data centers were either built for the sole use of one large company, or as [[carrier hotel]]s or [[Network-neutral data center]]s.

These facilities enable interconnection of carriers and act as regional fiber hubs serving local business in addition to hosting content [[Server (computing)|servers]].

==Data center tiers==
<!-- linked from [[data availability]] -->
The [[Telecommunications Industry Association]] is a trade association accredited by ANSI (American National Standards Institute). In 2005 it published [http://global.ihs.com/doc_detail.cfm?currency_code=USD&customer_id=2125452B2C0A&oshid=2125452B2C0A&shopping_cart_id=292558332C4A2020495A4D3B200A&country_code=US&lang_code=ENGL&item_s_key=00414811&item_key_date=940819&input_doc_number=TIA-942&input_doc_title= ANSI/TIA-942], Telecommunications Infrastructure Standard for Data Centers, which defined four levels (called tiers) of data centers in a thorough, quantifiable manner. TIA-942 was amended in 2008 and again in 2010. ''TIA-942:Data Center Standards Overview'' describes the requirements for the data center infrastructure. The simplest is a Tier 1 data center, which is basically a [[server room]], following basic guidelines for the installation of computer systems. The most stringent level is a Tier 4 data center, which is designed to host mission critical computer systems, with fully redundant subsystems and compartmentalized security zones controlled by [[biometric]] access controls methods. Another consideration is the placement of the data center in a subterranean context, for data security as well as environmental considerations such as cooling requirements.<ref>A ConnectKentucky article mentioning Stone Mountain Data Center Complex {{cite web|title=Global Data Corp. to Use Old Mine for Ultra-Secure Data Storage Facility|url=http://connectkentucky.org/_documents/connected_fall_FINAL.pdf|format=PDF|publisher=ConnectKentucky|accessdate=2007-11-01|date=2007-11-01}}</ref>

The German Datacenter star audit program uses an auditing process to certify 5 levels of "gratification" that affect Data Center criticality.

Independent from the ANSI/TIA-942 standard, the [[Uptime Institute]], a think tank and professional-services organization based in [[Santa Fe, New Mexico|Santa Fe]], [[New Mexico]], has defined its own four levels. The levels describe the availability of data from the hardware at a location. The higher the tier, the greater the availability. The levels are:
<ref>A document from the Uptime Institute describing the different tiers (click through the download page) {{cite web
 |title=Data Center Site Infrastructure Tier Standard: Topology 
 |url=http://uptimeinstitute.org/index.php?option=com_docman&task=doc_download&gid=82 
 |format=PDF 
 |publisher=Uptime Institute 
 |accessdate=2010-02-13 
 |date=2010-02-13 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20100613072610/http://uptimeinstitute.org/index.php?option=com_docman&task=doc_download&gid=82 
 |archivedate=2010-06-13 
 |df= 
}}</ref>
<ref>The rating guidelines from the Uptime Institute {{cite web
 |title=Data Center Site Infrastructure Tier Standard: Topology 
 |url=http://professionalservices.uptimeinstitute.com/UIPS_PDF/TierStandard.pdf 
 |format=PDF 
 |publisher=Uptime Institute 
 |accessdate=2010-02-13 
 |date=2010-02-13 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20091007121511/http://professionalservices.uptimeinstitute.com:80/UIPS_PDF/TierStandard.pdf 
 |archivedate=2009-10-07 
 |df= 
}}</ref>

{| class="wikitable"
|-
! Tier Level
! Requirements
|-
! 1
|
* Single non-redundant distribution path serving the IT equipment
* Non-redundant capacity components
* Basic site infrastructure with expected availability of 99.671%
|-
! 2
|
* Meets or exceeds all Tier 1 requirements
* Redundant site infrastructure capacity components with expected availability of 99.741%
|-
! 3
|
* Meets or exceeds all Tier 2 requirements
* Multiple independent distribution paths serving the IT equipment
* All IT equipment must be dual-powered and fully compatible with the topology of a site's architecture
* Concurrently maintainable site infrastructure with expected availability of 99.982%
|-
! 4
|
* Meets or exceeds all Tier 3 requirements
* All cooling equipment is independently dual-powered, including chillers and heating, ventilating and air-conditioning (HVAC) systems
* Fault-tolerant site infrastructure with electrical power storage and distribution facilities with expected availability of 99.995%
|}

The difference between 99.671%, 99.741%, 99.982%, and 99.995%, while seemingly nominal, could be significant depending on the application.

Whilst no down-time is ideal, the tier system allows for unavailability of services as listed below over a period of one year (525,600 minutes):
* Tier 1 (99.671%) status would allow 1729.224 minutes or 28.817 hours
* Tier 2 (99.741%) status would allow 1361.304 minutes or 22.688 hours
* Tier 3 (99.982%) status would allow 94.608 minutes or 1.5768 hours
* Tier 4 (99.995%) status would allow 26.28 minutes or 0.438 hours

The Uptime Institute also classifies the tiers in different categories: design documents, constructed facility, operational sustainability<ref name="uptimeinstitute">{{cite web|url=http://uptimeinstitute.com/TierCertification/|title=Uptime Institute - Tier Certification|publisher=uptimeinstitute.com|accessdate=2014-08-27}}</ref>

==Design considerations==
[[File:Rack001.jpg|thumb|right|A typical server rack, commonly seen in [[colocation center|colocation]]]]
A data center can occupy one room of a building, one or more floors, or an entire building. Most of the equipment is often in the form of servers mounted in [[19 inch rack]] cabinets, which are usually placed in single rows forming corridors (so-called aisles) between them. This allows people access to the front and rear of each cabinet. Servers differ greatly in size from [[Rack unit|1U servers]] to large freestanding storage silos which occupy many square feet of floor space. Some equipment such as [[mainframe computer]]s and [[computer storage|storage]] devices are often as big as the racks themselves, and are placed alongside them. Very large data centers may use [[intermodal container|shipping containers]] packed with 1,000 or more servers each;<ref>{{cite web|url=https://www.youtube.com/watch?v=zRwPSFpLX8I|title=Google Container Datacenter Tour (video)}}</ref> when repairs or upgrades are needed, whole containers are replaced (rather than repairing individual servers).<ref>{{cite web| title=Walking the talk: Microsoft builds first major container-based data center| url=http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=9075519| archiveurl=https://web.archive.org/web/20080612193106/http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=9075519| archivedate=2008-06-12| accessdate=2008-09-22}}</ref>

Local building codes may govern the minimum ceiling heights.

===Design programming===
Design programming, also known as architectural programming, is the process of researching and making decisions to identify the scope of a design project.<ref>Cherry, Edith. "Architectural Programming: Introduction", Whole Building Design Guide, Sept. 2, 2009</ref> Other than the architecture of the building itself there are three elements to design programming for data centers: facility topology design (space planning), engineering infrastructure design (mechanical systems such as cooling and electrical systems including power) and technology infrastructure design (cable plant). Each will be influenced by performance assessments and modelling to identify gaps pertaining to the owner's performance wishes of the facility over time.

Various vendors who provide data center design services define the steps of data center design slightly differently, but all address the same basic aspects as given below.

===Modeling criteria===
Modeling criteria are used to develop future-state scenarios for space, power, cooling, and costs in the data center.<ref>Mullins, Robert. "Romonet Offers Predictive Modelling Tool For Data Center Planning", Network Computing, June 29, 2011 [http://www.networkcomputing.com/data-center/231000669]</ref> The aim is to create a master plan with parameters such as number, size, location, topology, IT floor system layouts, and power and cooling technology and configurations. The purpose of this is to allow for efficient use of the existing mechanical and electrical systems and also growth in the existing data center without the need for developing new buildings and further upgrading of incoming power supply.

===Design recommendations===
Design recommendations/plans generally follow the modelling criteria phase. The optimal technology infrastructure is identified and planning criteria are developed, such as critical power capacities, overall data center power requirements using an agreed upon PUE (power utilization efficiency), mechanical cooling capacities, kilowatts per cabinet, raised floor space, and the resiliency level for the facility.

===Conceptual design===
Conceptual designs embody the design recommendations or plans and should take into account "what-if" scenarios to ensure all operational outcomes are met in order to future-proof the facility. Conceptual floor layouts should be driven by IT performance requirements as well as lifecycle costs associated with IT demand, energy efficiency, cost efficiency and availability. Future-proofing will also include expansion capabilities, often provided in modern data centers through modular designs.  These allow for more raised floor space to be fitted out in the data center whilst utilising the existing major electrical plant of the facility.

===Detailed design===
Detailed design is undertaken once the appropriate conceptual design is determined, typically including a proof of concept. The detailed design phase should include the detailed architectural, structural, mechanical and electrical information and specification of the facility.  At this stage development of facility schematics and construction documents as well as schematics and performance specification and specific detailing of all technology infrastructure, detailed IT infrastructure design and IT infrastructure documentation are produced.

===Mechanical engineering infrastructure designs===
[[File:CRAC Cabinets 2.jpg|thumb|CRAC Air Handler]]
Mechanical engineering infrastructure design addresses mechanical systems involved in maintaining the interior environment of a data center, such as heating, ventilation and air conditioning (HVAC); humidification and dehumidification equipment; pressurization; and so on.<ref name="nxtbook.com">Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today's Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 28. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]</ref>
This stage of the design process should be aimed at saving space and costs, while ensuring business and reliability objectives are met as well as achieving PUE and green requirements.<ref>Data Center Energy Management: Best Practices Checklist: Mechanical, Lawrence Berkeley National Laboratory [http://hightech.lbl.gov/dctraining/strategies/mam.html]</ref> Modern designs include modularizing and scaling IT loads, and making sure capital spending on the building construction is optimized.

===Electrical engineering infrastructure design===
Electrical Engineering infrastructure design is focused on designing electrical configurations that accommodate various reliability requirements and data center sizes. Aspects may include utility service planning; distribution, switching and bypass from power sources; uninterruptable power source (UPS) systems; and more.<ref name="nxtbook.com"/>

These designs should dovetail to energy standards and best practices while also meeting business objectives. Electrical configurations should be optimized and operationally compatible with the data center user's capabilities. Modern electrical design is modular and scalable,<ref>Clark, Jeff. "Hedging Your Data Center Power", The Data Center Journal, Oct. 5, 2011. [http://www.datacenterjournal.com/design/hedging-your-data-center-power/]</ref> and is available for low and medium voltage requirements as well as DC (direct current).

===Technology infrastructure design===
[[File:Under Floor Cable Runs Tee.jpg|thumb|Under Floor Cable Runs]]
Technology infrastructure design addresses the telecommunications cabling systems that run throughout data centers. There are cabling systems for all data center environments, including horizontal cabling, voice, modem, and facsimile telecommunications services, premises switching equipment, computer and telecommunications management connections, keyboard/video/mouse connections and data communications.<ref>Jew, Jonathan. "BICSI Data Center Standard: A Resource for Today's Data Center Operators and Designers," BICSI News Magazine, May/June 2010, page 30. [http://www.nxtbook.com/nxtbooks/bicsi/news_20100506/#/26]</ref> Wide area, local area, and storage area networks should link with other building signaling systems (e.g. fire, security, power, HVAC, EMS).

===Availability expectations===
The higher the availability needs of a data center, the higher the capital and operational costs of building and managing it. Business needs should dictate the level of availability required and should be evaluated based on characterization of the criticality of IT systems estimated cost analyses from modeled scenarios. In other words, how can an appropriate level of availability best be met by design criteria to avoid financial and operational risks as a result of downtime?
If the estimated cost of downtime within a specified time unit exceeds the amortized capital costs and operational expenses, a higher level of availability should be factored into the data center design. If the cost of avoiding downtime greatly exceeds the cost of downtime itself, a lower level of availability should be factored into the design.<ref>Clark, Jeffrey. "The Price of Data Center Availability—How much availability do you need?", Oct. 12, 2011, The Data Center Journal [http://www.datacenterjournal.com/home/news/languages/item/2792-the-price-of-data-center-availability]</ref>

===Site selection===
Aspects such as proximity to available power grids, telecommunications infrastructure, networking services, transportation lines and emergency services can affect costs, risk, security and other factors to be taken into consideration for data center design.   Whilst a wide array of location factors are taken into account (e.g. flight paths, neighbouring uses, geological risks) access to suitable available power is often the longest lead time item. Location affects data center design also because the climatic conditions dictate what cooling technologies should be deployed. In turn this impacts uptime and the costs associated with cooling.<ref>Tucci, Linda. "Five tips on selecting a data center location", May 7, 2008, SearchCIO.com [http://searchcio.techtarget.com/news/1312614/Five-tips-on-selecting-a-data-center-location]</ref> For example, the topology and the cost of managing a data center in a warm, humid climate will vary greatly from managing one in a cool, dry climate.

===Modularity and flexibility===
[[File:Cabinet Asile.jpg|thumb|Cabinet aisle in a data center]]
{{main article|Modular data center}}

Modularity and flexibility are key elements in allowing for a data center to grow and change over time. Data center modules are pre-engineered, standardized building blocks that can be easily configured and moved as needed.<ref>Niles, Susan. "Standardization and Modularity in Data Center Physical Infrastructure," 2011, Schneider Electric, page 4. [http://www.apcmedia.com/salestools/VAVR-626VPD_R1_EN.pdf]</ref>

A modular data center may consist of data center equipment contained within shipping containers or similar portable containers.<ref>Pitchaikani, Bala. "Strategies for the Containerized Data Center," DataCenterKnowledge.com, Sept. 8, 2011. [http://www.datacenterknowledge.com/archives/2011/09/08/strategies-for-the-containerized-data-center/]</ref> But it can also be described as a design style in which components of the data center are prefabricated and standardized so that they can be constructed, moved or added to quickly as needs change.<ref>Niccolai, James. "HP says prefab data center cuts costs in half," InfoWorld, July 27, 2010. [http://www.infoworld.com/d/green-it/hp-says-prefab-data-center-cuts-costs-in-half-837?page=0,0]</ref>

===Environmental control===
{{main article|Data center environmental control}}
The physical environment of a data center is rigorously controlled.
[[Air conditioning]] is used to control the temperature and humidity in the data center. [[ASHRAE]]'s "Thermal Guidelines for Data Processing Environments"<ref>{{cite book|title=Thermal Guidelines for Data Processing Environments|year=2012|publisher=American Society of Heating, Refrigerating and Air-Conditioning Engineers|isbn=978-1936504-33-6|author=ASHRAE Technical Committee 9.9, Mission Critical Facilities, Technology Spaces and Electronic Equipment|edition=3}}</ref> recommends a temperature range of {{convert|18|–|27|C|F}}, a dew point range of {{convert|5|–|15|C|F}}, and a relative humidity between 40% to 60% for data center environments.<ref name=ServersCheck>{{Cite web| title = Best Practices for data center monitoring and server room monitoring  | url=https://serverscheck.com/sensors/temperature_best_practices.asp | author = ServersCheck | accessdate = 2016-10-07}}</ref>  The temperature in a data center will naturally rise because the electrical power used heats the air. Unless the heat is removed, the ambient temperature will rise, resulting in electronic equipment malfunction. By controlling the air temperature, the server components at the board level are kept within the manufacturer's specified temperature/humidity range. Air conditioning systems help control [[humidity]] by cooling the return space air below the [[dew point]]. Too much humidity, and water may begin to [[condensation|condense]] on internal components. In case of a dry atmosphere, ancillary humidification systems may add water vapor if the humidity is too low, which can result in [[electrostatics|static electricity]] discharge problems which may damage components. Subterranean data centers may keep computer equipment cool while expending less energy than conventional designs.

Modern data centers try to use economizer cooling, where they use outside air to keep the data center cool. At least one data center (located in [[Upstate New York]]) will cool servers using outside air during the winter. They do not use chillers/air conditioners, which creates potential energy savings in the millions.<ref>{{cite news| url=http://www.reuters.com/article/pressRelease/idUS141369+14-Sep-2009+PRN20090914 | work=Reuters | title=tw telecom and NYSERDA Announce Co-location Expansion | date=2009-09-14}}</ref>  Increasingly [http://www.datacenterdynamics.com/focus/archive/2013/09/air-air-combat-indirect-air-cooling-wars-0 indirect air cooling] is being deployed in data centers globally which has the advantage of more efficient cooling which lowers power consumption costs in the data center.

Telcordia [http://telecom-info.telcordia.com/site-cgi/ido/docs.cgi?ID=SEARCH&DOCUMENT=GR-2930& GR-2930, ''NEBS: Raised Floor Generic Requirements for Network and Data Centers''], presents generic engineering requirements for raised floors that fall within the strict NEBS guidelines.

There are many types of commercially available floors that offer a wide range of structural strength and loading capabilities, depending on component construction and the materials used. The general types of [[raised floor]]s include stringer, stringerless,  and structural platforms, all of which are discussed in detail in GR-2930 and summarized below.
* '''''Stringered raised floors''''' - This type of raised floor generally consists of a vertical array of steel pedestal assemblies (each assembly is made up of a steel base plate, tubular upright, and a head) uniformly spaced on two-foot centers and mechanically fastened to the concrete floor. The steel pedestal head has a stud that is inserted into the pedestal upright and the overall height is adjustable with a leveling nut on the welded stud of the pedestal head.
* '''''Stringerless raised floors''''' - One non-earthquake type of raised floor generally consists of an array of pedestals that provide the necessary height for routing cables and also serve to support each corner of the floor panels. With this type of floor, there may or may not be provisioning to mechanically fasten the floor panels to the pedestals. This stringerless type of system (having no mechanical attachments between the pedestal heads) provides maximum accessibility to the space under the floor. However, stringerless floors are significantly weaker than stringered raised floors in supporting lateral loads and are not recommended.
* '''''Structural platforms''''' - One type of structural platform consists of members constructed of steel angles or channels that are welded or bolted together to form an integrated platform for supporting equipment. This design permits equipment to be fastened directly to the platform without the need for toggle bars or supplemental bracing. Structural platforms may or may not contain panels or stringers.

Data centers typically have [[raised floor]]ing made up of {{convert|60|cm|ft|abbr=on|0}} removable square tiles. The trend is towards {{convert|80|-|100|cm|in|abbr=on}} void to cater for better and uniform air distribution. These provide a [[plenum space|plenum]] for air to circulate below the floor, as part of the air conditioning system, as well as providing space for power cabling.

====Metal whiskers====
Raised floors and other metal structures such as cable trays and ventilation ducts have caused many problems with [[zinc whiskers]] in the past, and likely are still present in many data centers. This happens when microscopic metallic filaments form on metals such as zinc or tin that protect many metal structures and electronic components from corrosion. Maintenance on a raised floor or installing of cable etc. can dislodge the whiskers, which enter the airflow and may short circuit server components or power supplies, sometimes through a high current metal vapor [[plasma arc]]. This phenomenon is not unique to data centers, and has also caused catastrophic failures of satellites and military hardware.<ref>{{cite web|title=NASA - metal whiskers research|url=http://nepp.nasa.gov/whisker/other_whisker/index.htm|publisher=NASA|accessdate=2011-08-01}}</ref>

===Electrical power===

[[File:Datacenter Backup Batteries.jpg|thumb|right|A bank of batteries in a large data center, used to provide power until diesel generators can start]]

Backup power consists of one or more [[uninterruptible power supply|uninterruptible power supplies]], battery banks, and/or [[Diesel generator|diesel]] / [[gas turbine]] generators.<ref>Detailed explanation of UPS topologies {{cite web|url=http://www.emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |format=PDF |title=EVALUATING THE ECONOMIC IMPACT OF UPS TECHNOLOGY |deadurl=yes |archiveurl=https://web.archive.org/web/20101122074817/http://emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/Evaluating%20the%20Economic%20Impact%20of%20UPS%20Technology.pdf |archivedate=2010-11-22 |df= }}</ref>

To prevent [[single point of failure|single points of failure]], all elements of the electrical systems, including backup systems, are typically fully duplicated, and critical servers are connected to both the "A-side" and "B-side" power feeds. This arrangement is often made to achieve [[N+1 redundancy]] in the systems. [[Transfer switch#Static transfer switch|Static transfer switches]] are sometimes used to ensure instantaneous switchover from one supply to the other in the event of a power failure.

===Low-voltage cable routing===
Data cabling is typically routed through overhead [[cable tray]]s in modern data centers. But some{{Who|date=May 2012}} are still recommending under raised floor cabling for security reasons and to consider the addition of cooling systems above the racks in case this enhancement is necessary. Smaller/less expensive data centers without raised flooring may use anti-static tiles for a flooring surface. Computer cabinets are often organized into a [[Data center environmental control#Aisle containment|hot aisle]] arrangement to maximize airflow efficiency.

===Fire protection===
[[File:FM200 Three.jpg|thumb|[[FM200]] Fire Suppression Tanks]]
Data centers feature [[fire protection]] systems, including [[passive fire protection|passive]] and [[Active Design]] elements, as well as implementation of [[fire prevention]] programs in operations. [[Smoke detectors]] are usually installed to provide early warning of a fire at its incipient stage. This allows investigation, interruption of power, and manual fire suppression using hand held fire extinguishers before the fire grows to a large size. An [[active fire protection]] system, such as a [[fire sprinkler system]] or a [[clean agent]] fire suppression gaseous system, is often provided to control a full scale fire if it develops. High sensitivity smoke detectors, such as [[aspirating smoke detector]]s, activating [[clean agent]] fire suppression gaseous systems activate earlier than fire sprinklers.

* Sprinklers = structure protection and building life safety.
* Clean agents = business continuity and asset protection.
* No water = no collateral damage or clean up.

Passive fire protection elements include the installation of [[Firewall (construction)|fire walls]] around the data center, so a fire can be restricted to a portion of the facility for a limited time in the event of the failure of the active fire protection systems. Fire wall penetrations into the server room, such as cable penetrations, coolant line penetrations and air ducts, must be provided with fire rated penetration assemblies, such as [[fire stop]]ping.

===Security===
Physical security also plays a large role with data centers. Physical access to the site is usually restricted to selected personnel, with controls including a layered security system often starting with fencing, [[bollard]]s and [[mantrap (access control)|mantraps]].<ref>{{cite web|author=Sarah D. Scalet |url=http://www.csoonline.com/article/220665 |title=19 Ways to Build Physical Security Into a Data Center |publisher=Csoonline.com |date=2005-11-01 |accessdate=2013-08-30}}</ref> [[Video camera]] surveillance and permanent [[security guard]]s are almost always present if the data center is large or contains sensitive information on any of the systems within. The use of finger print recognition [[mantrap (snare)|mantrap]]s is starting to be commonplace.

==Energy use==
[[File:Google Data Center, The Dalles.jpg|thumb|[[Google Data Centers|Google Data Center]], [[The Dalles, Oregon]]]]
{{main article|IT energy management}}

Energy use is a central issue for data centers. Power draw for data centers ranges from a few kW for a rack of servers in a closet to several tens of MW for large facilities. Some facilities have power densities more than 100 times that of a typical office building.<ref>{{cite web|url=http://www1.eere.energy.gov/femp/program/dc_energy_consumption.html|title=Data Center Energy Consumption Trends|publisher=U.S. Department of Energy|accessdate=2010-06-10}}</ref> For higher power density facilities, electricity costs are a dominant [[operating expense]] and account for over 10% of the [[total cost of ownership]] (TCO) of a data center.<ref>J Koomey, C. Belady, M. Patterson, A. Santos, K.D. Lange. [http://www.intel.com/assets/pdf/general/servertrendsreleasecomplete-v25.pdf Assessing Trends Over Time in Performance, Costs, and Energy Use for Servers] Released on the web August 17th, 2009.</ref> By 2012 the cost of power for the data center is expected to exceed the cost of the original capital investment.<ref>{{cite web|url=http://www1.eere.energy.gov/femp/pdfs/data_center_qsguide.pdf |title=Quick Start Guide to Increase Data Center Energy Efficiency |publisher=U.S. Department of Energy |accessdate=2010-06-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20101122035456/http://www1.eere.energy.gov:80/femp/pdfs/data_center_qsguide.pdf |archivedate=2010-11-22 |df= }}</ref>

===Greenhouse gas emissions===
In 2007 the entire [[information and communication technologies]] or ICT sector was estimated to be responsible for roughly 2% of global [[Greenhouse gas|carbon emissions]] with data centers accounting for 14% of the ICT footprint.<ref name="smart1">{{cite web|url=http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |title=Smart 2020: Enabling the low carbon economy in the information age |publisher=The Climate Group for the Global e-Sustainability Initiative |accessdate=2008-05-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20110728032834/http://www.smart2020.org/_assets/files/03_Smart2020Report_lo_res.pdf |archivedate=2011-07-28 |df= }}</ref> The US EPA estimates that servers and data centers are responsible for up to 1.5% of the total US electricity consumption,<ref name="energystar1">{{cite web|url=http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf|title=Report to Congress on Server and Data Center Energy Efficiency|publisher=U.S. Environmental Protection Agency ENERGY STAR Program}}</ref> or roughly .5% of US GHG emissions,<ref>A calculation of data center electricity burden cited in the [http://www.energystar.gov/ia/partners/prod_development/downloads/EPA_Datacenter_Report_Congress_Final1.pdf Report to Congress on Server and Data Center Energy Efficiency] and electricity generation contributions to green house gas emissions published by the EPA in the [http://epa.gov/climatechange/emissions/downloads10/US-GHG-Inventory-2010_ExecutiveSummary.pdf Greenhouse Gas Emissions Inventory Report]. Retrieved 2010-06-08.</ref>  for 2007. Given a business as usual scenario greenhouse gas emissions from data centers is projected to more than double from 2007 levels by 2020.<ref name="smart1"/>

Siting is one of the factors that affect the energy consumption and environmental effects of a datacenter. In areas where climate favors cooling and lots of renewable electricity is available the environmental effects will be more moderate. Thus countries with favorable conditions, such as: Canada,<ref>[http://www.theglobeandmail.com/report-on-business/canada-called-prime-real-estate-for-massive-data-computers/article2071677/ Canada Called Prime Real Estate for Massive Data Computers - Globe & Mail] Retrieved June 29, 2011.</ref> Finland,<ref>[http://datacenter-siting.weebly.com/ Finland - First Choice for Siting Your Cloud Computing Data Center.]. Retrieved 4 August 2010.</ref> Sweden,<ref>{{cite web|url=http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|title=Stockholm sets sights on data center customers|accessdate=4 August 2010|archiveurl=https://web.archive.org/web/20100819190918/http://www.stockholmbusinessregion.se/templates/page____41724.aspx?epslanguage=EN|archivedate=19 August 2010}}</ref> Norway <ref>[http://www.innovasjonnorge.no/en/start-page/invest-in-norway/industries/datacenters/ In a world of rapidly increasing carbon emissions from the ICT industry, Norway offers a sustainable solution] Retrieved 1 March 2016.</ref> and Switzerland,<ref>[http://www.greenbiz.com/news/2010/06/30/swiss-carbon-neutral-servers-hit-cloud Swiss Carbon-Neutral Servers Hit the Cloud.]. Retrieved 4 August 2010.</ref> are trying to attract cloud computing data centers.

In an 18-month investigation by scholars at Rice University's Baker Institute for Public Policy in Houston and the Institute for Sustainable and Applied Infodynamics in Singapore, data center-related emissions will more than triple by 2020.
<ref>{{Cite news
 |author=Katrice R. Jalbuena 
 |title=Green business news. 
 |quote= 
 |publisher=EcoSeed 
 |date=October 15, 2010 
 |pages= 
 |url=http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy 
 |accessdate=2010-11-11 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20160618081417/http://ecoseed.org/en/business-article-list/article/1-business/8219-i-t-industry-risks-output-cut-in-low-carbon-economy 
 |archivedate=2016-06-18 
 |df= 
}}</ref>

===Energy efficiency===
The most commonly used metric to determine the energy efficiency of a data center is [[power usage effectiveness]], or PUE. This simple ratio is the total power entering the data center divided by the power used by the IT equipment.

:<math> \mathrm{PUE}  =  {\mbox{Total Facility Power} \over \mbox{IT Equipment Power}} </math>

Total facility power consists of power used by IT equipment plus any overhead power consumed by anything that is not considered a computing or data communication device (i.e. cooling, lighting, etc.). An ideal PUE is 1.0 for the hypothetical situation of zero overhead power. The average data center in the US has a PUE of 2.0,<ref name="energystar1"/> meaning that the facility uses two watts of total power (overhead + IT equipment) for every watt delivered to IT equipment. State-of-the-art data center energy efficiency is estimated to be roughly 1.2.<ref>{{cite web|url=https://microsite.accenture.com/svlgreport/Documents/pdf/SVLG_Report.pdf|title=Data Center Energy Forecast|publisher=Silicon Valley Leadership Group}}</ref> Some large data center operators like [[Microsoft]] and [[Yahoo!]] have published projections of PUE for facilities in development; [[Google]] publishes quarterly actual efficiency performance from data centers in operation.<ref>{{cite web|url=https://www.google.com/about/datacenters/efficiency/internal/|title=Efficiency: How we do it – Data centers|publisher=Google|accessdate=2015-01-19}}</ref>

The [[U.S. Environmental Protection Agency]] has an [[Energy Star]] rating for standalone or large data centers. To qualify for the ecolabel, a data center must be within the top quartile of energy efficiency of all reported facilities.<ref>Commentary on introduction of Energy Star for Data Centers {{cite web|title=Introducing EPA ENERGY STAR for Data Centers |url=http://www.emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |format=Web site |publisher=Jack Pouchet |accessdate=2010-09-27 |date=2010-09-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20100925210539/http://emerson.com/edc/post/2010/06/15/Introducing-EPA-ENERGY-STARc2ae-for-Data-Centers.aspx |archivedate=2010-09-25 |df= }}</ref>

European Union also has a similar initiative: EU Code of Conduct for Data Centres<ref>{{cite web|url=http://iet.jrc.ec.europa.eu/energyefficiency/ict-codes-conduct/data-centres-energy-efficiency |title=EU Code of Conduct for Data Centres |publisher=iet.jrc.ec.europa.eu |date= |accessdate=2013-08-30 }}</ref>

===Energy use analysis===
Often, the first step toward curbing energy use in a data center is to understand how energy is being used in the data center. Multiple types of analysis exist to measure data center energy use. Aspects measured include not just energy used by IT equipment itself, but also by the data center facility equipment, such as chillers and fans.<ref>Sweeney, Jim. "Reducing Data Center Power and Energy Consumption: Saving Money and 'Going Green,' " GTSI Solutions, pages 2–3. [http://www.gtsi.com/cms/documents/white-papers/green-it.pdf]</ref>

===Power and cooling analysis===
Power is the largest recurring cost to the user of a data center.<ref name=DRJ_Choosing>{{Citation
 | title = Choosing a Data Center
 | url = http://www.atlantic.net/images/pdf/choosing_a_data_center.pdf
 | publisher = Disaster Recovery Journal
 | year = 2009
 | author = Cosmano, Joe
 | accessdate = 2012-07-21
}}</ref>  A power and cooling analysis, also referred to as a thermal assessment, measures the relative temperatures in specific areas as well as the capacity of the cooling systems to handle specific ambient temperatures.<ref>Needle, David. "HP's Green Data Center Portfolio Keeps Growing," InternetNews, July 25, 2007. [http://www.internetnews.com/xSP/article.php/3690651/HPs+Green+Data+Center+Portfolio+Keeps+Growing.htm]</ref> A power and cooling analysis can help to identify hot spots, over-cooled areas that can handle greater power use density, the breakpoint of equipment loading, the effectiveness of a raised-floor strategy, and optimal equipment positioning (such as AC units) to balance temperatures across the data center. Power cooling density is a measure of how much square footage the center can cool at maximum capacity.<ref name=Inc_Howtochoose>{{Citation
 | title = How to Choose a Data Center
 | url = http://www.inc.com/guides/2010/11/how-to-choose-a-data-center_pagen_2.html
 | year = 2010
 | author = Inc. staff
 | accessdate = 2012-07-21
}}</ref>

===Energy efficiency analysis===
An energy efficiency analysis measures the energy use of data center IT and facilities equipment. A typical energy efficiency analysis measures factors such as a data center's power use effectiveness (PUE) against industry standards, identifies mechanical and electrical sources of inefficiency, and identifies air-management metrics.<ref>Siranosian, Kathryn. "HP Shows Companies How to Integrate Energy Management and Carbon Reduction," TriplePundit, April 5, 2011. [http://www.triplepundit.com/2011/04/hp-launches-program-companies-integrate-manage-energy-carbon-reduction-strategies/]</ref>

===Computational fluid dynamics (CFD) analysis===
{{main article|Computational fluid dynamics}}

This type of analysis uses sophisticated tools and techniques to understand the unique thermal conditions present in each data center—predicting the temperature, airflow, and pressure behavior of a data center to assess performance and energy consumption, using numerical modeling.<ref>Bullock, Michael. "Computation Fluid Dynamics - Hot topic at Data Center World," Transitional Data Services, March 18, 2010. [http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software] {{webarchive |url=https://web.archive.org/web/20120103183406/http://blog.transitionaldata.com/aggregate/bid/37840/Seeing-the-Invisible-Data-Center-with-CFD-Modeling-Software |date=January 3, 2012 }}</ref> By predicting the effects of these environmental conditions, CFD analysis in the data center can be used to predict the impact of high-density racks mixed with low-density racks<ref>Bouley, Dennis (editor). "Impact of Virtualization on Data Center Physical Infrastructure," The Green grid, 2010. [http://www.thegreengrid.org/~/media/WhitePapers/White_Paper_27_Impact_of_Virtualization_Data_On_Center_Physical_Infrastructure_020210.pdf?lang=en]</ref> and the onward impact on cooling resources, poor infrastructure management practices and AC failure of AC shutdown for scheduled maintenance.

===Thermal zone mapping===
Thermal zone mapping uses sensors and computer modeling to create a three-dimensional image of the hot and cool zones in a data center.<ref>Fontecchio, Mark. "HP Thermal Zone Mapping plots data center hot spots," SearchDataCenter, July 25, 2007. [http://searchdatacenter.techtarget.com/news/1265634/HP-Thermal-Zone-Mapping-plots-data-center-hot-spots]</ref>

This information can help to identify optimal positioning of data center equipment. For example, critical servers might be placed in a cool zone that is serviced by redundant AC units.

===Green data centers===
[[File:Magazin Vauban E.jpg|thumb| This water-cooled data center in the [[Independent Port of Strasbourg|Port of Strasbourg]], France claims the attribute ''green''.]]
Data centers use a lot of power, consumed by two main usages: the power required to run the actual equipment and then the power required to cool the equipment. The first category is addressed by designing computers and storage systems that are increasingly power-efficient.<ref name="ReferenceDC2"/> To bring down cooling costs data center designers try to use natural ways to cool the equipment. Many data centers are located near good fiber connectivity, power grid connections and also people-concentrations to manage the equipment, but there are also circumstances where the data center can be miles away from the users and don't need a lot of local management. Examples of this are the 'mass' data centers like Google or Facebook: these DC's are built around many standardized servers and storage-arrays and the actual users of the systems are located all around the world. After the initial build of a data center staff numbers required to keep it running are often relatively low: especially data centers that provide mass-storage or computing power which don't need to be near population centers.Data centers in arctic locations where outside air provides all cooling are getting more popular as cooling and electricity are the two main variable cost components.<ref>{{cite web|url=http://www.gizmag.com/fjord-cooled-data-center/20938/|title=Fjord-cooled DC in Norway claims to be greenest|access-date=23 December 2011}}</ref>

==Network infrastructure==
[[File:Paris servers DSC00190.jpg|thumb|left|An example of "rack mounted" servers]]
Communications in data centers today are most often based on [[computer network|networks]] running the [[Internet protocol|IP]] [[protocol (computing)|protocol]] suite. Data centers contain a set of [[Router (computing)|routers]] and [[Network switch|switches]] that transport traffic between the servers and to the outside world. [[Redundancy (engineering)|Redundancy]] of the Internet connection is often provided by using two or more upstream service providers (see [[Multihoming]]).

Some of the servers at the data center are used for running the basic Internet and [[intranet]] services needed by internal users in the organization, e.g., e-mail servers, [[proxy server]]s, and [[Domain Name System|DNS]] servers.

Network security elements are also usually deployed: [[firewall (networking)|firewalls]], [[VPN]] [[Gateway (computer networking)|gateways]], [[intrusion detection system]]s, etc. Also common are monitoring systems for the network and some of the applications. Additional off site monitoring systems are also typical, in case of a failure of communications inside the data center.

==Data center infrastructure management==
[[Data center infrastructure management]] (DCIM) is the integration of information technology (IT) and facility management disciplines to centralize monitoring, management and intelligent capacity planning of a data center's critical systems. Achieved through the implementation of specialized software, hardware and sensors, DCIM enables common, real-time monitoring and management platform for all interdependent systems across IT and facility infrastructures.

Depending on the type of implementation, DCIM products can help data center managers identify and eliminate sources of risk to increase availability of critical IT systems. DCIM products also can be used to identify interdependencies between facility and IT infrastructures to alert the facility manager to gaps in system redundancy, and provide dynamic, holistic benchmarks on power consumption and efficiency to measure the effectiveness of "green IT" initiatives.

It's important to measure and understand data center efficiency metrics.  A lot of the discussion in this area has focused on energy issues, but other metrics beyond the PUE can give a more detailed picture of the data center operations. Server, storage, and staff utilization metrics can contribute to a more complete view of an enterprise data center. In many cases, disc capacity goes unused and in many instances the organizations run their servers at 20% utilization or less.<ref>{{cite web|url=http://content.dell.com/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |title=Measuring Data Center Efficiency: Easier Said Than Done |publisher=Dell.com |accessdate=2012-06-25 |deadurl=yes |archiveurl=https://web.archive.org/web/20101027083349/http://content.dell.com:80/us/en/enterprise/d/large-business/measure-data-center-efficiency.aspx |archivedate=2010-10-27 |df= }}</ref> More effective automation tools can also improve the number of servers or virtual machines that a single admin can handle.

DCIM providers are increasingly linking with [[computational fluid dynamics]] providers to predict complex airflow patterns in the data center. The CFD component is necessary to quantify the impact of planned future changes on cooling resilience, capacity and efficiency.<ref name="gartner">{{cite web|url=http://www.gartner.com/it-glossary/computational-fluid-dynamic-cfd-analysis|title=Computational-Fluid-Dynamic (CFD) Analysis &#124; Gartner IT Glossary|publisher=gartner.com|accessdate=2014-08-27}}</ref>

==Managing the capacity of a data center==
{{unreferenced section|date=August 2016}}
[[File:Capacity of a datacenter - Life Cycle.jpg|thumbnail|left|Capacity of a datacenter - Life Cycle]]
Several parameters may limit the capacity of a data center. For long term usage, the main limitations will be available area, then available power. In the first stage of its life cycle, a data center will see its occupied space growing more rapidly than consumed energy. With constant densification of new IT technologies, the need in energy is going to become dominant, equaling then overcoming the need in area (second then third phase of cycle). The development and multiplication of connected objects, the needs in storage and data treatment lead to the necessity of data centers to grow more and more rapidly. It is therefore important to define a data center strategy before being cornered. The decision, conception and building cycle lasts several years. Therefore, it is imperative to initiate this strategic consideration when the data center reaches about 50% of its power capacity. Maximum occupation of a data center needs to be stabilized around 85%, be it in power or occupied area. Resources thus managed will allow a rotation zone for managing hardware replacement and will allow temporary cohabitation of old and new generations. In the case where this limit would be overcrossed durably, it would not be possible to proceed to material replacements, which would invariably lead to smothering the information system. The data center is a resource in its own right of the information system, with its own constraints of time and management (life span of 25 years), it therefore needs to be taken into consideration in the framework of the SI midterm planning (between 3 and 5 years).

==Applications==
[[File:IBMPortableModularDataCenter.jpg|thumb|right|A 40-foot [[Portable Modular Data Center]]]]

The main purpose of a data center is running the IT systems applications that handle the core business and operational data of the organization. Such systems may be proprietary and developed internally by the organization, or bought from [[enterprise software]] vendors. Such common applications are [[Enterprise resource planning|ERP]] and [[Customer relationship management|CRM]] systems.

A data center may be concerned with just [[operations architecture]] or it may provide other services as well.

Often these applications will be composed of multiple hosts, each running a single component. Common components of such applications are [[database]]s, [[file server]]s, [[application server]]s, [[middleware]], and various others.

Data centers are also used for off site backups. Companies may subscribe to backup services provided by a data center. This is often used in conjunction with [[Tape drive|backup tapes]]. Backups can be taken off servers locally on to tapes. However, tapes stored on site pose a security threat and are also susceptible to fire and flooding. Larger companies may also send their backups off site for added security. This can be done by backing up to a data center. Encrypted backups can be sent over the Internet to another data center where they can be stored securely.

For quick deployment or [[disaster recovery]], several large hardware vendors have developed mobile/modular solutions that can be installed and made operational in very short time. Companies such as
[[File:Edge Night 02.jpg|thumb|A modular data center connected to the power grid at a utility substation]]
* [[Cisco Systems]],<ref>{{cite web|title=Info and video about Cisco's solution |url=http://www.datacenterknowledge.com/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |publisher=Datacentreknowledge |accessdate=2008-05-11 |date=May 15, 2007 |deadurl=yes |archiveurl=https://web.archive.org/web/20080519213241/http://www.datacenterknowledge.com:80/archives/2008/May/15/ciscos_mobile_emergency_data_center.html |archivedate=2008-05-19 |df= }}</ref>
* [[Sun Microsystems]] ([[Sun Modular Datacenter]]),<ref>{{cite web|url=http://www.sun.com/products/sunmd/s20/specifications.jsp|archiveurl=https://web.archive.org/web/20080513090300/http://www.sun.com/products/sunmd/s20/specifications.jsp|archivedate=2008-05-13|title=Technical specs of Sun's Blackbox|accessdate=2008-05-11}}</ref><ref>And English Wiki article on [[Sun Modular Datacenter|Sun's modular datacentre]]</ref>
* [[Groupe Bull|Bull]] (mobull),<ref>{{cite web|title=Mobull Plug and Boot Datacenter|url=http://www.bull.com/extreme-computing/mobull.html|publisher=Bull|first=Daniel|last=Kidger|accessdate=2011-05-24}}</ref>
* [[IBM]] ([[Portable Modular Data Center]]),
* [[Schneider-Electric]] ([[Portable Modular Data Center]]),
* [[Hewlett-Packard|HP]] ([[HP Performance Optimized Datacenter|Performance Optimized Datacenter]]),<ref>{{cite web|url=http://h18004.www1.hp.com/products/servers/solutions/datacentersolutions/pod/index.html |title=HP Performance Optimized Datacenter (POD) 20c and 40c - Product Overview |publisher=H18004.www1.hp.com |date= |accessdate=2013-08-30}}</ref>
* [[Huawei]] (Container Data Center Solution),<ref>{{cite web|title=Huawei's Container Data Center Solution|url=http://www.huawei.com/ilink/enenterprise/download/HW_143893|publisher=Huawei|accessdate=2014-05-17}}
</ref> and
* [[Google]] ([[Google Modular Data Center]]) have developed systems that could be used for this purpose.<ref>{{cite web|url=http://www.crn.com/hardware/208403225 |publisher=ChannelWeb |accessdate=2008-05-11 |title=IBM's Project Big Green Takes Second Step |first=Brian |last=Kraemer |date=June 11, 2008 |deadurl=yes |archiveurl=https://web.archive.org/web/20080611114732/http://www.crn.com:80/hardware/208403225 |archivedate=2008-06-11 |df= }}</ref><ref>{{cite web|url=http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |title=Modular/Container Data Centers Procurement Guide: Optimizing for Energy Efficiency and Quick Deployment |format=PDF |date= |accessdate=2013-08-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20130531191212/http://hightech.lbl.gov/documents/data_centers/modular-dc-procurement-guide.pdf |archivedate=2013-05-31 |df= }}</ref>
* BASELAYER has a patent on the software defined modular data center.<ref>{{Citation|title = System and method of providing computer resources|url = http://www.google.com/patents/US8434804|date = May 7, 2013|accessdate = 2016-02-24|first = George|last = Slessman}}</ref><ref>{{Cite web|title = Modular Data Center Firm IO to Split Into Two Companies|url = http://www.datacenterknowledge.com/archives/2014/12/02/modular-data-center-firm-io-to-split-into-two-companies/|website = Data Center Knowledge|access-date = 2016-02-24|language = en-US}}</ref>

==US wholesale and retail colocation providers==
According to Synergy Research Group, "the scale of the wholesale colocation market in the United States is very significant relative to the retail market, with Q3 wholesale revenues reaching almost $700 million. [[Digital Realty]] Trust is the wholesale market leader, followed at a distance by [[DuPont Fabros]]." Synergy Research also describes the US colocation market as the most mature and well-developed in the world," based on revenue and the continued adoption of cloud infrastructure services.
* Contains estimates from Synergy Research Group.<ref name="srgresearch">{{cite web|url=https://www.srgresearch.com/articles/mature-us-colocation-market-led-equinix-and-centurylink-savvis|title=Mature US Colocation Market Led by Equinix and CenturyLink-Savvis &#124; Synergy Research Group|author=Synergy Research Group, Reno, NV|publisher=srgresearch.com|accessdate=2014-08-27}}</ref>

{| class="wikitable sortable"
|-
!Rank !! Company Name !! US Market Share
|-
!1
| Various Providers || 34%
|-
!2
| [[Equinix]] || 18%
|-
!3
| [[CenturyLink-Savvis]] || 8%
|-
!4
| [[SunGard]] || 5%
|-
!5
| [[AT&T]] || 5%
|-
!6
| [[Verizon]] || 5%
|-
!7
| Telx || 4%
|-
!8
| CyrusOne || 4%
|-
!9
| [[Level 3 Communications]] || 3%
|-
!10
| [[Internap]] || 2%
|}

==See also==
{{columns-list|colwidth=20em|
* [[Central apparatus room]]
* [[Colocation center]]
* [[Data center infrastructure management]]
* [[Disaster recovery]]
* [[Dynamic Infrastructure]]
* [[Electrical network]]
* [[HVAC]]
* [[Internet exchange point]]
* [[Internet hosting service]]
* [[Modular data center]]
* [[Neher–McGrath]]
* [[Network operations center]]
* [[Open Compute Project]], by [[Facebook]]
* [[Peering]]
* [[Server farm]]
* [[Server room]]
* [[Server Room Environment Monitoring System]]
* [[Server sprawl]]
* [[Sun Modular Datacenter]]
* [[Telecommunications network]]
* [[Utah Data Center]]
* [[Web hosting service]]
* [[Anderson Powerpole]] connector
}}

==References==
{{reflist|colwidth=30em}}

==External links==
{{Commons category|Data centers}}
{{wikibooks|The Design and Organization of Data Centers}}
{{wiktionary}}
* [http://hightech.lbl.gov/datacenters.html Lawrence Berkeley Lab] - Research, development, demonstration, and deployment of energy-efficient technologies and practices for data centers
* [http://hightech.lbl.gov/dc-powering/faq.html DC Power For Data Centers Of The Future] - FAQ: 380VDC testing and demonstration at a Sun data center.
* [http://www.dccompendium.com/ DC Compendium] - Repository and compendium of data centers globally.
* [http://media.wix.com/ugd/fb8983_e929404b24874e4fa7a8279f1cda58f8.pdf White Paper] - Property Taxes: The New Challenge for Data Centers

{{Authority control}}
{{Cloud computing}}

{{DEFAULTSORT:Data Center}}
[[Category:Computer networking]]
[[Category:Applications of distributed computing]]
[[Category:Cloud storage]]
[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Distributed data storage systems]]
[[Category:Servers (computing)]]
[[Category:Data centers| ]]
<=====doc_Id=====>:140
<=====title=====>:
Commit (data management)
<=====text=====>:
{{For|the revision control concept|Commit (revision control)}}
{{Unreferenced|date=May 2014}}

In [[computer science]] and [[data management]], a '''commit''' is the making of a set of tentative changes permanent. A popular usage is at the end of a [[database transaction|transaction]]. A ''commit'' is an act of committing.

==Data management==
A <code>[[COMMIT (SQL)|COMMIT]]</code> statement in [[SQL]] ends a [[database transaction|transaction]] within a [[relational database management system]] (RDBMS) and makes all changes visible to other users. The general format is to issue a <code>[[Begin work (SQL)|BEGIN WORK]]</code> statement, one or more SQL statements, and then the <code>COMMIT</code> statement. Alternatively, a <code>[[Rollback (data management)|ROLLBACK]]</code> statement can be issued, which undoes all the work performed since <code>BEGIN WORK</code> was issued. A <code>COMMIT</code> statement will also release any existing [[savepoint]]s that may be in use.

In terms of transactions, the opposite of commit is to discard the tentative changes of a transaction, a [[rollback (data management)|rollback]].

==See also==
* [[Commit (version control)]]
* [[Atomic commit]]
* [[Two-phase commit protocol]]
* [[Three-phase commit protocol]]

{{databases}}

{{DEFAULTSORT:Commit (Data Management)}}
[[Category:Data management]]
[[Category:SQL]]
[[Category:Transaction processing]]

{{comp-sci-stub}}
<=====doc_Id=====>:143
<=====title=====>:
Database engine
<=====text=====>:
A '''database engine''' (or '''storage engine''') is the underlying software component that a [[database management system]] (DBMS) uses to [[create, read, update and delete]] (CRUD) [[data]] from a [[database]]. Most database management systems include their own [[application programming interface]] (API) that allows the user to interact with their underlying engine without going through the user interface of the DBMS.

The term "database engine" is frequently used interchangeably with "[[database server]]" or "database management system". A 'database instance' refers to the processes and memory structures of the running '''database engine'''.

==Storage engines==
Many of the modern DBMS support multiple storage engines within the same database. For example, [[MySQL]] supports [[InnoDB]] as well as [[MyISAM]].

Some storage engines are [[Database transaction|transactional]].

{| class="wikitable"
|-
! Name !! License !! Transactional
|-
| [[Aria (storage engine)|Aria]] || GPL || {{No}}
|-
| BlitzDB || GPL || {{No}}
|-
| [[Falcon (storage engine)|Falcon]] || GPL || {{Yes}}
|-
| [[InnoDB]] || GPL || {{Yes}}
|-
| [[MyISAM]] || GPL || {{No}}
|-
| [[InfiniDB]] || CPL || {{No}}
|-
| [[TokuDB]] || GPL || {{Yes}}
|-
| [[WiredTiger]] || GPL || {{Yes}}
|-
| [[XtraDB]] || GPL || {{Yes}}
|}

Additional engine types include:
*[[Embedded database]] engines
*[[In-memory database]] engines

==Design considerations==
Database bits are laid out in storage in data structures and groupings that can take advantage of both known effective algorithms to retrieve and manipulate them and the storage own properties. Typically the storage itself is designed to meet requirements of various areas that extensively utilize storage, including databases. A DBMS in operation always simultaneously utilizes several storage types (e.g., memory, and external storage), with respective layout methods.

In principle the database storage can be viewed as a [[linear address space]], where every bit of data has its unique address in this address space. In practice, only a very small percentage of addresses are kept as initial reference points (which also requires storage); most data is accessed by indirection using displacement calculations (distance in bits from the reference points) and data structures which define access paths (using pointers) to all needed data in an effective manner, optimized for the needed data access operations.

===Database storage hierarchy===
A database, while in operation, resides simultaneously in several types of storage, forming a [[storage hierarchy]]. By the nature of contemporary computers most of the database part inside a computer that hosts the DBMS resides (partially replicated) in volatile storage. Data (pieces of the database) that are being processed/manipulated reside inside a processor, possibly in [[CPU cache|processor's caches]]. These data are being read from/written to memory, typically through a computer [[Bus (computing)|bus]] (so far typically volatile storage components). Computer memory is communicating data (transferred to/from) external storage, typically through standard storage interfaces or networks (e.g., [[fibre channel]], [[iSCSI]]). A [[Disk array|storage array]], a common external storage unit, typically has storage hierarchy of its own, from a fast cache, typically consisting of (volatile and fast) [[DRAM]], which is connected (again via standard interfaces) to drives, possibly with different speeds, like [[USB flash drive|flash drives]] and magnetic [[disk drive]]s (non-volatile). The drives may be connected to [[magnetic tape]]s, on which typically the least active parts of a large database may reside, or database backup generations.

Typically a correlation exists currently between storage speed and price, while the faster storage is typically volatile.

===Data structures===
{{Main|Database storage structures}}
A data structure is an abstract construct that embeds data in a well defined manner. An efficient data structure allows to manipulate the data in efficient ways. The data manipulation may include data insertion, deletion, updating and retrieval in various modes. A certain data structure type may be very effective in certain operations, and very ineffective in others. A data structure type is selected upon DBMS development to best meet the operations needed for the types of data it contains. Type of data structure selected for a certain task typically also takes into consideration the type of storage it resides in (e.g., speed of access, minimal size of storage chunk accessed, etc.). In some DBMSs database administrators have the flexibility to select among options of data structures to contain user data for performance reasons. Sometimes the data structures have selectable parameters to tune the database performance.

Databases may store data in many data structure types.<ref name="Physical Database Design">{{harvnb|Lightstone|Teorey|Nadeau|2007}}</ref> Common examples are the following:
*ordered/unordered [[flat file database|flat files]]
*[[hash table]]s
*[[B+ tree]]s
*[[ISAM]]
*[[heap (data structure)|heaps]]

===Data orientation and clustering===
In contrast to conventional row-orientation, relational databases can also be [[Column-oriented DBMS|column-oriented]] or [[Correlational database|correlational]] in the way they store data in any particular structure.

In general, substantial performance improvement is gained if different types of database objects that are usually utilized together are laid in storage in proximity, being "clustered". This usually allows to retrieve needed related objects from storage in minimum number of input operations (each sometimes substantially time consuming). Even for in-memory databases clustering provides performance advantage due to common utilization of large caches for input-output operations in memory, with similar resulting behavior.

For example, it may be beneficial to cluster a record of an "item" in stock with all its respective "order" records. The decision of whether to cluster certain objects or not depends on the objects' utilization statistics, object sizes, caches sizes, storage types, etc.

===Database indexing===
{{Main|Database index}}
Indexing is a technique some storage engines use for improving database performance. The many types of indexes share the common property that they reduce the need to examine every entry when running a query. In large databases, this can reduce query time/cost by orders of magnitude. The simplest form of index is a sorted list of values that can be searched using a [[binary search]] with an adjacent reference to the location of the entry, analogous to the index in the back of a book. The same data can have multiple indexes (an employee database could be indexed by last name and hire date).

Indexes affect performance, but not results. Database designers can add or remove indexes without changing application logic, reducing maintenance costs as the database grows and database usage evolves.  Indexes can speed up data access, but they consume space in the database, and must be updated each time the data is altered. Indexes therefore can speed data access but slow data maintenance. These two properties determine whether a given index is worth the cost.

==See also==
{{cleanup-merge}} <!--Into chart -->
*[[Architecture of Btrieve#Micro-Kernel Database Engine|Btrieve's Micro-Kernel Database Engine]]
*[[Berkeley DB]]
*[[c-treeACE|c-treeACE Database Engine]]
*[[FLAIM Database Engine]]
*[[Microsoft Jet Database Engine]]
*[[MySQL Cluster]], on the NDB storage engine of [[MySQL]]
*[[NuoDB]]

==References==
{{Reflist}}

==External links==
*http://dev.mysql.com/tech-resources/articles/storage-engine/part_3.html
*[https://books.google.com/books?id=PqZ6QytCemcC&pg=PT287&dq=storage+engines MySQL Administrator's Bible] Chapter 11 "Storage Engines"

{{DEFAULTSORT:Database Engine}}
[[Category:Data management]]
[[Category:Database engines| ]]
[[Category:Database management systems]]
<=====doc_Id=====>:146
<=====title=====>:
Query language
<=====text=====>:
{{redirect|Database language|other types of database languages|Database#Languages}}
{{Multiple issues|
{{prose|date=October 2010}}
{{refimprove|date=October 2010}}
}}

'''Query languages''' are [[computer language]]s used to make queries in [[database]]s and [[information system]]s.

==Types==
Broadly, query languages can be classified according to whether they are database query languages or [[information retrieval query language]]s. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.

==Examples==
Examples include:
* [[.QL]] is a proprietary object-oriented query language for querying [[relational database]]s; successor of Datalog;
* [[Contextual Query Language]] (CQL) a formal language for representing queries to [[information retrieval]] systems such as web indexes or bibliographic catalogues.
* CQLF (CODYASYL Query Language, Flat) is a query language for [[CODASYL]]-type databases;
* [[Concept-Oriented Query Language]] (COQL) is used in the concept-oriented model (COM). It is based on a novel [[data modeling]] construct, concept, and uses such operations as projection and de-projection for multi-dimensional analysis, analytical operations and inference;
* [[Cypher Query Language|Cypher]] is a query language for the [[Neo4j]] graph database;
* [[Data Mining Extensions|DMX]] is a query language for [[Data Mining]] models;
* [[Datalog]] is a query language for [[deductive database]]s;
* [[F-logic]] is a declarative object-oriented language for [[deductive database]]s and [[knowledge representation]].
* [[Facebook Query Language|FQL]] enables you to use a [[SQL]]-style interface to query the data exposed by the [[Graph API]]. It provides advanced features not available in the [[Graph API]].<ref>{{cite web|url=https://developers.facebook.com/docs/technical-guides/fql/|title=FQL Overview|work=Facebook Developers}}</ref>
* [[Gellish English]] is a language that can be used for queries in Gellish English Databases, for dialogues (requests and responses) as well as for information modeling and [[knowledge modeling]];<ref>http://gellish.wiki.sourceforge.net/Querying+a+Gellish+English+database{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>
* [[Gremlin (programming language)|Gremlin]] is an [[Apache Software Foundation]] graph traversal language for OLTP and OLAP graph systems.
* [[HTSQL]] is a query language that translates [[HTTP]] queries to [[SQL]];
* [[ISBL]] is a query language for [[PRTV]], one of the earliest relational database management systems;
* [[LINQ]] query-expressions is a way to query various data sources from [[.NET Framework|.NET]] languages
* [[LDAP]] is an [[application protocol]] for querying and modifying [[directory services]] running over [[TCP/IP]];
* LogiQL is a variant of Datalog and is the query language for the LogicBlox system.
* [[Molecular Query Language|MQL]] is a [[cheminformatics]] query language for a [[substructure search]] allowing beside nominal properties also numerical properties;
* [[MultiDimensional eXpressions|MDX]] is a query language for [[OLAP]] databases;
* [[N1QL]] is a [[Couchbase, Inc.|Couchbase]]'s query language finding data in [[Couchbase Server]]s;
* [[Object Query Language|OQL]] is Object Query Language;
* [[Object Constraint Language|OCL]] (Object Constraint Language). Despite its name, OCL is also an object query language and an [[Object Management Group|OMG]] standard;
* [[OPath]], intended for use in querying [[WinFS]] ''Stores'';
* [[OttoQL]], intended for querying tables, [[XML]], and databases;
* [[Poliqarp Query Language]] is a special query language designed to analyze annotated text. Used in the [[Poliqarp]] search engine;
* [[PQL]] is a [[special-purpose programming language]] for managing [[process model]]s based on information about [[wiktionary:Scenario|scenarios]] that these models describe;
* [[QUEL query languages|QUEL]] is a [[relational database]] access language, similar in most ways to [[SQL]];
* [[RDQL]] is a [[Resource Description Framework|RDF]] query language;
* [[ReQL]] is a query language used in [http://rethinkdb.com/docs/introduction-to-reql/ RethinkDB];
* [[Smiles arbitrary target specification|SMARTS]] is the [[cheminformatics]] standard for a [[substructure search]];
* [[SPARQL]] is a query language for [[Resource Description Framework|RDF]] [[Graph (discrete mathematics)|graphs]];
* [[SPL (Search Processing Language)|SPL]] is a search language for machine-generated [[big data]], based upon Unix Piping and SQL.
* SCL is the Software Control Language to query and manipulate [[Endevor]] objects
* [[SQL]] is a well known query language and [[Data Manipulation Language]] for [[relational database]]s;
* [[SuprTool]] is a proprietary query language for SuprTool, a database access program used for accessing data in ''Image/SQL'' (formerly [[TurboIMAGE]]) and Oracle databases;
* [[TMQL]] Topic Map Query Language is a query language for [[Topic Maps]];
* TQL is a language used to [http://cmshelpcenter.saas.hp.com/CMS/10.21/ucmdb-docs/docs/eng/doc_lib/Content/modeling/Tql_c_Overview.htm query topology for HP products] 
* [[D (data language specification)|Tutorial D]] is a query language for [[Relational database management system|truly relational database management systems]] (TRDBMS);
* [[XQuery]] is a query language for [[XML database|XML data sources]];
* [[XPath]] is a declarative language for navigating XML documents;
* [[XSPARQL]] is an integrated query language combining XQuery with SPARQL to query both XML and RDF data sources at once;
* [[Yahoo! query language|YQL]] is an [[SQL]]-like query language created by [[Yahoo!]]
* Search engine query languages, e.g., as used by [[Google Search|Google]]<ref>
{{cite web
| title = Search operators
| url = https://support.google.com/websearch/answer/2466433?hl=en
| accessdate = August 22, 2015
| publisher = Google
}}</ref> or [[Bing (search engine)|Bing]]<ref>
{{cite web
| title = Bing Query Language
| url = https://msdn.microsoft.com/en-us/library/ff795667.aspx
| accessdate = August 22, 2015
| publisher = Microsoft
}}</ref>

== See also ==
* [[Data manipulation language]]

== References ==
{{Reflist}}

{{Database}}
{{Databases}}
{{Computer language}}
{{Query languages}}

{{Authority control}}

{{DEFAULTSORT:Query Language}}
[[Category:Computer languages]]
[[Category:Data management]]
[[Category:Query languages|*]]

[[no:Database#Spørrespråk]]
<=====doc_Id=====>:149
<=====title=====>:
Enterprise Data Planning
<=====text=====>:
{{Orphan|date=October 2010}}
{| style="width: 80%; margin: 0 0 0 10%; border-collapse: collapse; background: #FBFBFB; border: 1px solid #aaa; border-left: 10px solid #f28500;"
|-
| style="width: 52px; padding: 2px 0px 2px 0.5em; text-align: center;" | [[Image:Newspaper nicu buculei 01.svg|50px]]
| style="padding: 0.25em 0.5em;" | '''This article or section reads like an [[Wikipedia:What Wikipedia is not#Wikipedia is not a soapbox|advertisement]] for EDMworks.'''<br/>To meet Wikipedia's [[:Category:Wikipedia style guidelines|quality standards]] and comply with Wikipedia's [[Wikipedia:Neutral point of view|neutral point of view]] policy, it may require [[Wikipedia:Cleanup|cleanup]].
|}
'''Enterprise Data Planning''' is the starting point for enterprise wide change. It states the destination and describes how you will get there. It defines benefits, costs and potential risks.  It provides measures to be used along the way to judge progress and adjust the journey according to changing circumstances.

[[Data]] is fundamental to investment enterprises. Effective, economic management of data underpins operations and enables transformations needed to satisfy customer demands, competition and regulation. Data warehouse(s) and other aspects of the overall [[data architecture]] are critical to the enterprise.

EDMworks has created a strategic data planning approach for the Investment Sector.  It consists of a planning process, planning intranets, templates and training materials.

EDMworks planning process is based on the belief that extensive domain knowledge significantly shortens planning iterations and enables progressively higher quality plans to be produced and implemented.<ref name=hull>Introduction to Futures and Options Markets (John Hull) 1995</ref><ref name=taylor>Mastering Derivatives Markets (Francesca Taylor) 2007</ref>  This approach drives the development of an effective and economic Enterprise Data Architecture.

Enterprise Data Planning is based on proven business disciplines.<ref name=stutely>The Definitive Business Plan (Richard Stutely) 2002</ref> Key architectural layers for data and applications are then added in order to provide an enterprise wide understanding of the uses and interdependencies of data.<ref name=tozer>Planning for Effective Business Information Systems ([[Edwin E. Tozer]]) 1998</ref> This enables the definition of the core components of the [[Enterprise data management|EDM]] plan:

* Industry structure and business objectives
* Assessment of systems and services
* Target architecture for applications, data and infrastructure
* Target organization structures
* Systems, database, infrastructure and organizational plans
* Business case, costs, benefits, results and risks.

EDMworks uses several components from the Open Systems Group [[TOGAF]] enterprise systems planning process. [[TOGAF]] acts as an extension to good business planning methods to provide a framework for the development of the systems and data architectural components.

==History==

[[James Martin (author)|James Martin]] was one of the pathfinders in data planning methodologies.  He was one of the first to identify data as being an enterprise wide asset that required management.  He developed a series of tools and methods to support that process.<ref name=Martin>Martin 1982</ref>
 
Most of the large consulting firms developed their own methods to address the same basic issue.  Frequently, their approaches were incorporated into their own branded system development methodologies that encompassed the complete systems development life-cycle. 
 
Others, such as [[Edwin E. Tozer|Ed Tozer]], developed more focused offerings that dealt with the complexities of extracting key business needs from senior management and then defining relevant architectural visions for the specific enterprise.<ref name=tozer/>
 
From these various sources, the concepts of Business, Data, Applications and Technology Architectures emerged. 
 
The Open Group Architectural Framework (TOGAF) has taken this work forward and has established a sound method in TOGAF version 9.
 
EDMworks approach is to adopt these planning and architectural practices as a basis and then add two additional dimensions to the planning and implementation focus:
* Domain knowledge of the Investments sector.  Investments is a complex global industry with a common set of characteristics about clients, information vendors, competition and regulation.  Domain knowledge significantly improves the quality of the planning and implementation processes
* Development of people and teams.  Change is a major feature of in any Enterprise Data Management program and people and teams both need development in order to make EDM effective throughout an organization.

== References ==
{{reflist}}

== External links ==
* [http://www.edmworks.com Enterprise Data Management works]

[[Category:Data management]]
<=====doc_Id=====>:152
<=====title=====>:
Synthetic data
<=====text=====>:
{{Citation style|date=May 2014}}'''Synthetic data''' are "any production data applicable to a given situation that are not obtained by direct measurement" according to the McGraw-Hill Dictionary of Scientific and Technical Terms;<ref name="McGraw">Synthetic data. (n.d.). ''McGraw-Hill Dictionary of Scientific and Technical Terms''. Retrieved November 29, 2009, from Answers.com Web site: [http://www.answers.com/topic/synthetic-data]</ref> where Craig S. Mullins, an expert in data management, defines production data as "information that is persistently stored and used by professionals to conduct business processes.".<ref name="Mullins">Mullins, Craig S. (2009, February 5). ''What is Production Data?'' Message posted to http://www.neon.com/blog/blogs/cmullins/archive/2009/02/05/What-is-Production-Data_3F00_.aspx</ref>

The creation of synthetic data is an involved process of data [[Anonymity|anonymization]]; that is to say that synthetic data is a [[subset]] of anonymized data.<ref name="MachanavajjhalaEtAl">{{Cite journal
  | title = Privacy: Theory meets Practice on the Map
  | journal = 2008 IEEE 24th International Conference on Data Engineering
  | doi = 10.1109/ICDE.2008.4497436
  | pages = 277–286
  | year = 2008
  | last1 = MacHanavajjhala
  | first1 = Ashwin
  | last2 = Kifer
  | first2 = Daniel
  | last3 = Abowd
  | first3 = John
  | last4 = Gehrke
  | first4 = Johannes
  | last5 = Vilhuber
  | first5 = Lars}}</ref> Synthetic data is used in a variety of fields as a filter for information that would otherwise compromise the [[confidentiality]] of particular aspects of the data. Many times the particular aspects come about in the form of human information (i.e. name, home address, [[IP address]], telephone number, social security number, credit card number, etc.).

== Usefulness ==

Synthetic data are generated to meet specific needs or certain conditions that may not be found in the original, real data.  This can be useful when designing any type of system because the synthetic data are used as a simulation or as a theoretical value, situation, etc.  This allows us to take into account unexpected results and have a basic solution or remedy, if the results prove to be unsatisfactory. Synthetic data are often generated to represent the authentic data and allows a baseline to be set.<ref name="Barse">Barse, E.L., Kvarnström, H., & Jonsson, E. (2003). ''Synthesizing test data for fraud detection systems.'' Manuscript submitted for publication, Department of Computer Engineering, Chalmbers University of Technology, Göteborg, Sweden. Retrieved from http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1254343&isnumber=28060</ref> Another use of synthetic data is to protect privacy and confidentiality of authentic data. As stated previously, synthetic data is used in testing and creating many different types of systems; below is a quote from the abstract of an article that describes a software that generates synthetic data for testing fraud detection systems that further explains its use and importance.
"This enables us to create realistic behavior profiles for users and attackers. The data is used to train the [[fraud]] detection system itself, thus creating the necessary adaptation of the system to a specific environment."<ref name="Barse"/>

==History==
The history of the generation of synthetic data dates back to 1993. In 1993, the idea of original fully synthetic data was created by [[Donald Rubin|Rubin]].<ref name="Rubin1993">{{Cite journal
  | authorlink = Rubin, Donald B.
  | title = Discussion: Statistical Disclosure Limitation
  | journal = Journal of Official Statistics
  | volume = 9
  | pages = 461–468
  | year = 1993}}
</ref> Rubin originally designed this to synthesize the Decennial Census long form responses for the short form households. He then released samples that did not include any actual long form records - in this he preserved anonymity of the household.<ref name="Abowd">
{{Cite web
  | last = Abowd
  | first = John M.
  | title = Confidentiality Protection of Social Science Micro Data: Synthetic Data and Related Methods. [Powerpoint slides]
  | url=http://www.idre.ucla.edu/events/PPT/2006_01_30_abowd_UCLA_synthetic_data_presentation.ppt
  | accessdate = 17 February 2011 }} 
</ref> Later that year, the idea of original partially synthetic data was created by Little. Little used this idea to synthesize the sensitive values on the public use file.<ref name="Little">{{Cite journal
  | authorlink = Little, Rod
  | title = Statistical Analysis of Masked Data
  | journal = Journal of Official Statistics
  | volume = 9
  | pages = 407–426
  | year = 1993}}
</ref>

In 1994, [[Stephen Fienberg|Fienberg]] came up with the idea of critical refinement, in which he used a parametric posterior predictive distribution (instead of a Bayes bootstrap) to do the sampling.<ref name="Abowd"/> Later, other important contributors to the development of synthetic data generation are [[Trivellore Raghunathan|Raghunathan]], [[Jerry Reiter|Reiter]], [[Donald Rubin|Rubin]], [[John M. Abowd|Abowd]], [[Jim Woodcock|Woodcock]]. Collectively they came up with a solution for how to treat partially synthetic data with missing data. Similarly they came up with the technique of Sequential Regression Multivariate [[Imputation (statistics)|Imputation]].<ref name="Abowd"/>

==Applications==
Synthetic data are used in the process of [[data mining]].  Testing and training [[fraud]] detection systems, confidentiality systems and any type of system is devised using synthetic data. As described previously, synthetic data may seem as just a compilation of “made up” data, but there are specific algorithms and generators that are designed to create realistic data.<ref name="Deng">Deng, R. (2002). ''Information and Communications Security''. Proceedings of the 4th International Conference, ICICS 2002 Singapore, December 2002. Retrieved from https://books.google.com/books?id=6mod7enQa8cC&pg=PA265&dq=%22synthetic+data%22#v=onepage&q=%22synthetic%20data%22&f=false</ref> This synthetic data assists in teaching a system how to react to certain situations or criteria. Researcher doing [[clinical trials]] or any other research may generate synthetic data to aid in creating a baseline for future studies and testing.  For example, intrusion detection software is tested using synthetic data. This data is a representation of the authentic data and may include intrusion instances that are not found in the authentic data. The synthetic data allows the software to recognize these situations and react accordingly. If synthetic data was not used, the software would only be trained to react to the situations provided by the authentic data and it may not recognize another type of intrusion.<ref name="Barse"/>

Synthetic data is also used to protect the [[privacy]] and [[confidentiality]] of a set of data. Real data contains personal/private/confidential information that a programmer, software creator or research project may not want to be disclosed.<ref name="Abowd2">Abowd, J.M., & Lane, J. (2004). ''New Approaches to Confidentiality Protection: Synthetic Data, Remote Access and Research Data Centers''. Manuscript submitted for publication, Cornell Institute for Social and Economic Research (CISER), Cornell University, Ithica, New York. Retrieved from http://www.springerlink.com/content/27nud7qx09qurg3p/fulltext.pdf</ref> Synthetic data holds no personal information and cannot be traced back to any individual; therefore, the use of synthetic data reduces confidentiality and privacy issues.

==Calculations==
Researchers test the framework on synthetic data, which is "the only source of ground truth on which they can objectively assess the performance of their [[algorithm]]s".<sup>10</sup>

"Synthetic data can be generated with random orientations and positions."<sup>8</sup>  Datasets can be get fairly complicated. A more complicated dataset can be generated by using a synthesizer build.  To create a synthesizer build, first use the original data to create a model or equation that fits the data the best. This model or equation will be called a synthesizer build. This build can be used to generate more data.<sup>9</sup>

Constructing a synthesizer build involves constructing a [[statistical model]].  In a [[linear regression]] line example, the original data can be plotted, and a best fit [[linear regression|linear line]] can be created from the data.  This [[linear regression|line]] is a synthesizer created from the original data.  The next step will be generating more synthetic data from the synthesizer build or from this linear line equation.  In this way, the new data can be used for studies and research, and it protects the [[confidentiality]] of the original data.<sup>9</sup>

David Jensen from the Knowledge Discovery Laboratory mentioned how to generate synthetic data in his "Proximity 4.3 Tutorial" chapter 6: "Researchers frequently need to explore the effects of certain data characteristics on their [[data model]]." To help construct [[data set|datasets]] exhibiting specific properties, such as [[autocorrelation|auto-correlation]] or degree disparity, proximity can generate synthetic data having one of several types of graph structure<sup>10</sup>:[[random graph]]s that is generated by some [[random process]];[[lattice graph]]s having a ring structure;[[lattice graph]]s having a grid structure, etc.
In all cases, the data generation process follows the same process:
1.	Generate the empty [[Graph (data structure)|graph structure]].
2.	Generate [[Attribute-value system|attribute values]] based on user-supplied prior probabilities.

Since the [[Attribute-value system|attribute values]] of one object may depend on the [[Attribute-value system|attribute values]] of related objects, the attribute generation process assigns values collectively.<sup>10</sup>

==References==
{{Reflist}}
* Wang, A, Qiu, T, & Shao, L. (2009). ''A Simple Method of Radial Distortion Correction with Centre of Distortion Estimation''. 35. Retrieved from http://www.springerlink.com/content/8180144q56t30314/fulltext.pdf
* Duncan, G. (2006). ''Statistical confidentiality: Is Synthetic Data the Answer?'' Retrieved from http://www.idre.ucla.edu/events/PPT/2006_02_13_duncan_Synthetic_Data.ppt
* Jensen, D. (2004). ''Proximity 4.3 Tutorial Chapter 6.'' Retrieved from http://kdl.cs.umass.edu/proximity/documentation/tutorial/ch06s09.html
* Jackson, C, Murphy, R, & Kovaˇcevic´, J. (2009). ''Intelligent Acquisition and Learning of Fluorescence Microscope Data Models.'' 18(9), Retrieved from http://www.andrew.cmu.edu/user/jelenak/Repository/08_JacksonMK.pdf
* {{cite book| author=Adam Coates and Blake Carpenter and Carl Case and Sanjeev Satheesh and Bipin Suresh and Tao Wang and David J. Wu and Andrew Y. Ng| chapter=Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning| title=ICDAR| year=2011| pages=440–445| accessdate=13 May 2014}}

==External links==
* The "DataGenerator" a model-based synthetic data generator: http://finraos.github.io/DataGenerator/
* The ''datgen'' synthetic data generator: http://www.datasetgenerator.com
* Fienberg, S. E. (1994). "Conflicts between the needs for access to statistical information and demands for confidentiality," Journal of Official Statistics 10, 115–132.
* Little, R (1993). "Statistical Analysis of Masked Data," Journal of Official Statistics, 9, 407-426.
* Raghunathan, T.E., Reiter, J.P., and Rubin, D.B. (2003). "Multiple Imputation for Statistical Disclosure Limitation," Journal of Official Statistics, 19, 1-16.
* Reiter, J.P. (2004). "Simultaneous Use of Multiple Imputation for Missing Data and Disclosure Limitation," Survey Methodology, 30, 235-242.

 	
{{FOLDOC}}
{{Statistics}}

[[Category:Data]]
[[Category:Computer data]]
[[Category:Data management]]
<=====doc_Id=====>:155
<=====title=====>:
White pages schema
<=====text=====>:
{{Unreferenced|date=December 2009}}
A '''white pages schema''' is a [[data model]], specifically a [[logical schema]], for organizing the data contained in entries in a [[directory service]], database, or application, such as an address book.  In a white pages directory, each entry typically represents an individual [[user (computing)|person that makes use of]] network resources, such as by receiving email or having an account to log in to a system.
In some environments, the schema may also include the representation of organizational divisions, roles, groups, and devices.  The term is derived from the [[white pages]], the listing of individuals in a [[telephone directory]], typically sorted by the individual's home location (e.g. city) and then by
their name.

While many [[Postal Telephone and Telegraph|telephone service providers]] have for decades published a list of their [[subscriber]]s in a [[telephone directory]], and similarly corporations published a list of their employees in an internal directory, it was not until the rise of [[electronic mail]] systems that a requirement for standards for the electronic exchange of [[subscriber]] information between different systems appeared.

A white pages schema typically defines, for each real-world object being represented:

* what attributes of that object are to be represented in the entry for that object
* what relationships of that object to other objects are to be represented
* how is the entry to be named in a [[Directory Information Tree|DIT]]
* how an entry is to be located by a client searching for it
* how similar entries are to be distinguished
* how are entries to be ordered when displayed in a list

One of the earliest attempts to standardize a white pages schema for electronic mail use was in [[X.520]] and [[X.521]], part of the [[X.500]] specifications,
that was derived from the addressing requirements of [[X.400]] and defined a [[Directory Information Tree]] that mirrored the international telephone system, with entries representing residential and organizational subscribers.  This evolved into the [[Lightweight Directory Access Protocol]] standard schema in RFC 2256.  One of the most widely deployed white pages schemas used in LDAP
for representing individuals in an organizational context is '''inetOrgPerson''', defined in RFC 2798, although versions of [[Active Directory]] require a different object class, '''User'''.  Many large organizations have
also defined their own white pages schemas for their employees or customers, as part of their [[Identity management]] architecture.  Converting between data bases and directories using different schemas is often the
function of a [[Metadirectory]], and data interchange standards such as [[Common Indexing Protocol]].

Some early directory deployments suffered due to poor design choices in their white pages schema, such as:

* attributes used for naming purposes were non-unique in large environments (such as a person's common name)
* attributes used for naming purposes were likely to change (such as surnames)
* attributes were included which could lead to [[Identity theft]], such as a [[Social security number]]
* users were required during [[provisioning]] to choose attributes which are unique but still memorable to them

Numerous other proposed schemas exist, both as standalone definitions suitable for use with general purpose
directories, or as embedded into network protocols.

Examples of other generic white pages schemas include [[vCard]], defined in RFC 2426, and [[FOAF (software)|FOAF]].

==See also==
* [[Recognition of human individuals]]

{{DEFAULTSORT:White Pages Schema}}
[[Category:Data modeling]]
[[Category:Data management]]
[[Category:Identity management]]
<=====doc_Id=====>:158
<=====title=====>:
Uniform data access
<=====text=====>:
{{Unreferenced stub|auto=yes|date=December 2009}}
'''Uniform data access''' is a computational concept describing an even-ness of connectivity and controllability across numerous target data sources.  

Necessary to fields such as [[Enterprise Information Integration]] (EII) and [[Electronic Data Interchange]] (EDI), it is most often used regarding analysis of disparate data types and data sources, which must be rendered into a [[uniform information representation]], and generally must appear [[wiktionary:Homogenous|homogenous]] to the analysis tools—when the data being analyzed is typically [[heterogeneous]] and widely varying in size, type, and original representation.{{DEFAULTSORT:Uniform Data Access}}
[[Category:Data management]]


{{Comp-sci-stub}}
<=====doc_Id=====>:161
<=====title=====>:
Data auditing
<=====text=====>:
{{Unreferenced|date=December 2009}}
'''Data auditing''' is the process of conducting a data audit to assess how company's data is fit for given purpose.  This involves [[data profiling|profiling]] the data and assessing the impact of [[data quality|poor quality data]] on the organization's performance and profits.

{{DEFAULTSORT:Data Auditing}}
{{Tech-stub}}

[[Category:Data management]]
[[Category:Data quality]]
<=====doc_Id=====>:164
<=====title=====>:
Data virtualization
<=====text=====>:
'''Data virtualization''' is any approach to data management that allows an application to retrieve and manipulate data without requiring technical details about the data, such as how it is formatted at source, or where it is physically located.<ref>[http://searchdatamanagement.techtarget.com/definition/data-virtualization "What is Data Virtualization?"], Margaret Rouse, TechTarget.com, retrieved 19 August 2013</ref>

Unlike the traditional [[extract, transform, load]] ("ETL") process, the data remains in place, and real-time access is given to the source system for the data, thus reducing the risk of data errors and reducing the workload of moving data around that may never be used.

Unlike a [[federated database system]], it does not attempt to impose a single data model on the data (heterogeneous data). The technology also supports the writing of transaction data updates back to the source systems.<ref name="morgan">[http://www.computerweekly.com/feature/Data-virtualisation-on-rise-as-ETL-alternative-for-data-integration "Data virtualisation on rise as ETL alternative for data integration"] Gareth Morgan, Computer Weekly, retrieved 19 August 2013</ref>

To resolve differences in source and consumer formats and semantics, various abstraction and transformation techniques are used.
This concept and software is a subset of [[data integration]] and is commonly used within [[business intelligence]], [[service-oriented architecture]] data services, [[cloud computing]], [[enterprise search]], and [[master data management]].

==Examples ==
* The Phone House—the trading name for the European operations of UK-based mobile phone retail chain [[Carphone Warehouse]]—implemented [[Denodo]]’s data virtualization technology between its Spanish subsidiary’s transactional systems and the Web-based systems of mobile operators.<ref name="morgan"/>
* [[Novartis]], which implemented a data virtualization tool from [[Composite Software]] to enable its researchers to quickly combine data from both internal and external sources into a searchable virtual data store.<ref name="morgan"/>
* The storage-agnostic [http://primarydata.com/ Primary Data] data virtualization platform enables applications, servers, and clients to transparently access data while it is intelligently migrated between direct-attached, network-attached, private and public cloud storage. Server flash memory pioneer [[Fusion-io]] co-founder David Flynn, now Primary Data CTO, saw the need to move data across storage types to maximize efficiency with data virtualization.
* [[Linked Data]] can use a single hyperlink-based [[Data Source Name]] ([[Data Source Name|DSN]]) to provide a connection to a virtual database layer that is internally connected to a variety of back-end data sources using [[ODBC]], [[JDBC]], [[OLE DB]], [[ADO.NET]], [[Service-oriented architecture|SOA]]-style services, and/or [[REST]] patterns.
* [[Database virtualization]] may use a single ODBC-based DSN to provide a connection to a similar virtual database layer.

==Functionality==

Data Virtualization software provides some or all of the following capabilities:

* '''Abstraction''' –  Abstract the technical aspects of stored data, such as location, storage structure, API, access language, and storage technology. 
* '''Virtualized Data Access''' – Connect to different data sources and make them accessible from a common logical data access point.
* '''Transformation''' – Transform, improve quality, reformat, etc. source data for consumer use. 
* '''Data Federation''' – Combine result sets from across multiple source systems. 
* '''Data Delivery''' – Publish result sets as views and/or data services executed by client application or users when requested.

Data virtualization software may include functions for development, operation, and/or management.

Benefits include:
* Reduce risk of data errors
* Reduce systems workload through not moving data around
* Increase speed of access to data on a real-time basis
* Significantly reduce development and support time
* Increase governance and reduce risk through the use of policies<ref>[http://www.informatica.com/us/products/data-virtualization/data-services/ "Rapid Access to Disparate Data Across Projects Without Rework"] Informatica, retrieved 19 August 2013</ref>
* Reduce data storage required<ref>[http://www.zdnet.com/blog/service-oriented/data-virtualization-6-best-practices-to-help-the-business-get-it/7897 Data virtualization: 6 best practices to help the business 'get it'] Joe McKendrick, ZDNet, 27 October 2011</ref>

Drawbacks include:
* May impact Operational systems response time, particularly if under-scaled to cope with unanticipated user queries or not tuned early on<ref>[http://searchdatamanagement.techtarget.com/news/2240165242/IT-pros-reveal-the-benefits-drawbacks-of-data-virtualization-software|IT pros reveal benefits, drawbacks of data virtualization software"] Mark Brunelli, SearchDataManagement, 11 October 2012</ref>
* Does not impose a heterogeneous data model, meaning the user has to interpret the data, unless combined with [[Federated database system|Data Federation]] and business understanding of the data<ref name="lawson">[http://www.itbusinessedge.com/cm/blogs/lawson/the-pros-and-cons-of-data-virtualization/?cs=48794 "The Pros and Cons of Data Virtualization"] Loraine Lawson, BusinessEdge, 7 October 2011</ref>
* Requires a defined Governance approach to avoid budgeting issues with the shared services
* Not suitable for recording the historic snapshots of data - data warehouse is better for this<ref name="lawson"/>
* Change management "is a huge overhead, as any changes need to be accepted by all applications and users sharing the same virtualization kit"<ref name="lawson"/>

== Technology ==

Some data virtualization technologies include:

*  [[Actifio]] Copy Data Virtualization
*  [[Capsenta]]'s Ultrawrap Platform <ref>https://capsenta.com/</ref>
*  [[Cisco]] Data Virtualization (formerly [[Composite Software]])
*  [[Denodo|Denodo Platform]]
* DataVirtuality
* Data Virtualization Platform
*  HiperFabric Data Virtualization and Integration
* Stonebond Enterprise Enabler Data Virtualization Platform
* [[Red Hat]] [[JBoss Enterprise Application Platform]] Data Virtualization
* [[XAware]] Data Services

==History==
[[Enterprise information integration]] (EII), first coined by Metamatrix, now known as Red Hat JBoss Data Virtualization, and [[federated database system]]s are terms used by some vendors to describe a core element of data virtualization: the capability to create relational JOINs in a federated VIEW.

==See also==

* [[Data integration]]
* [[Enterprise information integration]] (EII)
* [[Master data management]]
* [[Database virtualization]]
* [[Federated database system|Data Federation]]
* [[Disparate system]]

==References==
{{reflist}}

==Further reading==
* '''Data Virtualization: Going Beyond Traditional Data Integration to Achieve Business Agility''', Judith R. Davis and Robert Eve
* '''Data Virtualization for Business Intelligence Systems: Revolutionizing Data Integration for Data Warehouses''' Rick van der Lans
* '''Data Integration Blueprint and Modeling: Techniques for a Scalable and Sustainable Architecture ''' Anthony Giordano

[[Category:Data management]]
<=====doc_Id=====>:167
<=====title=====>:
Content migration
<=====text=====>:
{{Multiple issues|
{{primary sources|date=March 2011}}
{{cleanup|date=March 2011}}
}}

'''Content Migration''' is the process of moving information stored on a [[Web content management system]]  (CMS), [[Digital asset management]] (DAM), [[Document management system]] (DMS), or flat HTML based system to a new system. Flat HTML content can entail HTML files, [[Active Server Pages]] (ASP), [[JavaServer Pages]] (JSP), [[PHP]], or content stored in some type of [[HTML]]/[[JavaScript]] based system and can be either static or dynamic content.   

Content Migrations can solve a number of issues ranging from:
* Consolidation from one or more CMS systems into one system to allow for more centralized control, governance of content, and better   Knowledge    management and sharing.
* Reorganizing content due to mergers and acquisitions to assimilate as much content from the source systems for a unified look and feel.
* Converting content that has grown organically either in a CMS or Flat HTML and standardizing the formatting so standards can be applied for a unified branding of the content.

There are many ways to access the content stored in a CMS.  Depending on the CMS vendor they offer either an  [[Application programming interface]] (API), [[Web services]], rebuilding a record by writing [[SQL]] queries, [[XML]] exports, or through the web interface.

# The API<ref name="refname1"/> requires a developer to read and understand how to interact with the source CMS’s API layer then develop an application that extracts the content and stores it in a database, XML file, or Excel. Once the content is extracted the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
# Most CMSs use a database to store and associate content so if no API exists the SQL programmer must reverse engineer the table structure.  Once the structure is reverse engineered, very complex SQL queries are written to pull all the content from multiple tables into an intermediate table or into some type of [[Comma-separated values]] (CSV) or XML file.   Once the developer has the files or database the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
# XML export creates XML files of the content stored in a CMS but after the files are exported they need to be altered to fit the new scheme of the target CMS system.  This is typically done by a developer by writing some code to do the transformation.
# HTML files, JSP, ASP, PHP, or other application server file formats are the most difficult.  The  structure for Flat HTML files are based on a culmination of  folder structure, HTML file structure, and image locations.  In the early days of content migration, the developer had to use programming languages to parse the html files and save it as structured database, XML or CSV. Typically PERL, JAVA, C++, or C# were used because of the regular expression handling capability.  JSP, ASP, PHP, ColdFusion, and other Application Server technologies usually rely on server side includes to help simplify development but makes it very difficult to migrate content because the content is not assembled until the user looks at it in their web browser.  This makes is very difficult to look at the files and extract the content from the file structure.
# Web Scraping allows users to access most of the content directly from the Web User Interface.  Since a web interface is visual (this is the point of a CMS) some Web Scrapers leverage the UI to extract content and place it into a structure like a Database, XML, or CSV formats.  All CMSs, DAMs, and DMSs use  web interfaces so extracting the content for one or many source sites is basically the same process.  In some cases it is possible to push the content into the new CMS using the web interface but some CMSs use JAVA applets, or Active X Control which are not supported by most web scrapers.  In that case the developer must read and understand the target CMS API and develop code to push the content into the new System.  The same can be said for Web Services.
'''The basic content migration flow'''

1. Obtain an inventory of the content.<br />
2. Obtain an inventory of Binary content like Images, PDFs, CSS files, Office Docs, Flash, and any binary objects.<br />
3. Find any broken links in the content or content resources.<br />
4. Determine the Menu Structure of the Content.<br />
5. Find the parent/sibling connection to the content so the links to other content and resources are not broken when moving them.<br />
6. Extract the Resources from the pages and store them into a Database or File structure.  Store the reference in a database or a File.<br />
7. Extract the HTML content from the site and store locally.<br />
8. Upload the resources to the new CMS either by using the API or the web interface and store the new location in a Database or XML.<br />
9. Transform the HTML to meet the new CMSs standards and reconnect any resources.<br />
10. Upload the transformed content into the new system.

== References ==
<references>
<ref name="refname1">[http://msdn.microsoft.com/en-us/library/ms453426.aspx What the Content Migration APIs Are Not]</ref>
</references>

==External links==
* [http://www.cmswire.com/cms/web-publishing/no-small-task-migrating-content-to-a-new-cms-002437.php No Small Task: Migrating Content to a New CMS]

[[Category:Data management]]
<=====doc_Id=====>:170
<=====title=====>:
Data validation and reconciliation
<=====text=====>:
'''Industrial process data validation and reconciliation''', or more briefly, '''data validation and reconciliation (DVR)''', is a technology that uses process information and mathematical methods in order to automatically correct measurements in industrial processes. The use of DVR allows for extracting accurate and reliable information about the state of industry processes from raw measurement data and produces a single consistent set of data representing the most likely process operation.

==Models, data and measurement errors==
Industrial processes, for example chemical or thermodynamic processes in chemical plants, refineries, oil or gas production sites, or power plants, are often represented by two fundamental means:
# Models that express the general structure of the processes,
# Data that reflects the state of the processes at a given point in time.
Models can have different levels of detail, for example one can incorporate simple mass or compound conservation balances, or more advanced thermodynamic models including energy conservation laws. Mathematically the model can be expressed by a [[nonlinear system|nonlinear system of equations]] <math>F(y)=0\,</math> in the variables <math>y=(y_1,\ldots,y_n)</math>, which incorporates all the above-mentioned system constraints (for example the mass or heat balances around a unit). A variable could be the temperature or the pressure at a certain place in the plant.

===Error types===
<gallery caption="Random and systematic errors" widths="300%" perrow="2" align="right">
File:Normal_no_bias.jpg|Normally distributed measurements without bias.
File:Normal_with_bias.jpg|Normally distributed measurements with bias.
</gallery>
Data originates typically from [[measurements]] taken at different places throughout the industrial site, for example temperature, pressure, volumetric flow rate measurements etc. To understand the basic principles of DVR, it is important to first recognize that plant measurements are never 100% correct, i.e. raw measurement <math>y\,</math> is not a solution of the nonlinear system <math>F(y)=0\,\!</math>. When using measurements without correction to generate plant balances, it is common to have incoherencies. [[Observational error|Measurement errors]] can be categorized into two basic types:
# [[random error]]s due to intrinsic [[sensor]] [[accuracy]] and
# [[systematic errors]] (or gross errors) due to sensor [[calibration]] or faulty data transmission.

[[Random error]]s means that the measurement <math>y\,\!</math> is a [[random variable]] with [[mean]] <math>y^*\,\!</math>, where <math>y^*\,\!</math> is the true value that is typically not known. A [[systematic error]] on the other hand is characterized by a measurement <math>y\,\!</math> which is a random variable with [[mean]] <math>\bar{y}\,\!</math>, which is not equal to the true value <math>y^*\,</math>.  For ease in deriving and implementing an optimal estimation solution, and based on arguments that errors are the sum of many factors (so that the [[Central limit theorem]] has some effect), data reconciliation assumes these errors are [[normal distribution|normally distributed]].   

Other sources of errors when calculating plant balances include process faults such as leaks, unmodeled heat losses, incorrect physical properties or other physical parameters used in equations, and incorrect structure such as unmodeled bypass lines.  Other errors include unmodeled plant dynamics such as holdup changes, and other instabilities in plant operations that violate steady state (algebraic) models.  Additional dynamic errors arise when measurements and samples are not taken at the same time, especially lab analyses.  

The normal practice of using time averages for the data input partly reduces the dynamic problems.  However, that does not completely resolve timing inconsistencies for infrequently-sampled data like lab analyses.  

This use of average values, like a [[moving average]], acts as a [[low-pass filter]], so high frequency noise is mostly eliminated.   The result is that, in practice, data reconciliation is mainly making adjustments to correct systematic errors like biases.

===Necessity of removing measurement errors===
ISA-95 is the international standard for the integration of enterprise and control systems<ref>[http://www.isa-95.com/ "ISA-95: the international standard for the integration of enterprise and control systems"]. isa-95.com.</ref> It asserts that:
<blockquote>Data reconciliation is a serious issue for enterprise-control integration. The data have to be valid to be useful for the enterprise system. The data must often be determined from physical measurements that have associated error factors. This must usually be converted into exact values for the enterprise system. This conversion may require manual, or intelligent reconciliation of the converted values [...].

Systems must be set up to ensure that accurate data are sent to production and from production. Inadvertent operator or clerical errors may result in too much production, too little production, the wrong production, incorrect inventory, or missing inventory.</blockquote>

==History==
DVR has become more and more important due to industrial processes that are becoming more and more complex. DVR started in the early 1960s with applications aiming at closing [[mass balance|material balances]] in production processes where raw measurements were available for all [[variable (mathematics)|variables]].<ref>D.R. Kuehn, H. Davidson, ''Computer Control II. Mathematics of Control'', Chem. Eng. Process 57: 44–47, 1961.</ref> At the same time the problem of [[systematic error|gross error]] identification and elimination has been presented.<ref>V. Vaclavek, ''Studies on System Engineering I. On the Application of the Calculus of the Observations of Calculations of Chemical Engineering Balances'', Coll. Czech Chem. Commun 34: 3653, 1968.</ref> In the late 1960s and 1970s unmeasured variables were taken into account in the data reconciliation process.,<ref>V. Vaclavek, M. Loucka, ''Selection of Measurements Necessary to Achieve Multicomponent Mass Balances in Chemical Plant'', Chem. Eng. Sci. 31: 1199–1205, 1976.</ref><ref name="Mah-Stanley-Downing-1976">[http://gregstanleyandassociates.com/ReconciliationRectificationProcessData-1976.pdf R.S.H. Mah, G.M. Stanley, D.W. Downing, ''Reconciliation and Rectification of Process Flow and Inventory Data'', Ind. & Eng. Chem. Proc. Des. Dev. 15: 175–183, 1976.]</ref> DVR also became more mature by considering general nonlinear equation systems coming from thermodynamic models.,<ref>J.C. Knepper, J.W. Gorman, ''Statistical Analysis of Constrained Data Sets'', AiChE Journal 26: 260–164, 1961.</ref>
,<ref name="Stanley-Mah-1977">[http://gregstanleyandassociates.com/AIChEJ-1977-EstimationInProcessNetworks.pdf G.M. Stanley and R.S.H. Mah, ''Estimation of Flows and Temperatures in Process Networks'', AIChE Journal 23: 642–650, 1977.]</ref>
<ref>P. Joris, B. Kalitventzeff, ''Process measurements analysis and validation'', Proc. CEF’87: Use Comput. Chem. Eng., Italy, 41–46, 1987.</ref> Quasi steady state dynamics for filtering and simultaneous parameter estimation over time were introduced in 1977 by Stanley and Mah.<ref name="Stanley-Mah-1977"/>  Dynamic DVR was formulated as a nonlinear optimization problem by Liebman et al. in 1992.<ref>M.J. Liebman, T.F. Edgar, L.S. Lasdon, ''Efficient Data Reconciliation and Estimation for Dynamic Processes Using Nonlinear Programming Techniques'', Computers Chem. Eng. 16: 963–986, 1992.</ref>

==Data reconciliation==
Data reconciliation is a technique that targets at correcting measurement errors that are due to measurement noise, i.e. [[random error]]s. From a statistical point of view the main assumption is that no [[systematic errors]] exist in the set of measurements, since they may bias the reconciliation results and reduce the robustness of the reconciliation.

Given <math>n</math> measurements <math>y_i</math>, data reconciliation can mathematically be expressed as an [[optimization problem]] of the following form:

<math> \begin{align}
 \min_{x,y^*} & \sum_{i=1}^n\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2 \\
\text{subject to  }      & F(x,y^*)=0 \\
& y_\min \le y^*\le y_\max\\
& x_\min \le x\le x_\max,
\end{align}\,\!
</math>

where
<math>y_i^*\,\!</math> is the reconciled value of the <math>i</math>-th measurement (<math>i=1,\ldots,n\,\!</math>), <math>y_i\,\!</math> is the measured value of the <math>i</math>-th measurement (<math>i=1,\ldots,n\,\!</math>), <math>x_j\,\!</math> is the <math>j</math>-th unmeasured variable (<math>j=1,\ldots,m\,\!</math>),  and <math>\sigma_i\,\!</math> is the standard deviation of the <math>i</math>-th measurement (<math>i=1,\ldots,n\,\!</math>),
<math>F(x,y^*)=0\,\!</math> are the <math>p\,\!</math> process equality constraints and
<math>x_{\min}, x_{\max}, y_{\min}, y_{\max}\,\!</math> are the bounds on the measured and unmeasured variables.

The term <math>\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2\,\!</math> is called the ''penalty'' of measurement ''i''. The objective function is the sum of the penalties, which will be denoted in the following by <math>f(y^*)=\sum_{i=1}^n\left(\frac{y_i^*-y_i}{\sigma_i}\right)^2</math>.

In other words, one wants to minimize the overall correction (measured in the least squares term) that is needed in order to satisfy the [[constraint (mathematics)|system constraints]]. Additionally, each least squares term is weighted by the [[standard deviation]]  of the corresponding measurement.

===Redundancy===
<gallery caption="Sensor and topological redundancy" heights="150px" widths="225px" perrow="2" align="right">
File:sensor_red.jpg|Sensor redundancy arising from multiple sensors of the same quantity at the same time at the same place.
File:topological_red.jpg|Topological redundancy arising from model information, using the mass conservation constraint <math>a=b+c\,\!</math>, for example one can calculate <math>c\,\!</math>, when <math>a\,\!</math> and <math>b\,\!</math> are known.
</gallery>
Data reconciliation relies strongly on the concept of redundancy to correct the measurements as little as possible in order to satisfy the process constraints.  Here, redundancy is defined differently from [[Redundancy (information theory)|redundancy in information theory]].  Instead, redundancy arises from combining sensor data with the model (algebraic constraints), sometimes more specifically called "spatial redundancy",<ref name="Stanley-Mah-1977"/> "analytical redundancy", or "topological redundancy". 

Redundancy can be due to [[redundancy (engineering)|sensor redundancy]], where sensors are duplicated in order to have more than one measurement of the same quantity. Redundancy also arises when a single variable can be estimated in several independent ways from separate sets of measurements at a given time or time averaging period, using the algebraic constraints.  

Redundancy is linked to  the concept of [[observability]].  A variable (or system) is observable if the models and sensor measurements can be used to uniquely determine its value (system state).  A sensor is redundant if its removal causes no loss of observability.   Rigorous definitions of observability, calculability, and redundancy, along with criteria for determining it, were established by Stanley and Mah,<ref name="Stanley-Mah-1981a">
[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981a-ObservabilityRedundancy.pdf Stanley G.M. and Mah, R.S.H., "Observability and Redundancy in Process Data Estimation, Chem. Engng. Sci. 36, 259 (1981)]</ref> for these cases with set constraints such as algebraic equations and inequalities.    Next, we illustrate some special cases:

Topological redundancy  is intimately linked with the [[degrees of freedom (physics and chemistry)|degrees of freedom]] (<math>dof\,\!</math>) of a mathematical system,<ref name="vdi">VDI-Gesellschaft Energie und Umwelt, "Guidelines - VDI 2048 Blatt 1 - Uncertainties of measurements at acceptance tests for energy conversion and power plants - Fundamentals", ''[http://www.vdi.de/401.0.html Association of German Engineers]'', 2000.</ref> i.e. the minimum number of pieces of information (i.e. measurements) that are required in order to calculate all of the system variables. For instance, in the example above the flow conservation requires that <math>a=b+c\,</math>.  One needs to know the value of two of the 3 variables in order to calculate the third one. The degrees of freedom for the model in that case is equal to 2.  At least 2 measurements are needed to estimate all the variables, and 3 would be needed for redundancy.

When speaking about topological redundancy we have to distinguish between measured and unmeasured variables. In the following let us denote by <math>x\,\!</math> the unmeasured variables and <math>y\,\!</math> the measured variables. Then the system of the process constraints becomes <math>F(x,y)=0\,\!</math>, which is a nonlinear system in <math>y\,\!</math> and <math>x\,\!</math>.
If the system <math>F(x,y)=0\,\!</math> is calculable with the <math>n\,</math> measurements given, then the level of topological redundancy is defined as <math>red= n - dof\,\!</math>, i.e. the number of additional measurements that are at hand on top of those measurements which are required in order to just calculate the system. Another way of viewing the level of redundancy is to use the definition of <math>dof\,</math>, which is the difference between the number of variables (measured and unmeasured) and the number of equations. Then one gets

:<math>\begin{align}
red= n - dof = n-(n+m-p) = p-m,
\end{align}</math>

i.e. the redundancy is the difference between the number of equations <math>p\,</math> and the number of unmeasured variables <math>m\,</math>. The level of total redundancy is the sum of sensor redundancy and topological redundancy. We speak of positive redundancy if the system is calculable and the total redundancy is positive. One can see that the level of topological redundancy merely depends on the number of equations (the more equations the higher the redundancy) and the number of unmeasured variables (the more unmeasured variables, the lower the redundancy) and not on the number of measured variables. 

Simple counts of variables, equations, and measurements are inadequate for many systems, breaking down for several reasons: (a) Portions of a system might have redundancy, while others do not, and some portions might not even be possible to calculate, and  (b) Nonlinearities can lead to different conclusions at different operating points.  As an example, consider the following system with 4 streams and 2 units.

====Example of calculable and non-calculable systems====
<gallery caption="Calculable and non-calculable systems" heights="150px" widths="225px" perrow="2" align="right">
File:calculable_system.jpg|Calculable system, from <math>d\,\!</math> one can compute <math>c\,\!</math>, and knowing <math>a\,\!</math> yields <math>b\,\!</math>.
File:uncalculable_system.jpg|non-calculable system, knowing <math>c\,\!</math> does not give information about <math>a\,\!</math> and <math>b\,\!</math>.
</gallery>

We incorporate only flow conservation constraints and obtain <math>a+b=c\,\!</math> and <math>c=d\,\!</math>.  It is possible that the system <math>F(x,y)=0\,\!</math> is not calculable, even though <math>p-m\ge 0\,\!</math>.

If we have measurements for <math>c\,\!</math> and <math>d\,\!</math>, but not for <math>a\,\!</math> and <math>b\,\!</math>, then the system cannot be calculated (knowing <math>c\,\!</math> does not give information about <math>a\,\!</math> and <math>b\,\!</math>). On the other hand, if <math>a\,\!</math> and <math>c\,\!</math> are known, but not <math>b\,\!</math> and <math>d\,\!</math>, then the system can be calculated.

In 1981, observability and redundancy criteria were proven for these sorts of flow networks involving only mass and energy balance constraints.<ref name="Stanley-Mah-1981b">[http://gregstanleyandassociates.com/whitepapers/DataRec/CES-1981b-ObservabilityRedundancyProcessNetworks.pdf Stanley G.M., and Mah R.S.H., "Observability and Redundancy Classification in Process Networks", Chem. Engng. Sci. 36, 1941 (1981) ]</ref>  After combining all the plant inputs and outputs into an "environment node",  loss of observability corresponds to cycles of unmeasured streams.  That is seen in the second case above, where streams a and b are in a cycle of unmeasured streams.  Redundancy classification follows, by testing for a path of unmeasured streams, since that would lead to an unmeasured cycle if the measurement was removed.  Measurements c and d are redundant in the second case above, even though part of the system is unobservable.

===Benefits===
Redundancy can be used as a source of information to cross-check and correct the measurements <math>y\,\!</math> and increase their accuracy and precision: on the one hand they reconciled Further, the data reconciliation problem presented above also includes unmeasured variables <math>x\,\!</math>. Based on information redundancy, estimates for these unmeasured variables can be calculated along with their accuracies. In industrial processes these unmeasured variables that data reconciliation provides are referred to as [[soft sensor]]s or virtual sensors, where hardware sensors are not installed.

==Data validation==
Data validation denotes all validation and verification actions before and after the reconciliation step.

===Data filtering===
Data filtering denotes the process of treating measured data such that the values become meaningful and lie within the range of expected values. Data filtering is necessary before the reconciliation process in order to increase robustness of the reconciliation step. There are several ways of data filtering, for example taking the [[average]] of several measured values over a well-defined time period.

===Result validation===
Result validation is the set of validation or verification actions taken after the reconciliation process and it takes into account measured and unmeasured variables as well as reconciled values. Result validation covers, but is not limited to, penalty analysis for determining the reliability of the reconciliation, or bound checks to ensure that the reconciled values lie in a certain range, e.g. the temperature has to be within some reasonable bounds.

===Gross error detection===
Result validation may include statistical tests to validate the reliability of the reconciled values, by checking whether [[systematic error|gross errors]] exist in the set of measured values. These tests can be for example
* the chi square test (global test)
* the individual test.

If no gross errors exist in the set of measured values, then each penalty term in the objective function is a [[normal distribution|random variable]] that is normally distributed with mean equal to 0 and variance equal to 1. By consequence, the objective function is a random variable which follows a [[chi-square distribution]], since it is the sum of the square of normally distributed random variables. Comparing the value of the objective function <math>f(y^*)\,\!</math> with a given [[percentile]] <math>P_{\alpha}\,</math> of the probability density function of a chi-square distribution (e.g. the 95th percentile for a 95% confidence) gives an indication of whether a gross error exists: If <math>f(y^*)\le P_{95}</math>, then no gross errors exist with 95% probability. The chi square test gives only a rough indication about the existence of gross errors, and it is easy to conduct: one only has to compare the value of the objective function with the critical value of the chi square distribution.

The individual test compares each penalty term in the objective function with the critical values of the normal distribution. If the <math>i</math>-th penalty term is outside the 95% confidence interval of the normal distribution, then there is reason to believe that this measurement has a gross error.

==Advanced data validation and reconciliation==
Advanced data validation and reconciliation (DVR) is an integrated approach of combining data reconciliation and data validation techniques, which is characterized by
* complex models incorporating besides mass balances also thermodynamics, momentum balances, equilibria constraints, hydrodynamics etc.
* gross error remediation techniques to ensure meaningfulness of the reconciled values,
* robust algorithms for solving the reconciliation problem.

===Thermodynamic models===
Simple models include mass balances only. When adding thermodynamic constraints such as [[First law of thermodynamics|energy balances]] to the model, its scope and the level of [[Data redundancy|redundancy]] increases. Indeed, as we have seen above, the level of redundancy is defined as <math>p-m</math>, where <math>p</math> is the number of equations. Including energy balances means adding equations to the system, which results in a higher level of redundancy (provided that enough measurements are available, or equivalently, not too many variables are unmeasured).

===Gross error remediation===
[[image:scheme reconciliation.jpg|thumb|350px|The workflow of an advanced data validation and reconciliation process.]]
Gross errors are measurement systematic errors that may [[bias]] the reconciliation results. Therefore it is important to identify and eliminate these gross errors from the reconciliation process. After the reconciliation [[statistical tests]] can be applied that indicate whether or not a gross error does exist somewhere in the set of measurements. These techniques of gross error remediation are based on two concepts:
* gross error elimination
* gross error relaxation.
Gross error elimination determines one measurement that is biased by a systematic error and discards this measurement from the data set. The determination of the measurement to be discarded is based on different kinds of penalty terms that express how much the measured values deviate from the reconciled values. Once the gross errors are detected they are discarded from the measurements and the reconciliation can be done without these faulty measurements that spoil the reconciliation process. If needed, the elimination is repeated until no gross error exists in the set of measurements.

Gross error relaxation targets at relaxing the estimate for the uncertainty of suspicious measurements so that the reconciled value is in the 95% confidence interval. Relaxation typically finds application when it is not possible to determine which measurement around one unit is responsible for the gross error (equivalence of gross errors). Then measurement uncertainties of the measurements involved are increased.

It is important to note that the remediation of gross errors reduces the quality of the reconciliation, either the redundancy decreases (elimination) or the uncertainty of the measured data increases (relaxation). Therefore it can only be applied when the initial level of redundancy is high enough to ensure that the data reconciliation can still be done (see Section 2,<ref name="vdi" />).

===Workflow===
Advanced DVR solutions offer an integration of the techniques mentioned above:
# data acquisition from data historian, data base or manual inputs
# data validation and filtering of raw measurements
# data reconciliation of filtered measurements
# result verification
#* range check
#* gross error remediation (and go back to step 3)
# result storage (raw measurements together with reconciled values)
The result of an advanced DVR procedure is a coherent set of validated and reconciled process data.

==Applications==
DVR finds application mainly in industry sectors where either measurements are not accurate or even non-existing, like for example in the [[upstream (fossil-fuel industry)|upstream sector]] where [[flow measurement|flow meters]] are difficult or expensive to position (see <ref>P. Delava, E. Maréchal, B. Vrielynck, B. Kalitventzeff (1999), ''Modelling of a Crude Oil Distillation Unit in Term of Data Reconciliation with ASTM or TBP Curves as Direct Input – Application : Crude Oil Preheating Train'', Proceedings of ESCAPE-9 conference, Budapest, May 31-June 2, 1999, supplementary volume, p. 17-20.</ref>); or where accurate data is of high importance, for example for security reasons in [[nuclear power plants]] (see <ref>M. Langenstein, J. Jansky, B. Laipple (2004), ''Finding Megawatts in nuclear power plants with process data validation'', Proceedings of ICONE12, Arlington, USA, April 25–29, 2004.</ref>). Another field of application is [[Performance test (assessment)|performance and process monitoring]] (see <ref>Th. Amand, G. Heyen, B. Kalitventzeff, ''Plant Monitoring and Fault Detection: Synergy between Data Reconciliation and Principal Component Analysis'', Comp. and Chem, Eng. 25, p. 501-507, 2001.</ref>) in oil refining or in the chemical industry.

As DVR enables to calculate estimates even for unmeasured variables in a reliable way, the German Engineering Society (VDI Gesellschaft Energie und Umwelt) has accepted the technology of DVR as a means to replace expensive sensors in the nuclear power industry (see VDI norm 2048,<ref name="vdi" />).

==See also==
* [[Process simulation]]
* [[Pinch analysis]]
* [[Industrial processes]]
* [[Chemical engineering]]

==References==
<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes -->
{{Reflist}}

* Alexander, Dave, Tannar, Dave & Wasik, Larry "Mill Information System uses Dynamic Data Reconciliation for Accurate Energy Accounting" TAPPI Fall Conference 2007.[http://www.tappi.org/Downloads/Conference-Papers/2007/07EPE/07epe87.aspx]
* Rankin, J. & Wasik, L. "Dynamic Data Reconciliation of Batch Pulping Processes (for On-Line Prediction)" PAPTAC Spring Conference 2009.
* S. Narasimhan, C. Jordache, ''Data reconciliation and gross error detection: an intelligent use of process data'', Golf Publishing Company, Houston, 2000.
* V. Veverka, F. Madron, 'Material and Energy Balancing in the Process Industries'', Elsevier Science BV, Amsterdam, 1997.
* J. Romagnoli, M.C. Sanchez, ''Data processing and reconciliation for chemical process operations'', Academic Press, 2000.


{{DEFAULTSORT:Data Validation And Reconciliation}}
[[Category:Data management]]
<=====doc_Id=====>:173
<=====title=====>:
Distributed transaction
<=====text=====>:
{{POV|Commitment ordering|date=November 2011}}
A '''distributed transaction''' is a [[database transaction]] in which two or more network hosts are involved. Usually, hosts provide '''transactional resources''', while the '''transaction manager''' is responsible for creating and managing a global transaction that encompasses all operations against such resources. Distributed transactions, as any other [[Database transaction|transactions]], must have all four [[ACID|ACID (atomicity, consistency, isolation, durability)]] properties, where atomicity guarantees all-or-nothing outcomes for the unit of work (operations bundle).

Open Group, a vendor consortium, proposed the [[X/Open XA|X/Open Distributed Transaction Processing (DTP) Model]] (X/Open XA), which became a de facto standard for behavior of transaction model components.

Database are common transactional resources and, often, transactions span a couple of such databases. In this case, a distributed transaction can be seen as a [[database transaction]] that must be [[Synchronization|synchronized]] (or provide [[ACID]] properties) among multiple participating [[database]]s which are [[distributed computing|distributed]] among different physical locations. The [[isolation (computer science)|isolation]] property (the I of ACID) poses a special challenge for multi database transactions, since the (global) [[serializability]] property could be violated, even if each database provides it (see also [[global serializability]]). In practice most commercial database systems use [[Two phase locking|strong strict two phase locking (SS2PL)]] for [[concurrency control]], which ensures global serializability, if all the participating databases employ it. (see also [[commitment ordering]] for multidatabases.)

A common [[algorithm]] for ensuring [[correctness (computer science)|correct]] completion of a distributed transaction is the [[two-phase commit]] (2PC). This algorithm is usually applied for updates able to [[commit (data management)|commit]] in a short period of time, ranging from couple of milliseconds to couple of minutes.

There are also long-lived distributed transactions, for example a transaction to book a trip, which consists of booking a flight, a rental car and a hotel. Since booking the flight might take up to a day to get a confirmation, two-phase commit is not applicable here, it will lock the resources for this long. In this case more sophisticated techniques that involve multiple undo levels are used. The way you can undo the hotel booking by calling a desk and cancelling the reservation, a system can be designed to undo certain operations (unless they are irreversibly finished).

In practice, long-lived distributed transactions are implemented in systems based on [[Web Services]]. Usually these transactions utilize principles of [[Compensating transaction]]s, Optimism and Isolation Without Locking. X/Open standard does not cover long-lived DTP.

Several modern technologies, including [[Enterprise Java Beans]] (EJBs) and [[Microsoft Transaction Server]] (MTS) fully support distributed transaction standards.

==See also==
* [[Java Transaction API|Java Transaction API (JTA)]]
* [[Enduro/X|Enduro/X Open source X/Open XA and XATMI implementation]]

==References==
* {{cite web | title=Web-Services Transactions | work=Web-Services Transactions | url=http://xml.sys-con.com/read/43755.htm | accessdate=May 2, 2005 }}
* {{cite web | title=Nuts And Bolts Of Transaction Processing | work=Article about Transaction Management | url=http://www.subbu.org/articles/transactions/NutsAndBoltsOfTP.html
| accessdate=May 3, 2005 }}
* {{cite web | title=A Detailed Comparison of Enterprise JavaBeans (EJB) & The Microsoft Transaction Server (MTS) Models
 | url=http://gsraj.tripod.com/misc/ejbmtscomp.html }}

==Further reading==
* Gerhard Weikum, Gottfried Vossen, ''Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery'', Morgan Kaufmann, 2002, ISBN 1-55860-508-8

{{DEFAULTSORT:Distributed Transaction}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:176
<=====title=====>:
Association rule learning
<=====text=====>:
{{Redirect|OneR|filmmaking technique|Long take}}
{{machine learning bar}}
'''Association rule learning''' is a [[rule-based machine learning]] method for discovering interesting relations between variables in large databases.  It is intended to identify strong rules discovered in databases using some measures of interestingness.<ref name="piatetsky">Piatetsky-Shapiro, Gregory (1991), ''Discovery, analysis, and presentation of strong rules'', in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., ''Knowledge Discovery in Databases'', AAAI/MIT Press, Cambridge, MA.</ref>  Based on the concept of strong rules, [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]], [[Tomasz Imieliński]] and Arun Swami <ref name="mining">{{Cite book | last1 = Agrawal | first1 = R. | last2 = Imieliński | first2 = T. | last3 = Swami | first3 = A. | doi = 10.1145/170035.170072 | chapter = Mining association rules between sets of items in large databases | title = Proceedings of the 1993 ACM SIGMOD international conference on Management of data  - SIGMOD '93 | pages = 207 | year = 1993 | isbn = 0897915925 | pmid =  | pmc = }}</ref> introduced association rules for discovering regularities between products in large-scale transaction data recorded by [[point-of-sale]] (POS) systems in supermarkets. For example, the rule <math>\{\mathrm{onions, potatoes}\} \Rightarrow \{\mathrm{burger}\}</math> found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional [[pricing]] or [[product placement]]s. In addition to the above example from [[market basket analysis]] association rules are employed today in many application areas including [[Web usage mining]], [[intrusion detection]], [[Continuous production]], and [[bioinformatics]]. In contrast with [[sequence mining]], association rule learning typically does not consider the order of items either within a transaction or across transactions.

== Definition ==
{|class="wikitable" style="float: right; margin-left: 1em;"
|+ Example database with 5 transactions and 5 items
|-
! transaction ID !! milk !! bread !! butter !! beer !! diapers
|-
| 1 || 1 || 1 || 0 || 0 || 0
|-
| 2 || 0 || 0 || 1 || 0 || 0
|-
| 3 || 0 || 0 || 0 || 1 || 1
|-
| 4 || 1 || 1 || 1 || 0 || 0
|-
| 5 || 0 || 1 || 0 || 0 || 0
|-
|}

Following the original definition by Agrawal et al.<ref name="mining" /> the problem of association rule mining is defined as:

Let <math>I=\{i_1, i_2,\ldots,i_n\}</math> be a set of <math>n</math> binary attributes called ''items''.

Let <math>D = \{t_1, t_2, \ldots, t_m\}</math> be a set of transactions called the ''database''.

Each ''transaction'' in <math>D</math> has a unique transaction ID and contains a subset of the items in <math>I</math>.

A ''rule'' is defined as an implication of the form:

<math>X \Rightarrow Y</math>, where <math>X, Y \subseteq I</math>.

In Agrawal et al.,<ref name="mining" /> a ''rule'' is defined only between a set and a single item, <math>X \Rightarrow i_j</math> for <math>i_j \in I</math>.

Every rule is composed by two different sets of items, also known as ''itemsets'', <math>X</math> and <math>Y</math>, where <math>X</math> is called ''antecedent'' or left-hand-side (LHS) and <math>Y</math> ''consequent'' or right-hand-side (RHS).

To illustrate the concepts, we use a small example from the supermarket domain. The set of items is <math>I= \{\mathrm{milk, bread, butter, beer, diapers}\}</math> and in the table is shown a small database containing the items, where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction.

An example rule for the supermarket could be <math>\{\mathrm{butter, bread}\} \Rightarrow \{\mathrm{milk}\}</math> meaning that if butter and bread are bought, customers also buy milk.

Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant{{Citation needed|date=February 2015}}, and datasets often contain thousands or millions of transactions.

== Useful Concepts ==
In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.

Let <math>X</math> be an itemset, <math>X \Rightarrow Y</math> an association rule and <math>T</math> a set of transactions of a given database.

=== Support ===
Support is an indication of how frequently the itemset appears in the database.

The support of <math>X</math> with respect to <math>T</math> is defined as the proportion of transactions <math>t</math> in the database which contains itemset <math>X</math>.

<math>\mathrm{supp}(X) = \frac{|\{t \in T; X \subseteq t\}|}{|T|}</math>

In the example database, the itemset <math>X=\{\mathrm{beer, diapers}\}</math> has a support of <math>1/5=0.2</math> since it occurs in 20% of all transactions (1 out of 5 transactions). The argument of <math>\mathrm{supp}()</math> is a set of preconditions, and thus becomes more restrictive as it grows (instead of more inclusive).<ref name=":0">{{Cite journal|last=Hahsler|first=Michael|date=2005|title=Introduction to arules – A computational environment for mining association rules and frequent item sets|url=https://mran.revolutionanalytics.com/web/packages/arules/vignettes/arules.pdf|journal=Journal of Statistical Software|doi=|pmid=|access-date=}}</ref>

=== Confidence ===
Confidence is an indication of how often the rule has been found to be true.

The ''confidence'' value of a rule, <math>X \Rightarrow Y</math> , with respect to a set of transactions <math>T</math>, is the proportion of the transactions that contains <math>X</math> which also contains <math>Y</math>.

Confidence is defined as:

<math>\mathrm{conf}(X \Rightarrow Y) = \mathrm{supp}(X \cup Y) / \mathrm{supp}(X)</math>.

For example, the rule <math>\{\mathrm{butter,  bread}\} \Rightarrow \{\mathrm{milk}\}</math> has a confidence of <math>0.2/0.2=1.0</math> in the database, which means that for 100% of the transactions containing butter and bread the rule is correct (100% of the times a customer buys butter and bread, milk is bought as well).

Note that <math>\mathrm{supp}(X \cup Y)</math> means the support of the union of the items in X and Y. This is somewhat confusing since we normally think in terms of probabilities of [[Event (probability theory)|events]] and not sets of items. We can rewrite <math>\mathrm{supp}(X \cup Y)</math> as the joint probability <math>P(E_X \cup E_Y)</math>, where <math>E_X</math> and <math>E_Y</math> are the events that a transaction contains itemset <math>X</math> or <math>Y</math>, respectively.<ref name="michael.hahsler.net">Michael Hahsler (2015).  A Probabilistic Comparison of Commonly Used Interest Measures for Association Rules. http://michael.hahsler.net/research/association_rules/measures.html</ref>

Thus confidence can be interpreted as an estimate of the conditional probability <math>P(E_Y | E_X)</math>, the probability of finding the RHS of the rule in transactions under the condition that these transactions also contain the LHS.<ref name=":0" /><ref name="hipp">{{Cite journal | last1 = Hipp | first1 = J. | last2 = Güntzer | first2 = U. | last3 = Nakhaeizadeh | first3 = G. | title = Algorithms for association rule mining --- a general survey and comparison | doi = 10.1145/360402.360421 | journal = ACM SIGKDD Explorations Newsletter | volume = 2 | pages = 58 | year = 2000 | pmid =  | pmc = }}</ref>

=== Lift ===
The ''[[lift (data mining)|lift]]'' of a rule is defined as:

<math> \mathrm{lift}(X\Rightarrow Y) = \frac{ \mathrm{supp}(X \cup Y)}{ \mathrm{supp}(X) \times \mathrm{supp}(Y) } </math>

or the ratio of the observed support to that expected if X and Y were [[Independence (probability theory)|independent]].{{citation needed|reason=I couldn't find this in 'Witten: Data Mining - Practical Machine Learning Tools and Techniques'|date=May 2016}}

For example, the rule <math>\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}</math> has a lift of <math>\frac{0.2}{0.4 \times 0.4} = 1.25 </math>.

If the rule had a lift of 1, it would imply that the probability of occurrence of the antecedent and that of the consequent are independent of each other. When two events are independent of each other, no rule can be drawn involving those two events.

If the lift is > 1, that lets us know the degree to which those two occurrences are dependent on one another, and makes those rules potentially useful for predicting the consequent in future data sets.

The value of lift is that it considers both the confidence of the rule and the overall data set.<ref name=":0" />

=== Conviction ===
The ''conviction'' of a rule is defined as <math> \mathrm{conv}(X\Rightarrow Y) =\frac{ 1 - \mathrm{supp}(Y) }{ 1 - \mathrm{conf}(X\Rightarrow Y)}</math>.

For example, the rule <math>\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}</math> has a conviction of <math>\frac{1 - 0.4}{1 - 0.5} = 1.2 </math>, and can be interpreted as the ratio of the expected frequency that X occurs without Y (that is to say, the frequency that the rule makes an incorrect prediction) if X and Y were independent divided by the observed frequency of incorrect predictions. In this example, the conviction value of 1.2 shows that the rule <math>\{\mathrm{milk, bread}\} \Rightarrow \{\mathrm{butter}\}</math> would be incorrect 20% more often (1.2 times as often) if the association between X and Y was purely random chance.

== Process ==
[[File:FrequentItems.png|thumb|Frequent itemset lattice, where the color of the box indicates how many transactions contain the combination of items. Note that lower levels of the lattice can contain at most the minimum number of their parents' items; e.g. {ac} can have only at most <math>min(a,c)</math> items. This is called the ''downward-closure property''.<ref name="mining" />]] Association rules are usually required to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Association rule generation is usually split up into two separate steps:
# A minimum support threshold is applied to find all ''frequent itemsets'' in a database.
# A minimum confidence constraint is applied to these frequent itemsets in order to form rules.
While the second step is straightforward, the first step needs more attention.

Finding all frequent itemsets in a database is difficult since it involves searching all possible itemsets (item combinations).  The set of possible itemsets is the [[power set]] over <math>I</math> and has size <math>2^n-1</math> (excluding the empty set which is not a valid itemset). Although the size of the power-set grows exponentially in the number of items <math>n</math> in <math>I</math>, efficient search is possible using the '''''downward-closure property''''' of support<ref name="mining" /><ref>{{cite book |last1=Tan |first1=Pang-Ning |last2=Michael |first2=Steinbach |last3=Kumar |first3=Vipin |title=Introduction to Data Mining |publisher=[[Addison-Wesley]] |year=2005 |isbn=0-321-32136-7 |chapter=Chapter 6. Association Analysis: Basic Concepts and Algorithms |chapterurl=http://www-users.cs.umn.edu/~kumar/dmbook/ch6.pdf }}</ref> (also called ''anti-monotonicity''<ref name="pei">Pei, Jian; Han, Jiawei; and Lakshmanan, Laks V. S.; ''Mining frequent itemsets with convertible constraints'', in ''Proceedings of the 17th International Conference on Data Engineering, April 2–6, 2001, Heidelberg, Germany'', 2001, pages 433-442</ref>) which guarantees that for a frequent itemset, all its subsets are also frequent and thus no infrequent itemset can be a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori<ref name="apriori">Agrawal, Rakesh; and Srikant, Ramakrishnan; [http://rakesh.agrawal-family.com/papers/vldb94apriori.pdf ''Fast algorithms for mining association rules in large databases''], in Bocca, Jorge B.; Jarke, Matthias; and Zaniolo, Carlo; editors, ''Proceedings of the 20th International Conference on Very Large Data Bases (VLDB), Santiago, Chile, September 1994'', pages 487-499</ref> and Eclat<ref name="eclat">{{Cite journal | last1 = Zaki | first1 = M. J. | title = Scalable algorithms for association mining | doi = 10.1109/69.846291 | journal = IEEE Transactions on Knowledge and Data Engineering | volume = 12 | issue = 3 | pages = 372–390 | year = 2000 | pmid =  | pmc = }}</ref>) can find all frequent itemsets.

==History==
The concept of association rules was popularised particularly due to the 1993 article of Agrawal et al.,<ref name="mining" /> which has acquired more than 18,000 citations according to Google Scholar, as of August 2015, and is thus one of the most cited papers in the Data Mining field. However, it is possible that what is now called "association rules" is similar to what appears in  the 1966 paper<ref name="guha_oldest">Hájek, Petr; Havel, Ivan; Chytil, Metoděj; ''The GUHA method of automatic hypotheses determination'', Computing 1 (1966) 293-308</ref> on GUHA, a general data mining method developed by [[Petr Hájek]] et al.<ref name="pospaper">Hájek, Petr; Feglar, Tomas; Rauch, Jan; and Coufal, David; ''The GUHA method, data preprocessing and mining'', Database Support for Data Mining Applications, Springer, 2004, ISBN 978-3-540-22479-2</ref>

An early (circa 1989) use of minimum support and confidence to find all association rules is the Feature Based Modeling framework, which found all rules with <math>\mathrm{supp}(X)</math> and <math>\mathrm{conf}(X \Rightarrow Y)</math> greater than user defined constraints.<ref>{{cite journal|last1=Webb|first1=Geoffrey|title=A Machine Learning Approach to Student Modelling|journal=Proceedings of the Third Australian Joint Conference on Artificial Intelligence (AI 89)|date=1989|pages=195–205}}</ref>

== Alternative measures of interestingness ==
<!-- would be nice to explain each measure -->
In addition to confidence, other measures of ''interestingness'' for rules have been proposed. Some popular measures are:

*  All-confidence<ref name="allconfidence">Omiecinski, Edward R.; ''Alternative interest measures for mining associations in databases'', IEEE Transactions on Knowledge and Data Engineering, 15(1):57-69, Jan/Feb 2003</ref>
* Collective strength<ref name="collectivestrength">Aggarwal, Charu C.; and Yu, Philip S.; ''A new framework for itemset generation'', in ''PODS 98, Symposium on Principles of Database Systems, Seattle, WA, USA, 1998'', pages 18-24</ref>
*  Conviction<ref name="brin-dynamic-itemset1">Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; ''Dynamic itemset counting and implication rules for market basket data'', in ''SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997'', pp. 255-264</ref>
*  Leverage<ref name="leverage">Piatetsky-Shapiro, Gregory; ''Discovery, analysis, and presentation of strong rules'', Knowledge Discovery in Databases, 1991, pp. 229-248</ref>
*  Lift (originally called interest)<ref name="brin-dynamic-itemset2">Brin, Sergey; Motwani, Rajeev; Ullman, Jeffrey D.; and Tsur, Shalom; ''Dynamic itemset counting and implication rules for market basket data'', in ''SIGMOD 1997, Proceedings of the ACM SIGMOD International Conference on Management of Data (SIGMOD 1997), Tucson, Arizona, USA, May 1997'', pp. 265-276</ref>

Several more measures are presented and compared by Tan et al.<ref name="measurescomp">Tan, Pang-Ning; Kumar, Vipin; and Srivastava, Jaideep; ''Selecting the right objective measure for association analysis'', Information Systems, 29(4):293-313, 2004</ref> and by Hahsler.<ref name="michael.hahsler.net"/> Looking for techniques that can model what the user has known  (and using these models as interestingness measures) is currently an active research trend under the name of "Subjective Interestingness."

== Statistically sound associations ==

One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations.  These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance.  For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side.  There are approximately 1,000,000,000,000 such rules.  If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association.  If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules.  Statistically sound association discovery<ref>Webb, Geoffrey I. (2007); ''Discovering Significant Patterns'', Machine Learning 68(1), Netherlands: Springer, pp. 1-33 [http://link.springer.com/article/10.1007%2Fs10994-007-5006-x online access]</ref><ref>Gionis, Aristides; [[Heikki Mannila|Mannila, Heikki]]; Mielikäinen, Taneli; and Tsaparas, Panayiotis; ''Assessing Data Mining Results via Swap Randomization'', ACM Transactions on Knowledge Discovery from Data (TKDD), Volume 1, Issue 3 (December 2007), Article No. 14</ref> controls this risk, in most cases reducing the risk of finding ''any'' spurious associations to a user-specified significance levels.

== Algorithms ==

Many algorithms for generating association rules were presented over time.

Some well-known algorithms are [[Apriori algorithm|Apriori]], Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.

=== Apriori algorithm ===
{{Main article|Apriori algorithm}}

Apriori<ref name="apriori" /> uses a breadth-first search strategy to count the support of itemsets and uses a candidate generation function which exploits the downward closure property of support.

=== Eclat algorithm ===

Eclat<ref name="eclat" /> (alt. ECLAT, stands for Equivalence Class Transformation) is a depth-first search algorithm using set intersection. It is a naturally elegant algorithm suitable for both sequential as well as parallel execution with locality-enhancing properties. It was first introduced by Zaki, Parthasarathy, Li and Ogihara in a series of papers written in 1997.

Mohammed Javeed Zaki, Srinivasan Parthasarathy, M. Ogihara, Wei Li:
New Algorithms for Fast Discovery of Association Rules. KDD 1997.

Mohammed Javeed Zaki, Srinivasan Parthasarathy, Mitsunori Ogihara, Wei Li:
Parallel Algorithms for Discovery of Association Rules. Data Min. Knowl. Discov. 1(4): 343-373 (1997)

=== FP-growth algorithm ===

FP stands for frequent pattern.<ref>{{cite journal|last1=Han|title=Mining Frequent Patterns Without Candidate Generation|journal=Proceedings of the 2000 ACM SIGMOD International Conference on Management of Data|date=2000|volume=SIGMOD '00|pages=1–12|doi=10.1145/342009.335372}}</ref>

In the first pass, the algorithm counts occurrence of items (attribute-value pairs) in the dataset, and stores them to 'header table'. In the second pass, it builds the FP-tree structure by inserting instances.
Items in each instance have to be sorted by descending order of their frequency in the dataset, so that the tree can be processed quickly.
Items in each instance that do not meet minimum coverage threshold are discarded.
If many instances share most frequent items, FP-tree provides high compression close to tree root.

Recursive processing of this compressed version of main dataset grows large item sets directly, instead of generating candidate items and testing them against the entire database.
Growth starts from the bottom of the header table (having longest branches), by finding all instances matching given condition.
New tree is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the attribute, with each node getting sum of its children counts.
Recursive growth ends when no individual items conditional on the attribute meet minimum support threshold, and processing continues on the remaining header items of the original FP-tree.

Once the recursive process has completed, all large item sets with minimum coverage have been found, and association rule creation begins.<ref>Witten, Frank, Hall: Data mining practical machine learning tools and techniques, 3rd edition</ref>

=== Others ===

==== AprioriDP ====
AprioriDP<ref name="dharmesh2013" /> utilizes [[Dynamic Programming]]  in Frequent itemset mining. The working principle is to eliminate the candidate generation like FP-tree, but it stores support count in specialized data structure instead of tree.

==== Context Based Association Rule Mining Algorithm ====
{{Main article|Context Based Association Rules}}

CBPNARM is an algorithm, developed in 2013, to mine association rules on the basis of context. It uses context variable on the basis of which the support of an itemset is changed on the basis of which the rules are finally populated to the rule set.

==== Node-set-based algorithms ====
FIN,<ref name="deng2014" /> PrePost <ref name="deng2012" /> and PPV <ref name="deng2010" /> are three algorithms based on node sets. They use nodes in a coding FP-tree to represent itemsets, and employ a depth-first search strategy to discovery frequent itemsets using "intersection" of node sets.

==== GUHA procedure ASSOC ====

[[GUHA]] is a general method for exploratory data analysis that has theoretical foundations in [[observational calculi]].<ref name="ObservationalCalculi">Rauch, Jan; ''Logical calculi for knowledge discovery in databases'', in ''Proceedings of the First European Symposium on Principles of Data Mining and Knowledge Discovery'', Springer, 1997, pp. 47-57</ref>

The ASSOC procedure<ref>{{cite book |last=Hájek |first=Petr |author2=Havránek, Tomáš |title=Mechanizing Hypothesis Formation: Mathematical Foundations for a General Theory |publisher=Springer-Verlag |year=1978 |isbn=3-540-08738-9 |url=http://www.cs.cas.cz/hajek/guhabook/ }}</ref> is a GUHA method which mines for generalized association rules using fast [[bitstring]]s operations. The association rules mined by this method are more general than those output by apriori, for example "items" can be connected both with conjunction and disjunctions and the relation between antecedent and consequent of the rule is not restricted to setting minimum support and confidence as in apriori: an arbitrary combination of supported interest measures can be used.

==== OPUS search ====

OPUS is an efficient algorithm for rule discovery    that, in contrast to most alternatives, does not require either monotone or anti-monotone constraints such as minimum support.<ref name=OPUS>Webb, Geoffrey I. (1995); ''OPUS: An Efficient Admissible Algorithm for Unordered Search'', Journal of Artificial Intelligence Research 3, Menlo Park, CA: AAAI Press, pp. 431-465 [http://www.cs.washington.edu/research/jair/abstracts/webb95a.html online access]{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> Initially used to find rules for a fixed consequent<ref name="OPUS" /><ref name="Bayardo">{{Cite journal |doi=10.1023/A:1009895914772 |last1=Bayardo |first1=Roberto J., Jr. |last2=Agrawal |first2=Rakesh |last3=Gunopulos |first3=Dimitrios |year=2000 |title=Constraint-based rule mining in large, dense databases |journal=Data Mining and Knowledge Discovery |volume=4 |issue=2 |pages=217–240 }}</ref> it has subsequently been extended to find rules with any item as a consequent.<ref name="webb">Webb, Geoffrey I. (2000); ''Efficient Search for Association Rules'', in Ramakrishnan, Raghu; and Stolfo, Sal; eds.; ''Proceedings of the Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2000), Boston, MA'', New York, NY: The Association for Computing Machinery, pp. 99-107 [http://www.csse.monash.edu/~webb/Files/Webb00b.pdf online access]</ref> OPUS search is the core technology in the popular Magnum Opus association discovery system.

== Lore ==
A famous story about association rule mining is the "beer and diaper" story.  A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true.<ref name="dss">http://www.dssresources.com/newsletters/66.php</ref> Daniel Powers says:<ref name="dss" />

<blockquote>In 1992, Thomas Blischok, manager of a retail consulting group at [[Teradata]], and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis "did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.</blockquote>

== Other types of association mining ==

'''Multi-Relation Association Rules''': Multi-Relation Association Rules (MRAR) is a new class of association rules which in contrast to primitive, simple and even multi-relational association rules (that are usually extracted from multi-relational databases), each rule item consists of one entity but several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations ''live in'', ''nearby'' and ''humid'': “Those who ''live in'' a place which is ''near by'' a city with ''humid'' climate type and also are ''younger'' than 20 -> their ''health condition'' is good”. Such association rules are extractable from RDBMS data or semantic web data.<ref name="MRAR: Mining Multi-Relation Association Rules">Ramezani, Reza, Mohamad Saraee, and Mohammad Ali Nematbakhsh; ''MRAR: Mining Multi-Relation Association Rules'', Journal of Computing and Security, 1, no. 2 (2014)</ref>

'''[[Context Based Association Rules]]''' is a form of association rule. '''Context Based Association Rules''' claims more accuracy in association rule mining by considering a hidden variable named context variable which changes the final set of association rules depending upon the value of context variables. For example the baskets orientation in market basket analysis reflects an odd pattern in the early days of month.This might be because of abnormal context i.e. salary is drawn at the start of the month <ref name="Context Based Positive and Negative Spatio Temporal Association Rule Mining">Shaheen, M; Shahbaz, M; and Guergachi, A; ''Context Based Positive and Negative Spatio Temporal Association Rule Mining'', Elsevier Knowledge-Based Systems, Jan 2013, pp. 261-273</ref>

'''[[Contrast set learning]]''' is a form of associative learning. '''Contrast set learners''' use rules that differ meaningfully in their distribution across subsets.<ref name="webb03">{{cite conference
 | author = GI Webb and S. Butler and D. Newlands
 | year = 2003
 | title = On Detecting Differences Between Groups
 | conference = KDD'03 Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining
| url= http://portal.acm.org/citation.cfm?id=956781
 }}
</ref><ref name="busy">Menzies, Tim; and Hu, Ying; ''Data Mining for Very Busy People'', IEEE Computer, October 2003, pp. 18-25</ref>

'''Weighted class learning''' is another form of associative learning in which weight may be assigned to classes to give focus to a particular issue of concern for the consumer of the data mining results.

'''High-order pattern discovery''' facilitate the capture of high-order (polythetic) patterns or event associations that are intrinsic to complex real-world data.
<ref name="discovere">{{cite journal |last=Wong |first=Andrew K.C. |author2=Wang, Yang |title=High-order pattern discovery from discrete-valued data |journal=IEEE Transactions on Knowledge and Data Engineering (TKDE) |year=1997 |pages=877–893 }}</ref>

'''[[K-optimal pattern discovery]]''' provides an alternative to the standard approach to association rule learning that requires that each pattern appear frequently in the data.

'''Approximate Frequent Itemset''' mining is a relaxed version of Frequent Itemset mining that allows some of the items in some of the rows to be 0.<ref>Jinze Liu, Susan Paulsen, Xing Sun, Wei Wang, Andrew Nobel, J. P. (2006). Mining approximate frequent itemsets in the presence of noise: Algorithm and analysis. Retrieved from http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.3805{{dead link|date=October 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>

'''Generalized Association Rules''' hierarchical taxonomy (concept hierarchy)

'''Quantitative Association Rules''' categorical and quantitative data
<ref name="quantminer">{{cite journal |last=Salleb-Aouissi |first=Ansaf |author2=Vrain, Christel|author3= Nortet, Cyril |title=QuantMiner: A Genetic Algorithm for Mining Quantitative Association Rules |journal=International Joint Conference on Artificial Intelligence (IJCAI) |year=2007 |pages=1035–1040 }}</ref>

'''Interval Data Association Rules''' e.g. partition the age into 5-year-increment ranged

'''Maximal Association Rules'''

'''Sequential pattern mining ''' discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.<ref name="sequence">Zaki, Mohammed J. (2001); ''SPADE: An Efficient Algorithm for Mining Frequent Sequences'', Machine Learning Journal, 42, pp. 31–60</ref>

'''Sequential Rules''' discovering relationships between items while considering the time ordering. It is generally applied on a sequence database. For example, a sequential rule found in database of sequences of customer transactions can be that customers who bought a computer and CD-Roms, later bought a webcam, with a given confidence and support.

'''Subspace Clustering''', a specific type of [[Clustering high-dimensional data]], is in many variants also based on the downward-closure property for specific clustering models.<ref name="ZimekAssent2014">{{cite journal|last1=Zimek|first1=Arthur|last2=Assent|first2=Ira|last3=Vreeken|first3=Jilles|title=Frequent Pattern Mining Algorithms for Data Clustering|year=2014|pages=403–423|doi=10.1007/978-3-319-07821-2_16}}</ref>

'''Warmr '''is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.<ref>{{cite journal | pmid = 11272703 | volume=15 | issue=2 | title=Warmr: a data mining tool for chemical data. | date=Feb 2001 | journal=J Comput Aided Mol Des | pages=173–81}}</ref>

==See also==
* [[Sequence mining]]
* [[Production system (computer science)]]
* [[Learning classifier system]]
* [[Rule-based machine learning]]

==References==
{{reflist|3|refs=
<ref name="deng2014">Z. H. Deng and S. L. Lv. Fast mining frequent itemsets using Nodesets.[http://www.sciencedirect.com/science/article/pii/S0957417414000463]. Expert Systems with Applications, 41(10): 4505–4512, 2014.</ref>
<ref name="deng2012">Z. H. Deng, Z. Wang，and J. Jiang. A New Algorithm for Fast Mining Frequent Itemsets Using N-Lists [http://info.scichina.com:8084/sciFe/EN/abstract/abstract508369.shtml]. SCIENCE CHINA Information Sciences, 55 (9): 2008 - 2030, 2012.</ref>
<ref name="deng2010">Z. H. Deng and Z. Wang.   A New Fast Vertical Method for Mining Frequent Patterns [http://www.tandfonline.com/doi/abs/10.1080/18756891.2010.9727736]. International Journal of Computational Intelligence Systems, 3(6): 733 - 744, 2010.</ref>
<ref name="dharmesh2013">D. Bhalodiya, K. M. Patel and C. Patel. An Efficient way to Find Frequent Pattern with Dynamic Programming Approach [http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=6780102&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6780102]. NIRMA UNIVERSITY INTERNATIONAL CONFERENCE ON ENGINEERING, NUiCONE-2013, 28-30 NOVEMBER, 2013.</ref>
}}

==External links==

===Bibliographies===
* [http://www.uco.es/grupos/kdis/ARMBibliography Extensive Bibliography on Association Rules] by J.M. Luna
* [http://michael.hahsler.net/research/bib/association_rules/ Annotated Bibliography on Association Rules] by M. Hahsler
* [http://www.statsoft.com/textbook/association-rules/ Statsoft Electronic Statistics Textbook: Association Rules] by [[Dell]] Software

{{Prone to spam|date=February 2016}}
{{Z148}}<!--     {{No more links}}

       Please be cautious adding more external links.

Wikipedia is not a collection of links and should not be used for advertising.

     Excessive or inappropriate links will be removed.

 See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.

If there are already suitable links, propose additions or replacements on
the article's talk page, or submit your link to the relevant category at
DMOZ (dmoz.org) and link there using {{Dmoz}}.

-->

{{DEFAULTSORT:Association Rule Learning}}
[[Category:Data management]]
[[Category:Data mining]]
<=====doc_Id=====>:179
<=====title=====>:
Storage area network
<=====text=====>:
{{Distinguish|Network-attached storage}}
{{Use dmy dates|date=February 2013}}
{{Area networks}}

A '''storage area network''' ('''SAN''') <ref>{{cite web |url=http://cctvinstitute.co.uk/storage-area-network/|title=Storage Area Network by Noor Ul Mushtaq }}</ref> is a network which provides access to consolidated, [[Block device|block level data storage]]. SANs are primarily used to enhance storage devices, such as [[disk array]]s, [[tape library|tape libraries]], and [[optical jukebox]]es, accessible to [[Server (computing)|server]]s so that the devices appear to the [[operating system]] as [[Direct-attached storage|locally attached devices]]. A SAN typically has its own network of storage devices that are generally not accessible through the local area network (LAN) by other devices. The cost and complexity of SANs dropped in the early 2000s to levels allowing wider adoption across both enterprise and small to medium-sized business environments.

A SAN does not provide file abstraction, only block-level operations. However, [[file systems]] built on top of SANs do provide file-level access, and are known as [[shared-disk file system]]s.

== Storage ==
{{Refimprove section|date=February 2014}}
Historically, [[data centers]] first created "islands" of [[SCSI]] [[disk array]]s as [[direct-attached storage]] (DAS), each dedicated to an application, often visible as a number of "virtual hard drives" addressed as [[Logical Unit Number]]s (LUNs).<ref>{{cite web |url=http://www.novell.com/documentation/oes/stor_ovw/?page=/documentation/oes/stor_ovw/data/ami6rr0.html |title=Novel Doc: OES 1 - Direct Attached Storage Solutions}}</ref> Essentially, a SAN consolidates such storage islands together using a high-speed network.

Operating systems maintain their own [[file system]]s on their own dedicated, non-shared LUNs, as though they were local to themselves. If multiple systems were simply to attempt to share a LUN, these would interfere with each other and quickly corrupt the data. Any planned sharing of data on different computers within a LUN requires software, such as [[SAN file system]]s or [[clustered computing]].

Despite such issues, SANs help to increase storage capacity utilization, since multiple servers consolidate their private storage space onto the disk arrays.
Common uses of a SAN include provision of transactionally accessed data that require high-speed [[block device|block-level access]] to the hard drives such as email servers, databases, and high usage file servers.

===SAN compared to NAS===
[[Network-attached storage]] (NAS) was designed independently of SAN systems. In both a NAS and SAN, the various computers in a network, such as individual users' desktop computers and dedicated servers running applications ("[[application server]]s"), can share a more centralized collection of storage devices via a network connection such as a [[local area network]] (LAN).

Concentrating the storage on one or more NAS servers or in a SAN instead of placing storage devices on each application server allows application server configurations to be optimized for running their applications instead of also storing all the related data and moves the storage management task to the NAS or SAN system. Both NAS and SAN have the potential to reduce the amount of excess storage that must be purchased and provisioned as spare space. In a DAS-only architecture, each computer must be provisioned with enough excess storage to ensure that the computer does not run out of space at an untimely moment. In a DAS architecture the spare storage on one computer cannot be utilized by another. With a NAS or SAN architecture, where storage is shared across the needs of multiple computers, one normally provisions a pool of shared spare storage that will serve the peak needs of the connected computers, which typically is less than the total amount of spare storage that would be needed if individual storage devices were dedicated to each computer.

In a NAS the storage devices are directly connected to a file server that makes the storage available at a file-level to the other computers. In a SAN, the storage is made available at a lower "block-level", leaving file system concerns to the "client" side. SAN protocols include [[Fibre Channel]], [[iSCSI]], [[ATA over Ethernet]] (AoE) and [[HyperSCSI]]. One way to loosely conceptualize the difference between a NAS and a SAN is that NAS appears to the client OS (operating system) as a file server (the client can map network drives to shares on that server) whereas a disk available through a SAN still appears to the client OS as a disk, visible in disk and volume management utilities (along with client's local disks), and available to be formatted with a file system and mounted.

One drawback to both the NAS and SAN architecture is that the connection between the various CPUs and the storage units are no longer dedicated high-speed busses tailored to the needs of storage access. Instead the CPUs use the LAN to communicate, potentially creating bandwidth as well as performance bottlenecks. Additional data security considerations are also required for NAS and SAN setups, as information is being transmitted via a network that potentially includes design flaws, security exploits and other vulnerabilities that may not exist in a DAS setup.

While it is possible to use the NAS or SAN approach to eliminate all storage at user or application computers, typically those computers still have some local Direct Attached Storage for the operating system, various program files and related temporary files used for a variety of purposes, including [[cache (computing)|caching]] content locally.

To understand their differences, a comparison of SAN, DAS and NAS architectures may be helpful.<ref name="eval">{{cite web |title= Storage Architectures: DAS, SAN, NAS, iSCSI SAN |work= Marketing web site |url=  http://www.evaluatorgroup.com/document/storage-architectures/ |publisher= Evaluator Group |archivedate= September 17, 2016 |archiveurl= https://web.archive.org/web/20160917144751/http://www.evaluatorgroup.com/document/storage-architectures/ |accessdate= November 10, 2016 }}</ref>

===SAN-NAS hybrid===
[[Image:Compingles3.png|right|thumb|260px|Hybrid using SAN, [[Direct-attached storage|DAS]] and NAS technologies.]]
Despite their differences, SAN and NAS are not mutually exclusive, and may be combined as a SAN-NAS hybrid, offering both file-level protocols (NAS) and block-level protocols (SAN) from the same system. An example of this is [[Openfiler]], a free software product running on Linux-based systems. A shared disk file system can also be run on top of a SAN to provide filesystem services.

== Benefits ==
Sharing storage usually simplifies storage administration and adds flexibility since cables and storage devices do not have to be physically moved to shift storage from one server to another.

Other benefits include the ability to allow servers to boot from the SAN itself. This allows for a quick and easy replacement of faulty servers since the SAN can be reconfigured so that a replacement server can use the [[Logical Unit Number|LUN]] of the faulty server. While this area of technology is still new, many view it as being the future of the enterprise datacenter.<ref>{{cite web | title=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | url=http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx | work=SAN vs DAS: A Cost Analysis of Storage in the Enterprise | date=31 October 2008 | accessdate=2010-01-28}}</ref>

SANs also tend to enable more effective [[disaster recovery]] processes. A SAN could span a distant location containing a secondary storage array. This enables [[storage replication]] either implemented by [[disk array controller]]s, by server software, or by specialized SAN devices. Since IP [[Wide area network|WAN]]s are often the least costly method of long-distance transport, the [[Fibre Channel over IP]] (FCIP) and iSCSI protocols have been developed to allow SAN extension over IP networks. The traditional physical SCSI layer could support only a few meters of distance - not nearly enough to ensure business continuance in a disaster.

The economic consolidation of disk arrays has accelerated the advancement of several features including I/O caching, [[Snapshot (computer storage)|snapshotting]], and volume cloning ([[Business Continuance Volumes]] or BCVs).

==Network types==
Most storage networks use the [[SCSI]] protocol for communication between servers and disk drive devices. A mapping layer to other protocols is used to form a network:

* [[ATA over Ethernet|ATA over Ethernet (AoE)]], mapping of [[AT Attachment|ATA]] over [[Ethernet]]
* [[Fibre Channel Protocol]] (FCP), the most prominent one, is a mapping of SCSI over [[Fibre Channel]]
* [[Fibre Channel over Ethernet]] (FCoE)
* [[ESCON]] over Fibre Channel ([[FICON]]), used by [[mainframe computer]]s
* [[HyperSCSI]], mapping of SCSI over Ethernet
* [[iFCP]]<ref>{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=IPstorage |title=TechEncyclopedia: IP Storage |accessdate=2007-12-09}}</ref> or [[SANoIP]]<ref>{{cite web |url=http://www.techweb.com/encyclopedia/defineterm.jhtml?term=SANoIP |title=TechEncyclopedia: SANoIP |accessdate=2007-12-09}}</ref> mapping of FCP over IP
* [[iSCSI]], mapping of SCSI over [[TCP/IP]]
* [[iSCSI Extensions for RDMA]] (iSER), mapping of iSCSI over [[InfiniBand]]

Storage networks may also be built using [[Serial attached SCSI|SAS]] and [[Serial ATA|SATA]] technologies. SAS evolved from SCSI direct-attached storage. SATA evolved from [[Parallel ATA|IDE]] direct-attached storage. SAS and SATA devices can be networked using [[Serial attached SCSI#SAS expanders|SAS Expanders]].

Examples of stacked protocols using SCSI:

{| class="wikitable" style="text-align:center"
| colspan="5" | Applications
|-
| colspan="5" | [[SCSI]] Layer
|-
| rowspan="4" | [[Fibre Channel Protocol|FCP]]
| rowspan="3" | [[Fibre Channel Protocol|FCP]]
| [[Fibre Channel Protocol|FCP]]
| [[Fibre Channel Protocol|FCP]]
| rowspan="2" | [[iSCSI]]
|-
| [[Fibre Channel over IP|FCIP]]
| [[Internet Fibre Channel Protocol|iFCP]]
|-
| colspan="3" | [[Internet Protocol|TCP]]
|-
| [[Fibre Channel over Ethernet|FCoE]]
| colspan="3" | [[Internet Protocol|IP]]
|-
| [[Fibre Channel|FC]]
| colspan="4" | [[Ethernet]]
|}

== SAN infrastructure ==
[[Image:ML-QLOGICNFCCONN.JPG|thumb| [[Qlogic]] SAN-[[Fibre Channel switch|switch]] with optical [[Fibre Channel]] [[Electrical connector|connectors]] installed.]]
SANs often use a [[Fibre Channel fabric]] topology, an infrastructure specially designed to handle storage communications. It provides faster and more reliable access than higher-level protocols used in [[Network-attached storage|NAS]]. A fabric is similar in concept to a [[network segment]] in a local area network. A typical Fibre Channel SAN fabric is made up of a number of [[Fibre Channel switch]]es.

Many SAN equipment vendors also offer some form of Fibre Channel routing, and these can allow data to cross between different fabrics without merging them. These offerings use proprietary protocol elements, and the top-level architectures being promoted are radically different.
For example, they might map Fibre Channel traffic over IP or over [[Synchronous optical networking|SONET/SDH]].

== Compatibility ==
One of the early problems with Fibre Channel SANs was that the switches and other hardware from different manufacturers were not compatible. Although the basic storage protocol FCP was standard, some of the higher-level functions did not interoperate well. Similarly, many host operating systems would react badly to other operating systems sharing the same fabric.{{Citation needed|date=October 2010}}.

== In media and entertainment ==
[[Video editing]] systems require very high data transfer rates and very low latency.
SANs in media and entertainment are often referred to as serverless due to the nature of the configuration which places the video workflow (ingest, editing, playout) desktop clients directly on the SAN rather than attaching to servers. Control of data flow is managed by a distributed file system such as StorNext by Quantum.<ref>{{cite web|url=http://www.quantum.com/products/software/stornext/index.aspx |title=StorNext Storage Manager - High-speed file sharing, Data Management and Digital Archiving Software |publisher=Quantum.com |date= |accessdate=2013-07-08}}</ref>

Per-node bandwidth usage control, sometimes referred to as [[quality of service]] (QoS), is especially important in video editing as it ensures fair and prioritized bandwidth usage across the network.

==Storage virtualization==
{{main|Storage virtualization}}
[[Storage virtualization]] is the process of abstracting logical storage from physical storage. The physical storage resources are aggregated into storage pools, from which the logical storage is created. It presents to the user a logical space for data storage and transparently handles the process of mapping it to the physical location, a concept called [[location transparency]]. This is implemented in modern disk arrays, often using vendor proprietary technology. However, the goal of storage virtualization is to group multiple disk arrays from different vendors, scattered over a network, into a single storage device.  The single storage device can then be managed uniformly. {{Citation needed|date=September 2011}}

==Quality of service==
SAN Storage QoS enables the desired storage performance to be calculated and maintained for network customers accessing the device.
Some factors that affect SAN QoS are:

*[[Bandwidth (computing)|Bandwidth]] – The rate of data throughput available on the system.
*[[Latency (engineering)|Latency]] – The time delay for a read/write operation to execute.
*Queue depth – The number of outstanding operations waiting to execute to the underlying disks (traditional or [[solid-date drive]]s).

QoS can be impacted in a SAN storage system by unexpected increase in data traffic (usage spike) from one network user that can cause performance to decrease for other users on the same network. This can be known as the “noisy neighbor effect.” When QoS services are enabled in a SAN storage system, the “noisy neighbor effect” can be prevented and network storage performance can be accurately predicted.

Using SAN storage QoS is in contrast to using disk over-provisioning in a SAN environment. Over-provisioning can be used to provide additional capacity to compensate for peak network traffic loads. However, where network loads are not predictable, over-provisioning can eventually cause all bandwidth to be fully consumed and latency to increase significantly resulting in SAN performance degradation.

== See also ==
* [[ATA over Ethernet]] (AoE)
* [[Direct-attached storage]] (DAS)
* [[Disk array]]
* [[Fibre Channel]]
* [[Fibre Channel over Ethernet]]
* [[File Area Network]]
* [[Host Bus Adapter]] (HBA)
* [[iSCSI]]
* [[iSCSI Extensions for RDMA]]
* [[List of networked storage hardware platforms]]
* [[List of storage area network management systems]]
* [[Massive array of idle disks]] (MAID)
* [[Network-attached storage]] (NAS)
* [[Redundant array of independent disks]] (RAID)
* [[SCSI RDMA Protocol]] (SRP)
* [[Storage Management Initiative – Specification]] — (SMI-S)
* [[Storage hypervisor]]
* [[Storage Resource Management]] (SRM)
* [[Storage virtualization]]
* [[System area network]]

==References==
{{More footnotes|date=June 2008}}
<references/>

==External links==
<!-- ATTENTION! Please do not add links without discussion and consensus on the talk page. Undiscussed links will be removed. -->
* [https://www.redbooks.ibm.com/redbooks/pdfs/sg245470.pdf Introduction to Storage Area Networks Exhaustive Introduction into SAN, [[IBM Redbooks|IBM Redbook]]]
* [http://capitalhead.com/articles/san-vs-das-a-cost-analysis-of-storage-in-the-enterprise.aspx SAN vs. DAS: A Cost Analysis of Storage in the Enterprise]
* [http://searchstorage.techtarget.co.uk/generic/0,295582,sid181_gci1516893,00.html SAS and SATA, solid-state storage lower data center power consumption]
* [https://www.youtube.com/playlist?list=PLivYD7W2z2HMGGRIwRoRcqLL4HMpR1dIe SAN NAS Videos]
* [http://www.storageareanetworkinfo.blogspot.com.ar/ Storage Area Network Info]

<!--Interwikies-->

{{Authority control}}

{{DEFAULTSORT:Storage Area Network}}
[[Category:Data management]]
[[Category:Telecommunications engineering]]
[[Category:Storage area networks| ]]
<=====doc_Id=====>:182
<=====title=====>:
Semantic integration
<=====text=====>:
{{Unreferenced|date=October 2013}}
'''Semantic integration''' is the process of interrelating information from diverse sources, for example calendars and to do lists, email archives, presence information (physical, psychological, and social), documents of all sorts, contacts (including [[social graph]]s), search results, and advertising and marketing relevance derived from them. In this regard, [[semantics]] focuses on the organization of and action upon [[information]] by acting as an intermediary between heterogeneous data sources, which may conflict not only by structure but also context or value.

==Applications and methods==

In [[enterprise application integration]] (EAI), semantic integration can facilitate or even automate the communication between computer systems using [[metadata publishing]]. Metadata publishing potentially offers the ability to automatically link [[ontology (computer science)|ontologies]]. One approach to (semi-)automated ontology mapping requires the definition of a semantic distance or its inverse, [[semantic similarity]] and appropriate rules. Other approaches include so-called ''lexical methods'', as well as methodologies that rely on exploiting the structures of the ontologies.  For explicitly stating similarity/equality, there exist special properties or relationships in most ontology languages. [[Web Ontology Language|OWL]], for example has "owl:equivalentClass", "owl:equivalentProperty" and "owl:sameAs".

Eventually system designs may see the advent of composable architectures where published semantic-based interfaces are joined together to enable new and meaningful capabilities{{Citation needed|date=February 2014}}. These could predominately be described by means of design-time declarative specifications, that could ultimately be rendered and executed at run-time{{Citation needed|date=February 2014}}.

Semantic integration can also be used to facilitate design-time activities of interface design and mapping. In this model, semantics are only explicitly applied to design and the run-time systems work at the [[syntax]] level{{Citation needed|date=February 2014}}. This  "early semantic binding" approach can improve overall system performance while retaining the benefits of semantic driven design{{Citation needed|date=February 2014}}.

==Examples==

The [[Pacific Symposium on Biocomputing]] has been a venue for the popularization of the ontology mapping task in the biomedical domain, and a number of papers on the subject can be found in its proceedings.

==See also==
* [[Data integration]]
* [[Dataspaces]]
* [[Enterprise integration]]
* [[Ontology-based data integration]]
* [[Ontology matching]]
* [[Semantic heterogeneity]]
* [[Semantic translation]]
* [[Semantic unification]]

== References ==
{{Reflist|2}}

==External links==
*[https://web.archive.org/web/20070811204850/http://zapthink.com/report.html?id=ZapFlash-08082003 Semantic Integration: Loosely Coupling the Meaning of Data]
*[http://drops.dagstuhl.de/opus/volltexte/2005/40/ Ontology Mapping: The State of the Art] (2005 paper)
*[http://arxiv.org/ftp/arxiv/papers/0901/0901.4934.pdf 2010 paper by Carl Hewitt]
*[http://wwwhome.portavita.nl/~yeb/ooi.pdf OpenCyc to Oracle Interface]

{{Semantic Web}}

{{DEFAULTSORT:Semantic Integration}}
[[Category:Ontology (information science)]]
[[Category:Data management]]
[[Category:Semantics]]
[[Category:Bioinformatics]]
<=====doc_Id=====>:185
<=====title=====>:
Microsoft Query
<=====text=====>:
{{Refimprove|date=December 2009}}

{{wikibooks|Structured Query Language}}
{{wikibooks|SQL dialects reference}}

'''Microsoft Query''' is a visual method of creating [[database query|database queries]] using examples based on a text string, the name of a [[document]] or a list of documents. The QBE system converts the user input into a formal database query using [[SQL|Structured Query Language]] (SQL) on the backend, allowing the user to perform powerful searches without having to explicitly compose them in SQL, and without even needing to know SQL. It is derived from Moshé M. Zloof's original [[Query by Example]] (QBE) implemented in the mid-1970s at [[IBM]]'s Research Centre in [[Yorktown, New York]].<ref>Zloof, M. M., [http://dx.doi.org/10.1147/sj.164.0324 Query-by-Example: A data base language]</ref>

In the context of [[Microsoft Access]], QBE is used for introducing students to database querying, and as a user-friendly [[database management system]] for small businesses.

[[Microsoft Excel]] allows results of QBE queries to be embedded in spreadsheets.<ref>[https://support.office.com/en-us/article/Use-Microsoft-Query-to-retrieve-external-data-42a2ea18-44d9-40b3-9c38-4c62f252da2e Use Microsoft Query to retrieve external data]</ref>

==See also==
*[[Query by Example]]
*[[Microsoft Access]]
*[[Microsoft SQL Server]]

==References==
{{Reflist}}

{{DEFAULTSORT:Microsoft Query By Example}}
[[Category:Data management]]
[[Category:Microsoft database software]]


{{Microsoft-software-stub}}
{{database-software-stub}}
<=====doc_Id=====>:188
<=====title=====>:
DMAPI
<=====text=====>:
'''Data Management API''' ('''DMAPI''') is the interface defined in the [[X/Open]] document "Systems Management: Data Storage Management (XDSM) API" dated February 1997. [[XFS]], IBM [[JFS (file system)|JFS]], [[VxFS]], [[AdvFS]], [[StorNext]] and [[GPFS]] file systems support DMAPI for [[Hierarchical storage management|Hierarchical Storage Management]] (HSM).

== External links ==
* [http://pubs.opengroup.org/onlinepubs/9657099/ Systems Management: Data Storage Management (XDSM) API]
* [http://publib.boulder.ibm.com/infocenter/clresctr/vxrx/topic/com.ibm.cluster.gpfs34.dmapi.doc/bl1dmp_BookMap_xtoc.html GPFS V3.4 Data Management API Guide]
* [http://oss.sgi.com/projects/xfs/ Open Source XFS Source code with DMAPI Implementation and Test Suite ]

[[Category:Data management]]

{{compu-storage-stub}}
<=====doc_Id=====>:191
<=====title=====>:
Learning object metadata
<=====text=====>:
[[Image:LOM base schema.svg|340px|right|thumb|A schematic representation of the hierarchy of elements in the LOM data model]]

'''Learning Object Metadata''' is a data model, usually encoded in XML, used to describe a [[learning object]] and similar digital resources used to support learning. The purpose of learning object metadata is to support the reusability of learning objects, to aid discoverability, and to facilitate their interoperability, usually in the context of online [[learning management systems]] (LMS).

The IEEE 1484.12.1 – 2002 Standard for Learning Object Metadata is an internationally recognised open standard (published by the [[Institute of Electrical and Electronics Engineers]] Standards Association, New York) for the description of “[[learning object]]s”. Relevant attributes of learning objects to be described include: type of object; author; owner; terms of distribution; format; and [[pedagogy|pedagogical]] attributes, such as teaching or interaction style.

== IEEE 1484.12.1 – 2002 Standard for Learning Object Metadata ==

=== In brief ===
The IEEE working group that developed the standard defined learning objects, ''for the purposes of the standard,'' as being “any entity, digital or non-digital, that may be used for learning, education or training." This definition has struck many commentators as being rather broad in its scope, but the definition was intended to provide a broad class of objects to which LOM metadata might usefully be associated rather than to give an instructional or pedagogic definition of a learning object. ''IEEE 1484.12.1'' is the first part of a multipart standard, and describes the LOM data model. The LOM data model specifies which aspects of a learning object should be described and what vocabularies may be used for these descriptions; it also defines how this data model can be amended by additions or constraints. Other parts of the standard are being drafted to define bindings of the LOM data model, i.e. define how LOM records should be represented in [[XML]] and [[Resource Description Framework|RDF]] (''IEEE 1484.12.3'' and ''IEEE 1484.12.4'' respectively). This article focuses on the LOM data model rather than issues relating to XML or other bindings.

IMS Global Learning Consortium is an international consortium that contributed to the drafting of the IEEE Learning Object Metadata (together with the ARIADNE Foundation) and endorsed early drafts of the data model as part of the IMS Learning Resource Meta-data specification (IMS LRM, versions 1.0 – 1.2.2). Feedback and suggestions from the implementers of IMS LRM fed into the further development of the LOM, resulting in some drift between version 1.2 of the IMS LRM specification and what was finally published at the LOM standard. Version 1.3 of the IMS LRM specification realigns the IMS LRM data model with the IEEE LOM data model and specifies that the IEEE XML binding should be used. Thus, we can now use the term 'LOM' in referring to both the IEEE standard and version 1.3 of the IMS specification. The IMS LRM specification also provides an extensive ''Best Practice and Implementation Guide'', and an ''XSL transform'' that can be used to migrate metadata instances from the older versions of the IMS LRM XML binding to the IEEE LOM XML binding.

== Technical details ==

=== How the data model works ===
The LOM comprises a '''hierarchy of elements'''<!--, as shown in the diagram (top right)-->. At the first level, there are nine categories, each of which contains sub-elements; these sub-elements may be simple elements that hold data, or may themselves be aggregate elements, which contain further sub-elements. The semantics of an element are determined by its context: they are affected by the parent or container element in the hierarchy and by other elements in the same container. For example, the various ''Description'' elements (1.4, 5.10, 6.3, 7.2.2, 8.3 and 9.3) each derive their context from their parent element. In addition, description element 9.3 also takes its context from the value of element 9.1 ''Purpose'' in the same instance of ''Classification''.

The data model specifies that some elements may be repeated either individually or as a group; for example, although the elements 9.2 (''Description'') and 9.1 (''Purpose'') can only occur once within each instance of the ''Classification'' container element, the ''Classification'' element may be repeated - thus allowing many descriptions for different purposes.

The data model also specifies the '''value space''' and '''datatype''' for each of the simple data elements. The value space defines the restrictions, if any, on the data that can be entered for that element. For many elements, the value space allows any string of [[Unicode]] character to be entered, whereas other elements entries must be drawn from a declared list (i.e. a [[controlled vocabulary]]) or must be in a specified format (e.g. date and language codes). Some element datatypes simply allow a string of characters to be entered, and others comprise two parts, as described below:
* '''LangString''' items contain Language and String parts, allowing the same information to be recorded in multiple languages
* '''Vocabulary''' items are constrained in such a way that their entries have to be chosen from a controlled list of terms - composed of Source-Value pairs - with the Source containing the name of the list of terms being used and the Value containing the chosen term
* '''DateTime''' and '''Duration''' items contain one part that allows the date or duration to be given in a machine readable format, and a second that allows a description of the date or duration (for example “mid summer, 1968”).

When implementing the LOM as a data or service provider, it is not necessary to support all the elements in the data model, nor need the LOM data model limit the information which may be provided. The creation of an [[application profile]] allows a community of users to specify which elements and vocabularies they will use. Elements from the LOM may be dropped and elements from other metadata schemas may be brought in; likewise, the vocabularies in the LOM may be supplemented with values appropriate to that community.

=== Requirements ===
The key requirements for exploiting the LOM as a data or service provider are to:
* Understand user/community needs and to express these as an application profile
* Have a strategy for creating high quality metadata
* Store this metadata in a form which can be exported as LOM records
* Agree a binding for LOM instances when they are exchanged
* Be able to exchange records with other systems either as single instances or ''en masse''.

=== Related specifications ===
There are many metadata specifications; of particular interest is the [[Dublin Core]] Metadata Element Set (commonly known as Simple Dublin Core, standardised as ''ANSI/NISO Z39.85 – 2001''). Simple Dublin Core (DC) provides a non-complex, loosely defined set of elements which is useful for sharing metadata across a wide range of disparate services. Since the LOM standard used Dublin Core as a starting point, refining the Simple DC schema with qualifiers relevant to learning objects, there is some overlap between the LOM and DC standards.<ref>{{cite book|last1=Miller|first1=Steven J.|title=Metadata for Digital Collections: A How-To-Do-It Manual|date=2011|publisher=ALA Neal-Schuman|location=Chicago|isbn=978-1-55570-746-0|pages=56}}</ref> The Dublin Core Metadata Initiative is also working on a set of terms which allow the Dublin Core Element Set to be used with greater semantic precision (Qualified Dublin Core). The Dublin Education Working Group aims to provide refinements of [[Dublin Core]] for the specific needs of the education community.

Many other education-related specifications allow for LO metadata to be embedded within XML instances, such as: describing the resources in an IMS Content Package or Resource List; describing the vocabularies and terms in an [[IMS VDEX]] (Vocabulary Definition and Exchange) file; and describing the question items in an IMS QTI (Question and Test Interoperability) file.

The [[IMS VDEX|IMS Vocabulary Definition and Exchange (VDEX) specification]] has a double relation with the LOM, since not only can the LOM provide metadata on the vocabularies in a VDEX instance, but VDEX can be used to describe the controlled vocabularies which are the value space for many LOM elements.

LOM records can be transported between systems using a variety of protocols, perhaps the most widely used being [[OAI-PMH]].

=== Application profiles ===

==== UK LOM Core ====
For UK Further and Higher Education, the most relevant family of application profiles are those based around the ''UK LOM Core''.<ref>http://zope.cetis.ac.uk/profiles/uklomcore/</ref> The UK LOM Core is currently a draft schema researched by a community of practitioners to identify common UK practice in learning object content, by comparing 12 metadata schemas. UK LOM is currently legacy work, it is not in active development.

==== CanCore ====
''CanCore'' provides detailed guidance for the interpretation and implementation of each data element in the LOM standard.<ref name="CanCore">{{cite web | url = http://cancore.tru.ca/en/guidelines.html| title = CanCore Guidelines: Introduction | author = [[Norm Friesen]]| publisher = Athabasca University| date = 2003-01-20 | accessdate = 2009-02-23 |display-authors=etal}}</ref> These guidelines (2004) constitute a 250-page document, and have been developed over three years under the leadership of [[Norm Friesen]], and through consultation with experts across Canada and throughout the world. These guidelines are also available at no charge from the CanCore Website.

==== ANZ-LOM ====
ANZ-LOM is a metadata profile developed for the education sector in Australia and New Zealand. The profile sets obligations for elements and illustrates how to apply controlled vocabularies, including example regional vocabularies used in the "classification" element. The ANZ-LOM profile was first published by The Le@rning Federation (TLF) in January, 2008.

==== Vetadata ====
The Australian Vocational Training and Education (VET) sector uses an application profile of the IEEE LOM called Vetadata. The profile contains five mandatory elements, and makes use of a number of vocabularies specific to the Australian VET sector. This application profile was first published in 2005. The Vetadata and ANZ-LOM profiles are closely aligned.

==== NORLOM ====
NORLOM is the Norwegian LOM profile.
The profile is managed by NSSL (The Norwegian Secretariat for Standardization of Learning Technologies)

==== ISRACore ====
ISRACORE is the Israeli LOM profile.
The Israel Internet Association (ISOC-IL) and Inter University Computational Center (IUCC) have teamed up to manage and establish an e-learning objects database.

====SWE-LOM====
SWE-LOM is the Swedish LOM profile that is managed by IML at [[Umeå University]] as a part of the work with the national standardization group TK450 at [[Swedish Standards Institute]].

====TWLOM====
TWLOM is the Taiwanese LOM profile that is managed by Industrial Development and Promotion of Archives and e-Learning Project

====LOM-FR====
LOM-FR is a metadata profile developed for the education sector in France. This application profile was first published in 2006.

====NL LOM====
NL LOM is the Dutch metadata profile for educational resources in the Netherlands. This application profile was the result of merging the Dutch higher education LOM profile with the one used in primary and secondary Dutch education. The final version was released in 2011.

====LOM-CH====
LOM-CH is a metadata profile developed for the education sector in Switzerland. It is currently available in French and German. This application profile was published in July 2014.

====LOM-ES====
LOM-ES is a metadata profile developed for the education sector in Spain. It is available in Spanish.

====LOM-GR====
LOM-GR, also known as "LOM-GR ''Photodentro''" is the Greek LOM application profile for educational resources, currently being used for resources related to school education. It was published in 2012 and is currently available in Greek and English.<ref>https://git.dschool.edu.gr/photodentro/LOM-GR</ref> It is maintained by [[CTI DIOPHANTUS]] as part of the "[[Photodentro]] Federated Architecture for Educational Content for Schools" that includes a number of educational content repositories (for Learning Objects, Educational Video, and User Generated Content) and the Greek National Aggregator of Educational Content accumulating metadata from collections stored in repositories of other organizations.<ref name="Photodentro LOR">{{cite journal|last1=Megalou|first1=Elina|last2=Kaklamanis|first2=Christos|title=PHOTODENTRO LOR, THE GREEK NATIONAL LEARNING OBJECT REPOSITORY|journal=INTED2014 Proceedings|date=10–12 March 2014|pages=309–319|url=https://library.iated.org/view/MEGALOU2014PHO|accessdate=7 April 2016|series=8th International Technology, Education and Development Conference|publisher=IATED|location=Valencia, Spain|issn=2340-1079}}</ref> LOM-GR is a working specification of the TC48/WG3 working group of the [[Hellenic Organization for Standardization]].

==== Others ====
Other application profiles are those developed by the Celebrate project<ref>European Schoolnet, [http://web.archive.org/web/20071225053548/http://www.eun.org/ww/en/pub/celebrate_help/application_profile.htm CELEBRATE Application Profile] (2003).</ref> and the metadata profile that is part of the SCORM reference model.<ref>ADL, [http://www.adlnet.gov/capabilities/scorm#tab-learn SCORM].</ref>

== See also ==
* [[Application profile]]
* [[Content package]]
* [[Dublin Core]]
* IMS Global
* [[Learning object]]
* [http://dublincore.org/dcx/lrmi-terms/1.1/ LRMI (Learning Resource Metadata Initiative)]
* [[Metadata]]
* [[Metadata standards|Metadata Standards]]
* [[OAI-PMH]]
* [[SCORM]]
* [[XML]]
* [[:m:Learning Object Metadata]]

==References==
{{Reflist}}

== External links ==
{{wikiversity|Introduction to Learning Objects}}
* [http://cancore.athabascau.ca/en/ cancore.athabascau.ca] is a thorough element-by-element guide to implementing the IEEE LOM.
* [http://www.imsglobal.org/metadata/ www.imsglobal.org: IMS Global Learning Consortium Learning resource meta-data specification].
* [http://ltsc.ieee.org/wg12/files/IEEE_1484_12_03_d8_submitted.pdf ltsc.ieee.org: XML Binding Specification].
* [http://www.intrallect.com/support/metadata/ims2lom_metadata_mapping.htm www.intrallect.com: A mapping between the IEEE LOM and IMS Learning Resource Metadata]
* [http://www.ontopia.net/topicmaps/materials/tm-vs-thesauri.html www.ontopia.net: Metadata? Thesauri? Taxonomies? Topic Maps! Making sense of it all], 2004.
{{Prone to spam|date=October 2014}}
{{Z148}}<!--     {{No more links}}

       Please be cautious adding more external links.

Wikipedia is not a collection of links and should not be used for advertising.

     Excessive or inappropriate links will be removed.

 See [[Wikipedia:External links]] and [[Wikipedia:Spam]] for details.

If there are already suitable links, propose additions or replacements on
the article's talk page, or submit your link to the relevant category at
DMOZ (dmoz.org) and link there using {{Dmoz}}.

-->

{{Use dmy dates|date=October 2010}}

{{DEFAULTSORT:Learning Object Metadata}}
[[Category:Data management]]
[[Category:Educational technology]]
[[Category:Knowledge representation]]
[[Category:Library science]]
[[Category:Metadata]]
[[Category:Standards]]
[[Category:Technical communication]]
<=====doc_Id=====>:194
<=====title=====>:
Database schema
<=====text=====>:
A '''database schema''' ({{IPAc-en|ˈ|s|k|i|.|m|ə}} {{respell|SKEE|mə}}) of a [[database system]] is its structure described in a [[formal language]] supported by the [[database management system]] (DBMS). The term "schema" refers to the organization of data as a blueprint of how the database is constructed (divided into database tables in the case of [[relational databases]]). The formal definition of a [[database]] schema is a set of formulas (sentences) called [[integrity constraints]] imposed on a database.{{citation needed|date=January 2016}} These integrity constraints ensure compatibility between parts of the schema. All constraints are expressible in the same language. A database can be considered a structure in realization of the [[database language]].<ref name="source1" /> The states of a created [[conceptual schema]] are transformed into an [[Explicit and implicit methods|explicit mapping]], the database schema. This describes how real-world entities are modeled in the database.

"A database schema specifies, based on the [[database administrator]]'s knowledge of possible applications, the facts that can enter the database, or those of interest to the possible [[end-user]]s."<ref name="source3"/> The notion of a database schema plays the same role as the notion of theory in [[predicate calculus]]. A model of this "theory" closely corresponds to a database, which can be seen at any instant of time as a [[mathematical object]]. Thus a schema can contain formulas representing [[Data integrity#Types of integrity constraints|integrity constraints]] specifically for an application and the constraints specifically for a type of database, all expressed in the same database language.<ref name="source1" /> In a [[relational database]], the schema defines the [[Table (database)|tables]], [[Field (computer science)|fields]], [[Relational model|relationship]]s, [[View (database)|view]]s, [[Index (database)|index]]es, [[Software package (installation)|package]]s, [[stored procedure|procedure]]s, [[subroutine|function]]s, [[Queue (data structure)|queue]]s, [[Database trigger|trigger]]s, [[Data type|type]]s, [[sequence]]s, [[materialized view]]s, [[Synonym (database)|synonym]]s, [[database link]]s, [[Directory (file systems)|directories]], [[XML schema]]s, and other elements.

A database generally stores its schema in a [[data dictionary]]. Although a schema is defined in text database language, the term is often used to refer to a graphical depiction of the database structure. In other words, schema is the structure of the database that defines the objects in the database.

In an [[Oracle Database]] system, the term "schema" has a slightly different [[connotation]].

==Ideal requirements for schema integration==
{{See also|Database normalization}}

The requirements listed below influence the detailed structure of schemas that are produced. Certain applications will not require that all of these conditions are met, but these four requirements are the most ideal.

; Overlap preservation
: Each of the overlapping elements specified in the input mapping is also in a database schema relation.<ref name="source2" />

; Extended overlap preservation
: Source-specific elements that are associated with a source’s overlapping elements are passed through to the database schema.<ref name="source2" />

; Normalization
: Independent entities and relationships in the source data should not be grouped together in the same relation in the database schema. In particular, source specific schema elements should not be grouped with overlapping schema elements, if the grouping co-locates independent entities or relationships.<ref name="source2" />

; Minimality
: If any elements of the database schema are dropped then the database schema is not ideal.<ref name="source2" />

==Example of two schema integrations==
Suppose we want a mediated (database) schema to integrate two travel databases, Go-travel and Ok-travel.

'''<code>Go-travel</code>''' has two relations:
<syntaxhighlight lang="text">
Go-flight(f-num, time, meal(yes/no))
Go-price(f-num, date, price)
</syntaxhighlight>
(<code>f-num</code> being the flight number)

'''<code>Ok-travel</code>''' has just one relation:
<syntaxhighlight lang="text">
Ok-flight(f-num, date, time, price, nonstop(yes/no))
</syntaxhighlight>

The overlapping information in Ok-travel’s and Go-travel’s schemas could be represented in a mediated schema:<ref name="source2" />
<syntaxhighlight lang="text">
Flight(f-num, date, time, price)
</syntaxhighlight>

== Oracle database specificity ==
In the context of [[Oracle database]]s, a '''schema object''' is a logical [[Database storage structures|data storage structure]].<ref>
{{cite book
|first1= Lance |last1= Ashdown
|first2= Tom  |last2= Kyte
|others= ''et al''.
|title= Oracle Database Concepts 11g Release 2 (11.2)
|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111
|accessdate= 2010-04-14 |date=February 2010
|publisher= Oracle Corporation
|quote= A database schema is a logical container for data structures, called schema objects. Examples of schema objects are tables and indexes. 
}}
</ref>

An Oracle database associates a separate schema with each database '''user'''.<ref>
{{cite book
|title= Oracle Database Concepts 10g Release 2 (10.2)Part Number B14220-02 
|url= http://docs.oracle.com/cd/B19306_01/server.102/b14220/schema.htm
|accessdate= 2012-11-26
|quote= A schema is a collection of logical structures of data, or schema objects. A schema is owned by a database user and has the same name as that user. Each user owns a single schema. Schema objects can be created and manipulated with SQL. 
}}</ref>
A schema comprises a collection of schema objects. Examples of schema objects include:

* [[Table (database)|tables]]
* [[View (database)|views]]
* [[sequence]]s
* [[Synonym (database)|synonyms]]
* [[Index (database)|indexes]]
* clusters
* database links
* [[Snapshot (computer storage)|snapshot]]s
* [[stored procedure|procedure]]s
* functions
* packages

On the other hand, non-schema objects may include:<ref>{{cite book
|first1= Lance
|last1= Ashdown
|author1-link=
|first2= Tom
|last2= Kyte
|author2-link=
|others= et al.
|title= Oracle Database Concepts 11g Release 2 (11.2)
|url= http://download.oracle.com/docs/cd/E11882_01/server.112/e10713/tablecls.htm#CNCPT111
|accessdate= 2010-04-14
|date=February 2010
|publisher= Oracle Corporation
|location=
|isbn=
|quote= Other types of objects are also stored in the database and can be created and manipulated with SQL statements but are not contained in a schema. These objects include database users, roles, contexts, and directory objects.
}}</ref>

* users
* roles
* contexts
* directory objects

Schema objects do not have a one-to-one correspondence to physical files on disk that store their information. However, [[Oracle database]]s store schema objects logically within a [[tablespace]] of the database. The data of each object is physically contained in one or more of the tablespace's [[datafile]]s. For some objects (such as tables, indexes, and clusters) a [[database administrator]] can specify how much disk space the Oracle [[RDBMS]] allocates for the object within the tablespace's datafiles.

There is no necessary relationship between schemas and tablespaces: a tablespace can contain objects from different schemas, and the objects for a single schema can reside in different tablespaces.

== See also ==
{{too many see alsos|date=July 2013}}
* [[Core architecture data model]] (CADM)
* [[Data definition language]] (DDL)
* [[Database design]]
* [[Data dictionary]]
* [[Data element]]
* [[Data modeling]]
* [[Data mapping]]
* [[Database integrity]]
* [[Entity–relationship model]]
* [[Knowledge representation and reasoning]]
* [[Object-role modeling]]
* [[Relational algebra]]
* [[Schema matching]]
* [[Three schema approach]]

==References==
{{reflist|refs=
<ref name="source1">{{cite journal |last=Rybinski |first=H. |year=1987|title=On First-Order-Logic Databases |journal=ACM Transactions on Database Systems |volume=12 |issue=3 |pages=325–349 |doi= 10.1145/27629.27630}}</ref>
<ref name="source2">{{cite journal |last= Pottinger |first=P. |last2=Berstein |first2=P. |year=2008 |title= Schema merging and mapping creation for relational sources |journal= Proceedings of the 11th international conference on Extending database technology: Advances in database technology (EDBT '08) |publisher=ACM |location= New York, NY |pages=73–84 |doi= 10.1145/1353343.1353357}}</ref>
<ref name="source3">{{cite journal |last= Imielinski |first=T. |last2=Lipski |first2=W. |year=1982 |title=A systematic approach to relational database theory |journal= Proceedings of the 1982 ACM SIGMOD international conference on Management of data (SIGMOD '82) |publisher=ACM |location= New York, NY |pages=8–14 |DOI= 10.1145/582353.582356}}</ref>
}}

== External links ==
* [https://weblogs.asp.net/scottgu/Tip_2F00_Trick_3A00_-Online-Database-Schema-Samples-Library Tip/Trick: Online Database Schema Samples Library]
* [http://web.archive.org/web/20081217074637/http://msdn.microsoft.com/en-us/library/bb187299%28SQL.80%29.aspx Database Schema Samples]
* [http://web.archive.org/web/20080828210315/http://ciobriefings.com/Publications/WhitePapers/DesigningtheStarSchemaDatabase/tabid/101/Default.aspx Designing the Star Schema Database]

{{DEFAULTSORT:Database Schema}}
[[Category:Data management]]
[[Category:Data modeling]]
<=====doc_Id=====>:197
<=====title=====>:
Change data capture
<=====text=====>:
{{No footnotes|date=March 2016}}
In [[database]]s, '''change data capture''' (CDC) is a set of software [[Design pattern (computer science)|design patterns]] used to determine (and track) the data that has changed so that action can be taken using the changed data. Also, Change data capture (CDC) is an approach to data integration that is based on the identification, capture and delivery of the changes made to enterprise data sources.

CDC solutions occur most often in [[data warehouse|data-warehouse]] environments since capturing and preserving the state of data across time is one of the core functions of a data warehouse, but CDC can be utilized in any database or data repository system.

== Methodology ==

System developers can set up CDC mechanisms in a number of ways and in any one or a combination of system layers from application logic down to physical storage.

In a simplified CDC context, one computer system has data believed to have changed from a previous point in time, and a second computer system needs to take action based on that changed data.  The former is the source, the latter is the target.  It is possible that the source and target are the same system physically, but that would not change the design pattern logically.

Not uncommonly, multiple CDC solutions can exist in a single system.

=== Timestamps on rows ===
Tables whose changes must be captured may have a column that represents the time of '''last''' change.  Names such as LAST_UPDATE, etc. are common.  Any row in any table that has a timestamp in that column that is more recent than the last time data was captured is considered to have changed.

=== Version Numbers on rows ===
Database designers give tables whose changes must be captured a column that contains a version number.  Names such as VERSION_NUMBER, etc. are common.  When data in a row changes, its version number is updated to the current version.  A supporting construct such as a reference table with the current version in it is needed.  When a change capture occurs, all data with the latest version number is considered to have changed.  When the change capture is complete, the reference table is updated with a new version number.

Three or four major techniques exist for doing CDC with version numbers, the above paragraph is just one.

==== Use in Optimistic Locking ====
Version numbers can be useful with [[optimistic locking]] in ACID transactional or [[Relational database management system|relational database management systems (RDMBS)]]. For an example in read-then-update scenarios for [[Create, read, update and delete|CRUD]] applications in [[relational database management system]]s, a row is first read along with the state of its version number; in a separate transaction, a [[Update (SQL)|SQL UPDATE]] statement is executed along with an additional [[Where (SQL)|WHERE clause]] that includes the version number found from the initial read. If no record was updated, it usually means that the version numbers didn't match because some other action/transaction had already updated the row and consequently its version number. Several [[Object relational mapper|object relational mapping]] tools use this method to detect for optimistic locking scenarios (including [[Hibernate (Java)|Hibernate]]).

=== Status indicators on rows ===
This technique can either supplement or complement timestamps and versioning. It can configure an alternative if, for example, a status column is set up on a table row indicating that the row has changed (e.g. a boolean column that, when set to true, indicates that the row has changed). Otherwise, it can act as a complement to the previous methods, indicating that a row, despite having a new version number or an earlier date, still shouldn't be updated on the target (for example, the data may require human validation).

=== Time/Version/Status on rows ===
This approach combines the three previously discussed methods.  As noted, it is not uncommon to see multiple CDC solutions at work in a single system, however, the combination of time, version, and status provides a particularly powerful mechanism and programmers should utilize them as a trio where possible.  The three elements are not redundant or superfluous.  Using them together allows for such logic as, "Capture all data for version 2.1 that changed between 6/1/2005 12:00 a.m. and 7/1/2005 12:00 a.m. where the status code indicates it is ready for production."

=== Triggers on tables ===
May include a [[observer pattern|publish/subscribe]] pattern to communicate the changed data to multiple targets.  In this approach, [[Database trigger|triggers]] log events that happen to the transactional table into another queue table that can later be "played back".  For example, imagine an Accounts table, when transactions are taken against this table, triggers would fire that would then store a history of the event or even the deltas into a separate queue table.  The queue table might have schema with the following fields: Id, TableName, RowId, TimeStamp, Operation.  The data inserted for our Account sample might be:  1, Accounts, 76, 11/02/2008 12:15am, Update.
More complicated designs might log the actual data that changed.  This queue table could then be "played back" to replicate the data from the source system to a target.

[More discussion needed]

An example of this technique is the pattern known as the [[log trigger]].

=== Event Programming ===
Coding a change into an application at appropriate points is another method that can give  intelligent discernment that data changed.  Although this method involves programming vs. more easily implemented "dumb" triggers, it may provide more accurate and desirable CDC, such as only after a COMMIT, or only after certain columns changed to certain values - just what the target system is looking for.

=== Log scanners on databases ===
Most database management systems manage a [[transaction log]] that records changes made to the database contents and to [[metadata]]. By scanning and interpreting the contents of the database transaction log one can capture the changes made to the database in a non-intrusive manner.

Using transaction logs for change data capture offers a challenge in that the structure, contents and use of a transaction log is specific to a database management system. Unlike data access, no standard exists for transaction logs. Most database management systems do not document the internal format of their transaction logs, although some provide programmatic interfaces to their transaction logs (for example: Oracle, DB2, SQL/MP, SQL/MX and SQL Server 2008).

Other challenges in using transaction logs for change data capture include:

* Coordinating the reading of the transaction logs and the archiving of log files (database management software typically archives log files off-line on a regular basis).
* Translation between physical storage formats that are recorded in the transaction logs and the logical formats typically expected by database users (e.g., some transaction logs save only minimal buffer differences that are not directly useful for change consumers).
* Dealing with changes to the format of the transaction logs between versions of the database management system.
* Eliminating uncommitted changes that the database wrote to the transaction log and later [[Rollback (data management)|rolled back]].
* Dealing with changes to the metadata of tables in the database.

CDC solutions based on transaction log files have distinct advantages that include:

* minimal impact on the database (even more so if one uses [[log shipping]] to process the logs on a dedicated host).
* no need for programmatic changes to the applications that use the database.
* low [[Latency (engineering)|latency]] in acquiring changes.
* [[data integrity|transactional integrity]]: log scanning can produce a change stream that replays the original transactions in the order they were committed. Such a change stream include changes made to all tables participating in the captured transaction.
* no need to change the database schema

== Confounding factors ==
As often occurs in complex domains, the final solution to a CDC problem  may have to balance many competing concerns.

=== Unsuitable source systems ===

Change data capture both increases in complexity and reduces in value if the source system saves [[metadata]] changes when the data itself is not modified.  For example, some [[Data model]]s track the user who last looked at but did not change the data in the same structure as the data.  This results in [[Noise (signal processing)|noise]] in the Change Data Capture.

=== Tracking the capture ===

Actually tracking the changes depends on the data source.  If the data is being persisted in a modern [[database]] then Change Data Capture is a simple matter of permissions.  Two techniques are in common use:
# Tracking changes using [[Database Trigger]]s
# Reading the [[transaction log]] as, or shortly after, it is written.

If the data is not in a modern database, Change Data Capture becomes a programming challenge.

=== Push versus pull ===
* '''Push''': the source process creates a snapshot of changes within its own process and delivers rows downstream. The downstream process uses the snapshot, creates its own subset and delivers them to the next process.
* '''Pull''': the target that is immediately downstream from the source, prepares a request for data from the source. The downstream target delivers the snapshot to the next target, as in the push model.

=== Alternatives ===

Sometimes the [[Slowly changing dimension]] is used as a method, this is an example:
[[File:Scd model.png|frame|center|Scd model]]

==See also==

* [[Slowly Changing Dimension]]
* [[Referential integrity]]

== References ==
{{reflist}}

==External links==
* [https://github.com/linkedin/databus/wiki LinkedIn Databus]
* [http://www.informaticacloud.com/images/whitepapers/data%20replication%20best%20practices.pdf Data Replication as a Service Best Practices]
* [https://web.archive.org/web/20110902084451/http://www.attunity.com:80/attunity_stream Attunity Change Data Capture (CDC)]
* [http://www-01.ibm.com/software/data/infosphere/change-data-capture/  IBM Infosphere CDC]
* [https://web.archive.org/web/20060523023144/http://www.oracle.com:80/technology/oramag/oracle/03-nov/o63tech_bi.html Tutorial on setting up CDC in Oracle 9i]
* [http://social.technet.microsoft.com/wiki/contents/articles/how-to-enable-sql-azure-change-data-capture.aspx Tutorial on setting up SQL Azure Change Data Capture]
* [http://msdn2.microsoft.com/en-us/library/bb522489(SQL.100).aspx Details of the CDC facility included in Microsoft Sql Server 2008 Feb '08 CTP]
* [http://www.jumpmind.com/products/symmetricds/features SymmetricDS - Heterogeneous, Cross Platform CDC]
* [http://www.gamma-soft.com/wp/index.php?page_id=30 Gamma-Soft CDC Technology]
* [http://www.talend.com/download/talend-open-studio?qt-product_tos_download=3#qt-product_tos_download Talend]

{{DEFAULTSORT:Change Data Capture}}
[[Category:Computer data]]
[[Category:Data management]]
<=====doc_Id=====>:200
<=====title=====>:
Rainbow storage
<=====text=====>:
{{notability|date=November 2014}}
'''Rainbow storage''' is a developing [[paper]]-based [[data storage device|data storage]] technique first demonstrated by Indian student Sainul Abideen in November 2006.<ref name="ArabNews">[http://www.arabnews.com/?page=4&section=0&article=88962&d=18&m=11&y=2006 "Data Can Now Be Stored on Paper"] by M. A. Siraj, ''[[ArabNews]]'' (published November 18, 2006; accessed November 29, 2006)</ref> Abideen received his [[Master of Computer Applications|MCA]] from [[MES College of Engineering|MES Engineering College]] in [[Kuttipuram]] in [[Kerala]]'s [[Malappuram]] district.

Initial newspaper reports of the technology were disputed by multiple technical sources, although Abideen says those reports were based on a misunderstanding of the technology. The paper meant to demonstrate the capability of storing relatively large amounts of data (and not necessarily in the gigabyte range) using textures and diagrams.<ref name=theinq>[http://www.theinquirer.net/default.aspx?article=36294 Paper storage man misunderstood] &mdash; ''[[The Inquirer]]'' article, 12 December 2006 (retrieved 15 December 2006.</ref>

The Rainbow data storage technology claims to use [[Geometry|geometric]] shapes such as triangles, circles and squares of various colors to store a large amount of data on ordinary paper or plastic surfaces. This would provide several advantages over current forms of [[Optical disc|optical-]] or [[Magnetic storage|magnetic]] [[data storage device|data storage]] like less environmental pollution due to the biodegradability of paper, low cost and high capacity. Data could be stored on "Rainbow Versatile Disk" (RVD) or plastic/paper cards of any form factor (like SIM cards).<ref name="Techworld.com">[http://www.techworld.com/storage/news/index.cfm?newsID=7424 "Store 256GB on an A4 sheet"] by Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)</ref>

==Criticism==
Following the wide media attention this news received, some of the claims have been disputed by various experts.<ref name="ITSoup"> [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper] By ITSoup (published November 25, 2006; accessed November 25, 2006)</ref>	 <ref name="ArsTechnica"> [http://arstechnica.com/news.ars/post/20061126-8288.html "Can you get 256GB on an A4 sheet? No way!"]  By Chris Mellor, Techworld (published November 24, 2006; accessed November 29, 2006)</ref>	

Printing at 1,200 dots per inch (DPI) leads to a theoretical maximum of 1,440,000 colored dots per square inch. If a scanner can reliably distinguish between 256 unique colors (thus encoding one byte per dot), the maximum possible storage is approximately 140 megabytes for a sheet of A4 paper&ndash;much lower when the necessary error correction is employed. If the scanner were able to accurately distinguish between 16,777,216 colors (24 bits, or 3 bytes per dot), the capacity would triple, but it still falls well below the media stories' claims of several hundred gigabytes.

Printing this quantity of unique colors would require specialized equipment to generate many [[spot color]]s.  The [[process color]] model used by most printers provides only four colors, with additional colors simulated by a [[halftone|halftone pattern]].

At least one of three things must be true for the claim to be valid:
* The paper must be printed and scanned at a much higher resolution than 1,200 DPI, 	 
* the printer and scanner must be able to accurately produce and distinguish between an extraordinary number of distinct color values, or 	 
* the compression scheme must be a revolutionary [[lossless compression]] algorithm. 	 

The theory is: If Rainbow's "geometric" algorithm is to be encoded and decoded by a computer, it would equally viable to store the compressed data on a conventional disk rather than printing it to paper or other non-digital medium. Printing something as dots on a page rather than bits on a disk will not change the underlying compression ratio, so a lossless compression algorithm that could store 250 gigabytes within a few hundred megabytes of data would be revolutionary indeed. Likewise, data can be compressed with ''any'' algorithm and subsequently printed to paper as colored dots. The amount of data that can be reliably stored in this way is limited by the printer and scanner, as described above.

However Sainul Abideen says that the articles are based on misunderstandings. He claims, it as a method to store data in the form of colour, in any medium where colour can be represented, not only paper. Density of storage in paper will be very small and the density will be depends on the storage medium, capacity of colour representation and retrieval methods etc.

==Demonstrations==
Sainul Abdeen demonstrated his technology to the college and members of the Indian press in the MES College of Engineering computer lab, Kerala, and was able to compress 450 sheets plain text from [[Foolscap folio|foolscap paper]] into a 1 inch square. He also demonstrated a 45-second audio clip compressed using this technology on to an [[ISO 216|A4 sheet]]. Depending on the sampling frequency, bit depth, and audio compression (if any), a 45-second audio clip can consist of anywhere from a few kilobytes to a few megabytes of data.  Abideen claimed that the technology could be extended to 250 gigabytes by using specific materials and devices. {{Fact|date=June 2009}}

This technology is based on two principles:

;Principle I
:“Every color or color combinations can be converted into some values and from the values the colors or color combinations can be regenerated”.
;Principle II
:“Every different color or color combinations will produce different values”.

==References==
{{reflist}}


==Absolute Rainbow Dots==
Absolute rainbow dots are used to detect errors caused by scratches, and whether any fading has occurred. Absolute rainbow dots are predefined dots carrying a unique value. These dots can be inserted in the rainbow picture in pre-specified areas. If fading occurs these dot values will change accordingly, and at the reproduction stage this can be checked and corrected.
Absolute rainbow dots will be microscopically small so that they occupy very little space in the rainbow picture. These will be colored differently so that each dot will have its own fixed unique value.

==External links==
* [http://www.kerlontech.com/RandD.html Sainul Abideen's home page] (dead)
* [http://www.deccanherald.com/deccanherald/Sep62006/cyberspace163748200695.asp Deccan Herald's article on Rainbow Storage] (dead)
* [http://www.dailytech.com/article.aspx?newsid=5052 Article in DailyTech,]
* [http://itsoup.blogspot.com/2006/11/scam-of-indian-student-developing.html IT Soup: Scam of Indian student developing technology to store 450 GB of data on a sheet of paper]
* [http://www.theregister.co.uk/2006/11/23/rvd_system/ Article in The Register]
*[http://www.idm.net.au/storypages/storydata.asp?id=7749 IDM: Paper Storage Claims A Hoax?]  (dead)

[[Category:Data management]]
[[Category:Vaporware]]
<=====doc_Id=====>:203
<=====title=====>:
Project workforce management
<=====text=====>:
'''Project workforce management''' is the practice of combining the coordination of all logistic elements of a project through a single [[software application]] (or [[workflow engine]]). This includes planning and tracking of schedules and mileposts, cost and revenue, resource allocation, as well as overall management of these project elements.  Efficiency is improved by eliminating manual processes, like [[spreadsheet]] tracking<ref>
{{Cite web
| author      = Seema Haji
| title       = Business Intelligence Cures the Spreadsheet Problem
| url         = http://www.refresher.com/asmhbi.html
| publisher   = Refresher Publications Inc.
| year        = 2009
| accessdate  = October 30, 2009
}}</ref>  to monitor project progress. It also allows for at-a-glance status updates and ideally integrates with existing legacy applications in order to unify ongoing projects, [[enterprise resource planning]] (ERP) and broader organizational goals.<ref>
{{Cite web
| author      = Rudolf Melik
| title       = The Rise of the Project Workforce
| url         = http://www.projectworkforcebook.com/
| publisher   = Wiley: New York, NY
| year        = 2007
| accessdate  = October 30, 2009
}}</ref> There are a lot of logistic elements in a project. Different team members are responsible for managing each element and often, the organisation may have a mechanism to manage some logistic areas as well.

By coordinating these various components of [[project management]], [[workforce management]] and financials through a single solution, the process of configuring and changing project and workforce details is simplified.

== Introduction ==
<ref>{{Citation|title = Project workforce management|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref> A project workforce management system defines project tasks, project positions, and assigns personnel to the project positions. The project tasks and positions are correlated to assign a responsible project position or even multiple positions to complete each project task. Because each project position may be assigned to a specific person, the qualifications and availabilities of that person can be taken into account when determining the assignment. By associating project tasks and project positions, a manager can better control the assignment of the workforce and complete the project more efficiently.

When it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined. Therefore, all the logistic processes take place in the workflow engine.

== About ==

=== Technical Field ===
This invention relates to project management systems and methods, more particularly to a software-based system and method for project and workforce management.<ref>{{Citation|title = Project workforce management Technical Field|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>

=== Software Usage ===
Due to the software usage, all the project workflow management tasks can be fully automated without leaving many tasks for the project managers. This returns high efficiency to the project management when it comes to project tracking proposes. In addition to different tracking mechanisms, project workforce management software also offer a dashboard for the project team. Through the dashboard, the project team has a glance view of the overall progress of the project elements.

Most of the times, project workforce management software can work with the existing legacy software systems such as ERP (enterprise resource planning) systems. This easy integration allows the organisation to use a combination of software systems for management purposes.<ref>{{Citation|title = Project workforce management Software Use|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>

=== Background ===
Good project management is an important factor for the success of a project. A project may be thought of as a collection of activities and tasks designed to achieve a specific goal of the organisation, with specific performance or quality requirements while meeting any subject time and cost constraints. Project management refers to managing the activities that lead to the successful completion of a project. Furthermore, it focuses on finite deadlines and objectives. A number of tools may be used to assist with this as well as with assessment.

Project management may be used when planning personnel resources and capabilities. The project may be linked to the objects in a professional services life cycle and may accompany the objects from the opportunity over quotation, contract, time and expense recording, billing, period-end-activities to the final reporting. Naturally the project gets even more detailed when moving through this cycle.<ref>{{Citation|title = Project workforce management background|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>

For any given project, several project tasks should be defined. Project tasks describe the activities and phases that have to be performed in the project such as writing of layouts, customising, testing. What is needed is a system that allows project positions to be correlated with project tasks. Project positions describe project roles like project manager, consultant, tester, etc. Project-positions are typically arranged linearly within the project. By correlating project tasks with project positions, the qualifications and availability of personnel assigned to the project positions may be considered.

== Benefits of Project Management ==
<ref>{{Cite web|title = The advantages of project management and how it can help your business|url = https://www.nibusinessinfo.co.uk/content/advantages-project-management-and-how-it-can-help-your-business|website = nibusinessinfo.co.uk|accessdate = 2015-11-04|last = Migrator}}</ref> Good project management should:
* Reduce the chance of a project failing
* Ensure a minimum level of quality and that results meet requirements and expectations
* Free up other staff members to get on with their area of work and increase efficiency both on the project and within the business
* Make things simpler and easier for staff with a single point of contact running the overall project
* Encourage consistent communications amongst staff and suppliers
* Keep costs, timeframes and resources to budget

== Workflow Engine ==
When it comes to project workforce management, it is all about managing all the logistic aspects of a project or an organisation through a software application. Usually, this software has a workflow engine defined in them. So, all the logistic processes take place in the workflow engine.

The regular and most common types of tasks handled by project workforce management software or a similar workflow engine are:

=== Planning and Monitoring the Project Schedules and Milestones ===
Regularly monitoring your project’s schedule performance can provide early indications of possible activity-coordination problems, resource conflicts, and possible cost overruns. To monitor schedule performance. Collecting information and evaluating it ensure a project accuracy.<ref>{{Cite web|title = How to Monitor Project-Schedule Performance - For Dummies|url = http://www.dummies.com/how-to/content/how-to-monitor-schedule-performance.html|website = www.dummies.com|accessdate = 2015-11-04}}</ref>

=== Tracking the Cost and Revenue aspects of Projects ===
The importance of tracking actual costs and resource usage in projects depends upon the project situation.

Tracking actual costs and resource usage is an essential aspect of the project control function.<ref>{{Cite web|title = Why Track Actual Costs and Resource Usage on Projects?|url = http://www.projecttimes.com/articles/why-track-actual-costs-and-resource-usage-on-projects.html|website = www.projecttimes.com|accessdate = 2015-11-04}}</ref>

=== Resource Utilisation and Monitoring ===
Organisational profitability is directly connected to project management efficiency and optimal resource utilisation.To sum up, organisations that struggle with either or both of these core competencies typically experience cost overruns, schedule delays and unhappy customers.<ref>{{Cite web|title = Resource Utilization in Project Management|url = https://www.clarizen.com/work/resource-utilization-in-project-management|website = www.clarizen.com|accessdate = 2015-11-04}}</ref>

The focus for project management is the analysis of project performance to determine whether a change is needed in the plan for the remaining project activities to achieve the project goals.<ref>{{Cite web|title = Project Management Guru Monitoring and Controlling Tools|url = http://www.projectmanagementguru.com/controlling.html|website = www.projectmanagementguru.com|accessdate = 2015-11-04}}</ref>

=== Other Management Aspects of the Project Management<ref>{{Cite web|title = Project Management Guide - How to Manage a Project {{!}} TeamGantt|url = http://teamgantt.com/guide-to-project-management/|website = teamgantt.com|accessdate = 2015-11-04}}</ref> ===

==== Project risk management ====
Risk identification consists of determining which risks are likely to affect the project and documenting the characteristics of each.

==== Project communication management ====
Project communication management is about how communication is carried out during the course of the project

==== Project quality management ====
It is of no use completing a project within the set time and budget if the final product is of poor quality. The project manager has to ensure that the final product meets the quality expectations of the stakeholders. This is done by good: 

===== ''Quality Planning:'' =====
Identifying what quality standards are relevant to the project and determining how to meet them.

===== ''Quality Assurance:'' =====
Evaluating overall project performance on a regular basis to provide confidence that the project will satisfy the relevant quality standards.

===== ''Quality Control:'' =====

Monitoring specific project results to determine if they comply with relevant quality standards and identifying ways to remove causes of poor performance.

==Project Workforce Management vs. Traditional Management==
There are three main differences between Project Workforce Management and traditional [[project management]] and [[workforce management]] disciplines and solutions:<ref>{{Cite web
|author = Rudolf Melik|title = The Rise of the Project Workforce|url = https://books.google.co.uk/books?id=0b2RB81RqyQC&pg=PA121&lpg=PA121&dq=the+rise+of+project+workforce+pdf&source=bl&ots=_Io_xYQd2Q&sig=4KO0i1Gr5m_XoybVwJHqfP0enHk&hl=en&sa=X&ved=0CDIQ6AEwBGoVChMImJOZ3a33yAIVQ70aCh1yGAq4#v=onepage&q=the%20rise%20of%20project%20workforce%20pdf&f=false|publisher = Wiley: New York, NY|year = 2007|accessdate = November 4, 2015}}</ref>

=== Workflow-driven ===
All project and workforce processes are designed, controlled and audited using a built-in graphical workflow engine. Users can design, control and audit the different processes involved in the project. The graphical workflow is quite attractive for the users of the system and allows the users to have a clear idea of the workflow engine.<ref>{{Cite book|title = Flexibility of Data-Driven Process Structures|url = http://link.springer.com/chapter/10.1007/11837862_19|publisher = Springer Berlin Heidelberg|date = 2006-09-04|isbn = 978-3-540-38444-1|pages = 181–192|series = Lecture Notes in Computer Science|first = Dominic|last = Müller|first2 = Manfred|last2 = Reichert|first3 = Joachim|last3 = Herbst|editor-first = Johann|editor-last = Eder|editor-first2 = Schahram|editor-last2 = Dustdar}}</ref>

=== Organisation and Work Breakdown Structures ===
Project Workforce Management provides organization and work breakdown structures to create, manage and report on functional and approval hierarchies, and to track information at any level of detail. Users can create, manage, edit and report work breakdown structures. Work breakdown structures have different abstraction levels, so the information can be tracked at any level. Usually, project workforce management has approval hierarchies. Each workflow created will go through several records before it becomes an organisational or project standard. This helps the organisation to reduce the inefficiencies of the process, as it is audited by many stakeholders.<ref>{{Cite web|title = Organisational Breakdown Structure|url = http://www.successful-project-management.com/organisational-breakdown-structure.html|website = www.successful-project-management.com|accessdate = 2015-11-04}}</ref>

=== Connected Project, Workforce and Financial Processes ===
Unlike traditional disconnected project, workforce and billing management systems that are solely focused on tracking IT projects, internal workforce costs or billable projects, Project Workforce Management is designed to unify the coordination of all project and workforce processes, whether internal, shared (IT) or billable.

== Summary ==
A project workforce management system defines project tasks, project positions and assigns personnel to the project positions. The project tasks and project positions are correlated to assign a responsible project position or positions to complete each project task. Because each project position may be assigned to a specific person, the qualification and availabilities of the person can be taken into account when determining the assignment. By correlating the project tasks and project positions, a manager can better control the assignment of the workforce and complete projects more efficiently.<ref>{{Citation|title = Project workforce management abstract|url = http://www.google.com/patents/US20030236692|accessdate = 2015-11-04}}</ref>

Project workflow management is one of the best methods for managing different aspects of project. If the project is complex, then the outcomes for the project workforce management could be more effective.

For simple projects or small organisations, project workflow management may not add much value, but for more complex projects and big organisations, managing project workflow will make a big difference. This is because that small organisations or projects do not have a significant overhead when it comes to managing processes. There are many project workforce management, but many organisations prefer to adopt unique solutions.

Therefore, organisation gets software development companies to develop custom project workflow managing systems for them. This has proved to be the most suitable way of getting the best project workforce management system acquired for the company.

==Literature==
*{{Cite book
 | first = Rudolf
 | last = Melik
 | authorlink =
 | year = 2007
 | title = The Rise of the Project Workforce
 | edition =
 | publisher = Willey
 | location = New York, NY
 | isbn = 0-470-12430-X
}}

==References==
{{Wikiquote}}
{{Reflist}}

[[Category:Data management]]
[[Category:ERP software]]
[[Category:Project management]]
[[Category:Workflow technology]]
<=====doc_Id=====>:206
<=====title=====>:
Very large database
<=====text=====>:
{{About|Large size databases|International Conference on Very Large Databases|VLDB}}

A '''very large database''', or '''VLDB''', is a database that contains an extremely high number of [[tuple]]s (database rows), or occupies an extremely large physical [[filesystem]] storage space. The most common definition of VLDB is a database that occupies more than 1 [[terabyte]] or contains several billion rows, although naturally this definition changes over time.{{Citation needed|date=March 2013}}

Very large databases are often, but not necessarily, a core component in [[big data]] analysis.

==References==
{{reflist}}

{{Database}}

{{DEFAULTSORT:Very Large Database}}
[[Category:Data management]]
[[Category:Types of databases]]
<=====doc_Id=====>:209
<=====title=====>:
Enterprise bus matrix
<=====text=====>:
{{Multiple issues|
{{weasel|date=December 2010}}
{{orphan|date=February 2012}}
{{cleanup|date=December 2010}}
}}

The '''Enterprise Bus Matrix''' is a [[data Warehouse]] planning tool and model created by [[Ralph Kimball]], and is part of the Data Warehouse Bus Architecture. The Matrix is the logical definition of one of the core concepts of Kimball’s approach to Dimensional Modeling – Conformed dimensions.<ref>{{cite web|url=http://www.kimballgroup.com/2003/09/15/design-tip-49-off-the-bench/ |title=Design Tip #49: Off The Bench |publisher=Kimball Group |date=2003-09-15 |accessdate=2015-05-22 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>

The Bus Matrix defines part of the Data Warehouse Bus Architecture and is an output of the Business Requirements phase in [[The Kimball Lifecycle]]. It is applied in the following phases of [[dimensional modeling]] and development of the Data Warehouse . The matrix can be categorized as a hybrid model, being part technical design tool, part project management tool and part communication tool<ref name="Kimball">Kimball, Ralph & Ross, Margy; The Data Warehouse Toolkit: The Complete Guide to Dimensional Modeling, 2nd Edition John Wiley & Sons, 2002</ref>

==Background==
The Enterprise Bus Matrix stems from the issue of how one goes about creating the overall Data Warehouse environment.  Historically there has been the structure of the centralized and planned approach and the more loosely defined, department specific, solutions developed in a more independent matter. Autonomous projects can result in a range of isolated stove pipe data marts. Naturally each approach has its issues; the overall visionary approach often struggles with long delivery cycles and lack of reaction time as the formalities and scope issues is evident. On the other hand, the development of isolated data marts, leading to [[Stovepipe system]]s that lacks synergy in development. Over time this approach will lead to a so-called data-mart-in-a-box architecture<ref>[http://www.mimno.com/avoiding-mistakes3.html#6]  {{webarchive |url=https://web.archive.org/web/20100704220014/http://www.mimno.com/avoiding-mistakes3.html#6 |date=July 4, 2010 }}</ref> where [[interoperability]] and lack of cohesion is apparent, and can hinder the realization of an overall enterprise Data Warehouse. As an attempt to handle this matter [[Ralph Kimball]] introduced the enterprise bus.

==Bus matrix==
The bus matrix purpose is one of high abstraction and visionary planning on the Data Warehouse architectural level. By dictating coherency in the development and implementation of an overall Data Warehouse the Bus Architecture approach enables an overall vision of the broader enterprise integration and consistency while at the same time dividing the problem into more manageable parts<ref name="Kimball" /> – all in a technology and software independent manner .<ref>{{cite web|url=http://www.b-eye-network.com/view/713 |title=Data Warehouse: Ralph Kimball’s Vision by Katherine Drewek |publisher=Beyenetwork |date=2005-03-16 |accessdate=2015-05-22}}</ref>

The bus matrix and architecture builds upon the concept of conformed dimensions -  creating a structure of common dimensions that ideally can be used across the enterprise by all business processes related to the DW and the corresponding fact tables from which they derive their context. According to Kimball and Margy Ross's article  “Differences of Opinion”<ref>{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=0OVJNEHMPRXGRQE1GHRSKH4ATMY32JVN?articleID=17800088 |title=Enterprise Software News, Analysis, & Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}</ref> "''The Enterprise Data warehouse built on the bus architecture ”identifies and enforces the relationship between business process metrics (facts) and descriptive attributes (dimensions)''”.

The concept of a [[Bus (computing)|bus]] is well known in the language of [[Information Technology]], and is what reflects the conformed dimension concept in the Data Warehouse, creating the skeletal structure where all parts of a system connect, ensuring [[interoperability]] and consistency of data, and at the same time considers future expansion. This makes the conformed dimensions act as the integration ‘glue’, creating a robust backbone of the enterprise Data Warehouse.<ref>{{cite web|url=http://intelligent-enterprise.informationweek.com/showArticle.jhtml;jsessionid=GMS3H4SOBFQBBQE1GHOSKH4ATMY32JVN?articleID=17800088&pgno=2 |title=Enterprise Software News, Analysis, & Advice - InformationWeek |publisher=Intelligent-enterprise.informationweek.com |date= |accessdate=2015-05-22}}</ref>

==Establishment and applicability==
Figure 1<ref>{{cite web|url=http://www.widama.us/Documents/Kimball-DimensionalModeling.PDF |format=PDF |title=Dimensional Modeling Overview |author=Bob Becker |publisher=Widama.is |accessdate=2015-05-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20130322224742/http://www.widama.us:80/Documents/Kimball-DimensionalModeling.PDF |archivedate=2013-03-22 |df= }}</ref> shows the base for a single document planning tool for the whole of the DW implementation - a graphical overview of the enterprises core business processes or events each correspond to a measurement table of facts, that typically is complemented by a major source system in the horizontal rows.  In the vertical columns the groups of contextual data is found as the common, conformed dimensions.

In this way the shared dimensions are defined, as each process indicates what dimensions it applies to through the cells figure 2.<ref name="Kimball" /> By this definition and coordination of conformed dimensions and processes the development of the overall data DW bus architecture is realized.<ref name="Kimball" /> The matrix identifies the shared dimensions related to processes and fact tables, and can be a tool for planning, prioritizing what needs to be approached, coordinating implementation and communicating the importance for conformed dimensions .

Kimball extends the matrix bus in detail as seen in figure 3<ref name="Kimball" />  by introducing the other steps of the Datawarehouse Methodology; The Fact tables, Granularity, and at last the description of the needed facts.  description of the fact tables, granularity and fact instances of each process, structuring and specifying what is needed across the enterprise in a more specific matter, further exemplifying how the matrix can be used as a planning tool.

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise bus matrix}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing]]
[[Category:Information technology management]]
<=====doc_Id=====>:212
<=====title=====>:
Category:Data warehousing
<=====text=====>:
[[Category:Information technology management]]
[[Category:Business intelligence]]
[[Category:Data management|Warehousing]]
<=====doc_Id=====>:215
<=====title=====>:
Record linkage
<=====text=====>:
'''Record linkage''' (RL) refers to the task of finding [[Record (database)|records]] in a data set that refer to the same [[entity]] across different data sources (e.g., data files, books, websites, databases).  Record linkage is necessary when [[Join (SQL)|joining]] data sets based on entities that may or may not share a common identifier (e.g., [[Relational model|database key]], [[Uniform Resource Identifier|URI]], [[National identification number]]), as may be the case due to differences in record shape, storage location, and/or curator style or preference.  A data set that has undergone RL-oriented reconciliation may be referred to as being ''cross-linked''. 
Record Linkage is called Data Linkage in many jurisdictions, but is the same process.

== History ==
The initial idea of record linkage goes back to [[Halbert L. Dunn]] in his 1946 article titled "Record Linkage" published in the ''[[American Journal of Public Health]]''.<ref>{{cite journal
 | first = Halbert L. | last = Dunn | authorlink = Halbert L. Dunn
 | title = Record Linkage
 | journal = [[American Journal of Public Health]]
 |date=December 1946 | volume = 36 | issue = 12 | pages = ''pp.'' 1412&ndash;1416
 | url = http://www.ajph.org/cgi/reprint/36/12/1412
 | format = PDF
 | accessdate = 2008-05-31
 | doi = 10.2105/AJPH.36.12.1412
}}</ref>  Howard Borden Newcombe laid the probabilistic foundations of modern record linkage theory in a 1959 article in ''[[Science (journal)|Science]]'',<ref>{{cite journal|last=Newcombe|first=H. B. |author2=J.M. Kennedy |author3=S.J. Axford |author4=A. P. James|title=Automatic Linkage of Vital Records|journal=Science|date=October 1959|volume=130|issue=3381|pages=954–959|doi=10.1126/science.130.3381.954|pmid=14426783}}</ref> which were then formalized in 1969 by [[Ivan Fellegi]] and Alan Sunter who proved that the probabilistic decision rule they described was optimal when the comparison attributes were conditionally independent.  Their pioneering work "A Theory For Record Linkage"<ref name=FellegiSunter>{{cite journal 
 | first = Ivan  | last = Fellegi | authorlink = Ivan Fellegi 
 |author2=Sunter, Alan 
  | title = A Theory for Record Linkage 
 | journal = [[Journal of the American Statistical Association]] 
 |date=December 1969 | volume = 64 | issue = 328 | pages = ''pp.'' 1183&ndash;1210  
 | jstor = 2286061
 | doi = 10.2307/2286061 
| url = http://courses.cs.washington.edu/courses/cse590q/04au/papers/Felligi69.pdf | format = PDF
 }}</ref> remains the mathematical foundation for many record linkage applications even today.

Since the late 1990s, various machine learning techniques have been developed that can, under favorable conditions, be used to estimate the conditional probabilities required by the Fellegi-Sunter (FS) theory.  Several researchers have reported that the conditional independence assumption of the FS algorithm is often violated in practice; however, published efforts to explicitly model the conditional dependencies among the comparison attributes have not resulted in an improvement in record linkage quality.
{{Citation needed|date=May 2007}} On the other hand, machine learning or neural network algorithms that do not rely on these assumptions often provide far higher accuracy, when sufficient labeled training data is available.<ref name="ReferenceA">{{cite conference | first = D. Randall | last = Wilson, D. Randall | title = Beyond Probabilistic Record Linkage: Using Neural Networks and Complex Features to Improve Genealogical Record Linkage | conference = Proceedings of International Joint Conference on Neural Networks | location = San Jose, California, USA | date = July 31 – August 5, 2011 | url = http://axon.cs.byu.edu/~randy/pubs/wilson.ijcnn2011.beyondprl.pdf}}</ref>

Record linkage can be done entirely without the aid of a computer, but the primary reasons computers are often used for record linkage are to reduce or eliminate manual review and to make results more easily reproducible.  Computer matching has the advantages of allowing central supervision of processing, better quality control, speed, consistency, and better reproducibility of results.<ref>{{cite web|last=Winkler|first=William E.|title=Matching and Record Linkage|url=http://www.census.gov/srd/papers/pdf/rr93-8.pdf|publisher=U.S. Bureau of the Census|accessdate=12 November 2011}}</ref>

== Naming conventions ==
"Record linkage" is the term used by statisticians, epidemiologists, and historians, among others, to describe the process of joining records from one data source with another that describe the same entity.  Commercial mail and database applications refer to it as "merge/purge processing" or "list washing".  [[computer science|Computer scientists]] often refer to it as "data matching" or as the "object identity problem".  Other names used to describe the same concept include: "coreference/entity/identity/name/record resolution", "entity disambiguation/linking", "duplicate detection", "deduplication", "record matching", "(reference) reconciliation", "object identification", "data/information integration" and "conflation".<ref>http://homes.cs.washington.edu/~pedrod/papers/icdm06.pdf</ref>  This profusion of terminology has led to few cross-references between these research communities.<ref>[http://datamining.anu.edu.au/linkage.html Cristen, P & T: Febrl - Freely extensible biomedical record linkage (Manual, release 0.3) p.9]</ref><ref>
{{cite journal
 | first = Ahmed  | last = Elmagarmid 
 |author2=Panagiotis G. Ipeirotis |author3=Vassilios Verykios 
  | title = Duplicate Record Detection: A Survey 
 | journal = IEEE Transactions on Knowledge and Data Engineering 
 |date=January 2007  | volume = 19  | issue = 1  | pages = ''pp.'' 1&ndash;16 
 | url = http://www.cs.purdue.edu/homes/ake/pub/TKDE-0240-0605-1.pdf | format = PDF  | accessdate = 2009-03-30
 | doi = 10.1109/TKDE.2007.9
}}
</ref>

While they share similar names, record linkage and [[Linked Data]] are two separate concepts.  Whereas record linkage focuses on the more narrow task of identifying matching entities across different data sets, Linked Data focuses on the broader methods of structuring and publishing data to facilitate the discovery of related information.

== Methods ==

=== Data preprocessing ===
Record linkage is highly sensitive to the quality of the data being linked, so all data sets under consideration (particularly their key identifier fields) should ideally undergo a [[data quality assessment]] prior to record linkage.  Many key identifiers for the same entity can be presented quite differently between (and even within) data sets, which can greatly complicate record linkage unless understood ahead of time.  For example, key identifiers for a man named William J. Smith might appear in three different data sets as so:

{| class="wikitable"
|-
! Data set !! Name !! Date of birth !! City of residence
|-
| Data set 1 || William J. Smith || 1/2/73 || Berkeley, California
|-
| Data set 2 || Smith, W. J. || 1973.1.2 || Berkeley, CA
|-
| Data set 3 || Bill Smith || Jan 2, 1973 || Berkeley, Calif.
|}

In this example, the different formatting styles lead to records that look different but in fact all refer to the same entity with the same logical identifier values.  Most, if not all, record linkage strategies would result in more accurate linkage if these values were first ''normalized'' or ''standardized'' into a consistent format (e.g., all names are "Surname, Given name", and all dates are "YYYY/MM/DD").  Standardization can be accomplished through simple rule-based [[data transformation]]s or more complex procedures such as lexicon-based [[Tokenization (lexical analysis)|tokenization]] and probabilistic hidden Markov models.<ref>{{cite journal|last=Churches|first=Tim|author2=Peter Christen |author3=Kim Lim |author4=Justin Xi Zhu |title=Preparation of name and address data for record linkage using hidden Markov models|journal=BMC Medical Informatics and Decision Making|date=13 December 2002|volume=2|doi=10.1186/1472-6947-2-9|url=http://www.biomedcentral.com/1472-6947/2/9 |pages=9}}</ref>  Several of the packages listed in the ''Software Implementations'' section provide some of these features to simplify the process of data standardization.

===Entity resolution===
'''Entity resolution''' is an operational [[intelligence]] process, typically powered by an entity resolution engine or [[middleware]], whereby organizations can connect disparate data sources with a [[Opinion|view]] to understanding possible entity matches and non-obvious relationships across multiple [[data silos]]. It analyzes all of the [[information]] relating to individuals and/or entities from multiple sources of data, and then applies likelihood and probability scoring to determine which identities are a match and what, if any, non-obvious relationships exist between those identities.

Entity resolution engines are typically used to uncover [[risk]], [[fraud]], and conflicts of interest, but are also useful tools for use within [[Customer Data Integration]] (CDI) and [[Master Data Management]] (MDM) requirements. Typical uses for entity resolution engines include terrorist screening, insurance fraud detection, [[USA Patriot Act]] compliance, [[Organized retail crime]] ring detection and applicant screening.

For example: Across different data silos - employee records, vendor data, watch lists, etc. - an organization may have several variations of an entity named ABC, which may or may not be the same individual. These entries may, in fact, appear as ABC1, ABC2, or ABC3 within those data sources. By comparing similarities between underlying attributes such as [[Address (geography)|address]], [[date of birth]], or [[social security number]], the user can eliminate some possible matches and confirm others as very likely matches.

Entity resolution engines then apply rules, based on common sense logic, to identify hidden relationships across the data. In the example above, perhaps ABC1 and ABC2 are not the same individual, but rather two distinct people who share common attributes such as address or phone number.

====Data Matching====
While entity resolution solutions include data matching technology, many data matching offerings do not fit the definition of entity resolution. Here are four factors that distinguish entity resolution from data matching, according to John Talburt, director of the [[Ualr|UALR]] Center for Advanced Research in Entity Resolution and Information Quality:

* Works with both structured and unstructured records, and it entails the process of extracting references when the sources are unstructured or semi-structured
* Uses elaborate business rules and concept models to deal with missing, conflicting, and corrupted information
* Utilizes non-matching, asserted linking (associate) information in addition to direct matching
* Uncovers non-obvious relationships and association networks (i.e. who's associated with whom)

In contrast to data quality products, more powerful identity resolution engines also include a rules engine and workflow process, which apply business intelligence to the resolved identities and their relationships. These advanced technologies make automated decisions and impact business processes in real time, limiting the need for human intervention.

=== Deterministic record linkage ===
The simplest kind of record linkage, called ''deterministic'' or ''rules-based record linkage'', generates links based on the number of individual identifiers that match among the available data sets.<ref>{{cite journal|last=Roos|first=LL|author2=Wajda A |title=Record linkage strategies. Part I: Estimating information and evaluating approaches.|journal=Methods of Information in Medicine|date=April 1991|volume=30|issue=2|pages=117–123|pmid=1857246}}</ref>  Two records are said to match via a deterministic record linkage procedure if all or some identifiers (above a certain threshold) are identical.  Deterministic record linkage is a good option when the entities in the data sets are identified by a common identifier, or when there are several representative identifiers (e.g., name, date of birth, and sex when identifying a person) whose quality of data is relatively high.

As an example, consider two standardized data sets, Set A and Set B, that contain different bits of information about patients in a hospital system.  The two data sets identify patients using a variety of identifiers: [[Social Security Number]] (SSN), name, date of birth (DOB), sex, and [[ZIP code]] (ZIP).  The records in two data sets (identified by the "#" column) are shown below:

{| class="wikitable"
|-
! Data Set !! # !! SSN !! Name !! DOB !! Sex !! ZIP
|-
| rowspan="4" | Set A || 1 || 000956723 || Smith, William || 1973/01/02 || Male || 94701
|- style="background:#f0f0f0;"
| 2 || 000956723 || Smith, William || 1973/01/02 || Male || 94703
|-
| 3 || 000005555 || Jones, Robert || 1942/08/14 || Male || 94701
|- style="background:#f0f0f0;"
| 4 || 123001234 || Sue, Mary || 1972/11/19 || Female || 94109
|-
| rowspan="2" | Set B || 1 || 000005555 ||Jones, Bob || 1942/08/14 || ||
|- style="background:#f0f0f0;"
| 2 || || Smith, Bill || 1973/01/02 || Male || 94701
|}

The most simple deterministic record linkage strategy would be to pick a single identifier that is assumed to be uniquely identifying, say SSN, and declare that records sharing the same value identify the same person while records not sharing the same value identify different people.  In this example, deterministic linkage based on SSN would create entities based on A1 and A2; A3 and B1; and A4.  While A1, A2, and B2 appear to represent the same entity, B2 would not be included into the match because it is missing a value for SSN.

Handling exceptions such as missing identifiers involves the creation of additional record linkage rules.  One such rule in the case of missing SSN might be to compare name, date of birth, sex, and ZIP code with other records in hopes of finding a match.  In the above example, this rule would still not match A1/A2 with B2 because the names are still slightly different: standardization put the names into the proper (Surname, Given name) format but could not discern "Bill" as a nickname for "William".  Running names through a [[phonetic algorithm]] such as [[Soundex]], [[NYSIIS]], or [[metaphone]], can help to resolve these types of problems (though it may still stumble over surname changes as the result of marriage or divorce), but then B2 would be matched only with A1 since the ZIP code in A2 is different.  Thus, another rule would need to be created to determine whether differences in particular identifiers are acceptable (such as ZIP code) and which are not (such as date of birth).

As this example demonstrates, even a small decrease in data quality or small increase in the complexity of the data can result in a very large increase in the number of rules necessary to link records properly.  Eventually, these linkage rules will become too numerous and interrelated to build without the aid of specialized software tools.  In addition, linkage rules are often specific to the nature of the data sets they are designed to link together.  One study was able to link the Social Security [[Death Master File]] with two hospital registries from the [[Midwestern United States]] using SSN, NYSIIS-encoded first name, birth month, and sex, but these rules may not work as well with data sets from other geographic regions or with data collected on younger populations.<ref>{{cite journal|last=Grannis|first=SJ|author2=Overhage JM |author3=McDonald CJ |title=Analysis of identifier performance using a deterministic linkage algorithm|journal=Proc AMIA Symp.|year=2002|pages=305–9|pmid=12463836|pmc=2244404}}</ref>  Thus, continuous maintenance testing of these rules is necessary to ensure they continue to function as expected as new data enter the system and need to be linked.  New data that exhibit different characteristics than was initially expected could require a complete rebuilding of the record linkage rule set, which could be a very time-consuming and expensive endeavor.

=== Probabilistic record linkage ===
''Probabilistic record linkage'', sometimes called ''fuzzy matching'' (also ''probabilistic merging'' or ''fuzzy merging'' in the context of merging of databases), takes a different approach to the record linkage problem by taking into account a wider range of potential identifiers, computing weights for each identifier based on its estimated ability to correctly identify a match or a non-match, and using these weights to calculate the probability that two given records refer to the same entity.  Record pairs with probabilities above a certain threshold are considered to be matches, while pairs with probabilities below another threshold are considered to be non-matches; pairs that fall between these two thresholds are considered to be "possible matches" and can be dealt with accordingly (e.g., human reviewed, linked, or not linked, depending on the requirements).  Whereas deterministic record linkage requires a series of potentially complex rules to be programmed ahead of time, probabilistic record linkage methods can be "trained" to perform well with much less human intervention.

Many probabilistic record linkage algorithms assign match/non-match weights to identifiers by means of two probabilities called ''u'' and ''m''. The ''u'' probability is the probability that an identifier in two ''non-matching'' records will agree purely by chance.  For example, the ''u'' probability for birth month (where there are twelve values that are approximately uniformly distributed) is 1/12 ≈ 0.083; identifiers with values that are not uniformly distributed will have different ''u'' probabilities for different values (possibly including missing values).  The ''m'' probability is the probability that an identifier in ''matching'' pairs will agree (or be sufficiently similar, such as strings with high [[Jaro-Winkler distance]] or low [[Levenshtein distance]]).  This value would be 1.0 in the case of perfect data, but given that this is rarely (if ever) true, it can instead be estimated.  This estimation may be done based on prior knowledge of the data sets, by manually identifying a large number of matching and non-matching pairs to "train" the probabilistic record linkage algorithm, or by iteratively running the algorithm to obtain closer estimations of the ''m'' probability.  If a value of 0.95 were to be estimated for the ''m'' probability, then the match/non-match weights for the birth month identifier would be:

{| class="wikitable"
|-
! Outcome !! Proportion of links !! Proportion of non-links !! Frequency ratio !! Weight
|-
| Match || ''m'' = 0.95 || ''u'' ≈ 0.083 || ''m''/''u'' ≈ 11.4 || ln(''m''/''u'')/ln(2) ≈ 3.51
|-
| Non-match || 1−''m'' = 0.05 || 1-''u'' ≈ 0.917 || (1-''m'')/(1-''u'') ≈ 0.0545 || ln((1-''m'')/(1-''u''))/ln(2) ≈ -4.20
|}

The same calculations would be done for all other identifiers under consideration to find their match/non-match weights.  Then, every identifier of one record would be compared with the corresponding identifier of another record to compute the total weight of the pair: the ''match'' weight is added to the running total whenever a pair of identifiers agree, while the ''non-match'' weight is added (i.e. the running total decreases) whenever the pair of identifiers disagrees.  The resulting total weight is then compared to the aforementioned thresholds to determine whether the pair should be linked, non-linked, or set aside for special consideration (e.g. manual validation).<ref name="prl">{{cite journal|last=Blakely|first=Tony|author2=Salmond, Clare |title=Probabilistic record linkage and a method to calculate the positive predictive value|journal=International Journal of Epidemiology|date=December 2002|volume=31|issue=6|pages=1246–1252|doi=10.1093/ije/31.6.1246|pmid=12540730|url=http://ije.oxfordjournals.org/content/31/6/1246.full}}</ref>

Determining where to set the match/non-match thresholds is a balancing act between obtaining an acceptable [[Sensitivity and specificity#Sensitivity|sensitivity]] (or ''recall'', the proportion of truly matching records that are linked by the algorithm) and [[positive predictive value]] (or ''precision'', the proportion of records linked by the algorithm that truly do match).  Various manual and automated methods are available to predict the best thresholds, and some record linkage software packages have built-in tools to help the user find the most acceptable values.  Because this can be a very computationally demanding task, particularly for large data sets, a technique known as ''blocking'' is often used to improve efficiency.  Blocking attempts to restrict comparisons to just those records for which one or more particularly discriminating identifiers agree, which has the effect of increasing the positive predictive value (precision) at the expense of sensitivity (recall).<ref name=prl />  For example, blocking based on a phonetically coded surname and ZIP code would reduce the total number of comparisons required and would improve the chances that linked records would be correct (since two identifiers already agree), but would potentially miss records referring to the same person whose surname or ZIP code was different (due to marriage or relocation, for instance).  Blocking based on birth month, a more stable identifier that would be expected to change only in the case of data error, would provide a more modest gain in positive predictive value and loss in sensitivity, but would create only twelve distinct groups which, for extremely large data sets, may not provide much net improvement in computation speed.  Thus, robust record linkage systems often use multiple blocking passes to group data in various ways in order to come up with groups of records that should be compared to each other.

===Machine learning===
In recent years, a variety of machine learning techniques have been used in record linkage.  It has been recognized<ref name="ReferenceA"/> that a classic algorithm for probabilistic record linkage is equivalent to the [[Naive Bayes]] algorithm in the field of machine learning,<ref>Quass, Dallan, and Starkey, Paul. “Record Linkage for Genealogical Databases,” ACM SIGKDD ’03 Workshop on Data Cleaning, Record Linkage, and Object Consolidation, August 24–27, 2003, Washington, D.C.</ref> and suffers from the same assumption of the independence of its features (an assumption that is typically not true).<ref>Langley, Pat, Wayne Iba, and Kevin Thompson. “An Analysis of Bayesian Classifiers,” In Proceedings of the 10th National Conference on Artificial Intelligence, (AAAI-92), AAAI Press/MIT Press, Cambridge, MA, pp. 223-228, 1992.</ref><ref>Michie, D., D. Spiegelhalter, and C. Taylor. Machine Learning, Neural and Statistical Classification, Ellis Horwood, Hertfordshire, England. Book 19, 1994.</ref>  Higher accuracy can often be achieved by using various other machine learning techniques, including a single-layer [[perceptron]].<ref name="ReferenceA"/>

== Mathematical model ==
In an application with two files, A and B, denote the rows (''records'') by <math>\alpha (a)</math> in file A and <math>\beta (b)</math> in file B. Assign <math>K</math> ''characteristics'' to each record. The set of records that represent identical entities is defined by

<math> M = \left\{ (a,b); a=b; a \in A; b \in B \right\} </math>

and the complement of set <math>M</math>, namely set <math>U</math> representing different entities is defined as

<math> U = \{ (a,b); a \neq b; a \in A, b \in B \} </math>.

A vector, <math>\gamma</math> is defined, that contains the coded agreements and disagreements on each characteristic:

<math> \gamma \left[ \alpha ( a ), \beta ( b ) \right] = \{ \gamma^{1} \left[ \alpha ( a ) , \beta ( b ) \right] ,...,	\gamma^{K} \left[ \alpha ( a ), \beta ( b ) \right] \} </math>

where <math>K</math> is a subscript for the characteristics (sex, age, marital status, etc.) in the files. The conditional probabilities of observing a specific vector <math>\gamma</math> given <math>(a, b) \in M</math>, <math>(a, b) \in U</math> are defined as

<math>
 m(\gamma) = P \left\{ \gamma \left[ \alpha (a), \beta (b) \right] | (a,b) \in M \right\} =
 \sum_{(a, b) \in M} P \left\{\gamma\left[ \alpha(a), \beta(b) \right] \right\} \cdot
                 P \left[ (a, b) | M\right]
</math>

and

<math>
 u(\gamma) = P \left\{ \gamma \left[ \alpha (a), \beta (b) \right] | (a,b) \in U \right\} =
 \sum_{(a, b) \in U} P \left\{\gamma\left[ \alpha(a), \beta(b) \right] \right\} \cdot
                 P \left[ (a, b) | U\right],
</math>
respectively.<ref name=FellegiSunter />

== Applications ==

===Master data management===
Most [[Master data management]] (MDM) products use a record linkage process to identify records from different sources representing the same real-world entity. This linkage is used to create a "golden master record" containing the cleaned, reconciled data about the entity. The techniques used in MDM are the same as for record linkage generally. MDM expands this matching not only to create a "golden master record" but to infer relationships also. (i.e. a person has a same/similar surname and same/similar address, this might imply they share a household relationship).

=== Data warehousing and business intelligence ===
Record linkage plays a key role in [[data warehousing]] and [[business intelligence]].  Data warehouses serve to combine data from many different operational source systems into one [[logical data model]], which can then be subsequently fed into a business intelligence system for reporting and analytics.  Each operational source system may have its own method of identifying the same entities used in the logical data model, so record linkage between the different sources becomes necessary to ensure that the information about a particular entity in one source system can be seamlessly compared with information about the same entity from another source system.  Data standardization and subsequent record linkage often occur in the "transform" portion of the [[extract, transform, load]] (ETL) process.

=== Historical research ===
Record linkage is important to social history research since most data sets, such as [[census|census records]] and parish registers were recorded long before the invention of [[National identification number]]s.  When old sources are digitized, linking of data sets is a prerequisite for [[longitudinal study]].  This process is often further complicated by lack of standard spelling of names, family names that change according to place of dwelling, changing of administrative boundaries, and problems of checking the data against other sources.  Record linkage was among the most prominent themes in the [[History and computing]] field in the 1980s, but has since been subject to less attention in research.{{Citation needed|date=November 2011}}

=== Medical practice and research ===
<!-- any experts out there? -->
Record linkage is an important tool in creating data required for examining the health of the public and of the health care system itself. It can be used to improve data holdings, data collection, quality assessment, and the dissemination of information. Data sources can be examined to eliminate duplicate records, to identify under-reporting and missing cases (e.g., census population counts), to create person-oriented health statistics, and to generate disease registries and health surveillance systems. Some cancer registries link various data sources (e.g., hospital admissions, pathology and clinical reports, and death registrations) to generate their registries. Record linkage is also used to create health indicators. For example, fetal and infant mortality is a general indicator of a country's socioeconomic development, public health, and maternal and child services. If infant death records are matched to birth records, it is possible to use birth variables, such as birth weight and gestational age, along with mortality data, such as cause of death, in analyzing the data.  Linkages can help in follow-up studies of cohorts or other groups to determine factors such as vital status, residential status, or health outcomes. Tracing is often needed for follow-up of industrial cohorts, clinical trials, and longitudinal surveys to obtain the cause of death and/or cancer.  An example of a successful and long-standing record linkage system allowing for population-based medical research is the [[Rochester Epidemiology Project]] based in [[Rochester, Minnesota]].<ref name="data resource profile">{{cite journal | author1=St. Sauver JL  | author2=Grossardt BR | author3=Yawn BP | author4=Melton LJ 3rd | author5=Pankratz JJ |author6=Brue SM | author7=Rocca WA. | title = Data Resource Profile: The Rochester Epidemiology Project (REP) medical records-linkage system | journal = Int J Epidemiol | volume=41 | issue=6 | pages=1614–24 | year = 2012 | pmid = 23159830 | doi=10.1093/ije/dys195 | url=http://ije.oxfordjournals.org/content/41/6/1614.long}}</ref>

== Criticism of existing software implementations==
The main reasons cited are:
* '''Project costs''': costs typically in the hundreds of thousands of dollars
* '''Time''': lack of enough time to deal with large-scale [[data cleansing]] software
* '''Security''': concerns over sharing information, giving an application access across systems, and effects on legacy systems

== See also ==
* [[Capacity optimization]]
* [[Content-addressable storage]]
* [[Data deduplication]]
* [[Delta encoding]]
* [[Entity linking]]
* [[Entity-attribute-value model]]
* [[Identity resolution]]
* [[Linked data]]
* [[Named-entity recognition]]
* [[Open data]]
* [[Schema matching]]
* [[Single-instance storage]]

== Notes and references ==
{{Reflist|2}}

== External links ==
* [http://pike.psu.edu/linkage/ Data Linkage Project at Penn State, USA]
* [http://www.datadecision.com Datadecision - Data matching online tool]
* [http://www.nameapi.org/en/demos/name-matcher/ NameAPI - Name Matcher]
* [http://sourceforge.net/projects/oysterer/ OYSTER Entity Resolution]
* [http://datamining.anu.edu.au/ Febrl - Freely Extensible Biomedical Record Linkage]
* [http://infolab.stanford.edu/serf/ Stanford Entity Resolution Framework]
* [http://dbs.uni-leipzig.de/de/research/projects/large_scale_object_matching/ Dedoop - Deduplication with Hadoop]
* [https://sourceforge.net/projects/erframework/ BlockingFramework A framework for blocking-based Entity Resolution]
* [http://www.ipdln.org/ International Population Data Linkage Network]
* [https://github.com/yahoo/FEL Yahoo Fast Entity Linker Core]

{{DEFAULTSORT:Record Linkage}}
[[Category:Data management]]
<=====doc_Id=====>:218
<=====title=====>:
State transition network
<=====text=====>:
{{unreferenced|date=March 2011}}

A '''state transition network''' is a [[diagram]] that is developed from a set of data and charts the [[data flow|flow of data]] from particular data points (called states or nodes) to the next in a probabilistic manner.

==Use==
State transition networks are used in both [[academic]] and [[industry|industrial]] fields. 

==Examples==
State transition networks are a general construct, with more specific examples being augmented transition networks, recursive transition networks, and augmented recursive networks, among others.
==See also==
* [[State transition system]]
* [[Markov network]]
* [[History monoid]]

==References==
{{reflist}}

[[Category:Data management]]
<=====doc_Id=====>:221
<=====title=====>:
Data management plan
<=====text=====>:
A '''data management plan''' or '''DMP''' is a formal document that outlines how you will handle your [[data]] both during your research, and after the project is completed.<ref>http://www2.lib.virginia.edu/brown/data/plan.html</ref> The goal of a data management plan is to consider the many aspects of [[data management]], [[metadata]] generation, data preservation, and analysis before the project begins; this ensures that data are well-managed in the present, and prepared for preservation in the future.

== Importance ==

Preparing a data management plan before data are collected ensures that data are in the correct format, organized well, and better annotated.<ref>http://libraries.mit.edu/data-management/plan/why/</ref> This saves time in the long term because there is no need to re-organize, re-format, or try to remember details about data. It also increases research efficiency since both the data collector and other researchers will be able to understand and use well-annotated data in the future.  One component of a good data management plan is data archiving and preservation. By deciding on an archive ahead of time, the data collector can format data during collection to make its future submission to a database easier. If data are preserved, they are more relevant since they can be re-used by other researchers.  It also allows the data collector to direct requests for data to the database, rather than address requests individually.  Data that are preserved have the potential to lead to new, unanticipated discoveries, and they prevent duplication of scientific studies that have already been conducted. Data archiving also provides insurance against loss by the data collector.

Funding agencies are beginning to require data management plans as part of the proposal and evaluation process.<ref>http://www.nsf.gov/bfa/dias/policy/dmpfaqs.jsp</ref>

== Major Components ==

=== Information about data & data format ===

* Include a description of data to be produced by the project.<ref>{{Cite web|title = Elements of a Data Management Plan|url = http://www.icpsr.umich.edu/icpsrweb/content/datamanagement/dmp/elements.html|website = www.icpsr.umich.edu|accessdate = 2015-09-30}}</ref> This might include (but is not limited to) data that are:
** Experimental
** Observational
** Raw or derived
** Physical collections
** Models
** Simulations
** Curriculum materials
** Software
** Images
* How will the data be acquired? When and where will they be acquired?
* After collection, how will the data be processed? Include information about
** Software used
** Algorithms
** [[workflow|Scientific workflows]]
* Describe the file formats that will be used, justify those formats, and describe the naming conventions used.
* Identify the quality assurance & quality control measures that will be taken during sample collection, analysis, and processing.
* If existing data are used, what are their origins? How will the data collected be combined with existing data? What is the relationship between the data collected and existing data?
* How will the data be managed in the short-term? Consider the following:
** [[Version control]] for files
** Backing up data and data products
** Security & protection of data and data products
** Who will be responsible for management

=== Metadata content and format ===

[[Metadata]] are the contextual details, including any information important for using data. This may include descriptions of temporal and spatial details, instruments, parameters, units, files, etc. Metadata is commonly referred to as “data about data”.<ref>Michener,WK and JW Brunt. 2000. ''Ecological Data: Design, Management and Processing''. Blackwell Science, 180p.</ref> Consider the following:
* What metadata are needed? Include any details that make data meaningful.
* How will the metadata be created and/or captured? Examples include lab notebooks, GPS hand-held units, Auto-saved files on instruments, etc.
* What format will be used for the metadata? Consider the [[metadata standards]] commonly used in the scientific discipline that contains your work. There should be justification for the format chosen.

=== Policies for access, sharing, and re-use ===

* Describe any obligations that exist for sharing data collected. These may include obligations from funding agencies, institutions, other professional organizations, and legal requirements.
* Include information about how data will be shared, including when the data will be accessible, how long the data will be available, how access can be gained, and any rights that the data collector reserves for using data.
* Address any ethical or privacy issues with data sharing
* Address [[intellectual property]] & [[copyright]] issues. Who owns the copyright? What are the institutional, publisher, and/or funding agency policies associated with intellectual property? Are there embargoes for political, commercial, or patent reasons?
* Describe the intended future uses/users for the data
* Indicate how the data should be cited by others. How will the issue of persistent citation be addressed? For example, if the data will be deposited in a public archive, will the dataset have a [[digital object identifier]] (doi) assigned to it?

=== Long-term storage and data management ===

* Researchers should identify an appropriate archive for long-term preservation of their data. By identifying the archive early in the project, the data can be formatted, transformed, and documented appropriately to meet the requirements of the archive. Researchers should consult colleagues and professional societies in their discipline to determine the most appropriate database, and include a backup archive in their data management plan in case their first choice goes out of existence.
* Early in the project, the primary researcher should identify what data will be preserved in an archive. Usually, preserving the data in its most raw form is desirable, although data derivatives and products can also be preserved.
* An individual should be identified as the primary contact person for archived data, and ensure that contact information is always kept up-to-date in case there are requests for data or information about data.

=== Budget ===

Data management and preservation costs may be considerable, depending on the nature of the project. By anticipating costs ahead of time, researchers ensure that the data will be properly managed and archived. Potential expenses that should be considered are
* Personnel time for data preparation, management, documentation, and preservation
* Hardware and/or software needed for data management, backing up, security, documentation, and preservation
* Costs associated with submitting the data to an archive
The data management plan should include how these costs will be paid.

== NSF Data Management Plan ==

All grant proposals submitted to [[National Science Foundation|NSF]] must include a Data Management Plan that is no more than two pages.<ref>http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp</ref> This is a supplement (not part of the 15 page proposal) and should describe how the proposal will conform to the Award and Administration Guide policy (see below). It may include the following:
# The types of data
# The standards to be used for data and metadata format and content
# Policies for access and sharing
# Policies and provisions for re-use
# Plans for archiving data

Policy summarized from the [[National Science Foundation|NSF]] Award and Administration Guide, Section 4 (Dissemination and Sharing of Research Results):<ref>http://www.nsf.gov/bfa/dias/policy/dmp.jsp</ref>
# Promptly publish with appropriate authorship
# Share data, samples, physical collections, and supporting materials with others, within a reasonable time frame
# Share software and inventions
# Investigators can keep their legal rights over their intellectual property, but they still have to make their results, data, and collections available to others
# Policies will be implemented via
## Proposal review
## Award negotiations and conditions
## Support/incentives

== ESRC Data Management Plan ==

Since 1995, the UK's [[Economic and Social Research Council]] (ESRC) have had a research data policy in place. The current ESRC Research Data Policy states that research data created as a result of ESRC-funded research should be openly available to the scientific community to the maximum extent possible, through long-term preservation and high quality data management.<ref>[http://www.esrc.ac.uk/about-esrc/information/data-policy.aspx ESRC Research Data Policy 2010]</ref>

ESRC requires a data management plan for all research award applications where new data are being created. Such plans are designed to promote a structured approach to data management throughout the data lifecycle, resulting in better quality data that is ready to archive for sharing and re-use. The [[UK Data Service]], the ESRC's flagship data service, provides practical guidance on research data management planning suitable for social science researchers in the UK and around the world.<ref>[http://ukdataservice.ac.uk/manage-data.aspx Prepare and manage data: Guidance from the UK Data Service]</ref><ref>[http://www.sagepub.com/books/Book240297 SAGE handbook: Managing and Sharing Data: A Guide to Good Practice]</ref>

ESRC has a longstanding arrangement with the [[UK Data Archive]], based at the [[University of Essex]], as a place of deposit for research data, with award holders required to offer data resulting from their research grants via the UK Data Service.<ref>[http://www.data-archive.ac.uk/deposit/who UK Data Archive: Who can deposit data?]</ref> The Archive enables data re-use by preserving data and making them available to the research and teaching communities.

== References ==
{{Reflist}}

== Further reading ==
{{Cite book|title = Delivering research data management services|last = Pryor|first = Graham|publisher = Facet Publishing|year = 2014|isbn = 9781856049337|location = |pages = }}

== External links ==
* [http://www.sagepub.com/books/Book240297?&subject=B00&sortBy=defaultPubDate%20desc&fs=1 SAGE handbook]: Managing and Sharing Research Data: A Guide to Good Practice
* [http://dmp.cdlib.org DMPTool]: Guidance and resources for data management plans
* [http://www.cdlib.org/services/uc3/dmp/index.html California Digital Library], University of California Curation Center (UC3)
* [http://www.dataone.org/plans DataONE]
* [http://www2.lib.virginia.edu/brown/data/plan.html University of Virginia Library]
* [https://dmponline.dcc.ac.uk/ DMPonline]
* [http://www.dcc.ac.uk/resources/data-management-plans Digital Curation Centre]
* [http://www.lib.umich.edu/research-data-management-and-publishing-support/nsf-data-management-plans#directorate_guide University of Michigan Library]
* [http://www.nsf.gov/pubs/policydocs/pappguide/nsf11001/gpg_2.jsp#dmp NSF Grant Proposal Guidelines]
* [http://www.icpsr.umich.edu/icpsrweb/ICPSR/dmp/index.jsp Inter-University Consortium for Political and Social Research]
* [http://lno.lternet.edu/node/269 LTER Blog: How to write a data management plan]
* [http://www.gesis.org/en/archive-and-data-management-training-and-information-center/research-data-management/data-management-plan/ More information about data management plans at [[GESIS – Leibniz Institute for the Social Sciences]]]
* [http://ukdataservice.ac.uk/manage-data.aspx UK Data Service]: Prepare and Manage Data: Guidance and tools for social science researchers
* [http://www.consorciomadrono.es/pagoda Plan de Gestión de Datos PaGoDa]: DMP Toolkit of The Consortium of Universities of the Region of Madrid and the UNED for Library Cooperation (Madroño - Spain) 

<!--- Categories --->
[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]
<=====doc_Id=====>:224
<=====title=====>:
Category:Object-oriented database management systems
<=====text=====>:
Articles in this category are pure [[object-oriented database management system]]s.

[[Category:Data management]]
[[Category:Database management systems]]
<=====doc_Id=====>:227
<=====title=====>:
Meta-data management
<=====text=====>:
[[File:Linnaeus - Regnum Animale (1735).png|thumb|[[Linnaean taxonomy]], a metadata system used historically for grouping animals in zoos, first published in 1735]]
[[File:6123034166 card catalog.jpg|thumb|[[Card catalog]] and digital media access point]]
'''Meta-data management''' (also known as [[metadata]] management, without the hyphen) involves managing [[data]] about ''other data'', whereby this "other data" is generally referred to as ''content'' data. The term is used most often in relation to [[Digital media]], but older forms of metadata are catalogs, dictionaries, and taxonomies. For example, the [[Dewey Decimal Classification]] is a metadata management system for books developed in 1876 for libraries.

==Metadata schema==
Metadata management can be defined as the end-to-end process and governance framework for creating, controlling, enhancing, attributing, defining and managing a metadata schema, model or other structured aggregation system, either independently or within a repository and the associated supporting processes (often to enable the management of content). For web-based systems, [[Uniform Resource Locator|URL]]s, images, video etc. may be referenced from a triples table of object, attribute and value.
==Scope==
With specific [[knowledge domain]]s, the boundaries of the metadata for each must be managed, since a general [[ontology]] is not useful to experts in one field whose language is knowledge-domain specific.
==Metadata manager==
If one is in the process of making a knowledge management solution, creating a metadata schema and developing a system in which metadata is managed are very important. In such a project, a dedicated metadata manager may be appointed in order to maintain adherence to metadata and information management standards. {{Citation needed|date=January 2011}} This is a person who will be responsible for the metadata strategy, and possibly, the implementation. A metadata manager does not need to know about and be involved with everything concerning the solution, but it does help to have an understanding of as much of the process as possible to make sure a relevant schema is developed.
==Metadata management over time==

Managing the metadata in a knowledge management solution is an important step in a metadata strategy. It is part of the strategy to make sure that the metadata are complete, current and correct at any given time. Managing a metadata project is also about making sure that users of the system are aware of the possibilities allowed by a well-designed metadata system and how to maximize the benefits of metadata. Regularly monitoring the metadata to ensure that the schema remains relevant is advised.

===Wikipedia metadata===
Wikipedia is a project that actively manages metadata for its articles and files. For example, volunteer editors carefully curate new biographical articles based on the notability (''claim to fame''), name, birth, and/or death dates.<ref>See the internal Wikipedia project on the English Wikipedia called [[Wikipedia:WikiProject Biography]]</ref> Similarly, volunteer editors carefully curate new architectural articles based on name, municipality, or [[geo coordinates]].<ref>See [[Wikipedia:WikiProject Architecture]]</ref> When new articles with a valid alternate spelling are added to Wikipedia that match up to existing articles based on metadata, these are then manually checked and if needed, tagged for merging.<ref>See [[Wikipedia:WikiProject Merge]]</ref> When new articles are added that are considered out of scope or otherwise unfit for Wikipedia, these are nominated for deletion.<ref>See [[Wikipedia:Articles for deletion]]</ref> To help keep track of metadata on Wikipedia, the new Wikimedia project [[Wikidata]] was established in 2012. Click on the pictures to view more metadata about these images:
<gallery>
File:Sta-eulalia.jpg|This picture of the [[Barcelona Cathedral]] was uploaded to the English Wikipedia in 2003 to illustrate its Wikipedia article, and was transferred to [[Wikimedia Commons]] in 2007 so it could be used in other language versions of Wikipedia.
File:Article catedral pantalla estreta.png|This screenprint of the [[Catalan Wikipedia]] page on the cathedral features several photos including this one. The screenprint was uploaded to Wikimedia Commons in 2007 soon after the photo was available there, but [[:ca:Catedral de Barcelona|that article]] on the Catalan Wikipedia has since been expanded.
</gallery>

== See also ==
* [[Data Defined Storage]] 
* [[Metadata discovery]]
* [[Metadata publishing]]
* [[Metadata registry]]
* [[ISO/IEC 11179]]
* [[Dublin core]]

 
==References==
{{reflist}}
{{DEFAULTSORT:Meta-Data Management}}
[[Category:Metadata]]
[[Category:Data management]]
<=====doc_Id=====>:230
<=====title=====>:
Data monetization
<=====text=====>:
'''Data monetization''', a form of [[monetization]], is generating [[revenue]] from available data sources or real time streamed data by instituting the discovery, capture, storage, analysis, dissemination, and use of that data.  Said differently, it is the process by which data producers, data aggregators and data consumers, large and small, exchange sell or trade data. Data monetization leverages data generated through business operations as well as data associated with individual actors and with electronic devices and sensors participating in the [[internet of things]].  The ubiquity of the [[internet of things]] is generating [[location data]] and other data from sensors and [[mobile devices]] at an ever increasing rate. When this data is collated against traditional databases, the value and utility of both sources of data increases, leading to tremendous potential to mine data for social good, research and discovery, and achievement of business objectives.  Closely associated with data monetization are the emerging [[data as a service]] models for transactions involving data by the data item.

There are three [[ethical]] and regulatory vectors involved in data monetization due to the sometimes conflicting interests of actors involved in the [[data supply chain]].  The individual data creator who generates files and records through his own efforts or owns a device such as a sensor or a mobile phone that generates data has a claim to ownership of data.  The business entity that generates data in the course of its operations, such as its transactions with financial institutions or [[risk factors]] discovered through feedback from customers also has a claim on data captured through their systems and platforms. However, the person that contributed the data may also have a legitimate claim on the data.  Internet platforms and service providers, such as [[Google]] or [[Facebook]] that require a user to forgo some ownership interest in their data in exchange for use of the platform also have a legitimate claim on the data.  Thus the practice of data monetization, although common since 2000, is now getting increasing attention from regulators.  The [[European Union]] and the [[United States Congress]] have begun to address these issues.  For instance, in the financial services industry, regulations involving data are included in the [[Gramm–Leach–Bliley Act]] and [[Dodd-Frank]].  Some individual creators of data are shifting to using [[personal data vaults]]<ref>http://www.freepatentsonline.com/y2014/0032267.html</ref> and implementing [[vendor relationship management]]<ref>[[Vendor Relationship Management]]</ref> concepts as a reflection of an increasing resistance to their data being federated or aggregated and resold without compensation.  Groups such as the [[Personal Data Ecosystem Consortium]],<ref>http://personaldataecosystem.org</ref> [[Patient Privacy Rights]],<ref>http://patientprivacyrights.org/</ref> and others are also challenging corporate cooptation of data without compensation.

[[Financial services]] companies are a relatively good example of an industry focused on generating revenue by leveraging data.  [[Credit card]] issuers and [[retail banks]] use customer transaction data to improve targeting of [[cross-sell]] offers.  Partners are increasingly promoting merchant based [[reward programs]] which leverage a bank’s data and provide discounts to customers at the same time.

==Steps==
# Identification of available data sources – this includes data currently available for monetization as well as other external data sources that may enhance the value of what’s currently available.
# Connect, aggregate, attribute, validate, authenticate, and exchange data - this allows data to be converted directly into actionable or revenue generating insight or services.
# Set terms and prices and facilitate data trading - methods for data vetting, storage, and access. For example, many global corporations have locked and siloed data storage infrastructures, which stymies efficient access to data and cooperative and real time exchange. 
# Perform [[Research]] and [[analytics]] – draw predictive insights from existing data as a basis for using data for to reduce [[risk]], enhance product development or performance, or improve [[customer experience]] or business outcomes.
# Action and leveraging – the last phase of monetizing data includes determining alternative or improved datacentric products, ideas, or services.  Examples may include real time actionable triggered notifications or enhanced channels such as web or mobile response mechanisms.

==Pricing Variables and Factors==
*  A fee for use of a platform to connect buyers and sellers
*  A fee for use of a platform to configure, organize, and otherwise process data included in a data trade 
*  A fee for connecting or including a device or sensor into a data supply chain
*  A fee for connecting and credentialing a creator of a data source and a data buyer - often through a [[federated identity]]
*  A fee for connecting a data source to other data sources to be included into a data supply chain
*  A fee for use of an internet service or other transmission service for uploading and downloading data - sometimes, for an individual, through a [[personal cloud]]
*  A price or exchange or other trade value assigned by a data creator or generator to a data item or a data source 
*  A price or exchange or other trade value offered by a data buyer to a data creator 
*  A price or exchange or other trade value assigned by a data buyer for a data item or a data source formatted according to criteria set by a data buyer
*  An incremental fee assigned by a data buyer for a data item or a data set scaled to the reputation of the data creator
*  A fee for use of encrypted keys to achieve secure data transfer
*  A fee for use of a search algorithm specifically designed to tag data sources that contain data points of value to the data buyer
*  A fee for linking a data creator or generator to a data collection protocol or form
*  A fee for server actions - such as a notification - triggered by an update to a data item or data source included into a data supply chain

==Benefits==
*  Improved decision-making that leads to [[real time (media)|real time]] [[crowd sourced]] research, improved profits, decreased costs, reduced risk and improved compliance
*  More impactful decisions (e.g., make real time decisions)
*  More timely (lower latency) decisions (e.g., a vendor making purchase recommendations while the customer is still on the phone or in the store, a customer connecting with multiple vendors to discover a best price, triggered notifications when thresholds are reached for data values )
*  More granular decisions (e.g., localized pricing decisions at an individual or device or sensor level versus larger aggregates).

==Frameworks==
There are a wide variety of industries, firms and business models related to data monetization.  The following frameworks have been offered to help understand the types of business models that are used:

Doug Laney of [[Gartner]], a leading IT research and advisory firm, has posited a model for a range of data monetization methods:

* Indirect Data Monetization
**Using data to improve efficiencies
**Using data to measurably reduce risks
**Using data to develop new products, markets
**Using data to build and solidify partner relationships
**Publishing Branded indices
* Direct Data Monetization
**Bartering or trading with information
**Information-enhanced products or services
**Selling raw data through brokers
**Offering data/report subscriptions

He also suggests a set of feasibility tests and questions for any data monetization ideas being considered:

{| class="wikitable"
|-
! Type of Feasibility !! Feasibility Question 
|-
| Practical || Is the idea utilitarian, or merely interesting/cool? Is it usable?
|-
| Marketable || Would the idea have sufficiently broad appeal, internally or externally?
|-
| Scalable || Can the idea be developed and implemented to the extent required or intended?
|-
| Manageable || Do you have the skills to oversee the development & implementation of the idea?
|-
| Technological || Do you have the tools, information and skills to develop and rollout the idea?
|-
| Economical || Will the idea require too much investment or generate sufficient return on investment?
|-
| Legal || Does the idea conform to local laws where it will be used or implemented?
|-
| Ethical || Will the idea be something that has the potential for customer/user/public backlash?
|-
| Example || Will the idea cause significant positive vs. negative impact on the environment?
|}

Roger Ehrenberg of IA Ventures, a VC firm that invests in this space has defined three basic types of data product firms:
:"'''Contributory databases'''. The magic of these businesses is that a customer provides their own data in exchange for receiving a more robust set of aggregated data back that provides insight into the broader marketplace, or provides a vehicle for expressing a view. Give a little, get a lot back in return –  a pretty compelling value proposition, and one that frequently results in a payment from the data contributor in exchange for receiving enriched, aggregated data. Once these contributory databases are developed and customers become reliant on their insights, they become extremely valuable and persistent data assets.
:
:'''Data processing platforms'''. These businesses create barriers through a combination of complex data architectures, proprietary algorithms and rich analytics to help customers consume data in whatever form they please. Often these businesses have special relationships with key data providers, that when combined with other data and processed as a whole create valuable differentiation and competitive barriers. Bloomberg is an example of a powerful data processing platform. They pull in data from a wide array of sources (including their own home grown data), integrate it into a unified stream, make it consumable via a dashboard or through an API, and offer a robust analytics suite for a staggering number of use cases. Needless to say, their scale and profitability is the envy of the industry.
:
:'''Data creation platforms'''. These businesses solve vexing problems for large numbers of users, and by their nature capture a broad swath of data from their customers. As these data sets grow, they become increasingly valuable in enabling companies to better tailor their products and features, and to target customers with highly contextual and relevant offers. Customers don’t sign up to directly benefit from the data asset; the product is so valuable that they simply want the features offered out-of-the-box. As the product gets better over time, it just cements the lock-in of what is already a successful platform. Mint was an example of this kind of business. People saw value in the core product. But the product continued to get better as more customer data was collected and analyzed. There weren’t network effects, per se, but the sheer scale of the data asset that was created was an essential element of improving the product over time."<ref>{{cite web|last=Ehrenberg |first=Roger |title=Creating competitive advantage through data |url=http://www.iaventures.com/creating-competitive-advantage-through-data |publisher=IA Ventures' blog |accessdate=23 November 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20131203023719/http://www.iaventures.com/creating-competitive-advantage-through-data |archivedate=3 December 2013 |df= }}</ref>

Selvanathan and Zuk <ref>Big Data Realized: Developing New Data-Driven Products and Services to Drive Growth Perspective</ref> offer a framework that includes "monetization methods that are outside the bounds of the
traditional value capture systems employed by an enterprise... tuned to match the context and consumption models for the target customer."  They offer examples of "four distinct approaches: platforms, applications, data-as-a-service, and professional services."

Ethan McCallum and Ken Gleason published an O'Rielly eBook titled ''Business Models for the Data Economy''
:Collect/Supply
:Store/Host
:Filter/Refine
:Enhance/Enrich
:Simplify Access
:Analyze
:Obscure
:Consult/Advise<ref>{{cite book|last=Gleason|first=Ken|title=Business Models for the Data Economy|year=2013|publisher=O'Reilly|isbn=978-1-449-37223-1|url=http://www.oreilly.com/data/free/business-models-for-the-data-economy.csp}}</ref>

==Examples==
*  Packaging of data (with analytics) to be resold to customers for things such as wallet share, [[market share]] and [[benchmarking]]
*  Integration of data (with analytics) into new products as a value-added differentiator such as [[On-Star]] for [[General Motors]] cars
* [[GPS]] enabled [[smartphones]]
* [[Geolocation]]-based offers and location discounts, such as those offered by [[Facebook]]<ref>https://www.theguardian.com/technology/2011/jan/31/facebook-places-deals-uk-europe</ref> and [[Groupon]]<ref>http://mashable.com/2011/05/10/groupon-now-launches/</ref> are other prime examples of data monetization leveraging new emerging channels
* CRM based ad targeting and media attribution, such as those offered by Circulate

==Intellectual property landscape==
Some of the patents issued since 2010 by the [[USPTO]] for monetizing data generated by individuals include; 8,271,346, 8,612,307, 8,560,464, 8,510,176, and 7,860,760.  These are usually in the class 705 related to electronic commerce, data processing, and cost and price determination. Some of these patents use the term, the [[data supply chain]] to reflect emerging technology to federate and aggregate data in real time from many individuals and devices linked together through the [[internet of things]].  Another emerging term is [[information banking]].

An unexplored but potentially disruptive arena for data monetization is the use of [[Bitcoin micropayments]] for data transactions.  Because Bitcoins are emerging as competitors with payment services like Visa or PayPal that can readily enable and reduce or eliminate transaction costs, transactions for as little as a single data item can be facilitated. Consumers as well as enterprises who desire to monetize their participation in a data supply chain may soon be able to access social network enabled Bitcoin exchanges and platforms.<ref>Lomas, Natasha, Techcrunch, August 18, 2014</ref>  [[Clickbait]] and data hijacking may wither as micropayments for data are ubiquitous and enabled. Potentially, even the current need to build out data broker managed data trading exchanges may be bypassed.  Stanley Smith,<ref>http://www.linkedin.com/pub/stan-smith/9/3ab/b37/</ref> who introduced the notion of the data supply chain, has said that simple micropayments for data monetization are the key to evolution of ubiquitous implementation of user configurable data supply schemata, enabling data monetization on a universal scale for all data creators, including the burgeoning internet of things.

==Presentations and Publications ==

2016
* [https://www.gartner.com/doc/3267517 How CIOs and CDOs Can Use Infonomics to Identify, Justify and Fund Initiatives], Douglas Laney and Michael Smith, [[Gartner]] 29 March 2016
* [http://www.wsj.com/articles/accountings-21st-century-challenge-how-to-value-intangible-assets-1458605126 Accounting's 21st Century Challenge: How to Value Intangible Assets], [[WSJ]] CFO Journal, 22 March 2016 
* [https://s3.amazonaws.com/files.technologyreview.com/whitepapers/MIT_Oracle+Report-The_Rise_of_Data_Capital.pdf The Rise of Data Capital], [[Oracle Corporation|Oracle]] and [[MIT]] Technology Review Custom, 2016
* [http://www.gartner.com/smarterwithgartner/treating-information-as-an-asset/ Treating Information as an Asset], Christy Pettey, Douglas Laney and Michael M. Moran, Smarter With [[Gartner]], 17 February 2016
* [http://www.reuters.com/article/us-europe-data-competition-idUSKCN0VF0KV German competition watchdog wants 'big data' hoards considered in merger probes], [[Reuters]], 6 Feb 2016
* [https://www.gartner.com/doc/3188917 Shift From a Project to an Asset Perspective to Properly Value and Fund IT Investments], Michael Smith and Douglas Laney, [[Gartner]], 16 January 2016
* [http://www.iri.com/blog/iri/business/infonomics-and-you/ Infonomics and You], Eric Leohner, [[IRI CoSort]], January 2016
* [http://www.nowozin.net/sebastian/blog/the-fair-price-to-pay-a-spy-an-introduction-to-the-value-of-information.html The Fair Price to Pay a Spy: An Introduction to the Value of Information], Sebastian Nowozin, Nowozin.net blog, 9 January 2016 
2015
* [https://www.gartner.com/doc/3173343 Measure Your Information Yield to Maximize Return on Information and Analytics Investments], Frank Buytendijk, Andrew White, Douglas Laney and Thomas W. Oestreich, [[Gartner]], 1 December 2015
* [https://www.gartner.com/doc/3162520 IBM Storms Information, IoT Markets by Buying The Weather Company], Douglas Laney, [[Gartner]], 4 November 2015
* [https://www.gartner.com/doc/3158117 How to Adopt Open Data for Business Data and Analytics — And Why You Should], Alan D. Duncan & Douglas Laney, [[Gartner]], 28 October 2015
* [https://www.gartner.com/doc/3151321 Seven Steps to Monetizing Your Information Assets], Douglas Laney, [[Gartner]], 15 October 2015
* [http://www.gartner.com/smarterwithgartner/why-and-how-to-value-your-information-as-an-asset/ Why and How to Value Your Information as an Asset] Heather Levy & Douglas Laney, Smarter With [[Gartner]] blog, 3 September 2015
* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don't HDOs Appreciate Healthcare Infonomics?], Laura Craft & Douglas Laney, [[Gartner]], 5 August 2015
* In August 20, 2015 [[Gartner]] Analyst Doug Laney gave a publicly-available webinar (with replay available) on [http://www.gartner.com/webinar/3098518 Methods for Monetizing Your Data]. This is a reprise of the presentation he has given at various [[Gartner]] summits and symposia around the world. 
* [https://www.gartner.com/doc/3106721 Hackers Know the Value of Health Information, So Why Don't HDOs Appreciate Healthcare Infonomics?], Laura Craft & Douglas Laney, [[Gartner]], 5 August 2015
* [https://www.gartner.com/doc/3106719 Why and How to Measure the Value of Your Information Assets], Douglas Laney, [[Gartner]], 5 August 2015
* [http://prezi.com/xirqf54fix68/?utm_campaign=share&utm_medium=copy&rc=ex0share Applied Infonomics: Measuring the Economic Value of Information Assets], [http://www.mitcdoiq.org/ MIT Chief Data Officer Symposium], Doug Laney, [[Gartner]], 22 July 2015
* [http://www.kpmg.com/US/en/topics/data-analytics/Documents/kpmg-d-a-main-report-for-web-28-june-2015.pdf Data and Analytics: A New Driver of Performance and Valuation], [[Institutional Investor Research]] and [[KPMG]], 28 June 2015
* [http://www.thesummits.org/watch.htm?id=132356411 The Convergence of Information Economics and Economic Information] [[Corp Development Summit]] presentation replay, Doug Laney, [[Gartner]], 1 July 2015
* [http://smartdatacollective.com/rk-paleru/319941/data-opportunity-are-you-monetizing-information Data = Opportunity: But Are You Monetizing Information?] [[Smart Data Collective]], RK Paleru, 28 May 2015
* [http://blogs.gartner.com/doug-laney/keeping-busy-with-data-strategy/ Keeping Busy with Data Strategy], [[Gartner]] Blog Network, Doug Laney, 26 May 2015
* [http://blogs.wsj.com/cio/2015/05/20/dollar-value-of-data-radioshack-other-bankrupt-firms-auction-customer-data-to-pay-debt/ Dollar Value of Data: RadioShack, Other Bankrupt Firms Auction Customer Data to Pay Debt], [[Wall Street Journal]], Kim Nash, 20 May 2015
* [https://www.gartner.com/doc/3024417 The Benefits and Risks of Using Open Data], Doug Laney, [[Gartner]], 8 April 2015
* [http://goodstrat.com/2015/01/30/consider-this-does-all-data-have-value/ Consider this: Does all data have value?] Good Strategy blog, Martyn Jones, 30 January 2015
* [http://www.rsd.com/en/resources/white-papers/theory-infonomics-valuating-corporate-information-assets The Theory of Infonomics: Valuating Corporate Information Assets - white paper], [[RSD (company)|RSD]], January 2015
* [http://www.firstpost.com/business/customer-data-valuable-asset-treat-way-2046119.html Customer data is a valuable asset. Why not treat it that way?], [[F.Business]], Ajay Kelkar, 14 January 2015
* [https://www.youtube.com/watch?v=du4YVpu4VHE The Rise of Data Capital] (video), [[Oracle Corporation|Oracle]], 8 January 2015

2014
* [http://www.cmswire.com/cms/information-management/quantifying-the-value-of-your-data-026674.php Quantifying the Value of Your Data], [[CMS Wire]], Bassam Zarkout, 30 September 2014
* [http://www.rsd.com/en/blog/201409/what-infonomics What is Infonomics?], Ed Hallock, [[RSD (company)|RSD]] blog, 9 September 2014
* [http://cisr.mit.edu/blog/documents/2014/08/21/2014_0801_datamonetization_wixom.pdf/ Cashing In on Your Data], [[MIT Sloan]] Center for Information Systems Research, Barbara H. Wixom, Volume XIV, Number 8, August 2014
* [https://www.gartner.com/doc/2813227 Increase the Return on Your Information Investments With the Information Yield Curve], [[Gartner]], Andrew White and Douglas Laney, 31 July 2014
* [http://www.forbes.com/sites/gartnergroup/2014/07/21/the-hidden-shareholder-boost-from-information-assets/ The Hidden Shareholder Boost from Information Assets], [[Forbes]], Doug Laney, 21 July 2014
* [http://searchcio.bitpipe.com/data/demandEngage.action?resId=1401817861_376 CIO Decisions: The new infonomics reality: Determining the value of data], [[TechTarget]] SearchCIO, June 2014
* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[TechTarget]] SearchCIO, Linda Tucci, 13 May 2014
* [http://searchcio.techtarget.com/feature/Six-ways-to-measure-the-value-of-your-information-assets Six ways to measure the value of your information assets], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014
* [http://searchcio.techtarget.com/feature/Infonomics-treats-data-as-a-business-asset Infonomics treats data as a business asset], [[TechTarget]] SearchCIO, Nicole Laskowski, 13 May 2014
* [http://pv.tl/blog/2014/04/13/the-economics-of-information-management/?utm_content=buffer29c67&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer, The economics of information management], PVTL Blog, Felix Barbalet, 13 April 2014
* [http://www.forbes.com/sites/gartnergroup/2014/03/27/the-hidden-tax-advantage-of-monetizing-your-data/ The Hidden Tax Advantage of Monetizing Your Data], [[Forbes]], Doug Laney, 27 March 2014
* [http://blogs.teradata.com/anz/the-chief-data-officer-managing-the-value-of-data/#comment-2147 The Chief Data Officer – Managing the Value of Data], [[Teradata]] ANZ Blog, Renato Manongdo, March 2014
* [https://www.gartner.com/doc/2677518 How Organizations Can Monetize Customer Data], [[Gartner]], Olive Huang, Doug Laney, 6 March 2014
* [https://www.gartner.com/doc/2677515?ref=QuickSearch&sthkw=infonomics Improving the Value of Customer Data Through Applied Infonomics], [[Gartner]] Research Publication, Douglas Laney, Olive Huang, 6 March 2014
* [http://blogs.gartner.com/andrew_white/2014/02/14/information-value-accrual-and-its-asymmetry/ Information Value Accrual and Its Asymmetry], [[Gartner]] Blog Network, Andrew White, 14 February 2014
* [http://blogs.gartner.com/andrew_white/2014/01/29/does-information-utility-suffer-a-half-life/ Does Information Utility Suffer a Half Life?], [[Gartner]] Blog Network, Andrew White, 29 January 2014

2013
* [http://www.rsd.com/en/blog/201312/what-information-information-governance What is the "Information" in "Information Governance"?], [[RSD (company)|RSD]] Blog, James Amsler, 30 December 2013
* [http://blogs.gartner.com/doug-laney/to-twitter-youre-worth-101-70/ To Twitter You're Worth $101.70] [[Gartner]] Blog Network, by Douglas Laney, 12 November 2013
* [http://www.economistgroup.com/leanback/big-data-2/treat-data-like-money Treat data like money. CMO's Advice: Marketers must develop an investment strategy for data], [[The Economist Group]], Jim Davis, SVP & CMO, SAS, October 2013
* [http://www.ft.com/intl/cms/s/0/205ddf5c-1bf0-11e3-b678-00144feab7de.html#axzz2g8PCOrV3 Infonomics: The New Economics of Information], [[The Financial Times]], Doug Laney, VP Research, Gartner, September 2013
* [https://www.youtube.com/watch?v=mQk_5Q3VJv4 Value of Information], GigaOM presentation by Dave McCrory, SVP at [[Warner Music Group]], July 2013   
* [http://www.bankingtech.com/147432/accounting-for-the-value-of-big-data/ Accounting for the value of (big) data], [[Banking Technology Magazine]], David Bannister, 11 June 2013
* [http://searchcio.techtarget.com/opinion/Putting-a-price-on-information-The-nascent-field-of-infonomics Putting a price on information: The nascent field of infonomics], [[SearchCIO]] Journal, Linda Tucci, May 2013
* On March 19, 2013 the Chicago Chapter of the [[Product Development and Management Association]] (PDMA) held an event titled "Monetizing Data: An Evening with Eight of Chicago's Data Product Management Leaders"<ref>http://www.builtinchicago.org/blog/check-out-ppt-deck-monetizing-data-evening-eight-chicagos-data-product-management-leaders</ref>

2012
* [http://www.informationweek.com/big-data/news/big-data-analytics/whats-your-big-data-worth/240144449 What's Your Big Data Worth], [[InformationWeek]], Ellis Booker, 17 December 2012
* [https://www.gartner.com/doc/2278915 Future of Money: Infonomics Monetizing Value in Big Data Information Assets], Mary Knox, [[Gartner]], 14 December 2012
* [http://www.information-age.com/technology/information-management/2134803/an-introduction-to-infonomics An Introduction to Infonomics], [[InformationAge]], Pete Swabey, 26 November 2012
* [https://www.gartner.com/doc/2186116 The Birth of Infonomics: the New Economics of Information], [[Gartner]] research publication, Douglas Laney, 3 October 2012 (public summary, full text available to Gartner clients)
* [http://blogs.gartner.com/doug-laney/tobins-q-a-evidence-of-informations-real-market-value-2/ Tobin’s Q & A: Evidence of Information’s Real Market Value], [[Gartner]] Blog Network, Douglas Laney, 14 Aug 2012
* [http://www.ft.com/intl/cms/s/0/27476ad4-a6a5-11e1-968b-00144feabdc0.html#axzz1vzOCxVYw Extracting Value from Information], [[Financial Times]], interview with Douglas Laney by Paul Taylor, 25 May 2012 (free registration required)
* [http://www.forbes.com/sites/gartnergroup/2012/05/22/infonomics-the-practice-of-information-economics/ Infonomics: The Practice of Information Economics], [[Forbes]], by Douglas Laney, 22 May 2012
* [http://blogs.wsj.com/cio/2012/05/03/to-facebook-youre-worth-80-95/?mod=wsjcio_hps_cioreport# To Facebook You're Worth $80.95], [[Wall Street Journal]], by Douglas Laney, 3 May 2012
* [https://www.gartner.com/doc/1958016 Introducing Infonomics: Valuing Information as a Corporate Asset], [[Gartner]] research publication, Douglas Laney, 21 March 2012 (public summary, full text available to Gartner clients)
* [http://www.ijikm.org/Volume7/IJIKMv7p177-199Evans0650.pdf Barriers to the Effective Deployment of Information Assets: An Executive Management Perspective], [[Interdisciplinary Journal of Information, Knowledge, and Management]], Nina Evans and James Price, Volume 7, 2012

Older
* [http://imcue.com/wp-content/uploads/2011/06/What-is-EIM.pdf/ What is Enterprise Information Management (EIM)] by John Ladley, Morgan Kaufmann, 2010
* [http://blogs.informatica.com/perspectives/2010/01/26/data-as-an-asset/#comment-1072 Data as an Asset] blog series by John Schmidt, 2010
* [http://www.amazon.com%2FInformation-Driven-Business-Information-Maximum-Advantage%2Fdp%2F0470625775%2Fref%3Dsr_1_1%3Fie%3DUTF8%26s%3Dbooks%26qid%3D1267263302%26sr%3D8-1/ Information Driven Business: How to Manage Data and Information for Maximum Advantage] by Rob Hillard, Wiley 2010
* [http://www.amazon.com%2FHow-Measure-Anything-Intangibles-Business%2Fdp%2F0470539399%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316207185%26sr%3D1-1/ How to Measure Anything: Finding the Value of Intangibles in Business] by Douglas W. Hubbard, Wiley 2010
* [http://www.amazon.com%2FIntangible-Assets-Valuation-Economic-Benefit%2Fdp%2F0471671312%2Fref%3Dsr_1_3%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206576%26sr%3D1-3/ Intangible Assets: Valuation and Economic Benefit] by Jeffrey A. Cohen, Wiley 2005
* [http://www.amazon.com%2FValue-Driven-Intellectual-Capital-Intangible%2Fdp%2F0471351040%2Fref%3Dsr_1_1%3Fs%3Dbooks%26ie%3DUTF8%26qid%3D1316206774%26sr%3D1-1/ Value Driven Intellectual Capital: How to Convert Intangible Corporate Assets Into Market Value] by Patrick H. Sullivan, Wiley, 2000
* [http://www.vldb.org/conf/1998/p641.pdf Bank of America Case Study: The Information Currency Advantage], [[Teradata]], Felipe Carino and Mark Jahnke, Proceedings of the 24th VLDB Conference, New York, NY, 1998
* [http://www.amazon.com/Information-Payoff-Transformation-Work-Electronic/dp/0029317207/ Information Payoff: The Transformation of Work in the Electronic Age] by Paul A. Strassmann, The Free Press, 1985

== See also ==
*[[Infonomics]]
*[[Monetization]]
*[[Business intelligence]]
*[[Analytics]]
*[[Bitcoin]]
*[[Data as a service]]

== References ==
{{Reflist|33em}}

[[Category:Articles created via the Article Wizard]]
[[Category:Data management]]
<=====doc_Id=====>:233
<=====title=====>:
Information governance
<=====text=====>:
{{Governance}}

'''Information governance''', or '''IG''', is the set of multi-disciplinary structures, policies, procedures, processes and controls implemented to manage information at an enterprise level, supporting an organization's immediate and future regulatory, legal, risk, environmental and operational requirements. Information governance should determine the balance point between two potentially divergent organizational goals: extracting value from information and reducing the potential risk of information. Information governance reduces organizational risk in the fields of compliance, operational transparency, and reducing expenditures associated with e-discovery and litigation response. An organization can establish a consistent and logical framework for employees to handle data through their information governance policies and procedures. These policies guide proper behavior regarding how organizations and their employees handle electronically stored information ([[Electronically stored information (Federal Rules of Civil Procedure)|ESI]]).<ref>{{cite web|url=http://blogs.gartner.com/debra_logan/2010/01/11/what-is-information-governance-and-why-is-it-so-hard/|title=What is Information Governance? And Why is it So Hard? - Debra Logan|date=11 January 2010|publisher=}}</ref><ref>[Kooper, M., Maes, R., and Roos Lindgreen, E. (2011). On the governance of information: Introducing a new concept of governance to support the management of information. International Journal of Information Management, 31(3), 195-200]</ref>

Information governance encompasses more than traditional [[records management]].  It incorporates [[information security]] and protection, compliance, [[data governance]], [[electronic discovery]], [[risk management]], privacy, data storage and archiving, [[knowledge management]], business operations and management, audit, analytics, IT management, [[master data management]], [[enterprise architecture]], [[business intelligence]], [[big data]], [[data science]], and finance.<ref>{{cite web|url=http://iginitiative.com/igi-publishes-2014-annual-report/|title=IGI PUBLISHES 2014 ANNUAL REPORT - Information Governance Initiative|date=11 August 2014|publisher=}}</ref>

==History==

===Records management===
Records management deals with the creation, retention and storage and disposition of records.  A record can either be a physical, tangible object, or digital information such as a database, application data, and e-mail.  The [[records life-cycle|lifecycle]] was historically viewed as the point of creation to the eventual disposal of a record.  As data generation exploded in recent decades, and regulations and compliance issues increased, traditional records management failed to keep pace.  A more comprehensive platform for managing records and information became necessary to address all phases of the lifecycle, which led to the advent of information governance.<ref>http://www.arma.org/pdf/WhatIsRIM.pdf</ref>

In 2003 the Department of Health in England introduced the concept of broad-based information governance into the National Health Service, publishing version 1 of an online performance assessment tool with supporting guidance. The NHS IG Toolkit<ref>{{cite web|url=https://www.igt.hscic.gov.uk/|title=Home|publisher=}}</ref> is now used by over 30,000 NHS and partner organisations, supported by an e-learning platform with some 650,000 users.

In 2008, [[ARMA International]] introduced the Generally Accepted Recordkeeping Principles®, or "The Principles"<ref>{{cite web|url=http://www.arma.org/principles|title=Generally Accepted Recordkeeping Principles|publisher=}}</ref> and the subsequent "The Principles" Information Governance Maturity Model.<ref>http://www.arma.org/principles/metrics.cfm</ref> "The Principles" identify the critical hallmarks of information governance. As such, they apply to all sizes of organizations, in all types of industries, and in both the private and public sectors. Multi-national organizations can also use "The Principles" to establish consistent practices across a variety of business units. ARMA International recognized that a clear statement of "Generally Accepted Recordkeeping Principles®" ("The Principles") would guide:

* CEOs in determining how to protect their organizations in the use of information assets;
* Legislators in crafting legislation meant to hold organizations accountable; and
* Records management professionals in designing comprehensive and effective records management programs.

Information governance goes beyond retention and disposition to include privacy, access controls, and other compliance issues.  In electronic discovery, or e-discovery, relevant data in the form of [[electronically stored information]] is searched for by attorneys and placed on [[legal hold]].  IG includes consideration of how this data is held and controlled for e-discovery, and also provides a platform for defensible disposition and compliance.  Additionally, [[metadata]] often accompanies electronically stored data and can be of great value to the enterprise if stored and managed correctly.

With all of these additional considerations that go beyond traditional records management, IG emerged as a platform for organizations to define policies at the enterprise level, across multiple jurisdictions.  IG then also provides for the enforcement of these policies into the various repositories of information, data, and records.

A coalition of organizations known as Electronic Discovery Reference Model (EDRM), which was founded in 2005 to address issues related to electronic discovery and information governance, subsequently developed, as one of its projects, a resource called the Information Governance Reference Model (IGRM).<ref>{{cite web|author=EDRM|url=http://www.edrm.net/what-is-edrm|title=About EDRM|accessdate=2015-01-21}}</ref> In 2011, EDRM, in collaboration with ARMA International, published a white paper that describes ''How the Information Governance Reference Model (IGRM) Complements ARMA International’s Generally Accepted Recordkeeping Principles ("The Principles")''<ref>{{cite book|last=White Paper|title=How the Information Governance Reference Model (IGRM)Complements ARMA International’s Generally Accepted Recordkeeping Principles|year=2011|publisher=EDRM and ARMA International|pages=15|url=http://www.edrm.net/wp-content/uploads/downloads/2011/12/White-Paper-EDRM-Information-Governance-Reference-Model-IGRM-and-ARMAs-GARP-Principles-12-7-2011.pdf|editor-last=Ledergerber|editor-first=Marcus}}</ref>  The IGRM illustrates the relationship between key stakeholders and the Information Lifecycle and highlights the transparency required to enable effective governance IGRM v3.0 Update: Privacy & Security Officers As Stakeholders.<ref>[http://www.edrm.net/download/all_projects/igrm/The-Final..-IGRM_v3.0Update-Whitepaper_Oct_2012.pdf IGRM v3.0 Update: Privacy & Security Officers As Stakeholders]</ref>

Universities and professional associations started to develop information governance training and education programmes. In 2010, Dr Elizabeth Lomas (who had been aligning RM with information security, assurance and risk management models throughout the 2000s) authored distance learning materials for Information Governance modules delivered internationally through Northumbria University. ARMA subsequently started to deliver an Information Governance certification. These initiatives have now been picked up by other Universities, e.g. San Jose State University offers a graduate certificate in information governance, information assurance, and cyber security, and has also incorporated a required course in information governance as part of their 100% online Master of Archives and Records Administration<ref>[http://ischool.sjsu.edu/programs/master-archives-records-administration-mara]]</ref> (MARA) degree program.

In 2014, [[John Wiley & Sons]] published the first textbook on information governance, "Information Governance: Concepts, Strategies, and Best Practices"<ref>{{cite book|url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-1118218302.html|title=Information Governance: Concepts, Strategies, and Best Practices|isbn=978-1-118-21830-3|date=April 2014|publisher=John Wiley & Sons}}</ref> by [[Robert Smallwood]]. Also in 2014, The Information Governance Conference,<ref>Information Governance Conference [http://www.infogovcon.com InfoGovCon.com]</ref> an annual conference on information governance best practices began and the Information Governance Model<ref>Information Governance Model [http://www.infogovmodel.com InfoGovModel.com]</ref> was launched at the inaugural event, it is now in use at over 1000 organizations worldwide.

===Organizational structure===
In the past, records managers owned records management, perhaps within a compliance department at an enterprise.  In order to address the broader issues surrounding records management, several other key stakeholders must be involved.  Legal, IT, and Compliance tend to be the departments that touch information governance the most, though certainly other departments might seek representation.  Many enterprises create information governance committees to ensure that all necessary constituents are represented and that all relevant issues are addressed.<ref>{{cite web|url=http://www.law.com/jsp/cc/PubArticleFriendlyCC.jsp?id=1202533945005|title=From the Experts: Information Governance and Its Impact on Litigation|publisher=}}</ref>

===Tools===
To address retention and disposition, Records Management and Enterprise Content Management applications were developed.  Sometimes detached search engines or homegrown policy definition tools were created.  These were often employed at a departmental or divisional level; rarely were tools used across the enterprise.  While these tools were used to define policies, they lacked the ability to enforce those policies.  Monitoring for compliance with policies was increasingly challenging. Since information governance addresses so much more than traditional records management, several software solutions have emerged to include the vast array of issues facing records managers.

Other available tools include:
* ARMA International [[Www.arma.org/nextlevel|Next Level Information Governance Assessment]] ( Based upon the Generally Accepted Recordkeeping Principles)
* ARMA Generally Accepted Recordkeeping Principles<ref>ARMA International, [http://www.arma.org/r2/generally-accepted-br-recordkeeping-principles "The Principles"], ''ARMA International''</ref>
* EDRM Information Governance Reference Model<ref>EDRM, [http://www.edrm.net/projects/igrm "Information Governance Reference Model"], ''EDRM''</ref>
* Information Coalition Information Governance Model<ref>Information Coalition, [http://infocoalition.com/resources/models-methodologies/information-governance-model-infogovmodel "The Information Governance Model"], ''Information Coalition''</ref>
* NHS Information Governance Toolkit<ref>NHS, [https://www.igt.hscic.gov.uk/ "NHS Information Governance Toolkit"], ''NHS''</ref>

===Laws and regulations===
Key to IG are the regulations and laws that help to define corporate policies.  Some of these regulations include:
*The Foreign Account Tax Compliance Act, or [[Foreign Account Tax Compliance Act|FATCA]]<ref>{{cite web|url=http://www.irs.gov/businesses/corporations/article/0,,id=236667,00.html|title=Foreign Account Tax Compliance Act|publisher=}}</ref>
*Payment Card Industry Data Security Standard, or [[Payment Card Industry Data Security Standard|PCI Compliance]]<ref>{{cite web|url=https://www.pcisecuritystandards.org/|title=Official PCI Security Standards Council Site - Verify PCI Compliance, Download Data Security and Credit Card Security Standards|publisher=}}</ref>
*Health Insurance Portability and Accountability Act, or [[Health Insurance Portability and Accountability Act|HIPAA]]<ref>{{cite web|url=http://www.hhs.gov/hipaa/|title=Health Information Privacy|date=26 August 2015|publisher=}}</ref>
*Financial Services Modernization Act of 1999, or [[Gramm–Leach–Bliley Act|GLBA]]<ref>{{cite web|url=https://www.congress.gov/bill/106th-congress/senate-bill/900|title=S.900 - Gramm-Leach-Bliley Act}}</ref>
*Sarbanes–Oxley Act of 2002, or [[Sarbanes–Oxley|Sarbox or SOX]]<ref>{{cite web|url=https://www.sec.gov/about/laws/soa2002.pdf|title=Sarbanes–Oxley Act of 2002}}</ref>
*[[Federal Rules of Civil Procedure]]

===Guidelines===
*[[MoReq2]]<ref>{{cite web|url=http://www.moreq2.eu/|title=Home - MoReq2|publisher=}}</ref>
*MoReq2010<ref>{{cite web|url=http://moreq2010.eu/|title=Account Suspended|publisher=}}</ref>
*[[ISO 15489 Information and documentation -- Records management|ISO 15489 Information and Documentation - Records Management]]<ref>{{cite web|url=http://www.iso.org/iso/catalogue_detail?csnumber=31908|title=ISO 15489-1:2001 - Information and documentation -- Records management -- Part 1: General|publisher=}}</ref>
*DoD 5015.2, or [[Design Criteria Standard for Electronic Records Management Software Applications]]<ref>{{cite web|url=http://www.archives.gov/records-mgmt/initiatives/dod-standard-5015-2.html|title=DoD Standard 5015.2|publisher=}}</ref>

==See also==
*[[Data defined storage]]
* [[Data governance]]
*[[Electronic discovery]]
*[[Enterprise content management]]
*[[Information management]]
*[[Information technology governance]]
*[[Knowledge management]]
*[[National archives]]
*[[Records management]]

==References==
{{reflist|30em}}

==External links==
* [http://www.epa.gov/records/what/quest1.htm EPA 10 Reasons for RM]
* [http://www.druva.com/resources/analyst-reports/governance-takes-central-role-enterprises-shift-to-mobile/]

[[Category:Information governance|*]]
[[Category:Information technology management]]
[[Category:Content management systems]]
[[Category:Public records]]
[[Category:Data management]]
<=====doc_Id=====>:236
<=====title=====>:
ADO.NET
<=====text=====>:
{{No footnotes|date=March 2009}}
{{Infobox software
|
| operating system       = [[Microsoft Windows]]
| genre                  = [[Software framework]]
| license                = [[Proprietary software]] ([[Base Class Library|BCL]] portion under [[MIT license]]; source code under [[Ms-RSL]])
| website                = {{url|http://msdn2.microsoft.com/en-us/library/aa286484.aspx}}
| Nonprofit              =
}}
'''ADO.NET''' is a data access technology from the [[Microsoft]] [[.NET Framework]] which provides communication between relational and non-relational systems through a common set of components.
ADO.NET is a set of computer software components that programmers can use to access data and data services from the database. It is a part of the [[Base Class Library|base class library]] that is included with the Microsoft .NET Framework. It is commonly used by programmers to access and modify data stored in [[Relational DBMS|relational database systems]], though it can also access data in non-relational sources. ADO.NET is sometimes considered an evolution of [[ActiveX Data Objects]] (ADO) technology, but was changed so extensively that it can be considered an entirely new product.

== Architecture ==
{{Main|ADO.NET data provider}}
[[Image:DotNet3.0.svg|thumb|right|240px|This [[technology]] forms a part of [[.NET Framework 3.0]] (having been part of the framework since version 1.0)]]

ADO.NET is conceptually divided into ''[[ADO.NET consumer|consumers]]'' and ''[[ADO.NET provider|data providers]]''. The consumers are the applications that need access to the data, and the providers are the software components that implement the interface and thereby provide the data to the consumer.

Functionality exists in [[Microsoft Visual Studio|Visual Studio]] IDE to create specialized subclasses of the DataSet classes for a particular database schema, allowing convenient access to each field through strongly typed [[Property (programming)|properties]]. This helps catch more programming errors at compile-time and enhances the IDE's [[Intellisense]] feature.

== O/R Mapping ==
{{main|Object-relational mapping}}

=== Entity Framework ===
{{main|Entity Framework}}

Entity Framework (EF) is an open source object-relational mapping (ORM) framework for ADO.NET, part of .NET Framework. It is a set of technologies in ADO.NET that support the development of data-oriented software applications. Architects and developers of data-oriented applications have typically struggled with the need to achieve two very different objectives. The Entity Framework enables developers to work with data in the form of domain-specific objects and properties, such as customers and customer addresses, without having to concern themselves with the underlying database tables and columns where this data is stored. With the Entity Framework, developers can work at a higher level of abstraction when they deal with data, and can create and maintain data-oriented applications with less code than in traditional applications.

=== LINQ to SQL ===
{{main|LINQ to SQL}}

LINQ to SQL (formerly called DLINQ) allows [[LINQ]] to be used to query Microsoft SQL Server databases, including SQL Server Compact databases. Since SQL Server data may reside on a remote server, and because SQL Server has its own query engine, it does not use the query engine of LINQ. Instead, it converts a LINQ query to a SQL query that is then sent to SQL Server for processing. However, since SQL Server stores the data as relational data and LINQ works with data encapsulated in objects, the two representations must be mapped to one another. For this reason, LINQ to SQL also defines a mapping framework. The mapping is done by defining classes that correspond to the tables in the database, and containing all or a certain subset of the columns in the table as data members.

==See also==
* [[Comparison of ADO and ADO.NET]]

==External links==
;ADO.NET
* [http://msdn2.microsoft.com/en-us/library/aa286484.aspx ADO.NET Overview on MSDN]
* [http://msdn2.microsoft.com/en-us/library/ms973217.aspx ADO.NET for the ADO Programmer]
* [http://www.devlist.com/ConnectionStringsPage.aspx ADO.NET Connection Strings]

{{.NET Framework}}
{{Microsoft APIs}}{{Windows-software-stub}}{{Microsoft-stub}}{{Programming-software-stub}}

{{DEFAULTSORT:Ado.Net}}
[[Category:Data management]]
[[Category:.NET Framework terminology]]
[[Category:Microsoft application programming interfaces]]
[[Category:SQL data access]]
[[Category:ADO.NET Data Access technologies]]
<=====doc_Id=====>:239
<=====title=====>:
Database server
<=====text=====>:
{{refimprove|date=September 2014}}
A '''database server''' is a [[computer program]] that provides [[database]] services to other computer programs or to [[computer]]s, as defined by the [[client–server]] [[software modeling|model]].{{cn|date=January 2017}} The term may also refer to a computer dedicated to running such a program. [[Database management system]]s frequently provide database-server functionality, and some [[database management system]]s (DBMSs) (such as [[MySQL]]) rely exclusively on the client–server model for database access.

Users access a database server either through a "[[Front and back ends|front end]]" running on the user's computer - which displays requested data - or through the "[[Front and back ends|back end]]", which runs on the server and handles tasks such as data analysis and storage.

In a [[Master-slave (technology)|master-slave]] model, database master servers are central and primary locations of data while database slave servers are synchronized backups of the master acting as [[proxy server|proxies]].

Most database servers respond to a [[query language]]. Each database understands its query language and converts each submitted [[query (disambiguation) | query]] to server-readable form and executes it to retrieve results.

Examples of proprietary database servers include [[Oracle Database|Oracle]], [[IBM DB2|DB2]], [[Informix]], and [[Microsoft SQL Server]]. Examples of [[GNU General Public Licence]] database servers include [[Ingres (database)|Ingres]] and [[MySQL]].  Every server uses its own query logic and structure. The [[SQL]] (Structured Query Language) query language is more or less the same on all [[relational database]] servers.

[[DB-Engines]] lists over 200 DBMSs in its ranking.<ref>
{{cite web
|url= http://db-engines.com/en/ranking 
|title= DB-Engines Ranking 
|publisher= DB-Engines.com 
|date= 2013-12-01 
|accessdate= 2013-12-28
}}
</ref>

==History ==
The foundations for modeling large sets of data were first introduced by [[Charles Bachman]] in 1969.<ref name="dbhist">[http://knol.google.com/k/databases-history-early-development# Databases - History & Early Development]</ref> Bachman introduced [[Data structure diagram|Data Structure Diagrams (DSDs)]] as a means to graphically represent data. DSDs provided a means to represent the relationships between different data entities. In 1970, [[Edgar F. Codd|Codd]] introduced the concept that users of a database should be ignorant of the "inner workings" of the database.<ref name="dbhist"/> Codd proposed the "relational view" of data which later evolved into the [[Relational Model]] which most databases use today. In 1971, the Database Task Report Group of [[CODASYL]] (the driving force behind the development of the programming language [[COBOL]]) first proposed a "data description language for describing a database, a data description language for describing that part of the data base known to a program, and a data manipulation language." <ref name="dbhist"/> Most of the research and development of databases focused on the relational model during the 1970s.

In 1975 Bachman demonstrated how the relational model and the data structure set were similar and "congruent" ways of structuring data while working for the [[Honeywell]].<ref name="dbhist"/> The [[Entity-relationship model]] was first proposed in its current form by [[Peter Chen]] in 1976 while he was conducting research at [[MIT]].<ref>[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.123.1085 The Entity-Relationship Model: Toward a Unified View of Data (1976)]</ref> This model became the most frequently used model to describe relational databases. Chen was able to propose a model that was superior to the navigational model and was more applicable to the "real world" than the relational model proposed by Codd.<ref name="dbhist"/>

== References ==
{{Reflist}}

==See also==
* [[Replication (computer science)#Database replication|Database replication]]

{{Database}}

[[Category:Data management]]
[[Category:Servers (computing)]]
[[Category:Databases]]
<=====doc_Id=====>:242
<=====title=====>:
Data warehouse
<=====text=====>:
{{multiple issues|
{{Refimprove|date=February 2008}}
{{Citation style|date=September 2013}}
}}

[[File:Data warehouse overview.JPG|thumb|200px|Data Warehouse Overview]]

In [[computing]], a '''data warehouse''' ('''DW''' or '''DWH'''), also known as an '''enterprise data warehouse''' ('''EDW'''), is a system used for [[Business reporting|reporting]] and [[data analysis]], and is considered a core component of [[business intelligence]].<ref>Dedić, N. and Stanier C., 2016., "An Evaluation of the Challenges of Multilingualism in Data Warehouse Development" in 18th International Conference on Enterprise Information Systems - ICEIS 2016, p. 196.</ref> DWs are central repositories of integrated data from one or more disparate sources. They store current and historical data and are used for creating analytical reports for knowledge workers throughout the enterprise. Examples of reports could range from annual and quarterly comparisons and trends to detailed daily sales analysis.

The data stored in the warehouse is [[upload]]ed from the [[operational system]]s (such as marketing or sales). The data may pass through an [[operational data store]] for additional operations before it is used in the DW for reporting.

==Types of systems==
;[[Data mart]]:  A data mart is a simple form of a data warehouse that is focused on a single subject (or functional area), hence they draw data from a limited number of sources such as sales, finance or marketing. Data marts are often built and controlled by a single department within an organization. The sources could be internal operational systems, a central data warehouse, or external data.<ref>{{cite web |url=http://docs.oracle.com/html/E10312_01/dm_concepts.htm |title=Data Mart Concepts |publisher=Oracle |year=2007}}</ref> Denormalization is the norm for data modeling techniques in this system. Given that data marts generally cover only a subset of the data contained in a data warehouse, they are often easier and faster to implement.

{| class="wikitable"
|+ Difference between data warehouse and {{nowrap|data mart}}
|-
! Data warehouse
! Data mart
|-
| enterprise-wide data
| department-wide data
|-
| multiple subject areas
| single subject area
|-
| difficult to build
| easy to build
|-
| takes more time to build
| less time to build
|-
| larger memory
| limited memory
|}

'''Types of data marts'''
* Dependent data mart
* Independent data mart
* Hybrid data mart
;[[Online analytical processing]] (OLAP): OLAP is characterized by a relatively low volume of transactions. Queries are often very complex and involve aggregations. For OLAP systems, response time is an effectiveness measure. OLAP applications are widely used by [[Data Mining]] techniques. OLAP databases store aggregated, historical data in multi-dimensional schemas (usually star schemas). OLAP systems typically have data latency of a few hours, as opposed to data marts, where latency is expected to be closer to one day. The OLAP approach is used to analyze multidimensional data from multiple sources and perspectives. The three basic operations in OLAP are : Roll-up (Consolidation), Drill-down and Slicing & Dicing.<ref name=dwh>{{cite web |url=https://intellipaat.com/tutorial/data-warehouse-tutorial/ |title=Data Warehousing Tutorial For Beginners |publisher=Intellipaat}}</ref>

;[[Online transaction processing]] (OLTP): OLTP is characterized by a large number of short on-line transactions (INSERT, UPDATE, DELETE). OLTP systems emphasize very fast query processing and maintaining [[data integrity]] in multi-access environments. For OLTP systems, effectiveness is measured by the number of transactions per second. OLTP databases contain detailed and current data. The schema used to store transactional databases is the entity model (usually [[Third normal form|3NF]]).<ref>{{cite web |url=http://datawarehouse4u.info/OLTP-vs-OLAP.html |title=OLTP vs. OLAP |year=2009 |website=Datawarehouse4u.Info |quote=We can divide IT systems into transactional (OLTP) and analytical (OLAP). In general we can assume that OLTP systems provide source data to data warehouses, whereas OLAP systems help to analyze it.}}</ref> Normalization is the norm for data modeling techniques in this system.

;Predictive analysis: Predictive analysis is about [[pattern recognition|finding]] and quantifying hidden patterns in the data using complex mathematical models that can be used to [[prediction|predict]] future outcomes.  Predictive analysis is different from OLAP in that OLAP focuses on historical data analysis and is reactive in nature, while predictive analysis focuses on the future. These systems are also used for CRM ([[customer relationship management]]).

==Software tools==
The typical extract-transform-load ([[Extract, transform, load|ETL]])-based data warehouse uses [[Staging (data)|staging]], [[data integration]], and access layers to house its key functions. The staging layer or staging database stores raw data extracted from each of the disparate source data systems. The integration layer integrates the disparate data sets by transforming the data from the staging layer often storing this transformed data in an [[operational data store]] (ODS) database. The integrated data are then moved to yet another database, often called the data warehouse database, where the data is arranged into hierarchical groups often called dimensions and into facts and aggregate facts. The combination of facts and dimensions is sometimes called a [[star schema]].  The access layer helps users retrieve data.<ref name=IJCA96Patil>{{cite journal |url=http://www.ijcaonline.org/proceedings/icwet/number9/2131-db195 |author1=Patil, Preeti S. |author2=Srikantha Rao |author3=Suryakant B. Patil |title=Optimization of Data Warehousing System: Simplification in Reporting and Analysis |work=IJCA Proceedings on International Conference and workshop on Emerging Trends in Technology (ICWET) |year=2011 |volume=9 |issue=6 |pages=33–37 |publisher=Foundation of Computer Science}}</ref>

This definition of the data warehouse focuses on data storage. The main source of the data is cleaned, transformed, catalogued and made available for use by managers and other business professionals for [[data mining]], [[OLAP|online analytical processing]], [[market research]] and [[decision support]].<ref>Marakas & O'Brien 2009</ref> However, the means to retrieve and analyze data, to [[Extract, transform, load|extract, transform, and load]] data, and to manage the [[data dictionary]] are also considered essential components of a data warehousing system. Many references to data warehousing use this broader context. Thus, an expanded definition for data warehousing includes [[business intelligence tools]], tools to extract, transform, and load data into the repository, and tools to manage and retrieve [[metadata]].

==Benefits==
A data warehouse maintains a copy of information from the source transaction systems. This architectural complexity provides the opportunity to:
* Integrate data from multiple sources into a single database and data model. Mere congregation of data to single database so a single query engine can be used to present data is an ODS.
* Mitigate the problem of database isolation level lock contention in transaction processing systems caused by attempts to run large, long running, analysis queries in transaction processing databases.
* Maintain [[Provenance#Data provenance|data history]], even if the source transaction systems do not.
* Integrate data from multiple source systems, enabling a central view across the enterprise. This benefit is always valuable, but particularly so when the organization has grown by merger.
* Improve [[data quality]], by providing consistent codes and descriptions, flagging or even fixing bad data.
* Present the organization's information consistently.
* Provide a single common data model for all data of interest regardless of the data's source.
* Restructure the data so that it makes sense to the business users.
* Restructure the data so that it delivers excellent query performance, even for complex analytic queries, without impacting the [[operational system]]s.
* Add value to operational business applications, notably [[customer relationship management]] (CRM) systems.
*Make decision–support queries easier to write.
*Optimized data warehouse architectures allow data scientists to organize and disambiguate repetitive data.<ref>{{Cite web|url=https://www.idera.com/resourcecentral/whitepapers/modern-data-architecture|title=Modern Data Architecture {{!}} IDERA|last=|first=|date=|website=www.idera.com|publisher=|access-date=2016-09-18}}</ref>

==Generic environment==

The environment for data warehouses and marts includes the following:

* Source systems that provide data to the warehouse or mart;
* Data integration technology and processes that are needed to prepare the data for use;
* Different architectures for storing data in an organization's data warehouse or data marts;
*Different tools and applications for the variety of users;
*Metadata, data quality, and governance processes must be in place to ensure that the warehouse or mart meets its purposes.

In regards to source systems listed above, Rainer{{clarify|reason=Who is Rainer?|date=December 2014}} states, "A common source for the data in data warehouses is the company's operational databases, which can be relational databases".<ref name=rainer2012>{{cite book|last=Rainer|first=R. Kelly|title=Introduction to Information Systems: Enabling and Transforming Business, 4th Edition (Kindle Edition)|date=2012-05-01|publisher=Wiley|pages=127, 128, 130, 131, 133}}</ref>

Regarding data integration, Rainer states, "It is necessary to extract data from source systems, transform them, and load them into a data mart or warehouse".<ref name=rainer2012/>

Rainer discusses storing data in an organization’s data warehouse or data marts.<ref name=rainer2012 />

Metadata are data about data. "IT personnel need information about data sources; database, table, and column names; refresh schedules; and data usage measures".<ref name=rainer2012 />

Today, the most successful companies are those that can respond quickly and flexibly to market changes and opportunities. A key to this response is the effective and efficient use of data and information by analysts and managers.<ref name=rainer2012 /> A "data warehouse" is a repository of historical data that are organized by subject to support decision makers in the organization.<ref name=rainer2012 /> Once data are stored in a data mart or warehouse, they can be accessed.

==History==
The concept of data warehousing dates back to the late 1980s<ref>{{cite web|url=http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |title=The Story So Far |date=2002-04-15 |accessdate=2008-09-21 |deadurl=yes |archiveurl=https://web.archive.org/web/20080708182105/http://www.computerworld.com/databasetopics/data/story/0,10801,70102,00.html |archivedate=2008-07-08 |df= }}</ref> when IBM researchers Barry Devlin and Paul Murphy developed the "business data warehouse". In essence, the data warehousing concept was intended to provide an architectural model for the flow of data from operational systems to [[decision support system|decision support environments]]. The concept attempted to address the various problems associated with this flow, mainly the high costs associated with it. In the absence of a data warehousing architecture, an enormous amount of redundancy was required to support multiple decision support environments. In larger corporations it was typical for multiple decision support environments to operate independently. Though each environment served different users, they often required much of the same stored data. The process of gathering, cleaning and integrating data from various sources, usually from long-term existing operational systems (usually referred to as [[legacy system]]s), was typically in part replicated for each environment. Moreover, the operational systems were frequently reexamined as new decision support requirements emerged. Often new requirements necessitated gathering, cleaning and integrating new data from "[[data mart]]s" that were tailored for ready access by users.

Key developments in early years of data warehousing were:
* 1960s – [[General Mills]] and [[Dartmouth College]], in a joint research project, develop the terms ''dimensions'' and ''facts''.<ref name="kimball16">Kimball 2002, pg. 16</ref>
* 1970s – [[ACNielsen]] and IRI provide dimensional data marts for retail sales.<ref name="kimball16" />
* 1970s – [[Bill Inmon]] begins to define and discuss the term: Data Warehouse.{{citation needed|date=June 2014}}
* 1975 – [[Sperry Univac]] introduces [[MAPPER]] (MAintain, Prepare, and Produce Executive Reports) is a database management and reporting system that includes the world's first [[Fourth-generation programming language|4GL]]. First platform designed for building Information Centers (a forerunner of contemporary Enterprise [[Data Warehousing]] platforms)
* 1983 – [[Teradata]] introduces a database management system specifically designed for decision support.
* 1984 – [[Metaphor Computer Systems]], founded by [[David Liddle]] and Don Massaro, releases Data Interpretation System (DIS). DIS was a hardware/software package and GUI for business users to create a database management and analytic system.
* 1988 – Barry Devlin and Paul Murphy publish the article ''An architecture for a business and information system'' where they introduce the term "business data warehouse".<ref>{{cite journal|url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5387658|title=An architecture for a business and information system|journal=IBM Systems Journal | doi=10.1147/sj.271.0060|volume=27|pages=60–80}}</ref>
* 1990 – Red Brick Systems, founded by [[Ralph Kimball]], introduces Red Brick Warehouse, a database management system specifically for data warehousing.
* 1991 – Prism Solutions, founded by [[Bill Inmon]], introduces Prism Warehouse Manager, software for developing a data warehouse.
* 1992 – [[Bill Inmon]] publishes the book ''Building the Data Warehouse''.<ref>{{cite book|last=Inmon|first=Bill|title=Building the Data Warehouse|year=1992|publisher=Wiley|isbn=0-471-56960-7}}</ref>
* 1995 – The Data Warehousing Institute, a for-profit organization that promotes data warehousing, is founded.
* 1996 – [[Ralph Kimball]] publishes the book ''The Data Warehouse Toolkit''.<ref name=":0">{{cite book|title=The Data Warehouse Toolkit|last=Kimball|first=Ralph|publisher=Wiley|year=2011|isbn=9780470149775|page=237}}</ref>
* 2012 – [[Bill Inmon]] developed and made public technology known as "textual disambiguation". Textual disambiguation applies context to raw text and reformats the raw text and context into a standard data base format. Once raw text is passed through textual disambiguation, it can easily and efficiently be accessed and analyzed by standard business intelligence technology. Textual disambiguation is accomplished through the execution of textual ETL. Textual disambiguation is useful wherever raw text is found, such as in documents, Hadoop, email, and so forth.

==Information storage==

===Facts===
A fact is a value or measurement, which represents a fact about the managed entity or system.

Facts as reported by the reporting entity are said to be at raw level. E.g. in a mobile telephone system, if a BTS (base transceiver station) received 1,000 requests for traffic channel allocation, it allocates for 820 and rejects the remaining then it would report 3 '''facts''' or measurements to a management system:
* tch_req_total = 1000
* tch_req_success = 820
* tch_req_fail = 180

Facts at the raw level are further aggregated to higher levels in various [[Dimension (data warehouse)|dimensions]] to extract more service or business-relevant information from it. These are called aggregates or summaries or aggregated facts.

For instance, if there are 3 BTSs in a city, then the facts above can be aggregated from the BTS to the city level in the network dimension. For example:

* <math>tch\_req\_success\_city = tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3</math>
* <math>avg\_tch\_req\_success\_city = (tch\_req\_success\_bts1 + tch\_req\_success\_bts2 + tch\_req\_success\_bts3) / 3</math>

===Dimensional versus normalized approach for storage of data===
There are three or more leading approaches to storing data in a data warehouse&nbsp;— the most important approaches are the dimensional approach and the normalized approach.

The dimensional approach refers to [[Ralph Kimball]]’s approach in which it is stated that the data warehouse should be modeled using a Dimensional Model/[[star schema]]. The normalized approach, also called the [[Third normal form|3NF]] model (Third Normal Form) refers to Bill Inmon's approach in which it is stated that the data warehouse should be modeled using an E-R model/normalized model.

In a [[Star schema|dimensional approach]], [[transaction data]] are partitioned into "facts", which are generally numeric transaction data, and "[[dimension (data warehouse)|dimensions]]", which are the reference information that gives context to the facts. For example, a sales transaction can be broken up into facts such as the number of products ordered and the total price paid for the products, and into dimensions such as order date, customer name, product number, order ship-to and bill-to locations, and salesperson responsible for receiving the order.

A key advantage of a dimensional approach is that the data warehouse is easier for the user to understand and to use. Also, the retrieval of data from the data warehouse tends to operate very quickly.<ref name=":0" /> Dimensional structures are easy to understand for business users, because the structure is divided into measurements/facts and context/dimensions. Facts are related to the organization’s business processes and operational system whereas the dimensions surrounding them contain context about the measurement (Kimball, Ralph 2008). Another advantage offered by dimensional model is that it does not involve a relational database every time. Thus, this type of modeling technique is very useful for end-user queries in data warehouse.<ref name=dwh />

The main disadvantages of the dimensional approach are the following:
# In order to maintain the integrity of facts and dimensions, loading the data warehouse with data from different operational systems is complicated.
# It is difficult to modify the data warehouse structure if the organization adopting the dimensional approach changes the way in which it does business.

In the normalized approach, the data in the data warehouse are stored following, to a degree, [[database normalization]] rules. Tables are grouped together by ''subject areas'' that reflect general data categories (e.g., data on customers, products, finance, etc.). The normalized structure divides data into entities, which creates several tables in a relational database. When applied in large enterprises the result is dozens of tables that are linked together by a web of joins. Furthermore, each of the created entities is converted into separate physical tables when the database is implemented (Kimball, Ralph 2008){{Citation needed|date=November 2013}}.
The main advantage of this approach is that it is straightforward to add information into the database. Some disadvantages of this approach are that, because of the number of tables involved, it can be difficult for users to join data from different sources into meaningful information and to access the information without a precise understanding of the sources of data and of the [[data structure]] of the data warehouse.

Both normalized and dimensional models can be represented in entity-relationship diagrams as both contain joined relational tables. The difference between the two models is the degree of normalization (also known as [[Database normalization#Normal forms|Normal Forms]]). These approaches are not mutually exclusive, and there are other approaches. Dimensional approaches can involve normalizing data to a degree (Kimball, Ralph 2008).

In ''Information-Driven Business'',<ref>{{cite book|last=Hillard|first=Robert|title=Information-Driven Business|year=2010|publisher=Wiley|isbn=978-0-470-62577-4}}</ref> Robert Hillard proposes an approach to comparing the two approaches based on the information needs of the business problem. The technique shows that normalized models hold far more information than their dimensional equivalents (even when the same fields are used in both models) but this extra information comes at the cost of usability. The technique measures information quantity in terms of [[Entropy (information theory)|information entropy]] and usability in terms of the Small Worlds data transformation measure.<ref>{{cite web|url=http://mike2.openmethodology.org/wiki/Small_Worlds_Data_Transformation_Measure |title=Information Theory & Business Intelligence Strategy - Small Worlds Data Transformation Measure - MIKE2.0, the open source methodology for Information Development |publisher=Mike2.openmethodology.org |date= |accessdate=2013-06-14}}</ref>

==Design methods==
{{refimprove section|date=July 2015}}

===Bottom-up design===
In the ''bottom-up'' approach, [[data mart]]s are first created to provide reporting and analytical capabilities for specific [[business process]]es. These data marts can then be integrated to create a comprehensive data warehouse. The data warehouse bus architecture is primarily an implementation of "the bus", a collection of [[Dimension (data warehouse)#Types|conformed dimension]]s and [[Facts (data warehouse)#Types|conformed fact]]s, which are dimensions that are shared (in a specific way) between facts in two or more data marts.<ref>{{Cite web|url=http://decisionworks.com/2003/09/the-bottom-up-misnomer/|title=The Bottom-Up Misnomer - DecisionWorks Consulting|website=DecisionWorks Consulting|language=en-US|access-date=2016-03-06}}</ref>

===Top-down design===
The ''top-down'' approach is designed using a normalized enterprise [[data model]]. [[Data element|"Atomic" data]], that is, data at the greatest level of detail, are stored in the data warehouse. Dimensional data marts containing data needed for specific business processes or specific departments are created from the data warehouse.<ref name="ReferenceA">Gartner, Of Data Warehouses, Operational Data Stores, Data Marts and Data Outhouses, Dec 2005</ref>

===Hybrid design===
Data warehouses (DW) often resemble the [[hub and spokes architecture]]. [[Legacy system]]s feeding the warehouse often include [[customer relationship management]] and [[enterprise resource planning]], generating large amounts of data. To consolidate these various data models, and facilitate the [[extract transform load]] process, data warehouses often make use of an [[operational data store]], the information from which is parsed into the actual DW. To reduce data redundancy, larger systems often store the data in a normalized way. Data marts for specific reports can then be built on top of the DW.

The DW database in a hybrid solution is kept on [[third normal form]] to eliminate [[data redundancy]]. A normal relational database, however, is not efficient for business intelligence reports where dimensional modelling is prevalent. Small data marts can shop for data from the consolidated warehouse and use the filtered, specific data for the fact tables and dimensions required. The DW provides a single source of information from which the data marts can read, providing a wide range of business information. The hybrid architecture allows a DW to be replaced with a [[master data management]] solution where operational, not static information could reside.

The [[Data Vault Modeling]] components follow hub and spokes architecture. This modeling style is a hybrid design, consisting of the best practices from both third normal form and [[star schema]]. The Data Vault model is not a true third normal form, and breaks some of its rules, but it is a top-down architecture with a bottom up design. The Data Vault model is geared to be strictly a data warehouse. It is not geared to be end-user accessible, which when built, still requires the use of a data mart or star schema based release area for business purposes.

==Versus operational system==
Operational systems are optimized for preservation of [[data integrity]] and speed of recording of business transactions through use of [[database normalization]] and an [[entity-relationship model]]. Operational system designers generally follow the [[Codd's 12 rules|Codd rules]] of [[database normalization]] in order to ensure data integrity. Codd defined five increasingly stringent rules of normalization. Fully normalized database designs (that is, those satisfying all five Codd rules) often result in information from a business transaction being stored in dozens to hundreds of tables. [[Relational database]]s are efficient at managing the relationships between these tables. The databases have very fast insert/update performance because only a small amount of data in those tables is affected each time a transaction is processed. Finally, in order to improve performance, older data are usually periodically purged from operational systems.

Data warehouses are optimized for analytic access patterns.  Analytic access patterns generally involve selecting specific fields and rarely if ever 'select *' as is more common in operational databases.  Because of these differences in access patterns, operational databases (loosely, OLTP) benefit from the use of a row-oriented DBMS whereas analytics databases (loosely, OLAP) benefit from the use of a [[column-oriented DBMS]].  Unlike operational systems which maintain a snapshot of the business, data warehouses generally maintain an infinite history which is implemented through ETL processes that periodically migrate data from the operational systems over to the data warehouse.

==Evolution in organization use==
These terms refer to the level of sophistication of a data warehouse:

; Offline operational data warehouse: Data warehouses in this stage of evolution are updated on a regular time cycle (usually daily, weekly or monthly) from the operational systems and the data is stored in an integrated reporting-oriented data
; Offline data warehouse: Data warehouses at this stage are updated from data in the operational systems on a regular basis and the data warehouse data are stored in a data structure designed to facilitate reporting.
; On time data warehouse: Online Integrated Data Warehousing represent the real time Data warehouses stage data in the warehouse is updated for every transaction performed on the source data
; Integrated data warehouse: These data warehouses assemble data from different areas of business, so users can look up the information they need across other systems.<ref>{{cite web |url=http://www.tech-faq.com/data-warehouse.html |title=Data Warehouse }}</ref>

==See also==
{{colbegin|3}}
*[[Accounting intelligence]]
*[[Anchor modeling]]
*[[Business intelligence]]
*[[Business intelligence tools]]
*[[Data integration]]
*[[Data mart]]
*[[Data mining]]
*[[Data presentation architecture]]
*[[Data scraping]]
*[[Data warehouse appliance]]
*[[Database management system]]
*[[Decision support system]]
*[[Data vault modeling]]
*[[Executive information system]]
*[[Extract, transform, load]]
*[[Master data management]]
*[[Online analytical processing]]
*[[Online transaction processing]]
*[[Operational data store]]
*[[Semantic warehousing]]
*[[Snowflake schema]]
*[[Software as a service]]
*[[Star schema]]
*[[Slowly changing dimension]]
*[[Data warehouse automation]]
{{colend}}

==References==
{{Reflist|30em|<!--refs=
<ref name=ahsan>
{{Cite journal
|last1=Abdullah
|first1=Ahsan
|title=Analysis of mealybug incidence on the cotton crop using ADSS-OLAP (Online Analytical Processing) tool, Volume 69, Issue 1
|journal= Computers and Electronics in Agriculture
|year=2009
|pages=59–72
|doi=10.1016/j.compag.2009.07.003
|volume=69
}}
</ref>-->
}}

==Further reading==
* Davenport, Thomas H. and Harris, Jeanne G. ''Competing on Analytics: The New Science of Winning'' (2007) Harvard Business School Press. ISBN 978-1-4221-0332-6
* Ganczarski, Joe. ''Data Warehouse Implementations: Critical Implementation Factors Study'' (2009) [[VDM Verlag]] ISBN 3-639-18589-7 ISBN 978-3-639-18589-8
* Kimball, Ralph and Ross, Margy. ''The Data Warehouse Toolkit'' Third Edition (2013) Wiley, ISBN 978-1-118-53080-1
* Linstedt, Graziano, Hultgren. ''The Business of Data Vault Modeling'' Second Edition (2010) Dan linstedt, ISBN 978-1-4357-1914-9
* William Inmon. ''Building the Data Warehouse'' (2005) John Wiley and Sons, ISBN 978-8-1265-0645-3

==External links==
* [http://www.kimballgroup.com/html/articles.html Ralph Kimball articles]
* [http://www.ijcaonline.org/archives/number3/77-172 International Journal of Computer Applications]
* [http://dwreview.com/DW_Overview.html Data Warehouse Introduction]

{{data}}
{{Data warehouse}}

{{Authority control}}

{{DEFAULTSORT:Data Warehouse}}
[[Category:Business intelligence]]
[[Category:Data management]]
[[Category:Data warehousing| ]]
[[Category:Information technology management]]
<=====doc_Id=====>:245
<=====title=====>:
Clone (database)
<=====text=====>:
{{Multiple issues|
{{unreferenced|date=December 2006}}
{{orphan|date=February 2009}}
}}

A '''database clone ''' is a complete and separate copy of a database system that includes the business data , the [[DBMS]] software and any other application  tiers that make up the environment. Cloning is a different kind of operation to [[Data replication|replication]] and [[backup]]s in that the cloned environment is both fully functional and separate in its own right. Additionally the cloned environment may be modified at its inception due to configuration changes or data subsetting.

The cloning refers to the replication of the server in order to have a backup, to upgrade the environment.

{{DEFAULTSORT:Clone (Database)}}
[[Category:Data management]]
[[Category:Databases]]
<=====doc_Id=====>:248
<=====title=====>:
NewSQL
<=====text=====>:
'''NewSQL''' is a class of modern [[relational database management system|relational]] [[database management system]]s that seek to provide the same scalable performance of [[NoSQL]] systems for [[online transaction processing]] (OLTP) read-write workloads while still maintaining the [[ACID]] guarantees of a traditional database system.<ref name="aslett2012">
{{cite web 
| url = http://cs.brown.edu/courses/cs227/archives/2012/papers/newsql/aslett-newsql.pdf
| title = How Will The Database Incumbents Respond To NoSQL And NewSQL?
| first = Matthew
| last = Aslett
| publisher = 451 Group
| publication-date = 2011-04-04
| year = 2011
| accessdate = 2012-07-06
}}
</ref><ref>
{{cite web 
| url = http://cacm.acm.org/blogs/blog-cacm/109710-new-sql-an-alternative-to-nosql-and-old-sql-for-new-oltp-apps/fulltext
| title = NewSQL: An Alternative to NoSQL and Old SQL for New OLTP Apps
| first = Michael
| last = Stonebraker
| publisher = Communications of the ACM Blog
| publication-date = 2011-06-16
| accessdate  = 2012-07-06
}}
</ref><ref name="highscalability">
{{cite web 
| url = http://highscalability.com/blog/2012/9/24/google-spanners-most-surprising-revelation-nosql-is-out-and.html
| title = Google Spanner's Most Surprising Revelation: NoSQL is Out and NewSQL is In
| first = Todd
| last = Hoff
| publication-date = 2012-09-24
| accessdate  = 2012-10-07
}}
</ref>

== History ==
The term was first used by 451 Group analyst Matthew Aslett in a 2011 research paper discussing the rise of new database systems as challengers to established vendors.<ref name="aslett2010" /> Many enterprise systems that handle high-profile data (e.g., financial and order processing systems) also need to be able to scale but are unable to use NoSQL solutions because they cannot give up strong transactional and consistency requirements.<ref name="aslett2010">{{cite web 
| url = http://blogs.the451group.com/information_management/2011/04/06/what-we-talk-about-when-we-talk-about-newsql/
| title = What we talk about when we talk about NewSQL
| first = Matthew
| last = Aslett
| publisher = 451 Group
| publication-date = 2011-04-06
| year = 2010
| accessdate = 2012-10-07
}}</ref><ref>
{{cite web 
| url = http://berlinbuzzwords.de/sessions/keynote-0
| title = Building Spanner
| first = Alex
| last = Lloyd
| publisher = Berlin Buzzwords
| publication-date = 2012-06-05
| year = 2012
| accessdate = 2012-10-07
}}</ref> The only options previously available for these organizations were to either purchase a more powerful single-node machine or develop custom middleware that distributes queries over traditional DBMS nodes. Both approaches are prohibitively expensive and thus are not an option for many. Thus, in this paper, Aslett discusses how NewSQL upstarts are poised to challenge the supremacy of commercial vendors, in particular [[Oracle Database|Oracle]].

== Systems ==
Although NewSQL systems vary greatly in their internal architectures, the two distinguishing features common amongst them is that they all support the [[Relational model|relational data model]] and use [[SQL]] as their primary interface.<ref>{{Cite journal | last1 = Cattell | first1 = R. | title = Scalable SQL and NoSQL data stores | doi = 10.1145/1978915.1978919 | journal = ACM SIGMOD Record | volume = 39 | issue = 4 | pages = 12 | year = 2011 | url = http://cattell.net/datastores/Datastores.pdf| pmid =  | pmc = }}</ref>
The applications targeted by these NewSQL systems are characterized as having a large number of transactions that (1) are short-lived (i.e., no user stalls), (2) touch a small subset of data using index lookups (i.e., no full table scans or large distributed joins), and (3) are repetitive (i.e. executing the same queries with different inputs).<ref>
{{cite conference
| authorlink = Michael Stonebraker
| first = Mike | last = Stonebraker
| title = The end of an architectural era: (it's time for a complete rewrite
| booktitle = VLDB '07: Proceedings of the 33rd international conference on Very large data bases
| location = Vienna, Austria
| year = 2007
| url = http://hstore.cs.brown.edu/papers/hstore-endofera.pdf
| format = PDF |display-authors=etal}}</ref> These NewSQL systems achieve high performance and scalability by eschewing much of the legacy architecture of the original [[IBM System R]] design, such as heavyweight [[Algorithms for Recovery and Isolation Exploiting Semantics|recovery]] or [[concurrency control]] algorithms.<ref>{{Cite journal | last1 = Stonebraker | first1 = M. | last2 = Cattell | first2 = R. | doi = 10.1145/1953122.1953144 | title = 10 rules for scalable performance in 'simple operation' datastores | journal = Communications of the ACM | volume = 54 | issue = 6 | pages = 72 | year = 2011 | pmid =  | pmc = }}</ref> One of the first known NewSQL systems is the [[H-Store]] [[Parallel database|parallel database system]].<ref>
{{cite web 
| url = http://blogs.the451group.com/information_management/2008/03/04/is-h-store-the-future-of-database-management-systems/
| title = Is H-Store the future of database management systems?
| first = Matthew
| last = Aslett
| year = 2008
| publication-date = 2008-03-04
| accessdate  = 2012-07-05
}}
</ref><ref>
{{cite web 
| url = http://www.zdnet.com/blog/btl/h-store-complete-destruction-of-the-old-dbms-order/8055
| title = H-Store: Complete destruction of the old DBMS order?
| first = Larry
| last = Dignan
| year = 2008
| accessdate  = 2012-07-05
}}
</ref>

NewSQL systems can be loosely grouped into three categories:
<ref>
{{cite web 
| url = http://www.linuxforu.com/2012/01/newsql-handle-big-data/
| title = NewSQL - The New Way to Handle Big Data
| first =  Prasanna
| last = Venkatesh
| year = 2012
| publication-date = 2012-01-30
| accessdate  = 2012-10-07
}}
</ref><ref>
{{cite web 
| url = http://www.scalebase.com/the-story-of-newsql/
| title = The NewSQL Market Breakdown
| first = Doron
| last = Levari
| year = 2011
| accessdate  = 2012-04-08
}}
</ref>

=== New architectures ===
The first type of NewSQL systems are completely new database platforms. These are designed to operate in a distributed cluster of [[Shared nothing architecture|shared-nothing]] nodes, in which each node owns a subset of the data. These databases are often written from scratch with a distributed architecture in mind, and include components such as distributed concurrency control, flow control, and distributed query processing. Example systems in this category are [[Google Spanner]], [[Clustrix]], [[VoltDB]], [[MemSQL]], [[Pivotal Labs|Pivotal]]'s GemFire XD, [[SAP HANA]],<ref>{{cite web|title=SAP HANA|url=http://www.sap.com/pc/tech/data-management/software/extreme-transaction-oltp/index.html|publisher=SAP|accessdate=17 July 2014}}</ref> [[NuoDB]], [[TiDB]], and [[Trafodion]].<ref>
{{cite web 
| url = http://www.trafodion.org
| title = Trafodion: Transactional SQL-on-HBase
| year = 2014
}}
</ref>

=== SQL engines ===
The second category are highly optimized [[Database engine|storage engines]] for [[SQL]]. These systems provide the same programming interface as SQL, but scale better than built-in engines, such as [[InnoDB]]. Examples of these new storage engines include [[MySQL Cluster]], [[Infobright]], [[TokuDB]] and the now defunct [[InfiniDB]].

=== Transparent sharding ===
These systems provide a [[Shard (database architecture)|sharding]] [[middleware]] layer to automatically split databases across multiple nodes. [[ScaleBase]] is an example of this type of system.

==See also==
* [[Transaction processing]]
* [[Partition (database)]]

== References ==
{{Reflist|30em}}

{{Databases}}

<!--Categories-->
[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:NewSQL]]
<=====doc_Id=====>:251
<=====title=====>:
Data set (IBM mainframe)
<=====text=====>:
{{about|mainframe computer file|a general meaning in computing field|Data set}}
{{Refimprove|date=September 2014}}

In the context of [[IBM]] [[mainframe computer]]s, a '''data set''' (IBM preferred) or  '''dataset''' is a [[computer file]] having a [[record-oriented file|record organization]]. Use of this term began with [[OS/360]] and is still used by its successors, including the current [[z/OS]]. Documentation for these systems historically preferred this term rather than ''[[computer file|file]]''.

A data set is typically stored on a [[direct access storage device]] (DASD) or [[magnetic tape]], however unit record devices, such as punch card readers, card punch, and line printers can provide input/output (I/O) for a data set (file).<ref>http://publib.boulder.ibm.com/infocenter/zvm/v5r4/index.jsp?topic=/com.ibm.zvm.v54.hcpa7/hcse7b3050.htm</ref>

Data sets are not unstructured streams of [[byte]]s, but rather are organized in various logical record and block structures determined by the <code>DSORG</code> (data set organization), <code>RECFM</code> (record format), and other parameters. These parameters are specified at the time of the data set allocation (creation), for example with [[Job Control Language]] <code>DD</code> statements. Inside a job they are stored in the [[Data Control Block]] (DCB), which is a data structure used to access data sets, for example using [[access method]]s.

==Data set organization==
For OS/360, the DCB's DSORG parameter specifies how the data set is organized. It may be physically sequential ("PS"), indexed sequential ("IS"), partitioned ("PO"), or Direct Access ("DA"). Data sets on tape may only be DSORG=PS. The choice of organization depends on how the data is to be accessed, and in particular, how it is to be updated.

Programmers utilize various [[access method]]s (such as [[Queued Sequential Access Method|QSAM]] or [[VSAM]]) in programs for reading and writing data sets. Access method depends on the given data set organization.

==Record format (RECFM)==
Regardless of organization, the physical structure of each record is essentially the same, and is uniform throughout the data set. This is specified in the DCB <code>RECFM</code> parameter. <code>RECFM=F</code> means that the records are of fixed length, specified via the <code>LRECL</code> parameter, and <code>RECFM=V</code> specifies a variable-length record. V records when stored on media are prefixed by a Record Descriptor Word (RDW) containing the integer length of the record in bytes. With <code>RECFM=FB</code> and <code>RECFM=VB</code>, multiple logical records are grouped together into a single [[Block (data storage)|physical block]] on tape or disk. FB and VB are <code>fixed-blocked</code>, and <code>variable-blocked</code>, respectively. The <code>BLKSIZE</code> parameter specifies the maximum length of the block. <code>RECFM=FBS</code> could be also specified, meaning <code>fixed-blocked standard</code>, meaning all the blocks except the last one were required to be in full <code>BLKSIZE</code> length. <code>RECFM=VBS</code>, or <code>variable-blocked spanned</code>, means a logical record could be spanned across two or more blocks, with flags in the RDW indicating whether a record segment is continued into the next block and/or was continued from the previous one.

This mechanism eliminates the need for using any "delimiter" byte value to separate records. Thus data can be of any type, including binary integers, floating point, or characters, without introducing a false end-of-record condition. The data set is an abstraction of a collection of records, in contrast to files as unstructured streams of bytes.

=={{anchor|Partitioned datasets}}Partitioned data sets==
A '''partitioned data set''' ('''PDS''') is a data set containing multiple ''members'', each of which holds a separate sub-data set, similar to a [[directory (file systems)|directory]] in other types of [[file system]]s. This type of data set is often used to hold executable programs (''load modules''), source program libraries (especially Assembler macro definitions), and [[Job Control Language]]. A PDS may be compared to a [[ZIP (file format)|Zip]] file or [[COM Structured Storage]].

A Partitioned Data Set can only allocate on a single volume with the maximum size of 65535 tracks.

Besides members, a PDS consists also of their directory. Each member can be accessed directly using the directory structure. Once a member is located, the data stored in that member is handled in the same manner as a PS (sequential) data set.

Whenever a member is deleted, the space it occupied is unusable for storing other data. Likewise, if a member is re-written, it is stored in a new spot at the back of the PDS and leaves wasted “dead” space in the middle. The only way to recover “dead” space is to perform frequent file compression, that moves all members to the front of the data space and leaves free usable space at the back.  (Note that in modern parlance, this kind of operation might be called [[defragmentation]] or [[garbage collection (computer science)|garbage collection]]; [[data compression]] nowadays refers to a different, more complicated concept.)  PDS files can only reside on disk in order to use the directory structure to access individual members, not on tape. They are most often used for storing multiple JCL files, utility control statements and executable modules.

An improvement of this scheme is a Partitioned Data Set Extended (PDSE or PDS/E, sometimes just ''libraries'') introduced with [[MVS/XA]] system.

PDS/E structure is similar to PDS and is used to store the same types of data. However, PDS/E files have a better directory structure which does not require pre-allocation of directory blocks when the PDS/E is defined (and therefore does not run out of directory blocks if not enough were specified). Also, PDS/E automatically stores members in such a way that compression operation is not needed to reclaim "dead" space. PDS/E files can only reside on disk in order to use the directory structure to access individual members.

==See also==
* [[Volume table of contents]] (VTOC), a structure describing data sets stored on the disk
* [[Distributed Data Management Architecture]]

==References==
{{Reflist}}
* [http://publib-b.boulder.ibm.com/Redbooks.nsf/RedbookAbstracts/sg246366.html Introduction to the New Mainframe: z/OS Basics], Ch. 5, "Working with data sets", March 29, 2011. ISBN 0738435341

{{Mainframe I/O access methods}}

{{DEFAULTSORT:Data Set (IBM Mainframe)}}
[[Category:Data management]]
[[Category:IBM mainframe operating systems]]
[[Category:Computer file systems]]
[[Category:Computer files]]
<=====doc_Id=====>:254
<=====title=====>:
Data flow diagram
<=====text=====>:
{{context|date=July 2014}}
[[File:Data Flow Diagram Example.jpg|thumb|360px|Data flow diagram example.<ref>John Azzolini (2000). [http://ses.gsfc.nasa.gov/ses_data_2000/000712_Azzolini.ppt Introduction to Systems Engineering Practices]. July 2001.</ref>]]

A '''data flow diagram''' ('''DFD''') is a graphical representation of the "flow" of data through an [[information system]], modelling its ''process'' aspects. A DFD is often used as a preliminary step to create an overview of the system, which can later be elaborated.<ref>Bruza, P. D., Van der Weide, Th. P., "The Semantics of Data Flow Diagrams", University of Nijmegen, 1993.</ref> DFDs can also be used for the [[Data visualization|visualization]] of [[data processing]] (structured design).

A DFD shows what kind of information will be input to and output from the system, where the data will come from and go to, and where the data will be stored. It does not show information about the timing of process or information about whether processes will operate in sequence or in parallel (which is shown on a [[flowchart]]).

==History==
[[Larry Constantine]], the original developer of structured design,<ref>W. Stevens, G. Myers, L. Constantine, [http://domino.watson.ibm.com/tchjr/journalindex.nsf/d9f0a910ab8b637485256bc80066a393/a801ae3750be70ac85256bfa00685ded!OpenDocument "Structured Design"], IBM Systems Journal, 13 (2), 115-139, 1974.</ref> based on Martin and Estrin's "Data Flow Graph"  model of computation.

Starting in the 1970s, data flow diagrams (DFD) became a popular way to visualize the major steps and data involved in software system processes. DFDs were usually used to show data flow in a computer system, although they could in theory be applied to [[business process modeling]]. DFD were useful to document the major data flows or to explore a new high-level design in terms of data flow.<ref>Craig Larman, "Applying UML and Patterns", Pearson Education, ISBN 978-81-7758-979-5</ref>

==Theory==
[[File:DataFlowDiagram Example.png|thumb|360px|Data flow diagram example]]
[[File:Data-flow-diagram-notation.svg|thumb|160px|Data flow diagram - [[Edward Yourdon|Yourdon]]/[[Tom DeMarco|DeMarco]] notation]]

Data flow diagrams are also known as bubble charts.<ref>[http://www.orm.net/pdf/jcm13.pdf Introduced by Clive Finkelstein in Australia,  CACI in the UK, and later writers such as James Martin]</ref> DFD is a designing tool used in the top-down approach to Systems Design. This context-level DFD is next "exploded", to produce a Level 1 DFD that shows some of the detail of the system being modeled. The Level 1 DFD shows how the system is divided into sub-systems (processes), each of which deals with one or more of the data flows to or from an external agent, and which together provide all of the functionality of the system as a whole. It also identifies internal data stores that must be present in order for the system to do its job, and shows the flow of data between the various parts of the system.

Data flow diagrams are one of the three essential perspectives of the structured-systems analysis and design method [[SSADM]]. The sponsor of a project and the end users will need to be briefed and consulted throughout all stages of a system's evolution.  With a data flow diagram, users are able to visualize how the system will operate, what the system will accomplish, and how the system will be implemented.  The old system's dataflow diagrams can be drawn up and compared with the new system's data flow diagrams to draw comparisons to implement a more efficient system. Data flow diagrams can be used to provide the end user with a physical idea of where the data they input ultimately has an effect upon the structure of the whole system from order to dispatch to report. How any system is developed can be determined through a data flow diagram model. 

In the course of developing a set of ''levelled'' data flow diagrams the analyst/designer is forced to address how the system may be decomposed into component sub-systems, and to identify the [[transaction data]] in the [[data model]].

Data flow diagrams can be used in both Analysis and Design phase of the [[Systems development life cycle|SDLC]].

There are  different notations to draw data flow diagrams (Yourdon & Coad and [[Chris Gane (computer scientist)|Gane]] & [[Trish Sarson|Sarson]]<ref>[[Chris Gane (computer scientist)|Chris Gane]] and [[Trish Sarson]]. ''Structured Systems Analysis: Tools and Techniques.'' McDonnell Douglas Systems Integration Company, 1977</ref>), defining different visual representations for processes, data stores, data flow, and external entities.<ref>[http://www.smartdraw.com/tutorials/software/dfd/tutorial_01.htm How to draw Data Flow Diagrams]</ref>

===Physical vs. logical DFD=== 
A logical DFD captures the data flows that are necessary for a system to operate. It describes the processes that are undertaken, the data required and produced by each process, and the stores needed to hold the data. On the other hand, a physical DFD shows how the system is actually implemented, either at the moment (Current Physical DFD), or how the designer intends it to be in the future (Required Physical DFD). Thus, a Physical DFD may be used to describe the set of data items that appear on each piece of paper that move around an office, and the fact that a particular set of pieces of paper are stored together in a filing cabinet. It is quite possible that a Physical DFD will include references to data that are duplicated, or redundant, and that the data stores, if implemented as a set of [[database table]]s, would constitute an un-normalised (or de-normalised) relational database. In contrast, a Logical DFD attempts to capture the data flow aspects of a system in a form that has neither redundancy nor duplication.

==See also==
* [[Activity diagram]]
* [[Business Process Model and Notation]]
* [[Control flow diagram]]
* [[Data island]]
* [[Dataflow]]
* [[Directed acyclic graph]]
* [[DRAKON|Drakon-chart]]
* [[Functional flow block diagram]]
* [[Function model]]
* [[IDEF0]]
* [[Logical Data Flow]]
* [[Pipeline (software)|Pipeline]]
* [[Structured Analysis and Design Technique]]
* [[Structure chart]]
* [[System context diagram]]
* [[Value stream mapping]]
* [[Workflow]]

==References==
{{Reflist}}

==Further reading==
*[[Scott W. Ambler]]. [http://www.agilemodeling.com/artifacts/dataFlowDiagram.htm The Object Primer 3rd Edition Agile Model Driven Development with UML 2]

==External links==
*{{Commons-inline}}

{{Data model}}
{{Authority control}}

{{DEFAULTSORT:Data Flow Diagram1}}
[[Category:Information systems]]
[[Category:Data management]]
[[Category:Diagrams]]
[[Category:Visualization (graphic)]]
[[Category:Systems analysis]]
<=====doc_Id=====>:257
<=====title=====>:
XML database
<=====text=====>:
{{multiple issues|
{{refimprove|date=August 2011}}
{{update|date=March 2015}}
}}

An '''XML database''' is a [[data persistence]] software system that allows data to be specified, and sometimes stored, in [[XML]] format. This data can be [[XQuery|queried]], transformed, exported and returned to a calling system. XML databases are a flavor of [[document-oriented database]]s which are in turn a category of [[NoSQL]] database.

== Rationale for XML in databases ==
There are a number of reasons to directly specify data in XML or other document formats such as JSON. For XML in particular, they include:<ref name=nicola2010>{{cite web|last1=Nicola|first1=Matthias|title=5 Reasons for Storing XML in a Database|url=http://nativexmldatabase.com/2010/09/28/5-reasons-for-storing-xml-in-a-database/|website=Native XML Database|accessdate=17 March 2015|date=28 September 2010}}</ref>
<ref name=feldman2013>{{cite conference|last1=Feldman|first1=Damon|title=Moving from Relational Modeling to XML and MarkLogic Data Models|url=http://www.marklogic.com/resources/slides-moving-from-relational-modeling-to-xml-and-marklogic-data-models/resource_download/presentations/|conference=MarkLogic World|conferenceurl=http://world.marklogic.com/|date=11 April 2013|accessdate=17 March 2015}}</ref>
* An enterprise may have a lot of XML in an existing standard format
* Data may need to be exposed or ingested as XML, so using another format such as relational forces double-modeling of the data
* XML is very well suited to sparse data, deeply nested data and mixed content (such as text with embedded markup tags)
* XML is human readable whereas relational tables require expertise to access
* Metadata is often available as XML
* Semantic web data is available as RDF/XML

Steve O'Connell gives one reason for the use of XML in databases: the increasingly common use of XML for [[transport layer|data transport]], which has meant that "data is extracted from databases and put into XML documents and vice-versa".<ref name=oconnell2005>{{cite report|author=O'Connell, Steve|work=Advanced Databases Course Notes|title="Section 9.2"|type=Syllabus|date=2005|publisher=[[University of Southampton]]|location=Southampton, England}}</ref>{{update inline|date=March 2015}} It may prove more efficient (in terms of conversion costs) and easier to store the data in XML format.  In content-based applications, the ability of the native XML database also minimizes the need for extraction or entry of metadata to support searching and navigation.

== XML Enabled databases ==
XML enabled databases typically offer one or more of the following approaches to storing XML within the traditional relational structure:
#XML is stored into a CLOB ([[Character large object]])
#XML is `shredded` into a series of Tables based on a Schema<ref name=oracle>{{cite book|title=Oracle XML DB Developer's Guide, 10''g'' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm|accessdate=17 March 2015|chapter=XML Schema Storage and Query: Basic}}. Section [http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb05sto.htm#i1042421 Creating XMLType Tables and Columns Based on XML Schema]</ref>
#XML is stored into a native XML Type as defined by ISO Standard 9075-14<ref name=iso9075-2011>{{cite web|title=ISO/IEC 9075-14:2011: Information technology -- Database languages -- SQL -- Part 14: XML-Related Specifications (SQL/XML)|url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?csnumber=53686|publisher=[[International Organization for Standardization]]|accessdate=17 March 2015|date=2011}}</ref>

RDBMS that support the ISO XML Type are:
#IBM DB2 (pureXML<ref name=db2purexml>{{cite web|title=pureXML overview -- DB2 as an XML database|url=http://www-01.ibm.com/support/knowledgecenter/SSEPGG_10.1.0/com.ibm.db2.luw.xml.doc/doc/c0022308.html|website=IBM Knowledge Center|publisher=[[IBM]]|accessdate=17 March 2015}}</ref>)
#Microsoft SQL Server<ref name=sqlserver2005>{{cite web|title=Using XML in SQL Server|url=https://msdn.microsoft.com/en-us/library/ms190936.aspx|website=Microsoft Developer Network|publisher=[[Microsoft Corporation]]|accessdate=17 March 2015}}</ref>
#Oracle Database<ref name=oracle2>{{cite book|title=Oracle XML DB Developer's Guide, 10''g'' Release 2|date=August 2005|publisher=Oracle Corporation|chapter-url=http://docs.oracle.com/cd/B19306_01/appdev.102/b14259/xdb04cre.htm|accessdate=17 March 2015|chapter=XMLType Operations}}</ref>
#PostgreSQL<ref name=postgresql>{{cite book|title=PostgreSQL 9.0.19 Documentation|chapter-url=http://www.postgresql.org/docs/9.0/static/datatype-xml.html|accessdate=17 March 2015|chapter=8.13. XML Type}}</ref> <ref>[http://www.postgresql.org/docs/9.0/static/datatype-xml.html PostgreSQL - Data Types - XML Type]</ref>

Typically an XML enabled database is best suited where the majority of data are non-XML. For datasets where the majority of data are XML, a [[#Native XML databases|native XML database]] is better suited.

=== Example of XML Type Query in IBM DB2 SQL ===
<source lang="sql">
select
   id, vol, xmlquery('$j/name', passing journal as "j") as name
from
   journals
where 
   xmlexists('$j[licence="CreativeCommons"]', passing journal as "j")
</source>

== Native XML databases ==
These databases are typically better when much of the data is in XML or other non-relational formats.{{fact|date=August 2015}}
* [[BaseX]]
* [[Berkeley DB]] XML Edition 
* [[eXist]]
* [[MarkLogic Server]]
* [[Qizx]]
* [[Sedna (database)|Sedna]]

All the above databases uses XML as an interface to specify documents as tree structured data that may contain unstructured text, but on disk the data is stored as "optimized binary files." This makes query and retrieval faster. For MarkLogic it also allows XML and JSON to co-exist in one binary format.<ref>{{cite book|last1=Siegel|first1=Erik|last2=Retter|first2=Adam|title=eXist|date=December 2014|publisher=O'Reilly & Associates|isbn=978-1-4493-3710-0|url=https://www.safaribooksonline.com/library/view/exist/9781449337094/ch04.html|accessdate=18 March 2015|chapter=4. Architecture}}</ref>

Key features of native XML databases include:

* Has an [[XML]] document as at least one fundamental unit of (logical) storage, just as a [[relational database]] has a row in a table as a fundamental unit of (logical) storage.
* Need not have any particular underlying physical storage model. For example, NXDs can use optimized, proprietary storage formats. This is a key aspect of XML databases. Managing XML as large strings is inefficient due to the extra markup in XML. Compressing and indexing XML allows the illusion of directly accessing, querying and transforming XML while gaining the performance advantages of working with optimized binary tree structures.<ref name=kellogg2010>{{cite web|last1=Kellogg|first1=Dave|title=Yes, Virginia, MarkLogic is a NoSQL System|url=http://kellblog.com/2010/04/11/yes-virginia-marklogic-is-a-nosql-system/|website=Kellblog|accessdate=18 March 2015|date=11 April 2010}}</ref>

The standards for XML querying per W3C recommendation are [[XQuery]] 1.0 and XQuery 3.0.{{citation needed|date=March 2015}} XQuery includes [[XPath]] as a sub-language and XML itself is a valid sub-syntax of XQuery.

In addition to XPath, XML databases support [[XSLT]] as a method of transforming documents or query-results retrieved from the database. XSLT provides a [[declarative language]] written using an XML grammar. It aims to define a set of XPath [[Filter (software)|filter]]s that can transform documents (in part or in whole) into other formats including [[plain text]], XML, or [[HTML]].

But big picture, XML persistence describes only one format in the larger, faster moving [[NoSQL]] movement at this time. Many databases support XML plus other formats, even if XML is internally stored as an optimized, high-performance format and is a first-class citizen within the database. (see Google Trends Link above to see relative popularity of terms).

=== Language features  ===
{| class="wikitable sortable"
|-
! Name
! License
! Native Language
! XQuery 3.0
! XQuery Update
! XQuery Full Text
! EXPath Extensions
! EXQuery Extensions
! XSLT 2.0
|-
| BaseX
| [[BSD License]]
| Java
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| eXist
| [[LGPL|LGPL License]]
| Java
| {{Partial}} || {{Proprietary}} || {{Proprietary}} ||  {{No}} || {{Yes}} || {{Yes}}
|-
| MarkLogic Server
| [[Commercial software|Commercial]]
| C++
| {{Partial}} ||  {{Proprietary}} || {{Proprietary}} || {{No}}  || {{No}} || {{Yes}}
|-
| Qizx
| [[Commercial software|Commercial]]
| Java
| {{Yes}} || {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{Yes}}
|}

=== Supported APIs ===
{| class="wikitable sortable"
|-
! Name
! [[XQuery API for Java|XQJ]]
! XML:DB
! [[Representational State Transfer|RESTful]]
! RESTXQ
! WebDAV
|-
| BaseX
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| eXist
| {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| MarkLogic Server
| {{Yes}} || {{No}} || {{Yes}} || {{Yes}} || {{Yes}}
|-
| Qizx
| {{No}} || {{No}} || {{Yes}} || {{No}} || {{No}}
|-
| Sedna
| {{Yes}} || {{Yes}} || {{No}} || {{No}} || {{No}}
|}

== References ==
{{reflist}}
{{Refbegin}}

== External links ==
* [https://web.archive.org/web/20150906171257/http://db-engines.com/en/ranking/native+xml+dbms DB-Engines Ranking of Native XML DBMS] by popularity, updated monthly
* [http://www.cfoster.net/articles/xmldb-business-case XML Databases - The Business Case, Charles Foster, June 2008] - Talks about the current state of Databases and data persistence, how the current Relational Database model is starting to crack at the seams and gives an insight into a strong alternative for today's requirements.
* [http://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-3717 An XML-based Database of Molecular Pathways (2005-06-02)] Speed / Performance comparisons of eXist, X-Hive, Sedna and Qizx/open
* [https://web.archive.org/web/20070922082133/http://swing.felk.cvut.cz/index.php?option=com_docman&task=doc_view&gid=5&Itemid=62 XML Native Database Systems: Review of Sedna, Ozone, NeoCoreXMS] 2006
* [http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&toc=comp/mags/ic/2005/02/w2toc.xml&DOI=10.1109/MIC.2005.48 XML Data Stores: Emerging Practices]
* Bhargava, P.; Rajamani, H.; Thaker, S.; Agarwal, A. (2005) ''XML Enabled Relational Databases'', Texas, The University of Texas at Austin.
* [https://web.archive.org/web/20070113224941/http://xmldb-org.sourceforge.net/ Initiative for XML Databases]
* [http://www.rpbourret.com/xml/XMLAndDatabases.htm  XML and Databases, Ronald Bourret, September 2005]
* [https://web.archive.org/web/20071011101718/http://cafe.elharo.com/xml/the-state-of-native-xml-databases/  The State of Native XML Databases, Elliotte Rusty Harold, August 13, 2007]
* {{Official website|https://www.qualcomm.com/qizx|Qualcomm Qizx official website}}{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
{{Refend}}

{{Database models}}
{{Databases}}

[[Category:XML]]
[[Category:Data management]]
[[Category:Data modeling]]
[[Category:XML databases| ]]
<=====doc_Id=====>:260
<=====title=====>:
Clustered file system
<=====text=====>:
{{distinguish|Data cluster}}
{{redirect2|Network filesystem|Parallel file system|the Sun NFS protocol|Network File System|the IBM GPFS protocol|IBM General Parallel File System}}
{{multiple|
{{refimprove|date=December 2015}}
{{cleanup|date=December 2013|reason=Merges need to be smoothed over}}
}}

A '''clustered file system''' is a [[file system]] which is shared by being simultaneously [[Mount (computing)|mounted]] on multiple [[Server (computing)|servers]].  There are several approaches to [[computer cluster|clustering]], most of which do not employ a clustered file system (only [[direct attached storage]] for each node).  Clustered file systems can provide features like location-independent addressing and redundancy which improve reliability or reduce the complexity of the other parts of the cluster.  '''Parallel file systems''' are a type of clustered file system that spread data across multiple storage nodes, usually for redundancy or performance.<ref>http://www.dell.com/downloads/global/power/ps2q05-20040179-Saify-OE.pdf</ref>

== {{Anchor|SHARED-DISK}}Shared-disk file system ==
A '''shared-disk file system''' uses a [[storage area network|storage-area network]] (SAN) to allow multiple computers to gain  direct disk access at the [[Block (data storage)|block level]].  Access control and translation from file-level operations that applications use to block-level operations used by the SAN must take place on the client node.  The most common type of clustered file system, the shared-disk file system &mdash;by adding mechanisms for [[concurrency control]]&mdash;provides a consistent and [[serialization|serializable]] view of the file system, avoiding corruption and unintended [[data loss]] even when multiple clients try to access the same files at the same time. Shared-disk file-systems commonly employ some sort of [[Fencing (computing)|fencing]] mechanism to prevent data corruption in case of node failures, because an unfenced device can cause data corruption if it loses communication with its sister nodes and tries to access the same information other nodes are accessing.

The underlying storage area network may use any of a number of block-level protocols, including [[SCSI]], [[iSCSI]], [[HyperSCSI]], [[ATA over Ethernet]] (AoE), [[Fibre Channel]], [[network block device]], and [[InfiniBand]].

There are different architectural approaches to a shared-disk filesystem. Some distribute file information across all the servers in a cluster (fully distributed). Others utilize a centralized [[metadata]] server. Both achieve the same result of enabling all servers to access all the data on a shared storage device.{{Citation needed|date=December 2009}}

=== Examples ===
{{Div col||25em}}
* [[Blue Whale Clustered file system]] (BWFS)
* [[Silicon Graphics]] (SGI) clustered file system ([[CXFS]])
* [[Veritas Cluster File System]]
* DataPlow [[Nasan]] File System
* [[IBM General Parallel File System]] (GPFS)
* [[LizardFS]]
* [[Lustre (file system)|Lustre]]
* Microsoft [[Cluster Shared Volumes]] (CSV)
* [[OCFS|Oracle Cluster File System]] (OCFS)
* PolyServe storage solutions
* [[Quantum Corporation|Quantum]] [[StorNext File System|StorNext]] File System (SNFS), ex ADIC, ex CentraVision File System (CVFS)
* Red Hat [[Global File System]] (GFS)
* Sun [[QFS]]
* TerraScale Technologies TerraFS
* Versity VSM
* [[VMware VMFS]]
* [[Xsan]]
{{Div col end}}

=={{Anchor|DISTRIBUTED-FS}}Distributed file systems==
''Distributed file systems'' do not share block level access to the same storage but use a network [[protocol (computing)|protocol]].<ref>Silberschatz, Galvin (1994). ''Operating System concepts'', chapter 17 ''Distributed file systems''. Addison-Wesley Publishing Company. ISBN 0-201-59292-4.</ref><ref name="ostep-1">{{citation|title=Sun's Network File System|url=http://pages.cs.wisc.edu/~remzi/OSTEP/dist-nfs.pdf|publisher= Arpaci-Dusseau Books|date = 2014|first1 = Remzi H.|last1 =Arpaci-Dusseau|first2=Andrea C.|last2 = Arpaci-Dusseau}}</ref>  These are commonly known as ''network file systems'', even though they are not the only file systems that use the network to send data.{{citation needed|date=March 2013}}  Distributed file systems can restrict access to the file system depending on [[access list]]s or [[Capability-based security|capabilities]] on both the servers and the clients, depending on how the protocol is designed.

The difference between a distributed file system and a [[distributed data store]] is that a distributed file system allows files to be accessed using the same interfaces and semantics as local files{{snd}} for example, mounting/unmounting, listing directories, read/write at byte boundaries, system's native permission model.  Distributed data stores, by contrast, require using a different API or library and have different semantics (most often those of a database).{{cn|date=June 2016}}

A distributed file system may also be created by software implementing IBM's [[Distributed Data Management Architecture]] (DDM), in which programs running on one computer use local interfaces and semantics to create, manage and access files located on other networked computers.  All such client requests are trapped and converted to equivalent messages defined by the DDM. Using protocols also defined by the DDM, these messages are transmitted to the specified remote computer on which a DDM server program interprets the messages and uses the file system interfaces of that computer to locate and interact with the specified file.

===Design goals===
Distributed file systems may aim for "transparency" in a number of aspects.  That is, they aim to be "invisible" to client programs, which "see" a system which is similar to a local file system.  Behind the scenes, the distributed file system handles locating files, transporting data, and potentially providing other features listed below.

* ''Access transparency'' is that clients are unaware that files are distributed and can access them in the same way as local files are accessed.
* ''Location transparency''; a consistent name space exists encompassing local as well as remote files. The name of a file does not give its location.
* ''Concurrency transparency''; all clients have the same view of the state of the file system. This means that if one process is modifying a file, any other processes on the same system or remote systems that are accessing the files will see the modifications in a coherent manner.
* ''Failure transparency''; the client and client programs should operate correctly after a server failure.
* ''Heterogeneity''; file service should be provided across different hardware and operating system platforms.
* ''Scalability''; the file system should work well in small environments (1 machine, a dozen machines) and also scale gracefully to huge ones (hundreds through tens of thousands of systems).
* ''Replication transparency''; to support scalability, we may wish to replicate files across multiple servers. Clients should be unaware of this.
* ''Migration transparency''; files should be able to move around without the client's knowledge.

===History===
The [[Incompatible Timesharing System]] used virtual devices for transparent inter-machine file system access in the 1960s.  More file servers were developed in the 1970s. In 1976 [[Digital Equipment Corporation]] created the [[File Access Listener]] (FAL), an implementation of the [[Data Access Protocol]] as part of [[DECnet]] Phase II which became the first widely used network file system. In 1985 [[Sun Microsystems]] created the file system called "[[Network File System (protocol)|Network File System]]" (NFS) which became the first widely used [[Internet Protocol]] based network file system.<ref name="ostep-1" />  Other notable network file systems are [[Andrew File System]] (AFS), [[Apple Filing Protocol]] (AFP), [[NetWare Core Protocol]] (NCP), and [[Server Message Block]] (SMB) which is also known as Common Internet File System (CIFS).

In 1986, [[IBM]] announced client and server support for Distributed Data Management Architecture (DDM) for the [[System/36]], [[System/38]], and IBM mainframe computers running [[CICS]].  This was followed by the support for [[IBM Personal Computer]], [[AS/400]], IBM mainframe computers under the [[MVS]] and [[VSE (operating system)|VSE]] operating systems, and [[FlexOS]].   DDM also became the foundation for [[DRDA|Distributed Relational Database Architecture]], also known as DRDA.

=== Examples ===
{{Main|List of file systems#Distributed file systems|l1 = List of distributed file systems}}
{{Div col||25em}}
* [[BeeGFS]] (Fraunhofer)
* [[Ceph (software)|Ceph]] (Inktank, Red Hat, SUSE)
* [[Distributed File System (Microsoft)|Windows Distributed File System (DFS)]] (Microsoft)
* [[Infinit (file system)|Infinit]]
* [[Gfarm file system|GfarmFS]]
* [[GlusterFS]] (Red Hat)
* [[Google file system|GFS]] (Google Inc.)
* [[Hadoop distributed file system|HDFS]] (Apache Software Foundation)
* [[InterPlanetary File System|IPFS]]
* iRODS
* [[LizardFS]] (Skytechnology)
* [[Moose File System|MooseFS]] (Core Technology / Gemius)
* [[ObjectiveFS]]
* [[OneFS]] (EMC Isilon)
* OpenIO
* [[OrangeFS]] (Clemson University, Omnibond Systems), formerly [[Parallel Virtual File System]]
* [[Panasas|Panfs]] (Panasas)
* [[Parallel Virtual File System]] (Clemson University, Argonne National Laboratory, Ohio Supercomputer Center)
* [[RozoFS]] (Rozo Systems)
* Torus (CoreOS)
* [[XtreemFS]]
{{Div col end}}

==Network-attached storage==
{{Main|Network-attached storage}}

Network-attached storage (NAS) provides both storage and a file system, like a shared disk file system on top of a storage area network (SAN).  NAS typically uses file-based protocols (as opposed to block-based protocols a SAN would use) such as [[Network File System (protocol)|NFS]] (popular on [[UNIX]] systems), SMB/CIFS ([[Server Message Block|Server Message Block/Common Internet File System]]) (used with MS Windows systems), [[Apple Filing Protocol|AFP]] (used with [[Macintosh|Apple Macintosh]] computers), or NCP (used with [[Novell Open Enterprise Server|OES]] and [[NetWare|Novell NetWare]]).

==Design considerations==

===Avoiding single point of failure===

The failure of disk hardware or a given storage node in a cluster can create a [[single point of failure]] that can result in [[data loss]] or unavailability.  [[Fault tolerance]] and high availability can be provided through [[Replication (computing)|data replication]] of one sort or another, so that data remains intact and available despite the failure of any single piece of equipment.  For examples, see the lists of [[List of file systems#Distributed fault-tolerant file systems|distributed fault-tolerant file systems]] and [[List of file systems#Distributed parallel fault-tolerant file systems|distributed parallel fault-tolerant file systems]].

===Performance===

A common [[performance]] [[measurement]] of a clustered file system is the amount of time needed to satisfy service requests.  In conventional systems, this time consists of a disk-access time and a small amount of [[Central processing unit|CPU]]-processing time.  But in a clustered file system, a remote access has additional overhead due to the distributed structure.  This includes the time to deliver the request to a server, the time to deliver the response to the client, and for each direction, a CPU overhead of running the [[communication protocol]] [[software]].

===Concurrency===

Concurrency control becomes an issue when more than one person or client is accessing the same file or block and want to update it.  Hence updates to the file from one client should not interfere with access and updates from other clients. This problem is more complex with file systems due to concurrent overlapping writes, where different writers write to overlapping regions of the file concurrently.<ref>Pessach, Yaniv (2013). ''Distributed Storage: Concepts, Algorithms, and Implementations''. ISBN 978-1482561043.</ref> This problem is usually handled by [[concurrency control]] or [[lock (computer science)|locking]] which may either be built into the file system or provided by an add-on protocol.

==History==
IBM mainframes in the 1970s could share physical disks and file systems if each machine had its own channel connection to the drives' control units. In the 1980s, [[Digital Equipment Corporation]]'s [[TOPS-20]] and [[OpenVMS]] clusters (VAX/ALPHA/IA64) included shared disk file systems.{{Citation needed|date=May 2016}}

==See also==
{{Div col||25em}}
* [[Network-attached storage]]
* [[Storage area network]]
* [[Shared resource]]
* [[Direct-attached storage]]
* [[Peer-to-peer file sharing]]
* [[Disk sharing]]
* [[Distributed data store]]
* [[Distributed file system for cloud]]
* [[Global file system]]
* [[Gopher (protocol)]]
* [[List of file systems#Distributed file systems|List of distributed file systems]]
* [[CacheFS]]
* [[RAID]]
{{Div col end}}

==References==
{{reflist}}

==Further reading==
* [http://www.cloudbus.org/reports/DistributedStorageTaxonomy.pdf A Taxonomy of Distributed Storage Systems]
* [http://trac.nchc.org.tw/grid/raw-attachment/wiki/jazz/09-05-22/A_Taxonomy_and_Survey_on_Distributed_File_Systems.pdf A Taxonomy and Survey on Distributed File Systems]
* [http://www.cis.upenn.edu/~bcpierce/courses/dd/papers/satya89survey.ps A survey of distributed file systems]
* [http://www.snia-europe.org/objects_store/Christian_Bandulet_SNIATutorial%20Basics_EvolutionFileSystems.pdf The Evolution of File Systems]

{{File systems|state=collapsed}}

[[Category:Computer file systems]]
[[Category:Shared disk file systems| ]]
[[Category:Storage area networks]]
[[Category:Distributed file systems| ]]
[[Category:Data management]]
[[Category:Distributed data storage]]
[[Category:Network file systems]]
<=====doc_Id=====>:263
<=====title=====>:
Write–write conflict
<=====text=====>:
In [[computer science]], in the field of [[database]]s, '''write–write conflict''', also known as '''overwriting [[commit (data management)|uncommitted]] data''' is a computational anomaly associated with interleaved execution of [[Database transaction|transactions]].

Given a [[Schedule (computer science)|schedule]] S

<math>S = \begin{bmatrix}
T1 & T2 \\
W(A) & \\
 & W(B) \\
W(B) & \\
Com. & \\
 & W(A)\\
 & Com. \end{bmatrix}</math>

note that there is no read in this schedule. The writes are called '''''blind writes'''''.

We have a '''''lost update'''''.  Any attempts to make this schedule serial would give off two different results (either T1's version of A and B is shown, or T2's version of A and B is shown), and would not be the same as the above schedule.  This schedule would not be [[Serializability|serializable]].

[[Strict two-phase locking|Strict 2PL]] overcomes this inconsistency by locking T1 out from B.  Unfortunately, [[deadlock]]s are something Strict 2PL does not overcome all the time.

== See also ==

* [[Concurrency control]]
* [[Read–write conflict]]
* [[Write–read conflict]]

==References==
{{reflist}}
{{Unreferenced|date=August 2009}}

{{DEFAULTSORT:Write-write conflict}}
[[Category:Data management]]
[[Category:Transaction processing]]
<=====doc_Id=====>:266
<=====title=====>:
Single source of truth
<=====text=====>:
{{Unreferenced|date=December 2009}}

In [[information systems]] design and theory, '''single source of truth''' ('''SSOT'''), is the practice of structuring information models and associated [[Database schema|schemata]] such that every data element is stored exactly once (e.g., in no more than a single row of a single table).  Any possible linkages to this data element (possibly in other areas of the relational schema or even in distant [[federated database|federated databases]]) are by [[Reference (computer science)|reference]] only.  Because all other locations of the data just refer back to the primary "source of truth" location, updates to the data element in the primary location propagate to the entire system without the possibility of a duplicate value somewhere being forgotten.

Deployment of an SSOT architecture is becoming increasingly important in enterprise settings where incorrectly linked duplicate or de-normalized data elements (a direct consequence of intentional or unintentional [[denormalization]] of any explicit data model) poses a risk for retrieval of outdated, and therefore incorrect, information.  A common example would be the [[electronic health record]], where it is imperative to accurately validate patient identity against a single referential repository, which serves as the SSOT.  Duplicate representations of data within the enterprise would be implemented by the use of [[pointer (computer programming)|pointer]]s rather than duplicate database tables, rows, or cells.  This ensures that data updates to elements in the authoritative location are comprehensively distributed to all [[federated database]] constituencies in the larger overall enterprise architecture.{{fact|date=July 2012}}

SSOT systems provide data that is authentic, relevant, and referable.<ref>IBM Smarter Planet - Operational risk management for financial services[http://www.ibm.com/smarterplanet/za/en/banking_technology/nextsteps/solution/Z017038Z16405R75.html]</ref>

==Implementation==
The "ideal" implementation of SSOT as described above is rarely possible in most enterprises. This is because many organisations have multiple information systems, each of which needs access to data relating to the same entities (e.g., customer). Often these systems are purchased "off-the-shelf" from vendors and cannot be modified in non-trivial ways. Each of these various systems therefore needs to store its own version of common data or entities, and therefore each system must retain its own copy of a record (hence immediately violating the SSOT approach defined above). For example, an ERP (enterprise resource planning) system (such as SAP or Oracle e-Business Suite) may store a customer record; the CRM (customer relationship management) system also needs a copy of the customer record (or part of it) and the warehouse despatch system might also need a copy of some or all of the customer data (e.g., shipping address). In cases where vendors do not support such modifications, it is not always possible to replace these records with pointers to the SSOT.

For organisations (with more than one information system) wishing to implement a Single Source of Truth (without modifying all but one master system to store pointers to other systems for all entities), three supporting technologies are commonly used:{{fact|date=July 2012}}

*[[Enterprise service bus]] (ESB)
*[[Master data management]] (MDM)
*[[Data warehouse]] (DW)

===Enterprise service bus (ESB)===
An enterprise service bus (ESB) allows any number of systems in an organisation to receive updates of data that has changed in another system. To implement a Single Source of Truth, a single source system of correct data for any entity must be identified. Changes to this entity (creates, updates, and deletes) are then published via the ESB; other systems which need to retain a copy of that data subscribe to this update, and update their own records accordingly. For any given entity, the master source must be identified (sometimes called the Golden Record). It should be noted that any given system could publish (be the source of truth for) information on a particular entity (e.g., customer) and also subscribe to updates from another system for information on some other entity (e.g., product).{{fact|date=July 2012}}

An alternative approach is point-to-point data updates, but these become exponentially more expensive to maintain as the number of systems increases, and this approach is increasingly out of favour as an IT architecture.{{fact|date=July 2012}}

===Master data management (MDM)===
An MDM system can act as the source of truth for any given entity that might not necessarily have an alternative "source of truth" in another system. Typically the MDM acts as a hub for multiple systems, many of which could allow (be the source of truth for) updates to different aspects of information on a given entity. For example, the CRM system may be the "source of truth" for most aspects of the customer, and is updated by a call centre operator. However, a customer may (for example) also update their address via a customer service web site, with a different back-end database from the CRM system. The MDM application receives updates from multiple sources, acts as a broker to determine which updates are to be regarded as authoritative (the Golden Record) and then syndicates this updated data to all subscribing systems. The MDM application normally requires an ESB to syndicate its data to multiple subscribing systems.<ref>BAYT Job Site - June 2014[http://www.bayt.com/en/specialties/q/7370/what-are-the-top-business-processes-and-applications-that-need-master-data-management/]</ref>
[[Customer Data Integration]] (CDI), as a common application of Master Data Management, is sometimes abbreviated CDI-MDM.{{fact|date=July 2012}}

===Data warehouse (DW)===
While the primary purpose of a data warehouse is to support reporting and analysis of data that has been combined from multiple sources, the fact that such data has been combined (according to business logic embedded in the [[Extract, transform, load|data transformation and integration processes]]) means that the data warehouse is often used as a ''de facto'' SSOT. Generally, however, the data available from the data warehouse is not used to update other systems; rather the DW becomes the "single source of truth" for reporting to multiple stakeholders. In this context, the Data Warehouse is more correctly referred to as a "[[single version of the truth]]" since other versions of the truth exist in its operational data sources (no data originates in the DW;  it is simply a reporting mechanism for data loaded from operational systems).{{fact|date=July 2012}}

==See also==
*[[Don't repeat yourself]] (DRY)
*[[SOLID (object-oriented design)]]
*[[Database normalization]]
*[[Single version of the truth]]
*[[System of record]]

==References==
{{reflist}}

==External links==


{{DEFAULTSORT:Single Source Of Truth}}
[[Category:Data modeling]]
[[Category:Database normalization]]
[[Category:Data management]]
<=====doc_Id=====>:269
<=====title=====>:
Data room
<=====text=====>:
{{Refimprove|date=March 2015}}
'''Data rooms''' are spaces used for housing data, usually of a secure or privileged nature. They can be physical data rooms, [[virtual data room]]s, or [[data centers]].<ref>{{cite web|title=Data Room (entry)|url=http://financial-dictionary.thefreedictionary.com/Data+room|website=Financial Dictionary, The Free Dictionary by Farlex}}</ref><ref>{{cite web|title=Data Room (entry)|url=http://www.nasdaq.com/investing/glossary/d/data-room|website=Nasdaq}}</ref> They are used for a variety of purposes, including data storage, document exchange, file sharing, financial transactions, legal transactions, and more.

In mergers and acquisitions, the traditional data room will literally be a physically secure continually monitored room, normally in the vendor’s offices (or those of their lawyers), which the bidders and their advisers will visit in order to inspect and report on the various documents and other data made available. Often only one bidder at a time will be allowed to enter and if new documents or new versions of documents are required these will have to be brought in by [[courier]] as [[hardcopy]]. Teams involved in large [[due diligence]] processes will typically have to be flown in from many regions or countries and remain available throughout the process.  Such teams often comprise a number of experts in different fields and so the overall cost of keeping such groups on call near to the data room is often extremely high.  Combating the significant cost of physical datarooms is the [[virtual data room]], which provides for the secure, online dissemination of confidential information.

A [[virtual data room]] (VDR) is essentially a website with limited controlled access (using a secure log-on supplied by the vendor/authority which can be disabled at any time by the vendor/authority if a bidder withdraws) to which the bidders and their advisers are given access. Much of the information released will be confidential and restrictions should be applied to the viewers' ability to release this to third parties by forwarding, copying or printing. [[Digital rights management]] is sometimes applied to control information.

Detailed auditing must be provided for legal reasons so that a record is kept of who has seen which version of each document.

Data rooms are commonly used by [[legal]], [[accounting]], [[investment banking]] and [[private equity]] companies performing [[mergers and acquisitions]], [[fundraising]], [[insolvency]], [[corporate restructuring]], and joint ventures including bio-technology and tender processes.

==References==
{{Reflist}}
* [http://www.imaa-institute.org/docs/kummer-sliskovic_do%20virtual%20data%20rooms%20add%20value%20to%20the%20mergers%20and%20acquisitions%20process.pdf A report about the advantages and disadvantages of virtual vs. physical data rooms]

{{DEFAULTSORT:Data Room}}
[[Category:Data management]]
<=====doc_Id=====>:272
<=====title=====>:
Managed Memory Computing
<=====text=====>:
{{multiple issues|
{{Orphan|date=November 2013}}
{{unreferenced|date=November 2013}}
}}

== This article pertains to a new technology used in Business Intelligence. Managed Memory Computing uses aggregated data for in-memory analytics   ==

Aggregated data cubes are the most effective form of storage of aggregated or summarized data for quick analysis. This technology is driven by [[Online Analytical Processing|Online Analytical Processing technology]]. Utilizing these data cubes involves intense disk I/O operations. This at times lowers the speed for users of data.

Conventional, [[In-Memory Processing|in-memory processing]] does not rely on stored and summarized or aggregated data but brings all the relevant data to the memory. This technology then utilizes intense processing and large amounts of memory to perform all calculations and aggregations while in memory.

Managed Memory Computing blends the best of both methods, allowing users to define data cubes with per-structured and aggregated data, providing a logical business layer to users, and offering in-memory computation. These features make the response time for user interactions far superior and enable the most balanced approach between disk I/O and in-memory processing.

The hybrid approach of Managed Memory Computing provides analysis, dashboards, graphical interaction, ad hoc querying, presentation, and discussion driven analytic at blazing speeds, making the [[Business intelligence|Business Intelligence Tool]] ready for everything from an interactive session in the boardroom to a [[production planning]] meeting on the factory floor.

== References ==

[http://www.cioreview.in/magazine/ElegantJ-BI-Managed-Memory-Computing--Business-Intelligence-Redefined-CZSI499492332.html Introduction of Managed Memory Computing in CIOReview]

[[Category:Business intelligence]]
[[Category:Financial data analysis]]
[[Category:Data management]]
[[Category:Computer architecture]]
<=====doc_Id=====>:275
<=====title=====>:
Abstraction (software engineering)
<=====text=====>:
{{Refimprove|date=June 2011}}
{{Quote box|quote=The essence of abstractions is preserving information that is relevant in a given context, and forgetting information that is irrelevant in that context.|source=– John V. Guttag<ref>{{Cite book | edition = Spring 2013 | publisher = The MIT Press | isbn = 9780262519632 | last = Guttag | first = John V. | title = Introduction to Computation and Programming Using Python | location = Cambridge, Massachusetts | date = 2013-01-18}}</ref>|width=25%}}

In [[software engineering]] and [[computer science]], '''abstraction''' is a technique for arranging complexity of computer systems. It works by establishing a level of complexity on which a person interacts with the system, suppressing the more complex details below the current level. The programmer works with an idealized interface (usually well defined) and can add additional levels of functionality that would otherwise be too complex to handle. For example, a programmer writing code that involves numerical operations may not be interested in the way numbers are represented in the underlying hardware (e.g. whether they're ''16 bit'' or ''32 bit [[Integer (computer science)|integers]]''), and where those details have been suppressed it can be said that they were ''abstracted away'', leaving simply ''numbers'' with which the programmer can work. In addition, a task of sending an email message across continents would be extremely complex if the programmer had to start with a piece of fiber optic cable and basic hardware components. By using layers of complexity that have been created to abstract away the physical cables and network layout, and presenting the programmer with a virtual data channel, this task is manageable.

Abstraction can apply to control or to data: '''Control abstraction''' is the abstraction of actions while '''data abstraction''' is that of [[data structures]].

* Control abstraction involves the use of [[subroutine]]s and [[control flow]] abstractions
* Data abstraction allows handling pieces of data in meaningful ways. For example, it is the basic motivation behind the [[datatype]].

The notion of an [[object (computer science)|object]] in object-oriented programming can be viewed as a way to combine abstractions of data and code.

The same abstract definition can be used as a common [[Interface (computer science)|interface]] for a family of objects with different implementations and behaviors but which share the same meaning. The [[Inheritance (computer science)|inheritance]] mechanism in object-oriented programming can be used to define an [[Class (computer science)#Abstract|abstract class]] as the common interface.

The recommendation that programmers use abstractions whenever suitable in order to avoid duplication (usually [[code duplication|of code]]) is known as the [[Abstraction principle (computer programming)|abstraction principle]]. The requirement that a programming language provide suitable abstractions is also called the abstraction principle.

==Rationale==
Computing mostly operates independently of the concrete world: The hardware implements a [[model of computation]] that is interchangeable with others. The software is structured in [[software architecture|architecture]]s to enable humans to create the enormous systems by concentrating on a few issues at a time. These architectures are made of specific choices of abstractions. [[Greenspun's Tenth Rule]] is an [[aphorism]] on how such an architecture is both inevitable and complex.

A central form of abstraction in computing is language abstraction: new artificial languages are developed to express specific aspects of a system. ''[[Modeling languages]]'' help in planning. ''[[Computer language]]s'' can be processed with a computer. An example of this abstraction process is the generational development of [[programming language]]s from the [[First-generation programming language|machine language]] to the [[Second-generation programming language|assembly language]] and the [[Third-generation programming language|high-level language]]. Each stage can be used as a stepping stone for the next stage. The language abstraction continues for example in [[scripting language]]s and [[domain-specific programming language]]s.

Within a programming language, some features let the programmer create new abstractions. These include [[subroutine]]s, [[module (programming)|modules]], [[polymorphism (computer science)|polymorphism]], and [[software component]]s. Some other abstractions such as [[software design pattern]]s and [[software architecture#Architecture examples|architectural styles]] remain invisible to a [[translator (computing)|translator]] and operate only in the design of a system.

Some abstractions try to limit the range of concepts a programmer needs to be aware of, by completely hiding the abstractions that they in turn are built on. The software engineer and writer [[Joel Spolsky]] has criticised these efforts by claiming that all abstractions are ''[[leaky abstraction|leaky]]'' — that they can never completely hide the details below;<ref>{{cite web|last1=Spolsky|first1=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html}}</ref> however, this does not negate the usefulness of abstraction.

Some abstractions are designed to interoperate with other abstractions - for example, a programming language may contain a [[foreign function interface]] for making calls to the lower-level language.

==Language features==

===Programming languages===
{{Main|Programming language}}

Different programming languages provide different types of abstraction, depending on the intended applications for the language. For example:

* In [[object-oriented programming language]]s such as [[C++]], [[Object Pascal]], or [[Java (programming language)|Java]], the concept of '''abstraction''' has itself become a declarative statement – using the [[keyword (computer programming)|keyword]]s ''<code>virtual</code>'' (in [[C++]]) or ''<code>abstract</code>''<ref name="Oracle Java abstract">{{cite web|title=Abstract Methods and Classes|url=http://docs.oracle.com/javase/tutorial/java/IandI/abstract.html|website=The Java™ Tutorials|publisher=Oracle|accessdate=4 September 2014}}</ref> and ''<code>interface</code>''<ref name="Oracle Java interface">{{cite web|title=Using an Interface as a Type|url=http://docs.oracle.com/javase/tutorial/java/IandI/interfaceAsType.html|website=The Java™ Tutorials|publisher=Oracle|accessdate=4 September 2014}}</ref> (in [[Java (programming language)|Java]]). After such a declaration, it is the responsibility of the programmer to implement a [[Class (computer science)|class]] to instantiate the [[Object (computer science)|object]] of the declaration.
* [[Functional programming language]]s commonly exhibit abstractions related to functions, such as [[lambda abstraction]]s (making a term into a function of some variable) and [[higher-order function]]s (parameters are functions). <!-- This has to be merged in the following sections. -->
* Modern members of the Lisp programming language family such as [[Clojure]], [[Scheme (programming language)|Scheme]] and [[Common Lisp]] support [[Macro (computer science)#Syntactic macros|macro systems]] to allow syntactic abstraction. Other programming languages such as [[Scala (programming language)|Scala]] also have macros, or very similar [[metaprogramming]] features (for example, [[Haskell (programming language)|Haskell]] has [[Template Haskell]], and [[OCaml]] has [[MetaOCaml]]). These can allow a programmer to eliminate [[boilerplate code]], abstract away tedious function call sequences, implement new [[Control flow|control flow structures]], and implement [[Domain-specific language|Domain Specific Languages (DSLs)]], which allow domain-specific concepts to be expressed in concise and elegant ways. All of these, when used correctly, improve both the programmer's efficiency and the clarity of the code by making the intended purpose more explicit. A consequence of syntactic abstraction is also that any Lisp dialect and in fact almost any programming language can, in principle, be implemented in any modern Lisp with significantly reduced (but still non-trivial in some cases) effort when compared to "more traditional" programming languages such as [[Python (programming language)|Python]], [[C (programming language)|C]] or [[Java (programming language)|Java]].

===Specification methods===
{{Main|Formal specification}}

Analysts have developed various methods to formally specify software systems.  Some known methods include:

* Abstract-model based method (VDM, Z);
* Algebraic techniques (Larch, CLEAR, OBJ, ACT ONE, CASL);
* Process-based techniques (LOTOS, SDL, Estelle);
* Trace-based techniques (SPECIAL, TAM);
* Knowledge-based techniques (Refine, Gist).

===Specification languages===
{{Main|Specification language}}

Specification languages generally rely on abstractions of one kind or another, since specifications are typically defined earlier in a project, (and at a more abstract level) than an eventual implementation. The [[Unified Modeling Language|UML]] specification language, for example, allows the definition of ''abstract'' classes, which in a waterfall project, remain abstract during the architecture and specification phase of the project.

==Control abstraction==
{{Main|Control flow}}

Programming languages offer control abstraction as one of the main purposes of their use. Computer machines understand operations at the very low level such as moving some bits from one location of the memory to another location and producing the sum of two sequences of bits. Programming languages allow this to be done in the higher level. For example, consider this statement written in a [[Pascal (programming language)|Pascal]]-like fashion:

:<code>a := (1 + 2) * 5</code>

To a human, this seems a fairly simple and obvious calculation (''"one plus two is three, times five is fifteen"''). However, the low-level steps necessary to carry out this evaluation, and return the value "15", and then assign that value to the variable "a", are actually quite subtle and complex. The values need to be converted to binary representation (often a much more complicated task than one would think) and the calculations decomposed (by the compiler or interpreter) into assembly instructions (again, which are much less intuitive to the programmer: operations such as shifting a binary register left, or adding the binary complement of the contents of one register to another, are simply not how humans think about the abstract arithmetical operations of addition or multiplication). Finally, assigning the resulting value of "15" to the variable labeled "a", so that "a" can be used later, involves additional 'behind-the-scenes' steps of looking up a variable's label and the resultant location in physical or virtual memory, storing the binary representation of "15" to that memory location, etc.

Without control abstraction, a programmer would need to specify ''all'' the register/binary-level steps each time they simply wanted to add or multiply a couple of numbers and assign the result to a variable. Such duplication of effort has two serious negative consequences:

# it forces the programmer to constantly repeat fairly common tasks every time a similar operation is needed
# it forces the programmer to program for the particular hardware and instruction set

===Structured programming===
{{Main|Structured programming}}

Structured programming involves the splitting of complex program tasks into smaller pieces with clear flow-control and interfaces between components, with reduction of the complexity potential for side-effects.

In a simple program, this may aim to ensure that loops have single or obvious exit points and (where possible) to have single exit points from functions and procedures.

In a larger system, it may involve breaking down complex tasks into many different modules. Consider a system which handles payroll on ships and at shore offices:

* The uppermost level may feature a menu of typical end-user operations.
* Within that could be standalone executables or libraries for tasks such as signing on and off employees or printing checks.
* Within each of those standalone components there could be many different source files, each containing the program code to handle a part of the problem, with only selected interfaces available to other parts of the program. A sign on program could have source files for each data entry screen and the database interface (which may itself be a standalone third party library or a statically linked set of library routines).
*Either the database or the payroll application also has to initiate the process of exchanging data with between ship and shore, and that data transfer task will often contain many other components.

These layers produce the effect of isolating the implementation details of one component and its assorted internal methods from the others. Object-oriented programming embraces and extends this concept.

==Data abstraction==
{{Main|Abstract data type}}

Data abstraction enforces a clear separation between the ''abstract'' properties of a [[data type]] and the ''concrete'' details of its implementation. The abstract properties are those that are visible to client code that makes use of the data type—the ''interface'' to the data type—while the concrete implementation is kept entirely private, and indeed can change, for example to incorporate efficiency improvements over time. The idea is that such changes are not supposed to have any impact on client code, since they involve no difference in the abstract behaviour.

For example, one could define an [[abstract data type]] called ''lookup table'' which uniquely associates ''keys'' with ''values'', and in which values may be retrieved by specifying their corresponding keys. Such a lookup table may be implemented in various ways: as a [[hash table]], a [[binary search tree]], or even a simple linear [[List (computing)|list]] of (key:value) pairs. As far as client code is concerned, the abstract properties of the type are the same in each case.

Of course, this all relies on getting the details of the interface right in the first place, since any changes there can have major impacts on client code. As one way to look at this: the interface forms a ''contract'' on agreed behaviour between the data type and client code; anything not spelled out in the contract is subject to change without notice.

<!-- This makes no sense to me. [[User:TakuyaMurata|Taku]] 07:13, 19 June 2005 (UTC) -->
Languages that implement data abstraction include [[Ada programming language|Ada]] and [[Modula-2]]. [[Object-oriented]] languages are commonly claimed{{By whom|date=March 2009}} to offer data abstraction; however, their [[Inheritance (computer science)|inheritance]] concept tends to put information in the interface that more properly belongs in the implementation; thus, changes to such information ends up impacting client code, leading directly to the [[Fragile binary interface problem]].

==Abstraction in object oriented programming==
{{Main|Object (computer science)}}

In [[object-oriented programming]] theory, '''abstraction''' involves the facility to define objects that represent abstract "actors" that can perform work, report on and change their state, and "communicate" with other objects in the system. The term [[Encapsulation (object-oriented programming)|encapsulation]] refers to the hiding of [[state (computer science)|state]] details, but extending the concept of ''data type'' from earlier programming languages to associate ''behavior'' most strongly with the data, and standardizing the way that different data types interact, is the beginning of '''abstraction'''.  When abstraction proceeds into the operations defined, enabling objects of different types to be substituted, it is called [[polymorphism (computer science)|polymorphism]]. When it proceeds in the opposite direction, inside the types or classes, structuring them to simplify a complex set of relationships, it is called [[Delegation (object-oriented programming)|delegation]] or [[Inheritance (computer science)|inheritance]].

Various object-oriented programming languages offer similar facilities for abstraction, all to support a general strategy of [[polymorphism (computer science)|polymorphism]] in object-oriented programming, which includes the substitution of one [[type in object-oriented programming|type]] for another in the same or similar role. Although not as generally supported, a [[configuration in object-oriented programming|configuration]] or image or package may predetermine a great many of these [[name binding|bindings]] at [[compile-time]], [[link-time]], or [[loadtime]]. This would leave only a minimum of such bindings to change at [[Run time (program lifecycle phase)|run-time]].

[[Common Lisp Object System]] or [[Self (programming language)|Self]], for example, feature less of a class-instance distinction and more use of delegation for [[polymorphism in object-oriented programming|polymorphism]]. Individual objects and functions are abstracted more flexibly to better fit with a shared functional heritage from [[Lisp programming language|Lisp]].

C++ exemplifies another extreme: it relies heavily on [[generic programming|templates]] and [[method overloading|overloading]] and other static bindings at compile-time, which in turn has certain flexibility problems.

Although these examples offer alternate strategies for achieving the same abstraction, they do not fundamentally alter the need to support abstract nouns in code - all programming relies on an ability to abstract verbs as functions, nouns as data structures, and either as processes.

Consider for example a sample [[Java (programming language)|Java]] fragment to represent some common farm "animals" to a level of abstraction suitable to model simple aspects of their hunger and feeding. It defines an <code>Animal</code> class to represent both the state of the animal and its functions:

<source lang=java>
public class Animal extends LivingThing
{
     private Location loc;
     private double energyReserves;

     public boolean isHungry() {
         return energyReserves < 2.5;
     }
     public void eat(Food food) {
         // Consume food
         energyReserves += food.getCalories();
     }
     public void moveTo(Location location) {
         // Move to new location
         this.loc = location;
     }
}
</source>
With the above definition, one could create objects of type <tt>Animal</tt> and call their methods like this:

<source lang=java>
thePig = new Animal();
theCow = new Animal();
if (thePig.isHungry()) {
    thePig.eat(tableScraps);
}
if (theCow.isHungry()) {
    theCow.eat(grass);
}
theCow.moveTo(theBarn);
</source>
In the above example, the class ''<code>Animal</code>'' is an abstraction used in place of an actual animal, ''<code>LivingThing</code>'' is a further abstraction (in this case a generalisation) of ''<code>Animal</code>''.

If one requires a more differentiated hierarchy of animals — to differentiate, say, those who provide milk from those who provide nothing except meat at the end of their lives — that is an intermediary level of abstraction, probably DairyAnimal (cows, goats) who would eat foods suitable to giving good milk, and MeatAnimal (pigs, steers) who would eat foods to give the best meat-quality.

Such an abstraction could remove the need for the application coder to specify the type of food, so s/he could concentrate instead on the feeding schedule. The two classes could be related using [[Inheritance (computer science)|inheritance]] or stand alone, and the programmer could define varying degrees of [[polymorphism (computer science)|polymorphism]] between the two types. These facilities tend to vary drastically between languages, but in general each can achieve anything that is possible with any of the others. A great many operation overloads, data type by data type, can have the same effect at compile-time as any degree of inheritance or other means to achieve polymorphism. The class notation is simply a coder's convenience.

===Object-oriented design===
{{Main|Object-oriented design}}

Decisions regarding what to abstract and what to keep under the control of the coder become the major concern of object-oriented design and [[domain analysis]]&mdash;actually determining the relevant relationships in the real world is the concern of [[object-oriented analysis]] or [[legacy analysis]].

In general, to determine appropriate abstraction, one must make many small decisions about scope (domain analysis), determine what other systems one must cooperate with (legacy analysis), then perform a detailed object-oriented analysis which is expressed within project time and budget constraints as an object-oriented design. In our simple example, the domain is the barnyard, the live pigs and cows and their eating habits are the legacy constraints, the detailed analysis is that coders must have the flexibility to feed the animals what is available and thus there is no reason to code the type of food into the class itself, and the design is a single simple Animal class of which pigs and cows are instances with the same functions. A decision to differentiate DairyAnimal would change the detailed analysis but the domain and legacy analysis would be unchanged&mdash;thus it is entirely under the control of the programmer, and we refer to abstraction in object-oriented programming as distinct from abstraction in domain or legacy analysis.

==Considerations==
When discussing [[formal semantics of programming languages]], [[formal methods]] or [[abstract interpretation]], '''abstraction''' refers to the act of considering a less detailed, but safe, definition of the observed program behaviors. For instance, one may observe only the final result of program executions instead of considering all the intermediate steps of executions. Abstraction is defined to a '''concrete''' (more precise) model of execution.

Abstraction may be '''exact''' or '''faithful''' with respect to a property if one can answer a question about the property equally well on the concrete or abstract model. For instance, if we wish to know what the result of the evaluation of a mathematical expression involving only integers +, -, ×, is worth [[modular arithmetic|modulo]] ''n'', we need only perform all operations modulo ''n'' (a familiar form of this abstraction is [[casting out nines]]).

Abstractions, however, though not necessarily '''exact''', should be '''sound'''. That is, it should be possible to get sound answers from them&mdash;even though the abstraction may simply yield a result of [[undecidable problem|undecidability]]. For instance, we may abstract the students in a class by their minimal and maximal ages; if one asks whether a certain person belongs to that class, one may simply compare that person's age with the minimal and maximal ages; if his age lies outside the range, one may safely answer that the person does not belong to the class; if it does not, one may only answer "I don't know".

The level of abstraction included in a programming language can influence its overall [[usability]]. The [[Cognitive dimensions]] framework includes the concept of ''abstraction gradient'' in a formalism. This framework allows the designer of a programming language to study the trade-offs between abstraction and other characteristics of the design, and how changes in abstraction influence the language usability.

Abstractions can prove useful when dealing with computer programs, because non-trivial properties of computer programs are essentially [[undecidable problem|undecidable]] (see [[Rice's theorem]]). As a consequence, automatic methods for deriving information on the behavior of computer programs either have to drop termination (on some occasions, they may fail, crash or never yield out a result), soundness (they may provide false information), or precision (they may answer "I don't know" to some questions).

Abstraction is the core concept of [[abstract interpretation]]. [[Model checking]] generally takes place on abstract versions of the studied systems.

==Levels of abstraction==
{{Main|Abstraction layer}}

Computer science commonly presents ''levels'' (or, less commonly, ''layers'') of abstraction, wherein each level represents a different model of the same information and processes, but uses a system of expression involving a unique set of objects and compositions that apply only to a particular domain.
<ref>[[Luciano Floridi]], [http://www.cs.ox.ac.uk/activities/ieg/research_reports/ieg_rr221104.pdf ''Levellism and the Method of Abstraction'']
IEG – Research Report 22.11.04</ref>
Each relatively abstract, "higher" level builds on a relatively concrete, "lower" level, which tends to provide an increasingly "granular" representation. For example, gates build on electronic circuits, binary on gates, machine language on binary, programming language on machine language, applications and operating systems on programming languages. Each level is embodied, but not determined, by the level beneath it, making it a language of description that is somewhat self-contained.

===Database systems===
{{Main|Database management system}}

Since many users of database systems lack in-depth familiarity with computer data-structures, database developers often hide complexity through the following levels:

[[Image:Data abstraction levels.png|thumb|Data abstraction levels of a database system]]

'''Physical level:''' The lowest level of abstraction describes ''how'' a system actually stores data. The physical level describes complex low-level data structures in detail.

'''Logical level:''' The next higher level of abstraction describes ''what'' data the database stores, and what relationships exist among those data. The logical level thus describes an entire database in terms of a small number of relatively simple structures. Although implementation of the simple structures at the logical level may involve complex physical level structures, the user of the logical level does not need to be aware of this complexity. This referred to as [[physical data independence]]. [[Database administrator]]s, who must decide what information to keep in a database, use the logical level of abstraction.

'''View level:''' The highest level of abstraction describes only part of the entire database. Even though the logical level uses simpler structures, complexity remains because of the variety of information stored in a large database. Many users of a database system do not need all this information; instead, they need to access only a part of the database. The view level of abstraction exists to simplify their interaction with the system. The system may provide many [[view (database)|view]]s for the same database.

===Layered architecture===
{{Main|Abstraction layer}}
The ability to provide a [[design]] of different levels of abstraction can

* simplify the design considerably
* enable different role players to effectively work at various levels of abstraction
* support the portability of [[software artifact]]s (model-based ideally)

[[Systems design]] and [[Business process modeling|business process design]] can both use this. Some [[Software modeling|design processes]] specifically generate designs that contain various levels of abstraction.

Layered architecture partitions the concerns of the application into stacked groups (layers).
It is a technique used in designing computer software, hardware, and communications in which system or network components are isolated in layers so that changes can be made in one layer without affecting the others.

==See also==
* [[Abstraction principle (computer programming)]]
* [[Abstraction inversion]] for an anti-pattern of one danger in abstraction
* [[Abstract data type]] for an abstract description of a set of data
* [[Algorithm]] for an abstract description of a computational procedure
* [[Bracket abstraction]] for making a term into a function of a variable
* [[Data modeling]] for structuring data independent of the processes that use it
* [[Encapsulation (object-oriented programming)|Encapsulation]] for abstractions that hide implementation details
* [[Greenspun's Tenth Rule]] for an aphorism about an (the?) optimum point in the space of abstractions
* [[Higher-order function]] for abstraction where functions produce or consume other functions
* [[Lambda abstraction]] for making a term into a function of some variable
* [[List of abstractions (computer science)]]
* [[Program refinement|Refinement]] for the opposite of abstraction in computing

==References==
{{Reflist}}
{{FOLDOC}}

==Further reading==
{{refbegin}}
* {{cite book|author1=Harold Abelson|author2=Gerald Jay Sussman|author3=Julie Sussman|title=Structure and Interpretation of Computer Programs|url=http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-10.html|accessdate=22 June 2012|edition=2|date=25 July 1996|publisher=MIT Press|isbn=978-0-262-01153-2}}
* {{cite web|last=Spolsky|first=Joel|title=The Law of Leaky Abstractions|url=http://www.joelonsoftware.com/articles/LeakyAbstractions.html|work=Joel on Software|date=11 November 2002}}
* [http://www.cs.cornell.edu/courses/cs211/2006sp/Lectures/L08-abstraction/08_abstraction.html Abstraction/information hiding] - CS211 course, Cornell University.
* {{cite book|author=Eric S. Roberts|title=Programming Abstractions in C A Second Course in Computer Science|date=1997}}
* {{cite web|last=Palermo|first=Jeffrey|title=The Onion Architecture|url=http://jeffreypalermo.com/blog/the-onion-architecture-part-1/|work=Jeffrey Palermo|date=29 July 2008}} 
{{refend}}

==External links==
* [https://sites.google.com/site/simulationarchitecture/ SimArch] example of layered architecture for distributed simulation systems.

{{Use dmy dates|date=June 2011}}

{{DEFAULTSORT:Abstraction (computer science)}}
[[Category:Data management]]
[[Category:Programming paradigms]]
[[Category:Articles with example Java code]]
[[Category:Abstraction]]
<=====doc_Id=====>:278
<=====title=====>:
Quality of Data (QoD)
<=====text=====>:
{{Orphan|date=January 2016}}

'''Quality-of-Data (QoD)''' is a designation coined by L. Veiga, that specifies and describes the required Quality of Service of a distributed storage system from the Consistency point of view of its data.
. It can be used to support [[Big data|Big Data]] management frameworks, Workflow management, and HPC systems (mainly for data replication and consistency). It takes into account data semantics, namely Time interval of data freshness, Sequence of tolerable number of outstanding versions of the data read before refresh, and Value divergence allowed before displaying it. Initially it was based in a model from an existing research work regarding vector-field Consistency,<ref>{{cite conference |author1=Nuno Santos |author2=Luís Veiga |author3=Paulo Ferreira | year=2007 | title=Vector-Field Consistency for Adhoc Gaming| booktitle = ACM/IFIP/Usenix Middleware Conference 2007 | url=http://www.gsd.inesc-id.pt/~lveiga/msc-08-09/vfc-middleware-07.pdf | format=PDF}}</ref> awarded the best-paper prize in the ACM/IFIP/Usenix Middleware Conference 2007 and later enhanced for increased scalability and fault-tolerance.<ref>{{cite conference |author1=Luís Veiga |author2=André Negrão |author3=Nuno Santos |author4=Paulo Ferreira | year=2010 | title=Unifying Divergence Bounding and Locality Awareness in Replicated Systems with Vector-Field Consistency | booktitle = JISA, Journal of Internet Services and Applications, Volume 1, Number 2, 95-115, Springer, 2010 | url=http://www.gsd.inesc-id.pt/~lveiga/vfc-JISA-2010.pdf | format=PDF}}</ref>

This consistency model has been successfully applied and proven in Big Data key/value store [[Apache HBase]],<ref group="nb">url=https://hbase.apache.org</ref> initially designed as a [[middleware]]<ref>{{cite conference |author1=Sergio Estéves |author2=João Silva |author3=Luís Veiga  |last-author-amp=yes | year=2013 | title=Quality-of-service for consistency of data geo-replication in cloud computing  | booktitle = Euro-Par 2012 Parallel Processing. Springer Berlin Heidelberg, 2012. 285-297 | url=http://www.gsd.inesc-id.pt/~sesteves/papers/vfc3-europar12.pdf | format=PDF}}</ref> module seating between clusters from separate data centres. The HBase-QoD coupling <ref>{{cite conference |author1=Álvaro García-Recuero |author2=Sergio Estéves |author3=Luís Veiga | year=2013 | title=Quality-of-Data for Consistency Levels in Geo-replicated Cloud Data Stores  | booktitle = IEEE CloudCom 2013 | url=http://www.inesc-id.pt/ficheiros/publicacoes/9253.pdf | format=PDF}}</ref> minimises bandwidth usage and optimises resources allocation during replication achieving the desired consistency level at a more fine-grained level.

QoD is defined by the three-dimensions of vector k=(θ,σ,ν), but with a broader view of the issue, applicable also to large-scale data management techniques in regards to their timely delivery.<ref group="nb"><sub>url=http://www-01.ibm.com/software/data/quality/</sub></ref>

== Other Descriptions ==

Quality-of-Data should not be confused with other definitions for Data Quality such as
<ref>{{cite conference |author1=Richard Y. Wang | year= 1992 | title=Toward quality data : an attribute-based approach | booktitle=Decision Support Systems 13, MIT  | url=http://web.mit.edu/tdqm/www/tdqmpub/Toward%20Quality%20Data.pdf | format=PDF}}</ref>
<ref>{{cite conference |author1=George A. Mihaila |author2=Louiqa Raschid |author3=María-Esther Vidal | year= 2000 | title=Using Quality of Data Metadata for Source Selection and Ranking | booktitle =  | url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.9361 | format=}}</ref>
- Completeness
- Validity
- Accuracy

== Notes ==
<references group="nb"/>

== References ==
<references/>

[[Category:Data management]]
<=====doc_Id=====>:281
<=====title=====>:
Head/tail Breaks
<=====text=====>:
{{Multiple issues|
{{technical|date=June 2014}}
{{COI|date=July 2014}}
}}

[[File:Patterns1024Cities2.jpg|thumb|500px|1024 cities that follow exactly Zipf's law, which implies that the first largest city is size 1, the second largest city is size 1/2, the third largest city is size 1/3, ... and the smallest city is size 1/1024. The left pattern is produced by head/tail breaks, while the right one by natural breaks, also known as [[Jenks natural breaks optimization]].]]
'''Head/tail breaks''' is a new [[clustering algorithm]] scheme for data with a heavy-tailed distribution such as [[power laws]] and [[lognormal distributions]]. The heavy-tailed distribution can be simply referred to the scaling pattern of far more small things than large ones, or alternatively numerous smallest, a very few largest, and some in between the smallest and largest. The classification is done through dividing things into large (or called the head) and small (or called the tail) things around the arithmetic mean or average, and then recursively going on for the division process for the large things or the head until the notion of far more small things than large ones is no longer valid, or with more or less similar things left only.<ref name="Jiang1">Jiang, Bin (2013a). "Head/tail breaks: A new classification scheme for data with a heavy-tailed distribution", ''The Professional Geographer'', 65 (3), 482 – 494.</ref> Head/tail breaks is not just for classification, but also for visualization of big data by keeping the head, since the head is self-similar to the whole. Head/tail breaks can be applied not only to vector data such as points, lines and polygons, but also to raster data like digital elevation model (DEM). 

==Motivation==
The head/tail breaks is mainly motivated by inability of conventional classification methods such as equal intervals, quantiles, geometric progressions, standard deviation, and natural breaks - commonly known as [[Jenks natural breaks optimization]] for revealing the underlying scaling pattern of far more small things than large ones. Note that the notion of far more small things than large one is not only referred to geometric property, but also to topological and semantic properties. In this connection, the notion should be interpreted as far more unpopular (or less-connected) things than popular (or well-connected) ones, or far more meaningless things than meaningful ones.

==Method==
Given some variable X that demonstrates a heavy-tailed distribution, there are far more small x than large ones. Take the average of all xi, and obtain the first mean m1. Then calculate the second mean for those xi greater than m1, and obtain m2. In the same recursive way, we can get m3 depending on whether the ending condition of no longer far more small x than large ones is met. For simplicity, we assume there are three means, m1, m2, and m3. This classification leads to four classes: [minimum, m1], (m1, m2], (m2, m3], (m3, maximum]. In general, it can be represented as a recursive function as follows:

     Recursive function '''Head/tail Breaks''':
     Break the input data (around mean or average) into the head and the tail;  
     // the head for data values greater the mean
     // the tail for data values less the mean
     while (head <= 40%):
         '''Head/tail Breaks'''(head);
     End Function

The resulting number of classes is referred to as ht-index, an alternative index to [[fractal dimension]] for characterizing complexity of fractals or geographic features: the higher the ht-index, the more complex the fractals.<ref name="Jiang2">Jiang, Bin and Yin Junjun (2014). "Ht-index for quantifying the fractal or scaling structure of geographic features", ''Annals of the Association of American Geographers'', 104(3), 530–541.</ref> Recently, a more sensitive ht-index, namely CRG-index,<ref>{{Cite journal|title = CRG Index: A more sensitive ht-index for enabling dynamic views of geographic features|url = http://dx.doi.org/10.1080/00330124.2015.1099448|journal = The Professional Geographer|date = 2015-12-09|issn = 0033-0124|pages = 1–13|volume = 0|issue = 0|doi = 10.1080/00330124.2015.1099448|first = Peichao|last = Gao|first2 = Zhao|last2 = Liu|first3 = Meihui|last3 = Xie|first4 = Kun|last4 = Tian|first5 = Gang|last5 = Liu}}</ref> has been developed, and it is able to capture slight changes which ht-index is unable to. Thus while ht-index is an integer, CRG-index is a real number. A PostgreSQL function for calculating ht-index can be found here.<ref>{{Cite journal|title = A PostgreSQL function for calculating the ht-index|url = https://www.researchgate.net/publication/287533541_A_PostgreSQL_function_for_calculating_the_ht-index?channel=doi&linkId=56777e5b08aebcdda0e962fe&showFulltext=true|date = 2015-01-01|doi = 10.13140/RG.2.1.3041.0324|first = Peichao Gao|last = Kun Tian}}</ref>

=== Threshold or its sensitivity ===
The criterion to stop the iterative classification process using the head/tail breaks method is that the remaining data (i.e., the head part) are not heavy-tailed, or simply, the head part is no longer a minority (i.e., the proportion of the head part is no longer less than a threshold such as 40%). This threshold is suggested to be 40% by Jiang et al. (2013),<ref name="Jiang3" /> just as the codes above (i.e., head <= 40%). But sometimes a larger threshold, for example 50% or more, can be used, as Jiang and Yin (2014)<ref name="Jiang2" /> noted in another article: "this condition can be relaxed for many geographic features, such as 50 percent or even more". However, all heads' percentage on average must be smaller than 40% (or 41, 42%), indicating far more small things than large ones. This sensitivity issue deserves further research in the future.

=== Rank-size plot and RA index ===
A good tool to display the scaling pattern, or the heavy-tailed distribution, is the rank-size plot, which is a scatter plot to display a set of values according to their ranks. With this tool, a new index <ref>{{Cite journal|last=Gao|first=Peichao|last2=Liu|first2=Zhao|last3=Tian|first3=Kun|last4=Liu|first4=Gang|date=2016-03-10|title=Characterizing Trafﬁc Conditions from the Perspective of Spatial-Temporal Heterogeneity|url=http://www.mdpi.com/2220-9964/5/3/34|journal=ISPRS International Journal of Geo-Information|language=en|volume=5|issue=3|pages=34|doi=10.3390/ijgi5030034}}</ref> termed as the ratio of areas (RA) in a rank-size plot was defined to characterize the scaling pattern. The RA index has been successfully used in the estimation of traffic conditions. However, it should be noted that the RA index can only be used as a complementary method to the ht-index, because it is ineffective to capture the scaling structure of geographic features.

==Applications==
Instead of more or less similar things, there are far more small things than large ones surrounding us. Given the ubiquity of the scaling pattern, head/tail breaks is found to be of use to statistical mapping, map generalization, cognitive mapping and even perception of beauty
.<ref name="Jiang3">Jiang, Bin, Liu, Xintao and Jia, Tao (2013). "Scaling of geographic space as a universal rule for map generalization", ''Annals of the Association of American Geographers'', 103(4), 844 – 855.</ref><ref name="Jiang4">Jiang, Bin (2013b). "The image of the city out of the underlying scaling of city artifacts or locations", ''Annals of the Association of American Geographers'', 103(6), 1552-1566.</ref><ref name="Jiang5">Jiang, Bin and Sui, Daniel (2014). "A new kind of beauty out of the underlying scaling of geographic space", ''The Professional Geographer'', 66(4), 676–686</ref> It helps visualize big data, since big data are likely to show the scaling property of far more small things than large ones. The visualization strategy is to recursively drop out the tail parts until the head parts are clear or visible enough.<ref name="Jiang6">Jiang, Bin (2015). "Head/tail breaks for visualization of city structure and dynamics", ''Cities'', 43, 69 - 77.</ref> In addition, it helps delineate cities or natural cities to be more precise from various geographic information such as street networks, social media geolocation data, and nighttime images.

=== Characterizing the imbalance ===
As the head/tail breaks method can be used iteratively to obtain head parts of a data set, this method actually captures the underlying hierarchy of the data set. For example, if we divide the array (19, 8, 7, 6, 2, 1, 1, 1, 0) with the head/tail breaks method, we can get two head parts, i.e., the first head part (19, 8, 7, 6) and the second head part (19). These two head parts as well as the original array form a three-level hierarchy:

the 1st level (19),

the 2nd level (19, 8, 7, 6), and

the 3rd level (19, 8, 7, 6, 2, 1, 1, 1, 0).

The number of levels of the above-mentioned hierarchy is actually a characterization of the imbalance of the example array, and this number of levels has been termed as the ht-index.<ref name="Jiang2" /> With the ht-index, we are able to compare degrees of imbalance of two data sets. For example, the ht-index of the example array (19, 8, 7, 6, 2, 1, 1, 1, 0) is 3, and the ht-index of another array (19, 8, 8, 8, 8, 8, 8, 8, 8) is 2. Therefore, the degree of imbalance of the former array is higher than that of the latter array.
[[File:Natural_cities_of_Germany,_created_from_points_of_interest.jpg|thumb|250px|right|The left panel pattern contains 50,000 natural cities, which can be put into 7 hierarchical levels. It looks like a hair ball. Instead of showing all the 7 hierarchical levels, we show 4 top levels, by dropping out 3 low levels. Now with the right panel, the scaling pattern of far more small cities than large ones emerges. It is important to note that the right pattern (or the remaining part after dropping out the tails) is self-similar to the whole (or the left pattern). Thus the right pattern reflects the underlying structure of the left one, and enables us to see the whole.]]
[[File:Headtail breaks of American DEM.jpg|thumb|250px|The scaling pattern of US terrain surface is distorted by the natural breaks, but revealed by the head/tail breaks.]]

=== Delineating natural cities ===
The term ‘natural cities’ refers to the human settlements or human activities in general on Earth’s surface that are naturally or objectively defined and delineated from massive geographic information based on head/tail division rule, a non-recursive form of head/tail breaks.<ref name="Jiang7">Jiang, Bin and Miao, Yufan (2015). "The evolution of natural cities from the perspective of location-based social media", ''The Professional Geographer'', 67(2), 295 - 306.</ref><ref name="Long">Long, Ying (2016). "Redefining Chinese city system with emerging new data", ''Applied Geography'', 75, 36 - 48.</ref>  Such geographic information could be from various sources, such as massive street junctions <ref name="Long"/> and street ends, a massive number of street blocks, nighttime imagery and social media users’ locations etc. Distinctive from conventional cities, the adjective ‘natural’ could be explained not only by the sources of natural cities, but also by the approach to derive them. Natural cities are derived from a meaningful cutoff averaged from a massive amount of units extracted from geographic information.<ref name="Jiang6"/> Those units vary according to different kinds of geographic information, for example the units could be area units for the street blocks and pixel values for the nighttime images. A '''[http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d natural cities model]''' has been created using ArcGIS model builder,<ref name="Ren">Ren, Zheng (2016). "Natural cities model in ArcGIS", ''http://www.arcgis.com/home/item.html?id=47b1d6fdd1984a6fae916af389cdc57d''.</ref> it follows the same process of deriving natural cities from location-based social media,<ref name="Jiang7"/> namely, building up huge triangular irregular network (TIN) based on the point features (street nodes in this case) and regarding the triangles which are smaller than a mean value as the natural cities.

=== Color rendering DEM ===
Current color renderings for DEM or density map are essentially based on conventional classifications such as natural breaks or equal intervals, so they disproportionately exaggerate high elevations or high densities. As a matter of fact, there are not so many high elevations or high-density locations.<ref name="Jiang8">Jiang, Bin (2015). "Geospatial analysis requires a different way of thinking: The problem of spatial heterogeneity", ''GeoJournal'', 80(1), 1-13.</ref> It was found that coloring based head/tail breaks is more favorable than those by other classifications <ref name="Wu">Wu, Jou-Hsuan (2015). "Examining the new kind of beauty using the human being as a measuring instrument", ''http://www.diva-portal.org/smash/get/diva2:805296/FULLTEXT01.pdf''.</ref>

== Software implementations ==
The following implementations are available under [[Free and open-source software|Free/Open Source Software]] licenses.
* '''[https://github.com/digmaa/HeadTailBreaks HT calculator]''': a winform application for obtaining related metrics of head/tail breaks applying on a single data array.
* '''[http://jsfiddle.net/mhkeller/5yATK/ HT in JavaScript]''': a JavaScript implementation for applying head/tail breaks on a single data array.
* '''[http://fromto.hig.se/~bjg/axwoman/ HT Mapping tool]''': a function in the free plug-in Axwoman 6.3 to [[ArcMap]] 10.2 that conducts geo-data symbolization automatically based on the head/tail breaks classification.
* '''[https://github.com/chad-m/head_tail_breaks_algorithm HT in Python]''': Python and JavaScript code for the head/tail breaks algorithm. It's works great for choropleth map coloring.

==References==
{{one author|date=June 2014}}
{{Reflist}}

==Further reading==
{{Further reading cleanup|date=June 2014}}
* Lin, Yue (2013), A comparison study on natural and head/tail breaks involving digital elevation models. http://www.diva-portal.org/smash/get/diva2:658963/FULLTEXT02.pdf
* Wu, Jou-Hsuan (2015), The mirror of the self test: http://sharon19891101.wix.com/mirror-of-the-self

{{DEFAULTSORT:Head tail Breaks}}
[[Category:Data management]]
[[Category:Cartography]]
<=====doc_Id=====>:284
<=====title=====>:
ANSI 834 Enrollment Implementation Format
<=====text=====>:
{{Orphan|date=June 2014}}
Edi 834 files.
[[American National Standards Institute|ANSI]] 834 EDI Enrollment Implementation [[File format|Format]] is a standard format for electronically exchanging health plan enrollment data between employers and [[health insurance]] carriers. An 834 file contains a string of data elements and each data element represents a fact, such as a subscriber’s name, hire date, etc. The entire string is called a data segment. [[Health Insurance Portability and Accountability Act|The Health Insurance Portability and Accountability Act (HIPAA)]] requires that all health plans or health insurance carriers accept a standard enrollment format, ANSI 834A Version 5010. The ANSI 834A is the national standard for electronic enrollment and maintenance health plan.

The 834 is used to transfer enrollment information from the sponsor of the [[insurance]] coverage, benefits, or policy to a payer. The intent of this implementation guide is to meet the [[health care]] industry's specific need for the initial enrollment and subsequent maintenance of individuals who are enrolled in insurance products. This implementation guide specifically addresses the enrollment and maintenance of health care products only. One or more separate guides may be developed for life, flexible spending, and retirement products.

An example layout of an ANSI 834A Version 5010 file is shown below.

'''Sample File Output'''<br />
INS*Y*18*030*XN*A*E**FT~<br />
REF*OF*152239999~<br />
REF*1L*Blue~<br />
DTP*336*D8*20070101~<br />
NM1*IL*1*BLUTH*LUCILLE****34*152239999~<br />
N3*224 N DES PLAINES*6TH FLOOR~<br />
N4*CHICAGO*IL*60661*USA~<br />
DMG*D8*19720121*F*M~<br />
HD*030**VIS**EMP~<br />
DTP*348*D8*20111016~<br />
INS*N*19*030*XN*A*E***N*N~<br />
REF*OF*152239999~<br />
REF*1L*Blue~<br />
DTP*357*D8*20111015~<br />
NM1*IL*1*BLUTH*BUSTER~<br />
N3*224 N DES PLAINES*6TH FLOOR~<br />
N4*CHICAGO*IL*60661*USA~<br />
DMG*D**19911015*M-HD*030**VIS~<br />
DTP*348*D8*20110101~<br />
DTP*349*D8*20111015~

==See also==
* [[X12 Document List]]

==References==
{{reflist}}
* [http://getworkforce.com/ansi-834-file-layout/ "ANSI 834 File Layout"]
* [http://getworkforce.com/ansi-834-file-layout/ "Guardian Electronic User Guide 834 Enrollment and Maintenance"]
* [http://www.1edisource.com/transaction-sets?tset=834 "EDI 834 Benefit Enrollment and Maintenance"]

[[Category:Data management]]
<=====doc_Id=====>:287
<=====title=====>:
Personal, Inc.
<=====text=====>:
{{Infobox company |
| name=Personal, Inc.
| logo=[[File:Personal, Inc. logo.png]]
| type=[[Private company|Private]]
| foundation=2009
| location=[[Washington, D.C.]], US
| industry=[[Internet]]
| homepage={{URL|www.personal.com}}
}}

'''Personal''' (also referred to as Personal.com or Personal, Inc.) was a consumer [[Personal Data Service]] and [[identity management system]] for individuals to aggregate, manage and reuse their own data. It was re-launched in May 2016 as a collaborative data management and security solution for the workplace called TeamData.<ref>{{Cite web|url=http://www.prnewswire.com/news-releases/personalcom-becomes-teamdata-300275063.html|title=Personal.com Becomes TeamData|last=TeamData|website=www.prnewswire.com|access-date=2016-10-03}}</ref>

Personal's consumer products included: the Data Vault with Cloud Sync for secure management and sharing of data and documents between an individual and other individuals, companies, sites, apps and devices; and Data Imports to import information from third parties, including [[Social networking services|social media services]], companies and the [[United States Department of Education|U.S. Department of Education]], and  the Fill It App for automated completion of web and mobile forms, logins and checkouts.

The Personal platform supported user-centric [[DataPortability|data management and portability]] for over 1,200 different types (or fields) of structured, machine-readable, human-readable data. The platform also provided tools and APIs for developers and companies to integrate Fill It and the Data Vault into their websites and applications, primarily to give data back to their customers so they can autofill web and mobile forms.

==History==
Personal was founded in 2009 in [[Washington, DC]] by the management team that built The Map Network, which was acquired by [[Nokia]]/[[Navteq|NAVTEQ]] in 2006.<ref name=acquisition>{{cite web |url=http://www.directionsmag.com/pressreleases/navteq-announces-agreement-to-acquire-the-map-network/110396 |title=NAVTEQ Announces Agreement to Acquire The Map Network |date=6 December 2006 |website=Directions Magazine |accessdate=21 August 2014}}</ref> Founded in 1999, The Map Network (previously called URHere.com) built the first platform for places and events to create and distribute digital online and mobile maps, location data and content. The Map Network served as the official mapping solution for over 100 cities and thousands of events and venues, from the NFL Super Bowl to the Democratic and Republican National Conventions to the Smithsonian Institution. It also produced the most-used interactive map of 9/11 relief and rescue efforts.<ref name=acquisition /><ref>{{cite web |url=http://www.prnewswire.com/news-releases/interactive-relief-and-rescue-map-aids-in-nyc-response-72052587.html |title=Interactive Relief and Rescue Map Aids in NYC Response |date=17 September 2001 |website=PR Newswire |accessdate=25 August 2014}}</ref><ref>{{cite web |url=http://spatialnews.geocomm.com/dailynews/2001/sep/11/ |title=The Geospatial Industry's Response to Terrorism |date=11 September 2001 |website=GeoCommunity |accessdate=26 August 2014}}</ref>

Called a “life management platform” by [[The Economist]]<ref>{{cite web |url=http://www.economist.com/blogs/babbage/2011/11/personal-data |title=A life-management platform? |last=L. |first=G. |date=17 November 2011 |website=The Economist |accessdate=8 August 2014}}</ref> and a “personal encrypted cloud service” by TIME for its user-centric approach to data,<ref name="time.com">{{cite web|url=http://time.com/3069834/how-to-take-control-of-your-personal-data/|title=How to Take Control of Your Personal Data|last=Stokes|first=Natasha|date=1 August 2014|website=Time Inc.|accessdate=8 August 2014}}</ref> the company has been associated with both the [[Infomediary]] model originated in 1999 by [[John Hagel III]] and Mark Singer, as well as the [[Vendor relationship management|vendor relationship management (VRM)]] model developed by Doc Searls. Personal closed $7.6m in funding in December 2010, including [[Steve Case]]’s Revolution Ventures, Grotech Ventures, [[Allen & Company]], [[Ted Leonsis]], Neil Ashe and [[Jonathan Miller (businessman)|Jonathan Miller]].<ref>{{cite web |url=http://techcrunch.com/2011/01/06/personal-raises-7m-from-steve-case-and-others-to-help-consumers-protect-their-digital-data/ |title=Personal Raises $7M From Steve Case And Others To Help Consumers Protect Their Digital Data |last=Rao |first=Leena |date=6 January 2011 |website=TechCrunch |accessdate=8 August 2014}}</ref>

Personal was early to embrace “small data,” which it defines as “big data for the benefit of individuals.”<ref>{{cite web |url=http://blog.personal.com/2012/03/the-era-of-small-data-begins/ |title=The Era of Small Data Begins |last=Green |first=Shane |date=6 March 2012 |website=Personal |accessdate=20 August 2014 }}</ref> The term “small data” may have been originally coined by [[Jeremie Miller]] of Sing.ly, who mentioned it in a talk at the Web 2.0 Summit in November 2011 and is cited in ''The Intention Economy''.<ref>{{cite web |url=http://siliconprairienews.com/2011/11/watch-jeremie-miller-present-singly-at-the-web-2-0-summit/ |title=Watch Jeremie Miller present Singly at the Web 2.0 Summitt |first=Michael |date=9 November 2011 |website=Silicon Prairie News |accessdate=8 August 2014}}</ref> In 2011, Personal was a part of the first group of companies to join the [[Personal Data Ecosystem Consortium]]'s ''Startup Circle.''<ref>{{cite web |url=http://pde.cc/startup-circle/#2011 |title=Members of the PDEC Startup Circle |website=Personal Data Ecosystem Consortium |accessdate=20 August 2014}}</ref> A Small Data [[Meetup (website)|Meetup]] group has also formed in New York City, bringing together technology, legal and business experts to exchange ideas about user-centric and user-driven models for internet products and services.<ref>http://www.meetup.com/smalldata/</ref> Personal ultimately raised $24 million, including $4.5m from Bill Miller of [[Legg Mason]] and [[Esther Dyson]] of EDventures in October 2013.<ref>{{cite web |url=http://www.reuters.com/article/2013/10/15/idUS412005883920131015 |title=Personal raises $4.5 million to be the personal data vault we so desperately need |date=15 October 2013 |website=Reuters |accessdate=8 August 2014}}</ref>

==Products and services ==

===Overview===
The Personal Platform was a privacy- and security-by-design platform for individuals to manage and reuse their own data and information. The Fill It app was a 1-click form-filling solution for web and mobile logins, checkouts and forms, and the Data Vault app served as the main cloud-based repository for a user's data. Personal helped individuals take control and benefit from their information while knowing that the information in their Data Vault remained legally theirs and could not be used without their permission.<ref>{{Cite web|url=http://www.zdnet.com/article/intel-execs-on-big-data-and-privacy-its-a-balancing-act/|title=Intel execs on big data and privacy: It's a balancing act {{!}} ZDNet|last=King|first=Rachel|website=ZDNet|access-date=2016-10-03}}</ref>

===Data Vault with Cloud Sync===
Personal spent two years building the Personal Platform before launching its Data Vault product in beta in November 2011. Following [[Privacy by Design]] principles, Personal only enabled users to see or share the sensitive data and all the files they stored in their Data Vault. Such information was encrypted, and could only be decrypted with a user’s password. Only users could choose and know their passwords to their vault because Personal did not store user passwords – and therefore could not reset them without deleting a user’s sensitive data and all files stored in their vault.<ref>{{cite web |url=http://www.ipc.on.ca/images/Resources/pbd-pde.pdf |title=Privacy by Design and the Emerging Personal Data Ecosystem |last=Cavoukian |first=Ann |last2=Green |first2=Shane |date=October 2012 |website=Office of the Information and Privacy Commissioner |accessdate=8 August 2014}}</ref> All Personal apps and services were linked to a user’s private Data Vault.

The Data Vault featured automatic synchronization of data and files added on any device logged into Personal. It also featured a “Secure Share” function that created a live, private network, allowing registered users to share access to data and files through an exchange of encrypted keys without the risk of transmitting the data or files through non-secure, direct means. It also allowed users to immediately update data across their own network and revoke access to it when they choose.

Personal launched its [[Android (operating system)|Android]] app on November 30, 2011.<ref name=mashable>{{cite web |url=http://mashable.com/2011/11/17/personal/ |title=Never Fill Out a Form Again? Personal Seeks to Be the Data Vault for Your Private Information |last=Parr |first=Ben |date=17 November 2011 |website=Mashable |accessdate=8 August 2014}}</ref><ref>{{cite web |url=https://www.personal.com/s/pages/news/personal-android-release/ |title=Personal Releases Android App for Its Private, Personal Network and Data Vault Service |date=30 November 2011 |website=Personal, Inc. |accessdate=8 August 2014}}</ref> The [[iOS]] Data Vault app was released on May 7, 2012.<ref>{{cite web |url=http://techcrunch.com/2012/05/07/personal-takes-its-secure-vault-for-all-of-your-private-digital-data-mobile-with-ios-app/ |title=Personal Takes Its Secure Vault For All Of Your Private, Digital Data Mobile WIth iOS App |last=Rao |first=Leena |date=7 May 2012 |website=TechCrunch |accessdate=8 August 2014}}</ref> Personal officially launched its [[Application programming interface|application programming interface (APIs)]] on October 2, 2012 at the Mashery Business of APIs Conference.<ref>{{cite web |url=https://www.personal.com/s/pages/news/personal-launches-personal-platform/ |title=Personal Launches 'Personal Platform at Business of APIs Conference, Opening APIs for Developers |date=2 October 2012 |website=Personal, Inc. |accessdate=8 August 2014}}</ref> A review by [[CNET]] highlighted the challenges of getting people to trust such a new service with their sensitive data and spending the time required entering enough data to make it useful.<ref>{{cite web |url=http://www.cnet.com/news/what-hump-personals-private-database-faces-challenges/ |title=What hump? Personal's private database faces challenges |last=Needleman |first=Rafe |date=30 November 2011 |website=CNET |accessdate=8 August 2014}}</ref>

===Fill It App and Form Index===
When the Data Vault was launched in November 2011, ''[[Mashable]]'' posed the question: “Never Fill Out a Form Again?”<ref name=mashable /> The [[World Economic Forum]] in its February 2013 report highlighted the possibility of saving 10 billion hours globally “and improv[ing] the delivery of public and private sector services” through automated form-filling tools, specifically citing Personal’s Fill It app.<ref>{{cite web |url=http://www3.weforum.org/docs/WEF_IT_UnlockingValuePersonalData_CollectionUsage_Report_2013.pdf |title=Unlocking the Value of Personal Data: From Collection to Usage |date=February 2013 |website=World Economic Forum |accessdate=8 August 2014}}</ref> In January 2013, Personal launched Fill It in beta as a web bookmarklet for automatic form-filling.<ref>{{cite web |url=http://www.digitaltrends.com/web/personal-coms-new-fill-it-feature-makes-quick-work-of-long-web-forms/#!bBp6iJ |title=PERSONAL.COM'S NEW FILL IT APP MAKES QUICK WORK OF LONG ONLINE FORMS |last=Couts |first=Andrew |date=16 January 2013 |website=Digital Trends |accessdate=8 August 2014}}</ref>

On June 11, 2014, Personal released Fill It as a web extension and announced that it was publishing an index of over 140,000 1-click online forms at www.fillit.com.<ref name=extensionlaunch>{{cite web |url=http://tech.co/dc-based-startup-personal-launches-fill-it-for-quick-and-safe-auto-filling-on-online-forms-2014-06 |title=DC-Based Personal Launches Fill It for Quick and Safe Auto-Filling on Online Forms |last=Barba |first=Ronald |date=16 June 2014 |website=Tech Cocktail |accessdate=8 August 2014}}</ref> The company also announced that a mobile version of the product will launch later in the year. According to a story in ''[[Tech Cocktail]]'' about the launch, Personal’s “web extension and mobile app are able to support over 1,200 different types of reusable data, even enabling them to unlock more confidential information so they can complete longer forms, including patient registrations, job applications, event registrations, school admissions, insurance and bank applications, and government forms.”<ref name=extensionlaunch /> In November 2014, a mobile version of Fill It was launched that could autofill mobile forms using APIs.<ref>{{Cite web|url=http://tech.co/personal-launches-fill-it-mobile-2014-11|title=Personal Launches Fill It Mobile at #pii2014|date=2014-11-13|language=en-US|access-date=2016-10-03}}</ref>

Personal’s form portal ultimately indexed more than 500,000 forms with three components, which, together, allowed data to be captured and reused across any of the forms: (1) a form graph, which mapped individual form fields to the Personal ontology; (2) a semantic layer, which determined how data was required on a form (e.g. one field vs. three fields for a U.S. telephone number); and (3) a correlations graph, which helped individuals match their specific data to a form without looking at the data value (e.g. knowing which phone number is a mobile phone number, which address is a billing address, or that a person uses their middle name as a first name on most forms).<ref>{{cite web |url=http://tech.co/dc-startup-personal-university-data-privacy-security-2014-08 |title=Personal Launches "Personal University," a Video Series on Data Privacy and Security |last=Barba |first=Ronald |date=8 August 2014 |website=Tech Cocktail |accessdate=8 August 2014}}</ref>

===Monetizing personal data===
With the [[initial public offering]] of [[Facebook]] in May 2012, there was media interest in the question of the monetary value of personal data and whether tools and services might emerge to help consumers monetize their own data. Personal was frequently cited as a company that could potentially offer such a service. Articles and pieces focusing on this subject have appeared in ''[[The New York Times]]'', ''[[AdWeek]]'', the ''[[MIT Technology Review]]'', and on ''[[CNN]]'' and ''[[National Public Radio]]''.<ref>{{cite web |url=http://www.nytimes.com/2012/02/13/technology/start-ups-aim-to-help-users-put-a-price-on-their-personal-data.html?_r=0 |title=Start-Ups Seek to Help Users Put a Price on Their Personal Data |last=Brustein |first=Joshua |date=12 February 2012 |website=The New York Times |accessdate=8 August 2014}}</ref><ref>{{cite web |url=http://www.technologyreview.com/view/426235/is-personal-data-the-new-currency/ |title=Is Personal Data the New Currency? |last=Zax |first=David |date=30 November 2011 |website=MIT Technology Review |accessdate=8 August 2014}}</ref><ref>{{cite web |url=http://edition.cnn.com/2012/02/24/tech/web/owning-your-data-online |title=Manage (and make cash with?) your data online |last=Gross |first=Doug |date=27 February 2012 |website=CNN |accessdate=8 August 2014}}</ref> Company Co-founder and CEO Shane Green was quoted as saying that “the average American consumer would soon be able to realize over $1,000 per year” by granting limited, anonymous access to their data to marketers, but that figure was never supported by Green or the company.<ref>{{cite web |url=http://www.ft.com/cms/s/2/61c4c378-60bd-11e2-a31a-00144feab49a.html#axzz39ptB1It4 |title=Data mining offers rich seam |last=Palmer |first=Maija |date=18 February 2013 |website=Financial Times |accessdate=8 August 2014}}</ref>

==Reception and Re-launch as TeamData==
Personal was the first online consumer-facing company to be named an Ambassador for [[Privacy by Design]] for its technical, business and legal commitments to providing users with control over the data they store in Personal’s service.<ref>{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/personal-com/ |title=Personal.com |website=Privacy by Design |accessdate=15 August 2014}}</ref><ref>{{cite web |url=http://www.privacybydesign.ca/content/uploads/2010/03/2011-10-24-Personal.com_.pdf |title=Personal and Privacy by Design |website=Privacy by Design |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/joshua-p-galper/ |title=Joshua P. Galper |website=Privacy by Design |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://www.privacybydesign.ca/index.php/ambassador/shane-green/ |title=Shane Green |website=Privacy by Design |accessdate=20 August 2014}}</ref> The company received recognition for its user agreement, called the Owner Data Agreement,<ref>{{cite web |url=https://www.personal.com/owner-data-agreement/ |title=Owner Data Agreement |date=7 February 2014 |website=Personal, Inc. |accessdate=8 August 2014}}</ref> which acted like a reverse license agreement when data was shared between registered parties and emphasized that data ownership resides with the user. [[Doc Searls]] wrote in ''[[The Intention Economy: When Customers Take Charge]]'' that the Owner Data Agreement “had no precedent and modeled a new legal position, both for vendors and for intermediaries.”<ref>{{cite book |last=Searls |first=Doc |date=May 1, 2012 |title=The Intention Economy: When Customers Take Charge |publisher=Harvard Business Review Press |page=186 |isbn=978-1422158524 }}</ref> 
[[Fast Company (magazine)|Fast Company]] called the Data Vault “a tool that will simplify our lives.”<ref>{{cite web |url=http://www.fastcompany.com/1836521/personalcom-creates-online-vault-manage-all-your-data |title=PERSONAL.COM CREATES AN ONLINE VAULT TO MANAGE ALL YOUR DATA |last=Boyd |first=E.B. |date=7 May 2012 |website=Fast Company |accessdate=8 August 2014}}</ref> Personal has been included in case studies by Ctrl-Shift and Forrester regarding Personal Data Stores and Personal Identity Management.<ref>{{cite web |url=https://www.ctrl-shift.co.uk/index.php/research/product/64 |title=Personal Data Stores |website=Ctrl-Shift |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://blog.personal.com/uploads/2011/10/Forrester-Research-personal_identity_management.pdf |last=Khatibloo |first=Fatemeh |last2=Frankland |first2=Dave |last3=Maler |first3=Eve |last4=Smith |first4=Allison |date=30 September 2011 |title=Personal Identity Management |website=Forrester |accessdate=20 August 2014}}</ref>

In 2011, Personal received the Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) and participated in the Technology Showcase at pii2012.<ref>{{cite web |url=http://www.prweb.com/releases/2011/5/prweb8484188.htm |last=Fonseca |first=Natalie |title=Personal and Passtouch Receive Innovator Spotlight Award at Privacy Identity Innovation Conference (pii2011) |website=PRWeb |accessdate=20 August 2014}}</ref><ref>{{cite web |url=https://www.privacyidentityinnovation.com/pii2012-seattle/pii2012-technology-showcase |title=pii2012 Technology Showcase |website=Privacy Identity Innovation |accessdate=20 August 2014}}</ref> In 2012, TechHive named Personal as one of the top five apps or web services of [[SXSW]].<ref>{{cite web |url=http://www.techhive.com/article/251744/hot_apps_and_web_services_of_sxsw.html |last=Sullivan |first=Mark |date=13 March 2012 |title=Hot Apps and Web Services of SXSW |accessdate=20 August 2014}}</ref> Personal won the 2013 Campus Technology Innovators Award with Lone Star College in July 2013.<ref>{{cite web |url=http://campustechnology.com/articles/2013/07/23/2013-innovators-awards.aspx |last=Raths |first=David |last2=Namahoe |first2=Kanoe |last3=Lloyd |first3=Meg |date=23 July 2013 |title=2013 Innovators Awards |website=Campus Technology |accessdate=20 August 2014}}</ref> Personal was included in a list of Executive Travel Magazine's favorite travel apps for 2013 in its May/June issue.<ref>{{citation |url=http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |last=Null |first=Christopher |title=ET's Favorite Travel Apps of 2013 |archiveurl=https://web.archive.org/web/20131023184535/http://www.executivetravelmagazine.com/articles/ets-favorite-travel-apps-for-2013 |archivedate=2013-10-23 |accessdate=20 August 2014}}</ref>
In 2013, Personal was also included as part of NYU GovLab's Open Data 500 and was named by J. Walter Thompson as one of 100 things to watch for in 2014.<ref>{{cite web |url=http://www.opendata500.com/us/Personal-Inc/ |title=Personal, Inc. |website=Open Data 500 |accessdate=20 August 2014}}</ref><ref>{{cite web |url=http://www.jwtintelligence.com/2013/12/100-watch-2014/#axzz2qyBVCrMs |last=Mack |first=Ann |date=26 December 2013 |title=100 Things to Watch in 2014 |website=JWT Intelligence |accessdate=20 August 2014}}</ref> In 2015, the National Law Journal named Company Chief Policy Officer and General Counsel, Joshua P. Galper, as one of their 50 "Cybersecurity & Privacy Trailblazers."<ref>{{Cite web|url=http://pdfserver.amlaw.com/nlj/flipbook/Cybersecurity_Trailblazers_2015/Cyber_Security_Trailblazers_2015_Web.html#p%253D14%2523p=8|title=Cybersecurity_Trailblazers|website=pdfserver.amlaw.com|access-date=2016-10-03}}</ref>

In May 2016, Personal Co-Founder and CEO Shane Green announced the launch of TeamData with one of the other co-founders, Tarik Kurspahic, and new board chair [[Eric C. Anderson]]. TeamData focuses on the problem of securing and collaboratively managing data in the workplace, and is based on the technology and platform of Personal.<ref>{{Cite web|url=https://medium.com/@shanegreen/why-personal-com-graduated-to-teamdata-today-f75e0d539ba1#.yrkhukyec|title=Why Personal.com "graduated" to TeamData today|last=Green|first=Shane|date=2016-05-20|access-date=2016-10-03}}</ref> Onboardly included the new collaborative TeamData solution in its list of "Top 10 apps to keep your team on track" and as part of its Top 50 list of "all time best content marketing tools."<ref>{{Cite web|url=http://onboardly.com/content-marketing/all-time-best-tools-for-content-marketing-teams/|title=All Time Best Tools for Content Marketing Teams via @Onboardly|date=2016-04-07|language=en-US|access-date=2016-10-03}}</ref>

==References==
{{reflist|2}}

==External links==
*{{Official website}}
**{{URL|www.fillit.com|Fill It homepage}}
*{{ITunes Preview App|493536192|Personal}}
*{{ITunes Preview App|910517122|Fill It}}
*{{Google Play|com.personal.android|Personal}}
*{{Google Play|com.personal.fillit|Fill It}}
*[http://tech.co/tag/personal Personal] collected news and commentary at ''[[Tech Cocktail]]''
*{{Crunchbase|Personal|Personal}}

[[Category:American websites]]
[[Category:Android (operating system) software]]
[[Category:Companies based in Washington, D.C.]]
[[Category:Companies established in 2009]]
[[Category:Data management]]
[[Category:Data security]]
[[Category:Internet companies of the United States]]
[[Category:Internet properties established in 2009]]
[[Category:IOS software]]
<=====doc_Id=====>:290
<=====title=====>:
AnalytiX DS
<=====text=====>:
{{Use dmy dates|date=September 2016}}
{{Use Indian English|date=September 2016}}
{{Orphan|date=September 2014}}

{{Infobox company
| name             =AnalytiX DS
| logo             =[[File:AnalytiX DS Logo.png]]
| type             =[[Private limited company|Private]]
| location_city    =[[Chantilly, Virginia|Chantilly]], [[Virginia]]
| location_country =[[United States]]
| founder          ={{Unbulleted list|Mike Boggs|}} 
| foundation       =2006 
| area_served      =Worldwide
| key_people       =Madan K. ([[CEO]])<br />Mike Boggs ([[Chief Technology Officer|CTO]] & Founder)<br/>
Sam Benedict (VP-Sales & Marketing)<br/> John Carter (Director of Professional Services)
| industry         =[[Software Company]]
| services         =IT, [[business consulting]] and automation services
| homepage         ={{URL|www.analytixds.com}}
| intl             =yes
}}
''' AnalytiX Data Services '''  is a software vendor that provides specialized data mapping and ETL conversion tools for [[data integration]], [[data management]], [[enterprise application integration]] and [[big data]] software and services.<ref>{{cite web|title=Mapping Manager, the missing link in moving data around|url=http://www.bloorresearch.com/analysis/analytix-mapping-manager-missing-link-moving-data|publisher=Bloor|accessdate=17 September 2014}}</ref>  Headquarter's based in Chantilly, Virginia, AnalytiX DS has offices in Dallas, TX and Hyderabad, India and an international network of technical and services partners.<ref>{{cite web|title=AnalytiX DS Partners|url=http://analytixds.com/technology-partners/}}</ref>

Michael Boggs, the CTO and founder of AnalytiX DS coined the term "Pre ETL Mapping" now it is widely used and accepted synonym for design phase of data integration. 

Mapping Manager is the flagship product of AnalytiX DS, an agile Unified Platform designed to govern and accelerate data integration processes by eliminating manual processes and replacing them with software designed to automate, govern and accelerate manual processes.<ref>{{cite web|title=Mapping Manager|url=http://analytixds.com/amm/}}</ref>  Several versions of Mapping Manager were released since the release of its first version in 2006. 

AnalytiX DS in April 2016 launched Mapping Manager Version 7.0, a major release version which extends the unified platform for enterprise data mapping, governance and automation.<ref>{{cite web|title=AnalytiX Data Services announces the launch of major release of Mapping Manager Version 7.0|url=http://www.pr.com/press-release/680901|publisher= PR |accessdate= 27 July 2016}}</ref>The latest release has added plenty of features and revolutionary modules never before seen.

With every new release, out of box features and functionalities were introduced. Later, AnalytiX DS began to add new modules including Release Management, Reference Data Manager, Code Set Manager, CATfX, LiteSpeed Conversion, Code automation Templates for Data Vault, Mapping Manager Big Data Edition, Data Quality Assessment Manager(DQAM), Metadata Management, Data Vault-Code Gen Bundle, and Test Manager, which extends the tools capabilities above and beyond management of the data mapping process.

[[Mapping Manager Big Data Edition]]:<ref>{{cite web|title=Mapping Manager Big Data Edition|url=http://analytixds.com/mapping-manager-bigdata-edition/}}</ref>  Helps automate the big data mapping process and provides a bridge between structured and unstructured data to meet big data challenges.

[[Release Management]]:<ref>{{cite web|title=AnalytiX DS Release Manager|url=http://analytixds.com/arm/}}</ref> Helps track the release process approvals, audits and verifications through the approval process.

[[Reference Data Manager]]:<ref>{{cite web|title=AnalytiX DS Reference Data Manager|url=http://analytixds.com/rdm/}}</ref>  Helps create database like structure to maintain all reference data. 

[[Code Set Manager]]:<ref>{{cite web|title=AnalytiX DS CodeSet Manager|url=http://analytixds.com/code-set-manager/}}</ref>  Helps drive the organization of user defined reference data and Code Sets across an enterprise.

[[Customizable Code-Automation Framework (CATfX)]]:<ref>{{cite web|title=AnalytiX DS Customizable Code-Automation Framework (CATfX)|url=http://analytixds.com/catfx-code-automation-templates/}}</ref>  Helps automate manual coding and tasks for ETL integration and data profiling, Testing Automation and more.

[[LiteSpeed Conversion]]:<ref>{{cite web|title=AnalytiX DS LiteSpeed Conversion|url=http://analytixds.com/litespeed-conversion/}}</ref>  Helps automate the conversion of ETL tool platforms through an automated framework.

[[Code automation Templates for Data Vault]]:<ref>{{cite web|title=AnalytiX DS Code automation Templates for Data Vault|url=http://analytixds.com/data-vault-automation/}}</ref> Helps automatically generate the Hub ETL code, Link ETL code and the Satellite ETL code through your existing ETL Platform. 

[[Data Quality Assessment Manager(DQAM)]]:<ref>{{cite web|title=AnalytiX DS Data Quality Assessment Manager(DQAM)|url=http://analytixds.com/data-quality-assessment-manager/}}</ref>  Helps standardize and execute a formal data quality assessment methodology. 

[[Metadata Management]]:<ref>{{cite web|title=AnalytiX DS Metadata Management|url=http://analytixds.com/metadata-management/}}</ref> Helps metadata in Mapping Manager Unified Platform to be ported into third party metadata environments. 

[[Data Vault-Code Gen Bundle]]:<ref>{{cite web|title=AnalytiX DS Data Vault-Code Gen Bundle|url=http://analytixds.com/data-vault-code-gen-bundle/}}</ref>  Helps automate the code-generation process for building the Data Vault through Code automation Templates (CAT's). 

[[Test Manager]]:<ref>{{cite web|title=AnalytiX DS Test Manager|url=http://analytixds.com/test-manager/}}</ref> Helps Test Cases and Test SQL generation to be managed in a purpose built module for testing data mappings and ETL processes. 

Recently, AnalytiX DS expanded its presence in the US with the opening of a new office in Dallas. 

AnalytiX DS has been named to CIO Review's 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015.<ref>{{cite web|title=CIO Review's 20 Most PROMISING PRODUCTIVITY TOOLS SOLUTION PROVIDERS 2015|url=http://www.pr.com/press-release/662862|publisher= PR |accessdate= 18 March 2016}}</ref>AnalytiX DS is a platinum sponsor for WWDVC 2016.<ref>{{cite web|title=AnalytiX Data Services To Platinum-Sponsor for WWDVC 2016|url=http://wwdvc.com/sponsors/}}</ref>


== References ==
{{reflist}}

==External links==
* {{official website|http://www.analytixds.com/}}
* {{YouTube|u=AnalytiXDS|AnalytiX DS}}
* {{Facebook|AnalytiX.Data.Services|AnalytiX DS}}

[[Category:Software companies of India]]
[[Category:Data management]]
[[Category:Extract, transform, load tools]]
[[Category:Data mapping]]
[[Category:Data warehousing products]]
[[Category:International information technology consulting firms]]
[[Category:Multinational companies headquartered in the United States]]
<=====doc_Id=====>:293
<=====title=====>:
BBC Genome Project
<=====text=====>:
[[File:BBC Genome Logo.png|thumb|BBC Genome Logo]]

The '''BBC Genome Project''' is a digitised searchable database of programme listings from the [[Radio Times]] from the first issue in 1923, to 2009.<ref name=About>{{cite web|title=About this project|url=http://genome.ch.bbc.co.uk/about|publisher=[[BBC]]|accessdate=21 October 2014}}</ref> 

==History==
===Prior===
BBC Genome is not the BBC's first online searchable database; in April 2006 the BBC gave the public access to Infax, the BBC's programme database. Infax contained around 900,000 entries, but not every programme ever broadcast, and it ceased operation in December 2007.<ref name="About Infax">{{cite web|title=About This Prototype|url=http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|publisher=[[BBC]]|accessdate=2 February 2016|archiveurl=https://web.archive.org/web/20060613100552/http://open.bbc.co.uk/cataloguemeta/2005/11/about_this_prototype.html|archivedate=13 June 2006}}</ref> The front page of the website is still available to see via the [[Internet Archive]] [https://web.archive.org/web/20060512054648/http://open.bbc.co.uk/catalogue/infax here.] After Infax ceased, a message on the website said that it would be incorporating in the information into individual programme pages.<ref name="Prototype End">{{cite web|title=This experimental prototype trial has now concluded|url=http://www.bbc.co.uk/archive/catalogue_offline.shtml|publisher=[[BBC]]|accessdate=2 February 2016}}</ref> In 2012, it was replaced by the database Fabric but this is only for internal use within the BBC.

==''Radio Times''== 
[[File:BBC Genome OCR error.jpg|thumb|Screenshot of an OCR error (since corrected) in Genome. The text, "Uza TarbuclC's Christmas", should read "[[Liza Tarbuck]]'s Christmas".]]
In December 2012, the [[BBC]] completed a digitisation exercise, scanning the listings from [[Radio Times]] of all BBC programmes 1923-2009 from an entire run of about 4,500 copies of the magazine.<ref name="Kelion">{{cite web|url=http://www.bbc.co.uk/news/technology-20625884|title=BBC finishes Radio Times archive digitisation effort|last=Kelion|first=Leo|work=[[BBC Online]]|accessdate=20 January 2013}}</ref> They identified around five million programmes, involving 8.5 million actors, presenters, writers and technical staff.<ref name="Kelion" /> The listings are as published, in advance, and so do not include late changes or cancellations.

The issues were scanned at high resolution, producing [[TIF]] images and [[Optical Character Recognition]] (OCR) was then used to turn the text from the page into searchable text on the Genome database.<ref name=About/>

BBC Genome was released for public use on 15 October 2014.<ref name="Hilbish">{{cite web|url=http://www.bbc.co.uk/blogs/aboutthebbc/posts/Genome-The-Radio-Times-Archive-is-now-live|title=Genome – Radio Times archive now live|last=Bishop|first=Hilary|work=[[BBC Online]]|accessdate=15 October 2014}}</ref><ref>{{cite news|last=Sweney|first=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|newspaper=Guardian|date=16 October 2014}} </ref>

The aim of this project is to allow researchers to be able to find out information easier and to help [[BBC Archives]] to build up a picture of what exists and what is currently missing from the archive.<ref>{{cite web|title=BBC's Genome Project offers radio and TV archive listings|url=http://www.bbc.co.uk/news/technology-29643662|publisher=[[BBC]]|accessdate=21 October 2014|date=16 October 2014}}</ref><ref>{{cite web|last1=Sweney|first1=Mark|title=BBC digitises Radio Times back issues|url=https://www.theguardian.com/media/2014/oct/16/bbc-digitises-radio-times-back-issues-genome-project|publisher=[[The Guardian]]|accessdate=21 October 2014|date=16 October 2014}}</ref> Corrections to OCR errors and changes to advertised schedules are being [[Crowdsourcing|crowdsourced]],<ref name="Hilbish" /> with over 180,000 user generated edits accepted as of January 2017. <ref>{{Cite web|url=http://genome.ch.bbc.co.uk/schedules/bbcone/london/1964-04-20|title=BBC One London - 20 April 1964 - BBC Genome|website=genome.ch.bbc.co.uk|access-date=2017-01-09}}</ref> 

Each listing entry has a unique identifier which may be expressed as a URL. For example, the very first screening of ''[[Doctor Who]]'' is http://genome.ch.bbc.co.uk/8f81c193ba224e84981f353cae480d49 A broadcast programme may have more than one such identifier, if it was screened (and thus listed) on repeat occasions.

==See also==
*[[BBC Archives]]
* [[Timeline of the BBC]]

==References==
{{Reflist|2}}

==External links==

{{Wikidata property|P1573|BBC Genome identifiers}}

*[http://genome.ch.bbc.co.uk/ BBC Genome website]
*[http://www.bbc.co.uk/blogs/genome BBC Genome blog] 
*[http://twitter.com/bbcgenome/ BBC Genome on Twitter] 
*[http://www.facebook.com/bbcgenome/ BBC Genome on Facebook]

{{BBC}}

[[Category:BBC]]
[[Category:BBC New Media|Archives]]
[[Category:Data management]]
[[Category:Broadcasting websites]]
[[Category:British websites]]
[[Category:History of television in the United Kingdom]]
[[Category:History of radio]]
[[Category:BBC history]]
[[Category:Databases in the United Kingdom]]
<=====doc_Id=====>:296
<=====title=====>:
Rtolap
<=====text=====>:
{{multiple issues|
{{Unreferenced|date=October 2008}}
{{Original research|date=October 2008}}
}}

==RTOLAP - Real Time OLAP==

Whilst many [[OLAP]] Servers like [[Microsoft Analysis Services]] store pre-calculating consolidations and calculated elements to achieve rapid response times. A Real Time OLAP Server will calculate the values on the fly, when they are required. 
The essential characteristic of RTOLAP system is in holding all the data in RAM.

It is a protocol which analyzes fly values when required. It saves every bit of information in RAM. The calculations are executed in a “right-away” manner which reduces the setback linked with “information outburst” since it only saves information under the RAM size standard.

== Advantages ==
* Since precalculated values aren't stored, the size of a cube in an RTOLAP system is smaller than of an OLAP product which resorts to precalculation. RTOLAP often reduces the problem which may be associated with "Data explosion", by  means of storing less data. 
* RTOLAP essentially performs calculations "just-in-time" by only calculating values when they are needed space can be saved, since in a precalculated system, a great deal of calculations will be stored which may well never be called up.
* Incremental updates are available once they are loaded, and any modifications to data will flow through the system immediately. With RTOLAP when a change is made, everyone sees the result. This isn't a unique characteristic of RTOLAP, since other OLAP systems (e.g. [[SAS Institute]], [[Microsoft Analysis Services]], [[MicroStrategy]]) behave the same way.

== Disadvantages ==
* Since RTOLAP stores the entire cube in RAM, it doesn't scale to the data volumes larger than the RAM size
* Performance of queries can be slower since the values need to be calculated on the fly instead of being accessed from the precalculated storage

[[Category:Data management]]
[[Category:Information technology management]]
[[Category:Online analytical processing]]
<=====doc_Id=====>:299
<=====title=====>:
Category:Object storage
<=====text=====>:
[[Category:Data management]]
<=====doc_Id=====>:302
<=====title=====>:
SciDB
<=====text=====>:
{{Infobox software
| title       = SciDB
| developer   = Paradigm4
| publisher   =
| genre       = Database management system
| license     = [[Affero General Public License|AGPL]] v3<ref>[http://www.scidb.org/forum/viewtopic.php?f=16&t=364 Download & licensing]</ref>
| first       =
| last        =
| coauthors   =
| released    = 2008
| website     = {{URL|http://www.paradigm4.com/}}
}}
{{Distinguish2|[http://scidb.sourceforge.net/ Scidb: Chess database]}}

'''SciDB'''<ref>{{cite web | url = http://www.scidb.org/ | title=SciDB website}}</ref> is an array database designed for multidimensional data management and analytics common to scientific, geospatial, financial, and industrial applications.  It is developed by Paradigm4, co-founded by [[Michael Stonebraker]].

==History and Characteristics==
[[Michael Stonebraker]] co-created SciDB where, he claims, arrays are 100 or so times faster than a RDBMS on this class of problem.<ref>{{cite web | url = http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview | title=Stonebraker interview}}</ref> It is swapping rows and columns for mathematical arrays that put fewer restrictions on the data and can work in any number of dimensions unlike the conventionally widely used [[relational database management system]] model, in which each [[Relation (database)|relation]] supports only one dimension of records.

According to a Strata Conference presentation on SciDB,<ref>{{cite web |url=http://strataconf.com/stratany2011/public/schedule/detail/21376 |title=Big Data and Big Analytics: SciDB is not Hadoop}}</ref> it natively supports:
* An array data model for efficient storage and manipulation of larger-than-memory multi-dimensional arrays.
* Data versioning and provenance to allow tracking results back to original supporting data.
* What-if modeling, back-testing, and re-analysis.
* Massive scale math on the arrays for linear algebra and analytics.
* Uncertainty can be modeled by associating error-bars with data.
* Efficient storage.

==See also==
<!-- Please do not list specific implementations here! -->
* [[Comparison of object database management systems]]
* [[Comparison of structured storage software]]

== References ==
{{Reflist|2}}

==External links==
* [http://www.theregister.co.uk/2010/09/13/michael_stonebraker_interview/ Michael Stonebraker interview]
* [http://itknowledgeexchange.techtarget.com/soa-talk/stonebraker-sees-high-programming-overhead-for-nosql/ Stonebraker sees high programming overhead for NoSQL]

[[Category:Data management]]
[[Category:Distributed data stores]]
[[Category:Document-oriented databases]]
[[Category:Distributed computing architecture]]
[[Category:Free database management systems]]
[[Category:Structured storage]]
[[Category:NoSQL]]
[[Category:Software using the GNU AGPL license]]
<=====doc_Id=====>:305
<=====title=====>:
Tuple
<=====text=====>:
{{About|the mathematical concept|the musical term|Tuplet}}
{{Redir|Octuple|the type of rowing boat|Octuple scull}}
{{Redir|Duodecuple|the term in music|Twelve-tone technique}}

A '''tuple''' is a finite ordered list of [[Element (mathematics)|elements]].  In [[mathematics]], an '''{{math|''n''}}-tuple''' is a [[sequence]] (or ordered list) of {{math|''n''}} elements, where {{math|''n''}} is a non-negative integer. There is only one 0-tuple, an empty sequence. An {{math|''n''}}-tuple is [[Recursive definition|defined inductively]] using the construction of an [[ordered pair]]. Tuples are usually written by listing the elements within parentheses "<math>(\text{ })</math>" and separated by commas; for example, <math>(2, 7, 4, 1, 7)</math> denotes a 5-tuple. Sometimes other symbols are used to surround the elements, such as square brackets "[ ]" or angle brackets "< >". Braces "{ }" are only used in defining arrays in some programming languages such as [[Java (programming language)|Java]], but not in mathematical expressions, as they are the standard notation for [[set (mathematics)|sets]]. Tuples are often used to describe other mathematical objects, such as [[Vector (mathematics and physics)|vectors]]. In computer science, tuples are directly implemented as [[product type]]s in most [[functional programming|functional programming languages]].{{citation needed|date=January 2016}} More commonly, they are implemented as [[Record (computer science)|record types]], where the components are labeled instead of being identified by position alone.{{citation needed|date=January 2016}} This approach is also used in [[relational algebra]]. Tuples are also used in relation to programming the [[semantic web]]  with the [[Resource Description Framework]] (RDF). Tuples are also used in [[linguistics]]<ref>{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199202720.001.0001/acref-9780199202720-e-2276|title=N‐tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}</ref> and [[philosophy]].<ref>{{cite web|url=http://www.oxfordreference.com/view/10.1093/acref/9780199541430.001.0001/acref-9780199541430-e-2262|title=Ordered n-tuple - Oxford Reference|work=oxfordreference.com|accessdate=1 May 2015}}</ref>

==Etymology==
The term originated as an abstraction of the sequence: single, double, triple, quadruple, quintuple, sextuple, septuple, octuple, ..., {{math|''n''}}‑tuple, ..., where the prefixes are taken from the [[Latin]] names of the numerals. The unique 0‑tuple is called the null tuple. A 1‑tuple is called a singleton, a 2‑tuple is called an ordered pair and a 3‑tuple is a triple or triplet. {{math|''n''}} can be any nonnegative [[integer]]. For example, a [[complex number]] can be represented as a 2‑tuple, a [[quaternion]] can be represented as a 4‑tuple, an [[octonion]] can be represented as an 8‑tuple and a [[sedenion]] can be represented as a 16‑tuple.

Although these uses treat ''‑tuple'' as the suffix, the original suffix was ''‑ple'' as in "triple" (three-fold) or "decuple" (ten‑fold). This originates from  [[medieval Latin]]  ''plus'' (meaning "more") related to [[Greek language|Greek]] ‑πλοῦς, which replaced the classical and late antique  ''‑plex'' (meaning "folded"), as in "duplex".<ref>''OED'', ''s.v.'' "triple", "quadruple", "quintuple", "decuple"</ref>

===Names for tuples of specific lengths===

{| class="wikitable"
|-
! Tuple Length <math>n</math> !! Name !! Alternative names
|-
| align="right" | 0 || empty tuple || unit / empty sequence
|-
| align="right" | 1 || single || [[Singleton_(mathematics)|singleton]] / monuple 
|-
| align="right" | 2 || double || couple / (ordered) pair / dual / twin / product
|-
| align="right" | 3 || triple || treble / triplet / triad
|-
| align="right" | 4 || quadruple || quad
|-
| align="right" | 5 || quintuple || pentuple
|-
| align="right" | 6 || sextuple ||hextuple
|-
| align="right" | 7 || septuple ||heptuple
|-
| align="right" | 8 || octuple || 
|-
| align="right" | 9 || nonuple || 
|-
| align="right" | 10 || decuple || 
|-
| align="right" | 11 || undecuple || hendecuple
|-
| align="right" | 12 || duodecuple || 
|-
| align="right" | 13 || tredecuple || 
|-
| align="right" | 14 || quattuordecuple || 
|-
| align="right" | 15 || quindecuple ||
|-
| align="right" | 16 || sexdecuple ||
|-
| align="right" | 17 || septendecuple ||
|-
| align="right" | 18 || octodecuple ||
|-
| align="right" | 19 || novemdecuple ||
|-
| align="right" | 20 || vigintuple ||
|-
| align="right" | 21 || unvigintuple ||
|-
| align="right" | 22 || duovigintuple ||
|-
| align="right" | 23 || trevigintuple ||
|-
| align="right" | 24 || quattuorvigintuple ||
|-
| align="right" | 25 || quinvigintuple ||
|-
| align="right" | 26 || sexvigintuple ||
|-
| align="right" | 27 || septenvigintuple ||
|-
| align="right" | 28 || octovigintuple ||
|-
| align="right" | 29 || novemvigintuple ||
|-
| align="right" | 30 || trigintuple ||
|-
| align="right" | 31 || untrigintuple ||
|-
| align="right" | 40 || quadragintuple ||
|-
| align="right" | 50 || quinquagintuple ||
|-
| align="right" | 60 || sexagintuple ||
|-
| align="right" | 70 || septuagintuple ||
|-
| align="right" | 80 || octogintuple ||
|-
| align="right" | 90 || nongentuple ||
|-
| align="right" | 100 || centuple ||
|-
| align="right" | 1,000 || milluple ||
|-
|}

==Properties==
The general rule for the identity of two {{math|''n''}}-tuples is
: <math>(a_1, a_2, \ldots, a_n) = (b_1, b_2, \ldots, b_n)</math> [[if and only if]] <math>a_1=b_1,\text{ }a_2=b_2,\text{ }\ldots,\text{ }a_n=b_n.</math>

Thus a tuple has properties that distinguish it from a [[Set (mathematics)|set]].
# A tuple may contain multiple instances of the same element, so {{break|
}}tuple <math>(1,2,2,3) \neq (1,2,3)</math>; but set <math>\{1,2,2,3\} = \{1,2,3\}</math>.
# Tuple elements are ordered: tuple <math>(1,2,3) \neq (3,2,1)</math>, but set <math>\{1,2,3\} = \{3,2,1\}</math>.
# A tuple has a finite number of elements, while a set or a [[multiset]] may have an infinite number of elements.

==Definitions==

There are several definitions of tuples that give them the properties described in the previous section.

===Tuples as functions===
If we are dealing with sets, an {{math|''n''}}-tuple can be regarded as a [[Function (mathematics)#Definition|function]], {{math|''F''}},  whose domain is the tuple's implicit set of element indices, {{math|''X''}}, and whose codomain, {{math|''Y''}}, is the tuple's set of elements. Formally:
: <math>(a_1, a_2, \dots, a_n) \equiv (X,Y,F)</math>
where:
: <math>
    \begin{align}
      X & = \{1, 2, \dots, n\}                       \\
      Y & = \{a_1, a_2, \ldots, a_n\}                \\
      F & = \{(1, a_1), (2, a_2), \ldots, (n, a_n)\}. \\
    \end{align}
  </math>
In slightly less formal notation this says:
:<math> (a_1, a_2, \dots, a_n) := (F(1), F(2), \dots, F(n)).</math>

===Tuples as nested ordered pairs===
Another way of modeling tuples in Set Theory is as nested [[ordered pair]]s. This approach assumes that the notion of ordered pair has already been defined; thus a 2-tuple 
# The 0-tuple (i.e. the empty tuple) is represented by the empty set <math>\emptyset</math>.
# An {{math|''n''}}-tuple, with {{math|''n'' > 0}}, can be defined as an ordered pair of its first entry and an {{math|(''n'' − 1)}}-tuple (which contains the remaining entries when {{math|''n'' > 1)}}:
#: <math>(a_1, a_2, a_3, \ldots, a_n) = (a_1, (a_2, a_3, \ldots, a_n))</math>
This definition can be applied recursively to the {{math|(''n'' − 1)}}-tuple:
: <math>(a_1, a_2, a_3, \ldots, a_n) = (a_1, (a_2, (a_3, (\ldots, (a_n, \emptyset)\ldots))))</math>

Thus, for example:
: <math>
    \begin{align}
         (1, 2, 3) & = (1, (2, (3, \emptyset)))      \\
      (1, 2, 3, 4) & = (1, (2, (3, (4, \emptyset)))) \\
    \end{align}
  </math>

A variant of this definition starts "peeling off" elements from the other end:
# The 0-tuple is the empty set <math>\emptyset</math>.
# For {{math|''n'' > 0}}:
#: <math>(a_1, a_2, a_3, \ldots, a_n) = ((a_1, a_2, a_3, \ldots, a_{n-1}), a_n)</math>
This definition can be applied recursively:
: <math>(a_1, a_2, a_3, \ldots, a_n) = ((\ldots(((\emptyset, a_1), a_2), a_3), \ldots), a_n)</math>

Thus, for example:
: <math>
    \begin{align}
         (1, 2, 3) & = (((\emptyset, 1), 2), 3)      \\
      (1, 2, 3, 4) & = ((((\emptyset, 1), 2), 3), 4) \\
    \end{align}
  </math>

===Tuples as nested sets===
Using [[ordered pair#Kuratowski definition|Kuratowski's representation for an ordered pair]], the second definition above can be reformulated in terms of pure [[set theory]]:
# The 0-tuple (i.e. the empty tuple) is represented by the empty set <math>\emptyset</math>;
# Let <math>x</math> be an {{math|''n''}}-tuple <math>(a_1, a_2, \ldots, a_n)</math>, and let <math>x \rightarrow b \equiv (a_1, a_2, \ldots, a_n, b)</math>. Then, <math>x \rightarrow b \equiv \{\{x\}, \{x, b\}\}</math>.  (The right arrow, <math>\rightarrow</math>, could be read as "adjoined with".)

In this formulation:
: <math>
   \begin{array}{lclcl}
     ()      & &                     &=& \emptyset                                    \\
             & &                     & &                                              \\
     (1)     &=& ()    \rightarrow 1 &=& \{\{()\},\{(),1\}\}                          \\
             & &                     &=& \{\{\emptyset\},\{\emptyset,1\}\}            \\
             & &                     & &                                              \\
     (1,2)   &=& (1)   \rightarrow 2 &=& \{\{(1)\},\{(1),2\}\}                        \\
             & &                     &=& \{\{\{\{\emptyset\},\{\emptyset,1\}\}\},     \\
             & &                     & & \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\}    \\
             & &                     & &                                              \\
     (1,2,3) &=& (1,2) \rightarrow 3 &=& \{\{(1,2)\},\{(1,2),3\}\}                    \\
             & &                     &=& \{\{\{\{\{\{\emptyset\},\{\emptyset,1\}\}\}, \\
             & &                     & & \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\}\}, \\
             & &                     & & \{\{\{\{\{\emptyset\},\{\emptyset,1\}\}\},   \\
             & &                     & & \{\{\{\emptyset\},\{\emptyset,1\}\},2\}\},3\}\}                                       \\
    \end{array}
  </math>

=={{anchor|n-tuple}}{{math|''n''}}-tuples of {{math|''m''}}-sets ==

In [[discrete mathematics]], especially [[combinatorics]] and finite [[probability theory]], {{math|''n''}}-tuples arise in the context of various counting problems and are treated more informally as ordered lists of length {{math|''n''}}.<ref>{{harvnb|D'Angelo|West|2000|p=9}}</ref> {{math|''n''}}-tuples whose entries come from a set of {{math|''m''}} elements are also called ''arrangements with repetition'', ''permutations of a multiset'' and, in some non-English literature, ''[[Variation (disambiguation)#Mathematics|variations]] with repetition''. The number of {{math|''n''}}-tuples of an {{math|''m''}}-set is {{math|''m''<sup>''n''</sup>}}. This follows from the combinatorial [[rule of product]].<ref>{{harvnb|D'Angelo|West|2000|p=101}}</ref> If {{math|''S''}} is a finite set of [[cardinality]] {{math|''m''}}, this number is the cardinality of the {{math|''n''}}-fold [[Cartesian_product#Cartesian_power | Cartesian power]] {{math|''S'' × ''S'' × ... ''S''}}. Tuples are elements of this product set.

== Type theory ==
{{main|Product type}}
In [[type theory]], commonly used in [[programming language]]s, a tuple has a [[product type]]; this fixes not only the length, but also the underlying types of each component. Formally:
: <math>(x_1, x_2, \ldots, x_n) : \mathsf{T}_1 \times \mathsf{T}_2 \times \ldots \times \mathsf{T}_n</math>
and the [[Projection (mathematics)|projection]]s are term constructors:
: <math>\pi_1(x) : \mathsf{T}_1,~\pi_2(x) : \mathsf{T}_2,~\ldots,~\pi_n(x) : \mathsf{T}_n</math>

The tuple with labeled elements used in the [[#Relational_model|relational model]] has a [[Record (computer science)|record type]]. Both of these types can be defined as simple extensions of the [[simply typed lambda calculus]].<ref name="pierce2002">{{cite book|last=Pierce|first=Benjamin|title=Types and Programming Languages|publisher=MIT Press|year=2002|isbn=0-262-16209-1|pages=126–132}}</ref>

The notion of a tuple in type theory and that in set theory are related in the following way: If we consider the natural [[model theory|model]] of a type theory, and use the Scott brackets to indicate the semantic interpretation<!-- do not link; that article needs to be a dab first-->, then the model consists of some sets <math>S_1, S_2, \ldots, S_n</math> (note: the use of italics here that distinguishes sets from types) such that:
: <math>[\![\mathsf{T}_1]\!] = S_1,~[\![\mathsf{T}_2]\!] = S_2,~\ldots,~[\![\mathsf{T}_n]\!] = S_n</math>
and the interpretation of the basic terms is:
: <math>[\![x_1]\!] \in [\![\mathsf{T}_1]\!],~[\![x_2]\!] \in [\![\mathsf{T}_2]\!],~\ldots,~[\![x_n]\!] \in [\![\mathsf{T}_n]\!]</math>.

The {{math|''n''}}-tuple of type theory has the natural interpretation as an {{math|''n''}}-tuple of set theory:<ref>Steve Awodey, [http://www.andrew.cmu.edu/user/awodey/preprints/stcsFinal.pdf ''From sets, to types, to categories, to sets''], 2009, [[preprint]]</ref>
: <math>[\![(x_1, x_2, \ldots, x_n)]\!] = (\,[\![x_1]\!], [\![x_2]\!], \ldots, [\![x_n]\!]\,)</math>
The [[unit type]] has as semantic interpretation the 0-tuple.

==See also==
{{Wiktionary|tuple}}
* [[Arity]]
* [[Exponential object]]
* [[Formal language]]
* [[Multidimensional Expressions#MDX data types|OLAP: Multidimensional Expressions]]
* [[Prime k-tuple]]
* [[Relation (mathematics)]]
* [[Tuplespace]]

==Notes==
{{Reflist}}

==References==

{{refbegin}}
* {{citation|first1=John P.|last1=D'Angelo|first2=Douglas B.|last2=West|title=Mathematical Thinking/Problem-Solving and Proofs|year=2000|edition=2nd|publisher=Prentice-Hall|isbn=978-0-13-014412-6}}
* [[Keith Devlin]], ''The Joy of Sets''. Springer Verlag, 2nd ed., 1993, ISBN 0-387-94094-4, pp.&nbsp;7–8
* [[Abraham Adolf Fraenkel]], [[Yehoshua Bar-Hillel]], [[Azriel Lévy]], ''[https://books.google.com/books?q=Foundations+of+set+theory&btnG=Search+Books Foundations of set theory]'', Elsevier Studies in Logic Vol. 67, Edition 2, revised, 1973, ISBN 0-7204-2270-1, p.&nbsp;33
* [[Gaisi Takeuti]], W. M. Zaring, ''Introduction to Axiomatic Set Theory'', Springer [[Graduate texts in mathematics|GTM]] 1, 1971, ISBN 978-0-387-90024-7, p.&nbsp;14
* George J. Tourlakis, ''[https://books.google.com/books?as_isbn=9780521753746 Lecture Notes in Logic and Set Theory. Volume 2: Set theory]'', Cambridge University Press, 2003, ISBN 978-0-521-75374-6, pp.&nbsp;182–193

{{refend}}
{{Set theory}}

<!--Interwikies-->

<!--Categories-->
{{Authority control}}
[[Category:Data management]]
[[Category:Mathematical notation]]
[[Category:Sequences and series]]
[[Category:Basic concepts in set theory]]
[[Category:Type theory]]
[[ar:زوج مرتب]]
<=====doc_Id=====>:308
<=====title=====>:
Software intelligence
<=====text=====>:
{{one source|date=January 2015}}
'''Software Intelligence''' ('''SI''') is [[software]] designed to analyze [[source code]] to better understand [[Information technology|Information Technology]] environments. Similarly to [[Business intelligence|Business Intelligence]] (BI), Software Intelligence is a set of software tools and techniques for the [[Data mining|mining of data]] into meaningful and useful information.<ref>{{cite web|last1=Hassan|first1=Ahmed|last2=Xie|first2=Tao|title=Software Intelligence: The Future of Mining Software Engineering Data|url=http://web.engr.illinois.edu/~taoxie/publications/foser10-si.pdf|website=http://web.engr.illinois.edu|accessdate=19 January 2015}}</ref>

==References==
{{Reflist}}

[[Category:Data management]]
[[Category:Source code]]
<=====doc_Id=====>:311
<=====title=====>:
MaPS S.A.
<=====text=====>:
{{Orphan|date=July 2015}}

{{Infobox company
| name             = MaPS System 
| image            = [[File:MaPS-System Logo.png|250px]]
| industry         = [[Public limited company|PLC]]
| founder          = Thierry Muller
| headquarters     = [[Luxembourg]]
| area_served      = [[France]], [[Luxembourg]], [[Germany]], [[Switzerland]], [[Belgium]]
| products         = [[Product Information Management]], [[Digital Asset Management]], [[Master Data Management]] and [[Business Process Management]]
| homepage         = http://www.maps-system.com/
}}

'''MaPS S.A.''' is a software editor founded in 2011 by Thierry Muller which is headquartered in [[Luxembourg]]. Its platform, called MaPS System, provides [[Data management|Data Management]] solutions for [[Multichannel Marketing]].

==History and Funding==

In 1999, the founder, had realized that certain challenges arose with several tools for [[Customer relationship management|Customer Relationship Management]], [[Public relations|Public Relations]] and in particular [[Marketing]] tools set in place. Complex data structures developed difficulties for organizations who lost focus of their dispersed data when wanting to operate and sell in an international and [[Multichannel Marketing|Multichannel]] environment.

The founder drafted his initial ideas on the topic of [[Multichannel Marketing]] and developed his first version of MaPS System under the agency Prem1um S.A. in 2005, which in combination with the [[Data Management]] solution also provided various Multimedia & Marketing activities.

In 2011, after being successful, Prem1um S.A. decided to enable the software MaPS System to operate independently under MaPS S.A., as a separate company and editor of the software. The first financial supports were provided by Malta ICI, a Venture Capital firm, and the local partner Chameleon Invest, a seed-capital fund led by Business Angels, who invested €900.000. In a second investment round in 2014 led by Newion Investments, a Venture Capital firm, €1.4 Million were raised, thus amounting to total assets of €2.2 Million.

==Products==
The services included in MaPS System range from the data centralization, [[Data Governance]] to an optimized [[Multichannel Marketing]]. The software today features more than 35 modules for [[Master Data Management]], [[Product Information Management]], [[Digital Asset Management]], [[Business Process Management]] including catalogue [[Publishing]] features.

==References==
* {{Official website | http://www.maps-system.com/ MaPS S.A.}}
* Newion invests in MaPS System | http://www.newion-investments.com/news/newion-invests-in-maps-system/1]
* Introducing MaPS System | http://www.siliconluxembourg.lu/introducing-maps-system-a-centralized-information-management-solution/]

[[Category:Data management software]]
[[Category:Data management]]
[[Category:Software companies]]
[[Category:Companies of Luxembourg]]
<=====doc_Id=====>:314
<=====title=====>:
Data lake
<=====text=====>:
{{Use dmy dates|date=May 2016}}
A '''data lake''' is a method of storing [[data]] within a system or repository, in its natural format,<ref>[http://blogs.sas.com/content/datamanagement/2016/11/21/growing-import-big-data-quality/ The growing importance of big data quality]</ref> that facilitates the collocation of data in various schemata and structural forms, usually object blobs or files. 

== Invention ==
James Dixon, then chief technology officer at [[Pentaho]] coined the term<ref name="woods2011">{{cite news | title=Big data requires a big architecture |last=Woods |first=Dan |work=Forbes |date=21 July 2011 |department=Tech |url=http://www.forbes.com/sites/ciocentral/2011/07/21/big-data-requires-a-big-new-architecture/ }}</ref> to contrast it with [[data mart]], which is a smaller repository of interesting attributes extracted from raw data.<ref name="dixon2010">{{cite web | last=Dixon|first=James|title=Pentaho, Hadoop, and Data Lakes|url=https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/|website=James Dixon’s Blog|publisher=James|accessdate=7 November 2015 |quote=If you think of a datamart as a store of bottled water – cleansed and packaged and structured for easy consumption – the data lake is a large body of water in a more natural state. The contents of the data lake stream in from a source to fill the lake, and various users of the lake can come to examine, dive in, or take samples.}}</ref> He argued that data marts have several inherent problems, and that data lakes are the optimal solution. These problems are often referred to as [[information silo]]ing. [[PricewaterhouseCoopers]] said that data lakes could "put an end to data silos.<ref name="stein2014">{{cite report | url=http://www.pwc.com/en_US/us/technology-forecast/2014/cloud-computing/assets/pdf/pwc-technology-forecast-data-lakes.pdf |format=pdf |title=Data lakes and the promise of unsiloed data |last2=Morrison |first2=Alan |last=Stein |first=Brian |publisher=PricewaterhouseCooper |series=Technology Forecast: Rethinking integration |year=2014 }}</ref> In their study on data lakes they noted that enterprises were "starting to extract and place data for analytics into a single, Hadoop-based repository."

==Characteristics==
The idea of data lake is to have a single store of all data in the enterprise ranging from raw data (which implies exact copy of source system data) to transformed data which is used for various tasks including [[Data reporting|reporting]], [[data visualization|visualization]], [[data analytics|analytics]] and [[machine learning]].

The data lake includes structured data from relational databases (rows and columns), semi-structured data (CSV, logs, XML, JSON), unstructured data (emails, documents, PDFs) and even binary data (images, audio, video) thus creating a centralized data store accommodating all forms of data.

== Examples ==

One example of a data lake is the distributed file system [[Apache Hadoop]].

Many companies also use cloud storage services such as [[Amazon S3]].<ref name="tuulos2015">{{cite web | title=Petabyte-Scale Data Pipelines with Docker, Luigi and Elastic Spot Instances |last=Tuulos |first=Ville |date=22 September 2015 |url=http://tech.adroll.com/blog/data/2015/09/22/data-pipelines-docker.html}}</ref> There is a gradual academic interest in the concept of data lakes, for instance, [http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull Personal DataLake]<ref>http://ieeexplore.ieee.org/xpl/abstractAuthors.jsp?reload=true&arnumber=7310733</ref> an ongoing project at Cardiff University to create a new type of data lake which aims at managing big data of individual users by providing a single point of collecting, organizing, and sharing personal data.<ref>http://www.researchgate.net/publication/283053696_Personal_Data_Lake_With_Data_Gravity_Pull</ref>

The earlier data lake (Hadoop 1.0) had limited capabilities with its batch oriented processing (Map Reduce) and was the only processing paradigm associated with it. Interacting with the data lake meant you had to have expertise in Java with map reduce and higher level tools like Pig & Hive (which by themselves were batch oriented). With the dawn of Hadoop 2.0 and separation of duties with Resource Management taken over by YARN (Yet another resource negotiator), new processing paradigms like Streaming, interactive, on-line have become available via Hadoop and the Data Lake.

== Criticism ==
{{criticism section|date=December 2015}}

In June 2015, David Needle characterized "so-called data lakes" as "one of the more controversial ways to manage [[big data]]".<ref name="needle2015">{{cite news |  last=Needle |first=David | title=Hadoop Summit: Wrangling Big Data Requires Novel Tools, Techniques |date=10 June 2015 | work=eWeek | url= http://www.eweek.com/enterprise-apps/hadoop-summit-wrangling-big-data-requires-novel-tools-techniques-2.html | department=Enterprise Apps | access-date = 1 November 2015 | quote = Walter Maguire, chief field technologist at HP's Big Data Business Unit, discussed one of the more controversial ways to manage big data, so-called data lakes. }}</ref> [[PricewaterhouseCoopers]] were also careful to note in their research that not all data lake initiatives are successful. They quote Sean Martin, CTO of [[Cambridge Semantics]],
{{quote|sign=|source=|We see customers creating big data graveyards, dumping everything into HDFS [Hadoop Distributed File System] and hoping to do something with it down the road. But then they just lose track of what’s there.<ref name="stein2014"/>}}
They advise that "The main challenge is not creating a data lake, but taking advantage of the opportunities it presents."<ref name="stein2014"/> They describe companies that build successful data lakes as gradually maturing their lake as they figure out which data and metadata are important to the organization.

== References ==
<references/>

[[Category:Data management]]
<=====doc_Id=====>:317
<=====title=====>:
Information integration
<=====text=====>:
{{Refimprove|date=September 2014}}
'''Information integration''' ('''II''') (also called [[referential integrity]]) is the merging of information from heterogeneous sources with differing conceptual, contextual and typographical representations. It is used in [[data mining]] and consolidation of data from unstructured or semi-structured resources. Typically, ''information integration'' refers to textual representations of knowledge but is sometimes applied to [[rich-media]] content. '''Information fusion''', which is a related term, involves the combination of information into a new set of information towards reducing redundancy and uncertainty.<ref name="dca">M. Haghighat, M. Abdel-Mottaleb, &  W. Alhalabi (2016). [http://dx.doi.org/10.1109/TIFS.2016.2569061 Discriminant Correlation Analysis: Real-Time Feature Level Fusion for Multimodal Biometric Recognition]. IEEE Transactions on Information Forensics and Security, 11(9), 1984-1996.</ref>

Examples of [[Technology|technologies]] available to integrate information include [[data deduplication|deduplication]], and [[string metrics]] which allow the detection of similar text in different data sources by [[fuzzy string searching|fuzzy matching]]. A host of methods for these research areas are available such as those presented in the International Society of Information Fusion.

==See also==
* [[Data fusion]] (is a subset of Information integration)
* [[Sensor fusion]]
* [[Data integration]]
* [[Image fusion]]

==External links==
* [https://github.com/mhaghighat/dcaFuse Discriminant Correlation Analysis (DCA)]<ref name="dca"></ref>
* [http://webcache.googleusercontent.com/search?q=cache:OrNCxOpaXAMJ:infolab.stanford.edu/pub/papers/integration-using-views.ps+information+integration&cd=5&hl=en&ct=clnk&gl=us&client=firefox-a Information Integration Using Logical View] LNCS 1997.
* [http://www.isif.org/ International Society of Information Fusion]

==Books==
* Liggins, Martin E., David L. Hall, and James Llinas. Multisensor Data Fusion, Second Edition Theory and Practice (Multisensor Data Fusion). CRC, 2008. ISBN 978-1-4200-5308-1
* David L. Hall, Sonya A. H. McMullen, Mathematical Techniques in Multisensor Data Fusion (2004), ISBN 1-58053-335-3
* Springer, Information Fusion in Data Mining (2003), ISBN 3-540-00676-1
* H. B. Mitchell, Multi-sensor Data Fusion – An Introduction (2007) Springer-Verlag, Berlin, ISBN 978-3-540-71463-7
* S. Das, High-Level Data Fusion (2008), Artech House Publishers, Norwood, MA, ISBN 978-1-59693-281-4 and 1596932813
* Erik P. Blasch, Eloi Bosse, and Dale A. Lambert, High-Level Information Fusion Management and System Design (2012), Artech House Publishers, Norwood, MA. ISBN 1608071510 | ISBN 978-1608071517

==References==
{{Reflist}}


{{DEFAULTSORT:Information Integration}}
[[Category:Data management]]

[[ar:تكامل البيانات]]
[[de:Informationsintegration]]
<=====doc_Id=====>:320
<=====title=====>:
Chunked transfer encoding
<=====text=====>:
{{refimprove|date=June 2014}}
'''Chunked transfer encoding''' is a data transfer mechanism in version 1.1 of the [[Hypertext Transfer Protocol]] (HTTP) in which data is sent in a series of "chunks". It uses the [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]] HTTP header in place of the [[Content-Length]] header, which the earlier version of the protocol would otherwise require.<ref>http://tools.ietf.org/html/rfc1945#section-7.2</ref> Because the Content-Length header is not used, the sender does not need to know the length of the content before it starts transmitting a response to the receiver. Senders can begin transmitting dynamically-generated content before knowing the total size of that content.

The size of each chunk is sent right before the chunk itself so that the receiver can tell when it has finished receiving data for that chunk. The data transfer is terminated by a final chunk of length zero.

An early form of the chunked encoding was proposed in 1994.<ref>{{cite web|last=Connolly|first=Daniel|title=Content-Transfer-Encoding: packets for HTTP|url=http://1997.webhistory.org/www.lists/www-talk.1994q3/1147.html|accessdate=13 September 2013|date=27 Sep 1994|id=&lt;9409271503.AA27488@austin2.hal.com&gt;}}</ref> Later it was standardized in HTTP 1.1.

==Rationale==
The introduction of chunked encoding provided various benefits:

* Chunked transfer encoding allows a server to maintain an [[HTTP persistent connection]] for dynamically generated content. In this case, the HTTP Content-Length header cannot be used to delimit the content and the next HTTP request/response, as the content size is as yet unknown. Chunked encoding has the benefit that it is not necessary to generate the full content before writing the header, as it allows streaming of content as chunks and explicitly signaling the end of the content, making the connection available for the next HTTP request/response.
* Chunked encoding allows the sender to send additional header fields after the message body. This is important in cases where values of a field cannot be known until the content has been produced, such as when the content of the message must be digitally signed. Without chunked encoding, the sender would have to buffer the content until it was complete in order to calculate a field value and send it before the content.

==Applicability==
For version 1.1 of the HTTP protocol, the chunked transfer mechanism is considered to be always and anyways acceptable, even if not listed in the [[List of HTTP header fields#te-request-header|TE]] (transfer encoding) request header field, and when used with other transfer mechanisms, should always be applied last to the transferred data and never more than one time. This transfer coding method also allows additional entity header fields to be sent after the last chunk if the client specified the "trailers" parameter as an argument of the TE field. The origin server of the response can also decide to send additional entity trailers even if the client did not specify the "trailers" option in the TE request field, but only if the metadata is optional (i.e. the client can use the received entity without them). Whenever the trailers are used, the server should list their names in the Trailer header field; 3 header field types are specifically prohibited from appearing as a trailer field:  [[List of HTTP header fields#transfer-encoding-response-header|Transfer-Encoding]], [[List of HTTP header fields#content-length-response-header|Content-Length]] and [[List of HTTP header fields#trailer-response-header|Trailer]].

==Format==
If a <tt>Transfer-Encoding</tt> field with a value of "<tt>chunked</tt>" is specified in an HTTP message (either a request sent by a client or the response from the server), the body of the message consists of an unspecified number of chunks, a terminating chunk, trailer, and a final CRLF sequence (i.e. [[carriage return]] followed by [[line feed]]).

Each chunk starts with the number of [[Octet (computing)|octets]] of the data it embeds expressed as a [[hexadecimal]] number in [[ASCII]] followed by optional parameters (''chunk extension'') and a terminating CRLF sequence, followed by the chunk data. The chunk is terminated by CRLF.

If chunk extensions are provided, the chunk size is terminated by a semicolon and followed by the parameters, each also delimited by semicolons. Each parameter is encoded as an extension name followed by an optional equal sign and value. These parameters could be used for a running [[message digest]] or [[digital signature]], or to indicate an estimated transfer progress, for instance.

The terminating chunk is a regular chunk, with the exception that its length is zero. It is followed by the trailer, which consists of a (possibly empty) sequence of entity header fields. Normally, such header fields would be sent in the message's header; however, it may be more efficient to determine them after processing the entire message entity. In that case, it is useful to send those headers in the trailer.

Header fields that regulate the use of trailers are ''TE'' (used in requests), and ''Trailers'' (used in responses).

==Use with compression==

HTTP servers often use [[data compression|compression]] to optimize transmission, for example with <tt>Content-Encoding: [[gzip]]</tt> or <tt>Content-Encoding: [[deflate]]</tt>. If both compression and chunked encoding are enabled, then the content stream is first compressed, then chunked; so the chunk encoding itself is not compressed, and the data in each chunk is not compressed individually. The remote endpoint then decodes the stream by concatenating the chunks and uncompressing the result.

==Example==

===Encoded data===
In the following example, three chunks of length 4, 5 and 14 are shown. The chunk size is transferred as a hexadecimal number followed by \r\n as a line separator, followed by a chunk of data of the given size.

<pre>
4\r\n
Wiki\r\n
5\r\n
pedia\r\n
E\r\n
 in\r\n
\r\n
chunks.\r\n
0\r\n
\r\n
</pre>

Note: the chunk size indicates the size of the chunk data and excludes the trailing CRLF ("\r\n").<ref>http://skrb.org/ietf/http_errata.html</ref> In this particular example, the CRLF following "in" is counted toward the chunk size of 0xE (14). The CRLF in its own line is also counted toward the chunk size.
The period character at the end of "chunks" is the 14th character, so it is the
last data character in that chunk. The CRLF following the period is
the trailing CRLF, so it is not counted toward the chunk size of 0xE (14).

===Decoded data===
<pre>
Wikipedia in

chunks.
</pre>

==See also==
* [[List of HTTP header fields]]

==References==
{{Reflist}}
{{Refbegin}}
* See [http://tools.ietf.org/html/rfc7230#section-4.1 RFC 7230 section 4.1] for further details of chunked encoding.
* The previous (obsoleted) version is at [https://tools.ietf.org/html/rfc2616#section-3.6.1 RFC 2616 section 3.6.1].
{{Refend}}

{{DEFAULTSORT:Chunked Transfer Encoding}}
[[Category:Data management]]
[[Category:Hypertext Transfer Protocol]]
[[Category:Hypertext Transfer Protocol headers]]
<=====doc_Id=====>:323
<=====title=====>:
Copyright
<=====text=====>:
{{Redirect2|Copyrighting|Copyrights|the use of words to promote or advertise|Copywriting|the Wikipedia policy about copyright issues|Wikipedia:Copyrights}}
{{pp-move-indef|small=yes}}
{{Intellectual property}}
{{Capitalism|Concepts}}
{{Use dmy dates|date=January 2011}}
{{Use American English|date=January 2014}}

'''Copyright''' is a [[Natural and legal rights|legal right]] created by the law of a country that grants the creator of an original work [[exclusive right]]s for its use and distribution. This is usually only for a limited time. The exclusive rights are not absolute but limited by [[limitations and exceptions to copyright]] law, including fair use. A major limitation on copyright is that copyright protects only the original expression of ideas, and not the underlying ideas themselves.<ref>{{cite web|url=http://www.bitlaw.com/copyright/unprotected.html#ideas|title=Works Unprotected by Copyright Law|publisher=Bitlaw|author=Daniel A. Tysver}}</ref><ref>{{cite web|url=http://digital-law-online.info/lpdi1.0/treatise9.html |title=Legal Protection of Digital Information |page=''Chapter 1: An Overview of Copyright'', Section II.E. Ideas Versus Expression.|author=Lee A. Hollaar}}</ref>

Copyright is a form of [[intellectual property]], applicable to certain forms of creative work. Some, but not all jurisdictions require "fixing" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders.<ref>{{Citation|title=Copyright|publisher=University of California|year=2014|url=http://copyright.universityofcalifornia.edu/ownership/joint-works.html|accessdate=2014-12-15}}</ref><ref>http://www.jetlaw.org/publish/journal-conventions/</ref><ref>https://books.google.de/books?id=kz1F6uAHtaEC&pg=PA81&dq=%22rights+holder%22&hl=en&sa=X&ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIHDAA#v=onepage&q=%22rights%20holder%22&f=false</ref><ref>https://books.google.de/books?id=xD_iBwAAQBAJ&pg=PT465&dq=%22rights+holder%22&hl=en&sa=X&ved=0ahUKEwiG4OnUo87RAhXqBcAKHQgZAD8Q6AEIKDAC#v=onepage&q=%22rights%20holder%22&f=false</ref> These rights frequently include reproduction, control over [[derivative work]]s, distribution, [[Performing rights|public performance]], and "[[moral rights]]" such as attribution.<ref>{{Citation|title=17 U.S.C. § 106|publisher=United States of America|year=2011|url=http://www.copyright.gov/title17/92chap1.html#106|accessdate=2014-12-15}}</ref>

Copyrights are considered ''territorial rights'', which means that they do not extend beyond the territory of a specific jurisdiction.  While many aspects of national copyright laws have been standardized through [[international copyright agreements]], copyright laws vary by country.<ref name="International Copyright Law Survey">{{cite web|url=http://worldcopyrightlaw.com/copyrightsurvey|title=International Copyright Law Survey|publisher=Mincov Law Corporation}}</ref>

Typically, the [[Copyright term|duration of a copyright]] spans the author's life plus 50 to 100 years (that is, copyright typically expires 50 to 100 years after the author dies, depending on the jurisdiction).  Some countries require certain copyright [[copyright formalities|formalities]] to establishing copyright, but most recognize copyright in any completed work, without formal registration. Generally, copyright is enforced as a [[civil law (common law)|civil]] matter, though some jurisdictions do apply [[criminal law|criminal]] sanctions.

Most jurisdictions recognize copyright limitations, allowing "fair" exceptions to the creator's exclusivity of copyright and giving users certain rights. The development of digital media and computer network technologies have prompted reinterpretation of these exceptions, introduced new difficulties in enforcing copyright, and inspired additional challenges to copyright law's philosophic basis. Simultaneously, businesses with great economic dependence upon copyright, such as those in the music business, have advocated the extension and expansion of copyright and sought additional legal and technological enforcement.

==History==
{{main|History of copyright law}}
===Background===
Copyright came about with the invention of [[Printing press|the printing press]] and with wider literacy. As a legal concept, its origins in Britain were from a reaction to printers' monopolies at the beginning of the 18th&nbsp;century. [[Charles II of England]] was concerned by the unregulated [[copying]] of books and passed the [[Licensing of the Press Act 1662]] by Act of Parliament,<ref>''Copyright in Historical Perspective'', p. 136-137, Patterson, 1968, Vanderbilt Univ. Press</ref> which established a register of licensed books and required a copy to be deposited with the [[Worshipful Company of Stationers and Newspaper Makers|Stationers' Company]], essentially continuing the licensing of material that had long been in effect.

Copyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in [[Europe]] and not, for example, in Asia. In the [[Middle Ages]] in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which [[capitalism]] led to the [[commodification]] of many aspects of social life that earlier had no monetary or economic value per&nbsp;se.<ref>Bettig, Ronald V. (1996). ''Copyrighting Culture: The Political Economy of Intellectual Property. Westview Press''. p. 9–17. ISBN 0-8133-1385-6.</ref>

Copyright has grown from a legal concept regulating copying rights in the publishing of books and maps to one with a significant effect on nearly every modern industry, covering such items as [[Sound recording and reproduction|sound recordings]], films, photographs, software, and architectural works.

===National copyrights===
{{See also|Statute of Anne|History of US Copyright Law}}
[[File:Statute of anne.jpg|thumb|left|The [[Statute of Anne]] (the Copyright Act 1709) came into force in 1710.]]
Often seen as the first real copyright law, the 1709 British [[Statute of Anne]] gave the publishers rights for a fixed period, after which the copyright expired.<ref name="Rethinking copyright: history, theory, language">{{cite book|url=https://www.google.com/books?id=dMYXq9V1JBQC&dq=statute+of+anne+copyright&lr=&as_brr=3&source=gbs_navlinks_s |title=Rethinking copyright: history, theory, language |page=13 |author=Ronan, Deazley |isbn=978-1-84542-282-0 |year=2006 |publisher=Edward Elgar Publishing. |deadurl=yes |archiveurl=https://web.archive.org/web/20111119042246/https://www.google.com/books?id=dMYXq9V1JBQC&dq=statute+of+anne+copyright&lr=&as_brr=3&source=gbs_navlinks_s |archivedate=19 November 2011 }}</ref>
The act also alluded to individual rights of the artist. It began, "Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing... Books, and other Writings, without the Consent of the Authors... to their very great Detriment, and too often to the Ruin of them and their Families:".<ref>{{cite web|url=http://www.copyrighthistory.com/anne.html |title=Statute of Anne |publisher=Copyrighthistory.com |accessdate=2012-06-08}}</ref> A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.

The [[Copyright Clause]] of the United States Constitution (1787) authorized copyright legislation: "To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries." That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.

The original length of copyright in the United States was 14&nbsp;years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14‑year monopoly grant, but after that the work entered the [[public domain]], so it could be used and built upon by others.

Copyright law was enacted rather [[Copyright in Germany|late in German states]], and the historian Eckhard Höffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century.<ref name="thad">{{cite web |url=http://www.spiegel.de/international/zeitgeist/0,1518,710976,00.html | author=Frank Thadeusz |title=No Copyright Law: The Real Reason for Germany's Industrial Expansion? |publisher=[[Der Spiegel]] |date= 18 August 2010 |accessdate= 11 April 2015}}</ref>

===International copyright treaties===
{{See also|International copyright agreements|List of parties to international copyright agreements}}
[[File:Joseph Ferdinand Keppler - The Pirate Publisher - Puck Magazine - Restoration by Adam Cuerden.jpg|'' The Pirate Publisher—An International Burlesque that has the Longest Run on Record'', from ''[[Puck (magazine)|Puck]]'', 1886, satirizes the then-existing situation where a publisher could profit by simply stealing newly published works from one country, and publishing them in another, and vice versa.|thumb|300px]]
The 1886 [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] first established recognition of copyrights among [[Sovereignty|sovereign nations]], rather than merely bilaterally. Under the Berne Convention, copyrights for [[creative works]] do not have to be asserted or declared, as they are automatically in force at creation: an author need not "register" or "apply for" a copyright in countries adhering to the Berne Convention.<ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5">{{cite web |url=http://www.wipo.int/treaties/en/ip/berne/trtdocs_wo001.html#P109_16834 |title=Berne Convention for the Protection of Literary and Artistic Works Article 5 |accessdate=2011-11-18 |publisher=World Intellectual Property Organization}}</ref> As soon as a work is "fixed", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100&nbsp;years later with the passage of the ''Copyright, Designs and Patents Act of 1988''. The United States did not sign the Berne Convention until 1989.<ref>Garfinkle, Ann M; Fries, Janet; Lopez, Daniel; Possessky, Laura (1997). "Art conservation and the legal obligation to preserve artistic intent". [[JAIC]]   36 (2): 165–179.</ref>

The United States and most [[Latin America]]n countries instead entered into the [[Buenos Aires Convention]] in 1910, which required a copyright notice on the work (such as ''[[all rights reserved]]''), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms.<ref>[http://www.copyright.gov/circs/circ38a.pdf "International Copyright Relations of the United States"], U.S.&nbsp;Copyright Office Circular No.&nbsp;38a, August&nbsp;2003.</ref><ref>[http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf Parties to the Geneva Act of the Universal Copyright Convention] as of 2000-01-01: the dates given in the document are dates of ratification, not dates of coming into force. The Geneva Act came into force on 16 September 1955, for the first twelve to have ratified (which included four non-members of the Berne Union as required by Art.&nbsp;9.1), or three months after ratification for other countries. {{webarchive |url=https://web.archive.org/web/20080625003242/http://www.unesco.org/culture/copyright/html_eng/ucc52ms.pdf |date=25 June 2008 }}</ref><ref>[http://www.copyright.ht/en 165&nbsp;Parties to the Berne Convention for the Protection of Literary and Artistic Works] as of May 2012.</ref> The [[Universal Copyright Convention]] was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the [[Soviet Union]] and developing nations.

The regulations of the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] are incorporated into the [[World Trade Organization]]'s [[Agreement on Trade-Related Aspects of Intellectual Property Rights|TRIPS]] agreement (1995), thus giving the Berne Convention effectively near-global application.<ref name="Contemporary Intellectual Property: Law and Policy">{{cite book |title=Contemporary Intellectual Property: Law and Policy|url= https://www.google.com/books?id=_Iwcn4pT0OoC&dq=contemporary+intellectual+property&source=gbs_navlinks_s |page=39 |author1=MacQueen, Hector L |author2=Charlotte Waelde |author3=Graeme T Laurie |isbn=978-0-19-926339-4 |year=2007 |publisher=Oxford University Press}}</ref> 

In 1961, the [[United International Bureaux for the Protection of Intellectual Property]] signed the [[Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations]]. In 1996, this organization was succeeded by the founding of the [[World Intellectual Property Organization]], which launched the 1996 [[WIPO Performances and Phonograms Treaty]] and the 2002 [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]], which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The [[Trans-Pacific Partnership]] includes [[Trans-Pacific Partnership Intellectual Property Provisions|intellectual Property Provisions]] relating to copyright.

Copyright laws are standardized somewhat through these international conventions such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] and [[Universal Copyright Convention]]. These multilateral treaties have been ratified by nearly all countries, and [[international organizations]] such as the [[European Union]] or [[World Trade Organization]] require their member states to comply with them.

==Obtaining protection==
===Ownership===
The original holder of the copyright may be the employer of the author rather than the author himself, if the work is a "[[work for hire]]".<ref>17 U.S.C. § 201(b); Cmty. for Creative Non-Violence v. Reid, 490 U.S. 730 (1989)</ref> For example, in [[English law]] the ''Copyright, Designs and Patents Act'' 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a "Work for Hire."

===Eligible works===
Copyright may apply to a wide range of creative, intellectual, or artistic forms, or "works". Specifics vary by [[jurisdiction]], but these can include [[poem]]s, [[theses]], [[drama|plays]] and other [[book|literary works]], [[film|motion pictures]], [[choreography]], [[music|musical compositions]], [[sound recording]]s, [[painting]]s, [[drawing]]s, [[sculpture]]s, [[photography|photographs]], [[computer software]], [[radio]] and [[television]] [[Broadcasting|broadcasts]], and [[industrial design]]s. Graphic [[designs]] and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.<ref name="Intellectual Property and Information Wealth: Copyright and related rights">{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights|url=https://www.google.com/books?id=tgK9BzcF5WgC&dq=statute+of+anne+copyright&lr=&as_brr=3&source=gbs_navlinks_s|page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}</ref><ref>{{cite web |url= http://www.wipo.int/freepublications/en/intproperty/909/wipo_pub_909.pdf |format=PDF| last = World Intellectual Property Organization | title= Understanding Copyright and Related Rights|publisher=WIPO|accessdate=11 August 2016|page=8}}</ref>

Copyright does not cover ideas and information themselves, only the form or manner in which they are expressed.<ref name="Art and copyright">{{cite book |title=Art and copyright|url=https://www.google.com/books?id=h-XBqKIryaQC&dq=idea-expression+dichotomy&lr=&as_brr=3&source=gbs_navlinks_s|pages=48–49 |author=Simon, Stokes |isbn=978-1-84113-225-9 |year=2001 |publisher=Hart Publishing }}</ref> For example, the copyright to a [[Mickey Mouse]] cartoon restricts others from making copies of the cartoon or creating [[derivative work]]s based on [[The Walt Disney Company|Disney's]] particular [[anthropomorphic]] mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's.<ref name="Art and copyright"/> Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, [[Steamboat Willie]] is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.

===Originality===
{{main|Threshold of originality}}
Typically, a work must meet [[Threshold of originality|minimal standards of originality]] in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the [[United Kingdom]] there has to be some "skill, labour, and judgment" that has gone into it.<ref>''Express Newspaper Plc v News (UK) Plc'', F.S.R. 36 (1991)</ref> In [[Australia]] and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a [[trademark]] instead.

Copyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.

===Registration===
{{main|Copyright registration}}
[[File:Fermat Last Theorem "proof" registered by Ukraine officials.jpg|thumb|right|A copyright certificate for proof of the Fermat theorem, issued by the State Department of Intellectual Property of Ukraine.]]
In all countries where the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce his or her exclusive rights.<ref name="Berne Convention for the Protection of Literary and Artistic Works Article 5"/> However, while registration isn't needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as ''[[prima facie]]'' evidence of a valid copyright and enables the copyright holder to seek [[statutory damages for copyright infringement|statutory damages]] and attorney's fees.<ref>{{cite web|title=Subject Matter and Scope of Copyright|url=http://copyright.gov/title17/92chap1.pdf|website=copyright.gov|accessdate=4 June 2015}}</ref> (In the USA, registering after an infringement only enables one to receive actual damages and lost profits.)

A widely circulated strategy to avoid the cost of copyright registration is referred to as the [[poor man's copyright]]. It proposes that the creator send the work to himself in a sealed envelope by registered mail, using the [[postmark]] to establish the date. This technique has not been recognized in any published opinions of the United States courts. <!-- Note to editors: The previously-worded statement, "This technique has not been recognized by any United States court" is overbroad because not all such cases are reported, and it is impossible to know whether this is correct.--> The United States Copyright Office says the technique is not a substitute for actual registration.<ref>{{cite web|title=Copyright in General (FAQ)|url=http://www.copyright.gov/help/faq/faq-general.html#poorman|publisher=U.S Copyright Office|accessdate=11 Aug 2016}}</ref> The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original nor who the creator of the work is.<ref>[http://www.ipo.gov.uk/copy/c-claim/c-register.htm "Copyright Registers"], United Kingdom Intellectual Property Office</ref><ref>[http://www.ipo.gov.uk/types/copy/c-about/c-auto.htm "Automatic right"], United Kingdom Intellectual Property Office</ref>  <!-- Note to editors: The previously-worded statement, "The United Kingdom Intellectual Property Office discusses the technique but does not recommend its use." overstates the UK IPO position; the IPO does NOT recommend against the PMC approach.-->

===Fixing===
The [[Berne Convention]] allows member countries to decide whether creative works must be "fixed" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: "It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form." Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be "fixed in a tangible medium of expression" to obtain copyright protection.<ref name="cyber.law.harvard.edu">See Harvard Law School, [http://cyber.law.harvard.edu/copyrightforlibrarians/Module_3:_The_Scope_of_Copyright_Law#Fixation ''Module 3: The Scope of Copyright Law'']. See also Tyler T. Ochoa, [http://digitalcommons.law.scu.edu/chtlj/vol20/iss4/5 ''Copyright, Derivative Works and Fixation: Is Galoob a Mirage, or Does the Form(GEN) of the Alleged Derivative Work Matter?''], 20 {{smallcaps|Santa Clara High Tech.}} L.J. 991, 999–1002 (2003) ("Thus, both the text of the Act and its legislative history demonstrate that Congress intended that a derivative work does not need to be fixed in order to infringe."). The legislative history of the 1976 Copyright Act says this difference was intended to address transitory works such as ballets, pantomimes, improvised performances, dumb shows, mime performances, and dancing.</ref> U.S. law requires that the fixation be stable and permanent enough to be "perceived, reproduced or communicated for a period of more than transitory duration." Similarly, Canadian courts consider fixation to require that the work be "expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance."<ref name="cyber.law.harvard.edu"/>

===Copyright notice===
{{main|Copyright notice}}
[[File:Copyright.svg|thumb|upright|A copyright symbol used in copyright notice.]]
Before 1989, United States law required the use of a copyright notice, consisting of the [[copyright symbol]] (©, the letter '''C''' inside a circle), the abbreviation "Copr.", or the word "Copyright", followed by the year of the first publication of the work and the name of the copyright holder.<ref>Copyright Act of 1976, {{USPL|94|553}}, 90 Stat. 2541, § 401(a) (19 October 1976)</ref><ref>The Berne Convention Implementation Act of 1988 (BCIA), {{USPL|100|568}}, 102 Stat. 2853, 2857. One of the changes introduced by the BCIA was to section&nbsp;401, which governs copyright notices on published copies, specifying that notices "may be placed on" such copies; prior to the BCIA, the statute read that notices "shall be placed on all" such copies. An analogous change was made in section&nbsp;402, dealing with copyright notices on phonorecords.</ref> Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a [[sound recording copyright symbol]] (℗, the letter&nbsp;'''P''' inside a circle), which indicates a sound recording copyright, with the letter&nbsp;'''P''' indicating a "phonorecord". In addition, the phrase ''[[All rights reserved]]'' was once required to assert copyright, but that phrase is now legally obsolete.

In 1989 the United States enacted the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] Implementation Act, amending the 1976&nbsp;Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic.<ref>{{cite web|url=http://www.copyright.gov/circs/circ03.pdf |title=U.S. Copyright Office – Information Circular |format=PDF |accessdate=2012-07-07}}</ref> However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit&nbsp;– using notices of this form may reduce the likelihood of a defense of "innocent infringement" being successful.<ref>[[17 U.S.C.]]{{UnitedStatesCodeSec|17|401(d)}}</ref>

==Enforcement==
Copyrights are generally enforced by the holder in a [[Civil law (private law)|civil law]] court, but there are also criminal infringement statutes in some jurisdictions. While [[copyright registry|central registries]] are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily [[legal proof|prove]] that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the [[RIAA]] are increasingly targeting the [[file sharing]] home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (See: [[Legal aspects of file sharing]])

In most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative and or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.

===Copyright infringement===
{{main|Copyright infringement}}
For a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws and/or adheres to a bilateral treaty or established international convention such as the [[Berne Convention for the Protection of Literary and Artistic Works|Berne Convention]] or [[World Intellectual Property Organization Copyright Treaty|WIPO Copyright Treaty]]. Improper use of materials outside of legislation is deemed "unauthorized edition", not copyright infringement.<ref>{{Cite journal | last1 = Owen | first1 = L. | doi = 10.1087/09531510125100313 | title = Piracy | journal = Learned Publishing | volume = 14 | pages = 67–70 | year = 2001 | pmid =  | pmc = }}</ref>

Copyright infringement most often occurs to software, film and music. However, infringement upon books and other text works remains common, especially for educational reasons. Statistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available.<ref>Butler, S. Piracy Losses "Billboard" 199(36)</ref> Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect.<ref>{{cite web|url=http://www.ejpd.admin.ch/content/ejpd/de/home/dokumentation/mi/2011/2011-11-30.html |title=Urheberrechtsverletzungen im Internet: Der bestehende rechtliche Rahmen genügt |publisher=Ejpd.admin.ch}}</ref> In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.<ref>{{cite journal|publisher=Social Science Electronic Publishing|url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2425386|title=Video Killed the Radio Star? Online Music Videos and Digital Music Sales|ISSN=2042-2695|year=2014|authors=Tobias Kretschmer & Christian Peukert}}</ref>

==Rights granted==
===Exclusive rights===
Several exclusive rights typically attach to the holder of a copyright:
* to produce copies or reproductions of the work and to sell those copies (including, typically, electronic copies)
* to import or export the work
* to create [[derivative work]]s (works that adapt the original work)
* to perform or display the work publicly
* to sell or cede these rights to others
* to transmit or display by radio or video.<ref name=autogenerated1>{{cite book |title=Intellectual Property and Information Wealth: Copyright and related rights |page=346 |author=Peter K, Yu |isbn=978-0-275-98883-8 |year=2007 |publisher=Greenwood Publishing Group}}</ref>

The phrase "exclusive right" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a "negative right", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the [[unregistered design right]] in [[English law]] and [[European law]]. The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a [[Philosophy of copyright|philosophical interpretation of copyright law]] that is not universally shared. There is also debate on whether copyright should be considered a [[property right]] or a [[Moral rights (copyright law)|moral right]].<ref>Tom G. Palmer, [http://www.tomgpalmer.com/wp-content/uploads/papers/morallyjustified.pdf "Are Patents and Copyrights Morally Justified?"] Accessed 5 February 2013.</ref>

If a pictorial, graphic or sculptural work is a useful article, it is copyrighted only if its aesthetic features are separable from its utilitarian features. A useful article is an article having an intrinsic utilitarian function that is not merely to portray the appearance of the article or to convey information. They must be separable from the functional aspect to be copyrighted.<ref name="U.S Copyright Office - Copyright Law: Chapter 1">{{cite web |url=http://www.copyright.gov/title17/92chap1.pdf |title=U.S Copyright Office – Copyright Law: Chapter 1 |accessdate=2012-06-27}}</ref>

===Duration===<!-- This section is linked from [[Little Nemo]] -->
{{main|Copyright term|List of countries' copyright length}}
[[File:Tom Bell's graph showing extension of U.S. copyright term over time.svg|thumb|300px|Expansion of U.S. copyright law (currently based on the date of creation or publication).]]
Copyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been [[Publication|published]], and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States<ref>{{usc|17|305}}</ref> and the United Kingdom<ref>The Duration of Copyright and Rights in Performances Regulations 1995, [http://www.opsi.gov.uk/si/si1995/Uksi_19953297_en_3.htm part II], Amendments of the UK Copyright, Designs and Patents Act 1988</ref>), copyrights expire at the end of the calendar year in question.

The length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.<ref>{{cite book
|title=Copyright: Sacred Text, Technology, and the DMCA|last=Nimmer|first=David |publisher=Kluwer Law International|year=2003|isbn=  978-90-411-8876-2|oclc=50606064|page=63|url= https://books.google.com/books?id=RYfRCNxgPO4C}}</ref>

In the United States, all books and other works published before 1923 have expired copyrights and are in the public domain.<ref>"[http://copyright.cornell.edu/resources/publicdomain.cfm Copyright Term and the Public Domain in the United States].", ''[[Cornell University]]''.</ref> In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain.<ref>See Peter B. Hirtle, "Copyright Term and the Public Domain in the United States 1 January 2015" [https://copyright.cornell.edu/resources/publicdomain.cfm online at footnote 8]</ref>  Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.

But if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the U.S., the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.

In 1998, the length of a copyright in the United States was increased by 20 years under the [[Copyright Term Extension Act]]. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.<ref>Lawrence Lessig, ''Copyright's First Amendment'', 48 UCLA L. Rev. 1057, 1065 (2001)</ref>

{{globalize/US|date=September 2016}}

==Limitations and exceptions==
{{main|Limitations and exceptions to copyright|Traditional safety valves}}

In many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. It should be noted that US copyright does NOT cover names, title, short phrases or Listings (such as ingredients, recipes, labels, or formulas).<ref>[http://copyright.gov/circs/circ34.pdf (2012) ''Copyright Protection Not Available for Names, Titles, or Short Phrases'' U.S. Copyright Office]</ref> However, there are protections available for those areas copyright does not cover – such as [[trademark]]s and [[patent]]s.

There are some exceptions to what copyright will protect. Copyright will not protect:
* Names of products
* Names of businesses, organizations, or groups
* Pseudonyms of individuals
* Titles of works
* Catchwords, catchphrases, mottoes, slogans, or short advertising expressions
* Listings of ingredients in recipes, labels, and formulas, though the directions can be copyrighted

===Idea–expression dichotomy and the merger doctrine===
{{main|Idea–expression divide}}

The idea–expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of [[Baker v. Selden]], has since been codified by the [[Copyright Act of 1976]] at 17 U.S.C. § 102(b).

===The first-sale doctrine and exhaustion of rights===

{{main|First-sale doctrine|Exhaustion of rights}}
Copyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or [[compact disc|CD]]. In the United States this is known as the [[first-sale doctrine]], and was established by the [[court]]s to clarify the legality of reselling books in second-hand [[bookstore]]s.

Some countries may have [[parallel importation]] restrictions that allow the copyright holder to control the [[aftermarket (merchandise)|aftermarket]]. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as [[exhaustion of rights]] in other countries and is a principle which also applies, though somewhat differently, to [[patent]] and [[trademark]] rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.

In ''[[Kirtsaeng v. John Wiley & Sons, Inc.]]'',<ref>{{cite web|title=John Wiley & Sons Inc. v. Kirtsaeng |url= http://www.supremecourt.gov/opinions/12pdf/11-697_d1o2.pdf}}</ref> in 2013, the [[United States Supreme Court]] held in a 6-3 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission.  The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.

In addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying his or her own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement [[Moral rights (copyright law)|moral rights]], a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.

===Fair use and fair dealing===
{{main|Fair use|Fair dealing}}
Copyright does not prohibit all copying or replication. In the United States, the [[Fair use|fair use doctrine]], codified by the [[United States Copyright Act of 1976|Copyright Act of 1976]] as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:
# the purpose and character of one's use
# the nature of the copyrighted work
# what amount and proportion of the whole work was taken, and
# the effect of the use upon the potential market for or value of the copyrighted work.<ref>{{cite web|url=http://www4.law.cornell.edu/uscode/17/107.html |title=US CODE: Title 17,107. Limitations on exclusive rights: Fair use |publisher=.law.cornell.edu |date=2009-05-20 |accessdate=2009-06-16}}</ref>

In the [[United Kingdom]] and many other [[Commonwealth of Nations|Commonwealth]] countries, a similar notion of fair dealing was established by the [[court]]s or through [[legislation]]. The concept is sometimes not well defined; however in [[Canada]], private copying for personal use has been expressly permitted by statute since 1999. In ''[[Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)]]'', 2012 SCC 37, the [[Supreme Court of Canada]] concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the ''Copyright Act 1968'' (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. [[legal advice]]). Under current [[Law of Australia|Australian law]], although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to “format shift” that work from one medium to another for personal, private use, or to “time shift” a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.

In the United States the AHRA ([[Audio Home Recording Act]] Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.

:''Section 1008. Prohibition on certain infringement actions''

:''No action may be brought under this title alleging infringement of copyright based on the manufacture, importation, or distribution of a digital audio recording device, a digital audio recording medium, an analog recording device, or an analog recording medium, or based on the noncommercial use by a consumer of such a device or medium for making digital musical recordings or analog musical recordings.''

Later acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The [[Digital Millennium Copyright Act]] prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner.<ref name="Intellectual Property and Information Wealth: Copyright and related rights"/> An appellate court has held that fair use is not a defense to engaging in such distribution.

The [[copyright directive]] allows EU member states to implement a set of exceptions to copyright. Examples of those exceptions are:
*photographic reproductions on paper or any similar medium of works (excluding sheet music) provided that the rightholders receives fair compensation,
*reproduction made by libraries, educational establishments, museums or archives, which are non-commercial
*archival reproductions of broadcasts,
*uses for the benefit of people with a disability,
*for demonstration or repair of equipment,
*for non-commercial research or private study
*when used in [[parody]]

===Accessible copies===
It is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired persons without permission from the copyright holder.<ref>[http://www.copyright.gov/title17/92chap1.html#121 Copyright Law of the USA, Chapter 1 Section 121]</ref><ref>{{cite web|url=http://www.rnib.org.uk/xpedio/groups/public/documents/publicwebsite/public_cvipsact2002.hcsp|title=Copyright (Visually Impaired Persons) Act 2002 comes into force|publisher=Royal National Institute of Blind People|date=1 January 2011|accessdate=11 Aug 2016}}</ref>

=={{anchor|Transfer and licensing, and assignment}} Transfer, assignment and licensing==
{{see also|Collective rights management|extended collective licensing|Compulsory license|Copyright transfer agreement}}
[[File:All rights reserved.jpg|thumb|right|300px|DVD: [[All Rights Reserved]].]]
A copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another.<ref name="WIPO Guide on the Licensing of Copyright and Related Rights">{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&dq=copyright+transfer+and+licensing&as_brr=3&source=gbs_navlinks_s|isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization|page=15}}</ref> For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the [[Internet]]; however, the [[record industry]] attempts to provide promotion and marketing for the artist and his or her work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy and/or distribute the work in a particular region or for a specified period of time.

A transfer or licence may have to meet particular formal requirements in order to be effective,<ref name="WIPO Guide on the Licensing of Copyright and Related Rights(2)">{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&dq=copyright+transfer+and+licensing&as_brr=3&source=gbs_navlinks_s|page=8 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}</ref> for example under the Australian [[Copyright law of Australia#Copyright Act 1968|Copyright Act 1968]] the copyright itself must be expressly transferred in writing. Under the U.S. Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under [[Law of the United States|U.S. law]]. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a [[real estate]] transaction.

Copyright may also be [[license]]d.<ref name="WIPO Guide on the Licensing of Copyright and Related Rights"/> Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed [[statutory license]] (e.g. musical works in the United States used for radio broadcast or performance). This is also called a [[compulsory license]], because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made.<ref name="WIPO Guide on the Licensing of Copyright and Related Rights(3)">{{cite book |title=WIPO Guide on the Licensing of Copyright and Related Rights|url=https://www.google.com/books?id=LvRRvXBIi8MC&dq=copyright+transfer+and+licensing&as_brr=3&source=gbs_navlinks_s|page=16 |isbn=978-92-805-1271-7 |year=2004 |publisher=World Intellectual Property Organization}}</ref> Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, [[copyright collective]]s or [[collecting societies]] and [[performance rights organisation|performing rights organizations]] (such as [[ASCAP]], [[Broadcast Music Incorporated|BMI]], and [[SESAC]]) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.

===Free licences===
Copyright licenses known as ''open'' or [[free license]]s seek to grant several rights to licensees, either for a fee or not, to an effect inspired by the [[public domain]]. ''Free'' in this context isn't much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the [[Free Software Definition]], the [[Debian Free Software Guidelines]], the [[Open Source Definition]] and the [[Definition of Free Cultural Works]]. Further refinements to these licenses have resulted in categories such as [[copyleft]] and [[permissive license|permissive]]. Common examples of free licences are the [[GNU General Public License]], [[BSD license]]s and some [[Creative Commons licenses]].

Founded in 2001 by [[James Boyle (academic)|James Boyle]], [[Lawrence Lessig]], and [[Hal Abelson]], the [[Creative Commons]] (CC) is a non-profit organization<ref name="CC">{{cite web|url=http://creativecommons.org/ |title=Creative Commons Website|website=creativecommons.org|accessdate=24 October 2011}}</ref> which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, [[gratis versus libre|gratis]]. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.<ref name="CC" />

Terms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them aren't properly free per the above definitions and per Creative Commons' own advice). These are based upon copyright holder stipulations such as whether he or she is willing to allow modifications to the work, whether he or she permits the creation of derivative works and whether he or she is willing to permit commercial use of the work.<ref name="Rubin">Rubin, R. E. (2010) 'Foundations of Library and Information Science: Third Edition', Neal-Schuman Publishers, Inc., New York, p. 341</ref> {{As of|2009}} approximately 130 million individuals had received such licenses.<ref name="Rubin" />

==Criticism==
Some sources are critical of particular aspects of the copyright system. This is known as a debate over [[copynorms]]. Particularly on the internet, there is discussion about the [[copyright aspects of downloading and streaming]], the [[copyright aspects of hyperlinking and framing]]. Such concerns are often couched in the language of [[digital rights]] and [[database right]]s. Discussions include ''[[Free Culture (book)|Free Culture]]'', a 2004 book by [[Lawrence Lessig]]. Lessig coined the term [[permission culture]] to describe a worst-case system. ''[[Good Copy Bad Copy]]'' (documentary) and [[RiP!: A Remix Manifesto]], discuss copyright. Some suggest an [[alternative compensation system]]. 

Some groups reject copyright altogether, taking an [[anti-copyright]] stance. The perceived inability to enforce copyright online leads some to advocate [[Crypto-anarchism|ignoring legal statutes when on the web]].

==Public domain==
{{main|Public domain}}
Copyright, like other [[intellectual property rights]], is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be freely used or exploited by anyone. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a [[common law copyright]]. Public domain works should not be confused with works that are publicly available. Works posted in the [[internet]], for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.

==See also==
{{Portal|Social and political philosophy|Law}}
{{colbegin|colwidth=15em}}
* [[Adelphi Charter]]
* [[Artificial scarcity]]
* [[Conflict of laws]]
* [[Copyright Alliance]]
* [[Copyright in architecture in the United States]]
* [[Copyright on the content of patents and in the context of patent prosecution]]
* [[Copyright for Creativity]]
* [[Copyright infringement of software]]
* [[Copyright on religious works]]
* [[Creative Barcode]]
* [[Digital rights management]]
* [[Digital watermarking]]
* [[Entertainment law]]
* [[Freedom of panorama]]
* [[Intellectual property education]]
* [[Intellectual property protection of typefaces]]
* [[List of Copyright Acts]]
* [[List of copyright case law]]
* [[Model release]]
* [[Paracopyright]]
* [[Photography and the law]]
* [[Pirate Party]]
* [[Private copying levy]]
* [[Production music]]
* [[Rent-seeking]]
* [[Reproduction fees]]
* [[Samizdat]]
* [[Software copyright]]
* [[Threshold pledge system]]
{{colend}}

==References==
{{Reflist|30em}}

==Further reading==
{{refbegin|30em}}
* {{Cite book
  |author=Dowd, Raymond J.
  |title=Copyright Litigation Handbook
  |publisher=Thomson West |edition=1st |year=2006
  |isbn=0-314-96279-4
  |ref=Dowd, Litigation handbook
}}
* Ellis, Sara R. ''Copyrighting Couture: An Examination of Fashion Design Protection and Why the DPPA and IDPPPA are a Step Towards the Solution to Counterfeit Chic'', 78 Tenn. L. Rev. 163 (2010), ''available at'' http://ssrn.com/abstract=1735745.
* {{Cite book
  |author1=Gantz, John  |author2=Rochester, Jack B.
  |title=Pirates of the Digital Millennium
  |publisher=Financial Times Prentice Hall
  |year=2005
  |isbn=0-13-146315-2
  |ref=Gantz, Pirates
}}
* [[Shuman Ghosemajumder|Ghosemajumder, Shuman]]. ''[http://dspace.mit.edu/handle/1721.1/8438 Advanced Peer-Based Technology Business Models]''. [[MIT Sloan School of Management]], 2002.
* [[Bruce Lehman|Lehman, Bruce]]: ''Intellectual Property and the National Information Infrastructure'' (Report of the Working Group on Intellectual Property Rights, 1995)
* Lindsey, Marc: ''Copyright Law on Campus.'' [[Washington State University]] Press, 2003. ISBN 978-0-87422-264-7.
* Mazzone, Jason. ''[[Copyfraud]]''. [http://ssrn.com/abstract=787244 SSRN]
* McDonagh, Luke. ''Is Creative use of Musical Works without a licence acceptable under Copyright?'' International Review of Intellectual Property and Competition Law (IIC) 4 (2012) 401-426, available at [http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2521081 SSRN]
* {{Cite book
  | last = Nimmer | first = Melville |authorlink=Melville Nimmer |author2=David Nimmer | title = [[Nimmer on Copyright]] | publisher = Matthew Bender | year=1997| isbn = 0-8205-1465-9
}}
* {{Cite book
  |title=Copyright in Historical Perspective
  |author=Patterson, Lyman Ray
  |year=1968|publisher=Vanderbilt University Press
  |isbn=0-8265-1373-5
  |version=Online Version
}}
* Rife, by Martine Courant. ''Convention, Copyright, and Digital Writing''  (Southern Illinois University Press; 2013) 222 pages; Examines legal, pedagogical, and other aspects of online authorship.
* {{cite book | last = Rosen | first = Ronald | title = Music and Copyright | publisher = Oxford University Press | location = Oxford Oxfordshire | year = 2008 | isbn = 0-19-533836-7 }}
* Shipley, David E. [http://ssrn.com/abstract=1076789 Thin But Not Anorexic: Copyright Protection for Compilations and Other Fact Works] UGA Legal Studies Research Paper No. 08-001; Journal of Intellectual Property Law, Vol. 15, No. 1, 2007.
* Silverthorne, Sean. ''[http://hbswk.hbs.edu/item.jhtml?id=4206&t=innovation Music Downloads: Pirates- or Customers?]''. [[Harvard Business School|Harvard Business School Working Knowledge]], 2004.
* Sorce Keller, Marcello. "Originality, Authenticity and Copyright", ''Sonus'', VII(2007), no. 2, pp.&nbsp;77–85.
* {{Cite book
  |author1=Steinberg, S.H.  |author2=Trevitt, John
  |title=Five Hundred Years of Printing
  |location=London and New Castle |publisher=The British Library and Oak Knoll Press
  |edition=4th |year=1996
  |isbn=1-884718-19-1
  |ref=Steinberg, Five hundred years
}}
* {{Cite book
  |title=The Copy/South Dossier: Issues in the Economics, Politics and Ideology of Copyright in the Global South 
  |url=http://copysouth.org/en/documents/csdossier.pdf
  |editor1=Story, Alan |editor2=Darch, Colin |editor3=Halbert, Deborah |year=2006|publisher=Copy/South Research Group
  |isbn=978-0-9553140-1-8
}}
* [http://whynotaskme.org/ WhyNotAskMe.org]: ''Organization demanding democratic participation in copyright legislation and a moratorium on secret and fast-tracked copyright negotiations''
{{refend}}

==External links==
{{Wikisource1911Enc|Copyright}}
{{Wikisource|Wikisource:Copyright law|Copyright law}}
{{Spoken Wikipedia|En-Copyright.ogg|2008-12-30}}
{{Library resources box}}
* {{Wikiquote-inline}}
* {{Commons-inline|Copyright}}
* {{dmoz|Society/Law/Legal_Information/Intellectual_Property/Copyrights}}
* [http://www.wipo.int/clea/en/ Collection of laws for electronic access] from [[WIPO]] – intellectual property laws of many countries
* [http://purl.fdlp.gov/GPO/gpo55676 Compendium of Copyright Practices] (3rd ed.) [[United States Copyright Office]]
* [http://ucblibraries.colorado.edu/govpubs/us/copyrite.htm Copyright] from ''UCB Libraries GovPubs''
* [http://www.ipo.gov.uk/types/copy.htm About Copyright] at the UK Intellectual Property Office
* [http://www.lawtech.jus.unitn.it/index.php/copyright-history/bibliography A Bibliography on the Origins of Copyright and Droit d'Auteur]
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ 6.912 Introduction to Copyright Law] taught by Keith Winstein, MIT OpenCourseWare January IAP 2006
* [http://www.wipo.int/treaties/en/ShowResults.jsp?country_id=ALL&start_year=ANY&end_year=ANY&search_what=C&treaty_id=15 Copyright Berne Convention: Country List]  List of the 164 members of the Berne Convention for the protection of literary and artistic works
* [http://www.copyrightservice.co.uk/copyright/p01_uk_copyright_law UK Copyright Law fact sheet] (April 2000) a concise introduction to UK Copyright legislation
* [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance/reports/ipr.aspx IPR Toolkit – An Overview, Key Issues and Toolkit Elements] (September 2009) by Professor Charles Oppenheim and Naomi Korn at the [http://www.jisc.ac.uk/whatwedo/themes/content/contentalliance.aspx Strategic Content Alliance]
* [http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-912-introduction-to-copyright-law-january-iap-2006/ MIT OpenCourseWare 6.912 Introduction to Copyright Law] Free self-study course with video lectures as offered during the January 2006, Independent Activities Period (IAP)
* [http://www.loc.gov/rr/rarebook/coll/067.html Early Copyright Records] From the [http://www.loc.gov/rr/rarebook/ Rare Book and Special Collections Division at the Library of Congress]
* [http://copyright.gov/title17/ Copyright Law of the United States Documents], US Government
{{Copyright law by country}}
{{Intellectual property activism}}

{{Authority control}}

[[Category:Copyright law| ]]
[[Category:Data management]]
[[Category:Intellectual property law]]
[[Category:Metadata]]
[[Category:Monopoly (economics)]]
[[Category:Public records]]
<=====doc_Id=====>:326
<=====title=====>:
Data warehouse automation
<=====text=====>:
'''Data warehouse automation''' or DWA refers to the process of accelerating and automating the [[data warehouse]] development cycles, while assuring quality and consistency. DWA is believed to provide automation of the entire lifecycle of a data warehouse, from source [[Systems analysis|system analysis]] to [[Software testing|testing]] to  [[documentation]]. It helps improve productivity, reduce cost, and improve overall quality.<ref>{{cite web|title=Automate and accelerate your data transformations|url=http://www.attunity.com/products/prepare-data-compose|website=www.attunity.com|publisher=Attunity|accessdate=7 December 2015}}</ref>

==General==
Data warehouse automation primarily focuses on automation of each and every step involved in the lifecycle of a data warehouse, thus reducing the efforts required in managing it.<ref>{{cite web|title=New Buzzword! Data Warehouse Automation|url=http://blogs.jetreports.com/2015/03/05/new-buzzword-data-warehouse-automation/|website=blogs.jetreports.com|publisher=jetreports|accessdate=7 December 2015}}</ref>
Data warehouse automation works on the principles of design patterns. It comprises a central repository of design patterns, which encapsulate architectural standards as well as best practices for data design, data management, data integration, and data usage.<ref>{{cite web|url=https://www.wherescape.com/media/1988/data-warehouse-automation-decision-guide.pdf|title=Data Warehouse Automation - A Decision Guide|website=www.wherescape.com|publisher=David L. Wells, Infocentric LLC|accessdate=7 December 2015}}</ref>
In November 2015, an analyst firm has published a guide ''Which Data Warehouse Automation Tool is Right for You?'' covering four of the leading products in the DWA space.<ref>{{cite web|title=Which Data Warehouse Automation Tool is Right for You?|url=http://eckerson.com/register?content=which-data-warehouse-automation-tool-is-right-for-you|website=eckerson.com|publisher= Wayne Eckerson|accessdate=9 December 2015}}</ref> In November 2015, an international software and technology services company engaged in developing ‘agile tools’ for the data integration industry, was named by CIO Review as one of the 20 most promising productivity tools solution providers 2015 <ref>{{cite web|title=CIO Magazine Award - 20 Most promising productivity tools|url=http://analytixds.com/latest_news/analytix-data-services-wins-cio-reviews-2015/|website=www.analtyixds.com|publisher=AnalytiX DS|accessdate=25 November 2016}}</ref>

==Benefits==
Data warehouse automation can provide advantages like source data exploration, warehouse data models, ETL generation, test automation, metadata management, managed deployment, scheduling, change impact analysis and easier maintenance and modification of the data warehouse.<ref>{{cite web|title=Data Warehouse Automation (DWA)?|url=http://www.timextender.com/software/data-warehouse-automation/business-value/|website=timextender.com|publisher=TimeXtender Software 2015|accessdate=7 December 2015}}</ref>
More important than the technical features of DWA tools, however, is the ability to deliver projects faster and with less resources.<ref>{{cite web|title=Deliver Faster|url=http://kalido.com/products/kalido-information-engine/deliver-faster/|website=kalido.com|publisher=Magnitude Software|accessdate=9 December 2015}}</ref>

==References==
{{reflist}}

==External links==
* {{Dmoz|Computers/Software/Databases/Data_Warehousing/Data_Warehouse_Automation|Data Warehouse Automation}}
* [http://analytixds.com/wp-content/uploads/2016/07/analytix-data-services-top-20-cio-review.pdf CIO Magazine Award - 20 Most promising productivity tools 'CIO Review', November 10, 2015]

==See also==
*[[Data warehouse]]
*[[Data mart]]
*[[Data warehouse appliance]]
*[[Data integration]]

{{Data_warehouse}}

[[Category:Data management]]
[[Category:Data warehousing]]
<=====doc_Id=====>:329
<=====title=====>:
National Data Repository
<=====text=====>:
A '''National Data Repository''' ('''NDR''') is a data bank that seeks to preserve and promote a country’s natural resources data, particularly data related to the petroleum [[exploration and production]] (E&P) sector.

A National Data Repository is normally established by an entity that governs, controls and supports the exchange, capture, transference and distribution of E&P information, with the final target to provide the State with the tools and information to assure the growth, govern-ability, control, independence and sovereignty of the industry.

The two fundamental reasons for a country to establish an NDR are to '''preserve''' data generated inside the country by the industry, and to '''promote''' investments in the country by utilizing data to reduce the exploration, production, and transportation business risks.

Countries take different approaches towards preserving and promoting their natural resources data. The approach varies according to a country’s natural resources policies, level of openness, and its attitude towards foreign investment.

==Data types==
NDRs store a vast array of data related to a country’s natural resources. This includes wells, [[Well logging|well log data]], well reports, [[core sample]]s, [[seismic]] surveys, [[Seismic inversion#Post-stack seismic resolution inversion|post-stack seismic]], field data/tapes, seismic (acquisition/processing) reports, [[Oil production|production]] data, [[geological map]]s and reports, license data and [[geologic modeling|geological models]].

==Funding models==
Some NDRs are financed entirely by a country’s government. Others are industry-funded. Still some are hybrid systems, funded in part by industry and government.
NDRs typically charge fees for data requests and for data loading. The cost differs significantly between countries. In some cases an annual membership is charged to oil companies to store and access the data in the NDR.

==Standards body==
[[Energistics]] is the global energy standards resource center for the upstream oil and gas industry.

Energistics National Data Repository Work Group:
The standards body is Energistics.<ref>[http://energistics.org/energistics-standards-directory Energistics]</ref>

===Energistics-standards-directory===
Global regulators of upstream oil and natural gas information, including seismic, drilling, production and reservoir data, formed the National Data Repository (NDR) Work Group in 2008 to collaborate on the development of data management standards and to assist emerging nations with hydrocarbon reserves to better collect, maintain and deliver oil and gas data to the public and to the industry.

Ten countries, led by the [[Netherlands]], [[Norway]] and the [[United Kingdom]], formed NDR to share best practices and to formalize the development and deployment of data management standards for regulatory agencies. The other countries involved in the NDR Work Group’s formation are [[Australia]], [[Canada]], [[India]], [[Kenya]], [[New Zealand]], [[South Africa]] and the [[United States]].

Annual NDR Conference: Approximately every 18 months Energistics organizes a National Data Repository Conference. The purpose is to provide government and regulatory agencies from around the world an opportunity to attend a series of workshops dedicated to developing data exchange standards, improving communications with the oil and gas industry and learning data management techniques for natural resources information.<ref>[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group/ndr-meetings NDR Conference page on the Energistics website]</ref>

===Society of Exploration Geophysicists and The International Oil and Gas Producers Association===
The SEG is the custodian of the SEG standards which are used for the exchange, retention and release of seismic data.  They are commonly used by National Data Repositories with the SEGD and SEGY being the field and processed exchange standards respectively.

==NDRs around the world==

[https://www.google.com/maps/d/viewer?mid=1by9vDDoWwnZD0f8vNt_le2TThTU Click here to see a map of the NDRs around the world]

{| class="wikitable sortable"
|-
! Country  !!  Name  !!  Agency  !!  Scope  !! Status !!   Purposes   !! Data types/volumes !! Standards used !! Funding !! Website
|-
| {{flagcountry|Algeria}} || Banque de Données Nationale "BDN" || Agence Nationale pour la Valorisation des Ressources en Hydrocarbures (ALNAFT) || Onshore and Offshore Algeria || Ongoing project - agency created by new law in 2005 || Custodian of all E&P data of the country || Cultural, Seismic 2D & 3D, Wells, Data Wells, Wells report, Production, Facilities, Economical and Fiscality, Interpretation, Physical assets index, Data drilling, Transcription, Vectorisation, digitalization || ASCII, SEGY, UKOOA, LAS, DLIS, LIS, PDS, BIT, RODE, PDF, TIF....etc || Government funding/Agency revenue || http://www.alnaft.gov.dz/
|-
| {{flagcountry|Colombia}} || EPIS || Agencia Nacional de Hidrocarburos (ANH) || Onshore and offshore Colombia || Created originally for Ecopetrol and transferred to ANH when it was established in 2003. New system launched December 2009 || Promote and preserve all the technical E&P information assets of the country || wells, surveys, licenses, seismic sections, well reports, maps || REST Web services || Government funding ||http://www.epis.com.co
|-
|  {{flagcountry|Canada}} || CNSOPB || Nova Scotia Offshore Petroleum Board – Geoscience Research Centre- Digital Data Management Centre (DMC) || Offshore Nova Scotia, Canada ||  Operational since 2007 || To provide an effective & efficient system for the management of digital petroleum data, assist explorers in easily obtaining access to large volumes of data via the web, Data Preservation and Data Distribution ||  Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports || LAS, DLIS, SEGY || Funded 50/50 by the Federal and Provincial Governments with some funds from industry through cost recovery  || http://www.cnsopb.ns.ca/
|-
| {{flagcountry|Australia}} || PIMS ||Geoscience Australia || ||Active  || Various online and web based systems exist for E &P, geosciences
||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports ||  ||  ||
http://dbforms.ga.gov.au/pls/www/npm.pims_web.search
|-
| {{flagcountry|Western Australia}} || WAPIMS ||Government of Western Australia   || || Active || WAPIMS is a petroleum, geothermal and minerals exploration database ||Contains data on titles, wells, geophysical surveys and other petroleum exploration and production data submitted to DMP by the petroleum industry.
  ||  ||  || http://dmp.wa.gov.au
|-
| {{flagcountry|New South Wales}} ||  ||Government of New South Wales ||  || Active  || Various online geoscience databases to assist New South Wales including DIGS  ||  ||  ||  ||
http://www.dpi.nsw.gov.au/minerals
http://digsopen.minerals.nsw.gov.au/
|-
| {{flagcountry|Northern Territory}} ||  || Government of Northern Territory  ||  || Active  || Various online geoscience databases to assist Northern Territories ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
http://www.nt.gov.au/d/Minerals_Energy/index.cfm?header=Petroleum	
|-
| {{flagcountry|Queensland}} ||  || Government of Queensland ||  || Active  || Various online geoscience databases to assist Queensland including Q-DEX ||  ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
https://www.dnrm.qld.gov.au/
|-
| {{flagcountry|South Australia}} || SARIG || Government of South Australia  ||  || Active  || Various online geoscience databases to assist South Australia such as PEP-SA  || ||Wells, well log curves, well reports, cores and samples, field data/tapes, seismic (acquisition/processing) reports, production data, interpretative maps and reports  ||  ||
http://petroleum.statedevelopment.sa.gov.au/data_and_publications/sarig
https://sarig.pir.sa.gov.au/
|-
| {{flagcountry|Tasmania}} ||  ||  ||  ||   ||Various online geoscience databases to assist Tasmania   || || Active ||  ||
http://www.mrt.tas.gov.au/portal/page?_pageid=35,1&_dad=portal&_schema=PORTAL		
|-
| {{flagcountry|China }} || CNPC  || Chinese National Petroleum Corporation  ||  ||   || Various oil companies in China with CNPC the largest and parent of Petrochina ||  ||  ||  ||
http://www.cnpc.com.cn/en/
http://www.petrochina.com.cn/ptr/
http://www.cnooc.com.cn/
http://english.sinopec.com/index.shtml
|-
| {{flagcountry|Russia}} ||  ||  Sakhalin, DIGC RDC ||  || Various oil companies in Russia the largest being Rosneft which is state owned  ||  ||  ||  ||  ||
http://www.rosneft.com
http://www.lukoil.com
http://www.tnk-bp.com/en/
http://www.surgutneftegas.ru/
http://www.gazprom-neft.com/
http://www.tatneft.ru/wps/wcm/connect/tatneft/portal_rus/homepage/
|-
| {{flagcountry|Indonesia}} || Indonesia's National Data Centre (NDC) for petroleum, energy and minerals data || Agency for Research and Development in the Ministry of Energy and Mineral Resources of the Republic of Indonesia||Onshore & Offshore || In 1997 Indonesia established Migas Data Management (MDM) operated by PT. Patra Nusa Data (PND)  || PND manages and promotes petroleum investment opportunities by compiling and value adding available petroleum data and information. ||  ||  ||  || http://www.patranusa.com/
|-
|  {{flagcountry|New Zealand}} || New Zealand Online Exploration Database || New Zealand Petroleum & Minerals, Ministry of Business Innovation & Employment || New Zealand onshore and offshore out to the outer continental shelf.  || Opened to public in April 2007. || Data preservation, Investment facilitation, aid in monitoring regulatory compliance, maximise the return to the nation by informing public policy and business strategy. || Wells, well log curves, petroleum reports (includes wells and surveys), mineral reports, coal reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes, seismic acquisition/processing reports, geophysical and geochemical data acquired in mineral and coal exploration (incorporated as enclosures to reports), VSP (incorporated as enclosures to reports), Seismic survey observer logs. GIS data and projects (minerals and coal). Estimated total NDR Size: 2.5 TB loaded, 3.0 TB staged for loading, 40 TB field data offline.  || Closely follow Australian digital reporting standards. No naming standards for wells and surveys.  || 50% Government funding, 50% third party permit (license) fees paid by exploration companies. || https://data.nzpam.govt.nz
|-
| {{flagcountry|Jordan}} ||NRA  || Jordan Natural Resources Authority (NRA) ||Onshore || Active    || Online data room allows users to browse and select large data set quickly in a controlled and secure environment ||Reserves land records, field data, maps, engineering, seismic data, geological studies and well files.  ||  ||  || http://www.jordan.gov.jo
|-
| {{flagcountry|Angola}} ||  || Sonangol || Offshore Angola || Active || Promotion, Organisation & Management of all Exploration & Production (E&P) Data of Angola || Wells, surveys, licenses, seismic sections, well reports, maps ||  || Norad/OfD and NPD assistance || http://www.sonangol.co.ao
|-
| {{flagcountry|France}} ||BEPH ||  || French Territory || || Interactive maps of French territory of oil data are available to Internet users which includes: Permits for petroleum exploration, seismic exploration, oil drilling  (data, documents available) || Wells, Surveys, Licenses, Seismic Sections, Well Reports, Maps ||  ||  || http://www.beph.net/
|-
| {{flagcountry|São Tomé and Príncipe }} || ANP-STP  || National Petroleum Agency of São Tomé & Principe (ANP-STP)  || Offshore || ||  ||  ||  ||Norad/OfD and NPD assistance  || http://www.anp-stp.gov.st
|-
| {{flagcountry|Tanzania}} || TPDC  || Tanzania Petroleum Development Corp  ||  || Began in the early 1990s with Norwegian assistance || An E & P data archive centre || Geophysical survey data, Geological studies, Well drilling and completion reports, Cores and drill steam data ||  || Norad/OfD and NPD assistance || http://www.tpdc-tz.com
|-
| {{flagcountry|Oman}} || OGDR || Department of Petroleum Concession, Ministry of Oil and Gas || Onshore & Offshore || Operational, tendering OGDR as a managed service (fully outsourced) June 2015 || Preservation of E&P data, support concession promotion. || Well-related Data: Header, deviation, tops, field and processed logs, well documents. Seismic-related Data: Field and processed 2D/3D, Gravimag, VSP. || OGDR Data Submission Standard that uses industry standards where possible i.e. DLIS, SEG, UKOOA. || Government & concession holders.||  http://www.mog.gov.om/english/tabid/309/Default.aspx
|-
| {{flagcountry|Netherlands}} || DINO || The Geological Survey of the Netherlands, a division of [[Netherlands Organisation for Applied Scientific Research|TNO]] || The Netherlands including offshore waters ||Started in 2004. Currently BRO is being planned to succeed DINO. || To archive subsurface data of the Netherlands in one repository and provide easy access to the data to encourage multiple use of data.||  || WMS web services. DINO uses own naming conventions || 100% Government funding || http://www.nlog.nl/en/home/NLOGPortal.html
|-
| {{flagcountry|India}} ||DGH  || Directorate General of Hydrocarbons (DGH) ||  || Active - scheduled operation by April 2015
 ||  Establishing national data archival, improving data quality and access for quality exploration covering large area under exploration and providing basis for long term energy policy formulation as well as support OALP  ||Wells, Well Logs, Cores, Scanned core images, Seismic, Reports, production, Technical Reports
  ||  ||Government of India || http://www.dghindia.org/DataManagement.aspx#
|-
| {{flagcountry|Sri Lanka}} || PRDS || Ministry of Petroleum and Petroleum Resources Development ||  || Active since 2009 ||   The PRDS developed a website to disseminate petroleum data and information to public and to investors to assist promotion of offshore areas to attract investors for petroleum exploration ||Wells, surveys, licenses, seismic sections, well reports, maps. Data historic and current, archived on different media (paper, mylar, magnetic tape) || ||  || http://www.prds-srilanka.com/data/onlineData.faces
|-
| {{flagcountry|Argentina}} || ENARSA || Energia Argentina SA  || || Established in 2006  ||  ||  ||  ||  ||  http://www.enarsa.com.ar http://energia.mecon.gov.ar/upstream/US_Pterminados.asp
|-
| {{flagcountry|Peru}} || PeruPetro ||  ||  || Active ||  ||  || ||  || http://www.perupetro.com.pe
|-
| {{flagcountry|Kazakhstan}} ||  || Ministry of Energy and Mineral Resources of the Republic of Kazakhstan (MEMR) ||  || Active ||  ||  ||  ||  || http://www.petrodata.kz
|-
| {{flagcountry|Pakistan}} || PPEPDR || Directorate General Petroleum Concessions (DGPC)  ||  || Active since 2001 ||  || Repository contains more than 10 terabytes of secure petrotechnical data ||  ||  || http://www.ppepdr.net/
|-
| {{flagcountry|Nigeria}} ||Department of Petroleum Resources  ||  ||  || Active since December 2003.
 || Preserve, maintain the integrity and promote the National E&P data assets with improved quality, efficiency and accessibility in the most rapid, secure and reliable manner|| || International and PetroBank data management standards || Funded by Establishment Costs - one-off funding by Government and Running Costs - Subscription &  Transaction Fees by Operators ||http://ndr.dprnigeria.com/
|-
| {{flagcountry|Turkey}} || PetroBank MDS  || Turkish Petroleum Corporation (TPAO). It is NOC of Turkey. || 36˚-42˚ northern parallel and the 26-45˚ eastern meridian.  || Operational since 2007 || Data assets preservation, easy access to assets, assets access controlling and auditing, consolidation of assets, national archive, central management of all assets, standardization of assets according to international standards and naming conventions, working with the most convenient assets. || Wells, Well log curves, well reports, cores and samples, seismic surveys, post-stack seismic, field data/tapes and seismic acquisition/processing reports. || International and PetroBank data management standards || Funded fully by the Turkish Petroleum Corporation. Service usage is free of charge.  || http://www.tpao.gov.tr
|-
| {{flagcountry|Norway}} || DISKOS- Norwegian National Data Repository || Norwegian Petroleum Directorate (NPD) and DISKOS Group of oil companies || Norwegian continental shelf || Started in 1995 || To ensure compliance with NPD reporting regulations for digital E&P data. To reduce data redundancy. To ensure that data is made generally available to the oil and gas industry and to society as a whole Long term preservation of data.
 || Wells, Well Log Curves, Seismic Surveys, field, pre-stack &  post-stack seismic, seismic reports, production data (monthly allocated).Size of NDR estimated at more than 3 Petabytes.
 || SEG-D for seismic field data, SEG-Y for pre-stack and post-stack seismic data (currently only limited amounts of field and pre-stack data) All relevant well data standards such as LIS, DLIS, LAS, SPLA, SCAL etc. PDF and TIF are also used. || Costs are shared equally between all participating oil companies (around 50) in the Diskos consortium, including the NPD. In addition reporting companies pay to submit and download data. All Norwegian universities have free access to public data in Diskos. Non-oil companies can apply for Associated Membership, there are currently around 25 such members. ||http://www.diskos.no/ http://www.npd.no
|-
| {{flagcountry|United Kingdom}} || CDA || CDA Common Data Access Ltd  || UK Offshore Waters || Wells went live in 1995. Infrastructure started operations in 2000. Seismic went live in 2009. Estimated NDR size: 6 Terabytes||Save costs for licenses,Improve access to data,Comply with regulations || Well log curves, Well reports, Post-stack seismic, Seismic reports, VSP, deviation and test data. Estimated NDR size: 6 Terabytes || CDA has adopted DECC’s naming standards for wells and surveys and continues to work closely with DECC and industry to identify a range of standards (see the CDA and DECC websites for more on this) || Owned by the UK oil and gas industry ||
http://www.ukoilandgasdata.com
http://www.gov.uk/oil-and-gas-petroleum-operations-notices
http://www.cdal.com
|-
| {{flagcountry|United Kingdom}} || UKOGL || UK Onshore Geophysical Library || UK onshore || In operation since 1994. Managed and operated by Lynx Information Systems Ltd on behalf of UKOGL. || Custodian of all UK onshore seismic data || Seismic, well tops, logs, cultural. Current archive size approx 6TB || SEGY, UKOOA, LAS, DLIS || Self-funded through data sales ||
http://www.ukogl.org.uk
http://maps.lynxinfo.co.uk/UKOGL_LIVE/map.html
|-
| {{flagcountry|Brazil}} || ANP || Agência Nacional do Petróleo (ANP)||  || BDEP formed in May 2000
 || ||Stores seismic, well log, post stack and pre-stack seismic data and potential field data(Grav/Mag)  || ANP standards in place || Funded by Members || http://www.bdep.gov.br
|-
| {{flagcountry|Mexico}} || Ditep || Pemex ||  ||Established in 2002  || || Promotes and preserve all the technical E&P information assets of the country ||  || ||http://www.pep.pemex.com/index.html
|-
|  {{flagcountry|Israel}} ||  || The Ministry of National Infrastructures  || || Exploratory ||  ||  ||  ||  || http://www.mni.gov.il/mni/en-US/NaturalResources/OilandgasExploration/OilMaps/
|-
| {{flagcountry|Cyprus}} || MCIT || Ministry of Commerce, Industry and Tourism-Energy Service || Offshore  || Promotional || Responsible for granting licences for prospecting, exploration and exploitation of hydrocarbons ||  ||  ||  || http://www.mcit.gov.cy/mcit/mcit.nsf/dmlhexploration_en/dmlhexploration_en?OpenDocument
|-
| {{flagcountry|South Africa}} ||   || Petroleum Agency of South Africa ||  || Active || Seismic data, Well data, Samples, reports and diagrams ||  ||Standards: Formats – SEGD, SEGY, LIS, LAS, PDF and TIFF, Media – 3480, 3590, DLT, 8mm Exabyte, DAT   || From 2010 funded by Government || http://www.petroleumagencysa.com
|-
| {{flagcountry|Kenya}} || National Data Center (NDC) || National Oil Corporation of Kenya || Offshore and Onshore || Began in 2007, system implemented in 2010.  || Digital data preservation, National archive, to implement integrated data management systems, provide easy access to quality-controlled data for internal and external customers, attract oil and gas exploration investment and to reduce data management costs. || Wells, well log curves, well reports, post-stack seismic, field data/tapes, seismic acquisition/processing reports, interpretive maps and reports. || Seismic data- SEGY. 3590 or 3592 data cartridges.  || 100% Government Funded || http://www.nockenya.co.ke/
|-
| {{flagcountry|United States}} || BOEMRE || [[Bureau of Ocean Energy Management, Regulation and Enforcement|Bureau of Ocean Energy, Management, Regulation and Enforcement (BOEMRE)]] || Gulf of Mexico || Has replaced the former Minerals Management Service (MMS) ||  ||  ||  ||  || http://www.gomr.boemre.gov/homepg/data_center.html
|-
| {{flagcountry|United States}} || NGRDS || National Geoscience Data Repository System (NGDRS) ||  ||  || NGRDS is a system of geoscience data repositories, providing information about their respective holdings accessible through a web-based supercatalog. ||  || geologic, geophysical, and engineering data, maps, well logs, and samples || DOE has provided funds for the NGDRS since 1993  ||
http://www.agiweb.org/ngdrs/index.html
http://www.energy.gov/
http://www.agiweb.org/index.html

List of Repositories in US listed also as directory
http://www.agiweb.org/ngdrs/overview/datadirectory.html
|-
| {{flagcountry|Cambodia}} ||CNPA ||Cambodia National Petroleum Authority  || Onshore & Offshore  ||  ||Promotion and preservation of technical E&P information assets of the country  || ||  ||Norad/OfD and NPD assistance || http://www.cnpa-cambodia.com/
|-
| {{flagcountry|Afghanistan}} ||MOM ||Ministry of Mines of the Islamic Republic of Afghanistan (MoM)
|| Onshore ||  ||Promotion and preservation of technical E&P information assets of the country  || ||  ||Norad/OfD and NPD assistance ||http://mom.gov.af/en/news/1637
|-
| {{flagcountry|Bangladesh}} || MOEMR  ||Hydrocarbon Unit, Ministry of Power, Energy and Mineral Resources (MOEMR)|| Onshore & Offshore || Active and ongoing via HCU unit since 2005
|| A mini-data bank has established in the HCU to handle Production data, Resource data by using Database & GIS Software 2005 and promotion of technical E&P information assets of the country  || ||Funded assistance    ||Norad/OfD and NPD assistance ||
http://www.hcu.org.bd/
http://www.petrobangla.org.bd
http://www.bapex.com.bd
|-
| {{flagcountry|Ethiopia}} || MOME  ||Ministry of Mines and Energy Ethiopia  ||  || Active and ongoing ||Promotion of technical E&P information assets of the country   || ||  || ||
http://www.mome.gov.et/petroleum.html
|-
| {{flagcountry|Cameroon}} ||SNH ||SNH Cameroon   ||  || Active & Ongoing || Preservation and promotion of technical E&P information assets of the country  || ||  || || http://www.snh.cm
|-
| {{flagcountry|Malaysia}} ||PIRI  ||Petronas  ||  ||Yet to establish full NDR  || Promotion and preservation of technical E&P information assets of the country  || ||  || || http://www.petronas.com.my
|-
| {{flagcountry|Spain}} || ATH  ||  ||  ||  ||Online GIS databases to geophyscial information SIGEOF and ATH (Archivo de Hydrocarbures) || ||  ||  ||
http://www.mityc.es/energia/petroleo/Exploracion/Paginas/Estadisticas.aspx
http://hidrocarburos.mityc.es/ath/
http://www.igme.es/internet/sigeof/INICIOsiGEOF.htm
|-
| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.onhym.com
|-
| {{flagcountry|Madagascar}} ||OMNIS  ||  ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||Norad OfD and NPD assistance    ||
|-
| {{flagcountry|Sudan}} || PIC (Petroleum Information Center) || Ministry of Oil and Gas ||  || Active since 2000  || Preserve and promote E&P data,managing Oil Museum || Wells, well log, well reports, cores and samples,seismic (acquisition/processing) reports, production data,GIS ||  ||  ||
http://www.spc.gov.sd
|-
| {{flagcountry|Morocco}} ||ONHYM  || Office National des Hydrocarbures et des Mines ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.onhym.com
|-
| {{flagcountry|Nicaragua}} ||MEM || ||  ||  Active & ongoing ||  || ||  || Norad OfD and NPD assistance   ||
http://www.ine.gob.ni
http://www.mem.gob.ni
|-
| {{flagcountry|Iraq}} ||MoO ||  Ministry of Oil Republic of Iraq||  || Active and Ongoing since 2005  || MoO establishing a centralized data base and NDR for Iraqi petroleum data and to ensure that data & information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Norad/OfD and NPD assistance || http://www.oil.gov.iq
|-
| {{flagcountry|Latvia}} ||LEGMC  ||Latvian Environment, Geology and Meteorology Centre    || Offshore & Onshore || || An E & P data archive centre which provides data available for purchase  || Geological (well and seismic data, maps, reports etc.)  ||  ||  ||  http://mapx.map.vgd.gov.lv/geo3/VGD_OIL_PAGE/index.htm
|-
|  {{flagcountry|Albania}} || AKBN  || National Agency of Natural Resources ||  ||  || Generates and promotes exploration opportunities in Albania, maintains archive of E & P data. ||  ||  ||  || http://www.akbn.gov.al/index.php?ak=details&cid=5&lng=en
|-
| {{flagcountry|Uganda}} ||PEPD|| Petroleum Exploration & Production Dept (PEPD) || Onshore || || || Technical E & P data archive and information ||  ||Norad/Ofd assistance || http://www.statehouse.go.ug/government.php?catId=10 http://www.energyandminerals.go.ug
|-
| {{flagcountry|Zambia}} |||| Ministry of Mines and Minerals Development, Geological Survey Department (GSD)
 || Onshore ||Active and ongoing || || Technical E & P data archive and information - Technical Records Unit||  ||Norad/Ofd & NPD assistance ||
http://www.zambiageosurvey.gov.zm/
|-
| {{flagcountry|Ivory Coast}} ||MME|| Ministry of Mines & Energy || Onshore & Offshore || || || ||  ||Norad/Ofd and NPD assistance ||
http://www.cotedivoirepr.ci/
http://www.petroci.ci/index.php?numlien=31
|-
| {{flagcountry|Romania}} || || National Agency for Mineral Resources  ||  || ||Promotion and preservation of technical E&P information assets of the country  || ||  ||  || http://www.namr.ro/main_en.htm
|-
| {{flagcountry|Fiji}} ||SOPAC ||Mineral Resources Dept Fiji     |||| Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || In part externally managed ||
http://www.mrd.gov.fj/gfiji/
http://www.mrd.gov.fj/gfiji/petroleum/petroleum.html
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/fiji-islands
|-
| {{flagcountry|Papua New Guinea}} ||SOPAC||Department of Petroleum and Energy ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.petroleum.gov.pg
http://www.petrominpng.com.pg/about.html
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/papua-new-guinea
|-
| {{flagcountry|Solomon Islands}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia   || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations  || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/solomon-islands
|-
| {{flagcountry|Tonga}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia    || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/tonga
|-
| {{flagcountry|Vanuatu}} ||SOPAC ||  ||  || Created as SOPAC Petroleum Data Bank a cooperative effort with Geoscience Australia  || SOPAC acts as custodian and primary point for E & P data & information preserved on behalf of Pacific Island member nations || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || Externally managed ||
http://www.ga.gov.au/energy/projects/pacific-islands-applied-geoscience-commission.html
http://www.sopac.org/index.php/member-countries/vanuatu
|-
| {{flagcountry|Guyana}} ||GGMC  ||Guyana Geology and Mines Commission  ||  ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.ggmc.gov.gy
|-
| {{flagcountry|Syria}} ||SPC  || Syrian Petroleum Company  ||  ||  || || ||  ||  || http://www.spc-sy.com/en/aboutus/aboutus1_en.php
|-
| {{flagcountry|Liberia}} ||NOCAL || National Oil Company of Liberia ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.nocal-lr.com/
|-
| {{flagcountry|Chile}} ||ENAP || National Oil Company of Chile ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.enap.cl
|-
| {{flagcountry|Thailand}} ||PTTEP ||PTT Exploration and Production Public Company Ltd  ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.pttep.com/
http://www.pttep.com/en/index.aspx
|-
| {{flagcountry|Venezuela}} ||PDVSA ||Petroleos de Venezuela  || Onshore & Offshore ||  || Promotion and preservation of technical E&P information assets of the country || ||  ||  || http://www.pdvsa.com/
|-
| {{flagcountry|Trinidad}} || || Trinidad Ministry of Energy and Energy Affairs ||  ||  ||Promotion and preservation of technical E&P information assets of the country   || ||  ||  || http://www.energy.gov.tt/energy_industry.php?mid=31
http://www.petrotrin.com/Petrotrin2007/UpstreamBusiness.htm
|-
| {{flagcountry|Mozambique}} || NAPD ||  ||  || Established in 1999 under NORAD support  || To ensure that data & information from petroleum activities is made available and attract more investors by promoting the petroleum activities || Well logs, Maps, Magnetic tapes, Core & cutting samples, Other geological and geophysical information ||  || National Budget and INP funds || http://www.inp.gov.mz
|-
| {{flagcountry|Denmark}} || || Danish Energy Agency  ||  ||   || Online GIS service for wells and license data  || ||  ||  ||
http://www.ens.dk/EN-US/OILANDGAS/Sider/Oilandgas.aspx
|-
| {{flagcountry|Dominican Republic}} || || Directorate of Hydrocarbons  ||  ||   ||  ||  ||  ||  || http://www.dgm.gov.do/sdhidrocarburo/index.html
|-
| {{flagcountry|Equatorial Guinea}} || ||   ||  ||   ||   Exploration databank for Equatorial Guinea||  ||  || ||
http://www.equatorialoil.com
http://www.equatorialoil.com/database.html
|-
| {{flagcountry|Faroe Islands}} || Jardfeingi || Jardfeingi Faorese Earth and Energy Directorate  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || || http://www.jardfeingi.fo
|-
| {{flagcountry|Philippines}} || PNOC || Philippine National Oil Company  ||  ||   ||  Promotion of exploration and licensing rounds ||  ||  || ||
http://www.pnoc.com.ph
http://www.pnoc-ec.com.ph/business.php?id=2
|-
| {{flagcountry|Greenland}} || GreenPetroData || MMR- Ministry of Mineral Resources  ||  ||  ||  Web and GIS system providing access to all Released Well and Geophysical data. ||  ||  || ||
https://www.greenpetrodata.gl/
http://govmin.gl/
|-
| {{flagcountry|Iceland}} || Iceland Continental Shelf Portal (ICSP)  || Orkustofnunn - National Energy Authority   ||Offshore ||   The Iceland Continental Shelf Portal (ICSP)|| Provides access to information about data pertaining to the Icelandic Continental Shelf, in particular initially to the northern Dreki Area to assist with licensing round promotion || ||  ||  ||
http://www.os.is
http://www.nea.is/oil-and-gas-exploration/
|-
| {{flagcountry|Myanmar}} || MOGE || Myanmar Oil & Gas Enterprise   ||  ||   ||    ||  ||  || ||
http://www.energy.gov.mm/upstreampetroleumsubsector.htm
|-
| {{flagcountry|Yemen}} || PEPA || Petroleum Exploration and Production Authority (PEPA)
   ||  ||   ||    ||  ||  || ||
http://www.pepa.com.ye/
|-
| {{flagcountry|Tunisia}} || ETAP || Enterprise Tunisienne D’Activities Petrolieres   ||  ||   ||  Promotion and preservation of technical E&P information assets of the country    ||  ||  || ||
http://www.etap.com.tn
|-
| {{flagcountry|Gabon}} || DGH || Direction Generale des Hydrocarbures (DGH)||  ||   ||  ||  ||  || ||
http://www.gabon-industriel.com/les-actions/energie/petrole
|-
| {{flagcountry|Republic of the Congo  }} || SNPC || Société Nationale des pétroles du Congo ||  ||   ||    ||  ||  || ||
|-
| {{flagcountry|Mali}} || Aurep || {{Not a typo|Autorite}} pour la Promotion de la Recherce des Petroliere au Mali
||  ||   ||   Databank service managing the geological and geophysical data relative to petroleum research. ||  ||  || ||
http://www.aurep.org
http://www.aurep.org/htmlpages/mali.html
|-
| {{flagcountry|Guatemala}} || MEM || Dirección General de Hidrocarbures||  ||   ||  Online maps and images of wells, seismic, licenses, protected areas, exploration and production ||  ||  || ||
http://www.mem.gob.gt/Portal/home.aspx
http://www.mem.gob.gt/Portal/Home.aspx?secid=25
|-
| {{flagcountry|Iran}} || NIOC || National Oil Company of Iran ||  ||   ||   ||  ||  || ||
http://www.nioc.ir
|-
| {{flagcountry|Libya}} || NOC || NOC Libya ||  ||   ||  Virtual data room in place for promotion of exploration and exploitation of hydrocarbons ||  ||  || ||
|-
| {{flagcountry|United Arab Emirates}} || ADNOC || Abu Dhabi National Oil Company ||  ||   ||  ||  ||  ||  ||
http://www.adnoc.ae
|-
| {{flagcountry|Qatar}} ||  || Qatar Petroleum ||  ||   ||  ||  ||  ||  ||
http://www.qp.com.qa
|-
| {{flagcountry|South Korea}} || KNOC || Korea National Petroleum Corporation ||  ||   ||  ||  ||  ||  ||
http://www.knoc.co.kr
|-
| {{flagcountry|Seychelles}} || SNOC || Seychelles National Oil Company ||  ||   ||  ||  ||  ||  ||
|-
| {{flagcountry|Saudi Arabia}} ||  || Saudi Aramco ||  ||   ||  ||  ||  ||  ||
http://www.saudiaramco.com
|-
| {{flagcountry|Belarus}} || ||  ||  ||   ||  ||  ||  ||  ||
http://geologiya.org/index.php?categoryid=14
http://minpriroda.by/ru/napravlenia/minsyrbaza
|-
| {{flagcountry|East Timor}} || LAFAEK || Autoridade Nacional do Petróleo ||  || || Online GIS with wells and licences ||  ||  || Norad/OfD assistance  || http://www.anp-tl.org/webs/anptlweb.nsf/pgMaps
|}

==See also==
*[[Norwegian Petroleum Directorate]]
*[[Energistics]]
*[[Professional Petroleum Data Management Association (PPDM)]]
*[[Oil and gas industry in the United Kingdom]]
*[[Petroleum exploration in Guyana]]

==Notes==
{{Reflist}}

==External links==
*[http://www.energistics.org/regulatory/national-data-repository-ndr-work-group Energistics: National Data Repository Work Group]
*[http://www.kadme.com/wp-content/uploads/KADME-Oil-and-Gas-Technology-Jan2011.pdf National Data Repositories: the case for open data in the oil and gas industry]
*[http://www.seg.org/ts Society of Exploration Geophysicists]

[[Category:Data management]]
[[Category:Open standards]]
[[Category:Hydrocarbons]]
[[Category:Geophysics organizations]]
<=====doc_Id=====>:332
<=====title=====>:
Document-oriented database
<=====text=====>:
{{about|the software type|deployed applications of the software type|Full text database}}
{{primary sources|date=May 2012}}

A '''document-oriented database''', or '''document store''', is a [[computer program]] designed for storing, retrieving and managing document-oriented information, also known as [[Semi-structured model|semi-structured data]]. Document-oriented databases are one of the main categories of [[NoSQL]] databases, and the popularity of the term "document-oriented database" has grown<ref>[http://db-engines.com/en/ranking_categories DB-Engines Ranking per database model category]</ref> with the use of the term NoSQL itself. [[XML database]]s are a subclass of document-oriented databases that are optimized to work with [[XML]] documents. [[Graph databases]] are similar, but add another layer, the ''relationship'', which allows them to link documents for rapid traversal.

Document-oriented databases are inherently a subclass of the [[Key-value database|key-value store]], another NoSQL database concept. The difference lies in the way the data is processed; in a key-value store the data is considered to be inherently opaque to the database, whereas a document-oriented system relies on internal structure in the ''document'' in order to extract [[metadata]] that the database engine uses for further optimization. Although the difference is often moot due to tools in the systems,{{efn|To the point that document-oriented and key-value systems can often be interchanged in operation.}} conceptually the document-store is designed to offer a richer experience with modern programming techniques.

Document databases{{efn|And key-value stores in general.}} contrast strongly with the traditional [[relational database]] (RDB). Relational databases generally store data in separate ''tables'' that are defined by the programmer, and a single object may be spread across several tables. Document databases store all information for a given object in a single instance in the database, and every stored object can be different from every other. This makes mapping objects into the database a simple task, normally eliminating anything similar to an [[object-relational mapping]]. This makes document stores attractive for programming [[web application]]s, which are subject to continual change in place, and where speed of deployment is an important issue.

== Documents ==
The central concept of a document-oriented database is the notion of a ''document''. While each document-oriented database implementation differs on the details of this definition, in general, they all assume documents encapsulate and encode data (or information) in some standard formats or encodings. Encodings in use include [[XML]], [[YAML]], [[JSON]], and [[BSON]], as well as binary forms like PDF and Microsoft Office documents (MS Word, Excel, and so on).

Documents in a document store are roughly equivalent to the programming concept of an object. They are not required to adhere to a standard schema, nor will they have all the same sections, slots, parts, or keys. Generally, programs using objects have many different types of objects, and those objects often have many optional fields. Every object, even those of the same class, can look very different. Document stores are similar in that they allow different types of documents in a single store, allow the fields within them to be optional, and often allow them to be encoded using different encoding systems. For example, the following is a document, encoded in JSON:

<syntaxhighlight lang="javascript">
{
    FirstName: "Bob", 
    Address: "5 Oak St.", 
    Hobby: "sailing"
}
</syntaxhighlight>

A second document might be encoded in XML as:
<syntaxhighlight lang="xml">
  <contact>
    <firstname>Bob</firstname>
    <lastname>Smith</lastname>
    <phone type="Cell">(123) 555-0178</phone>
    <phone type="Work">(890) 555-0133</phone>
    <address>
      <type>Home</type>
      <street1>123 Back St.</street1>
      <city>Boys</city>
      <state>AR</state>
      <zip>32225</zip>
      <country>US</country>
    </address>
  </contact>
</syntaxhighlight>

These two documents share some structural elements with one another, but each also has unique elements. The structure and text and other data inside the document are usually referred to as the document's ''content'' and may be referenced via retrieval or editing methods, (see below). Unlike a relational database where every record contains the same fields, leaving unused fields empty; there are no empty 'fields' in either document (record) in the above example. This approach allows new information to be added to some records without requiring that every other record in the database share the same structure.

Document databases typically provide for additional [[metadata]] to be associated with and stored along with the document content. That metadata may be related to facilities the datastore provides for organizing documents, providing security, or other implementation specific features.

=== CRUD operations ===
The core operations a document-oriented database supports on documents are similar to other databases and while the terminology isn't perfectly standardized, most practitioners will recognize them as [[CRUD]]

* Creation (or insertion)
* Retrieval (or query, search, finds)
* Update (or edit)
* Deletion (or removal)

=== Keys ===
Documents are addressed in the database via a unique ''key'' that represents that document. This key is a simple [[identifier]] (or ID), typically a [[String (computer science)|string]], a [[URI]], or a [[Path (computing)|path]]. The key can be used to retrieve the document from the database. Typically the database retains an [[Database index|index]] on the key to speed up document retrieval, and in some cases the key is required to create or insert the document into the database.

=== Retrieval ===
Another defining characteristic of a document-oriented database is that, beyond the simple key-to-document lookup that can be used to retrieve a document, the database offers an API or query language that allows the user to retrieve documents based on content (or metadata). For example, you may want a query that retrieves all the documents with a certain field set to a certain value.  The set of query APIs or query language features available, as well as the expected performance of the queries, varies significantly from one implementation to another. Likewise, the specific set of indexing options and configuration that are available vary greatly by implementation.

It is here that the document store varies most from the key-value store. In theory, the values in a key-value store are opaque to the store, they are essentially black boxes. They may offer search systems similar to those of a document store, but may have less understanding about the organization of the content. Document stores use the metadata in the document to classify the content, allowing them, for instance, to understand that one series of digits is a phone number, and another is a postal code. This allows them to search on those types of data, for instance, all phone numbers containing 555, which would ignore the zip code 55555.

=== Editing ===
Document databases typically provide some mechanism for updating or editing the content (or other metadata) of a document, either by allowing for replacement of the entire document, or individual structural pieces of the document.

=== Organization ===
Document database implementations offer a variety of ways of organizing documents, including notions of

* Collections: groups of documents, where depending on implementation, a document may be enforced to live inside one collection, or may be allowed to live in multiple collections
* Tags and non-visible metadata: additional data outside the document content 
* Directory hierarchies: groups of documents organized in a tree-like structure, typically based on path or URI

Sometimes these organizational notions vary in how much they are logical vs physical, (e.g. on disk or in memory), representations.

==Relationship to other databases ==

=== Relationship to key-value stores ===

A document-oriented database is a specialized  [[Key-value database|key-value store]], which itself is another NoSQL database category.  In a simple key-value store, the document content is opaque. A document-oriented database provides APIs or a query/update language that exposes the ability to query or update based on the internal structure in the ''document''. This difference may be moot for users that do not need richer query, retrieval, or editing APIs that are typically provided by document databases. Modern key-value stores often include features for working with metadata, blurring the lines between document stores.

=== Relationship to search engines ===
Some search engines (aka [[information retrieval]]) systems like [[Elasticsearch]] provide enough of the core operations on documents to fit the definition of a document-oriented database.

=== Relationship to relational databases ===

{{cleanup|section|reason="Requires cleanup"|date=July 2016}}

In a relational database, data is first categorized into a number of predefined types, and ''tables'' are created to hold individual entries, or ''records'', of each type. The tables define the data within each record's ''fields'', meaning that every record in the table has the same overall form. The administrator also defines the ''relationships'' between the tables, and selects certain fields that they believe will be most commonly used for searching and defines ''indexes'' on them. A key concept in the relational design is that any data that may be repeated is normally placed in its own table, and if these instances are related to each other, a column is selected to group them together, the ''foreign key''. This design is known as ''[[database normalization]]''.<ref>{{cite web |url=https://support.microsoft.com/en-ca/kb/283878 |title=Description of the database normalization basics |website=Microsoft}}</ref>

For example, an address book application will generally need to store the contact name, an optional image, one or more phone numbers, one or more mailing addresses, and one or more email addresses. In a canonical relational database solution, tables would be created for each of these rows with predefined fields for each bit of data: the CONTACT table might include FIRST_NAME, LAST_NAME and IMAGE columns, while the PHONE_NUMBER table might include COUNTRY_CODE, AREA_CODE, PHONE_NUMBER and TYPE (home, work, etc.). The PHONE_NUMBER table also contains a foreign key column, "CONTACT_ID", which holds the unique ID number assigned to the contact when it was created. In order to recreate the original contact, the database engine uses the foreign keys to look for the related items across the group of tables and reconstruct the original data.

In contrast, in a document-oriented database there may be no internal structure that maps directly onto the concept of a table, and the fields and relationships generally don't exist as predefined concepts. Instead, all of the data for an object is placed in a single document, and stored in the database as a single entry. In the address book example, the document would contain the contact's name, image, and any contact info, all in a single record. That entry is accessed through its key, which allows the database to retrieve and return the document to the application. No additional work is needed to retrieve the related data; all of this is returned in a single object.

A key difference between the document-oriented and relational models is that the data formats are not predefined in the document case. In most cases, any sort of document can be stored in any database, and those documents can change in type and form at any time. If one wishes to add a COUNTRY_FLAG to a CONTACT, this field can be added to new documents as they are inserted, this will have no effect on the database or the existing documents already stored. To aid retrieval of information from the database, document-oriented systems generally allow the administrator to provide ''hints'' to the database to look for certain types of information. These work in a similar fashion to indexes in the relational case. Most also offer the ability to add additional metadata outside of the content of the document itself, for instance, tagging entries as being part of an address book, which allows the programmer to retrieve related types of information, like "all the address book entries". This provides functionality similar to a table, but separates the concept (categories of data) from its physical implementation (tables).

In the classic normalized relational model, objects in the database are represented as separate rows of data with no inherent structure beyond that given to them as they are retrieved. This leads to problems when trying to translate programming objects to and from their associated database rows, a problem known as [[object-relational impedance mismatch]].<ref>{{cite web |url=http://www.agiledata.org/essays/impedanceMismatch.html |title=The Object-Relational Impedance Mismatch |first=Scott |last=Wambler |website=Agile Data}}</ref> Document stores more closely, or in some cases directly, map programming objects into the store. This eliminates the impedance mismatch problem, and is offered as one of the main advantages of the NoSQL approach.

== Implementations ==
{{main cat|Document-oriented databases}}

{| class="wikitable sortable"
|-
! Name
! Publisher
! License
! Languages supported
! Notes
! [[Representational State Transfer|RESTful]] API
|-
| [[BaseX]]
| BaseX Team
| {{free|[[BSD License]]}}
| [[Java (programming language)|Java]], [[XQuery]]
| Support for XML, JSON and binary formats; client-/server based architecture; concurrent structural and full-text searches and updates.
| {{yes}}
|-
| [[InterSystems Caché|Caché]]
| [[InterSystems]] Corporation
| {{proprietary}}
| [[Java (programming language)|Java]], [[C Sharp (programming language)|C#]], [[Node.js]]
| Commonly used in Health, Business and Government applications.
| {{yes}}
|-
| [[Cloudant]]
| Cloudant, Inc.
| {{proprietary}}
| [[Erlang (programming language)|Erlang]], [[Java (programming language)|Java]], [[Scala (programming language)|Scala]], and [[C (programming language)|C]]
| Distributed database service based on [[BigCouch]], the company's [[open source]] fork of the [[Apache Software Foundation|Apache]]-backed [[CouchDB]] project.  Uses JSON model.
| {{yes}}
|-
| [[Clusterpoint|Clusterpoint Database]]
| Clusterpoint Ltd.
| {{proprietary}} with free download
| [[JavaScript]], [[SQL]], [[PHP]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[C (programming language)|C]], [[C++]], 
| Distributed document-oriented XML / JSON database platform with [[ACID]]-compliant [[transaction processing|transactions]]; [[high-availability]] [[data replication]] and [[sharding]]; built-in [[full text search]] engine with [[relevance]] [[ranking]]; JS/SQL [[query language]]; [[Geographic information system|GIS]]; Available as pay-per-use [[cloud database|database as a service]] or as an on-premise free software download.<ref>[http://www.clusterpoint.com Document-oriented Database]. Clusterpoint. Retrieved on 2015-10-08.</ref>
| {{yes}}
|-
| [[Couchbase Server]]
| [[Couchbase, Inc.]]
| {{free|[[Apache License]]}}
| [[C]], [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[PHP]], [[SQL]], [[GoLang]], [[Spring Framework]], [[LINQ]]
|Distributed NoSQL Document Database, JSON model and SQL based Query Language.
| {{yes}}<ref>[http://www.couchbase.com/docs/ Documentation]. Couchbase. Retrieved on 2013-09-18.</ref>
|-
| [[CouchDB]]
| [[Apache Software Foundation]]
| {{free|[[Apache License]]}}
| Any language that can make HTTP requests
| JSON over REST/HTTP with [[Multi-Version Concurrency Control]] and limited [[ACID]] properties. Uses [[map (higher-order function)|map]] and [[fold (higher-order function)|reduce]] for views and queries.<ref>[http://couchdb.apache.org/docs/overview.html CouchDB Overview] {{webarchive |url=https://web.archive.org/web/20111020074113/http://couchdb.apache.org/docs/overview.html |date=October 20, 2011 }}</ref>
| {{yes}}<ref>[http://wiki.apache.org/couchdb/HTTP_Document_API CouchDB Document API]</ref>
|-
| [[CrateIO]]
| CRATE Technology GmbH
| {{free|[[Apache License]]}}
| [[Java (programming language)|Java]]
| Use familiar SQL syntax for real time distributed queries across a cluster. Based on Lucene / Elasticsearch ecosystem with built-in support for binary objects (BLOBs).
| {{yes}}<ref>{{cite web|url=https://crate.io/docs/stable/sql/rest.html |title=Archived copy |accessdate=2015-06-22 |deadurl=yes |archiveurl=https://web.archive.org/web/20150622174526/https://crate.io/docs/stable/sql/rest.html |archivedate=2015-06-22 |df= }}</ref>
|-
|djondb
|djondb.com
|GNU GPL and Commercial
|C, .Net, Java, Python, NodeJS, PHP.
|Document Store with support to transactions.
|{{No}}
|-
| [[DocumentDB]]
| Microsoft
| {{proprietary}}
| [[.NET]], [[Java (programming language)|Java]], [[Python (programming language)|Python]], [[Node.js]], [[JavaScript]], [[SQL]]
| Platform-as-a-Service offering, part of the [[Microsoft Azure]] platform.
| {{yes}}
|-
|[[Elasticsearch]]
|[[Shay Banon]]
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|[[JSON]], Search engine.
|{{yes}}
|-
| [[eXist]]
| eXist
| {{free|[[LGPL]]}}
| [[XQuery]], [[Java (programming language)|Java]]
| XML over REST/HTTP, WebDAV, Lucene Fulltext search, binary data support, validation, versioning, clustering, triggers, URL rewriting, collections, ACLS, XQuery Update
| {{yes}}<ref>[http://exist-db.org eXist-db Open Source Native XML Database]. Exist-db.org. Retrieved on 2013-09-18.</ref>
|-
| [[HyperDex]]
| hyperdex.org
| {{free|[[BSD License]]}}
| [[C (programming language)|C]], [[C++]], [[Go (programming language)|Go]], [[Node.js]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]] 
| Support for [[JSON]] and binary documents.
| {{no}}
|-
| [[Informix]]
| IBM
| Proprietary, with no-cost editions<ref>http://www.ibm.com/developerworks/data/library/techarticle/dm-0801doe/</ref>
| Various (Compatible with MongoDB API)
| RDBMS with JSON, replication, sharding and ACID compliance.
| {{yes}}
|-
|[[Apache Jackrabbit|Jackrabbit]]
|Apache Foundation
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|[[Java Content Repository]] implementation
|{{dunno}}
|-
| [[Lotus Notes]] ([[IBM Lotus Domino]])
| IBM
| {{proprietary}}
| [[LotusScript]], [[Java (programming language)|Java]], Lotus @Formula
| [[MultiValue]]
| {{yes}}
|-
| [[MarkLogic]]
| MarkLogic Corporation
| Free Developer license or Commercial<ref>http://developer.marklogic.com/licensing</ref>
| [[REST]], [[Java (programming language)|Java]], [[JavaScript]], [[Node.js]], [[XQuery]], [[SPARQL]], [[XSLT]], [[C++]]
| Distributed document-oriented database for JSON, XML, and [[Resource Description Framework|RDF triples]]. Built-in [[Full text search]], [[ACID]] transactions, [[High availability]] and [[Disaster recovery]], certified security.
| {{yes}}
|-
| [[MongoDB]]
| MongoDB, Inc
| {{free|[[Affero General Public License|GNU AGPL v3.0]] for the DBMS, [[Apache 2 License]] for the client drivers}}<ref>[http://www.mongodb.org/about/licensing/ MongoDB Licensing]</ref>
| [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[Perl]], [[PHP]], [[Python (programming language)|Python]], [[Node.js]], [[Ruby (programming language)|Ruby]], [[Scala (programming language)|Scala]] <ref>[http://docs.mongodb.org/ecosystem/drivers/community-supported-drivers/ Additional 30+ community MongoDB supported drivers]</ref>
| Document database with replication and sharding, [[BSON]] store (binary format [[JSON]]).
| {{yes}}<ref>[http://www.mongodb.org/display/DOCS/Http+Interface#HttpInterface-RESTInterfaces MongoDB REST Interfaces]</ref>
|-
| [[MUMPS]] Database
| {{dunno}}
| [[proprietary software|Proprietary]] and [[Affero General Public License|Affero GPL]]<ref>[http://sourceforge.net/projects/fis-gtm/ GTM MUMPS FOSS on SourceForge]</ref>
| [[MUMPS]]
| Commonly used in health applications.
| {{dunno}}
|-
| [[ObjectDatabase++]]
| Ekky Software
| {{proprietary}}
| [[C++]], [[C Sharp (programming language)|C#]], [[TScript]]
| Binary Native C++ class structures
| {{dunno}}
|-
| [[OrientDB]]
| Orient Technologies
| {{free|[[Apache License]]}}
| [[Java (programming language)|Java]]
| JSON over HTTP, SQL support, [[ACID]] transactions
| {{yes}}
|-
| [[PostgreSQL]]
| PostgreSQL
| {{free | [http://www.postgresql.org/about/licence/ PostgreSQL Free License]}}
| [[C (programming language)|C]]
| HStore, JSON store (9.2+), JSON function (9.3+), HStore2 (9.4+), JSONB (9.4+)
|{{no}}
|-
| [[Qizx]]
| [[Qualcomm]]
| [[Commercial software|Commercial]]
| [[REST]], [[Java (programming language)|Java]], [[XQuery]], [[XSLT]], [[C (programming language)|C]], [[C++]], [[Python (programming language)|Python]]
| Distributed document-oriented [[XML database]] with integrated [[full text search]]; support for [[JSON]], text, and binaries.
|{{yes}}
|-
| [[RethinkDB]]
| {{dunno}}
| {{free|[[Affero General Public License|GNU AGPL]] for the DBMS, [[Apache 2 License]] for the client drivers}}
| [[C++]], [[Python (programming language)|Python]], [[JavaScript]], [[Ruby (programming language)|Ruby]], [[Java (programming language)|Java]]
| Distributed document-oriented [[JSON]] database with replication and sharding.
|{{no}}
|-
| [[Rocket U2]]
| Rocket Software
| {{proprietary}}
| {{dunno}}
| UniData, UniVerse
|{{yes}} (Beta)
|-
| [[Sedna (database)|Sedna]]
|sedna.org
|{{free|[[Apache License]]}}
|[[C++]], [[XQuery]]
|[[XML database]]
|{{no}}
|-
| [[SimpleDB]]
| Amazon
| Proprietary online service
|[[Erlang (programming language)|Erlang]]
|
|{{dunno}}
|-
| [[Solr]]
| Apache
|{{free|[[Apache License]]}}
|[[Java (programming language)|Java]]
|Search engine
|{{yes}}
|-
| [[TokuMX]]
|Tokutek
|{{free|[[GNU Affero General Public License]]}}
|[[C++]], [[C Sharp (programming language)|C#]], [[Go (Programming language)|Go]]
|[[MongoDB]] with [[Fractal tree index|Fractal Tree indexing]]
|{{dunno}}
|-
| [[Virtuoso Universal Server|OpenLink Virtuoso]]
| [[OpenLink Software]]
|GPLv2[1] and proprietary
|[[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[SPARQL]]
|[[Middleware]] and [[database engine]] hybrid
|{{yes}}
|}

===XML database implementations===
{{Further information|XML database}}
Most XML databases are document-oriented databases.

== See also ==
* [[Database theory]]
* [[Data hierarchy]]
* [[Full text search]]
* [[In-memory database]]
* [[Internet Message Access Protocol]] (IMAP)
* [[Machine-Readable Documents]]
* [[NoSQL]]
* [[Object database]]
* [[Online database]]
* [[Real time database]]
* [[Relational database]]

==Notes==
{{notelist}}

==References==
{{Reflist}}

==Further reading==
* Assaf Arkin. (2007, September 20). [https://web.archive.org/web/20080327222152/http://blog.labnotes.org:80/2007/09/20/read-consistency-dumb-databases-smart-services/ Read Consistency: Dumb Databases, Smart Services.]
{{refend}}

==External links==
* [http://db-engines.com/en/ranking/document+store DB-Engines Ranking of Document Stores] by popularity, updated monthly

{{Database models}}
{{Databases}}

[[Category:Document-oriented databases| ]]
[[Category:Data management]]
[[Category:Database management systems]]
[[Category:Types of databases]]
<=====doc_Id=====>:335
<=====title=====>:
Altitude3.Net
<=====text=====>:
{{Infobox software
| name                   = Altitude3.Net
| logo                   = Altitude-en.png
| screenshot             = 
| caption                = Altitude3.Net Dashboard
| developer              = Nmédia Solutions
| status                 = Active
| released               = {{start date and age|2000|06|01}}
| operating system       = 
| platform               = [[.NET Framework|.Net]]
| genre                  = [[Content Management System]], [[Content Management Framework]]
| alexa                  =
| website                = {{URL|http://altitude3.net}}
}}

'''Altitude3.Net''' is an electronic business development platform that allows to create web and mobile solutions along with interactive communication strategies. The platform has the same functionalities <ref>{{cite web|language=fr|title = Pinpoint|url = https://pinpoint.microsoft.com/fr-CA/Companies/4296539037/Services|publisher = pinpoint.microsoft.com|accessdate = 2015-06-22}}</ref><ref>{{cite web|title=Altitude3.Net|url=http://www.cmsmatrix.org/matrix/cms-matrix/altitude-3.net|website=CMS Matrix|accessdate=27 July 2015}}</ref> than a content management system (CMS) and communicates with other systems (accounting systems, manufacturing management software (MRP), business management software (enterprise resource planning (ERP)), database, Excel files, XML, CSV or all other kinds of structural data).

Nmédia solutions developed Altitude3.Net in 2001 using Microsoft's .NET Framework technology. The platform is currently using the 4.5 version of Microsoft’s Framework.

== History ==
In 2001, Nmédia solutions created the content management system Altitude<sup>mc</sup>.<ref>{{cite web|title=About|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}</ref> As it went on, many versions were developed:
* Altitude Moto and Altitude Auto (2001 to 2006);
* Altitude 2 (2006);
* Altitude3.Net (2010).

== List of main functionalities ==
The Altitude3.Net platform is structured in many modules:<ref>{{cite web|title=Altitude advantages|url=http://altitude3.net/home|website=Altitude3.Net|accessdate=27 July 2015}}</ref>
* Content management
* contact management and mass-emailing
* Control of advanced SEO parameters
* Microsoft flexibility & computability
* Security & access management
* Security & permission management
* E-commerce solutions: Centralized Product Management (CPM) services. This module includes several functionalities: interface for mass product modification, centralized coupon management, custom management by product group, inventory by store location, shopping cart, price & currency management, catalog management, centralized database, supplier management, product by media, product comparison tool (based on common characteristics), syncing accounting software inventory with Altitude3.Net
* A [[Microsoft Azure]] solution (cloud computing)
* Omnichannel marketing
* Other functionalities : On-site search engines for meta data and documents (text, Word, Excel and PDF); HTML5 video player with descending compatibility; Integrated functions enabling an entire site to be generated in Hypertext Markup Language (HTML) or enabling to export all its data (DATA) and import it in any other CMS

== Awards and recognitions ==
* In 2010: Altitude3.Net is finalist in the IT category (software application) of the Mérites du français during the Francofête.
* In 2011: Nmédia solutions wins the title of Web Development Partner of the Year awarded by the Microsoft Partner Network.<ref>{{cite news|title=Microsoft honore deux entreprises de la région|url=http://www.lapresse.ca/la-tribune/economie-et-innovation/201112/19/01-4479237-microsoft-honore-deux-entreprises-de-la-region.php|accessdate=27 July 2015|publisher=[[La Presse (Canadian newspaper)]]|date=19 December 2011|language=fr}}</ref><ref>{{cite news|title=Drummondville triomphe à Toronto|url=http://www.journalexpress.ca/Actualites/Economie/2011-12-05/article-2825943/Drummondville-triomphe-a-Toronto/1|accessdate=27 July 2015|publisher=Journal l'Express|date=5 December 2011|language=fr}}</ref>
* In 2012: Altitude3.Net wins the Prix Franco awarded by the Drummondville Young Chamber of Commerce during its annual gala.<ref>{{cite web|title=Prix Franco 2012 Nmédia Solutions Inc.|url=http://www.jccd.ca/Concours-Prix-Franco/Prix-Franco-2012/NMedia-Solutions-Inc.aspx|website=Jeune Chambre de Commerce de Drummondville|accessdate=27 July 2015|language=fr}}</ref>
* In 2015: The CPM module of Altitude3 is finalist in the Web Solutions category at the Octas.<ref>{{cite web|title=Les lauréats du concours des Octas 2015|url=http://www.actionti.com/microsites/octas/gagnants/nos-gagnants|website=Réseau Action TI|accessdate=27 July 2015|language=fr}}</ref><ref>{{cite news|title=Nmédia en lice aux Octas|url=http://www.journalexpress.ca/Actualites/2015-05-12/article-4144312/Nmedias-en-lice-aux-Octas/1|accessdate=27 July 2015|publisher=Journal l'Express|date=12 May 2015|language=fr}}</ref>

== See also ==
*[[List of content management systems]]

== References ==
{{Reflist|30em}}

== External links ==
* [http://altitude3.net Altitude3.Net's website]

[[:Category:Content management systems]]
[[:Category:Website management]]



[[Category:Content management systems]]
[[Category:Data management]]
[[Category:Technical communication]]
<=====doc_Id=====>:338
<=====title=====>:
WCF Data Services
<=====text=====>:
{{Primary sources|date=November 2010}}
'''[[Windows Communication Foundation|WCF]] Data Services''' (formerly '''ADO.NET Data Services''',<ref>{{cite web|url=http://blogs.msdn.com/astoriateam/archive/2009/11/17/simplifying-our-n-tier-development-platform-making-3-things-1-thing.aspx|title=Simplifying our n-tier development platform: making 3 things 1 thing|date=2009-11-17|accessdate=2009-12-17|publisher=ADO.NET Data Services Team Blog}}</ref> codename '''"Astoria"''')<ref>{{cite web | url = http://blogs.msdn.com/data/archive/2007/12/10/ado-net-data-services-ctp-released.aspx | title = ADO.NET Data Services CTP Released! | accessdate = 2007-11-12}}</ref> is a platform for what [[Microsoft]] calls ''Data Services''. It is actually a combination of the runtime and a [[web service]] through which the services are exposed. In addition, it also includes the '''Data Services Toolkit''' which lets Astoria Data Services be created from within [[ASP.NET]] itself. The Astoria project was announced at [[MIX (Microsoft)|MIX]] 2007, and the first developer preview was made available on April 30, 2007. The first [[Software release life cycle#Beta|CTP]] was made available as a part of the [[ASP.NET]] 3.5 Extensions Preview. The final version was released as part of [[Service Pack]] 1 of the [[.NET Framework 3.5]] on August 11, 2008.  The name change from ADO.NET Data Services to WCF data Services was announced at the 2009 [[Professional Developers Conference|PDC]].

==Overview==
WCF Data Services exposes data, represented as [[ADO.NET#Entity Framework|Entity Data Model]] (EDM) objects, via web services accessed over [[HTTP]]. The data can be addressed using a [[Representational State Transfer|REST]]-like [[URI]]. The data service, when accessed via the HTTP GET method with such a URI, will return the data. The web service can be configured to return the data in either plain [[XML]], [[JSON]] or [[Resource Description Framework|RDF+XML]]. In the initial release, formats like [[RSS]] and [[ATOM]] are not supported, though they may be in the future. In addition, using other HTTP methods like PUT, POST or DELETE, the data can be updated as well. POST can be used to create new entities, PUT for updating an entity, and DELETE for deleting an entity.

==Description==

Windows Communication Foundation (WCF) comes to the rescue when we find ourselves not able to achieve what we want to achieve using web services, i.e., other protocols support and even duplex communication. With WCF, we can define our service once and then configure it in such a way that it can be used via HTTP, TCP, IPC, and even Message Queues. We can consume Web Services using server side scripts (ASP.NET), JavaScript Object Notations (JSON), and even REST (Representational State Transfer).

'''Understanding the basics'''

When we say that a WCF service can be used to communicate using different protocols and from different kinds of applications, we will need to understand how we can achieve this. If we want to use a WCF service from an application, then we have three major questions:

'''1.'''Where is the WCF service located from a client's perspective?
'''2.'''How can a client access the service, i.e., protocols and message formats?
'''3.'''What is the functionality that a service is providing to the clients?

Once we have the answer to these three questions, then creating and consuming the WCF service will be a lot easier for us. The WCF service has the concept of endpoints. A WCF service provides endpoints which client applications can use to communicate with the WCF service. The answer to these above questions is what is known as the ABC of WCF services and in fact are the main components of a WCF service. So let's tackle each question one by one.

'''Address:''' Like a webservice, a WCF service also provides a URI which can be used by clients to get to the WCF service. This URI is called as the Address of the WCF service. This will solve the first problem of "where to locate the WCF service?" for us.

'''Binding:''' Once we are able to locate the WCF service, we should think about how to communicate with the service (protocol wise). The binding is what defines how the WCF service handles the communication. It could also define other communication parameters like message encoding, etc. This will solve the second problem of "how to communicate with the WCF service?" for us.

'''Contract:''' Now the only question we are left up with is about the functionalities that a WCF service provides. Contract is what defines the public data and interfaces that WCF service provides to the clients.


The URIs representing the data will contain the physical location of the service, as well as the service name. In addition, it will also need to specify an EDM Entity-Set or a specific entity instance, as in respectively
 <nowiki>http://dataserver/service.svc/MusicCollection</nowiki>
or
 <nowiki>http://dataserver/service.svc/MusicCollection[SomeArtist]</nowiki>
The former will list all entities in the ''Collection'' set whereas the latter will list only for the entity which is indexed by ''SomeArtist''.

In addition, the URIs can also specify a traversal of a relationship in the Entity Data Model. For example,
 <nowiki>http://dataserver/service.svc/MusicCollection[SomeSong]/Genre</nowiki>
traverses the relationship ''Genre'' (in SQL parlance, joins with the ''Genre'' table) and retrieves all instances of ''Genre'' that are associated with the entity ''SomeSong''. Simple predicates can also be specified in the URI, like
 <nowiki>http://dataserver/service.svc/MusicCollection[SomeArtist]/ReleaseDate[Year eq 2006]</nowiki>
will fetch the items that are indexed by ''SomeArtist'' and had their ''release'' in ''2006''. Filtering and partition information can also be encoded in the URL as
 <nowiki>http://dataserver/service.svc/MusicCollection?$orderby=ReleaseDate&$skip=100&$top=50</nowiki>
It is important to note that although the presence of skip and top keywords indicate paging support, in Data Services version 1 there is no method of determining the number of records available and thus impossible to determine how many pages there may be.  The [[Open Data Protocol|OData]] 2.0 spec adds support for the '''$count''' path segment (to return just a count of entities) and '''$inlineCount''' (to retrieve a page worth of entities and a total count without a separate round-trip....).<ref>http://msdn.microsoft.com/en-us/library/ee373845.aspx</ref>

==References==
{{Reflist}}
{{Refbegin}}
* {{cite web | url = http://blogs.msdn.com/pablo/archive/2007/04/30/codename-astoria-data-services-for-the-web.aspx | title = Codename "Astoria": Data Services for the Web | accessdate = 2007-04-30}}
* [http://astoria.mslivelabs.com/ ADO.NET Data Services Framework (formerly "Project Astoria")]
{{Refend}}

==External links==
*[http://msdn.microsoft.com/en-us/library/cc907912.aspx Using Microsoft ADO.NET Data Services]
*[http://www.asp.net/downloads/3.5-extensions/ ASP.NET 3.5 Extensions Preview]
*[http://blogs.msdn.com/astoriateam/ ADO.NET Data Services (Project Astoria) Team Blog]
*[http://entmag.com/news/article.asp?EditorialsID=9105 Access Cloud Data with Astoria: ENT News Online]

{{.NET Framework}}

[[Category:Data management]]
[[Category:Web services]]
[[Category:ADO.NET Data Access technologies]]
[[Category:.NET Framework]]
<=====doc_Id=====>:341
<=====title=====>:
Category:Clinical data management
<=====text=====>:
[[Category:Clinical research]]
[[Category:Pharmaceutical industry]]
[[Category:Data management]]
<=====doc_Id=====>:344
<=====title=====>:
Bright Computing
<=====text=====>:
{{Infobox company
| name = Bright Computing
| logo = 
| logo_size = 
| logo_alt = 
| logo_caption = 
| logo_padding = 
| image = 
| image_size = 
| image_alt = 
| image_caption = 
| type = [[Privately held company|Private]]
| founded = 2009 <!-- if known: {{start date and age|YYYY|MM|DD}} in [[city]], [[state]], [[country]] -->
| founder = {{Unbulleted list|Matthijs van Leeuwen|Alex Ninaber}}
| hq_location = {{Unbulleted list|[[Amsterdam, The Netherlands]]|[[San Jose, California]]}}
| hq_location_city = 
| hq_location_country = 
| area_served = Global <!-- or: | areas_served = -->
| key_people = {{Unbulleted list|Bill Wagner ([[Chief executive officer|CEO]])|Martijn de Vries ([[Chief technology officer|CTO]])|Kristin Hansen ([[Chief marketing officer|CMO]])|Bill Griffin ([[Chief financial officer|CFO]])}}
| industry = [[Enterprise software]]
| products = {{Unbulleted list|Bright Cluster Manager for HPC|Bright Cluster Manager for Big Data|Bright OpenStack}}
| brands = 
| services = 
| former_name = ClusterVision (spin-off) 
| website = {{URL|brightcomputing.com}} <!-- or: | homepage = --><!-- {{URL|example.com}} -->
}}
'''Bright Computing''', Inc. is a developer of [[software]] for deploying and managing [[Supercomputer|high-performance]] (HPC) clusters, [[big data]] clusters, and [[OpenStack]] in [[data center]]s and using [[cloud computing]].<ref>{{Cite web|url=http://www.hpcwire.com/2016/02/03/24601/|title=Create Mixed HPC/Big Data Clusters Today Says Bright Computing|date=2016-02-03|website=HPCwire|access-date=2016-05-24}}</ref>

== History ==
Bright Computing was founded by Matthijs van Leeuwen in 2009, who spun the company out of ClusterVision, which he had co-founded with Alex Ninaber and Arijan Sauer. Alex and Matthijs had worked together at UK’s Compusys, which was one of the first companies to commercially build HPC clusters.<ref>{{Cite web|url=http://www.bloomberg.com/research/stocks/private/person.asp?personId=282144720&privcapId=115561075&previousCapId=115561075&previousTitle=Bright%2520Computing,%2520Inc.|title=Matthijs van Leeuwen: Executive Profile & Biography - Businessweek|website=www.bloomberg.com|access-date=2016-05-24}}</ref><ref>{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}</ref> They left Compusys in 2002 to start ClusterVision in the [[Netherlands]], after determining there was a growing market for building and managing supercomputer clusters using off-the-shelf hardware components and open source software, tied together with their own customized scripts.<ref>{{Cite web|url=http://www.clustervision.com/content/management-team|title=ClusterVision - Europe's Leading HPC and Cloud Specialist|website=ClusterVision|language=en-GB|access-date=2016-05-24}}</ref> ClusterVision also provided delivery and installation support services for HPC clusters at universities and government entities.<ref>{{Cite web|url=http://clustervision.com/about-the-company/|title=About the Company - ClusterVision|website=ClusterVision|language=en-GB|access-date=2016-05-24}}</ref>

In 2004, Martijn de Vries joined ClusterVision and began development of cluster management software. The software was made available to customers in 2008, under the name ClusterVisionOS v4.<ref>{{Cite web|url=http://www.beowulf.org/pipermail/beowulf/2008-September/023265.html|title=Roll your own cluster management system with ClusterVisionOS v4 was: [Beowulf] What services do you run on your cluster nodes?|last=holway|first=andrew|date=2008-09-23|access-date=2016-05-24}}</ref>

In 2009, Bright Computing was spun out of ClusterVision. ClusterVisionOS was renamed Bright Cluster Manager, and van Leeuwen was named Bright Computing’s CEO.<ref>{{Cite web|url=http://www.hpcwire.com/2009/10/07/clustervision_spins_off_cluster_management_software_company/|title=ClusterVision Spins Off Cluster Management Software Company|date=2009-10-07|website=HPCwire|access-date=2016-05-24}}</ref>

In 2010, [[ING Group|ING Corporate Investments]] made a $2.5 million investment in Bright Computing. In 2014, [[Draper Fisher Jurvetson]] (DFJ) (US), [[DFJ Esprit]] (UK), Prime Ventures (NL), and [[ING Group|ING Corporate Investments]] invested $14.5 million in Bright Computing. At that time, Bright Computing and ClusterVision were completely separated.<ref>{{Cite web|url=https://www.crunchbase.com/organization/ing-corporate-investments#/entity|title=ING Corporate Investments {{!}} CrunchBase|website=www.crunchbase.com|access-date=2016-05-24}}</ref>

In February 2016, Bright appointed Bill Wagner as chief executive officer. Matthijs van Leeuwen became chief strategy officer and board member.<ref>{{Cite web|url=http://insidehpc.com/2016/02/bright-computing-names-bill-wagner-as-chief-executive-officer/|title=Bright Computing Names Bill Wagner as CEO - insideHPC|date=2016-02-16|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref>

== Customers  ==
Early customers included [[Boeing]],<ref>{{Cite web|url=http://insidehpc.com/2011/12/boeing-consolidates-on-brite-cluster-manager/|title=Boeing Consolidates on Bright Cluster Manager - insideHPC|date=2011-12-06|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref> [[Sandia National Laboratories]],<ref>{{Cite web|url=http://www.brightcomputing.com/news/sandia-labs-adopts-bright-cluster-manager|title=Sandia National Laboratories Adopts Bright Cluster Manager to Manage Departmental Clusters|last=Staff|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref> [[Virginia Tech]],<ref>{{Cite web|url=https://finance.yahoo.com/news/virginia-bioinformatics-institute-selects-bright-151500615.html|title=Virginia Bioinformatics Institute Selects Bright Cluster Manager for Big Data to Test New Research Methods|website=Yahoo Finance|access-date=2016-05-24}}</ref> [[Hewlett Packard Enterprise Services|Hewlett Packard]],<ref>{{Cite web|url=http://www8.hp.com/h20195/v2/GetPDF.aspx/4AA5-2604ENW.pdf|title=HPC Accelerates SMBs|last=|first=|date=|website=|publisher=Hewlett-Packard Development Company, L.P.|access-date=2007-05-24}}</ref> [[National Security Agency|NSA]], and [[Drexel University]]. Many early customers were introduced through resellers, including SICORP,<ref>{{Cite web|url=http://insidehpc.com/2010/02/bright-computing-and-sicorp-sign-reseller-agreement/|title=Bright Computing And SICORP Sign Reseller Agreement - insideHPC|date=2010-02-22|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref> [[Cray]],<ref>{{Cite web|url=http://www.cray.com/company/collaboration/partners|title=Partner Relationships {{!}} Cray|website=www.cray.com|access-date=2016-05-24}}</ref> [[Dell]],<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2015/11/11/MN54237|title=Bright Computing Announces Integration with Dell PowerEdge Servers for HPC Environments - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref> Appro, and Advanced HPC.<ref>{{Cite web|url=http://www.advancedhpc.com/high_performance_servers/gpu_computing/bright_computing.html|title=Advanced HPC - GPU Computing - GPU Software - Bright Computing|website=www.advancedhpc.com|access-date=2016-05-24}}</ref>

By 2014, the company estimated 400 customers, including more than 20 [[Fortune 500]] Companies.<ref>{{Cite web|url=http://www.primeventures.com/portfolio/bright-computing/|title=Prime Ventures - Bright Computing|website=www.primeventures.com|access-date=2016-05-25}}</ref>

== Products and services ==
Bright Cluster Manager for HPC lets customers deploy and manage complete clusters. It provides management for the hardware, the operating system, the HPC software, and users.<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-roll-new-line-products/|title=Bright Computing to Roll Out New Line of Products|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>

In 2014, Bright Computing introduced Bright Cluster Manager for [[Apache Hadoop]] clusters.<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/04/03/MN96527|title=Bright Computing Announces Full Support for Apache Hadoop at the Hadoop Summit Europe in Amsterdam - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref> It was later extended to support [[Apache Spark]] and other big data applications, and was renamed, Bright Cluster Manager for Big Data. It can be used as a complete solution or to manage big data software distributions from leading vendors, including [[Cloudera]] and [[Hortonworks]]. It also has several features that allow the combination of Big Data and HPC workloads on the same cluster.<ref>{{Cite web|url=http://www.deskeng.com/de/bright-computing-releases-bright-cluster-manager-7-2/|title=Bright Computing Releases Bright Cluster Manager 7.2|date=2016-01-22|website=Desktop Engineering|language=en-US|access-date=2016-05-24}}</ref>

In 2014, the company announced Bright OpenStack, software to deploy, provision, and manage [[OpenStack]]-based private cloud infrastructures.<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-announces-bright-openstack-integration-talligent-openbook-billing-software-openstack-clouds/|title=Bright Computing Announces Bright OpenStack Integration With Talligent Software|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>

In January 2016, version 7.2 was released. The updates supported containers using [[Docker (software)|Docker]], improved integration with [[Puppet (software)|Puppet]] and job-based metrics.<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-highlights-version-7-2-bright-cluster-manager-software-solution-enterprisehpc-2016/|title=Bright Computing Highlights Version 7.2 of Bright Cluster Manager Software Solutions at EnterpriseHPC 2016|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>

Bright Cluster Manager software is frequently sold through [[original equipment manufacturer]] (OEM) resellers, including Dell and Cray.<ref>{{Cite web|url=http://www.brightcomputing.com/usa|title=USA|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
Technology partners include:
{{Div col|3}}
* Adaptive Computing<ref>{{Cite web|url=http://www.adaptivecomputing.com/news/adaptive-computing-bright-computing-deepen-product-integration-enhance-provisioning-workflow-optimization-technical-computing-environments/|title=Adaptive Computing and Bright Computing Deepen Product Integration to Enhance Provisioning and Workflow Optimization in Technical Computing Environments - Adaptive Computing|date=2014-06-24|website=Adaptive Computing|language=en-US|access-date=2016-05-24}}</ref>
* Allinea<ref>{{Cite web|url=http://insidehpc.com/2015/09/bright-computing-collaborates-on-openhpec-accelerator-suite/|title=Bright Computing Collaborates on OpenHPEC Accelerator Suite|date=2015-09-16|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref>
* [[Altair Engineering|Altair]]<ref>{{Cite web|url=http://www.prweb.com/releases/2011/9/prweb8788932.htm|title=Bright Computing Now Resells Altair PBS Professional Workload Manager, Delivering HPC Operational Efficiencies and Cost Savings|website=PRWeb|access-date=2016-05-24}}</ref>
* [[Amazon Web Services|Amazon]]<ref>{{Cite web|url=http://www.hpcwire.com/2011/11/08/bright_computing_bursts_into_cloud/|title=Bright Computing Bursts Into Cloud|date=2011-11-08|website=HPCwire|access-date=2016-05-24}}</ref>
* [[Ansys]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-ansys|title=Technology Partner - Ansys|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* [[Cavium]]<ref>{{Cite web|url=http://www.cavium.com/newsevents_Cavium-Collaboration-with-BrightComputing.html|title=Cavium Announces Collaboration with Bright Computing to Support the ThunderX Processor Family|website=www.cavium.com|access-date=2016-05-24}}</ref>
* [[Cisco Systems|Cisco]]<ref>{{Cite web|url=https://gigaom.com/2014/07/28/bright-computing-takes-in-14-5m-to-push-its-cluster-management-smarts/|title=Bright Computing takes in $14.5M to push its cluster management smarts|last=Darrow|first=Barb|date=2014-07-28|website=gigaom.com|access-date=2016-05-24}}</ref>
* Cloudera<ref>{{Cite web|url=http://insidebigdata.com/2014/04/10/bright-computing-achieves-cloudera-certification-bright-cluster-manager/|title=Bright Computing Achieves Cloudera Certification for Bright Cluster Manager|last=Brueckner|first=Rich|date=2014-04-10|website=insideBIGDATA|access-date=2016-05-24}}</ref>
* Cray<ref>{{Cite web|url=https://cug.org/proceedings/cug2015_proceedings/includes/files/pap176-file2.pdf|title=Cray User Group - Bright Cluster Manager Presentation|last=|first=|date=|website=|publisher=|access-date=}}</ref>
* DDN Storage<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-ddn|title=Technology Partner - DDN|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* Dell<ref>{{Cite web|url=http://insidehpc.com/2015/12/dellbright/|title=Bright Cluster Manager Integrates with Dell PowerEdge Servers for HPC Environments - insideHPC|date=2015-12-09|website=insideHPC|language=en-US|access-date=2016-05-24}}</ref>
* [[Fujitsu]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-fujitsu|title=Technology Partner - Fujitsu|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* Hewlett Packard Enterprise<ref>{{Cite web|url=http://www.datacenterknowledge.com/archives/2014/07/29/bright-cluster-management-raises-dough/|title=Bright Networks Cluster Management Gets $14.5m Round|date=2014-07-29|website=Data Center Knowledge|language=en-US|access-date=2016-05-24}}</ref>
* Hortonworks<ref>{{Cite web|url=http://hortonworks.com/partner/bright-computing/|title=Bright Computing partners with Hortonworks for Hadoop|website=Hortonworks|language=en-US|access-date=2016-05-24}}</ref>
* [[Huawei]]<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2014/09/16/MN12537|title=Huawei signs global reseller agreement with Bright Computing - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref>
* [[IBM]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-ibm|title=Technology Partner - IBM|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* [[Inspur]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-inspur|title=Technology Partner - Inspur|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* [[Intel]]<ref>{{Cite web|url=https://ctovision.com/2015/08/bright-computing-integrated-intel-enterprise-edition-lustre/|title=Bright Computing Integrated with Intel Enterprise Edition for Lustre - CTOvision.com|date=2015-08-13|website=CTOvision.com|language=en-US|access-date=2016-05-24}}</ref>
* [[Lenovo]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-lenovo|title=Technology Partner - Lenovo|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* Magma<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-magma|title=Technology Partner - Magma|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* [[Mellanox Technologies]]<ref>{{Cite web|url=http://www.openstack.org/news/view/214/bright-computing-announces-bright-openstack%25E2%2584%25A2-integration-with-mellanox-technologies-infiniband-switches-and-virtual-extensible-lan-(vxlan)-offloading|title=» OpenStack Open Source Cloud Computing Software|website=www.openstack.org|access-date=2016-05-24}}</ref>
* [[Microsoft]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-microsoft|title=Technology Partner - Microsoft|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* NICE<ref>{{Cite web|url=https://www.nice-software.com/partners/bright-computing|title=Bright Computing - NICE|website=www.nice-software.com|access-date=2016-05-24}}</ref>
* [[Nvidia|NVidia]]<ref>{{Cite web|url=https://developer.nvidia.com/bright-cluster-manager|title=Bright Cluster Manager|date=2012-01-12|website=NVIDIA Developer|access-date=2016-05-24}}</ref>
* [[Red Hat|Redhat]]<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-redhat|title=Technology Partner - RedHat|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* Runtime Design Automation<ref>{{Cite web|url=http://www.brightcomputing.com/technology-partner-runtime|title=Technology Partner - Runtime|last=Computing|first=Bright|website=www.brightcomputing.com|access-date=2016-05-24}}</ref>
* [[Silicon Graphics International|SGI]]<ref>{{Cite web|url=https://www.sgi.com/partners/technology/partners.html|title=SGI - Partners: Technology Partners: SGI Partnerships|website=www.sgi.com|access-date=2016-05-24}}</ref>
* [[Supermicro]]<ref>{{Cite web|url=http://www.montana.edu/rci/HyaliteCluster.html|title=Hyalite Cluster - Research Computing Group {{!}} Montana State University|website=www.montana.edu|access-date=2016-05-24}}</ref>
* [[SUSE]]<ref>{{Citation|last=SUSE|title=Bright Computing and SUSE share common values system|date=2015-10-29|url=https://www.youtube.com/watch?v=4WwAKlOBhBo|accessdate=2016-05-24}}</ref>
* [[Taligent]]<ref>{{Cite web|url=https://finance.yahoo.com/news/bright-computing-announces-bright-openstack-073000503.html|title=Bright Computing Announces Bright OpenStack™ Integration with Talligent Openbook Billing Software for OpenStack Clouds|website=Yahoo Finance|access-date=2016-05-24}}</ref>
* [[Univa]]<ref>{{Cite web|url=http://www.prweb.com/releases/2012/10/prweb9961985.htm|title=Bright Computing and Univa Offer Combined Cluster & Workload Management Solution|website=PRWeb|access-date=2016-05-24}}</ref>
{{Div col end}}

Bright Computing was covered by [[Software Magazine]]<ref>{{Cite web|url=http://www.softwaremag.com/bright-computing-releases-version-7-2-of-bright-cluster-manager-for-hpc-bright-cluster-manager-for-big-data-and-bright-openstack/|title=Bright Computing Releases Version 7.2 of Bright Cluster Manager for HPC, Bright Cluster Manager for Big Data, and Bright OpenStack –|website=www.softwaremag.com|access-date=2016-05-24}}</ref> and [[Yahoo! Finance]],<ref>{{Cite web|url=https://finance.yahoo.com/news/bright-computing-showcase-bright-openstack-162500598.html|title=Bright Computing to Showcase Bright OpenStack™ and HPC Cluster-as-a-Service (CaaS) at 2016 OpenStack Summit in Austin|website=Yahoo Finance|access-date=2016-05-24}}</ref> among other publications.

== Awards ==
In 2016, Bright Computing was awarded a €1.5M [[Horizon 2020]] SME Instrument grant from the [[European Commission]].<ref>{{Cite web|url=http://www.hpcwire.com/off-the-wire/bright-computing-receives-horizon-2020-grant-european-commission/|title=Bright Computing Receives Horizon 2020 Grant From European Commission|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>

Bright Computing was one of only 33 grant recipients from 960 submitted proposals.<ref>{{Cite web|url=https://ec.europa.eu/easme/en/horizons-2020-sme-instrument|title=Horizon 2020's SME Instrument - EASME - European Commission|website=EASME|access-date=2016-05-24}}</ref> In its category only 5 out of 260 grants were awarded.<ref>{{Cite web|url=https://ec.europa.eu/digital-single-market/en/open-disruptive-innovation-0|title=Open Disruptive Innovation - Digital Single Market - European Commission|website=Digital Single Market|access-date=2016-05-24}}</ref>

* 2015 HPCwire Editor’s Choice Award for “Best HPC Cluster Solution or Technology."<ref>{{Cite web|url=http://www.hpcwire.com/2015-hpcwire-readers-choice-awards/|title=2015 HPCwire Awards – Readers’ & Editors’ Choice - HPCwire|website=HPCwire|language=en-US|access-date=2016-05-24}}</ref>
* Main Software 50 “Highest Growth” award winner, 2013.<ref>{{Cite web|url=http://www.bizjournals.com/prnewswire/press_releases/2013/11/14/MN16669|title=Bright Computing Wins Main Software 50 "Highest Growth" Award - The Business Journals|website=The Business Journals|access-date=2016-05-24}}</ref>
* [[Deloitte]] Technology Fast50 “Rising Star 2013” Award Winner.<ref>{{Cite web|url=http://www2.deloitte.com/be/en/pages/about-deloitte/articles/ayden-wins-fast50.html|title=Adyen wins Deloitte Technology Fast50 {{!}} Deloitte Belgium {{!}} TMT {{!}} News, press release|website=Deloitte Belgium|access-date=2016-05-24}}</ref>
* Bio-IT World Conference & Expo ‘13, Boston, MA, Winner of “IT Hardware & Infrastructure” category of the “Best of Show Award” program.<ref>{{Cite web|url=http://www.bio-itworld.com/2013/4/10/2013-bio-it-world-best-show-winners-named.html|title=2013 Bio-IT World Best of Show Winners Named - Bio-IT World|access-date=2016-05-24}}</ref>
* [[Red Herring (magazine)|Red Herring]] Top 100 Global Award, 2013.<ref>{{Cite web|url=http://www.redherring.com/events/rhna/2013-rhnawinners/|title=2013 Top 100 North America: Winners — Red Herring|website=Red Herring|language=en-US|access-date=2016-05-24}}</ref>

== References ==
{{reflist|30em}}

==Further reading==
* Morgan, Timothy Prickett (June 20, 2011). [http://www.theregister.co.uk/2011/06/20/bright_cluster_manager_5_2/ "Bright Computing revs up cluster manager"]. ''[[The Register]]''.
* Morgan, Timothy Prickett (November 8, 2011). [http://www.theregister.co.uk/2011/11/08/bright_cluster_manager_amazon_cloud_bursting/ "Bright Computing bursts HPC to EC2 clouds"]. ''The Register''.

[[Category:Big data companies]]
[[Category:Cloud computing]]
[[Category:Cloud infrastructure]]
[[Category:Cluster computing]]
[[Category:Data management]]
[[Category:Supercomputers]]
<=====doc_Id=====>:347
<=====title=====>:
Research data archiving
<=====text=====>:
'''Research data archiving''' is  the [[Computer_data_storage#Volatility|long-term storage]] of scholarly research [[data]], including  the natural sciences, social sciences, and life sciences. The various [[academic journals]] have differing policies regarding how much of their data and methods researchers are required to store in a public archive, and what is actually archived varies widely between different disciplines. Similarly, the major grant-giving institutions have varying attitudes towards public archival of data. In general, the tradition of science has been for publications to contain sufficient information to allow fellow researchers to replicate and therefore test the research. In recent years this approach has become increasingly strained as   research in some areas depends on large datasets which cannot easily be replicated independently.

Data archiving is more important in some fields than others.  In a few fields, all of the data necessary to replicate the work is already available in the journal article.  In drug development, a great deal of data is generated and must be archived so researchers can verify that the reports the drug companies publish accurately reflect the data.

The requirement of data archiving is a recent development in the [[history of science]].  It was made possible by advances in [[information technology]] allowing large amounts of data to be stored and accessed from central locations.  For example, the [[American Geophysical Union]] (AGU) adopted their first policy on data archiving in 1993, about three years after the beginning of the [[WWW]].<ref>”Policy on Referencing Data in and Archiving Data for AGU Publications” [http://www.agu.org/pubs/authors/policies/data_policy.shtml]</ref> This policy mandates that datasets cited in AGU papers must be archived by a recognised data center; it permits the creation of "data papers"; and it establishes AGU's role in maintaining data archives. But it makes no requirements on paper authors to archive their data.

Prior to organized data archiving, researchers wanting to evaluate or replicate a paper would have to request data and methods information from the author.  The academic community expects authors to [[Data sharing (Science)|share supplemental data]].  This process was recognized as wasteful of time and energy and obtained mixed results.  Information could become lost or corrupted over the years.  In some cases, authors simply refuse to provide the information.

The need for data archiving and due diligence is greatly increased when the research deals with health issues or public policy formation.<ref>"The Case for Due Diligence When Empirical Research is Used in Policy Formation" by Bruce McCullough and Ross McKitrick. [http://economics.ca/2006/papers/0685.pdf]</ref><ref>[http://gking.harvard.edu/replication.shtml "Data Sharing and Replication" a website by Gary King]</ref>

==Selected policies by journals==

===''The American Naturalist''===
{{quote|''[[The American Naturalist]]'' requires authors to deposit the data associated with accepted papers in a public archive. For gene sequence data and phylogenetic trees, deposition in [[GenBank]] or [[TreeBASE]], respectively, is required. There are many possible archives that may suit a particular data set, including the [[Dryad (repository)|Dryad]] repository for ecological and evolutionary biology data. All accession numbers for GenBank, TreeBASE, and Dryad must be included in accepted manuscripts before they go to Production. If the data is deposited somewhere else, please provide a link. If the data is culled from published literature, please deposit the collated data in Dryad for the convenience of your readers. Any impediments to data sharing should be brought to the attention of the editors at the time of submission so that appropriate arrangements can be worked out.|JSTOR<ref>[http://www.jstor.org/page/journal/amernatu/forAuthor.html#data Supporting Data and Material]</ref>}}

===''Journal of Heredity''===
{{quote|The primary data underlying the conclusions of an article are critical to the verifiability and transparency of the scientific enterprise, and should be preserved in usable form for decades in the future. For this reason, ''Journal of Heredity'' requires that newly reported nucleotide or amino acid sequences, and structural coordinates, be submitted to appropriate public databases (e.g., GenBank; the [[EMBL Nucleotide Sequence Database]]; DNA Database of Japan; the [[Protein Data Bank]] ; and [[Swiss-Prot]]). Accession numbers must be included in the final version of the manuscript. For other forms of data (e.g., microsatellite genotypes, linkage maps, images), the Journal endorses the principles of the Joint Data Archiving Policy (JDAP) in encouraging all authors to archive primary datasets in an appropriate public archive, such as Dryad, TreeBASE, or the Knowledge Network for Biocomplexity. Authors are encouraged to make data publicly available at time of publication or, if the technology of the archive allows, opt to embargo access to the data for a period up to a year after publication.

The American Genetic Association also recognizes the vast investment of individual researchers in generating and curating large datasets. Consequently, we recommend that this investment be respected in secondary analyses or meta-analyses in a gracious collaborative spirit.|oxfordjournals.org<ref>[http://www.oxfordjournals.org/our_journals/jhered/for_authors/msprep_submission.html#4.%20DATA%20ARCHIVING%20POLICY Data archiving policy]</ref>}}

===''Molecular Ecology''===
{{quote|[[Molecular Ecology]] expects that data supporting the results in the paper should be archived in an appropriate public archive, such as GenBank, [[Gene Expression Omnibus]], TreeBASE, Dryad, the [[Knowledge Network for Biocomplexity]], your own institutional or funder repository, or as Supporting Information on the Molecular Ecology web site. Data are important products of the scientific enterprise, and they should be preserved and usable for decades in the future. Authors may elect to have the data publicly available at time of publication, or, if the technology of the archive allows, may opt to embargo access to the data for a period up to a year after publication. Exceptions may be granted at the discretion of the editor, especially for sensitive information such as human subject data or the location of endangered species.|Wiley<ref>[http://www.wiley.com/bw/submit.asp?ref=0962-1083&site=1 Policy on data archiving]</ref>}}

===''Nature''===
{{quote|Such material must be hosted on an accredited independent site (URL and accession numbers to be provided by the author), or sent to the ''Nature'' journal at submission, either uploaded via the journal's online submission service, or if the files are too large or in an unsuitable format for this purpose, on CD/DVD (five copies). Such material cannot solely be hosted on an author's personal or institutional web site.<ref>[http://www.nature.com/authors/editorial_policies/availability.html "Availability of Data and Materials: The Policy of Nature Magazine]</ref>

''Nature'' requires the reviewer to determine if all of the supplementary data and methods have been archived.  The policy advises reviewers to consider several questions, including: "Should the authors be asked to provide supplementary methods or data to accompany the paper online? (Such data might include source code for modelling studies, detailed experimental protocols or mathematical derivations.)|[[Nature (journal)|Nature]]<ref>{{cite web|title=Guide to Publication Policies of the Nature Journals|date=March 14, 2007|url=http://www.nature.com/authors/gta.pdf}}</ref>}}

===''Science''===
{{quote|''Science'' supports the efforts of databases that aggregate published data for the use of the scientific community. Therefore, before publication, large data sets (including microarray data, protein or DNA sequences, and atomic coordinates or electron microscopy maps for macromolecular structures) must be deposited in an approved database and an accession number provided for inclusion in the published paper.<ref>[http://www.sciencemag.org/about/authors/prep/gen_info.dtl#datadep "General Policies of Science Magazine"]</ref>

"Materials and methods" – ''Science'' now requests that, in general, authors place the bulk of their description of materials and methods online as supporting material, providing only as much methods description in the print manuscript as is necessary to follow the logic of the text. (Obviously, this restriction will not apply if the paper is fundamentally a study of a new method or technique.)|[[Science (journal)|Science]]<ref>[http://www.sciencemag.org/about/authors/prep/prep_online.dtl ”Preparing Your Supporting Online Material”]</ref>}}

===Royal Society===
{{quote|To allow others to verify and build on the work published in [[Royal Society]] journals, it is a condition of publication that authors make available the data, code and research materials supporting the results in the article.
Datasets and code should be deposited in an appropriate, recognised, publicly available repository. Where no data-specific repository exists, authors should deposit their datasets in a general repository such as [[Dryad (repository)]] or [[Figshare]].
|[[Royal Society]]<ref>[https://royalsociety.org/journals/ethics-policies/data-sharing-mining/ "Data sharing and mining"]</ref>}}

==Policies by funding agencies==
In the United States, the [[National Science Foundation]] (NSF) has tightened requirements on data archiving.   Researchers seeking funding from NSF are now required to file a [[data management plan]] as a two-page supplement to the grant application.<ref>[http://news.sciencemag.org/scienceinsider/2010/05/nsf-to-ask-every-grant-applicant.html ”NSF to Ask Every Grant Applicant for Data Management Plan”]</ref>

The NSF [[Datanet]] initiative has resulted in funding of the '''Data Observation Network for Earth''' ([[DataONE]]) project, which will provide scientific data archiving for ecological and environmental data produced by scientists worldwide. DataONE's stated goal is to preserve and provide access to multi-scale, multi-discipline, and multi-national data. The community of users for DataONE includes scientists, ecosystem managers, policy makers, students, educators, and the public.

==Data archives==

===Natural sciences===
The following list refers to scientific data archives.
* [[CISL Research Data Archive]]
* [[Dryad (repository)|Dryad]]
* [[ESO/ST-ECF Science Archive Facility]]
* [http://www.ncdc.noaa.gov/paleo/treering.html International Tree-Ring Data Bank]
* [http://www.icpsr.umich.edu Inter-university Consortium for Political and Social Research]
* [http://knb.ecoinformatics.org Knowledge Network for Biocomplexity]
* [[National Archive of Computerized Data on Aging]]
* National Archive of Criminal Justice Data [http://www.icpsr.umich.edu/nacjd]
* [[National Climatic Data Center]]
* [[National Geophysical Data Center]]
* [[National Snow and Ice Data Center]]
* [[National Oceanographic Data Center]]
* [http://daac.ornl.gov Oak Ridge National Laboratory Distributed Active Archive Center]
* [[PANGAEA (data library)|Pangaea - Data Publisher for Earth & Environmental Science]]
* [[World Data Center]]
* [[DataONE]]

===Social sciences===
{{cleanup merge|Data archives|date=April 2016}}

'''Data archives''' are professional institutions for the acquisition, preparation, preservation, and dissemination of social and behavioral data. The term is also sometimes used about natural science institutions (e.g., [[CISL Research Data Archive]], see [[Scientific data archiving]] and Borgman, 2007, p.&nbsp;18<ref>Borgman, Christine L. (2007).''Scholarship in the digital age: information, infrastructure and the internet''. Cambridge, MA: The MIT Press.</ref>), but here seems '''data centers''' to be the most used term. Data archives in the social sciences evolved in the 1950s and has been perceived as an international movement: 

<blockquote>By 1964 the International Social Science Council (ISSC) had sponsored a second conference on Social Science Data Archives and had a standing Committee on Social Science Data, both of which stimulated the data archives movement. By the beginning of the twenty-first century, most developed countries and some developing countries had organized formal and well-functioning national data archives. In addition, college and university campuses often have `data libraries' that make data available to their faculty, staff, and students; most of these bear minimal archival responsibility, relying for that function on a national institution (Rockwell, 2001, p. 3227).<ref>Rockwell, R. C. (2001). Data Archives: International. IN: Smelser, N. J. & Baltes, P. B. (eds.) ''International Encyclopedia of the Social and Behavioral Sciences'' (vol. 5, pp. 3225- 3230). Amsterdam: Elsevier</ref></blockquote>

* [[Registry of Research Data Repositories | re3data.org]] is a global registry of research data repository indexing data archives from all disciplines: http://www.re3data.org
* CESSDA Members are data archives and other organisations that archive social science data and provide data for secondary use: http://www.cessda.net/about/members.html
* Consortium of European Social Science Data Archives: http://www.cessda.org/
* The Danish Data Archives: http://www.sa.dk/content/us/about_us ; specific page (only in Danish): http://www.sa.dk/dda/default.htm
* Inter-university Consortium for Political and Social Research: http://www.icpsr.umich.edu/
* The Roper Center for Public Opinion Research: http://www.ropercenter.uconn.edu
* The Social Science Data Archive: http://dataarchives.ss.ucla.edu/
* The NCAR Research Data Archive:  http://rda.ucar.edu

===Life sciences===
{{stub section|date=April 2016}}

==See also==
*[[Data archive]]

==References==
{{Reflist}}

==Notes==
* [[Registry of Research Data Repositories]] ''re3data.org'' [http://service.re3data.org/search/results?term=]
* Statistical checklist required by ''Nature'' [http://www.nature.com/nature/authors/gta/Statistical_checklist.doc]
* Policies of ''Proceedings of the National Academy of Sciences (U.S.)'' [http://www.pnas.org/misc/iforc.shtml#policies]
* The US National Committee for CODATA [http://www7.nationalacademies.org/usnc-codata/Archiving.html]
* The Role of Data and Program Code Archives in the Future of Economic Research  [http://research.stlouisfed.org/wp/2005/2005-014.pdf]
* Data sharing and replication – Gary King website [http://gking.harvard.edu/replication.shtml]
* The Case for Due Diligence When Empirical Research is Used in Policy Formation by McCullough and McKitrick [http://economics.ca/2006/papers/0685.pdf]
* Thoughts on Refereed Journal Publication by Chuck Doswell [http://www.cimms.ou.edu/~doswell/pubreviews.html]
* “How to encourage the right behaviour” An opinion piece published in ''Nature'',  March, 2002.[http://www.nature.com/nature/journal/v416/n6876/full/416001b.html]
* [[NASA Astrophysics Data System]] [http://cdsads.u-strasbg.fr/]
* [[Panton Principles]] for Open Data in Science, at Citizendium [http://en.citizendium.org/wiki/Panton_Principles]
* [[Inter-university Consortium for Political and Social Research]] [http://www.icpsr.umich.edu]


[[Category:Computer archives]]
[[Category:Data management]]
[[Category:Data publishing]]
[[Category:Digital preservation]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Structured storage]]
<=====doc_Id=====>:350
<=====title=====>:
Information repository
<=====text=====>:
{{other uses|Knowledge base}}

An '''information repository''' is an easy way to deploy a secondary tier of [[data storage device|data storage]] that can comprise multiple, networked data storage technologies running on diverse [[operating system]]s, where data that no longer needs to be in primary storage is protected, classified according to captured [[metadata]], processed, de-duplicated, and then purged, automatically, based on data service level objectives and requirements. In information repositories, data storage resources are virtualized as composite storage sets and operate as a [[Federation (information technology)|federated]] environment.

Information repositories were developed to mitigate problems arising from [[data proliferation]] and eliminate the need for separately deployed data storage solutions because of the concurrent deployment of diverse storage technologies running diverse operating systems. They feature centralized management for all deployed data storage resources. They are self-contained, support heterogeneous storage resources, support resource management to add, maintain, recycle, and terminate media, track of off-line media, and operate autonomously.

==Automated data management==
Since one of the main reasons for the implementation of an Information repository is to reduce the maintenance workload placed on IT staff by traditional data storage systems, information repositories are automated. Automation is accomplished via polices that can process data based on time, events, data age, and data content. Policies manage the following:
*File system space management
*Irrelevant data elimination (mp3, games, etc.)
*Secondary storage resource management
Data is processed according to media type, [[Storage virtualization|storage pool]], and [[data storage device|storage technology]].

Because information repositories are intended to reduce IT staff workload, they are designed to be easy to deploy and offer configuration flexibility, virtually limitless extensibility, redundancy, and reliable failover.

==Data recovery==
Information repositories feature robust, client based data search and recovery capabilities that, based on permissions, enable end users to search the information repository, view information repository contents, including data on off-line media, and recover individual files or multiple files to either their original [[Computer network|network]] computer or another network computer.

==References==
*NGDC Conference: Understand advanced IT infrastructures, Protecting Information: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.networkworld.com/ngdc/ 
*SNIA Enterprise Information World 2007 Conference: Benefits of a Federated Information Repository as a Secondary Storage Tier. http://www.enterpriseinformationworld.com/abstracts/benefits_federated_info.htm

{{DEFAULTSORT:Information Repository}}
[[Category:Information technology management|*]]
[[Category:Content management systems|*]]
[[Category:Data management]]
[[Category:Data security]]
[[Category:Records management]]
<=====doc_Id=====>:353
<=====title=====>:
Machine-Readable Documents
<=====text=====>:
'''Machine-readable documents''' are [[document]]s whose content can be readily processed by [[computer]]s.  Such documents are distinguished from [[machine-readable data]] by virtue of having sufficient structure to provide the necessary context to support the business processes for which they are created.  [[Data]] without [[context (language use)]] is meaningless and lacks the four essential [http://www.archives.gov/records-mgmt/policy/managing-web-records.html#1.0 characteristics] of trustworthy [[business record]]s specified in [[ISO 15489 Information and documentation -- Records management]]: 
* Reliability
* Authenticity
* Integrity
* [[Usability]]
The vast bulk of information is [[unstructured data]] and, from a business perspective, that means it is "immature", i.e., Level 1 (chaotic) of the [[Capability Maturity Model]].  Such immaturity fosters inefficiency, diminishes quality, and limits effectiveness.  Unstructured information is also ill-suited for [[records management]] functions, provides inadequate [[evidence]] for legal purposes, drives up the cost of [[discovery (law)]] in [[litigation]], and makes access and usage needlessly cumbersome in routine, ongoing [[business process]]es.

There are at least four aspects to machine-readability:  
* First, words or phrases should be discretely delineated (tagged) so that computer software and/or hardware logic can be applied to them as individual conceptual elements.  
* Second, the semantics of each element should be specified so that computers can help human beings achieve a common understanding of their meanings and potential usages.  
* Third, if the relationships among the individual elements are also specified, computers can automatically apply inferences to them, thereby further relieving human beings of the burden of trying to understand them, particularly for purposes of inquiry, discovery, and analysis. 
* Fourth, if the structures of the documents in which the elements occur are also specified, human understanding is further enhanced and the data becomes more reliable for legal and business-quality purposes.

As early as 1981, the U.S. [[Government Accountability Office]] (GAO) began reporting on the problem of inadequate record-keeping practices in the U.S. federal government.<ref>{{cite web
 | url=http://www.gao.gov/products/PLRD-81-2
 | title=FEDERAL RECORDS MANAGEMENT: A History of Neglect
 | work=gao.gov
 | date=1981-02-24
 | accessdate=2016-09-08}}
</ref>  Such deficiencies are not unique to government and advances in information technology mean that most information is now "born digital" and thus potentially far more easily managed by automated means.<ref>{{cite web
 | url=http://www.oclc.org/content/dam/research/activities/hiddencollections/borndigital.pdf
 | title=Defining "Born Digital": An Essay by Ricky Erway, OCLC Research
 | work=oclc.org
 | date=2010-11-30
 | accessdate=2016-09-08}}
</ref>  However, in testimony to Congress in 2010, GAO highlighted problems with managing electronic records, and as recently as 2015, GAO has continued to report inadequacies in the performance of Executive Branch agencies in meeting records management requirements.<ref>{{cite web
 | url=http://www.gao.gov/new.items/d10838t.pdf
 | title=INFORMATION MANAGEMENT: The Challenges of Managing Electronic Records, Statement of Valerie C. Melvin, Director, Information Management and Human Capital Issues 
 | work=gao.gov
 | date=2010-06-17
 | accessdate=2016-09-08}}
</ref>
<ref>{{cite web
 | url=http://www.gao.gov/products/GAO-15-339
 | title=INFORMATION MANAGEMENT: Additional Actions Are Needed to Meet Requirements of the Managing Government Records Directive
 | work=gao.gov
 | date=2015-05-14
 | accessdate=2016-09-08}}
</ref>  Moreover, more than two decades after a major and formerly highly respected auditing firm, [[Arthur Andersen]], met its demise due to a records destruction scandal, record-keeping practices became a central issue in the 2016 Presidential election.

On January 4, 2011, President Obama signed H.R. 2142, the [[Government Performance and Results Act]] (GPRA) Modernization Act of 2010 (GPRAMA), into law as P.L. 111-352. Section 10 of GPRAMA requires U.S. federal agencies to publish their strategic and performance plans and reports in searchable, machine-readable format.<ref>{{cite web
 | url=http://xml.fido.gov/stratml/references/PL111-532StratML.htm#SEC10
 | title=GPRAMA SEC. 10. FORMAT OF PERFORMANCE PLANS AND REPORTS.
 | work=congress.gov
 | date=2011-01-04
 | accessdate=2016-09-08}}
</ref>
Additionally, in 2013, he issued [[Executive Order]] 13642, Making Open and Machine Readable the New Default for Government Information in general.<ref>{{cite web
 | url=http://xml.fido.gov/stratml/carmel/EOOMRDwStyle.xml
 | title=Executive Order 13642 in open, standard, machine-readable Strategy Markup Language format
 | work=whitehouse.gov
 | date=2013-05-09
 | accessdate=2016-09-08}}
</ref>
On July 28, 2016, the [[Office of Management and Budget]] (OMB) followed up by including in the revised issuance of Circular A-130 direction for agencies to use [http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d15f0-5799-11e6-8d37-8523b3fa12e0 open, machine-readable formats] and to publish "[http://xml.fido.gov/stratml/carmel/iso/A130wStyle.xml#_3f7d449e-5799-11e6-8d37-8523b3fa12e0 public information online in a manner that promotes analysis and reuse for the widest possible range of purposes]", meaning that the information is both publicly accessible and machine-readable.

In support of such policy direction, technological advancement is enabling more efficient and effective management and use of machine-readable electronic records.  [[Document-oriented database]]s have been developed for storing, retrieving, and managing document-oriented information, also known as semi-structured data.  Extensible Markup Language ([[XML]]) is a World Wide Web Consortium ([[W3C]]) [[World Wide Web Consortium#W3C recommendation .28REC.29|Recommendation]] setting forth rules for encoding documents in a format that is both [[human-readable]] and machine-readable.  Many [[XML editor]] tools have been developed and most, if not all major information technology applications support XML to greater or lesser degrees.  The fact that XML itself is an open, standard, machine-readable format makes it relatively easy for application developers to do so.

The W3C's accompanying XML Schema ([[XSD]]) Recommendation specifies how to formally describe the elements in an XML document.  With respect to the specification of XML schemas, the [[Organization for the Advancement of Structured Information Standards]] (OASIS) is a leading [[standards-developing organization]]. [[JSON#JSON Schema|JSON Schema]] was proposed by the [[Internet Engineering Task Force]] (IETF) but was allowed to expire in 2013 and thus is less mature and a riskier alternative to XSD, the most recent version of which was approved by the W3C in 2012.

The W3C's Extensible Stylesheet Language ([[XSL]]) family of languages provides for the transformation and rendering of XML documents for human-readable presentation.  Machine-readable documents can be automatically rendered in human-readable format but documents formatted primarily for attractiveness of presentation cannot easily be processed by computers to support [[usability]] by human beings.  

The [[Portable Document Format]] (PDF) is a file format used to present documents in a manner independent of application software, hardware, and operating systems. Each PDF file encapsulates a complete description of the presentation of the document, including the text, fonts, graphics, and other information needed to display it.  [[PDF/A]] is an ISO-standardized version of the PDF specialized for use in the archiving and long-term preservation of electronic documents.  PDF/A-3 allows embedding of other file formats, including XML, into PDF/A conforming documents, thus potentially providing the best of both human- and machine-readability.  The W3C's [[XSL-FO]] (XSL Formatting Objects) markup language is commonly used to generate PDF files

[[Metadata]], data about data, can be used to organize electronic resources, provide digital identification, and support the archiving and preservation of resources.  In well-structured, machine-readable electronic records, the content can be [[Repurposing|repurposed]] as both data and metadata.  In the context of electronic record-keeping systems, the terms "management" and "metadata" are virtually synonymous.  Given proper metadata, records management functions can be automated, thereby reducing the risk of [[spoliation of evidence]] and other fraudulent manipulations of records.  Moreover, such records can be used to automate the process of [[audit]]ing data maintained in [[database]]s, thereby reducing the risk of single points of failure associated with the [[Machiavellianism#In the workplace|Machiavellian]] concept of a [[single source of truth]].

[[Blockchain (database)]] is a new technology for maintaining continuously-growing lists of records secured from tampering and revision.  A key feature is that every node in a decentralized system has a copy of the blockchain so there is no [[single point of failure]] subject to manipulation and [[fraud]].

==See also==

* [[Budapest Declaration on Machine Readable Travel Documents]]
* [[Comparison of XML editors]]
* [[Integrity]] and particularly [[Data integrity]]
* [[Linked data]]
* [[Machine-readable passport]]
* [[Open data]]
* [[Data reliability|Reliability]], particularly [[Reliability (statistics)]], [[Data reliability]], [[Reliability (computer networking)]], and [[Reliability (research methods)]]
* [[Strategy Markup Language]] (StratML)
* [[Structured document]]
* [[Tag (metadata)]]
* [[Universal Business Language]] (UBL)
* [[XBRL]] (eXtensible Business Reporting Language)

==References==
{{reflist}}

==External  links==
* [http://xml.fido.gov/stratml/carmel/M-13-13wStyle.xml#_78e85ef4-b91c-11e2-bf2b-79d279ad226c OMB M-13-13], Open Data Policy: Managing Information as an Asset, which requires agencies to use open, machine-readable, data format standards
* [http://ambur.net/CaponeConsultancyMethod.pdf Driving a Stake in the Heart of the Capone Consultancy Method of Records Management: Best Practices for Correcting Non-Records Non-Policy Nonsense], March 9, 2015
* The U.S. Code, which includes [http://uscode.house.gov/search.xhtml?searchString=machine-readable&pageNumber=1&itemsPerPage=100&sortField=CODE_ORDER&action=search&q=bWFjaGluZS1yZWFkYWJsZQ%3D%3D%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7C%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%7Cfalse%7C%5B%3A%3A%3A%3A%3A%3A%3A%3Afalse%3A%5D%7C%5B%3A%5D 51 references] to the term "machine-readable" as of September 10, 2016

[[Category:Data management]]
[[Category:Records management]]
<=====doc_Id=====>:356
<=====title=====>:
Big memory
<=====text=====>:
'''Big-memory''' is a term used to describe server workloads which need to run on machines with a large amount of RAM ([[Random-access memory]]) memory. Some example workloads are databases, in-memory caches, and graph analytics.<ref>{{cite web |url=http://research.cs.wisc.edu/multifacet/papers/isca13_direct_segment.pdf|title=Efficient Virtual Memory for Big Memory Servers|accessdate=2016-09-24 }}</ref>
Or, more generally, [[Data Science]] and [[Big data]].

Some database systems are designed to run mostly in memory, rarely if ever retrieving data from disk or flash memory. See a [[List of in-memory databases]].

The performance of big memory systems depends on how the CPU's or CPU cores access the memory, via a conventional [[Memory controller]] or via NUMA ( [[Non-uniform memory access]] ). Performance also depends on the size and design of the [[CPU cache]].

Performance also depends on OS design. The "Huge pages" feature in Linux can improve the efficiency of [[Virtual Memory]].<ref>{{cite web |url=http://lwn.net/Articles/374424/ |title=Huge pages part 1 (Introduction)  |accessdate=2016-09-24 }}</ref> The new "Transparent huge pages" feature in Linux can offer better performance for some big-memory workloads.<ref>{{cite web |url=http://lwn.net/Articles/423584/ |title=Transparent huge pages in 2.6.38 |accessdate=2016-09-24 }}</ref> The "Large-Page Support" in Microsoft Windows enables server applications to establish large-page memory regions which are typically three orders of magnitude larger than the native page size.<ref>{{cite web |url=https://msdn.microsoft.com/en-us/library/windows/desktop/aa366720(v=vs.85).aspx|title=Large-Page Support |accessdate=2016-09-24 }}</ref>

==References==
{{reflist}}


{{database-stub}}
[[Category:Big data| ]]
[[Category:Data management]]
[[Category:Distributed computing problems]]
[[Category:Technology forecasting]]
[[Category:Transaction processing]]
<=====doc_Id=====>:359
<=====title=====>:
Wiping
<=====text=====>:
{{About|the broadcasting practice||Wipe (disambiguation)}}
{{Multiple issues|
{{refimprove|date=February 2007}}
{{original research|date=September 2009}}
{{globalize|date=September 2009}}
}}

'''Wiping''', also known as '''junking''', is a colloquial term for action taken by [[radio]] and [[television]] production and broadcasting companies, in which old [[audiotape]]s, [[videotape]]s, and [[telerecording]]s ([[kinescope]]s), are [[List of lost television broadcasts|erased, reused, or destroyed]]. Although the practice was once very common, especially in the 1960s and 1970s, wiping is now practiced much less frequently. Older video and audio formats took up much more storage space than modern digital video or audio files, making their retention more costly, thus increasing the incentive of discarding existing broadcast material to recover storage space for newer programmes.

The advent of domestic audiovisual playback technology (e.g., videocassette and [[DVD]]) has made wiping less beneficial, with broadcasters and production houses realizing both the economic and cultural value of keeping archived material for both rebroadcast and potential profits through release on [[home video]].

==Australia==
Australian broadcasters did not gain access to videotape-recording technology until the early 1960s, and as a result nearly all programmes prior to that were broadcast live-to-air. Very little programming survives from the earliest years of Australian TV (1956–1960), as [[kinescope]] recording to film was expensive and most of what was recorded in this way has since been lost or destroyed. Some early programmes have survived, however; for example, ATN-7, a Sydney station, prerecorded (via kinescopes) some of their 1950s output such as ''[[Autumn Affair]]'' (1958–1959), ''[[The Pressure Pak Show]]'' (1957–1958) and ''[[Leave It to the Girls (Australian TV series)|Leave it to the Girls]]'' (1957–1958); some of these kinescopes have survived and are now held by the [[National Film and Sound Archive]],<ref>{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=autumn%20affair%20Media%3A%22TELEVISION%22;querytype=;resCount=200|title = NSFA, Autumn Affair}}</ref><ref>{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=;group=;groupequals=;page=0;parentid=;query=leave%20it%20to%20the%20girls%20Media%3A%22TELEVISION%22;querytype=;resCount=10|title = NSFA, Leave it to the Girls}}</ref><ref>{{cite web| url = http://colsearch.nfsa.gov.au/nfsa/search/summary/summary.w3p;adv=yes;group=;groupequals=;page=0;parentid=;query=pressure%20pak%20show%20Years%3A%3E%3D1957%20Years%3A%3C%3D1958%20Media%3A%22TELEVISION%22;querytype=;resCount=20|title = NSFA, Pressure Pak Show°}}</ref> with soap opera ''Autumn Affair'' surviving near-intact, likely one of the earliest Australian series for which this is the case.

===ABC===

The [[Australian Broadcasting Corporation]] (ABC) erased much of its early output. Much of the videotaped ABC programme material from the 1960s and early 1970s was erased as part of an economy policy instituted in the late 1970s in which old programme tapes were surrendered for bulk erasure and reuse. This policy particularly targeted older programmes recorded in black-and-white, leading to the loss of many recordings made before 1975, when Australian television converted to colour. The ABC continued erasing older television output into the early 1980s.

Programmes known to have been lost include most studio segments from the 1960s current affairs shows ''[[This Day Tonight]]'' and ''Monday Conference'', hundreds of episodes of the long-running rural serial ''[[Bellbird (TV series)|Bellbird]]'', all but a handful of episodes of the early-1970s drama series ''[[Certain Women (television series)|Certain Women]]'', an early-1970s miniseries of dramatizations based on [[Norman Lindsay]]'s novels, and nearly all of the first 18 months of the weekly pop-music show ''[[Countdown (Australian TV series)|Countdown]]''.

===Network Ten===

Many episodes of popular Australian commercial TV series are also lost. In the 1970s, [[Network Ten]] had an official policy to reuse tapes; hence, many tapes of ''[[Young Talent Time]]'' and ''[[Number 96 (TV series)|Number 96]]'' were wiped. To this day, Network Ten still only keeps some of its programming.{{Citation needed|date=November 2008}} Other notable losses from the Ten archive include hundreds of episodes of the Melbourne-based pop music shows commissioned and broadcast by ATV-0 Melbourne in the 1960s and early 1970s—''[[The Go!! Show]]'' (1964–1967), ''Kommotion'' (1964–1967), ''Uptight'' (1968–70), and the ''Happening 70s'' series (1970–1972).

===Nine Network===

The [[Nine Network]] discarded copies of some of their programs, including the popular [[GTV-9]] series ''[[In Melbourne Tonight]]'' hosted by [[Graham Kennedy]]. Though it ran five nights a week from 1957 to 1970, fewer than 100 episodes are known to survive, and many of the surviving episodes are edited prints made for rebroadcast across Australia. Early episodes of ''[[Hey Hey It's Saturday]]'' do not exist because the programme was broadcast live and did not begin videotape recordings until a number of years later.

==Brazil==
From 1968–1969, [[Rede Tupi|TV Tupi]] produced new episodes of the soap opera ''[[Beto Rockfeller]]'' by recording over previous episodes; as a result, few episodes survive. After the closure of TV Tupi in 1980 the 536 tapes at its São Paulo studios were simply left to deteriorate until they were recovered in 1985 and subsequently restored by [[TV Cultura]] in 1989. Only two TV Tupi O&Os are known to have any preserved videotapes; TV Itacolomi's archives are now owned by the unrelated [[TV Alterosa]], affiliated with [[Sistema Brasileiro de Televisão|SBT]], whereas the few remaining tapes belonging to TV Piratini are stored privately in a museum in Porto Alegre, albeit in a deteriorated state.

[[Rede Record]] also lost much footage from the 1960s due to wiping, fires, and deterioration; most of the [[Música popular brasileira|MPB]] music festivals no longer exist, and the sitcom ''[[:pt:Família Trapo|Família Trapo]]'' has only one surviving episode, featuring [[Pelé]]. Until 1997 Rede Record had no policy on archiving videotapes, since then at least 600 videotapes that were previously believed to be lost have been recovered.

[[Rede Globo]] lost the first 35 broadcasts of both ''[[Fantástico]]'' and ''[[Jornal Nacional]]'', in addition to many segments of their other soap operas, as a result of wiping, and also due to three fires that occurred in 1969, 1971 and 1976, where an estimated 920 to 1500 tapes were destroyed.

Most of [[Rede Excelsior]]'s output was damaged in a fire in 1969; however, in the late 1990s about 100 tapes of Rede Excelsior programming were discovered and these tapes were subsequently donated to the [[Cinemateca Brasileira]] in 2001.

==Canada==
The [[Canadian Broadcasting Corporation]] never practiced wiping, and maintains a complete archive of all programming that was recorded.<ref>{{cite web|url=http://archives.cbc.ca/info/archives/archives_en_04.asp?IDLan=1|title=CBC Archives|date=10 April 2013|publisher=}}</ref>

The [[CTV Television Network]] has admitted to wiping many programmes during the 1970s. Because of [[Canadian content]] requirements, the need for Canadian-produced programming led to more preservation of the shows they produced, and even very poorly received programmes (such as the infamous ''[[The Trouble with Tracy]]'') were saved and rerun for several years after their cancellation. Furthermore, Canadian rebroadcasts have been a source of some broadcasts that are otherwise lost in the United States and the United Kingdom.

==Japan==
Some TV stations in Japan practiced wiping, this example included the [[Doraemon (1973 anime)|first anime]] adaption of ''[[Doraemon]]''.

==Philippines==
Episodes from 1979 to 1982 of the longest running noontime show, ''[[Eat Bulaga!]]'', have been lost.

Another example of the wiping of TV archives in the Philippines was when martial law was declared, soldiers raided the [[ABS-CBN|ABS-CBN Broadcast Center]] and placed it under military control. As a result, ABS-CBN's pre-martial law archives, dating from 1953 to 1972, were lost.

==Mexico==
Due to its multiple studio facilities, namely its Chapultepec and [[Televisa San Angel|San Angel studios]], [[Televisa]] preserved most of its scripted series for broadcast years after the preserved programs had ended their original runs.  Some Televisa programs, however, were lost not due to wiping, but due to the [[1985 Mexico City earthquake]] that destroyed part of the network's archive. However, smaller channels, such as [[XEIPN-TV]] and [[XHDF-TV]], did not began to preserve their recorded broadcasts until the early 1980s. [[Monterrey]]'s [[Multimedios Televisión]] keeps most of its programming, though some special historical programming dealing with [[XHAW-TDT|its flagship station]]'s history clearly shows that some footage has been either donated by viewers recorded from its original broadcast, or uses footage of its programming recorded by fans and uploaded to [[YouTube]].

==United Kingdom==

===BBC===
The [[BBC]], the United Kingdom's first [[public service broadcaster]], had no policy on archiving until 1978.<ref>{{cite web|url=http://cuttingsarchive.org.uk/missing/mis_overv.htm |title=Cuttings Archive: The Missing Episodes - Overview |publisher=Cuttings Archive |accessdate=2008-11-23 |deadurl=yes |archiveurl=https://web.archive.org/web/20080725012437/http://www.cuttingsarchive.org.uk/missing/mis_overv.htm |archivedate=July 25, 2008 }}</ref> Much of the corporation's output between the 1930s and 1980s has been lost. Rationales behind this policy include:

====Technological====
The BBC's [[television]] service dates back to 1936 and was originally a nearly live-only medium. The hours of transmission were very limited and the bulk of the programming was transmitted either live from the studio, or from [[Outside broadcasting|outside broadcast (OB)]] units; film was a minor contributor to the output. When the first television broadcasts were made, there were two competing systems in use. The EMI electronic system (using [[405 lines]]) competed with the Baird 240-line [[mechanical television]] system. Baird adopted an intermediate film technique where the live material was filmed using a standard film camera mounted on a large cabinet which contained a rapid processing unit and an early [[flying spot scanner]] to produce the video output for transmission. The pioneer broadcasts were not, however, preserved on this intermediate film as the nitrate (celluloid) stock was scanned while still wet from the fixer bath and never washed to remove the fixer chemicals. Consequently, the film decomposed very soon after transmission; nothing is known to have survived.
No studio or OB programmes from 1936 to 1939 or 1946 to 1947 have survived because there was no means of preserving them. Historical 'firsts' from this era; the world's earliest television crime drama ''[[Telecrime]]'' (1938–39 and 1946) or ''[[Pinwright's Progress]]'' (1946–47, the world's first regular [[situation comedy]]), only remain visually as a handful of still photographs.

The earliest recording method for television was [[Kinescope|telerecording]], which involved recording the image from a special television monitor onto film with a modified film camera. Early examples made by this method include the first two episodes of ''[[The Quatermass Experiment]]'' (1953), transmitted live while simultaneously telerecorded. The visual quality of the second episode's recording was considered so poor—a fly entered the gap between the camera and monitor at one point—that the remainder of the series was not recorded.

Although [[Quadruplex videotape]] recording technology was utilised in the UK from 1958, this system was expensive and complex; recorded programmes were often erased after broadcast. The vast majority of live programmes were never recorded at all. Videotape was not initially thought to be a permanent archivable medium – its high cost and the potential reuse of the tapes led to the transfer of programme material to film via [[Kinescope|telerecording]] whenever sales of overseas screening rights were possible or preservation deemed worthwhile. The recycling of videotapes, coupled with savings made on the storage of the bulky 2" tapes,<ref>By 1973, about 20,000 hours of recorded material was stored on videotape at the BBC weighing about 400,000 lbs in total. See ''BBC Engineering'', No.95, September 1973, London: BBC Publications, p.3</ref> enabled the BBC to keep costs down.

====Cultural====
Drama and entertainment output was studio-based and followed the tradition of live [[theatre]]. Conventional filmmaking was only gradually introduced from the 1960s. ''The Sunday Night Play'' (a major event in the 1950s) was performed live in the studio. On Thursday, because telerecording was of insufficient broadcast quality, another live performance followed, the artists returning to perform the play again.

Today, most programmes are pre-recorded and it is relatively inexpensive to preserve programming for posterity; even so, the [[BBC Charter]] makes no mention of any obligation to retain all of them.

====Rights====
All television programmes have copyright and other rights issues associated with them. For some genres of programmes—such as drama and entertainment—the actors, writers, and musicians involved in a production all have underlying rights. In the past, these rights were defended rigorously—permission could even be denied by a contributor for the repeat or re-use of a programme. Talent [[Trade union|unions]] were highly suspicious of the threat to new work if programmes were repeated; indeed, before 1955 [[Equity (trade union)|Equity]] insisted that any telerecording made (of a repeat performance) could only "be viewed privately" on BBC premises and not transmitted.

====Colour television====
The introduction of colour television in the United Kingdom from 1967 meant that broadcasters felt there was even less value in retaining monochrome recordings. Such tapes could not be re-used for colour production, so they were disposed of to create space for the new colour tapes in the archives, which were quickly filling up. The increased cost of colour [[2 inch Quadruplex videotape]]—approximately £1000 per tape at today's prices—meant that companies still often re-used the tapes for efficiency. Negative attitudes to a programme's value also persisted. For these reasons, many programmes survive only as monochrome film recordings, if at all.

Some colour productions were telerecorded onto monochrome film for export to countries which did not yet have colour television. In some cases, early colour programmes only survive in this form.

====Significant wiped programmes====
High-profile examples of programme losses include many early episodes of [[Doctor Who]] (97), ''[[The Wednesday Play]]'', most of the seminal comedy series ''[[Not Only But Also]]'', all of the 1950s televised [[Francis Durbridge]] serials (further, the first two serials were never recorded), the vast majority of the BBC's [[Apollo 11]] [[British television Apollo 11 coverage|Moon landing studio coverage]], all but one of the 39 episodes of ''[[The First Lady (TV series)|The First Lady]]'',<ref>{{cite web|url=http://www.lostshows.com/default.aspx?programme=ec5863f7-6843-4552-acda-07e19396fdae|title=Lost UK TV Shows Search Engine|author=Simon Coward, Invisible Technology Ltd|publisher=}}</ref> and all 147 episodes of the [[soap opera]] ''[[United!]]''. There are many gaps in many long-running BBC series (''[[Dixon of Dock Green]]'', ''[[Hancock's Half Hour]]'', ''[[Sykes]]'', ''[[Out of the Unknown]]'', and ''[[Z-Cars]]''). [[The Beatles]]' only live appearance on ''[[Top Of The Pops]]'' in 1966, performing the single "[[Paperback Writer]]" is believed to have been wiped clean in a clear-out in the 1970s.

The first acting appearance of musician [[Bob Dylan]], in a 1963 play entitled ''[[The Madhouse on Castle Street]]'', was erased in 1968.<ref>{{cite news|url = http://www.offthetelly.co.uk/reviews/2005/arenadylan.htm|title = Arena: Dylan in the Madhouse|date = 2005-09-28|last = Worthington|first = TJ|work = OFF THE TELLY}}</ref>

There is lost material in all genres &mdash; as late as 1993, a large number of videotaped children's programmes from the 1970s and 1980s were irretrievably wiped by Adam Lee of the [[BBC]] [[archives]] on the assumption that they were of "no use", without consulting the BBC children's department itself.<ref>{{cite news |url=http://www.offthetelly.co.uk/oldott/www.offthetelly.co.uk/index8e01.html?page_id=781 |title=Of Finger Mice and Mr. Men - The Story of Watch with Mother Part Eleven: Andy is Waving Goodbye |last=Worthington |first=TJ |date = November 2006|work=Off the Telly}}</ref>

====Other lost material====
Virtually the entire runs of the corporation's pre-1970s soap operas have been lost. In the 1950s and 1960s, the [[BBC]] soap operas ''[[The Appleyards]]'', ''[[The Grove Family]]'', ''[[Compact (soap opera)|Compact]]'', ''[[The Newcomers (TV series)|The Newcomers]]'', ''[[199 Park Lane]]'', and ''[[United!]]'' produced approximately 1200 episodes altogether.

There are no episodes of either  ''United!'' or ''199 Park Lane'' in the archives, while only one episode of ''The Appleyards'', three episodes of ''The Grove Family'', and four episodes each of ''Compact'' and ''The Newcomers'' are known to exist.

Also vulnerable to the corporation's wiping policy were programmes that only lasted for one season. ''[[Abigail and Roger]]'', ''[[The Airbase]]'', ''[[As Good Cooks Go]]'', the 1960 adaptation of ''[[The Citadel (novel)|The Citadel]]'', the 1956 adaptation of ''[[David Copperfield (novel)|David Copperfield]]'', ''[[The Dark Island]]'', ''[[The Gnomes of Dulwich]]'', ''[[Hurricane]]'', ''[[For Richer...For Poorer]]'', ''[[Hereward the Wake]]'', ''The Naked Lady'', ''Night Train To Surbiton'', ''Outbreak of Murder'', ''Where do I Sit?'', and ''Witch Hunt'' have all been wiped with no footage surviving while four out of seven episodes of the paranormal anthology series ''[[Dead of Night (TV series)|Dead of Night]]'' were wiped.

An edition of ''[[Hugh and I]]'' ("Chinese Crackers"), starring [[Hugh Lloyd]], [[Terry Scott]], [[John Le Mesurier]] and [[David Jason]] was located by [[Kaleidoscope Publishing]] in 2010 in the archives of [[UCLA]], and brought to general public attention in February 2011.

Early episodes of the pop music-chart show ''[[Top of the Pops]]'' were wiped or never recorded while they were being transmitted live, including the only in-studio appearance by [[The Beatles]]. Clips of [[the Beatles]] miming "[[Can't Buy Me Love]]" and "[[You Can't Do That]]" on an episode from 25 March 1964 were found online by missing episode hunter Ray Langstone in 2015. The last lost edition dates from 8 September 1977. There are only four complete ''TOTP'' episodes surviving from the 1960s, while many otherwise-missing episodes survive only as fragments. Only two episodes still exist of ''[[The Sandie Shaw Supplement]]'' (a music-variety show hosted by the singer), recorded in 1967.

====Finding missing BBC programmes====
Since the establishment of an archival policy for television in 1978, BBC archivists and others over the years have used various contacts in the UK and abroad to try to track down missing programmes. For example, all [[BBC Worldwide]] customers—broadcasters around the world—who had bought programmes from the corporation were contacted to see if they still had copies which could be returned; ''Doctor Who'' is a prime example of how this method recovered episodes that the corporation did not hold itself. At the turn of the 21st century, the BBC established its [[BBC Archives#Archive Treasure Hunt|Archive Treasure Hunt]], a public appeal to recover lost productions, which has had some successes.<ref name="BBCTH">{{cite web|url=http://www.bbc.co.uk/cult/treasurehunt/about/listoffinds.shtml |title=BBC Online - Cult - Treasure Hunt - List of Finds |publisher=Bbc.co.uk |date= |accessdate=30 July 2010}}</ref>

The BBC also has close contacts with the [[National Film and Television Archive]], which is part of the [[British Film Institute]] and its "Missing Believed Wiped" event which was first held in 1993 and is part of a campaign to locate lost items from British television's past. There is also a network of collectors who, if they find any programmes missing from the BBC archives, will contact the corporation with information—or sometimes even the actual footage. Some examples of programmes recovered for the archives are ''[[Doctor Who]]'', ''[[Steptoe and Son]]'', ''[[Dad's Army]]'', ''[[Letter from America]]'',<ref>[http://www.bbc.co.uk/informationandarchives/archivenews/2014/letters_from_america_rediscovered.html Letter from America rediscovered], bbc.co.uk, 28 March 2014</ref> ''[[The Likely Lads]]'', and ''[[Play for Today]]''.

For many years the [[television pilot|pilot]] episode of ''[[Are You Being Served?]]'' survived only in black and white, appearing in this form on the 2003 DVD release of the show. In 2009, a colour version was [[colour recovery|reconstructed]] when it was realised that the black and white film reel had actually recorded sufficient colour information as a [[dot crawl]] pattern to allow [[colour recovery]].

===ITV===
The BBC was not alone in this practice – the commercial companies that formed its main rival [[ITV (TV network)|ITV]] also wiped videotapes and destroyed [[telerecording]]s, leaving gaps in their archive holdings. The state of the archives varies greatly between the different companies; [[Granada Television]] holds a large number of its older black-and-white programmes, the company having an unofficial policy of retaining as much of its broadcast material (albeit by telerecording) as possible despite financial hardship in its early years. This includes the entirety of the soap opera ''[[Coronation Street]]'' which is now held at the [[Yorkshire Television]] archive, which itself possesses largely intact archives, although some early colour shows from the late 1960s and the early 1970s such as the entire output of the drama ''Castle Haven'', the first two series of ''[[Sez Les]]'' and the children's variety show ''[[Junior Showtime]]'' are missing and believed wiped. The former ITV company [[Thames Television]] also has a significant library.

These cases tend to be the exception, however; the former nature of the ITV network, in which private independent companies were awarded licences to serve geographical areas for a set period of time, meant that when companies lost their licences their archives were often sold to third parties and became fragmented—and/or risked being destroyed, as ownership and [[copyright]] remained with the production companies rather than with the network. The archive of networked programmes made by [[Southern Television]], for example, is now owned by the otherwise-unconnected Australian media company [[Southern Star Group]] but Southern's regional output is in the hands of [[ITV plc]]. The few surviving tapes of [[Associated-Rediffusion]] belong to many different organisations as the majority of Associated-Rediffusion's tapes were recorded in [[monochrome]] and therefore deemed of no use upon the arrival of colour broadcasting; as such they were disposed of by London successor [[Thames Television]]), although in recent years there have been occasional discoveries such as a 1959 episode of ''[[Double Your Money]]'' and the remaining missing episode of ''[[Around the World with Orson Welles]]'', found by Ray Langstone in 2011. Many master tapes belonging to [[Associated Television|ATV]] have since deteriorated due to bad storage and are unsuitable for broadcasting. In particular, the ATV version of the popular soap ''[[Crossroads (soap opera)|Crossroads]]'' is missing 2,850 episodes of its original 3,555. Also often largely lost are quiz shows; few editions exist of the 1970s version of ''Celebrity Squares'' with [[Bob Monkhouse]], or Southern's children's quiz ''[[Runaround (game show)|Runaround]]''.{{cn|date=December 2016}}

Further, responsibility for archive preservation was left to individual companies. For example, ITV has no record of its live coverage of [[Apollo 11|the 1969 Moon landings]] after the station responsible for providing the coverage, [[London Weekend Television]], wiped the tapes. Of the 96 [[United Kingdom|British]] inserts to the 1980s franchised [[United Kingdom|Anglo]]-[[United States|American]]-[[Canada|Canadian]] children's show ''[[Fraggle Rock]]'', only 12 are known to exist as the library of the British producer ([[Television South|TVS]]) has been sold and subsequently split up.

In recent years, the trend of preserving material has started to change. The archives of [[Westward Television]] and [[Television South West]] are now held in trust for the public as the South West Film and Television Archive, whilst changes in legislation mean that ITV companies which lose their franchises must donate archives to the [[British Film Institute]]. However, the change of ITV from a federal structure to one centralised company means that changes of regional companies in the future seems highly unlikely.

Most material from the 1960s also only survive as telerecordings. Some early episodes are also believed to be damaged or in poor quality, whereas much of the output of other broadcasters – such as many early episodes of ''[[The Avengers (TV series)|The Avengers]]'' which were shot in the electronic studio rather than on film, produced by [[Associated British Corporation]] – have been destroyed.

No copies of ''The Adventures of [[Francie and Josie|Francie & Josie]]'' exist, as most of [[Scottish Television]]'s early shows were destroyed in a fire in late 1969 (although some sources state 1973). ''The Adventures Of Francie & Josie'' was made from 1961 to 1965 by STV.

===Recovery of missing programmes===
Since the BBC library was first audited in 1978, missing programmes or extracts on either film or tape are often found in unexpected places. An appeal to broadcasters in other countries who had shown missing programmes (notably [[Australia]], [[New Zealand]], [[Canada]], and [[Africa]]n nations such as [[Nigeria]]) produced "missing" episodes from the archives of those television companies. Episodes have also been returned to broadcasters by private film collectors who had acquired 16mm film copies from various sources.{{cn|date=December 2016}}
* Two Series 1 episodes of ''[[The Avengers (TV series)|The Avengers]]'' (an [[Associated British Corporation]] production) which were thought to be missing were recovered from the [[UCLA]] Film & Television Archive in the United States.
* It emerged in September 2010 that more than 60 recordings of BBC and ITV drama productions originally sent for broadcast in the United States by the [[Public Broadcasting Service|PBS]] station [[WNET]] (which serves [[New York City]] and [[New Jersey]]) had been found at the [[Library of Congress]].<ref>Vanessa Thorpe [https://www.theguardian.com/tv-and-radio/2010/sep/12/lost-tapes-classic-british-television "Lost tapes of classic British television found in the US"], ''The Observer'', 15 September 2010</ref>
* The BBC [[sitcom]] ''[[Steptoe and Son]]'' is completely intact, although approximately half of the colour episodes only exist in monochrome; this was after copies of episodes thought to be lost were recovered in the late 1990s from early non-broadcast standard video recordings made for writers [[Ray Galton]] and [[Alan Simpson (scriptwriter)|Alan Simpson]] by BBC technicians.
* A few audio recordings of ''[[Til Death Us Do Part]]'' have been recovered, as well as an extract of the pilot and two episodes from series three.

Copies of several compilations from the British 1960s comedy ''[[At Last The 1948 Show]]'', held by many to be a forerunner of ''[[Monty Python's Flying Circus]]'', were discovered in the archives of the Swedish broadcaster [[Sveriges Television|SVT]], to whom the producers [[Associated Rediffusion|Rediffusion London]] had sold them upon the companies' loss of its broadcasting licence. The master tapes, along with much of Rediffusion's programming, were wiped or disposed of by London successor Thames Television. Their recovery enabled the reconstruction of otherwise missing original editions of the programme, meaning most of the series exists in visual form.

Off-air home audio recordings of various television programmes have also been recovered, at least preserving the soundtracks to otherwise missing shows, and some of these (particularly from ''[[Doctor Who]]'') have been released on [[CD]] by the BBC following restoration and the addition of narration to describe purely visual elements. [[Tele-snaps]], a commercial service of off-screen shots of programmes often purchased by [[actor]]s and [[television director]]s to keep a record of their work in the days before [[videocassette recorder]]s, have also been recovered for many lost programmes.

===Preservation of the current archive===
Advances in technology have resulted in old programmes being transferred to new digital media, where they can be restored or (if they are damaged or otherwise cannot be restored) kept from decaying further. In the United Kingdom, the archives of both the BBC and those available of ITV, along with other channels, are being switched from cumbersome [[Quadruplex videotape|2-inch quadruplex videotape]] to digital format. This is an extensive and expensive process and one that will take many years to complete.

Live broadcasts in Britain are still not necessarily kept, and wiping of material has not ceased. According to writer and broadcaster [[Matthew Sweet (writer)|Matthew Sweet]], there are "big gaps in the record of children's television of the Nineties."<ref>Matthew Sweet [http://www.telegraph.co.uk/culture/tvandradio/10492487/Searching-for-televisions-missing-gems-Doctor-Who-Woody-Allen-Ridley-Scott-and-Dennis-Potter.html "Searching for television's missing gems: Doctor Who, Woody Allen, Ridley Scott and Dennis Potter"], telegraph.co.uk, 4 December 2013</ref>

==United States==
In the [[United States]], the major broadcast networks also engaged in the practice of wiping recordings until the late 1970s. Many episodes were erased, especially daytime and late-night programming such as daytime [[soap opera]]s and [[game show]]s. The daytime shows, almost all of them having been taped, were erased because it was believed at the time that nobody wanted to see them after their first broadcast. The success of [[cable television]] networks devoted to reruns of these genres proved that this was not the case, as the large number of episodes that were required for a daily program made even a short-run game show an ideal candidate for [[broadcast syndication|syndication]]. By this time, however, the damage had already been done.

===Preservation by institutions such as museums===

Some museums and other [[cultural institution]]s such as the Paley Center for Media have taken steps to discover and preserve (see, e.g., "[[Paley Center for Media#Archives]]") old recordings previously thought to have been wiped or discarded, lost, or misfiled.

===Hosting sequences===
Hosting sequences on videotape, nearly always featuring celebrities, were sometimes made for telecasts of family films, notably for the first nine telecasts of MGM's ''[[The Wizard of Oz (1939 film)|The Wizard of Oz]]''. It is not known if those made for ''Oz'' survived since they have not been seen since 1967. One hosting sequence from that era that does survive is the one [[Eddie Albert]]  made for the 1965 CBS telecast of ''[[The Nutcracker]]'', starring [[Edward Villella]], [[Patricia McBride]], and [[Melissa Hayden (dancer)|Melissa Hayden]]. It has even been included on the DVD release of the program.<ref>{{cite web|url=http://www.wbshop.com/product/nutcracker+the+1965+tv+sp+1000179869.do?from=Search|title=Nutcracker, The (1965 TV SP) (MOD)|work=www.WBShop.com}}</ref>

===Ernie Kovacs===
Many of [[Ernie Kovacs]]'s videotaped network programs were also wiped. During different times as comedian, writer, and performer Kovacs had programs on all four major television networks ([[American Broadcasting Company|ABC]], [[CBS]], [[DuMont Television Network|DuMont]], and [[NBC]]). After Kovacs's death, the networks wiped many programs. Kovacs's widow [[Edie Adams]] obtained as many programs and episodes as she could find, donating them to [[UCLA]]'s [[Special Collections]].

===Soap operas===
Though most soap operas transitioned from live broadcast to videotaping their shows during the 1960s, it was still common practice to wipe and reuse the tapes. This practice was due to the high cost of videotape at the time. While soap operas began routinely saving their episodes between 1976 and 1979, several soaps have saved recordings of most or all their episodes. ''[[Days of Our Lives]]''  has recordings of all its episodes; its first two episodes exist on their original master tapes, and were aired by [[SOAPnet]] in 2005. ''[[The Young and the Restless]]'',  ''[[Dark Shadows]]''  and ''[[Ryan's Hope]]'' saved most of their episodes, despite the fact that they debuted during the 1960s and 1970s, before retaining tapes became common practice. Episodes of ''[[The Doctors (soap opera)|The Doctors]]'' began to be saved no later than December 4, 1967; this is where reruns of the series began when picked up by [[Retro Television Network]] in September 2014. Episodes of other soaps broadcast during the 1950s to 1970s do exist in different forms and have been showcased in various places online.

[[Procter and Gamble]] started saving their shows around 1979. Very few pre-1979 color episodes of the Procter and Gamble-sponsored shows survive, with most extant episodes preserved as monochrome kinescopes. Exceptions include two episodes of ''[[The Guiding Light]]'' (1973 and 1977), which have been released on DVD. ''[[As the World Turns]]'' and ''[[The Edge of Night]]'' aired live until 1975, the year ''The Edge of Night'' moved to [[American Broadcasting Company|ABC]] and ''As the World Turns'' expanded from a 30-minute broadcast to one hour. Both shows began taping episodes in preparation for the move of ''The Edge of Night'' to ABC. ''The Edge of Night''<nowiki>'s</nowiki> ABC debut is believed to have survived. Overall, the number of surviving monochrome episodes recorded on kinescope outnumber color episodes for these programs.

[[Agnes Nixon]] initially produced her series ''[[One Life to Live]]'' and ''[[All My Children]]'' through her own production company, Creative Horizons, Inc., and kept a complete archive of monochrome kinescopes until ABC bought the shows from her in 1975. When the network wanted to expand ''All My Children'' from 30 minutes to a full hour in the late 1970s, Nixon agreed on the condition that the network would begin saving the episodes. ABC complied, and full hour broadcasts began on April 25, 1977. However, a fire destroyed the vast majority of the early-1970s kinescopes, leaving only a few random episodes from each season.

Virtually all episodes of ''[[General Hospital]]'', from its premiere in April 1963 through August 1970, are archived at [[UCLA]]. The [[UCLA Film & Television Archive]] holds a large number of daytime television airings that were spared from the wiping practice.  Also archived there are handfuls of episodes of each soap opera that was on the air from 1971 and 1973, including ''[[A World Apart (TV series)|A World Apart]]'', ''[[Where the Heart Is (1969 TV series)|Where the Heart Is]]'', and ''[[Return to Peyton Place (TV series)|Return to Peyton Place]]''.

===DuMont programs===
It is believed that virtually the entire archive of the [[DuMont Television Network]], covering its whole history from 1946 to 1956, was disposed of during the 1970s by a "successor" broadcaster (presumably [[Metromedia]], the holder of DuMont's assets), who dumped all of the kinescopes/videotapes into the [[East River]] to make room for other tapes (believed to be ABC's) at a New York City warehouse.<ref name="LoC">{{cite web|last = Adams|first = Edie|authorlink = Edie Adams|title = Television/Video Preservation Study: Los Angeles Public Hearing|work = National Film Preservation Board| publisher = Library of Congress|date = March 1996|url = http://www.loc.gov/film/hrng96la.html|accessdate = 2008-05-09}}</ref> Further, a large number of DuMont's kinescopes were destroyed in about 1958 for their silver content.

Of the over 20,000 shows carried by DuMont in its ten-year existence, [[List of surviving DuMont Television Network broadcasts|approximately 350 or so episodes of DuMont programming are known to exist today]], less than two percent of its total output. The remainder were either never recorded (e.g., ''[[NFL on DuMont]]'') or were dumped in the earlier purges.

===''The Tonight Show''===
{{See also|The Tonight Show Starring Johnny Carson}}
Almost all of ''[[The Tonight Show]]'' with [[Jack Paar]] and the first ten years hosted by his successor [[Johnny Carson]] were taped over by the network, with Carson's blessing, under the assumption that the broadcasts were of no real value.<ref>[http://www.washingtonpost.com/entertainment/tv/carson-on-tcm-shows-why-johnny-was-the-king/2013/07/03/3c25d1ce-e264-11e2-aef3-339619eab080_story.html Carson on TCM shows why Johnny was the king]. ''The Washington Post''. Retrieved July 9, 2013.</ref> This is part of the reason why Carson's late 1960s shows had poorer picture quality{{Citation needed|date=November 2010}} compared to his competitor [[Dick Cavett]] on [[American Broadcasting Company|ABC]]; [[NBC]] was using the ''Tonight Show'' tapes repeatedly. Another reason for their poorer quality is that many of the 1960s ''Tonight Show'' episodes only survived in the kinescope format. (Cavett's ABC shows were also taped over by his network in favor of other shows produced at ABC's studios in New York.)

===Early sporting events===
{{See also|List of World Series broadcasters|List of Super Bowl broadcasters|NFL on CBS|NFL on NBC}}

Many early sporting events, such as the [[World Series]] and the first two [[Super Bowl]]s, were also lost, though a nearly intact recording of the first Super Bowl was found in 2005.

====[[National Football League]]====
''[[Super Bowl I]]'' was aired by both [[CBS]] and [[NBC]] (the only Super Bowl to be aired by two networks), but neither network felt the need to preserve the game long-term; CBS saved the telecast for a few months and reran it as filler programming at least once before wiping it. A color videotape containing the first, second and fourth quarters of the telecast from [[WYOU]] (the CBS affiliate for [[Scranton, Pennsylvania]]) was found in 2005 and is in the process of being restored.<ref>Fybush, Scott (2011-02-07). [http://www.fybush.com/NERW/2011/110207/nerw.html Will New York Outlaw Pirate Radio?]. ''NorthEast Radio Watch''. Retrieved 2011-02-07.</ref> On January 15, 2016, the [[NFL Network]] reaired the first Super Bowl, featuring audio from [[NBC Radio]] and most of the TV network broadcast and newly discovered [[NFL Films]] footage of the game. ''[[Super Bowl II]]'' was aired exclusively by CBS and was believed to have been erased, but it was later found that the entire telecast fully exists and rests in the vaults of [[NFL Films]].<ref name="foo">{{cite web
 | url        = http://www.marketwatch.com/story/the-hunt-for-tvs-lost-baseball-treasures-2010-10-27?pagenumber=2
 | title      = The hunt for TV’s lost baseball treasures
 | author     = David B. Wilkerson
 | date       = October 27, 2010
 | work       =
 | publisher  = [[Wall Street Journal]] Marketwatch
 | accessdate = November 26, 2012
}}</ref>  Though the telecast of ''[[Super Bowl III]]'' exists in full color, only half of the ''[[Super Bowl IV]]'' broadcast does (the rest was preserved by Canadian television in black-and-white). The first three quarters of ''[[Super Bowl V]]'' broadcast by NBC Los Angeles affiliate [[KNBC]] exist, but the fourth quarter is missing, though the [[Mike Curtis (American football)|Mike Curtis]] interception and [[Jim O'Brien (American football)|Jim O'Brien]] game-winning field goal were recovered via news highlights from [[CBC Television|CBC]]. ''[[Super Bowl VI]]'' also exists in its entirety. It was not until ''[[Super Bowl VII]]'' that a continuous archive was established.<ref name="foo" />

Similarly, all of the telecasts of the [[NFL Championship Game]]s prior to the Super Bowl are believed to have been lost, with all surviving footage of those games coming from separately produced film. The status of most regular season and playoff games from the early years of television up to the immediate years following the 1970 [[AFL–NFL merger]] are also unknown. Among the footage that has survived include at least some of NBC's coverage from the 1972 AFC Divisional Playoff game between the [[1972 Pittsburgh Steelers season|Pittsburgh Steelers]] and [[1972 Oakland Raiders season|Oakland Raiders]] that featured the [[Immaculate Reception]], as well as the inaugural telecast of ''[[Monday Night Football]]'' between the [[1970 Cleveland Browns season|Cleveland Browns]] and the [[1970 New York Jets season|New York Jets]], though several ''Monday Night Football'' games in the ensuing seasons were lost. A [[1974 NFL season|1974 game]] that featured [[John Lennon]] being interviewed by [[Howard Cosell]] in the booth only survived due to a [[home video]] recording of the game; the game itself was wiped by ABC. CBS kept coverage of a 1978 [[Eagles–Giants rivalry|matchup]] between the [[1978 New York Giants season|New York Giants]] and [[1978 Philadelphia Eagles season|Philadelphia Eagles]] that would feature the now-infamous [[Miracle at the Meadowlands]], although the existence of many 1978 games on CBS by private collectors shows that the networks by that point started keeping recordings of regular season games. There are rare exceptions of CBS games from [[1977 NFL season|1977]] back, but by [[1978 NFL season|1978]] the library of most teams is almost fully complete. NBC is another story.

The NFL had its own filmmakers, [[NFL Films]], filming the game with its own equipment. Thus, preserving the telecasts on tape was not seen as a priority by the networks when another source was available – though the sportscasters' play-by-play comments, as a result, were lost.

====World Series telecasts====
All telecasts of World Series games starting in [[1975 World Series|1975]] ([[1975 Cincinnati Reds season|Reds]]–[[1975 Boston Red Sox season|Red Sox]]) are known to exist in full.<ref name="Surviving World Series Telecasts">{{cite web|url=http://www.dbsforums.com/vbulletin/showthread.php?t=78232|title=www.dbsforums.com|publisher=}}</ref> Follows is the known footage of World Series telecasts prior to 1975:
* [[1952 World Series|1952]] ([[1952 New York Yankees season|Yankees]]–[[1952 Brooklyn Dodgers season|Dodgers]]) – Games 6–7 are intact.
* [[1955 World Series|1955]] ([[1955 New York Yankees season|Yankees]]–[[1955 Brooklyn Dodgers season|Dodgers]]) – Only the first half of Game 5 is known to exist.
* [[1956 World Series|1956]] ([[1956 New York Yankees season|Yankees]]–[[1956 Brooklyn Dodgers season|Dodgers]]) – Only the last three innings of Game 2 are known to exist. Game 3 is intact minus the second and third inning. Game 5 ([[Don Larsen]]'s [[perfect game]]) is intact minus the first inning, and was aired on January 1, 2009 during the [[MLB Network]]'s first broadcast day.
* [[1957 World Series|1957]] ([[1957 New York Yankees season|Yankees]]–[[1957 Milwaukee Braves season|Braves]]) – Game 1 is intact by way of a print from the United States [[American Forces Network|Armed Forces Radio and Television Service]].<ref>https://www.youtube.com/watch?v=72Eo0pIka4o</ref> Game 3 is intact, minus a snip of [[Tony Kubek]]'s second home run in the top 7th inning. Games 6 (most of the first six innings) and 7 reportedly exist as well.
* [[1960 World Series|1960]] ([[1960 New York Yankees season|Yankees]]–[[1960 Pittsburgh Pirates season|Pirates]]) – Game 7 (with [[Bill Mazeroski]]'s series-clinching walk-off home run) was found intact on [[kinescope]] in December 2009 in the wine cellar of Pirates' part-owner [[Bing Crosby]], who had the game recorded at his own expense. MLB Network aired it in December 2010.<ref>{{cite news|url = http://www.nytimes.com/2010/09/24/sports/baseball/24crosby.html?_r=1&src=mv|title = In Bing Crosby's Wine Cellar, Vintage Baseball|first = Richard|last = Sandomir|authorlink = Richard Sandomir|publisher = ''[[The New York Times]]''|date = 2010-09-23|accessdate = 2010-09-25}}</ref>
* [[1961 World Series|1961]] ([[1961 New York Yankees season|Yankees]]–[[1961 Cincinnati Reds season|Reds]]) – Half-hour segments of Games 3 (the first two innings), 4 (the 4th and 5th innings), and 5 (open and top of the 1st inning) are known to exist.
* [[1963 World Series|1963]] ([[1963 New York Yankees season|Yankees]]–[[1963 Los Angeles Dodgers season|Dodgers]]) – Game 3 is intact.
* [[1965 World Series|1965]] ([[1965 Minnesota Twins season|Twins]]–[[1965 Los Angeles Dodgers season|Dodgers]]) – All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].
* [[1968 World Series|1968]] ([[1968 Detroit Tigers season|Tigers]]–[[1968 St. Louis Cardinals season|Cardinals]]) – All seven games were preserved by the [[CBC Television|CBC]] on [[kinescope]].
** It is likely the 1965 and 1968 Series were preserved by the CBC due to the Twins' and Tigers' proximity to Canada; the country would not get its own MLB team until the [[Montreal Expos]] began play in 1969.
* [[1969 World Series|1969]] ([[1969 Baltimore Orioles season|Orioles]]–[[1969 New York Mets season|Mets]]) – Games 1–2 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Games 3–5 exist on their original color videotape from "truck feeds".
* [[1970 World Series|1970]] ([[1970 Baltimore Orioles season|Orioles]]–[[1970 Cincinnati Reds season|Reds]]) – Games 1–4 were preserved by the [[CBC Television|CBC]] on [[kinescope]], while Game 5 exists on its original color videotape from the "truck feed".
* [[1971 World Series|1971]] ([[1971 Baltimore Orioles season|Orioles]]–[[1971 Pittsburgh Pirates season|Pirates]]) – Games 1–2 and 6–7 are intact, while Games 3–5 only partially exist and Game 4 (the first World Series night game) is near-complete.
* [[1972 World Series|1972]] ([[1972 Oakland Athletics season|A's]]–[[1972 Cincinnati Reds season|Reds]]) – Game 4 is intact, along with nearly all of Game 5 and a fair chunk of Game 2. Fragments exist for Games 1, 3, and 6, while Game 7 is missing.
* [[1973 World Series|1973]] ([[1972 Oakland Athletics season|A's]]–[[1972 New York Mets season|Mets]]) – Game 1 is intact, Game 2 is missing the last inning and a half (including both [[Mike Andrews]] plays), Game 3 is complete minus the last inning, Game 4 is intact from the pregame show to the top of the 4th inning, and Game 5 only has the last two innings. About 30 minutes of excerpts from Game 6 survive, while Game 7 cuts off with one out at the top of the 9th inning.
** While the last inning and a half of Game 2 is missing from the Major League Baseball/[[Major League Baseball on NBC|NBC]] copy, the Andrews plays (totaling about 60 seconds of coverage) survived because after the World Series, NBC put together a 20-minute presentation tape narrated by [[Curt Gowdy]] to submit to the [[Peabody Awards]] in order to get consideration for an award for their coverage by the committee; the tape includes the two Andrews plays with Gowdy and [[Tony Kubek]]'s calls and analysis of them. The presentation tape is held by the Peabody vault, creating a case where "reconstructing" a game in an incomplete format would require going to two different outlets.
* [[1974 World Series|1974]] ([[1974 Oakland Athletics season|A's]]–[[1974 Los Angeles Dodgers season|Dodgers]]) – Games 1–4 are complete. Game 5 is near intact, but the bottom of the 9th inning is missing and only exists on the original radio broadcast.

====League Championship Series telecasts====
For the League Championship Series telecasts spanning from 1969 to 1975, only Game 2 of the [[1972 American League Championship Series]] ([[1972 Oakland Athletics season|Oakland]]–[[1972 Detroit Tigers season|Detroit]]) is known to exist;<ref name="Surviving World Series Telecasts"/> however, the copy on the trade circuit is missing the [[Bert Campaneris]]–[[Lerrin LaGrow]] brawl.

There are some instances where the only brief glimpse of telecast footage of an early LCS game can be seen in a surviving newscast from that night.
* Clips of Game 5 of the [[1972 National League Championship Series]] featuring the then-[[Cincinnati Reds]] announcer [[Al Michaels]] calling the two crucial plays of the game, the game-tying home run by [[Johnny Bench]] and wild pitch bringing home [[George Foster (baseball)|George Foster]] with the series-clinching run, are available.
* The last out of the [[1973 National League Championship Series]] as described by [[Jim Simpson (sportscaster)|Jim Simpson]] was played on that night's ''[[NBC Nightly News]]'', but other than that the entire game is gone.
* On the day the [[1969 New York Mets season|New York Mets]] and [[1969 Baltimore Orioles season|Baltimore Orioles]] wrapped up their respective League Championship Series in 1969, a feature story on the ''[[CBS Evening News]]'' showed telecast clips of the [[1969 American League Championship Series|ALCS]] game (albeit with no original sound). This is all that likely remains of anything from that third game of the [[1969 Baltimore Orioles season|Orioles]]–[[1969 Minnesota Twins season|Twins]] series.

While all telecasts of World Series games starting with [[1972 World Series|1975]] are accounted for and exist, the LCS is still a spotty situation through the late 1970s:
* [[1976 American League Championship Series|1976 ALCS]] – Game 5 is intact, from the [[American Broadcasting Company|ABC]] vault.
* [[1976 National League Championship Series|1976 NLCS]] – Game 3 is intact, albeit an off-air recording taped in the [[KATU|Portland market]]. Apparently, this copy is the only extant version because the ABC vault copy has no sound.
* [[1977 National League Championship Series|1977 NLCS]] – Game 3 is intact, from the [[1977 Philadelphia Phillies season|Philadelphia Phillies]]' local [[KYW-TV|NBC affiliate]]. A copy is held by Major League Baseball, who also appears to have Game 4 as well.
* [[1977 American League Championship Series|1977 ALCS]] – Game 5 is intact, with both the [[WPIX-TV|WPIX]] and [[Major League Baseball on NBC|NBC]] versions existing through off-air recordings.
** Clips of these games may be seen in highlight shows such as ''[[Yankeeography]]''. It is believed that incomplete tapes of the ALCS exist. It is possible these games are not shown in part because the audio quality is poor. A common method of getting around such deficiencies would be to overlay a radio telecast or narration by a player or commentator where gaps exist.
* [[1978 American League Championship Series|1978 ALCS]] – All four games ([[Major League Baseball on ABC|ABC]] version) are intact via off-air recordings.
* [[1978 National League Championship Series|1978 NLCS]] – Game 4 is intact, again from off-air recordings.

====NBA Finals====
{{see also|List of NBA Finals broadcasters}}
* [[1963 NBA Finals|1963]]: [[Boston Celtics|Celtics]]–[[Los Angeles Lakers|Lakers]] – Game 6 is intact.
* [[1969 NBA Finals|1969]]: Celtics–Lakers – only the entire 4th quarter of Game 7 exists.
* [[1970 NBA Finals|1970]]: Lakers–[[New York Knicks|Knicks]] – Game 7 is intact.
* [[1971 NBA Finals|1971]]: [[Milwaukee Bucks|Bucks]]–[[Washington Bullets|Bullets]] – only nearly all of the second half of game 4 exists.
* [[1972 NBA Finals|1972]]: Knicks–Lakers – Game 5 is intact with the exception of the last 3–4 minutes of the game
* [[1973 NBA Finals|1973]]: Knicks–Lakers – Games 1–4 are missing, while the entire Game 5 wasn't found until 2013 and some of which was shown in the ''[[30 for 30]]'' documentary ''When The Garden Was Eden''.
* [[1974 NBA Finals|1974]]: Bucks–Celtics – only the 4th quarter and 2 overtime of Game 6 and the 4th quarter of Game 7 exist.*[1975 NBA Finals -.Bullets-Warriors game ..1,2,&3 intact
* [[1976]]: Suns-Celtics - Games 5 & 6 are intact.

===Wiped programs===

====Early live shows====
Many programs in the early days of television were live broadcasts that are lost because they were not recorded. Most prime-time programs that were preserved used the [[kinescope|kinescope recording]] process, which involved filming the live broadcast from a television screen using a motion-picture camera (videotape, for recording programs, was not perfected until the late 1950s and was not widely used until the late 1960s). This was also a common practice for broadcasting live TV shows to the [[West Coast of the United States|west coast]], as performers often performed a show back-to-back, but never back-to-back-to-back.

Daytime programs, however, were generally not kinescoped for preservation (although many were temporarily kinescoped for later broadcast, episodes recorded in this way were often junked). Many local station and network newscasts were prone to wiping.

====News====
Some early news programs, such as ''[[Camel News Caravan]]'', are largely lost. Moving images of [[Walter Cronkite]] reading the news in his studio every night for six years are gone with the exception of his coverage of the [[Cuban Missile Crisis]] in 1962 and the [[JFK assassination]] in 1963. Studio shots of [[Peter Jennings]] inside his [[American Broadcasting Company|ABC]] studio during his first year there (1965) are also gone.

[[Vanderbilt University]] has kept all evening national news telecasts since Monday, August 5, 1968.

As of 1997, CBS had saved 1,000,000 videotapes of news reports, broadcasts, stock footage, and outtakes according to a report that year from the [[National Film Preservation Board]]. The same report added, "Television stations still erase and recycle their video cassettes", referring to local news programs.<ref>{{cite web
 | url        = http://www.loc.gov/film/tvstudy.html
 | title      = Television/Video Preservation Study: Volume 1: Report
 | author     = [[Librarian of Congress]]
 | date       = October 1997
 | work       =
 | publisher  = [[Library of Congress]]
 | accessdate = 26 November 2012
}}</ref> Many local stations contract with outside companies for archiving news coverage.

====Situation comedy====
Little of the first [[sitcom]], ''[[Mary Kay and Johnny|The Mary Kay and Johnny Show]]'', remains today. It was initially live and not recorded, but later in its run kinescopes were made for rebroadcasting. Fragments of episodes and one complete installment are known to exist.

====Game shows====
[[Game show]]s, more than any other genre, were prone to wiping. Many games between 1941 and 1980 had insignificantly-short runs (some measured in a span of weeks or even days) that the networks felt it unnecessary to keep them for posterity, whereas recycling the tapes would be more profitable and less of an effort than attempting to sell the series in reruns, in an era before [[cable television]].

[[Mark Goodson]]–[[Bill Todman]] Productions (and to a lesser extent, [[Barry & Enright Productions|Barry-Enright Productions]] and [[Chuck Barris Productions]]) and to an even lesser extent [[Heatter-Quigley Productions]] had the foresight to preserve many of their games for later reruns; for years, these shows dominated the [[Game Show Network]] (GSN) line-up and now make up a major portion of [[Buzzr TV]]'s lineup.

Most other game shows from that era were not so fortunate.  All of the [[Bob Stewart (television)|Bob Stewart]] (except ''[[Pyramid (game show)|Pyramid]]''), [[Heatter-Quigley Productions|Heatter–Quigley]] except for ''[[PDQ (game show)|PDQ]]'' which aired in syndication as well as many episodes of ''[[Hollywood Squares]]'', [[Stefan Hatos-Monty Hall Productions|Hatos–Hall]] (except for a large portion of ''[[Let's Make a Deal]]''), and pre-1980 [[Merv Griffin]] productions have been destroyed, with the exception of a few rare pilots and "cast aside" episodes. The few remaining episodes have therefore become collectors' items, and an active trading circuit exists among collectors.

NBC and ABC continued the wiping process well into the 1970s; while ABC ceased in early 1978, NBC continued to wipe some shows into 1980, leaving much of their daytime game show content lost forever. CBS abandoned the wiping process by September 1972, largely as a result of their collaboration with Goodson-Todman; as a result, even the network's shorter-lived games (such as ''[[Spin-Off (game show)|Spin-Off]]'') still exist in their entirety. Incidentally, all three networks ended their wiping practices during the time [[Fred Silverman]] led their respective networks.

While it remained in business, DuMont wished to keep its programs as intact as possible. However, the network ceased to exist in 1956 and its archive was destroyed in the 1970s. The corporate successor to DuMont, [[Fox Broadcasting Company|Fox]], not only has never aired any daytime programming (other than its [[Fox Kids]] block from 1990 to 2001) but debuted in 1986, well beyond the wiping era.

====Award shows====
Several award shows from the 1950s and 1960s, such as the [[Academy Awards]] and the [[Emmy Awards]], only survive in kinescope format. From [[29th Academy Awards|1957]] to [[37th Academy Awards|1965]], the Academy Awards were taped in black and white, but only survive in kinescope format for overseas distribution, especially for the European TV audiences, which used another system ([[576i|625 lines]] as opposed to [[480i|525 lines]]), as the tapes used for late broadcasting were reused. All of the taped broadcasts of the Oscars from [[38th Academy Awards|1966]] (the first to be broadcast in color) remain intact.

==See also==
{{portal|Television}}
* [[Doctor Who missing episodes|''Doctor Who'' missing episodes]]
* [[British television Apollo 11 coverage]]
* [[Missing Believed Wiped]]
* [[Kinescope]]
* [[Lost film]]
* [[List of lost television broadcasts]]
* [[Film preservation]]
* [[List of surviving DuMont Television Network broadcasts]]

==Footnotes==
{{Reflist|2}}

==References==
*{{cite book |last= Fiddy |first= Dick |authorlink= |coauthors= |title= Missing, Believed Wiped: Searching for the Lost Treasures of British Television |year= 2002 |publisher= [[British Film Institute]] |location= London |isbn= 978-0-85170-866-9 }}

==External links==
*[http://www.bbc.co.uk/cult/treasurehunt/ Full Details of the BBC's treasure Hunt]
*[http://www.lostshows.com/default.aspx? Lost Shows (UK) search engine], Kaleidoscope website
*[http://www.missing-episodes.com/ British TV Missing Episodes Index]
*[http://www.wipednews.com/ Wiped News.Com - A news and features website devoted to missing TV, Film & Radio]
*[http://www.marketwatch.com/story/story/print?guid=E880D4C8-E078-11DF-B7D4-002128049AD6 The hunt for TV’s lost baseball treasures]
*[http://www.tvobscurities.com/lost/lostormissing/ Television Obscurities >> Television — Lost or Missing]

{{Major League Baseball on national television}}
{{National Basketball Association on television}}
{{National Football League on television and radio}}

[[Category:Television terminology]]
[[Category:Data management]]
[[Category:Television preservation]]
<=====doc_Id=====>:362
<=====title=====>:
Data definition specification
<=====text=====>:
{{technical|date=December 2012}}
In [[computing]], a '''data definition specification''' (DDS) is a guideline to ensure comprehensive and consistent data definition. It represents the attributes required to quantify data definition. A comprehensive data definition specification encompasses enterprise data, the hierarchy of [[data management]], prescribed guidance enforcement and criteria to determine compliance.

==Overview==
A data definition specification may be developed for any organization or specialized field, improving the quality of its products through consistency and transparency. It eliminates redundancy (since all contributing areas are referencing the same specification) and provides standardization, making it easier and more efficient to create, modify, verify, analyze and share information across the enterprise.<ref>Gouin, Deborah. & Corcoran, Charmane K. (2008).  Developing the MSU Enterprise Data Definition Standard.  Michigan State University Web site:  http://eis.msu.edu/uploads/---University%20EIS%20Working%20Committee%20Meetings/05%20August%202008/Enterprise%20Data%20Definition%20Standard%20Presentation082708.pdf</ref>
   
To understand how a data definition specification works in an enterprise, we must look at the elements of a DDS. Writing data definitions, defining business terms (or rules) in the context of a particular environment, provides structure for an organization’s [[data architecture]]. In developing these definitions, the words used must be traceable to clearly defined data.

A data definition specification may be used in the following activities to provide consistency and clarity between  departments supporting the activity:<ref name="datagovernance">Thomas, Gwen. (2008). Writing Enterprise-Quality Data Definitions: Tips for Creating Terms and Definitions. Data Governance Institute Web site: http://www.datagovernance.com/dgi_wp_writing_enterprise-quality_data_definitions.pdf</ref>
*  [[Business intelligence]]
*  [[Business process modeling]]
*  Business rules management
*  [[Data analysis]] and [[Data modeling|modeling]]
*  [[Information architecture]]
*  [[Metadata modeling]]
*  Report generation

== Criteria ==
A data definition specification requires data definitions to be:
* ''Atomic'' – singular, describing only one concept. Commonly used and ambiguous terms should be defined.<ref name="datagovernance" /> While a term refers to one concept, several words may be used in a term:
:*File – A concept identifiable with one word
:*File extension – A concept identifiable with more than one word 
* ''Traceable'' – Mapped to a specific data element. In business, a term may be traced to an entity (for example, a customer) or an attribute (such as a customer's name). A term may be a value in a [[data set]] (such as gender), or designate the data set itself. Traceability indicates relationships in the [[data hierarchy]].  
* ''Consistent'' - Used in a standard [[Syntax (programming languages)|syntax]]; if used in a specific context, the context is noted
* ''Accurate'' - Precise, correct and unambiguous, stating what the term is and is not<ref>International Organization for Standardization JTC1/SC32 Committee. (2004) ISO 11179-4. http://standards.iso.org/ittf/PubliclyAvailableStandards/index.html.</ref>
* ''Clear'' - Readily understood by the reader
* ''Complete'' - With the term, its description and contextual references
* ''Concise'' - To avoid circular references

== Applications ==

=== Enterprise data ===
A data definition specification was produced by the [[Open Mobile Alliance]] to document charging data.<ref>{{cite web|url=http://www.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|date=1 February 2011|website=Open Mobile Alliance|pages=6, 35|format=PDF|title=Charging Data|archiveurl=https://web.archive.org/web/20131006172727/http://technical.openmobilealliance.org/Technical/release_program/docs/Charging_Data/V1_0-20110201-A/OMA-DDS-Charging_Data-V1_0-20110201-A.pdf|archivedate=6 October 2013|accessdate=12 March 2014}}</ref> The document, the centralized catalog of data elements defined for interfaces, specifies the mapping of these data elements to protocol fields in the interfaces. Created for the exchange of financial data, Market Data Definition Language (MDDL) is an [[XML]] specification designed 
{{quote|to enable the interchange of information necessary to account, to analyze, and to trade financial instruments of the world's markets. It defines an XML-based interchange format and common data dictionary on the fields needed to describe: (1) financial instruments, (2) corporate events affecting value and tradability, and (3) market-related, economic and industrial indicators. The principal function of MDDL is to allow entities to exchange market data by standardizing formats and definitions. MDDL provides a common format for market data so that it can be efficiently passed from one processing system to another and provides a common understanding of market data content by standardizing terminology and by normalizing the relationships of various data elements to one another&nbsp;... From the user perspective, the goal of MDDL is to enable users to integrate data from multiple sources by standardizing both the input feeds used for data warehousing (i.e., define what's being provided by vendors) and the output methods by which client applications request the data (i.e., ensure compatibility on how to get data in and out of applications)."<ref>{{cite web|title=Market Data Definition Language (MDDL)|date=December 26, 2002|website=Cover Pages |url=http://xml.coverpages.org/mddl.html|archiveurl=https://web.archive.org/web/20131214075132/http://xml.coverpages.org/mddl.html|archivedate=December 14, 2013|accessdate=March 12, 2014}}</ref>}}

=== Clinical submissions ===
The [[Clinical Data Interchange Standards Consortium]], a global, multidisciplinary, non-profit organization, has established standards to support the acquisition, exchange, submission and archiving of clinical research data and metadata. CDISC standards are vendor-neutral, platform-independent and freely available from the CDISC website. The Case Report Tabulation Data Definition Specification (define.xml) draft version 2.0, the oldest data definition specification, is part of the evolution from the 1999 FDA electronic submission (eSub) guidance and electronic Common Technical Document (eCTD) documents specifying that a document describing the content and structure of included data be included in a submission. Define.xml was developed to automate the review process by generating a machine-readable data-definition document. Define.xml has standardized submissions to the [[Food and Drug Administration]], reducing review times from over two years to several months.<ref>{{cite web|title=Define-XML|year=2012|website=Clinical Data Interchange Standards Consortium|url=http://www.cdisc.org/define-xml|archiveurl=https://web.archive.org/web/20131004232219/http://www.cdisc.org/define-xml|archivedate=October 4, 2013|accessdate=March 12, 2014}}</ref>

=== Archival data ===
A data definition specification is the foundation of [[metadata]] for [[scientific data archiving]]. The [[Metadata Encoding and Transmission Standard]] (METS) uses one principle of a DDS: consistent use of key terms to catalog digital objects for global use. The METS schema is a flexible mechanism for encoding descriptive, administrative and structural metadata for a [[digital library]] object and expressing complex links between metadata, and can provide a useful standard for the exchange of digital-library objects between repositories.<ref>Metadata Encoding & Transmission Standard (METS) Web site from The Library of Congress- Standards http://www.loc.gov/standards/mets/</ref>

A similar effort is underway to preserve complex data associated with video-game archiving. Preserving Virtual Worlds attempted to address archival-format deficiencies, citing the lack of suitable documentation for interactive fiction and games at the [[bit]] level: specifically, the absence of "representation information" needed to map raw bits into higher-level data constructs.<ref>“Meta Data Schema Development” (2008) [http://pvw.illinois.edu/pvw/?page_id=25 Preserving Virtual Worlds website]</ref> Preserving Virtual Worlds 2 is a research project expanding on initial efforts in this field.<ref>Preserving Virtual Worlds 2, Researching best practices for videogame preservation. (2012). http://pvw.illinois.edu/pvw2/</ref>

== See also ==
* [[Clinical Data Interchange Standards Consortium]] (CDISC)
* [[Data governance]]
* [[ISO/IEC 11179]]
* [[Metadata Encoding and Transmission Standard]] (METS)
* [[OASIS (organization)|OASIS]]

==References==
{{reflist}}

[[Category:Data management]]
<=====doc_Id=====>:365
<=====title=====>:
Metadata
<=====text=====>:
{{pp-move-indef|small=yes}}
{{use dmy dates|date=October 2016}}
[[File:Schlagwortkatalog.jpg|thumb|200px|In the 2010s, metadata typically refers to digital forms; however, even traditional card catalogues from the 1960s and 1970s are an example of metadata, as the cards contain information about the books in the library (author, title, subject, etc.).]]

'''Metadata''' is "[[data]] [information] that provides information about other data".<ref>http://www.merriam-webster.com/dictionary/metadata</ref> Three distinct types of metadata exist: '''descriptive metadata''', '''structural metadata''', and '''administrative metadata'''.<ref name="Metadata Basics Outline">{{cite web | url=http://marciazeng.slis.kent.edu/metadatabasics/types.htm | title=Metadata Types and Functions | publisher=NISO | date=2004 | accessdate=5 October 2016 | author=Zeng, Marcia}}</ref>

* Descriptive metadata describes a resource for purposes such as discovery and identification. It can include elements such as title, abstract, author, and keywords.
* Structural metadata is metadata about containers of metadata and indicates how compound objects are put together, for example, how pages are ordered to form chapters.
* Administrative metadata provides information to help manage a resource, such as when and how it was created, file type and other technical information, and who can access it.<ref name="Understanding Metadata (2)">{{cite book | url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf | title=Understanding Metadata | publisher=NISO Press | author=National Information Standards Organization (NISO) | year=2001 | isbn=1-880124-62-9|page=1}}</ref>

== History ==
Metadata was traditionally used in the [[library catalog|card catalogs]] of [[library|libraries]] until the 1980s, when libraries converted their catalog data to digital databases. In the 2000s, as digital formats are becoming the prevalent way of storing data and information, metadata is also used to describe digital data using [[metadata standards]].

There are different metadata standards for each different discipline (e.g., [[museum]] collections, [[digital audio file]]s, [[website]]s, etc.). Describing the [[Content (media)|contents]] and [[Context (computing)|context]] of data or [[computer file|data files]] increases its usefulness. For example, a [[web page]] may include metadata specifying what software language the page is written in (e.g., HTML), what tools were used to create it, what subjects the page is about, and where to find more information about the subject. This metadata can automatically improve the reader's experience and make it easier for users to find the web page online.<ref name="Practices in Using Metadata">{{cite web | url=http://www.library.illinois.edu/dcc/bestpractices/chapter_11_structuralmetadata.html | title=Best Practices for Structural Metadata | publisher=University of Illinois | date=15 December 2010 | accessdate=17 June 2016}}</ref> A [[CD]] may include metadata providing information about the musicians, singers and songwriters whose work appears on the disc.

A principal purpose of metadata is to help users find relevant information and discover resources. Metadata also helps to organize electronic resources, provide digital identification, and support the archiving and preservation of resources. Metadata assists users in resource discovery by "allowing resources to be found by relevant criteria, identifying resources, bringing similar resources together, distinguishing dissimilar resources, and giving location information."<ref name = Understanding_Metadata/> Metadata of telecommunication activities including [[Internet]] traffic is very widely collected by various national governmental organizations. This data is used for the purposes of [[traffic analysis]] and can be used for mass [[surveillance]].<ref>https://www.schneier.com/essays/archives/2014/03/metadata_surveillanc.html</ref>

In many countries, the metadata relating to emails, telephone calls, web pages, video traffic, IP connections and cell phone locations are routinely stored by government organizations.<ref name="NSA_Watching">http://www.washingtonsblog.com/2014/03/nsa-recorded-every-single-call-one-country-country-america.html</ref>

== Definition ==
Metadata means "data about data". Although the "meta" prefix (from the [[Greek language|Greek]] [[preposition]] and [[prefix]] μετά-) means "after" or "beyond", it is used to mean "about" in [[epistemology]]. Metadata is defined as the data providing information about one or more aspects of the data; it is used to summarize basic information about data which can make tracking and working with specific data easier.<ref>{{cite web
| title = A Guardian Guide to your Metadata
| website = [[theguardian.com]]
| publisher = [[Guardian News and Media Limited]]
| date = 12 June 2013
| url = https://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=0000000
}}</ref> Some examples include:
* Means of creation of the data
* Purpose of the data
* Time and date of creation
* Creator or author of the data
* Location on a [[computer network]] where the data was created
* [[Technical standard|Standards]] used
* File size

For example, a [[digital image]] may include metadata that describes how large the picture is, the color depth, the image resolution, when the image was created, the shutter speed, and other data.<ref>{{cite web|url=http://www.adeoimaging.com |title=ADEO Imaging: TIFF Metadata |accessdate=2013-05-20}}</ref> A text document's metadata may contain information about how long the document is, who the author is, when the document was written, and a short summary of the document. Metadata within web pages can also contain descriptions of page content, as well as key words linked to the content.<ref name="Rouse, M (2014)">{{cite web
| last = Rouse
| first = Margaret
| title = Metadata
| work = WhatIs
| publisher = TechTarget
| date = July 2014
| url = http://whatis.techtarget.com/definition/metadata
}}</ref> These links are often called "Metatags", which were used as the primary factor in determining order for a web search until the late 1990s.<ref name="Rouse, M (2014)"/> The reliance of metatags in web searches was decreased in the late 1990s because of "keyword stuffing".<ref name="Rouse, M (2014)"/> Metatags were being largely misused to trick search engines into thinking some websites had more relevance in the search than they really did.<ref name="Rouse, M (2014)"/>

Metadata can be stored and managed in a [[database]], often called a [[metadata registry]] or [[metadata repository]].<ref>Hüner, K.; Otto, B.; Österle, H.: Collaborative management of business metadata, in: ''International Journal of Information Management'', 2011</ref> However, without context and a point of reference, it might be impossible to identify metadata just by looking at it.<ref>{{cite web|url=http://www.bls.gov/ore/pdf/st000010.pdf |title=Metadata Standards And Metadata Registries: An Overview |format=PDF |accessdate=2011-12-23}}</ref> For example: by itself, a database containing several numbers, all 13 digits long could be the results of calculations or a list of numbers to plug into an equation - without any other context, the numbers themselves can be perceived as the data. But if given the context that this database is a log of a book collection, those 13-digit numbers may now be identified as [[ISBN]]s - information that refers to the book, but is not itself the information within the book. The term "metadata" was coined in 1968 by Philip Bagley, in his book "Extension of Programming Language Concepts" where it is clear that he uses the term in the ISO 11179 "traditional" sense, which is "structural metadata" i.e. "data about the containers of data"; rather than the alternate sense "content about individual instances of data content" or metacontent, the type of data usually found in library catalogues.<ref name=Bagley>{{Cite journal
|author=Philip Bagley
|title=Extension of programming language concepts
|date=November 1968
| url = http://www.dtic.mil/dtic/tr/fulltext/u2/680815.pdf
|publisher=University City Science Center
|location=Philadelphia
}}</ref><ref>"The notion of "metadata" introduced by Bagley". {{Cite journal
 | last = Solntseff
 | first = N+1
 | last2 = Yezerski
 | first2 = A
 | year = 1974
 | title = A survey of extensible programming languages
 | series = Annual Review in Automatic Programming
 | publisher = Elsevier Science Ltd
 | volume = 7
 | pages = 267–307
 | doi = 10.1016/0066-4138(74)90001-9
}}</ref> Since then the fields of information management, information science, information technology, librarianship, and [[GIS]] have widely adopted the term. In these fields the word ''metadata'' is defined as "data about data".<ref name=NISO >{{Cite book
| last = NISO
| authorlink =NISO
| title = Understanding Metadata
| publisher = NISO Press
| url = http://www.niso.org/publications/press/UnderstandingMetadata.pdf
| isbn = 1-880124-62-9
| accessdate = 5 January 2010 }}
</ref>{{page needed|date=November 2016}} While this is the generally accepted definition, various disciplines have adopted their own more specific explanation and uses of the term.

== Types ==
While the metadata application is manifold, covering a large variety of fields, there are specialized and well-accepted models to specify types of metadata. [[Francis Bretherton|Bretherton]] & Singley (1994) distinguish between two distinct classes: structural/control metadata and guide metadata.<ref>{{Cite conference
| first1 = F. P. | last1 = Bretherton | author1-link = Francis Bretherton
|first2 = P.T. | last2 = Singley
| title = Metadata: A User's View, Proceedings of the International Conference on Very Large Data Bases (VLDB)
| pages = 1091–1094
| publisher =
| year = 1994}}
</ref> ''Structural metadata'' describes the structure of database objects such as tables, columns, keys and indexes. ''Guide metadata'' helps humans find specific items and are usually expressed as a set of keywords in a natural language. According to [[Ralph Kimball]] metadata can be divided into 2 similar categories: technical metadata and business metadata. ''Technical metadata'' corresponds to internal metadata, and ''business metadata'' corresponds to external metadata. Kimball adds a third category, ''process metadata''. On the other hand, NISO distinguishes among three types of metadata: descriptive, structural, and administrative.<ref name=NISO/>

''Descriptive metadata'' is typically used for discovery and identification, as information to search and locate an object, such as title, author, subjects, keywords, publisher. ''Structural metadata'' describes how the components of an object are organized. An example of structural metadata would be how pages are ordered to form chapters of a book. Finally, ''administrative metadata'' gives information to help manage the source. Administrative metadata refers to the technical information, including file type, or when and how the file was created. Two sub-types of administrative metadata are rights management metadata and preservation metadata. ''Rights management metadata'' explains intellectual property rights, while ''preservation metadata'' contains information to preserve and save a resource.<ref name = Understanding_Metadata>{{cite book|last=National Information Standards Organization|title=Understanding Metadata|year=2004|publisher=NISO Press|location=Bethesda, MD|isbn=1-880124-62-9|url=http://www.niso.org/publications/press/UnderstandingMetadata.pdf |author2=Rebecca Guenther |author3=Jaqueline Radebaugh|accessdate=2 April 2014}}</ref>{{page needed|date=November 2016}}

== Structures ==
Metadata (metacontent) or, more correctly, the vocabularies used to assemble metadata (metacontent) statements, is typically structured according to a standardized concept using a well-defined metadata scheme, including: [[metadata standards]] and [[Metadata modeling|metadata models]]. Tools such as [[Controlled vocabulary|controlled vocabularies]], [[Taxonomy (general)|taxonomies]], [[Thesaurus (information retrieval)|thesauri]], [[Data Dictionary|data dictionaries]], and [[Metadata registry|metadata registries]] can be used to apply further standardization to the metadata. Structural metadata commonality is also of paramount importance in [[data model]] development and in [[database design]].

=== Syntax ===
Metadata (metacontent) syntax refers to the rules created to structure the fields or elements of metadata (metacontent).<ref>{{cite web
| last = Cathro
| first = Warwick
| authorlink =
| title = Metadata: an overview
| year = 1997
| url = http://www.nla.gov.au/nla/staffpaper/cathro3.html
| accessdate = 6 January 2010
}}</ref> A single metadata scheme may be expressed in a number of different markup or programming languages, each of which requires a different syntax. For example, Dublin Core may be expressed in plain text, [[HTML]], [[XML]], and [[Resource Description Framework|RDF]].<ref>{{cite web
| last = DCMI
| authorlink =Dublin_Core_Metadata_Initiative
| title = Semantic Recommendations
| date =5 October 2009
| url = http://dublincore.org/specifications/
| accessdate = 6 January 2010
}}</ref>

A common example of (guide) metacontent is the bibliographic classification, the subject, the [[List of Dewey Decimal classes|Dewey Decimal class number]]. There is always an implied statement in any "classification" of some object. To classify an object as, for example, Dewey class number 514 (Topology) (i.e. books having the number 514 on their spine) the implied statement is: "<nowiki><book><subject heading><514></nowiki>. This is a subject-predicate-object triple, or more importantly, a class-attribute-value triple. The first two elements of the triple (class, attribute) are pieces of some structural metadata having a defined semantic. The third element is a value, preferably from some controlled vocabulary, some reference (master) data. The combination of the metadata and master data elements results in a statement which is a metacontent statement i.e. "metacontent = metadata + master data". All of these elements can be thought of as "vocabulary". Both metadata and master data are vocabularies which can be assembled into metacontent statements. There are many sources of these vocabularies, both meta and master data: UML, EDIFACT, XSD, Dewey/UDC/LoC, SKOS, ISO-25964, Pantone, Linnaean Binomial Nomenclature, etc. Using controlled vocabularies for the components of metacontent statements, whether for indexing or finding, is endorsed by [[ISO 25964]]: "If both the indexer and the searcher are guided to choose the same term for the same concept, then relevant documents will be retrieved."<ref>https://www.iso.org/obp/ui/#iso:std:iso:25964:-1:ed-1:v1:en</ref> This is particularly relevant when considering search engines of the internet, such as Google. The process indexes pages then matches text strings using its complex algorithm; there is no intelligence or "inferencing" occurring, just the illusion thereof.

=== Hierarchical, linear and planar schemata ===
Metadata schemata can be hierarchical in nature where relationships exist between metadata elements and elements are nested so that parent-child relationships exist between the elements.
An example of a hierarchical metadata schema is the [[Learning object metadata|IEEE LOM]] schema, in which metadata elements may belong to a parent metadata element.
Metadata schemata can also be one-dimensional, or linear, where each element is completely discrete from other elements and classified according to one dimension only.
An example of a linear metadata schema is the [[Dublin Core Metadata Initiative|Dublin Core]] schema, which is one dimensional.
Metadata schemata are often two dimensional, or planar, where each element is completely discrete from other elements but classified according to two orthogonal dimensions.<ref>{{cite web
| title = Types of Metadata
| publisher = [[University of Melbourne]]
| date = 15 August 2006
| url = http://www.infodiv.unimelb.edu.au/metadata/add_info.html
| accessdate = 6 January 2010
| archiveurl = https://web.archive.org/web/20091024112353/http://www.infodiv.unimelb.edu.au/metadata/add_info.html
| archivedate = 2009-10-24}}</ref>

=== Hypermapping ===
In all cases where the metadata schemata exceed the planar depiction, some type of [[hypermap]]ping is required to enable display and view of metadata according to chosen aspect and to serve special views. Hypermapping frequently applies to layering of geographical and geological information overlays.<ref>{{cite web |url=http://www.isprs.org/proceedings/XXXII/part4/kuebler51.pdf |title=THE DESIGN AND DEVELOPMENT OF A GEOLOGIC HYPERMAP PROTOTYPE |first1=Stefanie |last1=Kübler |first2=Wolfdietrich |last2=Skala |first3=Agnès |last3=Voisard}}</ref>

=== Granularity ===
The degree to which the data or metadata is structured is referred to as its [[Data granularity|"granularity"]]. "Granularity" refers to how much detail is provided. Metadata with a high granularity allows for deeper, more detailed, and more structured information and enables greater levels of technical manipulation. A lower level of granularity means that metadata can be created for considerably lower costs but will not provide as detailed information. The major impact of granularity is not only on creation and capture, but moreover on maintenance costs. As soon as the metadata structures become outdated, so too is the access to the referred data. Hence granularity must take into account the effort to create the metadata as well as the effort to maintain it.

== Standards ==
International standards apply to metadata. Much work is being accomplished in the national and international standards communities, especially [[ANSI]] (American National Standards Institute) and [[International Organization for Standardization|ISO]] (International Organization for Standardization) to reach consensus on standardizing metadata and registries. The core metadata registry standard is [[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] 11179 Metadata Registries (MDR), the framework for the standard is described in ISO/IEC 11179-1:2004.<ref>{{cite web
  |url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=39438
  |title=ISO/IEC 11179-1:2004 Information technology - Metadata registries (MDR) - Part 1: Framework
  |publisher=Iso.org |date=2009-03-18 |accessdate=2011-12-23
}}</ref> A new edition of Part 1 is in its final stage for publication in 2015 or early 2016. It has been revised to align with the current edition of Part 3, ISO/IEC 11179-3:2013<ref>{{cite web
  |url=http://standards.iso.org/ittf/PubliclyAvailableStandards/c050340_ISO_IEC_11179-3_2013.zip
  |title=ISO/IEC 11179-3:2013 Information technology-Metadata registries - Part 3: Registry metamodel and basic attributes
  |publisher=iso.org|date=2014}}</ref> which extends the MDR to support registration of Concept Systems.
(see [[ISO/IEC 11179]]). This standard specifies a schema for recording both the meaning and technical structure of the data for unambiguous usage by humans and computers. ISO/IEC 11179 standard refers to metadata as information objects about data, or "data about data". In ISO/IEC 11179 Part-3, the information objects are data about Data Elements, Value Domains, and other reusable semantic and representational information objects that describe the meaning and technical details of a data item. This standard also prescribes the details for a metadata registry, and for registering and administering the information objects within a Metadata Registry. ISO/IEC 11179 Part 3 also has provisions for describing compound structures that are derivations of other data elements, for example through calculations, collections of one or more data elements, or other forms of derived data. While this standard describes itself originally as a "data element" registry, its purpose is to support describing and registering metadata content independently of any particular application, lending the descriptions to being discovered and reused by humans or computers in developing new applications, databases, or for analysis of data collected in accordance with the registered metadata content. This standard has become the general basis for other kinds of metadata registries, reusing and extending the registration and administration portion of the standard.

The Geospatial community has a tradition of specialized [[geospatial metadata]] standards, particularly building on traditions of map- and image-libraries and catalogues. Formal metadata is usually essential for geospatial data, as common text-processing approaches are not applicable.

The [[Dublin Core]] metadata terms are a set of vocabulary terms which can be used to describe resources for the purposes of discovery. The original set of 15 classic<ref>{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=2009-12-14 |accessdate=2013-08-17}}</ref> metadata terms, known as the Dublin Core Metadata Element Set<ref>{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=2013-08-17}}</ref> are endorsed in the following standards documents:
* IETF RFC 5013<ref>{{cite web |url= http://www.ietf.org/rfc/rfc5013.txt |title=The Dublin Core Metadata Element Set |author=J. Kunze, T. Baker |work=ietf.org |year=2007 |accessdate=17 August 2013}}</ref>
* ISO Standard 15836-2009<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=2009-02-18 |accessdate=2013-08-17}}</ref>
* NISO Standard Z39.85.<ref>{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&gid=None&project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=2007-05-22 |accessdate=2013-08-17}}</ref>

Although not a standard, [[Microformat]] (also mentioned in the section [[Metadata#Metadata on the Internet|metadata on the internet]] below) is a web-based approach to semantic markup which seeks to re-use existing HTML/XHTML tags to convey metadata. Microformat follows XHTML and HTML standards but is not a standard in itself. One advocate of microformats, [[Tantek Çelik]], characterized a problem with alternative approaches: {{cquote|Here's a new language we want you to learn, and now you need to output these additional files on your server. It's a hassle. (Microformats) lower the barrier to entry.<ref name="Wharton000">{{cite web |title=What's the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&ID=1247}}</ref>}}

== Use ==

=== Photographs ===
Metadata may be written into a [[digital photo]] file that will identify who owns it, copyright and contact information, what brand or model of camera created the file, along with exposure information (shutter speed, f-stop, etc.) and descriptive information, such as keywords about the photo, making the file or image searchable on a computer and/or the Internet. Some metadata is created by the camera and some is input by the photographer and/or software after downloading to a computer. Most digital cameras write metadata about model number, shutter speed, etc., and some enable you to edit it;<ref>{{cite web|publisher=gurucamera.com|title=How To Copyright Your Photos With Metadata|url=https://gurucamera.com/copyright-photos-metadata/|work=Guru Camera}}</ref> this functionality has been available on most Nikon DSLRs since the [[Nikon D3]], on most new Canon cameras since the [[Canon EOS 7D]], and on most Pentax DSLRs since the Pentax K-3. Metadata can be used to make organizing in post-production easier with the use of key-wording. Filters can be used to analyze a specific set of photographs and create selections on criteria like rating or capture time.

Photographic Metadata Standards are governed by organizations that develop the following standards. They include, but are not limited to:
* [[IPTC Information Interchange Model]] IIM (International Press Telecommunications Council),
* [[International Press Telecommunications Council|IPTC]] Core Schema for XMP
* [[Extensible Metadata Platform|XMP]] – Extensible Metadata Platform (an ISO standard)
* [[Exchangeable image file format|Exif]] – Exchangeable image file format, Maintained by CIPA (Camera & Imaging Products Association) and published by JEITA (Japan Electronics and Information Technology Industries Association)
* [[Dublin Core]] (Dublin Core Metadata Initiative – DCMI)
* PLUS (Picture Licensing Universal System).
* [http://www.loc.gov/standards/vracore/schemas.html VRA Core] (Visual Resource Association)<ref>{{cite web|title=VRA Core Support Pages|url=http://core.vraweb.org|website=Visual Resource Association Foundation|publisher=Visual Resource Association Foundation|accessdate=27 February 2016}}</ref>

=== Telecommunications ===
Information on the times, origins and destinations of phone calls, electronic messages, instant messages and other modes of telecommunication, as opposed to message content, is another form of metadata. Bulk collection of this [[call detail record]] metadata by intelligence agencies has proven controversial after disclosures by [[Edward Snowden]] Intelligence agencies such as the NSA are keeping online metadata of millions of internet user for up to a year, regardless of whether or not they are persons of interest to the agency.

=== Video ===
Metadata is particularly useful in video, where information about its contents (such as transcripts of conversations and text descriptions of its scenes) is not directly understandable by a computer, but where efficient search of the content is desirable. There are two sources in which video metadata is derived: (1) operational gathered metadata, that is information about the content produced, such as the type of equipment, software, date, and location; (2) human-authored metadata, to improve search engine visibility, discoverability, audience engagement, and providing advertising opportunities to video publishers.<ref>{{cite web
| last = Webcase
| first = Weblog
| authorlink =
| title = Examining video file metadata
| year = 2011
| url = http://veresoftware.com/blog/?p=364
| accessdate = 25 November 2015
}}</ref> In today's society most professional video editing software has access to metadata. Avid's MetaSync and Adobe's Bridge are two prime examples of this.<ref>{{cite web
| last = Oak Tree Press
| authorlink =
| title = Metadata for Video
| year = 2011
| url = http://veresoftware.com/blog/?p=364
| accessdate = 25 November 2015
}}</ref>

=== Web pages ===
[[Web page]]s often include metadata in the form of [[Meta element|meta tags]]. Description and keywords in meta tags are commonly used to describe the Web page's content. Meta elements also specify page description, key words, authors of the document, and when the document was last modified.<ref name="Rouse, M (2014)"/> Web page metadata helps search engines and users to find the types of web pages they are looking for.

== Creation ==
Metadata can be created either by automated information processing or by manual work. Elementary metadata captured by computers can include information about when an object was created, who created it, when it was last updated, file size, and file extension. In this context an ''object'' refers to any of the following:

* A physical item such as a book, CD, DVD, a paper map, chair, table, flower pot, etc.
* An electronic file such as a digital image, digital photo, electronic document, program file, database table, etc.

=== Data virtualization ===
{{main article|Data virtualization}}
Data virtualization has emerged in the 2000s as the new software technology to complete the virtualization "stack" in the enterprise. Metadata is used in data virtualization servers which are enterprise infrastructure components, alongside database and application servers. Metadata in these servers is saved as persistent repository and describe [[business object]]s in various enterprise systems and applications. Structural metadata commonality is also important to support data virtualization.

=== Statistics and census services ===
Standardization work has had a large impact on efforts to build metadata systems in the statistical community{{Citation needed|date=May 2013}}. Several metadata standards{{Which|date=May 2013}} are described, and their importance to statistical agencies is discussed. Applications of the standards{{Which|date=May 2013}} at the Census Bureau, Environmental Protection Agency, Bureau of Labor Statistics, Statistics Canada, and many others are described{{Citation needed|date=May 2013}}. Emphasis is on the impact a metadata registry can have in a statistical agency.

=== Library and information science ===

Metadata has been used in various ways as a means of cataloging items in libraries in both digital and analog format. Such data helps classify, aggregate, identify, and locate a particular book, DVD, magazine or any object a library might hold in its collection. Until the 1980s, many library catalogues used 3x5 inch cards in file drawers to display a book's title, author, subject matter, and an abbreviated [[Alphanumeric|alpha-numeric]] string ([[call number]]) which indicated the physical location of the book within the library's shelves. The [[Dewey Decimal Classification|Dewey Decimal System]] employed by libraries for the classification of library materials by subject is an early example of metadata usage. Beginning in the 1980s and 1990s, many libraries replaced these paper file cards with computer databases. These computer databases make it much easier and faster for users to do keyword searches. Another form of older metadata collection is the use by US Census Bureau of what is known as the "Long Form." The Long Form asks questions that are used to create demographic data to find patterns of distribution.<ref>{{cite web
| title = AGLS Metadata Element Set - Part 2: Usage Guide - A non-technical guide to using AGLS metadata for describing resources
| author = National Archives of Australia
| year = 2002
| url = http://www.naa.gov.au/records-management/publications/agls-element.aspx
| accessdate = 17 March 2010}}
</ref> [[library|Libraries]] employ metadata in [[library catalog]]ues, most commonly as part of an [[Library management system|Integrated Library Management System]]. Metadata is obtained by [[Library cataloguing#Cataloging rules|cataloguing]] resources such as books, periodicals, DVDs, web pages or digital images. This data is stored in the integrated library management system, [[Library management system|ILMS]], using the [[MARC standards|MARC]] metadata standard. The purpose is to direct patrons to the physical or electronic location of items or areas they seek as well as to provide a description of the item/s in question.

More recent and specialized instances of library metadata include the establishment of [[Digital library|digital libraries]] including [[eprint|e-print]] repositories and digital image libraries. While often based on library principles, the focus on non-librarian use, especially in providing metadata, means they do not follow traditional or common cataloging approaches. Given the custom nature of included materials, metadata fields are often specially created e.g. taxonomic classification fields, location fields, keywords or copyright statement. Standard file information such as file size and format are usually automatically included.<ref name=solodovnik>{{cite journal | last1 = Solodovnik | first1 = Iryna | year = 2011 | title = Metadata issues in Digital Libraries: key concepts and perspectives | journal = [[JLIS.it|JLIS.it: Italian Journal of Library, Archives and Information Science]] | volume = 2 | issue = 2 | publisher = University of Florence | doi = 10.4403/jlis.it-4663 | url = http://leo.cilea.it/index.php/jlis/article/view/4663 | accessdate = 29 June 2013}}</ref> Library operation has for decades been a key topic in efforts toward [[international standardization]]. Standards for metadata in digital libraries include [[Dublin Core]], [[Metadata Encoding and Transmission Standard|METS]], [[Metadata Object Description Schema|MODS]], [[Data Documentation Initiative|DDI]], [[Digital object identifier|DOI]], [[Uniform Resource Name|URN]], [[Preservation Metadata: Implementation Strategies|PREMIS]] schema, [[Ecological Metadata Language|EML]], and [[Protocol for Metadata Harvesting|OAI-PMH]]. Leading libraries in the world give hints on their metadata standards strategies.<ref>{{cite web |author=Library of Congress Network Development and MARC Standards Office |url=http://www.loc.gov/standards/metadata.html |title=Library of Congress Washington DC on metadata |publisher=Loc.gov |date=2005-09-08 |accessdate=2011-12-23}}</ref><ref>{{cite web |url=http://www.dnb.de/DE/Netzpublikationen/Ablieferung/MetadatenKernset/metadatenkernset_node.html |title=Deutsche Nationalbibliothek Frankfurt on metadata}}</ref>

=== In museums ===
Metadata in a museum context is the information that trained cultural documentation specialists, such as [[archivist]]s, [[librarian]]s, museum [[registrar (museum)|registrar]]s and [[curator]]s, create to index, structure, describe, identify, or otherwise specify works of art, architecture, cultural objects and their images.<ref name=":0">{{Cite journal |last=Zange|first=Charles S.|date=31 January 2015 |title=Community makers, major museums, and the Keet S'aaxw: Learning about the role of museums in interpreting cultural objects |url=http://mw2015.museumsandtheweb.com/paper/community-makers-major-museums-and-the-keet-saaxw-learning-about-the-role-of-museums-in-interpreting-cultural-objects/ |publisher=Museums and the Web }}</ref><ref name=":1">{{Cite book |title=Cataloging cultural objects: a guide to describing cultural works and their images. Visual Resources Association |last=Baca |first=Murtha |publisher=Visual Resources Association |year=2006 |isbn=|location=|pages=}}</ref>{{page needed|date=November 2016}}<ref name=":2">{{Cite book|title=Introduction to Metadata: Second Edition. Los Angeles: Getty Information Institute|last=Baca|first=Murtha|publisher=Getty Information Institute|year=2008|isbn=|location=Los Angeles|pages=}}</ref>{{page needed|date=November 2016}} Descriptive metadata is most commonly used in museum contexts for object identification and resource recovery purposes.<ref name=":1" />

==== Usage ====
Metadata is developed and applied within collecting institutions and museums in order to:
* Facilitate resource discovery and execute search queries.<ref name=":2" />
* Create digital archives that store information relating to various aspects of museum collections and cultural objects, and serves for archival and managerial purposes.<ref name=":2" />
* Provide public audiences access to cultural objects through publishing digital content online.<ref name=":1" /><ref name=":2" />

==== Standards ====
Many museums and cultural heritage centers recognize that given the diversity of art works and cultural objects, no single model or standard suffices to describe and catalogue cultural works.<ref name=":0" /><ref name=":1" /><ref name=":2" /> For example, a sculpted Indigenous artifact could be classified as an artwork, an archaeological artifact, or an Indigenous heritage item. The early stages of standardization in archiving, description and cataloging within the museum community began in the late 1990s with the development of standards such as [[Categories for the Description of Works of Art (CDWA)]], Spectrum, the [[Conceptual Reference Model (CIDOC)]], [[Cataloging Cultural Objects (CCO)]] and the [[CDWA Lite XML schema]].<ref name=":1" /> These standards use [[HTML]] and [[XML]] markup languages for machine processing, publication and implementation.<ref name=":1" /> The [[Anglo-American Cataloguing Rules (AACR)]], originally developed for characterizing books, have also been applied to cultural objects, works of art and architecture.<ref name=":2" /> Standards, such as the CCO, are integrated within a Museum's [[Collection Management System (CMS)]], a database through which museums are able to manage their collections, acquisitions, loans and conservation.<ref name=":2" /> Scholars and professionals in the field note that the "quickly evolving landscape of standards and technologies" create challenges for cultural documentarians, specifically non-technically trained professionals.<ref name=":3">{{Cite book|title=Linked Data for Libraries, Archives and Museums: How to Clean, Link and Publish Your Metadata|last=Hooland|first=Seth Van|last2=Verborgh|first2=Ruben|publisher=Facet|year=2014|isbn=|location=London|pages=}}</ref>{{page needed|date=November 2016}} Most collecting institutions and museums use a [[relational database]] to categorize cultural works and their images.<ref name=":2" /> Relational databases and metadata work to document and describe the complex relationships amongst cultural objects and multi-faceted works of art, as well as between objects and places, people and artistic movements.<ref name=":1" /><ref name=":2" /> Relational database structures are also beneficial within collecting institutions and museums because they allow for archivists to make a clear distinction between cultural objects and their images; an unclear distinction could lead to confusing and inaccurate searches.<ref name=":2" />

==== Cultural objects and art works ====
An object's materiality, function and purpose, as well as the size (e.g., measurements, such as height, width, weight), storage requirements (e.g., climate-controlled environment) and focus of the museum and collection, influence the descriptive depth of the data attributed to the object by cultural documentarians.<ref name=":2" /> The established institutional cataloging practices, goals and expertise of cultural documentarians and database structure also influence the information ascribed to cultural objects, and the ways in which cultural objects are categorized.<ref name=":0" /><ref name=":2" /> Additionally, museums often employ standardized commercial collection management software that prescribes and limits the ways in which archivists can describe artworks and cultural objects.<ref name=":3" /> As well, collecting institutions and museums use [[Controlled vocabulary|Controlled Vocabularies]] to describe cultural objects and artworks in their collections.<ref name=":1" /><ref name=":2" /> [[Getty Vocabularies]] and the [[Library of Congress controlled vocabularies|Library of Congress Controlled Vocabularies]] are reputable within the museum community and are recommended by CCO standards.<ref name=":2" /> Museums are encouraged to use controlled vocabularies that are contextual and relevant to their collections and enhance the functionality of their digital information systems.<ref name=":1" /><ref name=":2" /> Controlled Vocabularies are beneficial within databases because they provide a high level of consistency, improving resource retrieval.<ref name=":1" /><ref name=":2" /> Metadata structures, including controlled vocabularies, reflect the [[Ontology (information science)|ontologies]] of the systems from which they were created. Often the processes through which cultural objects are described and categorized through metadata in museums do not reflect the perspectives of the maker communities.<ref name=":0" /><ref>{{Cite journal |last=Srinivasan |first=Ramesh |date=December 2006 |title=Indigenous, ethnic and cultural articulations of new media |url=http://ics.sagepub.com/content/9/4/497.abstract |journal=International Journal of Cultural Studies |volume=9 |issue=4 |doi=10.1177/1367877906069899}}</ref>

==== Museums and the Internet ====
Metadata has been instrumental in the creation of digital information systems and archives within museums, and has made it easier for museums to publish digital content online. This has enabled audiences who might not have had access to cultural objects due to geographic or economic barriers to have access to them.<ref name=":1" /> In the 2000s, as more museums have adopted archival standards and created intricate databases, discussions about [[Linked data|Linked Data]] between museum databases have come up in the museum, archival and library science communities.<ref name=":3" /> Collection Management Systems (CMS) and [[Digital asset management|Digital Asset Management]] tools can be local or shared systems.<ref name=":2" /> [[Digital humanities|Digital Humanities]] scholars note many benefits of interoperability between museum databases and collections, while also acknowledging the difficulties achieving such interoperability.<ref name=":3" />

=== Law ===

==== United States of America ====
{{Globalize
|date=March 2015
}}
Problems involving metadata in [[litigation]] in the [[United States]] are becoming widespread.{{when|date=February 2011}} Courts have looked at various questions involving metadata, including the [[discovery (law)|discoverability]] of metadata by parties. Although the Federal Rules of Civil Procedure have only specified rules about electronic documents, subsequent case law has elaborated on the requirement of parties to reveal metadata.<ref>{{Cite journal
  | last = Gelzer | first = Reed D.
  | title = Metadata, Law, and the Real World: Slowly, the Three Are Merging
  | journal = Journal of AHIMA
  | volume = 79
  | issue = 2
  | pages = 56–57, 64
  | publisher = American Health Information Management Association
  | date = February 2008
  | url = http://library.ahima.org/xpedio/groups/public/documents/ahima/bok1_036537.hcsp?dDocName=bok1_036537
  | accessdate = 8 January 2010}}</ref> In October 2009, the [[Arizona Supreme Court]] has ruled that metadata records are [[public record]].<ref>{{Cite news
  | last = Walsh | first = Jim
  | title = Ariz. Supreme Court rules electronic data is public record
  | newspaper = The Arizona Republic
  | location = Phoenix, Arizona
  | date = 30 October 2009
  | url = http://www.azcentral.com/arizonarepublic/local/articles/2009/10/30/20091030metadata1030.html
  | accessdate = 8 January 2010
}}</ref> Document metadata have proven particularly important in legal environments in which litigation has requested metadata, which can include sensitive information detrimental to a certain party in court. Using [[metadata removal tool]]s to "clean" or redact documents can mitigate the risks of unwittingly sending sensitive data. This process partially (see [[data remanence]]) protects law firms from potentially damaging leaking of sensitive data through [[electronic discovery]].

====Australia====

In Australia the need to strengthen national security has resulted in the introduction of a new metadata storage law.<ref>Senate passes controversial metadata laws</ref> This new law means that both security and policing agencies will be allowed to access up to two years of an individual's metadata, supposedly to make it easier to stop any terrorist attacks and serious crimes from happening. In the 2000s, the law does not allow access to content of people's messages, phone calls or email and web-browsing history, but these provisions could be changed by the government.

=== In healthcare ===
Australian medical research pioneered the definition of metadata for applications in health care. That approach offers the first recognized attempt to adhere to international standards in medical sciences instead of defining a proprietary standard under the [[World Health Organization]] (WHO) umbrella. The medical community yet did not approve the need to follow metadata standards despite research that supported these standards.<ref>M. Löbe, M. Knuth, R. Mücke [http://ceur-ws.org/Vol-559/Paper1.pdf TIM: A Semantic Web Application for the Specification of Metadata Items in Clinical Research], CEUR-WS.org, urn:nbn:de:0074-559-9</ref>

=== Data warehousing ===
[[Data warehouse]] (DW) is a repository of an organization's electronically stored data. Data warehouses are designed to manage and store the data. Data warehouses differ from [[business intelligence]] (BI) systems, because BI systems are designed to use data to create reports and analyze the information, to provide strategic guidance to management.<ref>Inmon, W.H. Tech Topic: What is a Data Warehouse? Prism Solutions. Volume 1. 1995.</ref> Metadata is an important tool in how data is stored in data warehouses. The purpose of a data warehouse is to house standardized, structured, consistent, integrated, correct, "cleaned" and timely data, extracted from various operational systems in an organization. The extracted data are integrated in the data warehouse environment to provide an enterprise-wide perspective. Data are structured in a way to serve the reporting and analytic requirements. The design of structural metadata commonality using a [[data modeling]] method such as [[entity relationship model]] diagramming is important in any data warehouse development effort. They detail metadata on each piece of data in the data warehouse. An essential component of a [[data warehouse]]/[[business intelligence]] system is the metadata and tools to manage and retrieve the metadata. [[Ralph Kimball]]<ref>{{Cite book
  |last=Kimball |first=Ralph
  |authorlink=Ralph Kimball
  |title=The Data Warehouse Lifecycle Toolkit
  |edition=Second
  |location=New York |publisher=Wiley
  |year=2008
  |isbn=978-0-470-14977-5
  |ref=harv
  |pages=10, 115–117, 131–132, 140, 154–155
}}</ref>{{page needed|date=November 2016}} describes metadata as the DNA of the data warehouse as metadata defines the elements of the [[data warehouse]] and how they work together.

[[Ralph Kimball|Kimball]] et al.<ref>{{harvnb|Kimball|2008|pages=116–117}}</ref> refers to three main categories of metadata: Technical metadata, business metadata and process metadata. Technical metadata is primarily [[definitional]], while business metadata and process metadata is primarily descriptive. The categories sometimes overlap.
* '''Technical metadata''' defines the objects and processes in a DW/BI system, as seen from a technical point of view. The technical metadata includes the system metadata, which defines the data structures such as tables, fields, data types, indexes and partitions in the relational engine, as well as databases, dimensions, measures, and data mining models. Technical metadata defines the data model and the way it is displayed for the users, with the reports, schedules, distribution lists, and user security rights.
* '''Business metadata''' is content from the data warehouse described in more user-friendly terms. The business metadata tells you what data you have, where they come from, what they mean and what their relationship is to other data in the data warehouse. Business metadata may also serve as a documentation for the DW/BI system. Users who browse the data warehouse are primarily viewing the business metadata.
* '''Process metadata''' is used to describe the results of various operations in the data warehouse. Within the [[Extract, transform, load|ETL]] process, all key data from tasks is logged on execution. This includes start time, end time, CPU seconds used, disk reads, disk writes, and rows processed. When troubleshooting the ETL or [[Information retrieval|query]] process, this sort of data becomes valuable. Process metadata is the fact measurement when building and using a DW/BI system. Some organizations make a living out of collecting and selling this sort of data to companies - in that case the process metadata becomes the business metadata for the fact and dimension tables. Collecting process metadata is in the interest of business people who can use the data to identify the users of their products, which products they are using, and what level of service they are receiving.

=== On the Internet ===
The [[HTML]] format used to define web pages allows for the inclusion of a variety of types of metadata, from basic descriptive text, dates and keywords to further advanced metadata schemes such as the [[Dublin Core]], [[e-GMS]], and AGLS<ref>National Archives of Australia, AGLS Metadata Standard, accessed 7 January 2010, [http://www.naa.gov.au/records-management/create-capture-describe/describe/AGLS/index.aspx]</ref> standards. Pages can also be [[geotagging|geotagged]] with [[Geographic coordinate system|coordinates]]. Metadata may be included in the page's header or in a separate file. [[Microformat]]s allow metadata to be added to on-page data in a way that regular web users do not see, but computers, [[web crawler]]s and [[search engine]]s can readily access. Many search engines are cautious about using metadata in their ranking algorithms due to exploitation of metadata and the practice of search engine optimization, [[Search engine optimization|SEO]], to improve rankings. See [[Meta element]] article for further discussion. This cautious attitude may be justified as people, according to Doctorow,<ref>Metacrap: Putting the torch to seven straw-men of the meta-utopia http://www.well.com/~doctorow/metacrap.htm</ref> are not executing care and diligence when creating their own metadata and that metadata is part of a competitive environment where the metadata is used to promote the metadata creators own purposes. Studies show that search engines respond to web pages with metadata implementations,<ref>The impact of webpage content characteristics on webpage visibility in search engine results http://web.simmons.edu/~braun/467/part_1.pdf</ref> and Google has an announcement on its site showing the meta tags that its search engine understands.<ref>{{cite web|url=https://support.google.com/webmasters/answer/79812?hl=en/ |title=Meta tags that Google understands |publisher=Google.com |accessdate=2014-05-22}}</ref> Enterprise search startup [[Swiftype]] recognizes metadata as a relevance signal that webmasters can implement for their website-specific search engine, even releasing their own extension, known as Meta Tags 2.<ref>{{Cite web|url = https://swiftype.com/documentation/meta_tags2|title = Swiftype-specific Meta Tags |work=Swiftype Documentation |publisher=Swiftype |date = 3 October 2014 }}</ref>

=== In broadcast industry ===
In [[broadcast]] industry, metadata is linked to audio and video [[broadcast media]] to:
* ''identify'' the media: [[Media clip|clip]] or [[playlist]] names, duration, [[timecode]], etc.
* ''describe'' the content: notes regarding the quality of video content, rating, description (for example, during a sport event, [[Index term|keywords]] like ''goal'', ''red card'' will be associated to some clips)
* ''classify'' media: metadata allows to sort the media or to easily and quickly find a video content (a [[TV news]] could urgently need some [[archiving|archive content]] for a subject). For example, the BBC have a large subject classification system, [[Lonclass]], a customized version of the more general-purpose [[Universal Decimal Classification]].

This metadata can be linked to the video media thanks to the [[Video server#Broadcast automation|video servers]]. Most major broadcast sport events like [[FIFA World Cup]] or the [[Olympic Games]] use this metadata to distribute their video content to [[TV station]]s through [[Index term|keywords]]. It is often the host broadcaster<ref>{{cite web|url=http://www.hbs.tv/hostbroadcasting/ |title=HBS is the FIFA host broadcaster |publisher=Hbs.tv |date=2011-08-06 |accessdate=2011-12-23}}</ref> who is in charge of organizing metadata through its ''International Broadcast Centre'' and its video servers. This metadata is recorded with the images and are entered by metadata operators (''loggers'') who associate in live metadata available in ''metadata grids'' through [[software]] (such as [[Multicam(LSM)]] or [[IPDirector]] used during the FIFA World Cup or Olympic Games).<ref>{{cite web|url=http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |title=Host Broadcast Media Server and Related Applications |format=PDF |accessdate=2013-08-17 |deadurl=yes |archiveurl=https://web.archive.org/web/20111102235256/http://www.evs-global.com/01/MyDocuments/CS_BOB_EVScontributon_0808_ENG.pdf |archivedate=2 November 2011 }}</ref><ref>{{cite web|url=http://broadcastengineering.com/worldcup/fifa-world-cup-techonlogy-0610/ |title=logs during sport events |publisher=Broadcastengineering.com |accessdate=2011-12-23}}</ref>

=== Geospatial ===
Metadata that describes geographic objects in electronic storage or format (such as datasets, maps, features, or documents with a geospatial component) has a history dating back to at least 1994 (refer [http://libraries.mit.edu/guides/subjects/metadata/standards/fgdc.html MIT Library page on FGDC Metadata]). This class of metadata is described more fully on the [[geospatial metadata]] article.

=== Ecological and environmental ===
Ecological and environmental metadata is intended to document the "who, what, when, where, why, and how" of data collection for a particular study. This typically means which organization or institution collected the data, what type of data, which date(s) the data was collected, the rationale for the data collection, and the methodology used for the data collection. Metadata should be generated in a format commonly used by the most relevant science community, such as [[Darwin Core]], [[Ecological Metadata Language]],<ref>[http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html ] {{webarchive |url=https://web.archive.org/web/20110423161141/http://knb.ecoinformatics.org/software/eml/eml-2.0.1/index.html |date=23 April 2011 }}</ref> or [[Dublin Core]]. Metadata editing tools exist to facilitate metadata generation (e.g. Metavist,<ref>{{cite web|url=http://metavist.djames.net/ |title=Metavist 2 |publisher=Metavist.djames.net |accessdate=2011-12-23}}</ref> [[Mercury: Metadata Search System]], Morpho<ref>{{cite web|url=http://knb.ecoinformatics.org/morphoportal.jsp |title=KNB Data :: Morpho |publisher=Knb.ecoinformatics.org |date=2009-05-20 |accessdate=2011-12-23}}</ref>). Metadata should describe [[data provenance|provenance]] of the data (where they originated, as well as any transformations the data underwent) and how to give credit for (cite) the data products.

=== Digital music ===
When first released in 1982, Compact Discs only contained a Table Of Contents (TOC) with the number of tracks on the disc and their length in samples.[http://s3.amazonaws.com/academia.edu.documents/32801641/Morris_2012_-_Making_Music_Behave.pdf?AWSAccessKeyId=AKIAJ56TQJRTWSMTNPEA&Expires=1477195681&Signature=2TLmhapcR0M5eYsfMQ8FgG2TZa0%3D&response-content-disposition=inline%3B%20filename%3DMaking_music_behave_Metadata_and_the_dig.pdf][https://books.google.com/books?id=GkIaGZ0HWcMC&pg=PA48&source=gbs_toc_r&cad=4#v=onepage&q&f=false] Fourteen years later in 1996, a revision of the [[Compact Disc Digital Audio|CD Red Book]] standard added [[CD-Text]] to carry additional metadata.[http://web.ncf.ca/aa571/cdtext.htm] But CD-Text was not widely adopted. Shortly thereafter, it became common for personal computers to retrieve metadata from external sources (e.g. [[CDDB]], [[Gracenote]]) based on the TOC.

Digital [[Sound recording and reproduction|audio]] formats such as [[digital audio file]]s superseded music formats such as [[cassette tape]]s and [[CDs]] in the 2000s. Digital audio files could be labelled with more information than could be contained in just the file name. That descriptive information is called the '''audio tag''' or audio metadata in general. Computer programs specializing in adding or modifying this information are called [[tag editor]]s. Metadata can be used to name, describe, catalogue and indicate ownership or copyright for a digital audio file, and its presence makes it much easier to locate a specific audio file within a group, typically through use of a search engine that accesses the metadata. As different digital audio formats were developed, attempts were made to standardize a specific location within the digital files where this information could be stored.

As a result, almost all digital audio formats, including [[mp3]], broadcast wav and [[AIFF]] files, have similar standardized locations that can be populated with metadata. The metadata for compressed and uncompressed digital music is often encoded in the [[ID3]] tag. Common editors such as [[TagLib]] support MP3, Ogg Vorbis, FLAC, MPC, Speex, WavPack TrueAudio, WAV, AIFF, MP4, and ASF file formats.

=== Cloud applications ===
With the availability of [[Cloud computing|Cloud]] applications, which include those to add metadata to content, metadata is increasingly available over the Internet.

== Administration and management ==

=== Storage ===
Metadata can be stored either ''internally'',<ref name=id3>{{cite web
| first=Dan |last=O'Neill
| url=http://id3.org
| title=ID3.org
}}</ref> in the same file or structure as the data (this is also called ''embedded metadata''), or ''externally'', in a separate file or field from the described data. A data repository typically stores the metadata ''detached'' from the data, but can be designed to support embedded metadata approaches. Each option has advantages and disadvantages:
* Internal storage means metadata always travels as part of the data they describe; thus, metadata is always available with the data, and can be manipulated locally. This method creates redundancy (precluding normalization), and does not allow managing all of a system's metadata in one place. It arguably increases consistency, since the metadata is readily changed whenever the data is changed.
* External storage allows collocating metadata for all the contents, for example in a database, for more efficient searching and management. Redundancy can be avoided by normalizing the metadata's organization. In this approach, metadata can be united with the content when information is transferred, for example in [[Streaming media]]; or can be referenced (for example, as a web link) from the transferred content. On the down side, the division of the metadata from the data content, especially in standalone files that refer to their source metadata elsewhere, increases the opportunities for misalignments between the two, as changes to either may not be reflected in the other.

Metadata can be stored in either human-readable or binary form. Storing metadata in a human-readable format such as [[XML]] can be useful because users can understand and edit it without specialized tools.<ref name=Sutter>{{Cite book
|first1=Robbie
|last1=De Sutter
|first2=Stijn
|last2=Notebaert
|first3=Rik
|last3=Van de Walle
|chapter=Evaluation of Metadata Standards in the Context of Digital Audio-Visual Libraries
|title=Research and Advanced Technology for Digital Libraries: 10th European Conference, EDCL 2006
|editor1-last=Gonzalo
|editor1-first=Julio
|editor2-last=Thanos
|editor2-first=Constantino
|editor3-last=Verdejo
|editor3-first=M. Felisa
|editor4-last=Carrasco
|editor4-first=Rafael
|date=September 2006
|url =https://books.google.com/books?id=kU7Lqqowp54C&pg=PA226&cad=4#v=onepage
|isbn= 978-3540446361
|publisher=Springer
|page=226
}}</ref> However, text-based formats are rarely optimized for storage capacity, communication time, or processing speed. A binary metadata format enables efficiency in all these respects, but requires special software to convert the binary information into human-readable content.

=== Database management ===
Each [[relational database]] system has its own mechanisms for storing metadata. Examples of relational-database metadata include:
* Tables of all tables in a database, their names, sizes, and number of rows in each table.
* Tables of columns in each database, what tables they are used in, and the type of data stored in each column.
In database terminology, this set of metadata is referred to as the [[database catalog|catalog]]. The [[SQL]] standard specifies a uniform means to access the catalog, called the [[information schema]], but not all databases implement it, even if they implement other aspects of the SQL standard. For an example of database-specific metadata access methods, see [[Oracle metadata]]. Programmatic access to metadata is possible using APIs such as [[JDBC]], or SchemaCrawler.<ref name=schemacrawler>{{cite web
| author=Sualeh Fatehi
| url=http://schemacrawler.sourceforge.net/
| title=SchemaCrawler
| work=SourceForge
}}</ref>

== See also ==
{{Div col||25em}}
* [[Agris: International Information System for the Agricultural Sciences and Technology]]
* [[Classification scheme]]
* [[Crosswalk (metadata)]]
* [[DataONE]]
* [[Data Dictionary]] (aka metadata repository)
* [[Dublin Core]]
* [[Folksonomy]]
* [[GEOMS – Generic Earth Observation Metadata Standard]]
* [[Geospatial metadata]]
* [[IPDirector]]
* [[ISO/IEC 11179]]
* [[Knowledge tag]]
* [[Mercury: Metadata Search System]]
* [[Meta element]]
* [[IF-MAP|Metadata Access Point Interface]]
* [[Metadata discovery]]
* [[Metadata facility for Java]]
* [[v:4-b: Metadata|Metadata from Wikiversity]]
* [[Metadata publishing]]
* [[Metadata registry]]
* [[Metamathematics]]
* [[METAFOR]] Common Metadata for Climate Modelling Digital Repositories
* [[Microcontent]]
* [[Microformat]]
* [[Multicam (LSM)]]
* [[Observations and Measurements]]
* [[Ontology (computer science)]]
* [[Official statistics]]
* [[Paratext]]
* [[Preservation Metadata]]
* [[SDMX]]
* [[Semantic Web]]
* [[SGML]]
* [[The Metadata Company]]
* [[Universal Data Element Framework]]
* [[Vocabulary OneSource]]
* [[XSD]]
{{Div col end}}

== References ==
{{Reflist|colwidth=30em}}

== External links ==
{{Wiktionary|metadata}}
* [http://www.niso.org/apps/group_public/download.php/17446/Understanding%20Metadata ''Understanding Metadata: What is metadata, and what is it for?''] — [[NISO]], 2017
* [http://web.archive.org/web/20140522165110/http://www.theguardian.com/technology/interactive/2013/jun/12/what-is-metadata-nsa-surveillance#meta=1111111 "A Guardian guide to your metadata"] — ''[[The Guardian]]'', Wednesday 12 June 2013.
* [http://www.well.com/~doctorow/metacrap.htm Metacrap: Putting the torch to seven straw-men of the meta-utopia] — [[Cory Doctorow]]'s opinion on the limitations of metadata on the [[Internet]], 2001
* [http://www.dataone.org DataONE] Investigator Toolkit
* [http://www.informaworld.com/openurl?genre=journal&issn=1938-6389 ''Journal of Library Metadata''], Routledge, Taylor & Francis Group, ISSN 1937-5034
* [http://www.inderscience.com/ijmso ''International Journal of Metadata, Semantics and Ontologies'' (''IJMSO'')], Inderscience Publishers, ISSN 1744-263X
* {{webarchive |url=https://web.archive.org/web/20130126101115/http://www.metalounge.org/_literature_52579/Stephen_Machin_%E2%80%93_ON_METADATA_AND_METACONTENT |date=26 January 2013 |title=Metadata and metacontent }} (PDF, archived version)

{{Software engineering}}
{{Data warehouse}}

{{Authority control}}

[[Category:Data management]]
[[Category:Records management]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata| ]]
[[Category:Technical communication]]
[[Category:Business intelligence]]
<=====doc_Id=====>:368
<=====title=====>:
Business intelligence
<=====text=====>:
{{Use dmy dates|date=March 2012}}

{{Business administration}}

'''Business Intelligence''' ('''BI''') are the set of strategies, processes, [[application software|applications]], [[data]], products, technologies and technical architectures which are used to support the collection, analysis, presentation and dissemination of business information.<ref>Dedić N. & Stanier C. (2016). Measuring the Success of Changes to Existing Business Intelligence Solutions to Improve Business Intelligence Reporting. Lecture Notes in Business Information Processing. Springer International Publishing. Volume 268, pp. 225-236.</ref> BI technologies provide historical, current and predictive views of business operations. Common functions of business intelligence technologies are [[Business reporting|reporting]], [[online analytical processing]], [[analytics]], [[data mining]], [[process mining]], [[complex event processing]], [[business performance management]], [[benchmarking]], [[text mining]], [[Predictive Analysis|predictive analytics]] and [[Prescriptive Analytics|prescriptive analytics]] and are capable of handling large amounts of structured and sometimes unstructured data to help identify, develop and otherwise create new strategic business opportunities. The goal is to allow for the easy interpretation of these [[big data]]. Identifying new opportunities and implementing an effective strategy based on insights can provide businesses with a competitive market advantage and long-term stability.<ref>({{cite book |title= Business Intelligence Success Factors: Tools for Aligning Your Business in the Global Economy |last= Rud|first= Olivia |year= 2009|publisher= Wiley & Sons|location= Hoboken, N.J|isbn= 978-0-470-39240-9 |page= |pages= |url= |accessdate=}})</ref>

Business intelligence can be used to support a wide range of business decisions ranging from operational to strategic. Basic operating decisions include product positioning or pricing. Strategic business decisions include priorities, goals and directions at the broadest level. In all cases, BI is most effective when it combines data derived from the market in which a company operates (external data) with data from company sources internal to the business such as financial and operations data (internal data). When combined, external and internal data can provide a more complete picture which, in effect, creates an "intelligence" that cannot be derived by any singular set of data.<ref>{{cite book| last1= Coker| first1= Frank| title= Pulse: Understanding the Vital Signs of Your Business| publisher= Ambient Light Publishing
| publication-date= 2014| pages= 41–42| isbn= 978-0-9893086-0-1}}</ref> Amongst myriad uses, business intelligence tools empower organisations to gain insight into new markets, assess demand and suitability of products and services for different market segments and gauge the impact of marketing efforts.<ref name=":0">Chugh, R & Grandhi, S 2013, ‘Why Business Intelligence? Significance of Business Intelligence tools and integrating BI governance with corporate governance’, International Journal of E-Entrepreneurship and Innovation, vol. 4, no.2, pp. 1-14. https://www.researchgate.net/publication/273861123_Why_Business_Intelligence_Significance_of_Business_Intelligence_Tools_and_Integrating_BI_Governance_with_Corporate_Governance</ref>

==Components==
Business intelligence is made up of an increasing number of components including:
* Multidimensional aggregation and allocation
* [[Denormalization]], tagging and standardization
* Realtime reporting with analytical alert
* A method of interfacing with [[unstructured data]] sources
* Group consolidation, budgeting and [[rolling forecast]]s
* [[Statistical inference]] and probabilistic simulation
* [[Key performance indicator]]s optimization
* Version control and process management
* Open item management

==History==
The earliest known use of the term "Business Intelligence" is in Richard Millar Devens’ in the ‘Cyclopædia of Commercial and Business Anecdotes’ from 1865. Devens used the term to describe how the banker, Sir Henry Furnese, gained profit by receiving and acting upon information about his environment, prior to his competitors. “''Throughout Holland, Flanders, France, and Germany, he maintained a complete and perfect train of business intelligence. The news of the many battles fought was thus received first by him, and the [[Siege of Namur (1695)|fall of Namur]] added to his profits, owing to his early receipt of the news''.” (Devens, (1865), p.&nbsp;210).  The ability to collect and react accordingly based on the information retrieved, an ability that Furnese excelled in, is today still at the very heart of BI.<ref name="Miller Devens">{{cite book|last=Miller Devens|first=Richard|title=Cyclopaedia of Commercial and Business Anecdotes; Comprising Interesting Reminiscences and Facts, Remarkable Traits and Humors of Merchants, Traders, Bankers Etc. in All Ages and Countries|url=https://books.google.dk/books?id=9MspAAAAYAAJ&pg=PA210&dq=%22business+intelligence%22&hl=en&ei=a5EPTdaRIsOWnAeVyYHQDg&sa=X&oi=book_result&ct=result&redir_esc=y#v=onepage&q=%22business%20intelligence%22&f=false|publisher=D. Appleton and company|accessdate=15 February 2014|page=210}}</ref>

In a 1958 article, [[IBM]] researcher [[Hans Peter Luhn]] used the term business intelligence. He employed the Webster's dictionary definition of intelligence: "the ability to apprehend the interrelationships of presented facts in such a way as to guide action towards a desired goal."<ref>
{{cite journal|url= http://www.research.ibm.com/journal/rd/024/ibmrd0204H.pdf|doi=10.1147/rd.24.0314|title= A Business Intelligence System|author=H P Luhn |authorlink= Hans Peter Luhn |year= 1958 |journal= IBM Journal|volume= 2|issue= 4|pages= 314}}
</ref>

Business intelligence as it is understood today is said to have evolved from the [[decision support system]]s (DSS) that began in the 1960s and developed throughout the mid-1980s. DSS originated in the computer-aided models created to assist with [[decision making]] and planning. From DSS, [[data warehouse]]s, [[Executive Information System]]s, [[Online analytical processing|OLAP]] and business intelligence came into focus beginning in the late 80s.

In 1989, Howard Dresner (later a [[Gartner]] analyst) proposed "business intelligence" as an umbrella term to describe "concepts and methods to improve business decision making by using fact-based support systems."<ref name=power>{{cite web |url= http://dssresources.com/history/dsshistory.html
|title= A Brief History of Decision Support Systems, version 4.0 |accessdate=10 July 2008
|author= D. J. Power |date= 10 March 2007|publisher= DSSResources.COM }}
</ref> It was not until the late 1990s that this usage was widespread.<ref>{{cite web |url=http://dssresources.com/history/dsshistory.html |title=A Brief History of Decision Support Systems |last=Power |first=D. J. |accessdate=1 November 2010 }}</ref>

Critics see BI as evolved from mere [[business reporting]] together with the advent of increasingly powerful and easy-to-use [[data analysis]] tools. In this respect it has also been criticized as a marketing buzzword in the context of the "[[big data]]" surge.<ref>{{cite web|title=Decoding big data buzzwords|year=2015|quote=BI refers to the approaches, tools, mechanisms that organizations can use to keep a finger on the pulse of their businesses. Also referred by unsexy versions -- “dashboarding”, “MIS” or “reporting.”|publisher=cio.com|url=http://www.cio.com/article/2919082/big-data/what-are-they-talking-about-decoding-big-data-buzzwords.html}}</ref>

==Data warehousing==
Often BI applications use data gathered from a [[data warehouse]] (DW) or from a [[data mart]], and the concepts of BI and DW sometimes combine as "'''BI/DW'''"<ref>
{{cite book
| last1                 = Golden
| first1                = Bernard
| title                 = Amazon Web Services For Dummies
| url                   = https://books.google.com/books?id=xSVwAAAAQBAJ
| series                = For dummies
| publisher             = John Wiley & Sons
| publication-date      = 2013
| page                  = 234
| isbn                  = 9781118652268
| accessdate            = 2014-07-06
| quote                 = [...] traditional business intelligence or data warehousing tools (the terms are used so interchangeably that they're often referred to as BI/DW) are extremely expensive [...]
}}
</ref>
or as "'''BIDW'''". A data warehouse contains a copy of analytical data that facilitates decision support. However, not all data warehouses serve for business intelligence, nor do all business intelligence applications require a data warehouse.

To distinguish between the concepts of business intelligence and data warehouses, [[Forrester Research]] defines business intelligence in one of two ways:

# Using a broad definition: "Business Intelligence is a set of methodologies, processes, architectures, and technologies that transform raw data into meaningful and useful information used to enable more effective strategic, tactical, and operational insights and decision-making."<ref>{{cite web |url=http://www.forrester.com/rb/Research/topic_overview_business_intelligence/q/id/39218/t/2 |title=Topic Overview: Business Intelligence |last=Evelson |first=Boris |date=21 November 2008}}</ref> Under this definition, business intelligence also includes technologies such as data integration, data quality, data warehousing, master-data management, text- and content-analytics, and many others that the market sometimes lumps into the "[[Information Management]]" segment. Therefore, Forrester refers to ''data preparation'' and ''data usage'' as two separate but closely linked segments of the business-intelligence architectural stack.
# Forrester defines the narrower business-intelligence market as, "...referring to just the top layers of the BI architectural stack such as reporting, analytics and [[Dashboards (management information systems)|dashboards]]."<ref>{{cite web
|url=http://blogs.forrester.com/boris_evelson/10-04-29-want_know_what_forresters_lead_data_analysts_are_thinking_about_bi_and_data_domain
|title=Want to know what Forrester's lead data analysts are thinking about BI and the data domain? |last=Evelson |first=Boris |date=29 April 2010}}</ref>

==Comparison with competitive intelligence==
Though the term business intelligence is sometimes a synonym for [[competitive intelligence]] (because they both support [[decision making]]), BI uses technologies, processes, and applications to analyze mostly internal, structured data and business processes while competitive intelligence gathers, analyzes and disseminates information with a topical focus on company competitors. If understood broadly, business intelligence can include the subset of competitive intelligence.<ref>{{cite web |url=http://blogs.forrester.com/james_kobielus/10-04-30-what%E2%80%99s_not_bi_oh_don%E2%80%99t_get_me_startedoops_too_latehere_goes |title=What’s Not BI? Oh, Don’t Get Me Started....Oops Too Late...Here Goes.... |last=Kobielus |first=James |date=30 April 2010 |quote=“Business” intelligence is a non-domain-specific catchall for all the types of analytic data that can be delivered to users in reports, dashboards, and the like. When you specify the subject domain for this intelligence, then you can refer to “competitive intelligence,” “market intelligence,” “social intelligence,” “financial intelligence,” “HR intelligence,” “supply chain intelligence,” and the like.}}</ref>

==Comparison with business analytics==
Business intelligence and [[business analytics]] are sometimes used interchangeably, but there are alternate definitions.<ref>{{cite web|url=http://timoelliott.com/blog/2011/03/business-analytics-vs-business-intelligence.html |title=Business Analytics vs Business Intelligence? |publisher=timoelliott.com |date=2011-03-09 |accessdate=2014-06-15}}</ref>  One definition contrasts the two, stating that the term business intelligence refers to collecting business data to find information primarily through asking questions, reporting, and online analytical processes. Business analytics, on the other hand, uses statistical and quantitative tools for explanatory and [[predictive modelling]].<ref>{{cite web|url=http://www.businessanalytics.com/difference-between-business-analytics-and-business-intelligence/ |title=Difference between Business Analytics and Business Intelligence |publisher=businessanalytics.com |date=2013-03-15 |accessdate=2014-06-15}}</ref>

In an alternate definition, [[Thomas H. Davenport|Thomas Davenport]], professor of information technology and management at [[Babson College]] argues that business intelligence should be divided into [[Information retrieval|querying]], [[Business reporting|reporting]], [[Online analytical processing]] (OLAP), an "alerts" tool, and business analytics. In this definition, business analytics is the subset of BI focusing on statistics, prediction, and optimization, rather than the reporting functionality.<ref>{{Cite interview |url=http://www.informationweek.com/news/software/bi/222200096 |title=Analytics at Work: Q&A with Tom Davenport |last=Henschen |first=Doug |date=4 January 2010}}</ref>

==Applications in an enterprise==
Business intelligence can be applied to the following business purposes, in order to drive business value.{{Citation needed|date=October 2010}}
# [[Measurement]]&nbsp;– program that creates a hierarchy of [[performance metrics]] (see also [[Metrics Reference Model]]) and [[benchmarking]] that informs business leaders about progress towards business goals ([[business process management]]).
# [[Analytics]]&nbsp;– program that builds quantitative processes for a business to arrive at optimal decisions and to perform business knowledge discovery. Frequently involves: [[data mining]], [[process mining]], [[statistical analysis]], [[predictive analytics]], [[predictive modeling]], [[business process modeling]], [[data lineage]], [[complex event processing]] and [[Prescriptive Analytics|prescriptive analytics]].
# [[Business reporting|Reporting]]/[[enterprise reporting]]&nbsp;– program that builds infrastructure for strategic reporting to serve the strategic management of a business, not operational reporting. Frequently involves [[data visualization]], [[executive information system]] and [[OLAP]].
# [[Collaboration]]/[[collaboration platform]]&nbsp;– program that gets different areas (both inside and outside the business) to work together through [[data sharing]] and [[electronic data interchange]].
# [[Knowledge management]]&nbsp;– program to make the company data-driven through strategies and practices to identify, create, represent, distribute, and enable adoption of insights and experiences that are true business knowledge. Knowledge management leads to [[learning management]] and [[regulatory compliance]].

In addition to the above, business intelligence can provide a pro-active approach, such as alert functionality that immediately notifies the end-user if certain conditions are met. For example, if some business metric exceeds a pre-defined threshold, the metric will be highlighted in standard reports, and the business analyst may be alerted via e-mail or another monitoring service. This end-to-end process requires data governance, which should be handled by the expert.{{Citation needed|date=January 2012}}

==Prioritization of projects==
It can be difficult to provide a positive business case for business intelligence initiatives, and often the projects must be prioritized through strategic initiatives. BI projects can attain higher prioritization within the organization if managers consider the following:
* As described by Kimball<ref>Kimball et al., 2008: 29</ref> the BI manager must determine the tangible benefits such as eliminated cost of producing legacy reports.
* Data access for the entire organization must be enforced.<ref>{{cite web|url= http://content.dell.com/us/en/enterprise/d/large-business/ready-business-intelligence.aspx|title= Are You Ready for the New Business Intelligence?|publisher=Dell.com | accessdate=19 June 2012}}</ref> In this way even a small benefit, such as a few minutes saved, makes a difference when multiplied by the number of employees in the entire organization.
* As described by Ross, Weil & Roberson for Enterprise Architecture,<ref>[[Jeanne W. Ross]], [[Peter Weill]], [[David C. Robertson]] (2006) ''Enterprise Architecture As Strategy'', p. 117 ISBN 1-59139-839-8.</ref> managers should also consider letting the BI project be driven by other business initiatives with excellent business cases. To support this approach, the organization must have enterprise architects who can identify suitable business projects.
* Using a structured and quantitative methodology to create defensible prioritization in line with the actual needs of the organization, such as a weighted decision matrix.<ref>{{cite web|last=Krapohl|first=Donald|title=A Structured Methodology for Group Decision Making|url=http://www.augmentedintel.com/wordpress/index.php/a-structured-methodology-for-group-decision-making/|publisher=AugmentedIntel|accessdate=22 April 2013}}</ref>

==Success factors of implementation==
According to Kimball et al., there are three critical areas that organizations should assess before getting ready to do a BI project:<ref>Kimball et al. 2008: p. 298</ref>

# The level of commitment and sponsorship of the project from senior management.
# The level of business need for creating a BI implementation.
# The amount and quality of business data available.

===Business sponsorship===

The commitment and [[:wikt:sponsor|sponsor]]ship of senior management is according to Kimball ''et al.'', the most important criteria for assessment.<ref>Kimball et al., 2008: 16</ref> This is because having strong management backing helps overcome shortcomings elsewhere in the project. However, as Kimball ''et al.'' state: “even the most elegantly designed DW/BI system cannot overcome a lack of business [management] sponsorship”.<ref>Kimball et al., 2008: 18</ref>

It is important that personnel who participate in the project have a vision and an idea of the benefits and drawbacks of implementing a BI system. The best business sponsor should have organizational clout and should be well connected within the organization. It is ideal that the business sponsor is demanding but also able to be realistic and supportive if the implementation runs into delays or drawbacks. The management sponsor also needs to be able to assume accountability and to take responsibility for failures and setbacks on the project. Support from multiple members of the management ensures the project does not fail if one person leaves the steering group. However, having many managers work together on the project can also mean that there are several different interests that attempt to pull the project in different directions, such as if different departments want to put more emphasis on their usage. This issue can be countered by an early and specific analysis of the business areas that benefit the most from the implementation. All stakeholders in the project should participate in this analysis in order for them to feel invested in the project and to find common ground.

Another management problem that may be encountered before the start of an implementation is an overly aggressive business sponsor. Problems of [[scope creep]] occur when the sponsor requests data sets that were not specified in the original planning phase.

===Business needs===

Because of the close relationship with senior management, another critical thing that must be assessed before the project begins is whether or not there is a business need and whether there is a clear business benefit by doing the implementation.<ref name="Kimball et al., 2008: 17">Kimball et al., 2008: 17</ref>
The needs and benefits of the implementation are sometimes driven by competition and the need to gain an advantage in the market. Another reason for a business-driven approach to implementation of BI is the acquisition of other organizations that enlarge the original organization it can sometimes be beneficial to implement DW or BI in order to create more oversight.

Companies that implement BI are often large, multinational organizations with diverse subsidiaries.<ref>{{cite web|title=How Companies Are Implementing Business Intelligence Competency Centers |url=http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |publisher=Computer World |deadurl=yes |accessdate=1 April 2014 |archiveurl=https://web.archive.org/web/20130528054421/http://www.computerworld.com/pdfs/SAS_Intel_BICC.pdf |archivedate=28 May 2013 }}</ref> A well-designed BI solution provides a consolidated view of key business data not available anywhere else in the organization, giving management visibility and control over measures that otherwise would not exist.

===Amount and quality of available data===

Without proper data, or with too little quality data, any BI implementation fails; it does not matter how good the management sponsorship or business-driven motivation is. Before implementation it is a good idea to do [[data profiling]]. This analysis identifies the “content, consistency and structure [..]”<ref name="Kimball et al., 2008: 17"/> of the data. This should be done as early as possible in the process and if the analysis shows that data is lacking, put the project on hold temporarily while the IT department figures out how to properly collect data.

When planning for business data and business intelligence requirements, it is always advisable to consider specific scenarios that apply to a particular organization, and then select the business intelligence features best suited for the scenario.

Often, scenarios revolve around distinct business processes, each built on one or more data sources. These sources are used by features that present that data as information to knowledge workers, who subsequently act on that information. The business needs of the organization for each business process adopted correspond to the essential steps of business intelligence. These essential steps of business intelligence include but are not limited to:
#Go through business data sources in order to collect needed data
#Convert business data to information and present appropriately
#Query and analyze data
#Act on the collected data
The '''quality aspect''' in business intelligence should cover all the process from the source data to the final reporting. At each step, the '''quality gates''' are different:
# Source Data:
#* Data Standardization: make data comparable (same unit, same pattern...)
#* [[Master data management|Master Data Management:]] unique referential
# [[Operational data store|Operational Data Store (ODS)]]:
#* [[Data cleansing|Data Cleansing:]] detect & correct inaccurate data
#* Data Profiling: check inappropriate value, null/empty
# [[Data warehouse]]:
#* Completeness: check that all expected data are loaded
#* [[Referential integrity]]: unique and existing referential over all sources
#* Consistency between sources: check consolidated data vs sources
# Reporting:
#* Uniqueness of indicators: only one share dictionary of indicators
#* Formula accuracy: local reporting formula should be avoided or checked

==User aspect==

Some considerations must be made in order to successfully integrate the usage of business intelligence systems in a company. Ultimately the BI system must be accepted and utilized by the users in order for it to add value to the organization.<ref name = kimball>Kimball</ref><ref name = swain>Swain Scheps ''Business Intelligence for Dummies'', 2008, ISBN 978-0-470-12723-0</ref> If the [[usability]] of the system is poor, the users may become frustrated and spend a considerable amount of time figuring out how to use the system or may not be able to really use the system. If the system does not add value to the users´ mission, they simply don't use it.<ref name = swain />

To increase user acceptance of a BI system, it can be advisable to consult business users at an early stage of the DW/BI lifecycle, for example at the requirements gathering phase.<ref name = kimball /> This can provide an insight into the [[business process]] and what the users need from the BI system. There are several methods for gathering this information, such as questionnaires and interview sessions.

When gathering the requirements from the business users, the local IT department should also be consulted in order to determine to which degree it is possible to fulfill the business's needs based on the available data.<ref name = kimball />

Taking a user-centered approach throughout the design and development stage may further increase the chance of rapid user adoption of the BI system.<ref name = swain />

Besides focusing on the user experience offered by the BI applications, it may also possibly motivate the users to utilize the system by adding an element of competition. Kimball<ref name = kimball /> suggests implementing a function on the Business Intelligence portal website where reports on system usage can be found. By doing so, managers can see how well their departments are doing and compare themselves to others and this may spur them to encourage their staff to utilize the BI system even more.

In a 2007 article, H. J. Watson gives an example of how the competitive element can act as an incentive.<ref name = watson>{{cite journal|title=The Current State of Business Intelligence|year=2007|doi=10.1109/MC.2007.331|last1=Watson|first1=Hugh J.|last2=Wixom|first2=Barbara H.|journal=Computer|volume=40|issue=9|pages=96}}</ref> Watson describes how a large call centre implemented performance dashboards for all call agents, with monthly incentive bonuses tied to performance metrics. Also, agents could compare their performance to other team members. The implementation of this type of performance measurement and competition significantly improved agent performance.

BI chances of success can be improved by involving senior management to help make BI a part of the [[organizational culture]], and by providing the users with necessary tools, training, and support.<ref name = watson /> Training encourages more people to use the BI application.<ref name = kimball />

Providing user support is necessary to maintain the BI system and resolve user problems.<ref name = swain /> User support can be incorporated in many ways, for example by creating a website. The website should contain great content and tools for finding the necessary information. Furthermore, helpdesk support can be used. The help desk can be manned by power users or the DW/BI project team.<ref name = kimball />

==BI Portals==
A '''Business Intelligence portal''' (BI portal) is the primary access interface for [[Data warehouse|Data Warehouse]] (DW) and Business Intelligence (BI) applications. The BI portal is the user's first impression of the DW/BI system. It is typically a browser application, from which the user has access to all the individual services of the DW/BI system, reports and other analytical functionality.
The BI portal must be implemented in such a way that it is easy for the users of the DW/BI application to call on the functionality of the application.<ref name="Ralph">''The Data Warehouse Lifecycle Toolkit (2nd ed.). Ralph Kimball (2008).''</ref>

The BI portal's main functionality is to provide a navigation system of the DW/BI application. This means that the portal has to be implemented in a way that the user has access to all the functions of the DW/BI application.

The most common way to design the portal is to custom fit it to the business processes of the organization for which the DW/BI application is designed, in that way the portal can best fit the needs and requirements of its users.<ref name="Wiley">''Microsoft Data Warehouse Toolkit. Wiley Publishing. (2006)''</ref>

The BI portal needs to be easy to use and understand, and if possible have a look and feel similar to other applications or web content of the organization the DW/BI application is designed for ([[consistency]]).

The following is a list of desirable features for [[web portal]]s in general and BI portals in particular:

;Usable: User should easily find what they need in the BI tool.
;Content Rich: The portal is not just a report printing tool, it should contain more functionality such as advice, help, support information and documentation.
;Clean: The portal should be designed so it is easily understandable and not over-complex as to confuse the users
;Current: The portal should be updated regularly.
;Interactive: The portal should be implemented in a way that makes it easy for the user to use its functionality and encourage them to use the portal. Scalability and customization give the user the means to fit the portal to each user.
;Value Oriented: It is important that the user has the feeling that the DW/BI application is a valuable resource that is worth working on.

==Marketplace==
There are a number of business intelligence vendors, often categorized into the remaining independent "pure-play" vendors and consolidated "megavendors" that have entered the market through a recent trend<ref>{{cite news|url=http://www.zdnet.com/gartner-releases-2013-bi-magic-quadrant-7000011264/ |title=Gartner releases 2013 BI Magic Quadrant |publisher=ZDNet |author=Andrew Brust| date= 2013-02-14|accessdate=21 August 2013}}</ref> of acquisitions in the BI industry.<ref>{{cite web |url=http://www.bi-verdict.com/fileadmin/FreeAnalyses/consolidations.htm |title=Consolidations in the BI industry |date=7 March 2008 |last=Pendse |first=Nigel |work=The OLAP Report}}</ref> The business intelligence market is gradually growing. In 2012 business intelligence services brought in $13.1 billion in revenue.<ref>{{cite web|title=Why Business Intelligence Is Key For Competitive Advantage|url=https://cisonline.bu.edu/news-resources/why-business-intelligence-is-key-for-competitive-advantage/|website=Boston University|accessdate=23 October 2014}}</ref>

Some companies adopting BI software decide to pick and choose from different product offerings (best-of-breed) rather than purchase one comprehensive integrated solution (full-service).<ref>{{cite web |url=http://www.b-eye-network.com/view/2608 |title=Three Trends in Business Intelligence Technology |last=Imhoff |first=Claudia |date=4 April 2006}}</ref>

===Industry-specific===
Specific considerations for business intelligence systems have to be taken in some sectors such as [[Bank regulation|governmental banking regulations]] or healthcare.<ref>{{cite journal |vauthors=Mettler T, Vimarlund V |title=Understanding business intelligence in the context of healthcare |journal=Health Informatics Journal |volume=15 |issue=3 |pages=254–264 |year=2009 |doi=10.1177/1460458209337446 }}</ref> The information collected by banking institutions and analyzed with BI software must be protected from some groups or individuals, while being fully available to other groups or individuals. Therefore, BI solutions must be sensitive to those needs and be flexible enough to adapt to new regulations and changes to existing law.{{citation needed|date=May 2016}}

==Semi-structured or unstructured data==
Businesses create a huge amount of valuable information in the form of e-mails, memos, notes from call-centers, news, user groups, chats, reports, web-pages, presentations, image-files, video-files, and marketing material and news. According to Merrill Lynch, more than 85% of all business information exists in these forms. These information types are called either ''[[Semi-structured data|semi-structured]]'' or ''[[Unstructured data|unstructured]]'' data. However, organizations often only use these documents once.<ref name = rao>{{cite journal|doi=10.1109/MITP.2003.1254966 |url=http://www.ramanarao.com/papers/rao-itpro-2003-11.pdf|title=From unstructured data to actionable intelligence|year=2003|last1=Rao|first1=R.|journal=IT Professional|volume=5|issue=6|pages=29}}</ref>

The managements of semi-structured data is recognized as a major unsolved problem in the information technology industry.<ref name = blumberg>{{cite journal|url=http://soquelgroup.com/Articles/dmreview_0203_problem.pdf|author1=Blumberg, R.  |author2=S. Atre  |lastauthoramp=yes |title=The Problem with Unstructured Data|journal=DM Review |year=2003|pages=42–46}}</ref> According to projections from Gartner (2003), white collar workers spend anywhere from 30 to 40 percent of their time searching, finding and assessing unstructured data. BI uses both structured and unstructured data, but the former is easy to search, and the latter contains a large quantity of the information needed for analysis and decision making.<ref name = blumberg /><ref name = negash>{{cite journal|url=http://site.xavier.edu/sena/info600/businessintelligence.pdf|author=Negash, S |title=Business Intelligence|journal= Communications of the Association of Information Systems|volume=13|year= 2004|pages=177–195}}</ref> Because of the difficulty of properly searching, finding and assessing unstructured or semi-structured data, organizations may not draw upon these vast reservoirs of information, which could influence a particular decision, task or project. This can ultimately lead to poorly informed decision making.<ref name = rao />

Therefore, when designing a business intelligence/DW-solution, the specific problems associated with semi-structured and unstructured data must be accommodated for as well as those for the structured data.<ref name = negash />

===Unstructured data vs. semi-structured data===
Unstructured and semi-structured data have different meanings depending on their context. In the context of relational database systems, unstructured data cannot be stored in predictably ordered [[columns]] and [[Row (database)|rows]]. One type of unstructured data is typically stored in a [[BLOB]] (binary large object), a catch-all data type available in most [[relational database]] management systems. Unstructured data may also refer to irregularly or randomly repeated column patterns that vary from row to row within each file or document.{{citation needed|date=May 2016}}

Many of these data types, however, like e-mails, word processing text files, PPTs, image-files, and video-files conform to a standard that offers the possibility of metadata. Metadata can include information such as author and time of creation, and this can be stored in a relational database.
Therefore, it may be more accurate to talk about this as semi-structured documents or data,<ref name = blumberg /> but no specific consensus seems to have been reached.

Unstructured data can also simply be the knowledge that business users have about future business trends. Business forecasting naturally aligns with the BI system because business users think of their business in aggregate terms. Capturing the business knowledge that may only exist in the minds of business users provides some of the most important data points for a complete BI solution.

===Problems with semi-structured or unstructured data===
There are several challenges to developing BI with semi-structured data. According to Inmon & Nesavich,<ref name = inmon>Inmon, B. & A. Nesavich, "Unstructured Textual Data in the Organization" from "Managing Unstructured data in the organization", Prentice Hall 2008, pp. 1–13</ref> some of those are:

# Physically accessing unstructured textual data&nbsp;– unstructured data is stored in a huge variety of formats.
# [[Terminology]]&nbsp;– Among researchers and analysts, there is a need to develop a standardized terminology.
# Volume of data&nbsp;– As stated earlier, up to 85% of all data exists as semi-structured data. Couple that with the need for word-to-word and semantic analysis.
# Searchability of unstructured textual data&nbsp;– A simple search on some data, e.g. apple, results in links where there is a reference to that precise search term. (Inmon & Nesavich, 2008)<ref name = inmon /> gives an example: “a search is made on the term felony. In a simple search, the term felony is used, and everywhere there is a reference to felony, a hit to an unstructured document is made. But a simple search is crude. It does not find references to crime, arson, murder, embezzlement, vehicular homicide, and such, even though these crimes are types of felonies.”

===The use of metadata===
To solve problems with searchability and assessment of data, it is necessary to know something about the content. This can be done by adding context through the use of [[metadata]].<ref name = rao /> Many systems already capture some metadata (e.g. filename, author, size, etc.), but more useful would be metadata about the actual content&nbsp;– e.g. summaries, topics, people or companies mentioned. Two technologies designed for generating metadata about content are automatic categorization and [[information extraction]].

==2009 predictions==
A 2009 paper predicted<ref>[http://www.gartner.com/it/page.jsp?id=856714 Gartner Reveals Five Business Intelligence Predictions for 2009 and Beyond]. gartner.com. 15 January 2009</ref> these developments in the business intelligence market:
* Because of lack of information, processes, and tools, through 2012, more than 35 percent of the top 5,000 global companies regularly fail to make insightful decisions about significant changes in their business and markets.
* By 2012, business units will control at least 40 percent of the total budget for business intelligence.
* By 2012, one-third of analytic applications applied to business processes will be delivered through [[Granularity|coarse-grained]] application [[mashup (web application hybrid)|mashups]].
* BI has a huge scope in Entrepreneurship however majority of new entrepreneurs ignore its potential.<ref>[http://brighterkashmir.com/role-of-business-intelligence-in-entrepreneurship/ huge scope in Entrepreneurship]</ref>

A 2009 ''Information Management'' special report predicted the top BI trends: "[[green computing]], [[social networking service]]s, [[data visualization]], [[Mobile business intelligence|mobile BI]], [[predictive analytics]], [[composite application]]s, [[cloud computing]] and [[Multi-touch|multitouch]]".<ref>{{cite web |url=http://www.information-management.com/specialreports/2009_148/business_intelligence_data_vizualization_social_networking_analytics-10015628-1.html |title=10 Red Hot BI Trends |last=Campbell |first=Don |date=23 June 2009 |work=Information Management}}</ref> Research undertaken in 2014 indicated that employees are more likely to have access to, and more likely to engage with, cloud-based BI tools than traditional tools.<ref>{{cite web |url=http://www.aberdeen.com/Aberdeen-Library/8906/RR-analytics-cloud-saas-bi.aspx |title=Cloud Analytics in 2014: Infusing the Workforce with Insight |last=Lock|first=Michael|date=27 March 2014 }}</ref>

Other business intelligence trends include the following:

* Third party SOA-BI products increasingly address [[Extract, transform, load|ETL]] issues of volume and throughput.
* Companies embrace in-memory processing, 64-bit processing, and pre-packaged analytic BI applications.
* Operational applications have callable BI components, with improvements in response time, scaling, and concurrency.
* Near or real time BI analytics is a baseline expectation.
* Open source BI software replaces vendor offerings.

Other lines of research include the combined study of business intelligence and uncertain data.<ref>{{Cite journal
 | last=Rodriguez | first=Carlos
 | last2=Daniel | first2=Florian
 | last3=Casati | first3=Fabio
 | last4=Cappiello | first4=Cinzia
 | year=2010
 | title=Toward Uncertain Business Intelligence: The Case of Key Indicators |doi=10.1109/MIC.2010.59
 | journal=IEEE Internet Computing
 | volume=14
 | issue=4
 | pages=32 }}</ref><ref>{{citation |author=Rodriguez, C. |author2=Daniel, F. |author3=Casati, F. |author4=Cappiello, C. |last-author-amp=yes |url=http://mitiq.mit.edu/ICIQ/Documents/IQ%20Conference%202009/Papers/3-C.pdf |title=Computing Uncertain Key Indicators from Uncertain Data  |pages=106–120 | conference = ICIQ'09 | year = 2009}}</ref> In this context, the data used is not assumed to be precise, accurate and complete. Instead, data is considered uncertain and therefore this uncertainty is propagated to the results produced by BI.

According to a study by the Aberdeen Group, there has been increasing interest in [[Software-as-a-Service]] (SaaS) business intelligence over the past years, with twice as many organizations using this deployment approach as one year ago&nbsp;– 15% in 2009 compared to 7% in 2008.<ref>{{cite web|last1=Julian|first1=Taylor|title=Business intelligence implementation according to customer's needs|url=http://apro-software.com/services/software-development/business-intelligence|publisher=APRO Software|accessdate=16 May 2016|date=10 January 2010}}</ref>

An article by InfoWorld’s Chris Kanaracus points out similar growth data from research firm IDC, which predicts the SaaS BI market will grow 22 percent each year through 2013 thanks to increased product sophistication, strained IT budgets, and other factors.<ref>[http://infoworld.com/d/cloud-computing/saas-bi-growth-will-soar-in-2010-511 SaaS BI growth will soar in 2010 | Cloud Computing]. InfoWorld (2010-02-01). Retrieved 17 January 2012.</ref>

An analysis of top 100 Business Intelligence and Analytics scores and ranks the firms based on several open variables<ref>{{cite web|url=http://www.appsbi.com/top-100-analytics-companies-ranked-and-scored-by-mattermark|title=Top 100 analytics companies ranked and scored by Mattermark -  Business Intelligence - Dashboards - Big Data|publisher=}}</ref>

==See also==
{{colbegin|3}}
* [[Accounting intelligence]]
* [[Analytic applications]]
* [[Artificial intelligence marketing]]
* [[Business Intelligence 2.0]]
* [[Business process discovery]]
* [[Business process management]]
* [[Business activity monitoring]]
* [[Business service management]]
* [[Comparison of OLAP Servers]]
* [[Customer dynamics]]
* [[Data Presentation Architecture]]
* [[Data visualization]]
* [[Decision engineering]]
* [[Enterprise planning systems]]
* [[Infonomics]]
* [[Intelligent document|Document intelligence]]
* [[Integrated business planning]]
* [[Location intelligence]]
* [[Media intelligence]]
* [[Meteorological intelligence]]
* [[Mobile business intelligence]]
* [[Multiway Data Analysis]]
* [[Operational intelligence]]
* [[Business Information Systems]]
* [[Business intelligence tools]]
* [[Process mining]]
* [[Real-time business intelligence]]
* [[Runtime intelligence]]
* [[Sales intelligence]]
* [[Test and learn]]

{{colend}}

==References==
{{Reflist|30em}}

==Bibliography==
*Ralph Kimball ''et al.'' "The Data warehouse Lifecycle Toolkit" (2nd ed.) Wiley ISBN 0-470-47957-4
*Peter Rausch, Alaa Sheta, Aladdin Ayesh : ''Business Intelligence and Performance Management: Theory, Systems, and Industrial Applications'', Springer Verlag U.K., 2013, ISBN 978-1-4471-4865-4.

==External links==
* [http://online.sju.edu/resource/engineering-technology/key-role-hadoop-plays-in-business-intelligence "The Key Role Hadoop Plays in Business Intelligence and Data Warehousing" - St. Joseph's University]
* {{cite journal
|url=http://cacm.acm.org/magazines/2011/8/114953-an-overview-of-business-intelligence-technology/fulltext
|title=An Overview Of Business Intelligence Technology
|date=August 2011 | accessdate=26 October 2011
|first1=Surajit |last1=Chaudhuri |first2=Umeshwar |last2=Dayal |first3=Vivek |last3=Narasayya
|journal=Communications of the ACM
|volume =54 |issue= 8 |pages=88–98
|doi=10.1145/1978542.1978562 }}

{{Data warehouse}}

{{DEFAULTSORT:Business Intelligence}}
[[Category:Business intelligence| ]]
[[Category:Financial data analysis]]
[[Category:Data management]]
[[Category:Financial technology]]
[[Category:Information management]]
<=====doc_Id=====>:371
<=====title=====>:
Category:Computer logging
<=====text=====>:
[[Category:Computing]]
[[Category:Data management]]
<=====doc_Id=====>:374
<=====title=====>:
Data profiling
<=====text=====>:
{{multiple issues|
{{Expert needed|Mathematics|ex2=Science|talk=Section title goes here|reason=it needs additional citations|date=August 2016}}
{{refimprove article|date=August 2010}}
{{copy edit|for=Incorrect citation formatting|date=August 2016}}
}}

'''Data profiling''' is the process of examining data available from an existing information source (e.g. a [[database]] or a [[computer file|file]]) and collecting [[descriptive statistics|statistics]] or informative summaries about that data.<ref name="Johnson2009">[Theodore Johnson (2009), "Data Profiling", in Encyclopedia of Database Systems, Springer, Heidelberg]</ref> The purpose of these statistics may be to:
# Find out whether existing data can easily be used for other purposes
# Improve the ability to search data by [[tag (metadata)|tagging]] it with [[Index term|keywords]], descriptions, or assigning it to a category
# Give [[Software metric|metrics]] on [[data quality]], including whether the data conforms to particular standards or patterns
# Assess the risk involved in [[data integration|integrating data]] in new applications, including the challenges of [[Join (SQL)|join]]s
# Discover [[metadata]] of the source database, including value patterns and [[frequency distribution|distributions]], [[candidate key|key candidates]], [[inclusion dependency|foreign-key candidates]], and [[functional dependency|functional dependencies]]
# Assess whether known metadata accurately describes the actual values in the source database
# Understanding data challenges early in any data intensive project, so that late project surprises are avoided. Finding data problems late in the project can lead to delays and cost overruns.
# Have an enterprise view of all data, for uses such as [[master data management]], where key data is needed, or [[data governance]] for improving data quality.

== Introduction ==

Data profiling refers to the analysis of information for use in a [[data warehouse]] in order to clarify the structure, content, relationships, and derivation rules of the data.<ref name="Kimball2008">[Ralph Kimball et al. (2008), “The Data Warehouse Lifecycle Toolkit”, Second Edition, Wiley Publishing, Inc., ISBN 9780470149775], (p. 297) (p. 376)</ref> Profiling helps to not only understand anomalies and assess data quality, but also to discover, register, and assess enterprise metadata.<ref name="Loshin2009">[David Loshin (2009), “Master Data Management”, Morgan Kaufmann Publishers, ISBN 9780123742254], (pp. 94–96)</ref><ref name="Loshin2003">[David Loshin (2003), “Business Intelligence: The Savvy Manager’s Guide, Getting Onboard with Emerging IT”, Morgan Kaufmann Publishers, ISBN 9781558609167], (pp. 110–111)]</ref> The result of the analysis is used to determine the suitability of the candidate source systems, usually giving the basis for an early go/no-go decision, and also to identify problems for later solution design.<ref name="Kimball2008"/>

== How Data Profiling is Conducted ==

Data profiling utilizes methods of descriptive statistics such as minimum, maximum, mean, mode, percentile, standard deviation, frequency, variation, aggregates such as count and sum, and additional metadata information obtained during data profiling such as data type, length, discrete values, uniqueness, occurrence of null values, typical string patterns, and abstract type recognition.<ref name="Loshin2009"/><ref name="Rahm2000">[Erhard Rahm and Hong Hai Do (2000), “Data Cleaning: Problems and Current Approaches” in “Bulletin of the Technical Committee on Data Engineering”, IEEE Computer Society, Vol. 23, No. 4, December 2000]</ref><ref name="Singh2010">[Ranjit Singh, Dr Kawaljeet Singh et al. (2010), “A Descriptive Classification of Causes of Data Quality Problems in Data Warehousing”, IJCSI International Journal of Computer Science Issue, Vol. 7, Issue 3, No. 2, May 2010]</ref>
The metadata can then be used to discover problems such as illegal values, misspellings, missing values, varying value representation, and duplicates.

Different analyses are performed for different structural levels. E.g. single columns could be profiled individually to get an understanding of frequency distribution of different values, type, and use of each column. Embedded value dependencies can be exposed in a cross-columns analysis. Finally, overlapping value sets possibly representing foreign key relationships between entities can be explored in an inter-table analysis.<ref name="Loshin2009"/>

Normally, purpose-built tools are used for data profiling to ease the process.<ref name="Kimball2008"/><ref name="Loshin2009"/><ref name="Rahm2000"/><ref name="Singh2010"/><ref name="Kimball2004">"[Ralph Kimball (2004), “Kimball Design Tip #59: Surprising Value of Data Profiling”, Kimball Group, Number 59, September 14, 2004, (www.rkimball.com/html/designtipsPDF/ KimballDT59 SurprisingValue.pdf)]</ref><ref name="Olson2003">[Jack E. Olson (2003), “Data Quality: The Accuracy dimension”, Morgan Kaufmann Publishers], (pp. 140–142)</ref> The computation complexity increases when going from single column, to single table, to cross-table structural profiling. Therefore, performance is an evaluation criterion for profiling tools.<ref name="Loshin2003"/>

== When Data Profiling is Conducted ==

According to Kimball,<ref name="Kimball2008"/> data profiling is performed several times and with varying intensity throughout the data warehouse developing process. A light profiling assessment should be undertaken immediately after candidate source systems have been identified and DW/BI business requirements have been satisfied. The purpose of this initial analysis is to clarify at an early stage if the correct data is available at the appropriate detail level and that anomalies can be handled subsequently. If this is not the case the project may be terminated.<ref name="Kimball2008"/>

Addition, more in-depth profiling is done prior to the dimensional modeling process in order assess what is required to convert data into a dimensional model. Detailed profiling extends into the ETL system design process in order to determine the appropriate data to extract and which filters to apply to the data set.<ref name="Kimball2008"/>

Additionally, data may be conducted in the data warehouse development process after data has been loaded into staging, the data marts, etc. Conducting data at these stages helps ensure that data cleaning and transformations have been done correctly and in compliance of requirements.

==Benefits==

The benefits of data profiling are to improve data quality, shorten the implementation cycle of major projects, and improve users' understanding of data.<ref name="Olson2003"/> Discovering business knowledge embedded in data itself is one of the significant benefits derived from data profiling.<ref name="Loshin2003"/> Data profiling is one of the most effective technologies for improving data accuracy in corporate databases.<ref name="Olson2003"/>

==See also==
* [[Data quality]]
* [[Data governance]]
* [[Master data management]]
* [[Database normalization]]
* [[Data visualization]]
* [[Analysis paralysis]]

==References==
{{Reflist}}

{{DEFAULTSORT:Data Profiling}}
[[Category:Data management]]
[[Category:Data quality]]
[[Category:Data analysis]]
<=====doc_Id=====>:377
<=====title=====>:
Draft:Cloudiway
<=====text=====>:
{{New unreviewed article
| source = ArticleWizard
| date = January 2017
}}

{{Infobox company
 | name = Cloudiway
| logo = 
| type = [[Privately held company]]
| founder = Emmanuel Dreux
| area_served = Worldwide
| industry = [[Information technology]]<br/>[[Computer software]]<br/>[[Cloud computing]]<br>[[Email management]]
| products = Coexistence, mail migration, file migration, group migration, site migration, on-premises to cloud migration, mail archive migration, data migration
| foundation = 2010
| location_city = 
| location_country = 
| location = Annecy, France
| locations = 
| homepage = [http://www.cloudiway.com/ www.cloudiway.com]
}}

'''Cloudiway''' is an international [[Software as a service|SaaS]] company founded in 2010 in Annecy, France. The company builds cloud-based tools such as enterprise coexistence (free/busy calendar sharing, mail routing and automatic address list synchronisation), data migration to the cloud, and identity access management tools.

Cloudiway are currently the only company to offer enterprise coexistence between G Suite, Office 365 and Microsoft Exchange.

== Product history ==
After releasing CloudAnywhere, a locally-installed identity access management tool, Cloudiway began working on mail migration tools to enable migration between remote systems such as [[Gmail|G Mail]], [[Office 365]], [[Internet Message Access Protocol|IMAP]] and [[Microsoft Exchange Server|Exchange]]. Soon after, file migration and site migration utilities were launched, followed by enterprise coexistence in 2016<ref>{{Cite web|url=http://inpublic.globenewswire.com/releaseDetails.faces?rId=1999484|title=GlobeNewswire: Cloudiway is launching its solution for Coexistence between Google apps and Office 365/Exchange|website=inpublic.globenewswire.com|access-date=2017-01-26}}</ref>. Mail archive migration and group migration utilities were launched shortly after, also in 2016. 

With a stable set of utilities under its belt, Cloudiway introduced a wider variety of mail migration sources and destinations, including [[Lotus Notes Mail|Lotus Notes mail]], [[Zimbra]] and Amazon WorkMail. File migration, which had been launched with [[Google Drive]], [[OneDrive]] and [[SharePoint]], was expanded in early 2017 to include file systems (such as Windows servers), Azure [[Binary large object|blob]] storage and [[Amazon S3]] storage.

== Company history ==
Cloudiway is headed by Emmanuel Dreux, a [[Microsoft Most Valuable Professional]] (MVP), who worked at IBM/Lotus and [[Microsoft]] before working on CloudAnywhere — Cloudiway's first product. The company moved to new headquarters in 2016<ref>{{Cite news|url=http://www.finanznachrichten.de/nachrichten-2016-06/37631705-cloudiway-cloudiway-continues-to-expand-its-global-expansion-and-announces-new-location-in-miami-florida-399.htm|title=CLOUDIWAY: Cloudiway continues to expand its global expansion and announces New location in Miami, Florida|newspaper=FinanzNachrichten.de|language=de|access-date=2017-01-26}}</ref> to accommodate growing staff numbers. 

Cloudiway are a [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner].  

The company offers consulting services in English and French, with offices in Florida, USA and Annecy, France. Through their consulting services and partnerships, the company has provided cloud migration solutions to a broad range of businesses, including education, the public sector, small private businesses and global enterprises. 


== References ==
{{Reflist}}

== External links ==
* [http://www.cloudiway.com Official Cloudiway website]
* [https://partnercenter.microsoft.com/en-us/pcv/solution-providers/cloudiway_4298856859/861247_1 Microsoft Partner Microsoft Partner page]
* [http://www.distributique.com/actualites/lire-cloudiway-%C2%A0sur-le-cloud-le-marche-francais-n-etait-pas-pret%C2%A0-25414.html Article in French about Cloudiway]


[[Category:Data management]]
[[Category:Cloud computing]]
[[Category:As a service]]
[[Category:Software companies of France]]
<=====doc_Id=====>:380
<=====title=====>:
Category:Audio storage
<=====text=====>:
{{Commons category|Audio storage media}}
{{Cat main|Sound recording and reproduction}}

[[Category:Storage media]]
[[Category:Electronic documents]]
[[Category:Sound recording technology]]
[[Category:Sound production technology]]
<=====doc_Id=====>:383
<=====title=====>:
Document retrieval
<=====text=====>:
'''Document retrieval''' is defined as the matching of some stated user query against a set of [[free-text]] records. These records could be any type of mainly [[natural language|unstructured text]], such as [[newspaper article]]s, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.

Document retrieval is sometimes referred to as, or as a branch of, '''text retrieval'''. Text retrieval is a branch of [[information retrieval]] where the information is stored primarily in the form of [[natural language|text]]. Text databases became decentralized thanks to the [[personal computer]] and the [[CD-ROM]]. Text retrieval is a critical area of study today, since it is the fundamental basis of all [[internet]] [[search engine]]s.

==Description==
Document retrieval systems find information to given criteria by matching text records (''documents'') against user queries, as opposed to [[expert system]]s that answer questions by [[Inference|inferring]] over a logical [[knowledge base|knowledge database]]. A document retrieval system consists of a database of documents, a [[classification algorithm]] to build a full text index, and a user interface to access the database.

A document retrieval system has two main tasks:
# Find relevant documents to user queries
# Evaluate the matching results and sort them according to relevance, using algorithms such as [[PageRank]].

Internet [[search engines]] are classical applications of document retrieval. The vast majority of retrieval systems currently in use range from simple Boolean systems through to systems using [[statistical]] or [[natural language processing]] techniques.

==Variations==
There are two main classes of indexing schemata for document retrieval systems: ''form based'' (or ''word based''), and ''content based'' indexing. The document classification scheme (or [[Search engine indexing|indexing algorithm]]) in use determines the nature of the document retrieval system.

===Form based===
Form based document retrieval addresses the exact syntactic properties of a text, comparable to substring matching in string searches. The text is generally unstructured and not necessarily in a natural language, the system could for example be used to process large sets of chemical representations in molecular biology. A [[suffix tree]] algorithm is an example for form based indexing.

===Content based===
The content based approach exploits semantic connections between documents and parts thereof, and semantic connections between queries and documents. Most content based document retrieval systems use an [[inverted index]] algorithm.

A ''signature file'' is a technique that creates a ''quick and dirty'' filter, for example a [[Bloom filter]], that will keep all the documents that match to the query and ''hopefully'' a few ones that do not. The way this is done is by creating for each file a signature, typically a hash coded version. One method is superimposed coding. A post-processing step is done to discard the false alarms. Since in most cases this structure is inferior to [[inverted file]]s in terms of speed, size and functionality, it is not used widely. However, with proper parameters it can beat the inverted files in certain environments.

==Example: PubMed==
The [[PubMed]]<ref>{{cite journal |vauthors=Kim W, Aronson AR, Wilbur WJ |title=Automatic MeSH term assignment and quality assessment |journal=Proc AMIA Symp |pages=319–23 |year=2001 |pmid=11825203 |pmc=2243528 }}
</ref> form interface features the "related articles" search which works through a comparison of words from the documents' title, abstract, and [[Medical Subject Headings|MeSH]] terms using a word-weighted algorithm.<ref>{{cite web|url=https://www.ncbi.nlm.nih.gov/books/NBK3827/#pubmedhelp.Computation_of_Related_Citati|title=Computation of Related Citations}}</ref><ref>{{cite journal|journal=BMC Bioinformatics|date=Oct 30, 2007|volume=8|pages=423|pmid=17971238|title=PubMed related articles: a probabilistic topic-based model for content similarity|author=Lin J1, Wilbur WJ|doi=10.1186/1471-2105-8-423|pmc=2212667}}</ref>

== See also ==

* [[Compound term processing]]
* [[Document classification]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Latent semantic indexing]]
* [[Search engine]]

== References ==

<references/>

==Further reading==
* {{cite journal|first1=Christos|last1=Faloutsos|first2=Stavros|last2=Christodoulakis|title=Signature files: An access method for documents and its analytical performance evaluation|journal=ACM Transactions on Information Systems|volume=2|issue=4|year=1984|pages=267–288|doi=10.1145/2275.357411}}
* {{cite journal|author1=Justin Zobel |author2=Alistair Moffat |author3=Kotagiri Ramamohanarao |title=Inverted files versus signature files for text indexing|journal=ACM Transactions on Database Systems|volume=23|issue=4|year=1998|pages= 453–490|url=http://www.cs.columbia.edu/~gravano/Qual/Papers/19%20-%20Inverted%20files%20versus%20signature%20files%20for%20text%20indexing.pdf|doi=10.1145/296854.277632}}
* {{cite journal|author1=Ben Carterette |author2=Fazli Can |title=Comparing inverted files and signature files for searching a large lexicon|journal=Information Processing and Management|volume= 41|issue=3|year=2005|pages= 613–633|url=http://www.users.miamioh.edu/canf/papers/ipm04b.pdf|doi=10.1016/j.ipm.2003.12.003}}

== External links ==
* [http://cir.dcs.uni-pannon.hu/cikkek/FINAL_DOMINICH.pdf Formal Foundation of Information Retrieval], Buckinghamshire Chilterns University College

[[Category:Information retrieval genres]]
[[Category:Electronic documents]]
[[Category:Substring indices]]
[[Category:Search engine software]]

[[zh:文本信息检索]]
<=====doc_Id=====>:386
<=====title=====>:
Electronic Document Professional
<=====text=====>:
The '''EDP''' ('''Electronic Document Professional''') is a professional designation awarded by [[Xplor International]] for participants in the [[electronic document]] industry who have satisfied a number of criteria.

The EDP is not a measure of specific knowledge, but awardees normally have a broad knowledge of the electronic document industry along with specific knowledge in one or more areas. Thus, the EDP differs from certifications such as the [[CDIA+]] from CompTIA in which the awardee has passed a formal exam. Rather, the EDP is more closely related to the older MIT and LIT designations from [[AIIM]]. Generally, the EDP does show that the awardee has been in the industry for at least 5 years, and has participated in at least 3 major projects showing competence in a number of 'bodies of knowledge'.

The EDP program is regulated by the EDP Commission of [[Xplor International]], which is a body of senior professionals in the electronic document industry who set the standards and judge the qualifications of the applicants.

== History ==

The first EDP 'class' (as the annual group of awardees is called) was in 1990, when 12 industry professionals were given the award. In 1991, the class was much smaller (only 6), but in 1992 and in all the years following, the number of awardees has generally been double digits, with as many as 25 at one time. The Dutch Chapter of Xplor International has particularly stressed the EDP designation as an essential part of being a professional in the electronic document industry, with the result that in some years, more than ten Dutch members alone were named EDPs, and there are more EDPs per capita in The Netherlands today than in any other country.

In 2009, Xplor relaunched its certification program, so that there is now a three-level certification process to help employers benchmark their staff.<ref>http://www.printingnews.com/web/online/Industry-News/Xplor-Expands-EDP-Certification/1$10502</ref>

The EDP and the Master-EDP awards are presented once a year at the annual international conference of Xplor. While the first EDPs were awarded at the 1990 conference in Nashville, Tennessee (USA), the first 'class' of Master-EDPs was awarded at the association's annual event in St. Petersburg, Florida (USA) in March, 2010. At that event, the following industry professionals received the association's highest certification:<ref name="m-edp">[http://www.xplor.org/EDP/MEDPlist.cfm]</ref>
* William Broddy, M-EDP
* Ernie Crawford, M-EDP
* Scott Draeger, M-EDP
* Oscar Dubbeldam, M-EDP
* William J. "Bill" McCalpin, M-EDP, CDIA, MIT, LIT
* Walter Riddock, M-EDP, CMDSM
* Donald Scrima, M-EDP

At the association's next annual event in April, 2011, two more industry professionals will be named M-EDPs:<ref name="m-edp"/>
* Pat McGrew, M-EDP
* Carrie Murphy, M-EDP

On the other hand, the new EDA designation is awarded at the point that the individual's application is accepted and verified, throughout the year. Currently (December 2010), there are 48 industry professionals who have received this designation,<ref>http://www.xplor.org/EDP/EDA.cfm?viewpage=EDAlist</ref> nearly all of whom received the designation as a result of attending courses certified by Xplor and taught by [http://www.acadami.org ''acadami''], two of whose principals are M-EDPs.

== Levels of certification ==

=== Electronic Document Associate (EDA) ===
The EDA designation recognizes electronic document sales, development and support specialists who have shown significantly more knowledge of the industry than someone in another discipline.
It requires candidates to be in the industry for 2+ years and have successfully completed 5 days of Xplor Continuing Education Unit (CEU) certified courses, or the equivalent.<ref>http://www.xplor.org/EDP/EDA.cfm</ref>

=== Electronic Document Professional (EDP) ===
EDPs have clearly shown enough working knowledge of the process to make significant decisions regarding technology or process deployment. For example, management should trust them to lead projects, or support teams.
To become certified as an EDP, a candidate must be in the industry for 5+ years, have successfully completed 10 days of Xplor CEU training (or the equivalent), and have shown their working knowledge and experience through 3 work examples.<ref>http://www.xplor.org/EDP/EDP.cfm</ref>

=== Master Electronic Document Professional (M-EDP) ===
M-EDPs are the recognized experts on specific technologies, processes, or management skills. For example, an M-EDP may have co-developed a composition or print stream transform system. Another might be the expert on print costing, or statement design. By earning their M-EDP, they are clearly recognized as one of the ‘go to’ people in the industry.

To earn the M-EDP, a candidate must have been in the industry for at least 10 years, have been an EDP for at least 5 years, and be able to prove their area of expertise through published material.<ref>http://www.xplor.org/EDP/MEDP.cfm</ref>

== Designees ==
In all, there are more than 500 industry professionals with the EDP designation, including:
* [http://www.xplorcanada.org/media/eastern/Broddy-Bio-2005.pdf William Broddy, M-EDP] - formerly the Vice Chair of the EDP Commission
* [http://www.mccalpin.com William J. 'Bill' McCalpin, M-EDP] - author of ''The Document Dilemma'' and former General Manager of Xplor International 
* [http://www.acadami.org/about.html Dr. Michael Turton, EDP] - recognized expert in the design of transaction documents and the use of color in transaction documents
* Scott Kelly, EDP - current Xplor International board member
* [http://www.crawfordtech.com/ManagementTeam.htm Ernie Crawford, M-EDP] - President of Crawford Technologies
* [http://www.nautilussolutions.com Stephen Poe, EDP] - Principal at Nautilus Solutions
* [http://www.gmc.net Scott Draeger, M-EDP] - Vice President of Product at GMC Software
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Loic Avenel, EDP] - EMEA Product Marketing Manager at HP Exstream
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Olivier DOILLON, EDP] - EMEA South Presales Manager at HP Exstream
* Roberta McKee-Jackson, EDP - Principal at RSM Consulting and current Vice-Chair of EDP Commission
* Skip Henk, EDP - Current President/CEO of Xplor International
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Kent Lewis, EDP] - Product Manager, HP Exstream
* [http://welcome.hp.com/country/us/en/prodserv/software/eda/products/hpexstream.html Matt Riley, EDP] - Solution Support Manager, HP Exstream
* Donald A. Scrima, M-EDP - Current Chair of EDP Commission (2009-20120) and Principal at AFP Education & Consulting
* [http://www.deberichtenfabriek.nl/wie-wij-zijn/dick-paul-en-rene-joor René M. Joor, EDP] - Current member of the EDP Commission, Partner at De Berichtenfabriek BV, the Netherlands 
* Kenneth H. Pugh, EDP - Senior Output Analyst at Aon Corporation
* Kathy Rixham, EDP - Print System Administrator at RR Donnelley
* Kees van de Graaf, EDP - Consultant at PostNL
* James Shand, EDP - Xplor International member of +30-years, Current President Xplor UK & Ireland and member of the Xplor International Board of Directors

==References==
{{reflist}}

==External links==
* [http://www.xplor.org/ Xplor International Web Page]
* [http://www.xplor.org/edp/index.cfm Direct Link to Xplor International Certification Program]

[[Category:Electronic documents]]
<=====doc_Id=====>:389
<=====title=====>:
Category:Content management systems
<=====text=====>:
{{Category diffuse}}
{{Cat main|Content management system}}
{{Commons category}}

A '''[[content management system]]''' ('''CMS''') is a system used to organize and facilitate collaborative content creation. Recently, the term has been used specifically to refer to programs on [[WWW]] [[Web server|servers]], but it can also refer to hardware devices that manage documents on a large network.

[[Category:Web software]]
[[Category:Internet Protocol based network software]]
[[Category:Data management software]]
[[Category:Office software]]
[[Category:Electronic documents]]
<=====doc_Id=====>:392
<=====title=====>:
Electronic document
<=====text=====>:
{{Unreferenced|date=July 2016}}
[[File:Text-txt.svg|thumb|An example of a [[text file]] icon, one of the common representations of an electronic document.]]
An '''electronic document''' is any electronic media content (other than computer programs or system files) that are intended to be used in either an electronic form or as printed output.
Originally, any computer data were considered as something internal &mdash; the final data output was always on paper. However, the development of computer networks has made it so that in most cases it is much more convenient to distribute electronic documents than printed ones. And the improvements in electronic display technologies mean that in most cases it is possible to view documents on screen instead of printing them (thus saving paper and the space required to store the printed copies).

However, using electronic documents for final presentation instead of paper has created the problem of multiple incompatible file formats. Even plain text computer files are not free from this problem &mdash; e.g. under MS-DOS, most programs could not work correctly with UNIX-style text files (see newline), and for non-English speakers, the different code pages always have been a source of trouble.

Even more problems are connected with complex file formats of various [[word processor]]s, [[spreadsheet]]s and [[graphics software]]. To alleviate the problem, many software companies distribute free [[file viewer]]s for their proprietary file formats (one example is [[Adobe Systems|Adobe]]'s [[Portable Document Format|Acrobat Reader]]). The other solution is the development of standardized non-[[Proprietary software|proprietary]] file formats (such as [[HTML]] and [[OpenDocument]]), and electronic documents for specialized uses have specialized formats &ndash; the specialized [[electronic article]]s in physics  use [[TeX]] or [[PostScript]].
{{reflist}} 
==See also==
*[[Digital era governance]]
*[[Electronic paper]]
*[[Paperless office]]
*[[Bureaucrat]]
*[[E-Government Act of 2002]]
*[[E-government]]
*[[Public administration]]

== External links ==
* [http://people.ischool.berkeley.edu/~buckland/digdoc.html What is a digital document]
* [http://www.msimaging.com/faq Digital Imaging Frequent Questions]

[[Category:Electronic documents]]
[[Category:Word processors]]
<=====doc_Id=====>:395
<=====title=====>:
Archival Resource Key
<=====text=====>:
An '''Archival Resource Key''' ('''ARK''') is a [[Uniform Resource Locator]] (URL) that is a multi-purpose [[persistent identifier]] for information objects of any type.  An ARK contains the label '''ark:''' after the URL's hostname, which sets the expectation that, when submitted to a web browser, the URL terminated by '?' returns a brief metadata record, and the URL terminated by '??' returns metadata that includes a commitment statement from the current service provider.  The ARK and its inflections ('?' and '??') gain access to three facets of a provider's ability to provide persistence.

Implicit in the design of the ARK scheme is that persistence is purely '''a matter of service''' and not a property of a naming syntax.  Moreover, that a "persistent identifier" cannot be born persistent, but an identifier from any scheme may only be proved persistent over time.  The inflections provide information with which to judge an identifier's likelihood of persistence.

ARKs can be maintained and resolved locally using open source software such as [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers)] or via services such as [http://ezid.cdlib.org EZID] and the central [http://n2t.net N2T (Name-to-Thing)] resolver.

== Structure ==
 <nowiki>[http://NMAH/]ark:/NAAN/Name[Qualifier]</nowiki>

* NAAN: Name Assigning Authority Number - mandatory unique identifier of the organization that originally named the object
* NMAH: Name Mapping Authority Host - optional and replaceable hostname of an organization that currently provides service for the object
* Qualifier: optional string that extends the base ARK to support access to individual '''hierarchical''' subcomponents of an object,<ref>Hierarchy qualifiers begin with a slash character.</ref> and to '''variants''' (versions, languages, formats) of components.<ref>Variant qualifiers begin with a dot character.</ref>

== Name Assigning Authority Numbers (NAANs) ==
A complete NAAN registry<ref>[http://www.cdlib.org/services/uc3/naan_table.html Name Assigning Authority Number registry]</ref> is maintained by the [[California Digital Library]] and replicated at the [[Bibliothèque nationale de France|Bibliothèque Nationale de France]] and the [[National Library of Medicine|US National Library of Medicine]]. In 2015 it contained over 395 entries, some of which appear below.

* 12025: [[National Library of Medicine]]
* 12148: [[Bibliothèque Nationale de France]]
* 13030: [[California Digital Library]]
* 13038: [[World Intellectual Property Organization]]
* 13960: [[Internet Archive]]
* 14023: Revista de Arte, Ciência e Comunicação
* 15230: [[Rutgers University]]
* 17101: [[Centre for Ecology & Hydrology]]
* 20775: [[University of California, San Diego]]
* 21198: [[University of California Los Angeles]]
* 25031: [[University of Kansas]]
* 25593: [[Emory University]]
* 25652: [[École nationale supérieure des mines de Paris]]
* 26677: [[Library and Archives Canada]]
* 27927: Portico/Ithaka Electronic-Archiving Initiative
* 28722: [[University of California Berkeley]]
* 29114: [[University of California San Francisco]]
* 35911: [[IEEE]]
* 39331: [[National Library of Hungary]]
* 45487: Russian Linguistic Bulletin (Российский Лингвистический Бюллетень)
* 48223: [[UNESCO]]
* 52327: [[Bibliothèque et Archives Nationales du Québec]]
* 61001: [[University of Chicago]]
* 62624: [[New York University]]
* 64269: [[Digital Curation Centre]]
* 65323: [[University of Calgary]]
* 67531: [[University of North Texas]]
* 78319: [[Google]]
* 78428: [[University of Washington]]
* 80444: [[Northwest Digital Archives]]
* 81055: [[British Library]]
* 88435: [[Princeton University]]
* 87925: [[University College Dublin]]

== Generic Services ==
Three generic ARK services have been defined. They are described below in protocol-independent terms. Delivering these services may be implemented through many possible methods given available technology (today’s or future).

=== Access Service (access, location) ===
*Returns (a copy of) the object or a redirect to the same, although a sensible object proxy may be substituted (for instance a table of contents instead of a large document).
*May also return a discriminated list of alternate object locators.
*If access is denied, returns an explanation of the object’s current (perhaps permanent) inaccessibility.

=== Policy Service (permanence, naming, etc.) ===
*Returns declarations of policy and support commitments for given ARKs.
*Declarations are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.
*Policy subareas may be addressed in separate requests, but the following areas should be covered:
**object permanence,
**object naming,
**object fragment addressing, and
**operational service support.

=== Description Service ===
*Returns a description of the object. Descriptions are returned in either a structured metadata format or a human readable text format; sometimes one format may serve both purposes.
*A description must at a minimum answer the '''who''', '''what''', '''when''', and '''where''' questions concerning an expression of the object.
*Standalone descriptions should be accompanied by the modification date and source of the description itself.
*May also return discriminated lists of ARKs that are related to the given ARK.

== See also ==
* [[Persistent identifier]]
* [[Digital object identifier]] (DOI)
* [[Handle System]] (Handle)
* [[Persistent uniform resource locator]] (PURL)
* [[Uniform resource name]] (URN)
* [[Info URI scheme]]

== Notes and references ==
<references/>

== External links ==
* [http://www.cdlib.org/inside/diglib/ark/ ARK (Archival Resource Key)], [[California Digital Library]]
* [https://confluence.ucop.edu/download/attachments/16744455/arkcdl.pdf Towards Electronic Persistence Using ARK Identifiers], California Digital Library
* [http://tools.ietf.org/html/draft-kunze-ark  The ARK Identifier Scheme], [[Internet Engineering Task Force]]
* [http://n2t.net Name-to-Thing Resolver]
* [http://search.cpan.org/dist/Noid/ Noid (Nice Opaque Identifiers) open source software]
* [http://ezid.cdlib.org EZID identifier manager]

[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
<=====doc_Id=====>:398
<=====title=====>:
DataCite
<=====text=====>:
[[File:DataCite logo.png|thumb|DataCite's logo.]]

'''DataCite''' is an international [[not-for-profit]] organization which aims to improve [[data citation]] in order to:
*establish easier access to research data on the Internet
*increase acceptance of research data as legitimate, citable contributions to the scholarly record
*support data archiving that will permit results to be verified and re-purposed for future study.<ref>{{cite web|publisher=DataCite|title=What is DataCite?|url=http://www.datacite.org/whatisdatacite|accessdate=17 March 2014}}</ref> 

==Background==
In August 2009 a paper was published laying out an approach for a global registration agency for research data.<ref>{{cite web|title=Approach for a joint global registration agency for research data|doi=10.3233/ISU-2009-0595}}<!--| accessdate=2011-05-23--></ref> DataCite was subsequently founded in London on 1 December 2009<ref>{{cite journal|last1=Neumann|first1=Janna|last2=Brase|first2=Jan|title=DataCite and DOI names for research data|journal=Journal of Computer-Aided Molecular Design|date=20 July 2014|volume=28|issue=10|pages=1035–1041|doi=10.1007/s10822-014-9776-5}}</ref> by organisations from 6 countries: the [[British Library]]; the Technical Information Center of Denmark (DTIC); the [[TU Delft]] Library from the Netherlands; the National Research Council’s [[Canada Institute for Scientific and Technical Information]] (NRC-CISTI); the [[California Digital Library]] (University of California Curation Center);<ref>{{cite web|url=http://webarchives.cdlib.org/sw1st7gf68/http://www.universityofcalifornia.edu/news/article/23055 |title=University of California becomes founding member of Datacite |accessdate=2014-05-05 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref> [[Purdue University]] (USA);<ref>{{cite web |url= http://blogs.lib.purdue.edu/news/2010/02/16/purdue-libraries-a-founding-member-of-international-cooperative-to-advance-research/ | title=Purdue Libraries becomes founding member of Datacite | accessdate=2010-04-15}}</ref> and the German National Library of Science and Technology (TIB).<ref>{{cite web|url=http://www.tib-hannover.de/en/the-tib/news/news/id/133/ | title=TIB becomes founding member of Datacite | accessdate=2010-04-15}}</ref>

After the founding of DataCite, leading research libraries and information centres converged for the first official members’ convention in Paris on 5 February 2010. The inclusion of five further members was approved in the office of the International Council for Science (ICSU): Australian National Data Service (ANDS);<ref>{{cite web|url=http://ands.org.au/guides/doi.html | title=ANDS joins Datacite | accessdate=2010-04-15}}</ref> Deutsche Zentralbibliothek für Medizin (ZB MED); GESIS - Leibniz-Institut für Sozialwissenschaften; French Institute for Scientific and Technical Information (INIST);<ref>{{cite web|url=http://www.inist.fr/spip.php?article66 |title=Inist join Datacite consortium |accessdate=2010-04-15 |deadurl=yes |archiveurl=https://web.archive.org/web/20100309010702/http://www.inist.fr:80/spip.php?article66 |archivedate=2010-03-09 |df= }}</ref> and Eidgenössische Technische Hochschule (ETH) Zürich.

== Technical ==

The primary means of establishing easier access to research data is by DataCite members assigning persistent identifiers, such as [[digital object identifier]]s (DOIs), to data sets. Although currently leveraging the well-established DOI infrastructure, DataCite takes an open approach to identifiers, and considers other systems and services that help forward its objectives.<ref>{{cite web|publisher=DataCite |title=What do we do? |url=http://www.datacite.org/whatdowedo |accessdate=17 March 2014 |deadurl=yes |archiveurl=https://web.archive.org/web/20140317195125/http://www.datacite.org/whatdowedo |archivedate=17 March 2014 |df= }}</ref> 

DataCite's recommended format for a data citation is: 
*Creator (PublicationYear): Title. Publisher. Identifier
OR
*Creator (PublicationYear): Title. Version. Publisher. ResourceType. Identifier

DataCite recommends that DOI names are displayed as linkable, permanent URLs.<ref>{{cite web|publisher=DataCite|title=Why cite data?|url=http://www.datacite.org/whycitedata|accessdate=17 March 2014}}</ref>

Third-party tools allow the migration of content to and from other services such as ODIN, for [[ORCID]]<ref name="ODIN">{{cite web |url=http://odin-project.eu/2013/05/13/new-orcid-integrated-data-citation-tool/|title=New ORCID-integrated data citation tool |last=Thorisson |first=Gudmundur |date=2013-05-13 |publisher=ODIN Project |accessdate=7 May 2014}}</ref>

==Members==
* Australia:
** [[Australian National Data Service]] - ANDS
* Canada:
** [[National Research Council (Canada)|National Research Council Canada]] - NRC-CNRC
* China:
** [[Beijing Genomics Institute]] - BGI 
* Denmark:
** Technical Information Center of Denmark (DTU Library)
* Estonia:
** [[University of Tartu]] (/UT)
* France:
** [[Institut de l'information scientifique et technique]] - INIST-CNRS
* Germany:
** [[German National Library of Economics]] - ZBW
** [[German National Library of Medicine]] - ZB MED
** [[German National Library of Science and Technology]] - TIB
** Leibniz Institute for the Social Sciences - GESIS
** Göttingen State and University Library - [[Göttingen State and University Library|SUB]]
** Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen - GWDG 
* Hungary:
** Library and Information Centre, Hungarian Academy of Sciences - MTA KIK
* International:
** [[World Data System|ICSU World Data System]] - ICSU-WDS 
* Italy:
** [[Conference of Italian University Rectors]] - CRUI
* Japan:
** Japan Link Center - JaLC
* Netherlands:
** [[TU Delft]] Library
* Norway:
** [[BIBSYS]]
* Republic of Korea:
** [[Korea Institute of Science and Technology Information]] - KISTI 
* South Africa:
** South African Environmental Observation Network - SAEON
* Sweden:
** Swedish National Data Service - SND
* Switzerland:
** [[CERN]] - European Organization for Nuclear Research
** [[Swiss Federal Institute of Technology Zurich]] - ETH
* Thailand:
** National Research Council of Thailand - NRCT
* United Kingdom:
** [[The British Library]] - BL
** [[Digital Curation Centre]] 
* United States:
** [[California Digital Library]] - CDL
** [[OSTI|Office of Scientific and Technical Information, US Department of Energy]] - OSTI
** [[Purdue University|Purdue University Libraries]] - PUL
** [[Inter-university Consortium for Political and Social Research]] - ICPSR 
** [[Harvard University Library]] 
** [[Institute of Electrical and Electronics Engineers]] - IEEE 

==References==
<references />

== External links ==
* [http://www.datacite.org/ Official '''DataCite''' website]

[[Category:Academic publishing]]
[[Category:Data publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
[[Category:Non-profit organizations]]
[[Category:Non-profit technology]]
<=====doc_Id=====>:401
<=====title=====>:
Xena (software)
<=====text=====>:
{{Other uses|Xena (disambiguation)}}

'''Xena''' is [[open-source software]] for use in [[digital preservation]]. Xena is short for XML Electronic Normalising for Archives.

Xena is a [[Java (programming language)|Java]] application developed by the [[National Archives of Australia]]. It is available free of charge under the [[GNU General Public License]].

Version 6.1.0 was released 31 July 2013. Source code and binaries for Linux, OS X and Windows are available from [[SourceForge]].

==Mode of operation==
Xena attempts to avoid [[digital obsolescence]] by converting files into an openly specified format, such as [[OpenDocument|ODF]] or [[Portable Network Graphics|PNG]]. If the file format is not supported or the Binary Normalisation option is selected, Xena will perform [[ASCII]] [[Base64]] encoding on binary files and wrap the output in XML metadata. The resulting .xena file is plain text, although the content of the data itself is not directly human-readable. The exact original file can be retrieved by stripping the metadata and reversing the Base64 encoding, using an internal viewer.

==Features==
Platforms supported by Xena are [[Microsoft Windows]], [[Linux]] and [[Mac OS X]].

Xena uses a series of plugins to identify file formats and convert them to an appropriate openly specified format.

Xena has an [[application programming interface]] which allows any reasonably skilled Java developer to develop a plugin to cover a new file type.

Xena can process individual files or whole directories. When processing a whole directory, it can preserve the original directory structure of the converted records.

Xena can create plain text versions of file formats such as [[Tagged Image File Format|TIFF]], [[Microsoft Word|Word]] and [[Portable Document Format|PDF]], with the use of [[Tesseract (software)]].

The Xena interface or Xena Viewer can be used to view or export a Xena file (extension .xena) in its target file format. These files contain the normalised file as well as any extra information relevant to the normalisation process.
The Xena Viewer supports bulk export of Xena files to target file formats.

Xena can be used via its [[graphical user interface]] or the [[command line]].

For Xena to be fully functional, it requires a local installation of the following external software:
*[[LibreOffice]] suite - to convert office documents to OpenDocument format
*[[Tesseract (software)|Tesseract]] - to create plain text versions of file formats
*[[ImageMagick]] - to convert a subset of image files to [[Portable Network Graphics|PNG]]
*Readpst - to convert [[Microsoft Outlook]] PST files to XML. Readpst is part of the free and open source [http://www.five-ten-sg.com/libpst/ libpst software suite].
*[[Free Lossless Audio Codec|FLAC]] - to convert audio files to FLAC format. This is also required to play back audio files using Xena.

==Supported file types==
Xena will recognize and process the file types listed below, plus a few others of minor importance. Unsupported file types will automatically undergo binary normalization.

Office file formats:
*[[Microsoft Office]] files (including [[Microsoft Office XML formats|MS Office XML]], [[SYLK]] spreadsheets and [[Rich Text Format]]) are converted to the corresponding OpenDocument files
*[[Microsoft Outlook]] [[Personal Storage Table|PST]] files are parsed for their individual messages, which are converted to XML files and a Xena index file is created
*[[Microsoft Project]] MPP files are converted to XML
*[[OpenOffice.org XML]] files (SXC, SXI, SXW) are converted to the corresponding OpenDocument formats
*[[WordPerfect]] WPD files are converted to OpenDocument ODT
*[[OpenDocument]] documents (ODT, ODS, ODB, ODP) are preserved unchanged
*Acrobat PDF files are stored as binaries
*Mailbox files (MBX) are converted to individual XML files

Graphics:
*[[BMP file format|BMP]], [[Graphics Interchange Format|GIF]], [[Adobe Photoshop|PSD]], [[PCX]], [[.ras|RAS]], and the [[X Window System]] [[X BitMap|XBM]] and [[X PixMap|XPM]] bitmap files are converted to [[Portable Network Graphics|PNG]]; [[Tagged Image File Format|TIFF]] files additionally get embedded metadata stored in Xena XML. If the [[Tesseract (software)|Tesseract]] [[Optical character recognition|OCR software]] is installed, text will be extracted from TIFF files.
*OpenDocument Drawings (ODG) and [[Scalable Vector Graphics|SVG]] files are wrapped in Xena XML
*JPG and PNG files are stored unchanged

Archive Files:
*Files are extracted from [[File archiver|archives]] ([[ZIP (file format)|ZIP]], [[gzip|GZIP]], [[tar (file format)|TAR/TAR.gz]], [[JAR (file format)|JAR]], [[WAR (Sun file format)|WAR]], Mac binary) and normalised into a separate Xena file. A Xena index file is created, which when opened in the internal Xena viewer will display the files in a table.

Audio files:
*[[MP3]], [[WAV]], [[AIFF]], and [[Vorbis|OGG]] formats are converted to [[Free Lossless Audio Codec|FLAC]] files.

Databases:
*[[SQL]] files are processed as plain text wrapped in XML

Other file types:
*HTML is converted to XHTML
*TXT text files are stored as plain text wrapped in XML; CSS files are stored as plain text wrapped in XML

==Reviews==
An April 22, 2010 review in Practical e-Records rated Xena at 82/100 points. At present Xena has no target preservation format for video files.<ref>{{cite web |url=http://e-records.chrisprom.com/review-of-xena-normalization-software/ |title=Review of XENA Normalization Software |date=2010-04-22 |accessdate= |archiveurl=http://archive.is/yKw1 |archivedate=2012-07-08}}</ref>

==References==
<references/>

==External links==
*[http://xena.sourceforge.net/ Xena on SourceForge]
*[http://sourceforge.net/apps/mediawiki/xena/index.php?title=Main_Page Xena wiki on SourceForge]
*[https://web.archive.org/web/20100610095405/http://www.ask-oss.mq.edu.au/index.php?option=com_content&task=view&id=66&Itemid=69 Xena project description at The Australian Service for Knowledge of Open Source Software]
*[http://www.naa.gov.au/records-management/secure-and-store/e-preservation/at-naa/software.aspx#section1 National Archives of Australia - software]

{{DEFAULTSORT:Xena (Software)}}
[[Category:Digital preservation]]
[[Category:Electronic documents]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Binary-to-text encoding formats]]
[[Category:Mass digitization]]
<=====doc_Id=====>:404
<=====title=====>:
Category:Data security
<=====text=====>:
{{Commons category|Computer security}}
{{Portal|Computer security}}
{{Cat main|Data security}}

[[Category:Computer security]]
[[Category:Computer data|Security]]
[[Category:Electronic documents]]
[[Category:National security]]
[[Category:Information technology management]]

[[fi:Luokka:Tietoturva]]
<=====doc_Id=====>:407
<=====title=====>:
Bibcode
<=====text=====>:
{{selfref|For the Wikipedia template to link to bibcoded articles, see [[Template:bibcode]]}}
{{Infobox identifier
| name          = Bibcode
| image         = 
| image_size    = 
| image_caption = 
| image_alt     = 
| image_border  = 
| full_name     = Bibliographic code
| acronym       = 
| number        = 
| start_date    = 1990s<!-- {{Start date|YYYY|MM|DD|df=y}} -->
| organisation  = 
| digits        = 19
| check_digit   = none
| example       = 1924MNRAS..84..308E
| website       = <!-- {{URL|example.org}} -->
}}
The '''bibcode''' (also known as the "refcode") is a compact identifier used by several [[astronomy|astronomical]] data systems to uniquely specify literature references.

== Adoption ==
The Bibliographic Reference Code (REFCODE) was originally developed to be used in [[SIMBAD]] and the [[NASA/IPAC Extragalactic Database]] (NED), but it became a de facto standard and is now used more widely, for example, by the NASA [[Astrophysics Data System]] who coined and prefer the term "bibcode".<ref name=a>{{ cite book| url=http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| chapter= NED and SIMBAD Conventions for Bibliographic Reference Coding| title=Information & On-Line Data in Astronomy|editor= Daniel Egret|editor2= Miguel A. Albrecht|publisher= Kluwer Academic Publishers|date=1995|isbn =0-7923-3659-3|author= M. Schmitz|author2= G. Helou|author3= P. Dubois|author4= C. LaGue|author5= B.F. Madore|author6= H. G. Corwin Jr.|author7= S. Lesteven|last-author-amp= yes|accessdate= 2011-06-22| archiveurl= https://web.archive.org/web/20110607153038/http://cdsweb.u-strasbg.fr/simbad/refcode/refcode-paper.html| archivedate= 7 June 2011 <!--DASHBot-->| deadurl= no}}</ref><ref name=b>{{cite web|url=http://adsabs.harvard.edu/abs_doc/help_pages/data.html|title= The ADS Data, help page|publisher= NASA ADS |accessdate=November 5, 2007| archiveurl= https://web.archive.org/web/20071014195855/http://adsabs.harvard.edu/abs_doc/help_pages/data.html| archivedate= 14 October 2007 <!--DASHBot-->| deadurl= no}}</ref>

== Format ==
The code has a fixed length of 19 characters and has the form
<center><tt>YYYYJJJJJVVVVMPPPPA</tt></center>
where <tt>YYYY</tt> is the four-digit year of the reference and <tt>JJJJJ</tt> is a code indicating where the reference was published. In the case of a journal reference, <tt>VVVV</tt> is the volume number, <tt>M</tt> indicates the section of the journal where the reference was published (e.g., <tt>L</tt> for a letters section), <tt>PPPP</tt> gives the starting page number, and <tt>A</tt> is the first letter of the last name of the first author. Periods (<tt>.</tt>) are used to fill unused fields and to pad fields out to their fixed length if too short; padding is done on the right for the publication code and on the left for the volume number and page number.<ref name=a /><ref name=b /> Page numbers greater than 9999 are continued in the <tt>M</tt> column. The 6-digit article ID numbers (in lieu of page numbers) used by the Physical Review publications since the late 1990s are treated as follows: The first two digits of the article ID, corresponding to the issue number, are converted to a lower-case letter (01 = a etc.) and inserted into column <tt>M</tt>. The remaining four digits are used in the page field.<ref name=b />

== Examples ==
Some examples of the code are:
{| class="wikitable"
|-
! Bibcode
! Reference
|-
| <tt>[http://adsabs.harvard.edu/abs/1974AJ.....79..819H 1974AJ.....79..819H]</tt>
| {{cite journal
   |last=Heintz |first=W. D.
   |date=1974
   |title=Astrometric study of four visual binaries
   |journal=[[The Astronomical Journal]]
   |volume=79 |pages=819–825
   |doi=10.1086/111614
   |bibcode = 1974AJ.....79..819H }}
|-
| <tt>[http://adsabs.harvard.edu/abs/1924MNRAS..84..308E 1924MNRAS..84..308E]</tt>
| {{cite journal
   |last=Eddington |first=A. S.
   |date=1924
   |title=On the relation between the masses and luminosities of the stars
   |journal=[[Monthly Notices of the Royal Astronomical Society]]
   |volume=84 |issue=5
   |pages=308–332
   |bibcode = 1924MNRAS..84..308E 
   | doi = 10.1093/mnras/84.5.308 }}
|-
| <tt>[http://adsabs.harvard.edu/abs/1970ApJ...161L..77K 1970ApJ...161L..77K]</tt>
| {{cite journal
   |last1=Kemp |first1=J. C.
   |last2=Swedlund |first2=J. B.
   |last3=Landstreet |first3=J. D.
   |last4=Angel |first4=J. R. P.
   |date=1970
   |title=Discovery of circularly polarized light from a white dwarf
   |journal=[[The Astrophysical Journal Letters]]
   |volume=161 |pages=L77–L79
   |doi=10.1086/180574
 |bibcode = 1970ApJ...161L..77K }}
|-
| <tt>[http://adsabs.harvard.edu/abs/2004PhRvL..93o0801M 2004PhRvL..93o0801M]</tt>
| {{cite journal
   |last1=Mukherjee |first1=M.
   |last2=Kellerbauer |first2=A.
   |last3=Beck |first3=D.
   |last4=Blaum |first4=K.
   |last5=Bollen |first5=G.
   |last6=Carrel |first6=F.
   |last7=Delahaye |first7=P.
   |last8=Dilling |first8=J.
   |last9=George |first9=S.
   |last10=Guénaut |first10=C.
   |last11=Herfurth |first11=F.
   |last12=Herlert |first12=A.
   |last13=Kluge |first13=H.-J.
   |last14=Köster |first14=U.
   |last15=Lunney |first15=D.
   |last16=Schwarz |first16=S.
   |last17=Schweikhard |first17=L.
   |last18=Yazidjian |first18=C.
   |display-authors=3
   |date=2004
   |title=The Mass of <sup>22</sup>Mg
   |journal=[[Physical Review Letters]]
   |volume=93 |issue=15
   |pages=150801
   |doi=10.1103/PhysRevLett.93.150801
 |bibcode = 2004PhRvL..93o0801M }}
|}

== See also ==
* [[Digital object identifier]]

== References ==
{{Reflist|30em}}

[[Category:Index (publishing)]]
[[Category:Identifiers]]
[[Category:Electronic documents]]
[[Category:Computational astronomy]]
<=====doc_Id=====>:410
<=====title=====>:
Registry of Research Data Repositories
<=====text=====>:
{{Infobox website
| name = re3data.org
| logo = Re3data Logo RGB 72dpi.png
| logocaption = The logo of re3data.org, the online Registry of Research Data Repositories
| registration = none
| language = English
| type = Online registry
| owner = [[Karlsruhe Institute of Technology]], [[GFZ German Research Centre for Geosciences]], [[Berlin School of Library and Information Science]]
| commercial = no
| launch date = {{Start date and years ago|mf=yes|2013|05|28}}
| current status  = Online
| content license = Website: [[Creative Commons licenses|CC-BY]], Database: [[Creative Commons licenses|CC0]] 
| url = {{URL|http://www.re3data.org/}}
}}

The '''Registry of Research Data Repositories''' ('''re3data.org''') is an [[Open Science]] tool that offers researchers, funding organizations, libraries and publishers an overview of existing international [[disciplinary repository|repositories]] for [[research data]].

== Background ==

re3data.org is a global registry of research data repositories from all academic disciplines. It provides an overview of existing research data repositories in order to help researchers to identify a suitable repository for their data and thus comply with requirements set out in data policies.<ref name=Pampel2013>{{cite journal|last=Pampel|first=Heinz|author2=Vierkant, Paul |author3=Scholze, Frank |author4=Bertelmann, Roland |author5=Kindling, Maxi |title=Making Research Data Repositories Visible: The re3data.org Registry|journal=PLoS ONE|date=4 November 2013|volume=8|issue=11|pages=e78080|doi=10.1371/journal.pone.0078080|pmid=24223762|url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078080|accessdate=7 February 2014|bibcode=2013PLoSO...878080P|pmc=3817176|display-authors=etal}}</ref>
The registry was officially launched in May 2013.<ref name=Wellander2013>{{cite web | url=http://sparceurope.org/registry-of-research-data-repositories-launched-re3data-org/ | title=Registry of Research Data Repositories launched – re3data.org | author=Wellander, Janna | date= 4 June 2013 | work=SPARC Europe | accessdate= 5 February 2014}}</ref>

== Content ==

In March 2014 the registry lists 634 research data repositories from around the world covering all academic disciplines. 586 of these are described in detail using the re3data.org schema.<ref>{{cite web|title=re3data.org – from Funding to Growing|url=http://www.re3data.org/2014/03/re3data-org-from-funding-to-growing/|work=re3data.org|accessdate=21 March 2014|date=19 March 2014}}</ref>
The project makes all metadata in the registry available for open use under the Creative Commons deed CC0.<ref>{{cite web|title=DataCite, re3data.org, and Databib Announce Collaboration|url=http://www.re3data.org/2014/03/datacite-re3data-org-databib-collaboration/|work=re3data|accessdate=25 March 2014}}</ref>
[[File:DRYAD entry in re3data.org 2014-03-21.png|thumb|A screenshot of the DataDryad entry in re3data.org.]]

== Features ==

The majority of the listed research data repositories are described in detail by a comprehensive schema, namely the re3data.org Schema for the Description of Research Data Repositories.<ref name=Vierkant>{{cite web|last=Vierkant|first=Paul|title=Schema for the description of research data repositories|work=GFZ Helmholtz-Zentrum Potsdam|accessdate=7 February 2014|author2=Spier, Shaked |author3=Rücknagel, Jessika |author4= et. al. |url=http://gfzpublic.gfz-potsdam.de/pubman/faces/viewItemFullPage.jsp?itemId=escidoc:301641|doi=10.2312/re3.004}}</ref>
Information icons support researchers to identify an adequate repository for the storage and reuse of their data.<ref name=Wellander2013 /><ref>{{cite web|last=Fenner|first=Martin|title=registry of research data repositories launched|url=http://blogs.plos.org/mfenner/2013/06/01/re3data-org-registry-of-research-data-repositories-launched/|work=PLOS Blog|publisher=Gobbledygook|accessdate=5 February 2014}}</ref>
[[File:Journal.pone.0078080.g001.png|thumb|Aspects of a Research Data Repository with the corresponding icons used in re3data.org.]]

== Inclusion criteria ==

A repository is indexed when the minimum requirements for inclusion in re3data.org are met: the repository has to be run by a legal entity, such as a sustainable institution (e.g. library, university) and clearly state access conditions to the data and repository as well as the terms of use. Additionally, an English [[graphical user interface]] (GUI) plus a focus on research [[data]] is needed.<ref name=Vierkant />

== Partners and Cooperation ==

re3data.org is a joint project of the [[Berlin School of Library and Information Science]], the [[GFZ German Research Centre for Geosciences]] and the Library of the [[Karlsruhe Institute of Technology]] (KIT). The project is funded by the [[Deutsche Forschungsgemeinschaft|German Research Foundation]] (DFG).<ref name=Pampel2013 />
The project cooperates with other Open Science initiatives like Databib,<ref>{{cite web|last=Kratz|first=John|title=Finding Disciplinary Data Repositories with DataBib and re3data|url=http://datapub.cdlib.org/2014/03/03/finding-disciplinary-data-repositories-with-databib-and-re3data/|work=Data Pub|accessdate=21 March 2014|author2=Nicholls, Natsuko |date=3 March 2014}}</ref> BioSharing,<ref>{{cite web|title=Databases|url=http://www.biosharing.org/biodbcore|work=biosharing|accessdate=5 February 2014}}</ref> [[DataCite]]<ref>{{cite web|title=Resources|url=http://www.datacite.org/resources|work=DataCite|accessdate=5 February 2014}}</ref> and OpenAIRE.<ref>{{cite web|title=re3data.org and OpenAIRE sign Memorandum of Understanding|url=https://www.openaire.eu/it/component/content/article/481-re3data-and-openaire-sign-memorandum-of-understanding|work=OpenAIRE|accessdate=5 February 2014|date=21 October 2013}}</ref> Several publishers, research institutions and funders refer to re3data.org in their Editorial Policies and guidelines as a tool for the identification of suitable data repositories, e.g. [[Nature (journal)|Nature]],<ref>{{cite web|title=The paper trail|url=http://www.nature.com/news/the-paper-trail-1.13123|work=Nature|accessdate=5 February 2014|date=4 June 2013}}</ref> [[Springer Science+Business Media|Springer]]<ref>{{cite web|title=About SpringerPlus - Editorial policies|url=http://www.springerplus.com/about#editorialpolicies|work=SpringerPlus|accessdate=5 February 2014}}</ref> and the [[European Commission]].<ref>{{cite web|title=Guidelines on Open Access to Scientific Publications and Research Data in Horizon 2020|url=http://ec.europa.eu/research/participants/data/ref/h2020/grants_manual/hi/oa_pilot/h2020-hi-oa-pilot-guide_en.pdf|publisher=European Commission|accessdate=20 March 2014}}</ref>

== See also ==

*[[Scientific data archiving]]
*[[Data sharing]]
*[[Data archive]]
*[[Data library]]
*[[Data curation]]

== External links ==
* [http://www.re3data.org/ Official website]

== References ==

{{reflist}}

<!-- Just press the "Save page" button below without changing anything! Doing so will submit your article submission for review. Once you have saved this page you will find a new yellow 'Review waiting' box at the bottom of your submission page. If you have submitted your page previously, either the old pink 'Submission declined' template or the old grey 'Draft' template will still appear at the top of your submission page, but you should ignore it. Again, please don't change anything in this text box. Just press the "Save page" button below. -->


[[Category:Academic publishing]]
[[Category:Electronic documents]]
[[Category:Identifiers]]
[[Category:Index (publishing)]]
[[Category:Open science]]
<=====doc_Id=====>:413
<=====title=====>:
Category:Open-access archives
<=====text=====>:
{{cat main|Open archive}}
{{cat see also|Eprint archives}}
 
[[Category:Open access (publishing)|Archives]]
[[Category:Online archives]]
[[Category:Academic publishing]]
[[Category:Digital libraries]]
[[Category:Electronic documents]]
[[Category:Full text scholarly online databases]]
<=====doc_Id=====>:416
<=====title=====>:
Zathura (document viewer)
<=====text=====>:
{{notability|Products|date=April 2015}}
{{Infobox software
| name                   = Zathura
| screenshot             = Zathura Screenshot.png
| caption = Screenshot of zathura viewing a PDF file in Arch Linux.
| author                 = Moritz Lipp, Sebastian Ramacher
| developer              = pwmt<ref>{{cite web|url=https://pwmt.org |title=Programs With Movie Titles}}</ref>
| released               = {{Start date|2009|09|18}}
| latest release version = 0.3.6
| latest release date    = {{Release date|2016|04|18}}<ref>{{cite web |url=https://pwmt.org/news/zathura-0-3-6/ |title=ZATHURA 0.3.6 |website=pwmt.org |date=2016-04-18 |accessdate=2016-08-28}}</ref>
| programming language   = [[C (programming language)|C]]
| operating system       = [[Unix-like]]
| status                 = Active
| genre                  = Document viewer
| license                = [[Free software license|Free software]]
| website                = {{URL|pwmt.org/projects/zathura}}
}}

'''Zathura''' is a [[Free software|free]], [[Plug-in (computing)|plugin-based]] [[document viewer]]. Plugins are available for [[Portable Document Format|PDF]] (via [[Poppler_(software)|poppler]] or [[MuPDF]]), [[PostScript]], [[DjVu]], and [[EPUB]]. It was written to be lightweight and controlled with [[Vim (text editor)|vim]]-like keybindings. Zathura's customizability makes it well-liked by many Linux users.<ref>{{cite web|url=http://www.maketecheasier.com/8-alternative-pdf-readers-for-your-consideration/|title=MakeTechEasier list of alternative PDF viewers|access-date=24 April 2015}}</ref>

Zathura has a mature, well-established codebase and a large development team.<ref>{{cite web|url=https://www.openhub.net/p/zathura-pdf-viewer|title=OpenHUB analysis of Zathura PDF Viewer|access-date=24 April 2015}}</ref> It has official packages available in [[Arch linux]],<ref>{{cite web|url=https://www.archlinux.org/packages/community/x86_64/zathura/|title=Arch Linux zathura package}}</ref>
[[Debian]],<ref>{{cite web|url=https://packages.debian.org/en/sid/zathura|title=Debian zathura package}}</ref>
[[Fedora (operating system)|Fedora]],<ref>{{cite web|url=http://pkgs.org/altlinux-sisyphus/classic-x86_64/zathura-devel-0.3.3-alt1.x86_64.rpm.html|title=Fedora zathura package}}</ref>
[[Gentoo linux|Gentoo]],<ref>{{cite web|url=https://packages.gentoo.org/package/app-text/zathura|title=Gentoo zathura package}}</ref>
[[Ubuntu (operating system)|Ubuntu]],<ref>{{cite web|url=http://packages.ubuntu.com/precise/zathura|title=Ubuntu zathura package}}</ref>
[[Source Mage GNU/Linux]],<ref>{{cite web|url=http://download.sourcemage.org/codex/test/doc/zathura/|title=Source Mage zathura package}}</ref>
[[OpenBSD]],<ref>{{cite web|url=http://openports.se/textproc/zathura|title=OpenBSD zathura package}}</ref>
and [[Mac OS X]].<ref>{{cite web|url=https://www.macports.org/ports.php?by=name&substr=zathura|title=MacPorts zathura package}}</ref>

Zathura was named after the [[Zathura (film)|film]] of the same name.<ref>https://git.pwmt.org/groups/pwmt</ref>

== History ==

Development on Zathura began on 12 August 2009.<ref>{{cite web|url=https://github.com/pwmt/zathura/commit/0eeb457bea2f93983e556d07028c2cfdb49b898c|title=Zathura initial commit}}</ref> On 18 September 2009, version 0.0.1 was announced to the Arch Linux community.<ref>{{cite web|url=https://bbs.archlinux.org/viewtopic.php?id=80458|title=zathura - a document viewer}}</ref>

Zathura has been an official Arch Linux package since April 2010.<ref>{{cite web|url=https://projects.archlinux.org/svntogit/community.git/log/trunk?h=packages/zathura|title=Arch Linux package history for Zathura}}</ref> Same year, by the end of July it was imported into Source Mage test grimoire.<ref>{{cite web|url=http://scmweb.sourcemage.org/?p=smgl/grimoire.git;a=commit;h=c0963f5c0a65a0536d21a03a528ffaff4245cce7|title=zathura package in Source Mage}}</ref> It has been an official Debian package since at least 2011, as part of Debian Squeeze.<ref>{{cite web|url=https://packages.debian.org/squeeze/zathura|title=Debian Squeeze package for Zathura}}</ref>

== Features ==

Zathura automatically reloads documents. When working in compiled documents such as those written in [[LaTeX]], Zathura will refresh the output whenever compilation takes place. Zathura has the option of enabling [[inverse search]] (using "synctex").<ref>{{cite web|url=https://wiki.math.cmu.edu/iki/wiki/tips/20140310-zathura-fsearch.html|title=LaTeX forward/inverse searches with Zathura}}</ref><ref>{{cite web|url=https://gist.github.com/vext01/16df5bd48019d451e078|title=Vim+Zathura+Synctex}}</ref>

Zathura can adjust the document to best-fit or to fit width, and it can rotate pages. It can view pages side-by-side and has a fullscreen mode. Pages can also be recolored to have a black background and white foreground.

Zathura can search for text and copy text to the [[X Window selection|primary X selection]]. It supports bookmarks and can open encrypted files.

The behavior and appearance of Zathura can be customised using a [[configuration file]]. Zathura has the ability to execute external [[List of command-line interpreters#Unix-like systems|shell]] commands. It can be opened in tabs using {{URL|http://tools.suckless.org/tabbed/|tabbed}}.<ref>{{cite web|url=http://taitran.ca/vim/latex/markdown/2015/03/11/vim-latex-and-markdown-preview-scripts.html|title=Vim, Latex and Markdown preview scripts}}</ref>

== See also ==
{{Portal|Free software}}

* [[List of PDF software]]
* [[Zathura (film)]]

==References==
{{Reflist|30em}}

==External links==
* {{official website|http://pwmt.org/projects/zathura/}}
* {{URL|https://wiki.archlinux.org/index.php/List_of_applications/Documents#Graphical_2|Arch Linux list of document viewers}}

{{PDF readers}}

[[Category:Free PDF readers]]
[[Category:PostScript]]
[[Category:Free software programmed in C]]
[[Category:Office software that uses GTK+]]
[[Category:Electronic documents]]
<=====doc_Id=====>:419
<=====title=====>:
Category:Electronic lab notebook
<=====text=====>:
{{catmain}}

[[Category:Electronic documents]]
[[Category:Data management software]]
[[Category:Science software]]
[[Category:Scientific documents]]
<=====doc_Id=====>:422
<=====title=====>:
Email
<=====text=====>:
{{about|the communications medium|the former manufacturing conglomerate|Email Limited}}
{{redirect|Inbox|the Google product|Inbox by Gmail}}
<!--Before adding {{lowercase title}} again, please see talk page.-->
[[File:2016-03-22-trojita-home.png|thumb|right|400px|This screenshot shows the "Inbox" page of an email system, where users can see new emails and take actions, such as reading, deleting, saving, or responding to these messages]]
[[File:(at).svg|thumb|100px|The [[at sign]], a part of every SMTP [[email address]]<ref>{{cite web|url=https://tools.ietf.org/html/rfc5321#section-2.3.11|title=RFC 5321 – Simple Mail Transfer Protocol|accessdate=19 January 2015|work=Network Working Group }}</ref>]]
'''Electronic mail''', or '''email''', is a method of exchanging digital messages between people using digital devices such as computers, tablets and mobile phones. Email first entered substantial use in the 1960s and by the mid-1970s had taken the form now recognized as email. Email operates across [[computer network]]s, which in the 2010s is primarily the [[Internet]]. Some early email systems required the author and the recipient to both be [[Online and offline|online]] at the same time, in common with [[instant messaging]]. Today's email systems are based on a [[store-and-forward]] model. Email [[Server (computing)|servers]] accept, forward, deliver, and store messages. Neither the users nor their computers are required to be online simultaneously; they need to connect only briefly, typically to a [[Message transfer agent|mail server]] or a [[webmail]] interface, for as long as it takes to send or receive messages.

Originally an [[ASCII]] text-only communications medium, Internet email was extended by [[Multipurpose Internet Mail Extensions]] (MIME) to carry text in other character sets and multimedia content attachments. [[International email]], with internationalized email addresses using [[UTF-8]], has been standardized, but as of 2016 it has not been widely adopted.{{citation needed|date=March 2016}}

The history of modern Internet email services reaches back to the early [[ARPANET]], with standards for encoding email messages published as early as 1973 (RFC 561). An email message sent in the early 1970s looks very similar to a basic email sent today. Email played an important part in creating the Internet,<ref>{{Harv|Partridge|2008}}</ref> and the conversion from ARPANET to the Internet in the early 1980s produced the core of the current services.

==Terminology==
Historically, the term ''electronic mail'' was used generically for any electronic document transmission. For example, several writers in the early 1970s used the term to describe [[fax]] document transmission.<ref>Ron Brown, Fax invades the mail market, [https://books.google.com/books?id=Ry64sjvOmLkC&pg=PA218 New Scientist], Vol. 56, No. 817 (Oct., 26, 1972), pages 218–221.</ref><ref>Herbert P. Luckett, What's News: Electronic-mail delivery gets started, [https://books.google.com/books?id=cKSqa8u3EIoC&pg=PA85 Popular Science], Vol. 202, No. 3 (March 1973); page 85</ref> As a result, it is difficult to find the first citation for the use of the term with the more specific meaning it has today.

Electronic mail has been most commonly called ''email'' or ''e-mail'' since around 1993,<ref>{{cite book|url=https://books.google.com/ngrams/graph?content=electronic+mail%2Ce-mail&year_start=1980&year_end=1995&corpus=15&smoothing=0&share= |title=Google Ngram Viewer |publisher=Books.google.com |accessdate=2013-04-21}}</ref> but variations of the [[spelling]] have been used:

* ''email'' is the most common form used online, and is required by [[IETF]] [[Request for Comments|Requests for Comments]] (RFC) and working groups<ref>{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt|publisher=IETF|title=RFC Editor Terms List}} This is suggested by the [https://www.rfc-editor.org/rfc-style-guide/rfc-style-manual-08.txt RFC Document Style Guide]</ref> and increasingly by [[style guide]]s.<ref>{{cite web|url=http://styleguide.yahoo.com/word-list/e |title=Yahoo style guide |publisher=Styleguide.yahoo.com |accessdate=2014-01-09}}</ref><ref name="aces2011">[http://www.huffingtonpost.com/2011/03/18/ap-removes-hyphen-from-em_n_837833.html "AP Removes Hyphen From ‘Email’ In Style Guide"], 18 March 2011, huffingtonpost.com</ref> This spelling also appears in most dictionaries.<ref name="AskOxford Language Query team">{{cite web | url=http://www.askoxford.com/asktheexperts/faq/aboutspelling/email | title=What is the correct way to spell 'e' words such as 'email', 'ecommerce', 'egovernment'? | publisher=[[Oxford University Press]] | work=FAQ | accessdate=4 September 2009 | author=AskOxford Language Query team | archiveurl=https://web.archive.org/web/20080701194047/http://www.askoxford.com/asktheexperts/faq/aboutspelling/email?view=uk | quote=We recommend email, as this is now by far the most common form | archivedate=July 1, 2008}}</ref><ref name="Reference.com">{{cite web|url=http://dictionary.reference.com/browse/email |title=Reference.com |publisher=Dictionary.reference.com |accessdate=2014-01-09}}</ref><ref name="ReferenceA">Random House Unabridged Dictionary, 2006</ref><ref name="ReferenceB">The American Heritage Dictionary of the English Language, Fourth Edition</ref><ref name="Princeton University WordNet 3.0">Princeton University WordNet 3.0</ref><ref name="ReferenceC">The American Heritage Science Dictionary, 2002</ref><ref name="Merriam-Webster Dictionary">{{cite web|title=Merriam-Webster Dictionary|url=http://www.merriam-webster.com/dictionary/email|publisher=Merriam-Webster|accessdate=9 May 2014}}</ref>
* ''e-mail'' is the format that sometimes appears in edited, published American English and British English writing as reflected in the [[Corpus of Contemporary American English]] data,<ref>{{cite web|url=http://english.stackexchange.com/questions/1925/email-or-e-mail|title= "Email" or "e-mail"|work=English Language & Usage – Stack Exchange|date=August 25, 2010|accessdate=September 26, 2010}}</ref> but is falling out of favor in style guides.<ref name="aces2011" /><ref name="ap">{{cite web|title=AP changes e-mail to email|url=http://www.aces2011.org/sessions/18/the-ap-stylebook-editors-visit-aces-2011/|work=15th National Conference of the American Copy Editors Society (2011, Phoenix)|publisher=ACES|accessdate=23 March 2011|author=Gerri Berendzen|authorlink=AP Stylebook editors share big changes|author2=Daniel Hunt}}</ref>
* ''mail'' was the form used in the original protocol standard, ''RFC&nbsp;524''.<ref name=rfc524>{{cite web|url=http://www.faqs.org/rfcs/rfc524.html |title=RFC 524 (rfc524) – A Proposed Mail Protocol |publisher=Faqs.org |date=1973-06-13 |accessdate=2016-11-18}}</ref> The service is referred to as ''mail'', and a single piece of electronic mail is called a ''message''.<ref name="11above">{{cite web|url=http://www.faqs.org/rfcs/rfc1939.html |title=RFC 1939 (rfc1939) – Post Office Protocol – Version 3 |publisher=Faqs.org |accessdate=2014-01-09}}</ref><ref name="12above">{{cite web|url=http://www.faqs.org/rfcs/rfc3501.html |title=RFC 3501 (rfc3501) – Internet Message Access Protocol – version 4rev1 |publisher=Faqs.org |accessdate=2014-01-09}}</ref>
* ''EMail'' is a traditional form that has been used in RFCs for the "Author's Address"<ref name="11above"/><ref name="12above"/> and is expressly required "for historical reasons".<ref>{{cite web|url=https://www.rfc-editor.org/rfc-style-guide/terms-online.txt |title='&#39;"RFC Style Guide"'&#39;, Table of decisions on consistent usage in RFC |accessdate=2014-01-09}}</ref>
* ''E-mail'' is sometimes used, capitalizing the initial ''E'' as in similar abbreviations like ''[[E-piano]]'', ''[[E-guitar]]'', ''[[A-bomb]]'', and ''[[H-bomb]]''.<ref>{{cite web|url=http://alt-usage-english.org/excerpts/fxhowdoy.html |title=Excerpt from the FAQ list of the Usenet newsgroup alt.usage.english |publisher=Alt-usage-english.org |accessdate=2014-01-09}}</ref>

== {{anchor|history}} Origin ==
The [[AUTODIN]] network, first operational in 1962, provided a message service between 1,350 terminals, handling 30 million messages per month, with an average message length of approximately 3,000 characters. Autodin was supported by 18 large computerized switches, and was connected to the [[United States General Services Administration]] Advanced Record System, which provided similar services to roughly 2,500 terminals.<ref name="NAS USPS">USPS Support Panel, Louis T Rader, Chair, Chapter IV: Systems, [https://books.google.com/books?id=5TQrAAAAYAAJ&pg=PA27 Electronic Message Systems for the U.S. Postal Service], National Academy of Sciences, Washington, D.C., 1976; pages 27–35.</ref> By 1968, AUTODIN linked more than 300 sites in several countries.

===Host-based mail systems===
With the introduction of [[Massachusetts Institute of Technology|MIT]]'s [[Compatible Time-Sharing System]] (CTSS) in 1961,<ref>"CTSS, Compatible Time-Sharing System" (September 4, 2006), [[University of South Alabama]], [http://www.cis.usouthal.edu/faculty/daigle/project1/ctss.htm USA-CTSS].</ref> multiple users could log in to a central system<ref>an [[IBM 7094]]</ref> from remote dial-up terminals, and store and share files on the central disk.<ref>[[Tom Van Vleck]], "The IBM 7094 and CTSS" (September 10, 2004), ''Multicians.org''
 ([[Multics]]), web: [http://www.multicians.org/thvv/7094.html Multicians-7094].</ref> Informal methods of using this to pass messages were developed and expanded:
* 1965 – [[Massachusetts Institute of Technology|MIT]]'s [[Compatible Time-Sharing System|CTSS]] MAIL.<ref name="thvv">{{cite web|url=http://www.multicians.org/thvv/mail-history.html|title=The History of Electronic Mail|author=Tom Van Vleck}}</ref>

Developers of other early systems developed similar email applications:
<!-- Please do not delete references without first reading them. I've added a page number. -->
* 1962 – [[IBM Administrative Terminal System|1440/1460 Administrative Terminal System]]<ref>{{cite book
 |     author = IBM
 |      title = 1440/1460 Administrative Terminal System (1440-CX-07X and 1460-CX-08X) Application Description
 |    section =
 | sectionurl =
 |    version = Second Edition
 |  publisher = IBM
 |       date =
 |        url = http://bitsavers.org/pdf/ibm/144x/H20-0129-1_1440_admTermSys.pdf
 |         id = H20-0129-1
 | accessdate =
 |      quote =
 |       page = 10
 |      pages =
 |        ref =
 |mode=cs2
 }}</ref>
* 1968 – [[IBM Administrative Terminal System|ATS/360]]<ref>{{cite book
 |     author = IBM
 |      title = System/36O Administrative Terminal System DOS (ATS/DOS) Program Description Manual
 |    section =
 | sectionurl =
 |    version =
 |  publisher = IBM
 |       date =
 |        url =
 |         id = H20-0508
 | accessdate =
 |      quote =
 |       page =
 |      pages =
 |        ref =
 |mode=cs2
 }}</ref><ref>{{cite book
 |     author = IBM
 |      title = System/360 Administrative Terminal System-OS (ATS/OS) Application Description Manual
 |    section =
 | sectionurl =
 |    version =
 |  publisher = IBM
 |       date =
 |        url =
 |         id = H20-0297
 | accessdate =
 |      quote =
 |       page =
 |      pages =
 |        ref =
 |mode=cs2
 }}</ref>
* 1971 –  ''[[SNDMSG]]'', a local inter-user mail program incorporating the experimental file transfer program, ''CPYNET'', allowed the first [[Computer network|networked]] electronic mail<ref name="firstnetworkemail">{{cite web|author=Ray Tomlinson |url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html |title=The First Network Email |publisher=Openmap.bbn.com |accessdate=2014-01-09}}</ref>
* 1972 – [[Unix]] [[mail (Unix)|mail]] program<ref>{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V3/man/man1/mail.1 |title=Version 3 Unix mail(1) manual page from 10/25/1972 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://minnie.tuhs.org/cgi-bin/utree.pl?file=V6/usr/man/man1/mail.1 |title=Version 6 Unix mail(1) manual page from 2/21/1975 |publisher=Minnie.tuhs.org |accessdate=2014-01-09}}</ref>
* 1972 – APL Mailbox by [[Lawrence M. Breed|Larry Breed]]<ref>[http://www.jsoftware.com/papers/APLQA.htm APL Quotations and Anecdotes], including [[Leslie H. Goldsmith|Leslie Goldsmith]]'s story of the Mailbox</ref><ref>{{cite web|url=http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx|title=Home > Communications > The Internet > History of the internet > Internet in its infancy|archive-url=https://web.archive.org/web/20110227151622/http://www.actewagl.com.au/Education/communications/Internet/historyOfTheInternet/InternetOnItsInfancy.aspx| archive-date=2011-02-27 |work=actewagl.com.au |accessdate=2016-11-03}}</ref><ref>{{cite web |url=https://www.youtube.com/watch?v=mjgkhK-nXmk | title=The STSC Story: It's About Time |date=c. 1979 |editor=[http://aprogramminglanguage.com Catherine Lathwell] |publisher=[[Scientific Time Sharing Corporation]] |at=7:08 |accessdate=2017-01-06 |}} Promotional video for Scientific Time Sharing Corporation, which features President [[Jimmy Carter]]'s press secretary [[Jody Powell]] explaining how the company's "APL Mailbox" enabled the 1976 Carter presidential campaign to easily move information around the country to coordinate the campaign. </ref>
* 1974 – The [[PLATO (computer system)|PLATO]] IV Notes on-line [[message board]] system was generalized to offer 'personal notes' in August 1974.<ref name="NAS USPS" /><ref name=wooley>David Wooley, [http://www.thinkofit.com/plato/dwplato.htm#pnotes PLATO: The Emergence of an Online Community], 1994.</ref>
* 1978 – ''Mail'' client written by Kurt Shoens for Unix and distributed with the Second Berkeley Software Distribution included support for aliases and distribution lists, forwarding, formatting messages, and accessing different mailboxes.<ref name="shoens">The Mail Reference Manual, Kurt Shoens, University of California, Berkeley, 1979.</ref> It used the Unix ''mail'' client to send messages between system users. The concept was extended to communicate remotely over the Berkeley Network.<ref name="berknet">An Introduction to the Berkeley Network, Eric Schmidt, University of California, Berkeley, 1979.</ref>
* 1979 – ''EMAIL'', an application written by [[Shiva Ayyadurai]]. He has been associated with controversial, self-made claims that he had "invented" email due to its presence of certain functionality. These claims have been disputed by various parties.<ref>{{cite web|url=http://www.bizjournals.com/boston/news/2016/05/10/cambridge-man-who-claims-he-invented-email-sues.html|title=Cambridge man who claims he invented email sues Gawker for $35M - Boston Business Journal|last=Harris|first=David L.|date=May 10, 2016|website=Boston Business Journal|access-date=2016-05-16}}</ref><ref>[https://assets.documentcloud.org/documents/2829697/Gov-Uscourts-Mad-180248-1-0.pdf ''Shiva Ayyadurai v. Gawker Media, et. al''., Complaint] (D. Mass, filed May 10, 2016)</ref><ref name=DavidCrockerWaPo>{{cite news|last=Crocker|first=David|title=A history of e-mail: Collaboration, innovation and the birth of a system|url=http://www.washingtonpost.com/national/on-innovations/a-history-of-e-mail-collaboration-innovation-and-the-birth-of-a-system/2012/03/19/gIQAOeFEPS_story.html|accessdate=10 June 2012|newspaper=Washington Post|date=20 March 2012}}</ref><ref>{{cite web|url=http://blogs.smithsonianmag.com/aroundthemall/2012/02/a-piece-of-email-history-comes-to-the-american-history-museum/|title=A Piece of Email History Comes to the American History Museum|date=22 February 2012|accessdate=11 June 2012|first=Joseph|last=Stromberg|publisher=[[Smithsonian Institution]]}}</ref><ref name=SmithsonianStatement>{{Cite press release|url=http://americanhistory.si.edu/press/releases/statement-national-museum-american-history-collection-materials-va-shiva-ayyudurai|title=Statement from the National Museum of American History: Collection of Materials from V.A. Shiva Ayyadurai|date=23 February 2012|accessdate=19 February 2013|publisher=National Museum of American History}}</ref>
* 1979 – [[MH Message Handling System]] developed at RAND provided several tools for managing electronic mail on Unix.<ref name="borden-mh">A Mail Handling System, Bruce Borden, The Rand Corporation, 1979.</ref>
* 1981 – [[IBM OfficeVision|PROFS]] by IBM<ref>[http://www.ibm.com/ibm100/us/en/icons/networkbus/ ''"...PROFS changed the way organizations communicated, collaborated and approached work when it was introduced by IBM's Data Processing Division in 1981..."''], IBM.com</ref><ref>[https://fas.org/spp/starwars/offdocs/reagan/chron.txt ''"1982 – The National Security Council (NSC) staff at the White House acquires a prototype electronic mail system, from IBM, called the Professional Office System (PROFs)...."''], fas.org</ref>
* 1982 – [[ALL-IN-1]]<ref>{{cite web|url=https://research.microsoft.com/en-us/um/people/gbell/Digital/timeline/1982.htm |title=Gordon Bell's timeline of Digital Equipment Corporation |publisher=Research.microsoft.com |date=1998-01-30 |accessdate=2014-01-09}}</ref> by [[Digital Equipment Corporation]]
* 1982 – HP Mail (later HP DeskManager) by [[Hewlett-Packard]]<ref>{{cite web|url=http://www.hpmuseum.net/divisions.php?did=10|title=HP Computer Museum}}</ref>

These original messaging systems had widely different features and ran on systems that were incompatible with each other. Most of them only allowed communication between users logged into the same host or "mainframe", although there might be hundreds or thousands of users within an organization.

===LAN email systems===
In the early 1980s, networked [[personal computer]]s on [[LAN]]s became increasingly important. Server-based systems similar to the earlier mainframe systems were developed. Again, these systems initially allowed communication only between users logged into the same server infrastructure. Examples include:
* [[cc:Mail]]
* [[Lantastic]]
* [[WordPerfect Office]]
* [[Microsoft Mail]]
* [[Banyan VINES]]
* [[Lotus Notes]]
Eventually these systems too could link different organizations as long as they ran the same email system and proprietary protocol.<ref>with various vendors supplying gateway software to link these incompatible systems</ref>

===Email networks===
To facilitate electronic mail exchange between remote sites and with other organizations, telecommunication links, such as dialup modems or leased lines, provided means to transport email globally, creating local and global networks. This was challenging for a number of reasons, including the widely [[Non-Internet email address|different email address formats]] in use. 
* In 1971 the first [[ARPANET]] email was sent,<ref>{{cite web|title=The First Network Email|url=http://openmap.bbn.com/~tomlinso/ray/firstemailframe.html|author=Ray Tomlinson}}</ref> and through RFC 561, RFC 680, RFC 724, and finally 1977's RFC 733, became a standardized working system.
* PLATO IV was networked to individual terminals over leased data lines prior to the implementation of personal notes in 1974.<ref name=wooley/>
* Unix mail was networked by 1978's [[uucp]],<ref>{{cite web|url=http://cm.bell-labs.com/7thEdMan/vol2/uucp.bun |title=Version 7 Unix manual: "UUCP Implementation Description" by D. A. Nowitz, and "A Dial-Up Network of UNIX Systems" by D. A. Nowitz and M. E. Lesk |accessdate=2014-01-09}}</ref> which was also used for [[USENET]] newsgroup postings, with similar headers.
* BerkNet, the Berkeley Network, was written by [[Eric Schmidt]] in 1978 and included first in the Second Berkeley Software Distribution. It provided support for sending and receiving messages over serial communication links. The Unix mail tool was extended to send messages using BerkNet.<ref name="berknet"/> 
* The [[delivermail]] tool, written by [[Eric Allman]] in 1979 and 1980 (and shipped in 4BSD), provided support for routing mail over dissimilar networks, including Arpanet, UUCP, and BerkNet. (It also provided support for mail user aliases.)<ref name="joy-4bsd">Setting up the Fourth Berkeley Software Tape, William N. Joy, Ozalp Babaoglu, Keith Sklower, University of California, Berkeley, 1980.</ref>
* The mail client included in 4BSD (1980) was extended to provide interoperability between a variety of mail systems.<ref name="shoens-mail">Mail(1), UNIX Programmer's Manual, 4BSD, University of California, Berkeley, 1980.</ref>
* [[BITNET]] (1981) provided electronic mail services for educational institutions. It was based on the IBM VNET email system.<ref>[http://www.livinginternet.com/u/ui_bitnet.htm "BITNET History"], livinginternet.com</ref>
* 1983 – [[MCI Mail]] Operated by MCI Communications Corporation.  This was the first commercial public email service to use the internet. MCI Mail also allowed subscribers to send regular postal mail (overnight) to non-subscribers.<ref>"[[MCI Mail]]", MCI Mail</ref>
* In 1984, IBM PCs running DOS could link with [[FidoNet]] for email and shared bulletin board posting.

===Email address internationalization===
Globally countries started adopting [[Internationalized domain name|IDN]] registrations for supporting country specific scripts (non-English) for domain names. In 2010  Egypt, the Russian Federation, Saudi Arabia, and the United Arab Emirates started offering IDN registrations. The government of India also registered [[Bhārat Gaṇarājya|.bharat]]<ref>{{Cite web|url=https://registry.in/Internationalized_Domain_Names_IDNs|title=Internationalized Domain Names (IDNs) {{!}} Registry.In|website=registry.in|access-date=2016-10-17}}</ref> in 8 languages/scripts in 2014. 
In 2016 Data Xgen Technologies was credited as World's first email platform offering EAI in India and [[Russia]].<ref>http://economictimes.indiatimes.com/tech/internet/datamail-worlds-first-free-linguistic-email-service-supports-eight-india-languages/articleshow/54923001.cms</ref><ref>http://digitalconqurer.com/gadgets/made-india-datamail-empowers-russia-email-address-russian-language/</ref>

===Attempts at interoperability===
{{Refimprove section|date=August 2010}}
Early interoperability among independent systems included:
* [[ARPANET]], a forerunner of the Internet, defined protocols for dissimilar computers to exchange email.
* [[uucp]] implementations for Unix systems, and later for other operating systems, that only had dial-up communications available.
* [[CSNET]], which initially used the UUCP protocols via dial-up to provide networking and mail-relay services for non-ARPANET hosts.
* Action Technologies developed the [[Message Handling System]] (MHS) protocol (later bought by [[Novell]],<ref>[https://books.google.com/books?id=vxcEAAAAMBAJ&pg=PA64 "Delivering the Enterprise Message], 19 Sep 1994, Daniel Blum, Network World</ref><ref>[http://www.networkworld.com/archive/1994/94-03-07hot_.html ''"...offers improved performance, greater reliability and much more flexibility in everything from communications hardware to scheduling..."''], 03/07/94, Mark Gibbs,
Network World</ref><ref>{{cite web | url = http://support.microsoft.com/kb/118859 | title = MHS: Correct Addressing format to DaVinci Email via MHS | work = Microsoft Support Knowledge Base | accessdate = 2007-01-15 }}</ref> which abandoned it after purchasing the non-MHS WordPerfect Office&mdash;renamed [[Novell GroupWise|Groupwise]]).
* [[HP OpenMail]] was known for its ability to interconnect several other APIs and protocols, including MAPI, cc:Mail, SMTP/MIME, and X.400.
* Soft-Switch released its eponymous email gateway product in 1984, acquired by [[Lotus Software]] ten years later.<ref>https://www.linkedin.com/in/nickshelness</ref>
* The [[Coloured Book protocols]] ran on [[United Kingdom|UK]] academic networks until 1992.
* [[X.400]] in the 1980s and early 1990s was promoted by major vendors, and mandated for government use under [[GOSIP]], but abandoned by all but a few in favor of [[Internet]] [[Simple Mail Transfer Protocol|SMTP]] by the mid-1990s.

===From SNDMSG to MSG===
In the early 1970s, [[Ray Tomlinson]] updated an existing utility called [[SNDMSG]] so that it could copy messages (as files) over the network. [[Lawrence Roberts (scientist)|Lawrence Roberts]], the project manager for the ARPANET development, took the idea of READMAIL, which dumped all "recent" messages onto the user's terminal, and wrote a programme for [[TOPS-20#TENEX|TENEX]] in [[Text Editor and Corrector|TECO]] macros called ''RD'', which permitted access to individual messages.<ref name="livinginternet1">{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}</ref> Barry Wessler then updated RD and called it ''NRD''.<ref>* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April–June 2008|url=http://www.ir.bbn.com/~craig/email.pdf|format=PDF | doi =  10.1109/mahc.2008.32|pages=3–29}}</ref>

Marty Yonke rewrote NRD to include reading, access to SNDMSG for sending, and a help system, and called the utility ''WRD'', which was later known as ''BANANARD''. John Vittal then updated this version to include three important commands: ''Move'' (combined save/delete command), ''Answer'' (determined to whom a reply should be sent) and ''Forward'' (sent an email to a person who was not already a recipient). The system was called ''MSG''. With inclusion of these features, MSG is considered to be the first integrated modern email programme, from which many other applications have descended.<ref name="livinginternet1"/>

===ARPANET mail===
Experimental email transfers between separate computer systems began shortly after the creation of the [[ARPANET]] in 1969.<ref name="thvv" /> [[Ray Tomlinson]] is generally credited as having sent the first email across a network, initiating the use of the "[[At sign|@]]" sign to separate the names of the user and the user's machine in 1971, when he sent a message from one [[Digital Equipment Corporation]] [[DEC-10]] computer to another DEC-10. The two machines were placed next to each other.<ref name="firstnetworkemail"/><ref>Wave New World,Time Magazine, October 19, 2009, p.48</ref> Tomlinson's work was quickly adopted across the ARPANET, which significantly increased the popularity of email. Tomlinson is internationally known as the inventor of modern email.<ref>{{cite web|url=http://www.npr.org/2016/03/06/469428062/ray-tomlinson-inventor-of-modern-email-has-died|title=Ray Tomlinson, Inventor Of Modern Email, Dies|date=6 March 2016|work=NPR.org}}</ref>

Initially addresses were of the form, ''username@hostname''<ref>RFC 805, 8 February 1982, Computer Mail Meeting Notes</ref> but were extended to "username@host.domain" with the development of the [[Domain Name System]] (DNS).

As the influence of the ARPANET spread across academic communities, [[Gateway (telecommunications)|gateways]] were developed to pass mail to and from other networks such as [[CSNET]], [[JANET NRS|JANET]], [[BITNET]], [[X.400]], and [[FidoNet]]. This often involved addresses such as:
:hubhost!middlehost!edgehost!user@uucpgateway.somedomain.example.com
which routes mail to a user with a "[[UUCP#Mail routing|bang path]]" address at a UUCP host.

==Operation==
The diagram to the right shows a typical sequence of events<ref>{{cite video|title=How E-mail Works|medium=internet video|publisher=howstuffworks.com|year=2008|url=http://www.webcastr.com/videos/informational/how-email-works.html}}</ref> that takes place when sender [[Placeholder names in cryptography|Alice]] transmits a message using a [[E-mail client|mail user agent]] (MUA) addressed to the [[email address]] of the recipient.
<span style="float:right">[[File:email.svg|400px|Email operation]]</span>
# The MUA formats the message in email format and uses the submission protocol, a profile of the [[Simple Mail Transfer Protocol]] (SMTP), to send the message to the local [[mail submission agent]] (MSA), in this case ''smtp.a.org''.
# The MSA determines the destination address provided in the SMTP protocol (not from the message header), in this case ''bob@b.org''. The part before the @ sign is the ''local part'' of the address, often the [[username]] of the recipient, and the part after the @ sign is a [[domain name]]. The MSA resolves a domain name to determine the [[fully qualified domain name]] of the [[Message transfer agent|mail server]] in the [[Domain Name System]] (DNS).
# The [[DNS server]] for the domain ''b.org'' (''ns.b.org'') responds with any [[MX record]]s listing the mail exchange servers for that domain, in this case ''mx.b.org'', a [[message transfer agent]] (MTA) server run by the recipient's ISP.<ref>[https://dnsdb.cit.cornell.edu/explain_mx.html "MX Record Explanation"], it.cornell.edu</ref>
# smtp.a.org sends the message to mx.b.org using SMTP. This server may need to forward the message to other MTAs before the message reaches the final [[message delivery agent]] (MDA).
# The MDA delivers it to the [[Email Mailbox|mailbox]] of user ''bob''.
# Bob's MUA picks up the message using either the [[Post Office Protocol]] (POP3) or the [[Internet Message Access Protocol]] (IMAP).

In addition to this example, alternatives and complications exist in the email system:
* Alice or Bob may use a client connected to a corporate email system, such as [[IBM]] [[Lotus Notes]] or [[Microsoft]] [[Microsoft Exchange Server|Exchange]]. These systems often have their own internal email format and their clients typically communicate with the email server using a vendor-specific, proprietary protocol. The server sends or receives email via the Internet through the product's Internet mail gateway which also does any necessary reformatting. If Alice and Bob work for the same company, the entire transaction may happen completely within a single corporate email system.
* Alice may not have a MUA on her computer but instead may connect to a [[webmail]] service.
* Alice's computer may run its own MTA, so avoiding the transfer at step 1.
* Bob may pick up his email in many ways, for example logging into mx.b.org and reading it directly, or by using a webmail service.
* Domains usually have several mail exchange servers so that they can continue to accept mail even if the primary is not available.

Many MTAs used to accept messages for any recipient on the Internet and do their best to deliver them. Such MTAs are called ''[[open mail relay]]s''. This was very important in the early days of the Internet when network connections were unreliable.{{citation needed|date=July 2015}}  However, this mechanism proved to be exploitable by originators of [[email spam|unsolicited bulk email]] and as a consequence open mail relays have become rare,<ref name="IMCR-016">{{cite web|url=http://www.imc.org/ube-relay.html |title=Allowing Relaying in SMTP: A Series of Surveys |accessdate=2008-04-13 |last=Hoffman |first=Paul |date=2002-08-20 |work=IMC Reports |publisher=[[Internet Mail Consortium]] |archiveurl=https://web.archive.org/web/20070118121843/http://www.imc.org/ube-relay.html |archivedate=2007-01-18 }}</ref> and many MTAs do not accept messages from open mail relays.

==Message format {{anchor|Internet Message Format}}==
The Internet email message format is now defined by RFC 5322, with multimedia content attachments being defined in RFC 2045 through RFC 2049, collectively called ''[[Multipurpose Internet Mail Extensions]]'' or ''MIME''. RFC 5322 replaced the earlier RFC 2822 in 2008, and in turn RFC 2822 in 2001 replaced RFC 822 – which had been the standard for Internet email for nearly 20 years. Published in 1982, RFC 822 was based on the earlier RFC 733 for the [[ARPANET]].<ref>{{cite web|first=Ken|last=Simpson|title=An update to the email standards|date=October 3, 2008|publisher=MailChannels Blog Entry|url= http://blog.mailchannels.com/2008/10/update-to-email-standards.html}}</ref>

Internet email messages consist of two major sections, the message header and the message body.  The header is structured into [[Field (computer science)|fields]] such as From, To, CC, Subject, Date, and other information about the email. In the process of transporting email messages between systems, SMTP communicates delivery parameters and information using message header fields. The body contains the message, as unstructured text, sometimes containing a [[signature block]] at the end. The header is separated from the body by a blank line.

===Message header===
<!-- This section is linked from [[Bracket]] -->
Each message has exactly one [[Header (computing)|header]], which is structured into [[Field (computer science)|fields]]. Each field has a name and a value. RFC 5322 specifies the precise syntax.

Informally, each line of text in the header that begins with a [[Printable characters|printable character]] begins a separate field. The field name starts in the first character of the line and ends before the separator character ":". The separator is then followed by the field value (the "body" of the field). The value is continued onto subsequent lines if those lines have a space or tab as their first character. Field names and values are restricted to 7-bit [[ASCII]] characters. Non-ASCII values may be represented using MIME [[MIME#Encoded-Word|encoded words]].

====Header fields====
Email header fields can be multi-line, and each line should be at most 78 characters long and in no event more than 998 characters long.<ref>{{cite web|url=https://tools.ietf.org/html/rfc5322|title=RFC 5322, Internet Message Format|author=P. Resnick, Ed.|date=October 2008|publisher=IETF}}</ref> Header fields defined by RFC 5322 can only contain [[US-ASCII]] characters; for encoding characters in other sets, a syntax specified in RFC 2047 can be used.<ref>{{cite web|last=Moore|first=K|title=MIME (Multipurpose Internet Mail Extensions) Part Three: Message Header Extensions for Non-ASCII Text|url=https://tools.ietf.org/html/rfc2047|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=2012-01-21| date=November 1996 }}</ref> Recently the IETF EAI working group has defined some standards track extensions,<ref>{{cite web|url=https://tools.ietf.org/html/rfc6532|title=RFC 6532, Internationalized Email Headers|author=A Yang, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}</ref><ref>{{cite web|url=https://tools.ietf.org/html/rfc6531|title=RFC 6531, SMTP Extension for Internationalized Email Addresses|author=J. Yao, Ed., W. Mao, Ed.|date=February 2012|publisher=IETF|ISSN=2070-1721}}</ref> replacing previous experimental extensions, to allow [[UTF-8]] encoded [[Unicode]] characters to be used within the header. In particular, this allows email addresses to use non-ASCII characters. Such addresses are supported by Google and Microsoft products, and promoted by some governments.<ref name="economictimes.indiatimes.com">{{Cite news|url=http://economictimes.indiatimes.com/tech/internet/now-get-your-email-address-in-hindi/articleshow/53830034.cms|title=Now, get your email address in Hindi - The Economic Times|newspaper=The Economic Times|access-date=2016-10-17}}</ref>

The message header must include at least the following fields:<ref>{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6 |title=RFC 5322, 3.6. Field Definitions |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}</ref><ref>{{cite web|url=https://tools.ietf.org/html/rfc5322#section-3.6.4 |title=RFC 5322, 3.6.4. Identification Fields |publisher=Tools.ietf.org |date=October 2008|accessdate=2014-01-09}}</ref>
* ''From'': The [[email address]], and optionally the name of the author(s). In many email clients not changeable except through changing account settings.
* ''Date'': The local time and date when the message was written. Like the ''From:'' field, many email clients fill this in automatically when sending. The recipient's client may then display the time in the format and time zone local to him/her.

RFC 3864 describes registration procedures for message header fields at the [[Internet Assigned Numbers Authority|IANA]]; it provides for [http://www.iana.org/assignments/message-headers/perm-headers.html permanent] and [http://www.iana.org/assignments/message-headers/prov-headers.html provisional] field names, including also fields defined for MIME, netnews, and HTTP, and referencing relevant RFCs. Common header fields for email include:<ref>{{cite web|url=https://tools.ietf.org/html/rfc5064 |title=RFC 5064 |publisher=Tools.ietf.org|date=December 2007 |accessdate=2014-01-09}}</ref>

* ''To'': The email address(es), and optionally name(s) of the message's recipient(s). Indicates primary recipients (multiple allowed), for secondary recipients see Cc: and Bcc: below.
* ''Subject'': A brief summary of the topic of the message. [[E-mail subject abbreviations|Certain abbreviations]] are commonly used in the subject, including [[E-mail subject abbreviations|"RE:" and "FW:"]].
* ''Cc'': [[Carbon copy]]; Many email clients will mark email in one's inbox differently depending on whether they are in the To: or Cc: list. (''Bcc'': [[Blind carbon copy]]; addresses are usually only specified during SMTP delivery, and not usually listed in the message header.)
* [[Content-Type]]: Information about how the message is to be displayed, usually a [[MIME]] type.
* ''Precedence'': commonly with values "bulk", "junk", or "list"; used to indicate that automated "vacation" or "out of office" responses should not be returned for this mail, e.g. to prevent vacation notices from being sent to all other subscribers of a mailing list. [[Sendmail]] uses this field to affect prioritization of queued email, with "Precedence: special-delivery" messages delivered sooner. With modern high-bandwidth networks, delivery priority is less of an issue than it once was. [[Microsoft Exchange Server|Microsoft Exchange]] respects a fine-grained automatic response suppression mechanism, the ''X-Auto-Response-Suppress'' field.<ref>Microsoft, Auto Response Suppress, 2010, [http://msdn.microsoft.com/en-us/library/ee219609(v=EXCHG.80).aspx microsoft reference], 2010 Sep 22</ref>
* ''Message-ID'': Also an automatically generated field; used to prevent multiple delivery and for reference in In-Reply-To: (see below).
* ''In-Reply-To'': [[Message-ID]] of the message that this is a reply to. Used to link related messages together. This field only applies for reply messages.
* ''References'': [[Message-ID]] of the message that this is a reply to, and the message-id of the message the previous reply was a reply to, etc.
* ''Reply-To'': Address that should be used to reply to the message.
* ''Sender'': Address of the actual sender acting on behalf of the author listed in the From: field (secretary, list manager, etc.).
* ''Archived-At'': A direct link to the archived form of an individual email message.

Note that the ''To:'' field is not necessarily related to the addresses to which the message is delivered. The actual delivery list is supplied separately to the transport protocol, [[Simple Mail Transfer Protocol|SMTP]], which may or may not originally have been extracted from the header content. The "To:" field is similar to the addressing at the top of a conventional letter which is delivered according to the address on the outer envelope. In the same way, the "From:" field does not have to be the real sender of the email message. Some mail servers apply [[email authentication]] systems to messages being relayed. Data pertaining to server's activity is also part of the header, as defined below.

SMTP defines the ''trace information'' of a message, which is also saved in the header using the following two fields:<ref>{{cite IETF|title=Simple Mail Transfer Protocol|rfc=5321|author=[[John Klensin]]|sectionname=Trace Information|section=4.4| date=October 2008 |publisher=[[Internet Engineering Task Force|IETF]]}}</ref>
* ''Received'': when an SMTP server accepts a message it inserts this trace record at the top of the header (last to first).
* ''Return-Path'': when the delivery SMTP server makes the ''final delivery'' of a message, it inserts this field at the top of the header.

Other fields that are added on top of the header by the receiving server may be called ''trace fields'', in a broader sense.<ref>{{cite web|url=http://www.ietf.org/mail-archive/web/apps-discuss/current/msg04115.html|title=Trace headers|author=John Levine|date=14 January 2012|work=email message|publisher=[[Internet Engineering Task Force|IETF]]|accessdate=16 January 2012|quote=there are many more trace fields than those two}}</ref>
* ''Authentication-Results'': when a server carries out authentication checks, it can save the results in this field for consumption by downstream agents.<ref>This extensible field is defined by RFC 7001, that also defines an [[Internet Assigned Numbers Authority|IANA]] registry of [http://www.iana.org/assignments/email-auth/ Email Authentication Parameters].</ref>
* ''Received-SPF'': stores results of [[Sender Policy Framework|SPF]] checks in more detail than Authentication-Results.<ref>RFC 7208.</ref>
* ''Auto-Submitted'': is used to mark automatically generated messages.<ref>Defined in RFC 3834, and updated by RFC 5436.</ref>
* ''VBR-Info'': claims [[Vouch by Reference|VBR]] whitelisting<ref>RFC 5518.</ref>

===Message body===

====Content encoding====
Email was originally designed for 7-bit [[ASCII]].<ref>{{cite book|title=TCP/IP Network Administration|year=2002|isbn=978-0-596-00297-8|author=Craig Hunt|publisher=[[O'Reilly Media]]|page=70}}</ref> Most email software is [[8-bit clean]] but must assume it will communicate with 7-bit servers and mail readers. The [[MIME]] standard introduced character set specifiers and two content transfer encodings to enable transmission of non-ASCII data: [[quoted printable]] for mostly 7-bit content with a few characters outside that range and [[base64]] for arbitrary binary data. The [[8BITMIME]] and [[BINARY]] extensions were introduced to allow transmission of mail without the need for these encodings, but many [[mail transport agent]]s still do not support them fully. In some countries, several encoding schemes coexist; as the result, by default, the message in a non-Latin alphabet language appears in non-readable form (the only exception is coincidence, when the sender and receiver use the same encoding scheme). Therefore, for international [[character set]]s, [[Unicode]] is growing in popularity.{{citation needed|date=September 2014}}

====Plain text and HTML====
Most modern graphic [[email client]]s allow the use of either [[plain text]] or [[HTML#HTML email|HTML]] for the message body at the option of the user. [[HTML email]] messages often include an automatically generated plain text copy as well, for compatibility reasons. Advantages of HTML include the ability to include in-line links and images, set apart previous messages in [[block quote]]s, wrap naturally on any display, use emphasis such as [[underline]]s and [[italics]], and change [[font]] styles. Disadvantages include the increased size of the email, privacy concerns about [[web bug]]s, abuse of HTML email as a vector for [[phishing]] attacks and the spread of [[malware|malicious software]].<ref>{{cite web|title=Email policies that prevent viruses|url=http://advosys.ca/papers/mail-policies.html}}</ref>

Some web-based [[mailing list]]s recommend that all posts be made in plain-text, with 72 or 80 [[characters per line]]<ref>{{cite web|url=http://helpdesk.rootsweb.com/listadmins/plaintext.html |title=When posting to a RootsWeb mailing list... |publisher=Helpdesk.rootsweb.com |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.openbsd.org/mail.html |title=...Plain text, 72 characters per line... |publisher=Openbsd.org |accessdate=2014-01-09}}</ref> for all the above reasons, but also because they have a significant number of readers using [[List of email clients#Text-based|text-based email clients]] such as [[Mutt (email client)|Mutt]]. Some [[Microsoft]] [[email client]]s allow rich formatting using their proprietary [[Rich Text Format]] (RTF), but this should be avoided unless the recipient is guaranteed to have a compatible [[email client]].<ref>{{cite web|url=http://support.microsoft.com/kb/138053 |title=How to Prevent the Winmail.dat File from Being Sent to Internet Users |publisher=Support.microsoft.com |date=2010-07-02 |accessdate=2014-01-09}}</ref>

==Servers and client applications==
[[File:Mozilla Thunderbird 3.1.png|thumb|right|300px|The interface of an email client, [[Mozilla Thunderbird|Thunderbird]].]]
<!-- This section is linked from [[Catch-all (Mail)]]. See [[WP:MOS#Section management]] -->
Messages are exchanged between hosts using the [[Simple Mail Transfer Protocol]] with software programs called [[mail transfer agent]]s (MTAs); and delivered to a mail store by programs called [[mail delivery agent]]s (MDAs, also sometimes called local delivery agents, LDAs). Accepting a message obliges an MTA to deliver it,<ref>In practice, some accepted messages may nowadays not be delivered to the recipient's InBox, but instead to a Spam or Junk folder which, especially in a corporate environment, may be inaccessible to the recipient</ref> and when a message cannot be delivered, that MTA must send a [[bounce message]] back to the sender, indicating the problem.

Users can retrieve their messages from servers using standard protocols such as [[Post Office Protocol|POP]] or [[IMAP]], or, as is more likely in a large [[corporation|corporate]] environment, with a [[Proprietary software|proprietary]] protocol specific to [[Novell Groupwise]], [[Lotus Notes]] or [[Microsoft Exchange Server]]s.  Programs used by users for retrieving, reading, and managing email are called [[mail user agent]]s (MUAs).

Mail can be stored on the [[client (computing)|client]], on the [[Server (computing)|server]] side, or in both places. Standard formats for mailboxes include [[Maildir]] and [[mbox]]. Several prominent email clients use their own proprietary format and require conversion software to transfer email between them. Server-side storage is often in a proprietary format but since access is through a standard protocol such as [[IMAP]], moving email from one server to another can be done with any [[Mail user agent|MUA]] supporting the protocol.

Many current email users do not run MTA, MDA or MUA programs themselves, but use a web-based email platform, such as Gmail, Hotmail, or Yahoo! Mail, that performs the same tasks.<ref>http://dir.yahoo.com/business_and_economy/business_to_business/communications_and_networking/internet_and_world_wide_web/email_providers/free_email/</ref> Such [[webmail]] interfaces allow users to access their mail with any standard [[web browser]], from any computer, rather than relying on an email client.

===Filename extensions===
Upon reception of email messages, [[email client]] applications save messages in operating system files in the file system. Some clients save individual messages as separate files, while others use various database formats, often proprietary, for collective storage. A historical standard of storage is the ''[[mbox]]'' format. The specific format used is often indicated by special [[filename extension]]s:
;<tt>eml</tt>
:Used by many email clients including [[Novell GroupWise]], [[Microsoft Outlook Express]], [[Lotus notes]], [[Windows Mail]], [[Mozilla Thunderbird]], and Postbox. The files are [[plain text]] in [[MIME]] format, containing the email header as well as the message contents and attachments in one or more of several formats.
;<tt>emlx</tt>
:Used by [[Apple Mail]].
;<tt>msg</tt>
:Used by [[Microsoft Outlook|Microsoft Office Outlook]] and [[OfficeLogic|OfficeLogic Groupware]].
;<tt>mbx</tt>
:Used by [[Opera Mail]], [[KMail]], and [[Apple Mail]] based on the [[mbox]] format.

Some applications (like [[Apple Mail]]) leave attachments encoded in messages for searching while also saving separate copies of the attachments. Others separate attachments from messages and save them in a specific directory.

===URI scheme mailto===
{{main article|mailto}}
The [[URI scheme]], as registered with the [[Internet Assigned Numbers Authority|IANA]], defines the <tt>mailto:</tt> scheme for SMTP email addresses. Though its use is not strictly defined, URLs of this form are intended to be used to open the new message window of the user's mail client when the URL is activated, with the address as defined by the URL in the ''To:'' field.<ref>RFC 2368 section 3 : by Paul Hoffman in 1998 discusses operation of the "mailto" URL.</ref>

==Types==

===Web-based email===
{{main article|Webmail}}
Many email providers have a web-based email client (e.g. [[AOL Mail]], [[Gmail]], [[Outlook.com]], [[Hotmail]] and [[Yahoo! Mail]]). This allows users to log in to the email account by using any compatible [[web browser]] to send and receive their email. Mail is typically not downloaded to the client, so can't be read without a current Internet connection.

===POP3 email services===
The [[Post Office Protocol]] 3 (POP3) is a mail access protocol used by a client application to read messages from the mail server. Received messages are often deleted from the [[server (computing)|server]]. POP supports simple download-and-delete requirements for access to remote mailboxes (termed maildrop in the POP RFC's).<ref name="Windows to Linux">{{cite book | last = Allen | first = David | title = Windows to Linux | publisher = Prentice Hall | year = 2004 | location = | page =192 | url = https://books.google.com/books?id=UD0h_GqgbHgC&printsec=frontcover&dq=network%2B+guide+to+networks&hl=en&src=bmrr&ei=hMnATfmmA8j00gGMsOC2Cg&sa=X&oi=book_result&ct=result&resnum=1&ved=0CE8Q6AEwAA#v=onepage&q&f=false}}</ref>

===IMAP email servers===
The [[Internet Message Access Protocol]] (IMAP) provides features to manage a mailbox from multiple devices. Small portable devices like [[smartphone]]s are increasingly used to check email while travelling, and to make brief replies, larger devices with better keyboard access being used to reply at greater length. IMAP shows the headers of messages, the sender and the subject and the device needs to request to download specific messages. Usually mail is left in folders in the mail server.

===MAPI email servers===
[[MAPI|Messaging Application Programming Interface]] (MAPI) is used by [[Microsoft Outlook]] to communicate to [[Microsoft Exchange Server]] - and to a range of other email server products such as [[Axigen|Axigen Mail Server]], [[Kerio Connect]], [[Scalix]], [[Zimbra]], [[HP OpenMail]], [[IBM Lotus Notes]], [[Zarafa (software)|Zarafa]], and [[Bynari]] where vendors have added MAPI support to allow their products to be accessed directly via Outlook.

==Uses==
{{Refimprove section|date=November 2007}}

===Business and organizational use===
Email has been widely accepted by business, governments and non-governmental organizations in the developed world, and it is one of the key parts of an 'e-revolution' in workplace communication (with the other key plank being widespread adoption of highspeed [[Internet]]). A sponsored 2010 study on workplace communication found 83% of U.S. knowledge workers felt email was critical to their success and productivity at work.<ref name=om>By Om Malik, GigaOm. "[http://gigaom.com/collaboration/is-email-a-curse-or-a-boon/ Is Email a Curse or a Boon?]" September 22, 2010. Retrieved October 11, 2010.</ref>

It has some key benefits to business and other organizations, including:
; Facilitating logistics
: Much of the business world relies on communications between people who are not physically in the same building, area, or even country; setting up and attending an in-person meeting, [[telephone call]], or [[conference call]] can be inconvenient, time-consuming, and costly. Email provides a method of exchanging information between two or more people with no set-up costs and that is generally far less expensive than a physical meeting or phone call.
; Helping with synchronisation
: With [[Real-time computing|real time]] communication by meetings or phone calls, participants must work on the same schedule, and each participant must spend the same amount of time in the meeting or call. Email allows [[wikt:asynchrony|asynchrony]]: each participant may control their schedule independently.
; Reducing cost
: Sending an email is much less expensive than sending postal mail, or [[long distance telephone call]]s, [[telex]] or [[telegrams]].
; Increasing speed
: Much faster than most of the alternatives.
; Creating a "written" record
: Unlike a telephone or in-person conversation, email by its nature creates a detailed written record of the communication, the identity of the sender(s) and recipient(s) and the date and time the message was sent. In the event of a contract or legal dispute, saved emails can be used to prove that an individual was advised of certain issues, as each email has the date and time recorded on it.

====Email marketing====
Email marketing via "[[opt-in email|opt-in]]" is often successfully used to send special sales offerings and new product information.<ref name=brett>{{cite journal | last1 = Martin | first1 = Brett A. S. | last2 = Van Durme | first2 = Joel | last3 = Raulas | first3 = Mika | last4 = Merisavo | first4 = Marko | year = 2003 | title = E-mail Marketing: Exploratory Insights from Finland | url = http://www.basmartin.com/wp-content/uploads/2010/08/Martin-et-al-2003.pdf | format = PDF | journal = Journal of Advertising Research | volume = 43 | issue = 3| pages = 293–300 | doi=10.1017/s0021849903030265}}</ref> Depending on the recipient's culture,<ref>{{cite web|url=http://www.computerworld.com/article/2467778/endpoint-security/spam-culture--part-1--china.html|title=Spam culture, part 1: China|first=Amir|last=Lev|publisher=}}</ref> email sent without permission&mdash;such as an "opt-in"&mdash;is likely to be viewed as unwelcome "[[email spam]]".

===Personal use===

====Desktop====
Many users access their personal email from friends and family members using a [[desktop computer]] in their house or apartment.

====Mobile====
Email has become widely used on [[smartphone]]s and [[Wi-Fi]]-enabled [[laptop]]s and [[tablet computer]]s. Mobile "apps" for email increase accessibility to the medium for users who are out of their home. While in the earliest years of email, users could only access email on desktop computers, in the 2010s, it is possible for users to check their email when they are away from home, whether they are across town or across the world. Alerts can also be sent to the smartphone or other device to notify them immediately of new messages. This has given email the ability to be used for more frequent communication between users and allowed them to check their email and write messages throughout the day. Today, there are an estimated 1.4 billion email users worldwide and 50 billion non-spam emails that are sent daily.

Individuals often check email on smartphones for both personal and work-related messages. It was found that US adults check their email more than they browse the web or check their [[Facebook]] accounts, making email the most popular activity for users to do on their smartphones. 78% of the respondents in the study revealed that they check their email on their phone.<ref>{{cite web|url=http://marketingland.com/smartphone-activities-study-email-web-facebook-37954|title=Email Is Top Activity On Smartphones, Ahead Of Web Browsing & Facebook [Study]|date=28 March 2013|publisher=}}</ref> It was also found that 30% of consumers use only their smartphone to check their email, and 91% were likely to check their email at least once per day on their smartphone. However, the percentage of consumers using email on smartphone ranges and differs dramatically across different countries. For example, in comparison to 75% of those consumers in the US who used it, only 17% in India did.<ref>{{cite web|url=http://www.emailmonday.com/mobile-email-usage-statistics|title=The ultimate mobile email statistics overview|publisher=}}</ref>

==Issues==
{{Refimprove section|date=October 2016}}

===Attachment size limitation===
{{Main article|Email attachment}}
Email messages may have one or more attachments, which are additional files that are appended to the email. Typical attachments include [[Microsoft Word]] documents, [[pdf]] documents and scanned images of paper documents. In principle there is no technical restriction on the size or number of attachments, but in practice email clients, [[server (computing)|server]]s and Internet service providers implement various limitations on the size of files, or complete email - typically to 25MB or less.<ref>[http://exchangepedia.com/2007/09/exchange-server-2007-setting-message-size-limits.html ''"Setting Message Size Limits in Exchange 2010 and Exchange 2007"''].</ref><ref>[http://www.geek.com/articles/news/google-updates-file-size-limits-for-gmail-and-youtube-20090629/#ixzz0oIzFY0Q8 ''"Google updates file size limits for Gmail and YouTube"'', geek.com].</ref><ref>[http://mail.google.com/support/bin/answer.py?answer=8770&topic=1517 ''"Maximum attachment size"'', mail.google,com].</ref> Furthermore, due to technical reasons, attachment sizes as seen by these transport systems can differ to what the user sees,<ref>{{cite web|url=http://technet.microsoft.com/en-us/magazine/2009.01.exchangeqa.aspx?pr=blog|title=Exchange 2007: Attachment Size Increase,...|date=2010-03-25|publisher=TechNet Magazine, Microsoft.com US}}</ref> which can be confusing to senders when trying to assess whether they can safely send a file by email. Where larger files need to be shared, [[file hosting service]]s of various sorts are available; and generally suggested.<ref>[https://support.office.com/en-us/article/Send-large-files-to-other-people-7005da19-607a-47d5-b2c5-8f3982c6cc83 "Send large files to other people"], Microsoft.com</ref><ref>[http://www.makeuseof.com/tag/8-ways-to-email-large-attachments/ "8 ways to email large attachments"], Chris Hoffman, December 21, 2012, makeuseof.com</ref> Some large files, such as digital photos, color presentations and video or music files are too large for some email systems.

===Information overload===
The ubiquity of email for knowledge workers and "white collar" employees has led to concerns that recipients face an "[[information overload]]" in dealing with increasing volumes of email.<ref>{{cite web|last=Radicati|first=Sara|title=Email Statistics Report, 2010|url=http://www.radicati.com/wp/wp-content/uploads/2010/04/Email-Statistics-Report-2010-2014-Executive-Summary2.pdf}}</ref><ref>{{cite news|last=Gross|first=Doug|title=Happy Information Overload Day!|url=http://articles.cnn.com/2010-10-20/tech/information.overload.day_1_mails-marsha-egan-rss?_s=PM:TECH|work=CNN|date=July 26, 2011}}</ref> This can lead to increased stress, decreased satisfaction with work, and some observers even argue it could have a significant negative economic effect,<ref>{{cite news|url=http://www.nytimes.com/2008/04/20/technology/20digi.html?_r=2&oref=slogin&oref=slogin|title=Struggling to Evade the E-Mail Tsunami|date=2008-04-20|publisher=The New York Times|first=Randall|last=Stross|accessdate=May 1, 2010}}</ref> as efforts to read the many emails could reduce [[productivity]].

===Spam===
{{Main article|Email spam}}
Email "spam" is the term used to describe unsolicited bulk email. The low cost of sending such email meant that by 2003 up to 30% of total email traffic was already spam.<ref>[http://visionedgemarketing.com/growth-of-spam-email-2/ "Growth of Spam Email"]</ref><ref name="R">Rich Kawanagh. The top ten email spam list of 2005. ITVibe news, 2006, January 02, [http://itvibe.com/news/3837/ ITvibe.com]</ref><ref>How Microsoft is losing the war on spam [http://dir.salon.com/story/tech/feature/2005/01/19/microsoft_spam/index.html Salon.com]</ref> and was threatening the usefulness of email as a practical tool. The US [[CAN-SPAM Act of 2003]] and similar laws elsewhere<ref>Spam Bill 2003 ([http://www.aph.gov.au/library/pubs/bd/2003-04/04bd045.pdf PDF])</ref> had some impact, and a number of effective [[anti-spam techniques (email)|anti-spam techniques]] now largely mitigate the impact of spam by filtering or rejecting it for most users,<ref>[http://www.wired.com/2015/07/google-says-ai-catches-99-9-percent-gmail-spam/ "Google Says Its AI Catches 99.9 Percent of Gmail Spam"], Cade Metz, July 09 2015, wired.com</ref> but the volume sent is still very high&mdash;and increasingly consists not of advertisements for products, but malicious content or links.<ref name="securelist">[https://securelist.com/analysis/quarterly-spam-reports/74682/spam-and-phishing-in-q1-2016/ "Spam and phishing in Q1 2016"],  May 12, 2016, securelist.com</ref>

===Malware===
A range of malicious email types exist. These range from [[List of email scams|various types of email scams]], including [[Social engineering (security)|"social engineering"]] scams such as [[advance-fee scam]] "Nigerian letters", to [[phishing]], [[email bomb]]ardment and [[Computer worm|email worms]].

===Email spoofing===
{{Main article|Email spoofing}}
[[Email spoofing]] occurs when the email message header is designed to make the message appear to come from a known or trusted source. [[Email spam]] and [[phishing]] methods typically use spoofing to mislead the recipient about the true message origin. Email spoofing may be done as a prank, or as part of a criminal effort to defraud an individual or organization. An example of a potentially fraudulent email spoofing is if an individual creates an email which appears to be an invoice from a major company, and then sends it to one or more recipients. In some cases, these fraudulent emails incorporate the logo of the purported organization and even the email address may appear legitimate.

===Email bombing===
{{main article|Email bomb}}

[[Email bomb]]ing is the intentional sending of large volumes of messages to a target address. The overloading of the target email address can render it unusable and can even cause the mail server to crash.

===Privacy concerns===
{{Main article|Internet privacy}}

Today it can be important to distinguish between Internet and internal email systems. Internet email may travel and be stored on networks and computers without the sender's or the recipient's control. During the transit time it is possible that third parties read or even modify the content. Internal mail systems, in which the information never leaves the organizational network, may be more secure, although [[information technology]] personnel and others whose function may involve monitoring or managing may be accessing the email of other employees.

Email privacy, without some security precautions, can be compromised because:
* email messages are generally not encrypted.
* email messages have to go through intermediate computers before reaching their destination, meaning it is relatively easy for others to intercept and read messages.
* many Internet Service Providers (ISP) store copies of email messages on their mail servers before they are delivered. The backups of these can remain for up to several months on their server, despite deletion from the mailbox.
* the "Received:"-fields and other information in the email can often identify the sender, preventing anonymous communication.

There are [[cryptography]] applications that can serve as a remedy to one or more of the above. For example, [[Virtual Private Network]]s or the [[Tor (anonymity network)|Tor anonymity network]] can be used to encrypt traffic from the user machine to a safer network while [[GNU Privacy Guard|GPG]], [[Pretty Good Privacy|PGP]], SMEmail,<ref>[http://www.arxiv.org/pdf/1002.3176 SMEmail – A New Protocol for the Secure E-mail in Mobile Environments], Proceedings of the Australian Telecommunications Networks and Applications Conference (ATNAC'08), pp. 39–44, Adelaide, Australia, Dec. 2008.</ref> or [[S/MIME]] can be used for [[end-to-end principle|end-to-end]] message encryption, and SMTP STARTTLS or SMTP over [[Transport Layer Security]]/Secure Sockets Layer can be used to encrypt communications for a single mail hop between the SMTP client and the SMTP server.

Additionally, many [[mail user agent]]s do not protect logins and passwords, making them easy to intercept by an attacker. Encrypted authentication schemes such as [[Simple Authentication and Security Layer|SASL]] prevent this. Finally, attached files share many of the same hazards as those found in [[Peer-to-peer|peer-to-peer filesharing]]. Attached files may contain [[Trojan horse (computing)|trojans]] or [[Computer virus|viruses]].

===Flaming===
[[Flaming (Internet)|Flaming]] occurs when a person sends a message (or many messages) with angry or antagonistic content. The term is derived from the use of the word "incendiary" to describe particularly heated email discussions. The ease and impersonality of email communications mean that the [[social norms]] that encourage civility in person or via telephone do not exist and civility may be forgotten.<ref>{{cite journal|author1=S. Kiesler |author2=D. Zubrow |author3=A.M. Moses |author4=V. Geller |title=Affect in computer-mediated communication: an experiment in synchronous terminal-to-terminal discussion|journal=Human-Computer Interaction|volume=1|pages=77–104|year=1985|doi=10.1207/s15327051hci0101_3}}</ref>

===Email bankruptcy===
{{main article|Email bankruptcy}}
Also known as "email fatigue", email bankruptcy is when a user ignores a large number of email messages after falling behind in reading and answering them. The reason for falling behind is often due to information overload and a general sense there is so much information that it is not possible to read it all. As a solution, people occasionally send a "boilerplate" message explaining that their email inbox is full, and that they are in the process of clearing out all the messages. [[Harvard University]] law professor [[Lawrence Lessig]] is credited with coining this term, but he may only have popularized it.<ref>{{cite news|title=All We Are Saying.|url=http://www.nytimes.com/2007/12/23/weekinreview/23buzzwords.html?ref=weekinreview|publisher=The New York Times|date=December 23, 2007|accessdate=2007-12-24|first=Grant|last=Barrett}}</ref>

===Tracking of sent mail===
The original SMTP mail service provides limited mechanisms for tracking a transmitted message, and none for verifying that it has been delivered or read. It requires that each mail server must either deliver it onward or return a failure notice (bounce message), but both software bugs and system failures can cause messages to be lost. To remedy this, the [[Internet Engineering Task Force|IETF]] introduced [[Delivery Status Notification]]s (delivery receipts) and [[Return receipt#Email|Message Disposition Notifications]] (return receipts); however, these are not universally deployed in production. (A complete Message Tracking mechanism was also defined, but it never gained traction; see RFCs 3885<ref>RFC 3885, ''SMTP Service Extension for Message Tracking''</ref> through 3888.<ref>RFC 3888, ''Message Tracking Model and Requirements''</ref>)

Many ISPs now deliberately disable non-delivery reports (NDRs) and delivery receipts due to the activities of spammers:
* Delivery Reports can be used to verify whether an address exists and if so, this indicates to a spammer that it is available to be spammed.
* If the spammer uses a forged sender email address ([[email spoofing]]), then the innocent email address that was used can be flooded with NDRs from the many invalid email addresses the spammer may have attempted to mail. These NDRs then constitute spam from the ISP to the innocent user.

In the absence of standard methods, a range of system based around the use of [[web bug]]s have been developed. However, these are often seen as underhand or raising privacy concerns,<ref>{{cite news|url=http://query.nytimes.com/gst/fullpage.html?res=940CE0D9143AF931A15752C1A9669C8B63&sec=&spon=&pagewanted=print|title=Software That Tracks E-Mail Is Raising Privacy Concerns|author=Amy Harmon|publisher=The New York Times|date=2000-11-22|accessdate=2012-01-13}}</ref><ref>{{cite web|url=http://email.about.com/od/emailbehindthescenes/a/html_return_rcp.htm |title=About.com |publisher=Email.about.com |date=2013-12-19 |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.webdevelopersnotes.com/tips/yahoo/notification-when-yahoo-email-is-opened.php |title=Webdevelopersnotes.com |publisher=Webdevelopersnotes.com |accessdate=2014-01-09}}</ref> and only work with email clients that support rendering of HTML. Many mail clients now default to not showing "web content".<ref>[http://www.slipstick.com/outlook/email/microsoft-outlook-web-bugs-blocked-html-images "Outlook: Web Bugs & Blocked HTML Images"], slipstick.com</ref> [[Webmail]] providers can also disrupt web bugs by pre-caching images.<ref>[http://arstechnica.com/information-technology/2013/12/gmail-blows-up-e-mail-marketing-by-caching-all-images-on-google-servers/ "Gmail blows up e-mail marketing..."], Ron Amadeo, Dec 13 2013, Ars Technica</ref>

==U.S. government==
The U.S. state and federal governments have been involved in electronic messaging and the development of email in several different ways. Starting in 1977, the U.S. Postal Service (USPS) recognized that electronic messaging and electronic transactions posed a significant threat to First Class mail volumes and revenue. The USPS explored an electronic messaging initiative in 1977 and later disbanded it. Twenty years later, in 1997, when email volume overtook postal mail volume, the USPS was again urged to embrace email, and the USPS declined to provide email as a service.<ref>{{cite web|url=http://www.fastcompany.com/1780716/can-technology-save-us-postal-service|title=Can Technology Save The U.S. Postal Service?|work=Fast Company}}</ref><ref>{{cite web|url=http://tech.mit.edu/V131/N60/emaillab.html|title=Can an MIT professor save the USPS? - The Tech|work=mit.edu}}</ref><ref>{{cite web|url=http://www.fedtechmagazine.com/article/2013/01/why-united-states-postal-service-taking-cues-silicon-valley|title=Why the USPS Is Taking Cues from Silicon Valley|work=FedTech}}</ref> The USPS initiated an experimental email service known as [[E-COM]]. E-COM provided a method for the simple exchange of text messages. In 2011, shortly after the USPS reported its state of financial bankruptcy, the USPS Office of Inspector General (OIG) began exploring the possibilities of generating revenue through email servicing.<ref>{{cite web|url=http://cmsw.mit.edu/usps-can-save-itself/|title=Shiva Ayyadurai: USPS can save itself|work=MIT Comparative Media Studies/Writing}}</ref><ref>{{cite web|url=http://bostinno.streetwise.co/2012/01/13/could-email-save-snail-mail-or-is-the-internet-too-reliant-on-the-usps/|title=Could Email Save Snail Mail, Or Is The Internet Too Reliant on the USPS?|date=6 March 2012|work=BostInno}}</ref><ref>{{cite web|url=http://www.bostonglobe.com/lifestyle/2012/03/02/dear-usps/V4GJ8w9UCcfV4v0WiVjvmK/story.html|title=‘Dear USPS . . .’|work=BostonGlobe.com}}</ref> Electronic messages were transmitted to a post office, printed out, and delivered as hard copy. To take advantage of the service, an individual had to transmit at least 200 messages. The delivery time of the messages was the same as First Class mail and cost 26 cents. Both the [[Postal Regulatory Commission]] and the [[Federal Communications Commission]] opposed E-COM. The FCC concluded that E-COM constituted common carriage under its jurisdiction and the USPS would have to file a [[tariff]].<ref>In re Request for declaratory ruling and investigation by Graphnet Systems, Inc., concerning the proposed E-COM service, FCC Docket No. 79-6 (September 4, 1979)</ref> Three years after initiating the service, USPS canceled E-COM and attempted to sell it off.<ref>Hardy, Ian R; [https://archive.org/web/*/http:/www.ifla.org/documents/internet/hari1.txt The Evolution of ARPANET Email]; 1996-05-13; History Thesis Paper; University of California at Berkeley</ref><ref>James Bovard, The Law Dinosaur: The US Postal Service, CATO Policy Analysis (February 1985)</ref><ref name="JayAkkad">{{cite web|url=http://www.cs.ucsb.edu/~almeroth/classes/F04.176A/homework1_good_papers/jay-akkad.html |title=Jay Akkad, The History of Email |publisher=Cs.ucsb.edu |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.gao.gov/archive/2000/gg00188.pdf |title=US Postal Service: Postal Activities and Laws Related to Electronic Commerce, GAO-00-188 |format=PDF |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://govinfo.library.unt.edu/ota/Ota_4/DATA/1982/8214.PDF |title=Implications of Electronic Mail and Message Systems for the U.S. Postal Service , Office of Technology Assessment, Congress of the United States, August 1982 |format=PDF |accessdate=2014-01-09}}</ref>

The early ARPANET dealt with multiple email clients that had various, and at times incompatible, formats. For example, in the [[Multics]], the "@" sign meant "kill line" and anything before the "@" sign was ignored, so Multics users had to use a command-line option to specify the destination system.<ref name="thvv"/> The [[United States Department of Defense|Department of Defense]] [[DARPA]] desired to have uniformity and interoperability for email and therefore funded efforts to drive towards unified inter-operable standards. This led to David Crocker, John Vittal, Kenneth Pogran, and [[Austin Henderson]] publishing RFC 733, "Standard for the Format of ARPA Network Text Message" (November 21, 1977), a subset of which provided a stable base for common use on the ARPANET, but which was not fully effective, and in 1979, a meeting was held at BBN to resolve incompatibility issues. [[Jon Postel]] recounted the meeting in RFC 808, "Summary of Computer Mail Services Meeting Held at BBN on 10 January 1979" (March 1, 1982), which includes an appendix listing the varying email systems at the time. This, in turn, led to the release of David Crocker's RFC 822, "Standard for the Format of ARPA Internet Text Messages" (August 13, 1982).<ref>{{cite web|url=http://www.livinginternet.com/e/ei.htm |title=Email History, How Email was Invented, Living Internet |publisher=Livinginternet.com |date=1996-05-13 |accessdate=2014-01-09}}</ref> RFC 822 is a small adaptation of RFC 733's details, notably enhancing the [[host (network)|host]] portion, to use [[Domain Name]]s, that were being developed at the same time.

The [[National Science Foundation]] took over operations of the ARPANET and Internet from the Department of Defense, and initiated [[NSFNet]], a new [[backbone network|backbone]] for the network. A part of the NSFNet AUP forbade commercial traffic.<ref>{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/notes/internet_history80s.htm |title=Internet History |publisher=Cybertelecom |accessdate=2014-01-09}}</ref> In 1988, [[Vint Cerf]] arranged for an interconnection of [[MCI Mail]] with NSFNET on an experimental basis. The following year [[Compuserve]] email interconnected with NSFNET. Within a few years the commercial traffic restriction was removed from NSFNETs AUP, and NSFNET was privatised. In the late 1990s, the [[Federal Trade Commission]] grew concerned with fraud transpiring in email, and initiated a series of procedures on spam, fraud, and phishing.<ref>[http://www.cybertelecom.org/spam/Spamref.htm Cybertelecom : SPAM Reference] {{webarchive |url=https://web.archive.org/web/20140919090804/http://www.cybertelecom.org/spam/Spamref.htm |date=September 19, 2014 }}</ref> In 2004, FTC jurisdiction over spam was codified into law in the form of the [[Can Spam Act|CAN SPAM Act.]]<ref>{{cite web|author=Robert Cannon |url=http://www.cybertelecom.org/spam/canspam.htm |title=Can Spam Act |publisher=Cybertelecom |accessdate=2014-01-09}}</ref> Several other U.S. federal agencies have also exercised jurisdiction including the [[United States Department of Justice|Department of Justice]] and the [[United States Secret Service|Secret Service]]. NASA has provided email capabilities to astronauts aboard the Space Shuttle and International Space Station since 1991 when a [[Macintosh Portable]] was used aboard [[Space Shuttle]] mission [[STS-43]] to send the first email via [[AppleLink]].<ref>{{cite web|last=Cowing |first=Keith |url=http://www.spaceref.com/news/viewnews.html?id=213 |title=2001: A Space Laptop &#124; SpaceRef – Your Space Reference |publisher=Spaceref.com |date=2000-09-18 |accessdate=2014-01-09}}</ref><ref>{{cite web|url=http://www.macobserver.com/columns/thisweek/2004/20040831.shtml |title=The Mac Observer – This Week in Apple History – August 22–31: "Welcome, IBM. Seriously," Too Late to License |publisher=Macobserver.com |date=2004-10-31 |accessdate=2014-01-09}}</ref><ref>{{cite book|last=Linzmayer|first=Owen W.|title=Apple confidential 2.0 : the definitive history of the world's most colorful company|year=2004|publisher=No Starch Press|location=San Francisco, Calif.|isbn=1-59327-010-0|edition=[Rev. 2. ed.].}}</ref> Today astronauts aboard the International Space Station have email capabilities via the [[WI-FI|wireless networking]] throughout the station and are connected to the ground at 10 [[Mbit/s]] Earth to station and 3 Mbit/s station to Earth, comparable to home [[DSL]] connection speeds.<ref name=issit>{{cite news|title=First Tweet from Space|url=http://bits.blogs.nytimes.com/2010/01/22/first-tweet-from-space/|newspaper=The New York Times|first=Nick|last=Bilton|date=January 22, 2010}}</ref>

==See also==
{{colbegin||22em}}
* [[Anonymous remailer]]
* [[Anti-spam techniques]]
* [[biff]]
* [[Bounce message]]
* [[Comparison of email clients]]
* [[Dark Mail Alliance]]
* [[Disposable email address]]
* [[E-card]]
* [[Electronic mailing list]]
* [[Email art]]
* [[Email authentication]]
* [[Email digest]]
* [[Email encryption]]
* [[Email hosting service]]
* [[Email storm]]
* [[Email tracking]]
* [[HTML email]]
* [[Information overload]]
* [[Internet fax]]
* [[Internet mail standard]]s
* [[List of email subject abbreviations]]
* [[MCI Mail]]
* [[Netiquette]]
* [[Posting style]]
* [[Privacy-enhanced Electronic Mail]]
* [[Push email]]
* [[RSS]]
* [[Telegraphy]]
* [[Unicode and email]]
* [[Usenet quoting]]
* [[Webmail]], [[Comparison of webmail providers]]
* [[X-Originating-IP]]
* [[X.400]]
* [[Yerkish]]
{{colend}}

==References==
{{Reflist|2}}

==Further reading==
{{refbegin}}
* Cemil Betanov, ''Introduction to X.400'', Artech House, ISBN 0-89006-597-7.
* Marsha Egan, "[http://www.inboxdetox.com Inbox Detox and The Habit of Email Excellence]", Acanthus Publishing ISBN 978-0-9815589-8-1
* Lawrence Hughes, ''Internet e-mail Protocols, Standards and Implementation'', Artech House Publishers, ISBN 0-89006-939-5.
* Kevin Johnson, ''Internet Email Protocols: A Developer's Guide'', Addison-Wesley Professional, ISBN 0-201-43288-9.
* Pete Loshin, ''Essential Email Standards: RFCs and Protocols Made Practical'', John Wiley & Sons, ISBN 0-471-34597-0.
* {{Cite journal|last=Partridge|first=Craig|title=The Technical Development of Internet Email|journal=IEEE Annals of the History of Computing|volume=30|issue=2|publisher=IEEE Computer Society|location=Berlin|date=April–June 2008|url=http://www.ir.bbn.com/~craig/papers/email.pdf|format=PDF|issn=1934-1547|ref=harv|postscript={{inconsistent citations}} | doi =  10.1109/mahc.2008.32|pages=3–29}}
* Sara Radicati, ''Electronic Mail: An Introduction to the X.400 Message Handling Standards'', Mcgraw-Hill, ISBN 0-07-051104-7.
* John Rhoton, ''Programmer's Guide to Internet Mail: SMTP, POP, IMAP, and LDAP'', Elsevier, ISBN 1-55558-212-5.
* John Rhoton, ''X.400 and SMTP: Battle of the E-mail Protocols'', Elsevier, ISBN 1-55558-165-X.
* David Wood, ''Programming Internet Mail'', O'Reilly, ISBN 1-56592-479-7.
{{refend}}

==External links==
{{Wiktionary|email|outbox}}
* [http://www.iana.org/assignments/message-headers/perm-headers.html IANA's list of standard header fields]
* [http://emailhistory.org/ The History of Email] is Dave Crocker's attempt at capturing the sequence of 'significant' occurrences in the evolution of email; a collaborative effort that also cites this page.
* [http://www.multicians.org/thvv/mail-history.html The History of Electronic Mail] is a personal memoir by the implementer of an early email system
* [http://www.circleid.com/posts/20140903_a_look_at_the_origins_of_network_email/ A Look at the Origins of Network Email] is a short, yet vivid recap of the key historical facts
* [https://www.fbi.gov/news/stories/2015/august/business-e-mail-compromise Business E-Mail Compromise - An Emerging Global Threat], [[FBI]]
<!-- please see http://en.wikipedia.org/wiki/WP:EL before adding links -->
{{Computer-mediated communication}}
{{E-mail clients}}

{{Authority control}}

{{DEFAULTSORT:Email}}
[[Category:Email| ]]
[[Category:Internet terminology]]
[[Category:Electronic documents]]
[[Category:History of the Internet]]
[[Category:1971 introductions]]
<=====doc_Id=====>:425
<=====title=====>:
Portable Document Format
<=====text=====>:
{{Redirect|PDF|3=}}
{{Infobox file format
| name                   = Portable Document Format
| icon                   = [[File:Adobe PDF.svg|frameless|SVG logo|150px]]
| iconcaption            = Adobe PDF icon
| extension              = .pdf
| mime                   = {{plainlist|
* <code>application/pdf</code>,<ref name="rfc3778">{{citation |url=http://tools.ietf.org/html/rfc3778 |title=The application/pdf Media Type, RFC 3778, Category: Informational |year=2004}}</ref>
* <code>application/x-pdf</code>
* <code>application/x-bzpdf</code>
* <code>application/x-gzpdf</code>
 }}
| _nomimecode            = true
| magic                  = <code>%PDF</code>
| released               = {{Start date and age|1993|6|15}}
| standard               = ISO 32000-1
| free                   = Yes
| url                    = {{URL|https://www.adobe.com/devnet/pdf/pdf_reference_archive.html}}
| image                  = 
| typecode               = 'PDF '<ref name="rfc3778" /> (including a single space)
| uniform type           = com.adobe.pdf
| owner                  = [[Adobe Systems]]
| latest release version = 1.7
| latest release date    = <!-- {{Start date and age|YYYY|mm|dd|df=yes}} -->
| genre                  =
| container for          =
| contained by           =
| extended from          =
| extended to            = [[PDF/A]], [[PDF/E]], [[PDF/UA]], [[PDF/VT]], [[PDF/X]]
}}
The '''Portable Document Format''' ('''PDF''') is a [[file format]] used to present [[document]]s in a manner independent of [[application software]], [[Computer hardware|hardware]], and [[operating system]]s.<ref name="pdf-ref-1.7">Adobe Systems Incorporated, [https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf PDF Reference, Sixth edition, version 1.23 (30 MB)], Nov 2006, p. 33.</ref> Each PDF file encapsulates a complete description of a fixed-layout flat document, including the text, [[font]]s, graphics, and other information needed to display it. <!-- Today, three dimensional objects can be embedded in PDF documents with Acrobat 3D using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.<ref name="3d#1" /><ref name="3d#2" /> -->

<blockquote>A PDF file captures document text, fonts, images, and even formatting of documents from a variety of applications. You can e-mail a PDF document to your friends and it will look the same on their screens as it looks on yours, even if they have Apple computers and you have a PC.<ref>[http://techterms.com/definition/pdf TechTerms.com]</ref>
</blockquote>

== History and standardization ==

{{main article|History and standardization of Portable Document Format}}

PDF was developed in the early 1990s<ref>{{cite web|url=http://www.planetpdf.com/enterprise/article.asp?ContentID=6650|title=Adobe's Bob Wulff knows Acrobat and PDF -- inside and out}}</ref> as a way to share computer documents, including text formatting and inline images.<ref>{{cite web|url=http://www.planetpdf.com/planetpdf/pdfs/warnock_camelot.pdf|title=The Camelot Project}}</ref> It was among a number of competing formats such as [[DjVu]], [[Envoy (WordPerfect)|Envoy]], Common Ground Digital Paper, Farallon Replica and even [[Adobe Systems|Adobe]]'s own [[PostScript]] format. In those early years before the rise of the [[World Wide Web]] and [[HTML]] documents, PDF was popular mainly in [[desktop publishing]] [[workflow]]s.
Adobe Systems made the PDF specification available free of charge in 1993. PDF was a [[proprietary format]] controlled by Adobe, until it was officially released as an [[open standard]] on July 1, 2008, and published by the [[International Organization for Standardization]] as ISO 32000-1:2008,<ref name="iso-standard">{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=51502 |title=ISO 32000-1:2008 - Document management – Portable document format – Part 1: PDF 1.7 |publisher=Iso.org |date=2008-07-01 |accessdate=2010-02-21}}</ref><ref>{{cite web|last=Orion |first=Egan |title=PDF 1.7 is approved as ISO 32000 |work=[[The Inquirer]] |publisher=[[The Inquirer]] |date=2007-12-05 |url=http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |accessdate=2007-12-05 |deadurl=yes |archiveurl=https://web.archive.org/web/20071213004627/http://www.theinquirer.net/gb/inquirer/news/2007/12/05/pdf-approved-iso-32000 |archivedate=December 13, 2007 }}</ref> at which time control of the specification passed to an ISO Committee of volunteer industry experts. In 2008, Adobe published a Public Patent License to ISO 32000-1 granting [[royalty-free]] rights for all patents owned by Adobe that are necessary to make, use, sell, and distribute PDF compliant implementations.<ref>{{citation |url=https://www.adobe.com/pdf/pdfs/ISO32000-1PublicPatentLicense.pdf |title=Public Patent License, ISO 32000-1: 2008 – PDF 1.7 |author=Adobe Systems Incorporated |year=2008 |accessdate=2011-07-06}}</ref>

However, there are still some proprietary technologies defined only by Adobe, such as [[XFA|Adobe XML Forms Architecture]] (XFA) and [[JavaScript]] extension for Acrobat, which are referenced by ISO 32000-1 as [[normative]] and indispensable for the application of the ISO 32000-1 specification. These proprietary technologies are not standardized and their specification is published only on Adobe’s website.<ref>{{cite web |url=http://eur-lex.europa.eu/LexUriServ/LexUriServ.do?uri=SWD:2013:0224:FIN:EN:PDF |title=Guide for the procurement of standards-based ICT - Elements of Good Practice, Against lock-in: building open ICT systems by making better use of standards in public procurement |quote=Example: ISO/IEC 29500, ISO/IEC 26300 and ISO 32000 for document formats reference information that is not accessible by all parties (references to proprietary technology and brand names, incomplete scope or dead web links). |publisher=European Commission |date=2013-06-25 |accessdate=2013-10-20}}</ref><ref name="iso-meeting-n603">{{citation |url=http://pdf.editme.com/files/pdfREF-meetings/ISO-TC171-SC2-WG8_N0603_SC2WG8_MtgRept_SLC.pdf |title=ISO/TC 171/SC 2/WG 8 N 603 - Meeting Report |quote=XFA is not to be ISO standard just yet. ... The Committee urges Adobe Systems to submit the XFA Specification, XML Forms Architecture (XFA), to ISO for standardization ... The Committee is concerned about the stability of the XFA specification ... Part 2 will reference XFA 3.1 |date=2011-06-27}}</ref><ref>{{cite web |url=http://www.plosone.org/article/fetchSingleRepresentation.action?uri=info:doi/10.1371/journal.pone.0069446.s001 |title=Embedding and publishing interactive, 3-dimensional, scientificfigures in Portable Document Format (PDF) files |quote=... the implementation of the U3D standard was not complete and proprietary extensions were used. |accessdate=2013-10-20}}</ref><ref name="rosenthol-adobe-2012">{{cite web |url=http://cdn.parleys.com/p/5148922a0364bc17fc56c6e5/iSUM2012_00_LRO_presentation.pdf |title=PDF and Standards |author=Leonard Rosenthol, Adobe Systems |year=2012 |accessdate=2013-10-20}}</ref><ref>{{citation |url=http://www.planetpdf.com/enterprise/article.asp?ContentID=Is_PDF_an_open_standard&page=1 |title=Is PDF an open standard? - Adobe Reader is the de facto Standard, not PDF |author=Duff Johnson |date=2010-06-10 |accessdate=2014-01-19}}</ref> Many of them are also not supported by popular third-party implementations of PDF.  So when organizations publish PDFs which use these proprietary technologies, they present accessibility issues for some users.

In 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").<ref name="DRAFT INTERNATIONAL">{{Cite web|url=http://www.iso.org/iso/catalogue_detail.htm?csnumber=63534|title=DRAFT INTERNATIONAL STANDARD ISO/DIS 32000-2|last=|first=|date=|website=|publisher=ISO|access-date=2016-08-04|quote=Editor’s note: XFA forms have been deprecated from ISO 32000-2 in accordance with the outcome of the letter ballot following the Pretoria meetings.}}</ref>

== Technical foundations ==
The PDF combines three technologies:
* A subset of the [[PostScript]] page description programming language, for generating the layout and graphics.
* A [[font embedding|font-embedding]]/replacement system to allow fonts to travel with the documents.
* A structured storage system to bundle these elements and any associated content into a single file, with [[data compression]] where appropriate.

=== PostScript ===
[[PostScript]] is a [[page description language]] run in an [[interpreter (computing)|interpreter]] to generate an image, a process requiring many resources. It can handle graphics and standard features of [[programming language]]s such as <code>if</code> and <code>loop</code> commands. PDF is largely based on PostScript but simplified to remove flow control features like these, while graphics commands such as <code>lineto</code> remain.

Often, the PostScript-like PDF code is generated from a source PostScript file. The graphics commands that are output by the PostScript code are collected and [[Lexical analysis|tokenized]]. Any files, graphics, or fonts to which the document refers also are collected. Then, everything is compressed to a single file. Therefore, the entire PostScript world (fonts, layout, measurements) remains intact.

As a document format, PDF has several advantages over PostScript:
* PDF contains tokenized and interpreted results of the PostScript source code, for direct correspondence between changes to items in the PDF page description and changes to the resulting page appearance.
* PDF (from version 1.4) supports [[transparency (graphic)|graphic transparency]]; PostScript does not.
* PostScript is an [[interpreted programming language]] with an implicit global state, so instructions accompanying the description of one page can affect the appearance of any following page. Therefore, all preceding pages in a PostScript document must be processed to determine the correct appearance of a given page, whereas each page in a PDF document is unaffected by the others. As a result, PDF viewers allow the user to quickly jump to the final pages of a long document, whereas a PostScript viewer needs to process all pages sequentially before being able to display the destination page (unless the optional PostScript [[Document Structuring Conventions]] have been carefully complied with).

== Technical overview ==

=== File structure ===

A PDF file is a 7-bit ASCII file, except for certain elements that may have binary content.
A PDF file starts with a header containing the [[magic number (programming)|magic number]] and the version of the format such as <code>%PDF-1.7</code>. The format is a subset of a COS ("Carousel" Object Structure) format.<ref>{{cite web|url=http://jimpravetz.com/blog/2012/12/in-defense-of-cos/|title=In Defense of COS, or Why I Love JSON and Hate XML|author=Jim Pravetz|work=jimpravetz.com}}</ref> A COS tree file consists primarily of ''objects'', of which there are eight types:<ref>Adobe Systems, PDF Reference, p. 51.</ref>
* [[Boolean data type|Boolean]] values, representing ''true'' or ''false''
* Numbers
* [[String (computer science)|Strings]], enclosed within parentheses (<code>(...)</code>), may contain 8-bit characters.
* Names, starting with a forward slash (<code>/</code>)
* [[Array data type|Array]]s, ordered collections of objects enclosed within square brackets (<code>[...]</code>)
* [[Dictionary (data structure)|Dictionaries]], collections of objects indexed by Names enclosed within double pointy brackets (<code>&lt;&lt;...&gt;&gt;</code>)
* [[Stream (computing)|Streams]], usually containing large amounts of data, which can be compressed and binary
* The [[Pointer (computer programming)|null]] object
Furthermore, there may be comments, introduced with the percent sign (<code>%</code>). Comments may contain 8-bit characters.

Objects may be either ''direct'' (embedded in another object) or ''indirect''. Indirect objects are numbered with an ''object number'' and a ''generation number'' and defined between the <code>obj</code> and <code>endobj</code> keywords. An index table, also called the cross-reference table and marked with the <code>xref</code> keyword, follows the main body and gives the byte offset of each indirect object from the start of the file.<ref>Adobe Systems, PDF Reference, pp. 39–40.</ref> This design allows for efficient [[random access]] to the objects in the file, and also allows for small changes to be made without rewriting the entire file (''incremental update''). Beginning with PDF version 1.5, indirect objects may also be located in special streams known as ''object streams''. This technique reduces the size of files that have large numbers of small indirect objects and is especially useful for ''Tagged PDF''.

At the end of a PDF file is a trailer introduced with the <code>trailer</code> keyword. It contains

* a dictionary
* an offset to the start of the cross-reference table (the table starting with the <code>xref</code> keyword)
* and the <code>%%EOF</code> [[end-of-file]] marker.

The dictionary contains

* a reference to the root object of the tree structure, also known as the ''catalog''
* the count of indirect objects in the cross-reference table
* and other optional information.

There are two layouts to the PDF files: non-linear (not "optimized") and linear ("optimized"). Non-linear PDF files consume less disk space than their linear counterparts, though they are slower to access because portions of the data required to assemble pages of the document are scattered throughout the PDF file. Linear PDF files (also called "optimized" or "web optimized" PDF files) are constructed in a manner that enables them to be read in a Web browser plugin without waiting for the entire file to download, since they are written to disk in a linear (as in page order) fashion.<ref name="pdf-ref">{{cite web |url=https://www.adobe.com/devnet/pdf/pdf_reference.html |title=Adobe Developer Connection: PDF Reference and Adobe Extensions to the PDF Specification |publisher=Adobe Systems |accessdate=2010-12-13}}</ref> PDF files may be optimized using [[Adobe Acrobat]] software or [[QPDF]].

=== Imaging model ===
The basic design of how [[graphics]] are represented in PDF is very similar to that of PostScript, except for the use of [[transparency (graphic)|transparency]], which was added in PDF 1.4.

PDF graphics use a [[device independence|device-independent]] [[Cartesian coordinate system]] to describe the surface of a page. A PDF page description can use a [[matrix (mathematics)|matrix]] to [[scale (ratio)|scale]], [[rotate]], or [[Shear mapping|skew]] graphical elements. A key concept in PDF is that of the ''graphics state'', which is a collection of graphical parameters that may be changed, saved, and restored by a ''page description''. PDF has (as of version 1.6) 24 graphics state properties, of which some of the most important are:
* The ''current transformation matrix'' (CTM), which determines the coordinate system
* The ''[[clipping path]]''
* The ''[[color space]]''
* The ''[[alpha compositing|alpha constant]]'', which is a key component of transparency

==== Vector graphics ====
As in PostScript, [[vector graphics]] in PDF are constructed with ''paths''. Paths are usually composed of lines and cubic [[Bézier curve]]s, but can also be constructed from the outlines of text. Unlike PostScript, PDF does not allow a single path to mix text outlines with lines and curves. Paths can be stroked, filled, or used for [[clipping path|clipping]]. Strokes and fills can use any color set in the graphics state, including ''patterns''.

PDF supports several types of patterns. The simplest is the ''tiling pattern'' in which a piece of artwork is specified to be drawn repeatedly. This may be a ''colored tiling pattern'', with the colors specified in the pattern object, or an ''uncolored tiling pattern'', which defers color specification to the time the pattern is drawn. Beginning with PDF 1.3 there is also a ''shading pattern'', which draws continuously varying colors. There are seven types of shading pattern of which the simplest are the ''axial shade'' (Type 2) and ''radial shade'' (Type 3). <!-- Pictures desperately needed here! -->

==== Raster images ====
[[Raster graphics|Raster images]] in PDF (called ''Image XObjects'') are represented by dictionaries with an associated stream. The dictionary describes properties of the image, and the stream contains the image data. (Less commonly, a raster image may be embedded directly in a page description as an ''inline image''.) Images are typically ''filtered'' for compression purposes. Image filters supported in PDF include the general purpose filters
* '''ASCII85Decode''' a filter used to put the stream into 7-bit [[ASCII]]
* '''ASCIIHexDecode''' similar to ASCII85Decode but less compact
* '''FlateDecode''' a commonly used filter based on the [[deflate]] algorithm defined in RFC 1951 (deflate is also used in the [[gzip]], [[Portable Network Graphics|PNG]], and [[ZIP (file format)|zip]] file formats among others); introduced in PDF 1.2; it can use one of two groups of predictor functions for more compact zlib/deflate compression: ''Predictor 2'' from the [[TIFF]] 6.0 specification and predictors (filters) from the [[Portable Network Graphics|PNG]] specification (RFC 2083)
* '''LZWDecode''' a filter based on [[LZW]] Compression; it can use one of two groups of predictor functions for more compact LZW compression: ''Predictor 2'' from the TIFF 6.0 specification and predictors (filters) from the PNG specification
* '''RunLengthDecode''' a simple compression method for streams with repetitive data using the [[run-length encoding]] algorithm and the image-specific filters
* '''DCTDecode''' a [[lossy]] filter based on the [[JPEG]] standard
* '''CCITTFaxDecode''' a [[lossless]] [[bi-level image|bi-level]] (black/white) filter based on the Group 3 or [[Group 4 compression|Group 4]] [[CCITT]] (ITU-T) [[fax]] compression standard defined in ITU-T [[T.4]] and T.6
* '''JBIG2Decode''' a lossy or lossless bi-level (black/white) filter based on the [[JBIG2]] standard, introduced in PDF 1.4
* '''JPXDecode''' a lossy or lossless filter based on the [[JPEG 2000]] standard, introduced in PDF 1.5

Normally all image content in a PDF is embedded in the file. But PDF allows image data to be stored in external files by the use of ''external streams'' or ''Alternate Images''. Standardized subsets of PDF, including [[PDF/A]] and [[PDF/X]], prohibit these features.

==== Text ====
Text in PDF is represented by ''text elements'' in page content streams. A text element specifies that ''characters'' should be drawn at certain positions. The characters are specified using the ''encoding'' of a selected ''font resource''.

===== Fonts =====
A font object in PDF is a description of a digital [[typeface]]. It may either describe the characteristics of a typeface, or it may include an embedded ''font file''. The latter case is called an ''embedded font'' while the former is called an ''unembedded font''. The font files that may be embedded are based on widely used standard digital font formats: '''[[PostScript fonts|Type 1]]''' (and its compressed variant '''CFF'''), '''[[TrueType]]''', and (beginning with PDF 1.6) '''[[OpenType]]'''. Additionally PDF supports the '''Type 3''' variant in which the components of the font are described by PDF graphic operators. <!--- Type 3 bit is awkward and should be cleaned up --->

===== Standard Type 1 Fonts (Standard 14 Fonts) =====
Fourteen typefaces, known as the ''standard 14 fonts'', have a special significance in PDF documents:
* [[Times Roman|Times]] (v3) (in regular, italic, bold, and bold italic)
* [[Courier (typeface)|Courier]] (in regular, oblique, bold and bold oblique)
* [[Helvetica]] (v3) (in regular, oblique, bold and bold oblique)
* [[Symbol (typeface)|Symbol]]
* [[Zapf Dingbats]]
These fonts are sometimes called the ''base fourteen fonts''.<ref>{{cite web|url=http://desktoppub.about.com/od/glossary/g/base14fonts.htm|title=Desktop Publishing: Base 14 Fonts - Definition|work=About.com Tech}}</ref> These fonts, or suitable substitute fonts with the same metrics, should be available in most PDF readers. However, since Adobe Acrobat version 6, most of these fonts are not ''guaranteed'' to be available in the reader, and may only display correctly if the system has them installed.<ref name="aquarium">[http://www.planetpdf.com/planetpdf/pdfs/pdf2k/03e/merz_fontaquarium.pdf The PDF Font Aquarium]</ref> Fonts may be substituted if they are not embedded in a PDF.

===== Encodings =====
Within text strings, characters are shown using ''character codes'' (integers) that map to glyphs in the current font using an ''encoding''. There are a number of predefined encodings, including ''WinAnsi'', ''MacRoman'', and a large number of encodings for East Asian languages, and a font can have its own built-in encoding. (Although the WinAnsi and MacRoman encodings are derived from the historical properties of the [[Microsoft Windows|Windows]] and [[Macintosh]] operating systems, fonts using these encodings work equally well on any platform.) PDF can specify a predefined encoding to use, the font's built-in encoding or provide a lookup table of differences to a predefined or built-in encoding (not recommended with TrueType fonts).<ref>{{cite web|url=https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf |title=PDF Reference Sixth Edition, version 1.7, table 5.11}}</ref> The encoding mechanisms in PDF were designed for Type 1 fonts, and the rules for applying them to TrueType fonts are complex.

For large fonts or fonts with non-standard glyphs, the special encodings ''Identity-H'' (for horizontal writing) and ''Identity-V'' (for vertical) are used. With such fonts it is necessary to provide a ''ToUnicode'' table if semantic information about the characters is to be preserved.

==== Transparency ====
The original imaging model of PDF was, like PostScript's, ''opaque'': each object drawn on the page completely replaced anything previously marked in the same location. In PDF 1.4 the imaging model was extended to allow transparency. When transparency is used, new objects interact with previously marked objects to produce blending effects. The addition of transparency to PDF was done by means of new extensions that were designed to be ignored in products written to the PDF 1.3 and earlier specifications. As a result, files that use a small amount of transparency might view acceptably in older viewers, but files making extensive use of transparency could be viewed incorrectly in an older viewer without warning.

The transparency extensions are based on the key concepts of ''transparency groups'', ''blending modes'', ''shape'', and ''alpha''. The model is closely aligned with the features of [[Adobe Illustrator]] version 9. The blend modes were based on those used by [[Adobe Photoshop]] at the time. When the PDF 1.4 specification was published, the formulas for calculating blend modes were kept secret by Adobe. They have since been published.<ref>[https://www.adobe.com/content/dam/Adobe/en/devnet/pdf/pdfs/pdf_reference_archives/blend_modes.pdf PDF Blend Modes Addendum]</ref>

The concept of a transparency group in PDF specification is independent of existing notions of "group" or "layer" in applications such as Adobe Illustrator. Those groupings reflect logical relationships among objects that are meaningful when editing those objects,
but they are not part of the imaging model.

=== Interactive elements ===

PDF files may contain interactive elements such as annotations, form fields, video and Flash animation.

'''Rich Media PDF''' is a term that is used to describe interactive content that can be embedded or linked to inside of a PDF. This content must be produced using the Flash file format. When Adobe bought Macromedia, the jewel of the company was Flash, and the Flash player was embedded inside Adobe Acrobat and Adobe Reader, removing the need for third-party plug-ins such as Flash, QuickTime, or Windows Media. Unfortunately, this caused a rift with Apple as QuickTime video was prohibited from PDF.  [[Rich Media]] expert [[Bob Connolly (Canadian film director)#Books, eBooks and Magazine Articles|Robert Connolly]] believes this event triggered the war between Apple and Adobe over the Flash iPhone/iPad dispute. Rich Media PDF will not operate in Apple's iOS devices such as the iPad, and interactivity is limited.

'''Interactive Forms''' is a mechanism to add forms to the PDF file format.

PDF currently supports two different methods for integrating data and PDF forms. Both formats today coexist in PDF specification:<ref name="iso32000">{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/PDF32000_2008.pdf |title=Document Management – Portable Document Format – Part 1: PDF 1.7, First Edition |author=Adobe Systems Incorporated |date=2008-07-01 |accessdate=2010-02-19}}</ref><ref>{{cite web |url=http://gnupdf.org/Forms_Data_Format |title=Gnu PDF - PDF Knowledge - Forms Data Format |archiveurl=https://web.archive.org/web/20130101054615/http://www.gnupdf.org/Forms_Data_Format |archivedate=2013-01-01 |accessdate=2010-02-19}}</ref><ref>{{cite web |url=http://livedocs.adobe.com/coldfusion/8/htmldocs/help.html?content=formsPDF_02.html |title=About PDF forms |accessdate=2010-02-19}}</ref><ref>{{cite web |url=http://forums.adobe.com/thread/301733 |title=Convert XFA Form to AcroForm? |year=2008 |accessdate=2010-02-19}}</ref>
* '''AcroForms''' (also known as '''Acrobat forms'''), introduced in the PDF 1.2 format specification and included in all later PDF specifications.
* '''[[XML Forms Architecture|Adobe XML Forms Architecture]] (XFA)''' forms, introduced in the PDF 1.5 format specification. The XFA specification is not included in the PDF specification, it is only referenced as an optional feature. Adobe XFA Forms are not compatible with AcroForms.<ref>{{cite web |url=http://partners.adobe.com/public/developer/tips/topic_tip2.html |title=Migrating from Adobe Acrobat forms to XML forms |accessdate=2010-02-22}}</ref>

==== AcroForms ====
AcroForms were introduced in the PDF 1.2 format. AcroForms permit using objects (''e.g.'' [[text box]]es, [[Radio button]]s, ''etc.'') and some code (''e.g.'' [[JavaScript]]).

Alongside the standard PDF action types, interactive forms (AcroForms) support submitting, resetting, and importing data. The "submit" action transmits the names and values of selected interactive form fields to a specified uniform resource locator (URL). Interactive form field names and values may be submitted in any of the following formats, (depending on the settings of the action’s ExportFormat, SubmitPDF, and XFDF flags):<ref name="iso32000" />
* HTML Form format (HTML 4.01 Specification since PDF 1.5; HTML 2.0 since 1.2)
* Forms Data Format (FDF)
* XML Forms Data Format (XFDF) (external XML Forms Data Format Specification, Version 2.0; supported since PDF 1.5; it replaced the "XML" form submission format defined in PDF 1.4)
* PDF (the entire document can be submitted rather than individual fields and values). (defined in PDF 1.4)

AcroForms can keep form field values in external stand-alone files containing key:value pairs. The external files may use Forms Data Format (FDF) and XML Forms Data Format (XFDF) files.<ref>{{cite web |url=http://kb2.adobe.com/cps/325/325874.html |title=Using Acrobat forms and form data on the web |author=Adobe Systems Incorporated |date=2007-10-15 |accessdate=2010-02-19}}</ref><ref name="xfdf">{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfdf_2.0.pdf |format=PDF |title=XML Forms Data Format Specification, version 2 |date=September 2007 |accessdate=2010-02-19}}</ref><ref name="fdf-exchange">{{citation |url=https://www.adobe.com/devnet/acrobat/pdfs/fdf_data_exchange.pdf |format=PDF |title=FDF Data Exchange Specification |date=2007-02-08 |accessdate=2010-02-19}}</ref> The usage rights (UR) signatures define rights for import form data files in FDF, XFDF and text ([[comma-separated values|CSV]]/[[delimiter-separated values|TSV]]) formats, and export form data files in FDF and XFDF formats.<ref name="iso32000" />

===== Forms Data Format (FDF) =====
{{Infobox file format
| name                   = Forms Data Format (FDF)
| icon                   =
| logo                   =
| screenshot             =
| caption                =
| extension              = .fdf
| mime                   = application/vnd.fdf<ref>{{citation |url=http://www.iana.org/assignments/media-types/application/ |title=IANA Application Media Types - vnd.fdf |accessdate=2010-02-22}}</ref>
| type code              = 'FDF'
| uniform type           =
| magic                  =
| owner                  = [[Adobe Systems]]
| released               = {{Start date|1996}}<!-- {{Start date|YYYY|mm|dd|df=yes}} --> (PDF 1.2)
| latest release version =
| latest release date    = <!-- {{Start date and age|YYYY|mm|dd|df=yes}} -->
| genre                  =
| container for          =
| contained by           =
| extended from          = PDF
| extended to            = XFDF
| standard               = ISO 32000-1:2008
| free                   = Yes
| url                    =
}}

The Forms Data Format (FDF) is based on PDF, it uses the same syntax and has essentially the same file structure, but is much simpler than PDF, since the body of an FDF document consists of only one required object. Forms Data Format is defined in the PDF specification (since PDF 1.2). The Forms Data Format can be used when submitting form data to a server, receiving the response, and incorporating into the interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. Beginning in PDF 1.3, FDF can be used to define a container for annotations that are separate from the PDF document they apply to. FDF typically encapsulates information such as [[X.509|X.509 certificates]], requests for certificates, directory settings, timestamp server settings, and embedded PDF files for network transmission.<ref name="fdf-exchange" /> The FDF uses the MIME content type application/vnd.fdf, filename extension .fdf and on Mac OS it uses file type 'FDF'.<ref name="iso32000" /> Support for importing and exporting FDF stand-alone files is not widely implemented in free or freeware PDF software. For example, there is no import/export support in Evince, Okular, Poppler, KPDF or Sumatra PDF, however, Evince, Okular and Poppler support filling in of PDF Acroforms and saving filled data inside the PDF file. Import support for stand-alone FDF files is implemented in Adobe Reader; export and import support (including saving of FDF data in PDF) is for example implemented in Foxit Reader and PDF-XChange Viewer Free; saving of FDF data in a PDF file is also supported in pdftk.

===== XML Forms Data Format (XFDF) =====
{{Infobox file format
| name                   = XML Forms Data Format (XFDF)
| icon                   =
| logo                   =
| screenshot             =
| caption                =
| extension              = .xfdf
| mime                   = application/vnd.adobe.xfdf<ref>{{citation |url=http://www.iana.org/assignments/media-types/application/vnd.adobe.xfdf |title=IANA Application Media Types - Vendor Tree - vnd.adobe.xfdf |accessdate=2010-02-22}}</ref>
| type code              = 'XFDF'
| uniform type           =
| magic                  =
| owner                  = [[Adobe Systems]]
| released               = {{Start date|2003|07|df=yes}} (referenced in PDF 1.5)
| latest release version = 3.0
| latest release date    = {{Start date and age|2009|08|df=yes}}
| genre                  =
| container for          =
| contained by           =
| extended from          = PDF, FDF, [[XML]]
| extended to            =
| standard               = No (under standardization as ISO/CD 19444-1<ref name="iso-xfdf">{{citation |url=http://www.iso.org/iso/home/store/catalogue_ics/catalogue_detail_ics.htm?ics1=35&ics2=240&ics3=30&csnumber=64911 |title=ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0 |accessdate=2014-11-26}}</ref>)
| free                   =
| url                    = [https://partners.adobe.com/public/developer/en/xml/XFDF_Spec_3.0.pdf XFDF 3.0 specification]
}}

XML Forms Data Format (XFDF) is the XML version of Forms Data Format, but the XFDF implements only a subset of FDF containing forms and annotations. There are not XFDF equivalents for some entries in the FDF dictionary - such as the Status, Encoding, JavaScript, Pages keys, EmbeddedFDFs, Differences and Target. In addition, XFDF does not allow the spawning, or addition, of new pages based on the given data; as can be done when using an FDF file. The XFDF specification is referenced (but not included) in PDF 1.5 specification (and in later versions). It is described separately in ''XML Forms Data Format Specification''.<ref name="xfdf" /> The PDF 1.4 specification allowed form submissions in XML format, but this was replaced by submissions in XFDF format in the PDF 1.5 specification. XFDF conforms to the XML standard. As of November 2014, XFDF 3.0 is in the ISO/IEC standardization process under the formal name ''ISO/CD 19444-1 - Document management - XML forms data format - Part 1: XFDF 3.0''.<ref name="iso-xfdf"/>

XFDF can be used the same way as FDF; e.g., form data is submitted to a server, modifications are made, then sent back and the new form data is imported in an interactive form. It can also be used to export form data to stand-alone files that can be imported back into the corresponding PDF interactive form. A support for importing and exporting XFDF stand-alone files is not widely implemented in free or freeware PDF software. Import of XFDF is implemented in Adobe Reader 5 and later versions; import and export is implemented in PDF-XChange Viewer Free; embedding of XFDF data in PDF form is implemented in pdftk (pdf toolkit).

==== Adobe XML Forms Architecture (XFA) ====
{{Main article|XFA|l1=XML Forms Architecture}}
In the PDF 1.5 format, [[Adobe Systems]] introduced a new, proprietary format for forms, namely Adobe XML Forms Architecture (XFA) forms. The XFA 2.02 is referenced in the PDF 1.5 specification (and also in later versions) but is described separately in ''Adobe XML Forms Architecture (XFA) Specification'', which has several versions.<ref name="xfa-adobe">{{cite web |url=http://partners.adobe.com/public/developer/xml/index_arch.html |title=Adobe XML Forms Architecture (XFA) |author=Adobe Systems Incorporated |accessdate=2010-02-19}}</ref> XFA specification is not included in ISO 32000-1 PDF 1.7 and is only referenced as an external proprietary specification created by Adobe. XFA was not standardized as an ISO standard. In 2011 the ISO Committee (TC 171/SC 2/WG 8) urged Adobe Systems to submit the XFA Specification for standardization.<ref name="iso-meeting-n603" />

Adobe XFA Forms are not compatible with AcroForms. Adobe Reader contains "disabled features" for use of XFA Forms, that activate only when opening a PDF document that was created using enabling technology available only from Adobe.<ref>{{citation |url=https://www.adobe.com/products/eulas/pdfs/Reader_Player_AIR_WWEULA-Combined-20080204_1313.pdf |format=PDF |title=Adobe Reader - Software license agreement |accessdate=2010-02-19}}</ref><ref>{{cite web|url=https://www.adobe.com/go/readerextensions |title=LiveCycle Reader Extensions ES features and benefits |accessdate=2010-02-19 |deadurl=yes |archiveurl=https://web.archive.org/web/20091219163323/http://www.adobe.com/go/readerextensions |archivedate=December 19, 2009 }}</ref> The XFA Forms are not compatible with Adobe Reader prior to version 6.

XFA forms can be created and used as PDF files or as XDP ([[XML Data Package]]) files. The format of an XFA resource in PDF is described by the XML Data Package Specification.<ref name="iso32000" /> The XDP may be a standalone document or it may in turn be carried inside a PDF document. XDP provides a mechanism for packaging form components within a surrounding XML container. An XDP can also package a PDF file, along with XML form and template data.<ref name="xfa-adobe" /> PDF may contain XFA (in XDP format), but also XFA may contain PDF.<ref name="xfa-adobe" /> When the XFA (XML Forms Architecture) grammars used for an XFA form are moved from one application to another, they must be packaged as an XML Data Package.<ref name="xfa25">{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_2_5.pdf |format=PDF |title=XML Forms Architecture (XFA) Specification Version 2.5 |date=2007-06-08 |accessdate=2010-02-19}}</ref>

When the PDF and XFA are combined, the result is a form in which each page of the XFA form overlays a PDF background. This architecture is
sometimes referred to as XFAF (XFA Foreground). The alternative is to express all of the form, including boilerplate, directly in XFA (without using PDF, or only using "Shell PDF" which is a container for XFA with minimal skeleton of PDF markup, or using a pre-rendered depiction of a static XFA form as PDF pages). It is sometimes called ''full'' XFA.<ref name="xfa25" />

Starting with PDF 1.5, the text contents of variable text form fields, as well as markup annotations may include formatting information (style information). These rich text strings are XML documents that conform to the rich text conventions specified for the XML Forms Architecture specification 2.02, which is itself a subset of the XHTML 1.0 specification, augmented with a restricted set of CSS2 style attributes.<ref name="iso32000" />
In PDF 1.6, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.2.
In PDF 1.7, PDF supports the rich text elements and attributes specified in the XML Forms Architecture (XFA) Specification, 2.4.<ref name="iso32000" />

Most PDF processors do not handle XFA content. When generating a shell PDF it is recommended to include in the PDF markup a simple one-page PDF image displaying a warning message (e.g. "To view the full contents of this document, you need a later version of the PDF viewer.", etc.). PDF processors that can render XFA content should either not display the supplied warning page image or replace it quickly with the dynamic form content.<ref name="xfa33">{{citation |url=http://partners.adobe.com/public/developer/en/xml/xfa_spec_3_3.pdf |title=XML Forms Architecture (XFA) Specification Version 3.3 |date=2012-01-09 |accessdate=2014-04-09}}</ref> Examples of PDF software with some support of XFA rendering include Adobe Reader for Windows, Linux, macOS (but not Adobe Reader Mobile for Android or iOS) or Nuance PDF Reader.

In 2014, ISO TC 171 voted to deprecate XFA for ISO 32000-2 ("Next-generation PDF").<ref name="DRAFT INTERNATIONAL" />

=== Logical structure and accessibility ===

A "tagged" PDF (ISO 32000-1:2008 14.8) includes document structure and semantics information to enable reliable text extraction and accessibility. Technically speaking, tagged PDF is a stylized use of the format that builds on the logical structure framework introduced in PDF 1.3. Tagged PDF defines a set of standard structure types and attributes that allow page content (text, graphics, and images) to be extracted and reused for other purposes.<ref>[http://www.planetpdf.com/enterprise/article.asp?ContentID=6067 What is Tagged PDF?]</ref>

Tagged PDF is not required in situations where a PDF file is intended only for print. Since the feature is optional, and since the rules for Tagged PDF as specified in ISO 32000-1 are relatively vague, support for tagged PDF amongst consuming devices, including assistive technology (AT), is uneven.<ref>{{cite web|url=http://www.washington.edu/doit/Stem/articles?1002|title=Is PDF accessible?|work=washington.edu}}</ref>

An [[AIIM]] project to develop an ISO-standardized subset of PDF specifically targeted at accessibility began in 2004, eventually becoming [[PDF/UA]].

=== Security and signatures ===

A PDF file may be encrypted for security, or digitally signed for authentication.

The standard security provided by Acrobat PDF consists of two different methods and two different passwords, ''user password'', which encrypts the file and prevents opening, and ''owner password'', which specifies operations that should be restricted even when the document is decrypted, which can include: printing, copying text and graphics out of the document, modifying the document, or adding or modifying text notes and [[Acroforms|AcroForm]] fields. The user password (controls opening) encrypts the file and requires [[password cracking]] to defeat, with difficulty depending on password strength and encryption method – it is potentially very secure (assuming good password and encryption method without known attacks). The owner password (controls operations) does not encrypt the file, and instead relies on client software to respect these restrictions, and is not secure. An "owner password" can be removed by many commonly available "PDF cracking" software, including some free online services.<ref>{{cite web|url=http://freemypdf.com/|title=FreeMyPDF.com - Removes passwords from viewable PDFs|work=freemypdf.com}}</ref> Thus, the use restrictions that a document author places on a PDF document are not secure, and cannot be assured once the file is distributed; this warning is displayed when applying such restrictions using Adobe Acrobat software to create or edit PDF files.

Even without removing the password, most freeware or open source PDF readers ignore the permission "protections" and allow the user to print or make copy of excerpts of the text as if the document were not limited by password protection.<ref>{{cite web |url= http://www.macworld.com/article/1137343/pdf.html |title=Adobe admits new PDF password protection is weaker |author= Jeremy Kirk}}</ref><ref>{{cite web |url= http://www.cs.cmu.edu/~dst/Adobe/Gallery/PDFsecurity.pdf  |title= How secure is PDF |author=Bryan Guignard}}</ref><ref>{{cite web |url= http://www.planetpdf.com/planetpdf/pdfs/pdf2k/01W/merz_securitykeynote.pdf |title= PDF Security Overview: Strengths and Weaknesses }}</ref>

There are a number of commercial solutions including [[Adobe LiveCycle]] [[Adobe LiveCycle#LiveCycle Rights Management ES4|Rights Management]] and Locklizard PDF DRM<ref>{{cite web |url=http://www.infosecurity-magazine.com/news/locklizard-develops-zero-footprint-solution-for/ |title=LockLizard Develops Zero Footprint Solution for PDF Security }}</ref> that are more robust means of [[information rights management]]. Not only can they restrict document access but they also reliably enforce [[File system permissions|permissions]] in ways that the standard security handler does not.<ref>{{cite web |url=http://www.locklizard.com/pdf_security_drm/ |title=PDF DRM Security Software for Adobe Document Protection}}</ref>

==== Usage rights ====
Beginning with PDF 1.5, Usage rights (UR) signatures are used to enable additional interactive features that are not available by default in a particular PDF viewer application. The signature is used to validate that the permissions have been granted by a bona fide granting authority. For example, it can be used to allow a user:<ref name="iso32000" />
* to save the PDF document along with modified form and/or annotation data
* import form data files in FDF, XFDF and text (CSV/TSV) formats
* export form data files in FDF and XFDF formats
* submit form data
* instantiate new pages from named page templates
* apply a [[Digital data|digital]] [[signature]] to existing [[digital signature]] form field
* create, delete, modify, copy, import, export annotations

For example, Adobe Systems grants permissions to enable additional features in Adobe Reader, using public-key [[cryptography]]. Adobe Reader verifies that the signature uses a [[Public key certificate|certificate]] from an Adobe-[[authorize]]d certificate authority. The PDF 1.5 specification declares that other PDF viewer applications are free to use this same mechanism for their own purposes.<ref name="iso32000" />

=== File attachments ===

PDF files can have document-level and page-level file attachments, which the reader can access and open or save to their local filesystem. PDF attachments can be added to existing PDF files for example using [[pdftk]]. Adobe Reader provides support for attachments, and [[poppler (software)|poppler]]-based readers like [[Evince]] or [[Okular]] also have some support for document-level attachments.

=== Metadata ===
PDF files can contain two types of metadata.<ref>[https://www.adobe.com/devnet/acrobat/pdfs/pdf_reference_1-7.pdf Adobe PDF reference version 1.7], section 10.2</ref> The first is the Document Information Dictionary, a set of key/value fields such as author, title, subject, creation and update dates. This is stored in the optional Info trailer of the file. A small set of fields is defined, and can be extended with additional text values if required.

In PDF 1.4, support was added for Metadata Streams, using the [[Extensible Metadata Platform]] (XMP) to add XML standards-based extensible metadata as used in other file formats. This allows metadata to be attached to any stream in the document, such as information about embedded illustrations, as well as the whole document (attaching to the document catalog), using an extensible schema.

== Intellectual property ==

Anyone may create applications that can read and write PDF files without having to pay royalties to [[Adobe Systems]]; Adobe holds patents to PDF, but licenses them for [[royalty-free]] use in developing software complying with its PDF specification.<ref>{{cite web|url=http://partners.adobe.com/public/developer/support/topic_legal_notices.html|title=Developer Resources|work=adobe.com}}</ref>

== Technical issues ==

=== Accessibility ===
PDF files can be created specifically to be accessible for disabled people.<ref>{{cite web |url=http://www.webaim.org/techniques/acrobat/ |title=PDF Accessibility |publisher=WebAIM |accessdate=2010-04-24}}</ref><ref>{{cite web |url=http://www.alistapart.com/articles/pdf_accessibility |title=Facts and Opinions About PDF Accessibility |author=Joe Clark |date=2005-08-22 |accessdate=2010-04-24}}</ref><ref>{{cite web |url=http://wac.osu.edu/pdf/ |title=Accessibility and PDF documents |publisher=Web Accessibility Center |accessdate=2010-04-24}}</ref><ref>{{cite web |url=http://www.bbc.co.uk/guidelines/futuremedia/accessibility/accessible_pdf.shtml |title=PDF Accessibility Standards v1.2 |accessdate=2010-04-24}}</ref><ref>{{citation |url=http://www.csus.edu/training/handouts/workshops/creating_accessible_pdfs.pdf |format=PDF |title=PDF Accessibility |publisher=California State University |accessdate=2010-04-24}}</ref> PDF file formats in use {{As of|2014|lc=on}} can include tags ([[XML]]), text equivalents, captions, audio descriptions, etc. Tagged PDF is required in the [[PDF/A]]-1a specification.<ref>{{citation |url=http://www.aiim.org/documents/standards/PDF-A/19005-1_FAQ.pdf |title=Frequently Asked Questions (FAQs) – ISO 19005-1:2005 – PDF/A-1, Date: July 10, 2006 |format=PDF |date=2006-07-10 |accessdate=2011-07-06}}</ref><ref name="pdfa1-tech">{{cite web |url=http://www.pdfa.org/doku.php?id=artikel:en:pdfa_a_look_at_the_technical-side |title=PDF/A – A Look at the Technical Side |accessdate=2011-07-06}}</ref> Some software can automatically produce tagged PDFs, but this feature is not always enabled by default.<ref>{{citation |url=http://help.libreoffice.org/Common/Export_as_PDF#PDF.2FA-1a |title=LibreOffice Help - Export as PDF |accessdate=2012-09-22}}</ref><ref>{{citation |url=http://www.oooninja.com/2008/01/generating-pdfa-for-long-term-archiving.html |title=Exporting PDF/A for long-term archiving |date=2008-01-11}}</ref> Leading [[screen reader]]s, including [[JAWS (screen reader)|JAWS]], [[Window-Eyes]], Hal, and [[Kurzweil Educational Systems|Kurzweil 1000 and 3000]] can read tagged PDFs aloud, as can later versions of the Acrobat and Acrobat Reader programs.<ref>{{cite web |url=http://help.adobe.com/en_US/Reader/8.0/help.html?content=WS58a04a822e3e50102bd615109794195ff-7d15.html |title=Adobe Reader 8 - Read a PDF with Read Out Loud |accessdate=2010-04-24}}</ref><ref>{{cite news |url=http://gadgetwise.blogs.nytimes.com/2009/04/10/tip-of-the-week-adobe-readers-read-aloud-feature/ |title=Tip of the Week: Adobe Reader’s ‘Read Aloud’ Feature |accessdate=2010-04-24 | work=The New York Times | date=2009-04-10 |first=J.D. |last=Biersdorfer}}</ref><ref>{{citation |url=https://www.adobe.com/accessibility/pdfs/accessing-pdf-sr.pdf |format=PDF |title=Accessing PDF documents with assistive technology: A screen reader user's guide |publisher=Adobe |accessdate=2010-04-24}}</ref> Moreover, tagged PDFs can be re-flowed and magnified for readers with visual impairments. Problems remain with adding tags to older PDFs and those that are generated from scanned documents. In these cases, accessibility tags and re-flowing are unavailable, and must be created either manually or with OCR techniques. These processes are inaccessible to some disabled people.

One of the significant challenges with PDF accessibility is that PDF documents have three distinct views, which, depending on the document's creation, can be inconsistent with each other. The three views are (i) the physical view, (ii) the tags view, and (iii) the content view. The physical view is displayed and printed (what most people consider a PDF document). The tags view is what screen readers and other assistive technologies use to deliver a high-quality navigation and reading experience to users with disabilities. The content view is based on the physical order of objects within the PDF's content stream and may be displayed by software that does not fully support the tags view, such as the Reflow feature in Adobe's Reader.

[[PDF/UA]], the International Standard for accessible PDF based on ISO 32000-1 was published as ISO 14289-1 in 2012, and establishes normative language for accessible PDF technology.

=== Viruses and exploits ===
{{see also|Adobe Acrobat#Security}}
PDF attachments carrying viruses were first discovered in 2001. The virus, named ''OUTLOOK.PDFWorm'' or ''Peachy'', uses [[Microsoft Outlook]] to send itself as an attachment to an Adobe PDF file. It was activated with Adobe Acrobat, but not with Acrobat Reader.<ref>Adobe Forums, [https://forums.adobe.com/thread/302989 Announcement: PDF Attachment Virus "Peachy"], 15 August 2001.</ref>

From time to time, new vulnerabilities are discovered in various versions of Adobe Reader,<ref>{{cite web|url=https://www.adobe.com/support/security/#readerwin |title=Security bulletins and advisories |publisher=Adobe |date= |accessdate=2010-02-21}}</ref> prompting the company to issue security fixes. Other PDF readers are also susceptible. One aggravating factor is that a PDF reader can be configured to start automatically if a web page has an embedded PDF file, providing a vector for attack. If a malicious web page contains an infected PDF file that takes advantage of a vulnerability in the PDF reader, the system may be compromised even if the browser is secure. Some of these vulnerabilities are a result of the PDF standard allowing PDF documents to be scripted with JavaScript. Disabling JavaScript execution in the PDF reader can help mitigate such future exploits, although it does not protect against exploits in other parts of the PDF viewing software. Security experts say that JavaScript is not essential for a PDF reader, and that the security benefit that comes from disabling JavaScript outweighs any compatibility issues caused.<ref>[http://www.grc.com/sn/sn-187.txt Steve Gibson - SecurityNow Podcast]</ref> One way of avoiding PDF file exploits is to have a local or web service convert files to another format before viewing.

On March 30, 2010 security researcher Didier Stevens reported an Adobe Reader and Foxit Reader exploit that runs a malicious executable if the user allows it to launch when asked.<ref>{{cite web|url=http://blogs.pcmag.com/securitywatch/2010/03/malicious_pdfs_execute_code_wi.php|title=Malicious PDFs Execute Code Without a Vulnerability|work=PCMAG}}</ref>

=== Usage restrictions and monitoring ===

PDFs may be [[encrypted]] so that a password is needed to view or edit the contents. The PDF Reference defines both 40-bit and 128-bit encryption, both making use of a complex system of [[RC4]] and [[MD5]]. The PDF Reference also defines ways that third parties can define their own encryption systems for PDF.

PDF files may also contain embedded [[digital rights management|DRM]] restrictions that provide further controls that limit copying, editing or printing. The restrictions on copying, editing, or printing depend on the reader software to obey them, so the security they provide is limited.

The PDF Reference has technical details for an end-user overview.<ref>{{cite web|url=http://createpdf.adobe.com/cgi-feeder.pl/help_security?BP=&LOC=en_US |title=Create Adobe PDF Online - Security Settings Help |publisher=Createpdf.adobe.com |date= |accessdate=2010-02-21}}</ref>  Like HTML files, PDF files may submit information to a web server. This could be used to track the [[IP address]] of the client PC, a process known as [[phoning home]]. After update 7.0.5 to Acrobat Reader, the user is notified "...&nbsp;via a dialogue box that the author of the file is auditing usage of the file, and be offered the option of continuing."<ref>[https://www.adobe.com/support/techdocs/332208.html New features and issues addressed in the Acrobat 7.0.5 Update (Acrobat and Adobe Reader for Windows and Mac OS)]</ref>

Through its [[Adobe LiveCycle|LiveCycle Policy Server]] product, Adobe provides a method to set security policies on specific documents. This can include requiring a user to authenticate and limiting the period during which a document can be accessed or amount of time a document can be opened while offline. Once a PDF document is tied to a policy server and a specific policy, that policy can be changed or revoked by the owner. This controls documents that are otherwise "in the wild." Each document open and close event can also be tracked by the policy server. Policy servers can be set up privately or Adobe offers a public service through Adobe Online Services. As with other forms of DRM, adherence to these policies and restrictions may or may not be enforced by the reader software being used.

=== Default display settings ===
PDF documents can contain display settings, including the page display layout and zoom level. Adobe Reader uses these settings to override the user's default settings when opening the document.<ref>{{cite web | title=Getting Familiar with Adobe Reader &gt; Understanding Preferences | url=http://www.adobepress.com/articles/article.asp?p=412914 | accessdate=2009-04-22}}</ref> The free Adobe Reader cannot remove these settings.

== Content ==
A PDF file is often a combination of [[vector graphics]], text, and [[bitmap graphics]]. The basic types of content in a PDF are:
* Text stored as content streams (i.e., not text)
* Vector graphics for illustrations and designs that consist of shapes and lines
* Raster graphics for photographs and other types of image
* Multimedia objects in the document

In later PDF revisions, a PDF document can also support links (inside document or web page), forms, JavaScript (initially available as plugin for Acrobat 3.0), or any other types of embedded contents that can be handled using plug-ins.

PDF 1.6 supports interactive 3D documents embedded in the PDF - 3D drawings can be embedded using [[U3D]] or [[PRC (file format)|PRC]] and various other data formats.<ref name="3d#1">{{cite web|url=https://www.adobe.com/manufacturing/resources/3dformats/ |title=3D supported formats |publisher=Adobe |date=2009-07-14 |accessdate=2010-02-21}}</ref><ref name="3d#2">{{cite web|url=https://www.adobe.com/devnet/acrobat3d/ |title=Acrobat 3D Developer Center |publisher=Adobe |date= |accessdate=2010-02-21}}</ref>

Two PDF files that look similar on a computer screen may be of very different sizes. For example, a high resolution raster image takes more space than a low resolution one. Typically higher resolution is needed for printing documents than for displaying them on screen. Other things that may increase the size of a file is embedding full fonts, especially for Asiatic scripts, and storing text as graphics.

== Software ==
{{Details|List of PDF software}}
PDF viewers are generally provided free of charge, and many versions are available from a variety of sources.

There are many software options for creating PDFs, including the PDF printing capabilities built into [[macOS]] and most [[Linux]] distributions, [[LibreOffice]], [[Microsoft Office 2007]] (if updated to [[Office 2007#Service Pack 2|SP2]]) and later,<ref>{{cite web |url=http://support.microsoft.com/kb/953195|title=Description of 2007 Microsoft Office Suite Service Pack 2 (SP2) |publisher=[[Microsoft]] |accessdate=2009-05-09}}</ref> [[WordPerfect]] 9, [[Scribus]], numerous PDF print drivers for [[Microsoft Windows]], the [[pdfTeX]] typesetting system, the [[DocBook]] PDF tools, applications developed around [[Ghostscript]] and [[Adobe Acrobat]] itself as well as [[Adobe InDesign]], [[Adobe FrameMaker]], [[Adobe Illustrator]], [[Adobe Photoshop]]. [[Google]]'s online office suite [[Google Docs]] also allows for uploading and saving to PDF.

[[Raster image processor]]s (RIPs) are used to convert PDF files into a [[raster graphics|raster format]] suitable for imaging onto paper and other media in printers, digital production presses and [[prepress]] in a process known as [[rasterisation]]. RIPs capable of processing PDF directly include the Adobe PDF Print Engine<ref>{{cite web|url=https://www.adobe.com/products/pdfprintengine/overview.html|title=Adobe PDF Print Engine|work=adobe.com}}</ref> from [[Adobe Systems]] and Jaws<ref>{{cite web|url=http://www.globalgraphics.com/products/jaws_rip/|title=Jaws® 3.0 PDF and PostScript RIP SDK|work=globalgraphics.com}}</ref> and the [[Harlequin RIP]] from [[Global Graphics]].

=== Editing ===
{{Expand section|date=July 2010|reason=[[hybrid PDF]], a variant of [[LibreOffice]] isn't mentioned}}
There is specialized software for editing PDF files, though the choices are much more limited and often more expensive than creating and editing standard editable document formats. Version 0.46 and later of [[Inkscape]] allows PDF editing through an intermediate translation step involving [[Poppler (software)|Poppler]].

[[Serif PagePlus]] can open, edit and save existing PDF documents, as well as publishing of documents created in the package.

[[Enfocus]] PitStop Pro, a plugin for Acrobat, allows manual and automatic editing of PDF files,<ref>{{cite web|url=http://www.enfocus.com/product.php?id=855|title=Preflight and edit PDF files in Acrobat|work=enfocus.com}}</ref> while the free Enfocus Browser makes it possible to edit the low-level structure of a PDF.<ref>{{cite web|url=http://www.enfocus.com/product.php?id=4530|title=Enfocus product overview - online store|work=enfocus.com}}</ref>

[[Dochub]], is a free online PDF editing tool that can be used without purchasing anything.<ref>{{Cite web|title = DocHub|url = http://www.dochub.com|website = DocHub|accessdate = 2015-12-12}}</ref>

=== Annotation ===
{{See also|Comparison of notetaking software}}
[[Adobe Acrobat]] is one example of proprietary software that allows the user to annotate, highlight, and add notes to already created PDF files. One UNIX application available as [[free software]] (under the [[GNU General Public License]]) is [[PDFedit]]. Another GPL-licensed application native to the unix environment is Xournal. Xournal allows for annotating in different fonts and colours, as well as a rule for quickly underlining and highlighting lines of text or paragraphs. Xournal also has a shape recognition tool for squares, rectangles and circles. In Xournal annotations may be moved, copied and pasted. The [[freeware]] [[Foxit Reader]], available for [[Microsoft Windows]], [[macOS]] and [[Linux]], allows annotating documents. Tracker Software's [[PDF-XChange Viewer]] allows annotations and markups without restrictions in its freeware alternative. [[Apple Inc.|Apple]]'s [[macOS]]'s integrated PDF viewer, Preview, does also enable annotations as does the freeware [[Skim (software)|Skim]], with the latter supporting interaction with [[LaTeX]], SyncTeX, and PDFSync and integration with [[BibDesk]] reference management software. Freeware [[Qiqqa]] can create an annotation report that summarizes all the annotations and notes one has made across their library of PDFs.

For mobile annotation, [[iAnnotate PDF]] (from Branchfire) and [[GoodReader]] (from Aji) allow annotation of PDFs as well as exporting summaries of the annotations.

There are also [[web annotation]] systems that support annotation in pdf and other documents formats, e.g., [[A.nnotate]], [[crocodoc]], WebNotes.

In cases where PDFs are expected to have all of the functionality of paper documents, ink annotation is required. Some programs that accept ink input from the mouse may not be responsive enough for handwriting input on a tablet. Existing solutions on the PC include [[PDF Annotator]] and [[Qiqqa]].

=== Other ===
Examples of PDF software as online services including [[Scribd]] for viewing and storing, [[Pdfvue]] for online editing, and [[Zamzar]] for conversion.

In 1993 the Jaws [[raster image processor]] from [[Global Graphics]] became the first shipping prepress RIP that interpreted PDF natively without conversion to another format. The company released an upgrade to their Harlequin RIP with the same capability in 1997.<ref>{{cite web |url= http://www.globalgraphics.com/products/harlequin-multi-rip |title=Harlequin MultiRIP|accessdate=2014-03-02}}</ref>

[[Agfa-Gevaert]] introduced and shipped Apogee, the first prepress workflow system based on PDF, in 1997.

Many commercial offset printers have accepted the submission of press-ready PDF files as a print source, specifically the PDF/X-1a subset and variations of the same.<ref>[http://www.prepressx.com/ Press-Ready PDF Files] "For anyone interested in having their graphic project commercially printed directly from digital files or PDFs." (last checked on 2009-02-10).</ref> The submission of press-ready PDF files are a replacement for the problematic need for receiving collected native working files.

PDF was selected as the "native" [[metafile]] format for [[macOS|Mac OS X]], replacing the [[PICT]] format of the earlier [[classic Mac OS]]. The imaging model of the [[Quartz (graphics layer)|Quartz]] graphics layer is based on the model common to [[Display PostScript]] and PDF, leading to the nickname ''Display PDF''. The Preview application can display PDF files, as can version 2.0 and later of the [[Safari (web browser)|Safari]] web browser. System-level support for PDF allows Mac OS X applications to create PDF documents automatically, provided they support the OS-standard printing architecture. The files are then exported in PDF 1.3 format according to the file header. When taking a screenshot under Mac OS X versions 10.0 through 10.3, the image was also captured as a PDF; later versions save screen captures as a [[Portable Network Graphics|PNG]] file, though this behaviour can be set back to PDF if desired.

In 2006 PDF was widely accepted as the standard print job format at the [[Open Source Development Labs]] Printing Summit. It is supported as a print job format by the [[CUPS|Common Unix Printing System]] and desktop application projects such as [[GNOME]], [[KDE]], [[Firefox]], [[Mozilla Thunderbird|Thunderbird]], [[LibreOffice]] and [[OpenOffice.org|OpenOffice]] have switched to emit print jobs in PDF.<ref>{{cite web|title=PDF as Standard Print Job Format|url=http://www.linuxfoundation.org/collaborate/workgroups/openprinting/pdf_as_standard_print_job_format|website=The Linux Foundation|publisher=[[Linux Foundation]]|accessdate=21 June 2016}}</ref>

Some desktop printers also support direct PDF printing, which can interpret PDF data without external help. Currently, all PDF capable printers also support PostScript, but most PostScript printers do not support direct PDF printing.

The [[Free Software Foundation]] once considered one of their [[High priority free software projects|high priority projects]] to be "developing a free, high-quality and fully functional set of libraries and programs that implement the PDF file format and associated technologies to the ISO 32000 standard."<ref>On 2014-04-02, a note dated 2009-02-10 referred to [http://www.fsf.org/campaigns/priority.html Current FSF High Priority Free Software Projects] as a source. Content of the latter page, however, changes over time.</ref><ref>{{cite web |url=http://gnupdf.org/Goals_and_Motivations |title=Goals and Motivations |authors=GNUpdf contributors| publisher=''GNUpdf'' |date=2007-11-28 |website=gnupdf.org |accessdate=2014-04-02 }}</ref> In 2011, however, the [[GNU PDF]] project was removed from the list of "high priority projects" due to the maturation of the [[Poppler (software)|Poppler library]],<ref>{{cite web|title=GNU PDF project leaves FSF High Priority Projects list; mission complete! |url=http://www.fsf.org/blogs/community/gnu-pdf-project-leaves-high-priority-projects-list-mission-complete|date=2011-10-06|first=Matt|last=Lee|publisher=Free Software Foundation|website=fsf.org|accessdate=2014-04-02}}</ref> which has enjoyed wider use in applications such as [[Evince]] with the [[GNOME]] desktop environment. Poppler is based on [[Xpdf]]<ref>[http://poppler.freedesktop.org/ Poppler homepage] "Poppler is a PDF rendering library based on the xpdf-3.0 code base." (last checked on 2009-02-10)</ref><ref>[http://cgit.freedesktop.org/poppler/poppler/tree/README-XPDF Xpdf license] "Xpdf is licensed under the GNU General Public License (GPL), version 2 or 3." (last checked on 2012-09-23).</ref> code base. There are also commercial development libraries available as listed in [[List of PDF software]].

The [[Apache PDFBox]] project of the [[Apache Software Foundation]] is an open source Java library for working with PDF documents. PDFBox is licensed under the [[Apache License]].<ref>[http://pdfbox.apache.org/ The Apache PDFBox project] . Retrieved 2009-09-19.</ref>

== See also ==
{{Portal|Software}}{{columns-list|2|
* [[Open XML Paper Specification]]
* [[Comparison of OpenXPS and PDF]]
* [[DjVu]]
* [[PAdES]], <small>PDF Advanced Electronic Signature</small>
* [[Web document]]
* [[XSL Formatting Objects]]
}}

== References ==
{{Reflist|30em}}

== Further reading ==
* {{Cite book | last1 = Hardy | first1 = M. R. B. | last2 = Brailsford | first2 = D. F. | chapter = Mapping and displaying structural transformations between XML and PDF | title = Proceedings of the 2002 ACM symposium on Document engineering  - DocEng '02 | pages = 95–102| year = 2002 | url = http://www.cs.nott.ac.uk/~dfb/Publications/Download/2002/Hardy02.pdf| doi = 10.1145/585058.585077| publisher = Proceedings of the 2002 ACM symposium on Document engineering| isbn = 1-58113-594-7}}
*Standards
** PDF 1.7 [http://www.adobe.com/content/dam/Adobe/en/devnet/acrobat/pdfs/pdf_reference_1-7.pdf]
** PDF 1.6 (ISBN 0-321-30474-8)
** PDF 1.4 (ISBN 0-201-75839-3)
** PDF 1.3 (ISBN 0-201-61588-6)

== External links ==
{{Commons category|PDF}}
* [http://www.quora.com/PDF-file-format/How-was-the-PDF-format-created How was the PDF format created? Quora]
* [http://www.pdfa.org/ PDF Association] - The PDF Association is the industry association for software developers producing or processing PDF files.
* [http://partners.adobe.com/public/developer/tips/topic_tip31.html Adobe PDF 101: Summary of PDF]
* [https://www.adobe.com/print/features/psvspdf/ Adobe: PostScript vs. PDF] – Official introductory comparison of PS, EPS vs. PDF.
* {{webarchive |url=https://web.archive.org/web/20110424013530/http://www.aiim.org/Resources/Archive/Magazine/2007-Jul-Aug/33448 |date=April 24, 2011 |title=''PDF Standards....transitioning the PDF specification from a de facto standard to a de jure standard'' }} – Information about PDF/E and PDF/UA specification for accessible documents file format (archived by [[Wayback Machine|The Wayback Machine]])
* [http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=38920 ISO 19005-1:2005] the PDF/A-1 ISO standard published by the [[International Organization for Standardization]] (chargeable)
* [https://www.adobe.com/devnet/pdf/pdf_reference.html PDF Reference and Adobe Extensions to the PDF Specification]
* [http://www.mactech.com/articles/mactech/Vol.15/15.09/PDFIntro/ Portable Document Format: An Introduction for Programmers] – Introduction to PDF vs. PostScript and PDF internals (up to v1.3)
* [http://www.planetpdf.com/enterprise/article.asp?ContentID=6519 The Camelot Paper] – the paper in which John Warnock outlined the project that created PDF
* [http://river-valley.zeeba.tv/everything-you-wanted-to-know-about-pdf-but-were-afraid-to-ask/ Everything you wanted to know about PDF but were afraid to ask] - recording of talk by Leonard Rosenthol (Adobe Systems) at TUG 2007
* [http://www.data2type.de/en/xml-xslt-xslfo/xsl-fo/ How to produce PDF with XSL-FO]
* [http://pdfextractoronline.com/ PDF To Excel Converter]
{{Graphics file formats}}
{{Office document file formats}}
{{ISO standards}}
{{Ebooks}} <!--navbox-->

[[Category:1993 introductions]]
[[Category:Adobe Systems]]
[[Category:Digital press]]
[[Category:Electronic documents]]
[[Category:Graphics file formats]]
[[Category:ISO standards]]
[[Category:Office document file formats]]
[[Category:Open formats]]
[[Category:Page description languages]]
[[Category:Vector graphics]]
<=====doc_Id=====>:428
<=====title=====>:
Arts and Humanities Citation Index
<=====text=====>:
{{ infobox bibliographic database
| image       = 
| caption     = 
| producer    =Thomson Reuters 
| country     =United States 
| history     = 
| languages   = 
| providers   =Web of Science, Dialog Bluesheets 
| cost        =Subscription 
| disciplines =Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, Religion, Television, Theater, and Radio 
| depth       =Index, abstract, citation indexing, author 
| formats     =original research articles, reviews, editorials, chronologies, abstracts,   scripts, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, book reviews, films, music, and theatrical performances 
| temporal    =1975 to present 
| geospatial  =global 
| number      = 
| updates     = 
| p_title     = 
| p_dates     = 
| ISSN        = 
| web         = 
| titles      =  
}}

The '''''Arts & Humanities Citation Index''''' ('''A&HCI'''), also known as '''''Arts & Humanities Search''''', is a [[citation index]], with abstracting and indexing for more than 1,700 arts and humanities journals, and coverage of disciplines that includes social and natural science journals. Part of this database is derived from [[Current Contents]] records. Furthermore, the print counterpart is Current Contents.

Subjects covered are the Arts, Humanities, Language (including Linguistics), Poetry, Music, Classical works, History, Oriental Studies, Philosophy, Archaeology, Architecture, History, Religion, Television, Theater, and Radio.

Available citation (source) coverage includes articles, letters, editorials, meeting abstracts, errata, poems, short stories, plays, music scores, excerpts from books, chronologies, bibliographies and filmographies, as well as citations to reviews of books, films, music, and theatrical performances.

This database can be accessed online through ''[[Web of Science]]''. It provides access to current and retrospective bibliographic information and cited references. It also covers individually selected, relevant items from approximately 1,200 titles, mostly arts and humanities journals but with an unspecified number of titles from other disciplines.

According to Thomson Reuters, the ''Arts & Humanities Search'', can be accessed via Dialog, DataStar, and OCLC, with weekly updates and backfiles to 1980.<ref name=dialog-blue>
{{Cite web
  | title =Arts & Humanities Search (File 255) 
  | publisher =Dialog bluesheets  
  | date = 
  | url =http://library.dialog.com/bluesheets/html/bl0439.html 
  | format =Online web page 
  | accessdate =2011-07-03}}</ref><ref name=Iowa>
Description of Arts & Humanities Search. 
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/artshm.html
  | format =Online web page 
  | accessdate =2011-07-03}}</ref><ref name=Iowa-wos>
Description of Web of Science coverage.  
{{Cite web
  | title =e-Library catalog
  | publisher =Iowas State University  
  | year =2008 
  | url =http://www.lib.iastate.edu/collections/db/websci.html
  | format =Online web page 
  | accessdate =2011-07-03}}</ref><ref name=TR>
See the page entitled "Tech Specs" 
{{Cite web
  | title =Database description
  | publisher =Thomson Reuters  
  | year = 
  | url =http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/#tab3
  | format =Online web page 
  | accessdate =2011-07-03}}</ref>

==History==
The index was originally developed by the [[Institute for Scientific Information]], which was later acquired by [[Thomson Scientific]]. It is now published by [[Thomson Reuters]]' IP & Science division.

==See also==
* [[Science Citation Index]]
* [[Social Sciences Citation Index]]

==References==
{{Reflist}}

== External links ==
* {{Official website|http://thomsonreuters.com/products_services/science/science_products/a-z/arts_humanities_citation_index/}} at Thomson Reuters.
* [http://science.thomsonreuters.com/cgi-bin/jrnlst/jlsubcatg.cgi?PC=H Subject categories] of the Arts and Humanities Citation Index.

{{Thomson Reuters}}

{{DEFAULTSORT:Arts And Humanities Citation Index}}
[[Category:Citation indices]]
[[Category:Thomson Reuters]]
[[Category:Arts journals| ]]
[[Category:Humanities journals| ]]
<=====doc_Id=====>:431
<=====title=====>:
Islamic World Science Citation Database
<=====text=====>:
'''Islamic World Science Citation Database''' ('''ISC''') is a [[citation index]] established by the Iranian [[Ministry of Science, Research and Technology]] after it was approved by the [[Organisation of the Islamic Conference]].  It only indexes journals from the [[Islamic world]].

It was announced in [[Baku]], Azerbaijan during the Fourth Islamic Conference of the Ministers of Higher Education and Scientific Research held in October 2008.<ref>{{cite news | url = http://www.scidev.net/en/science-communication/science-publishing/news/islamic-countries-to-get-own-science-citation-inde.html | title = Islamic countries to get own science citation index | author = Wagdy Sawahel | date = 17 October 2008 | publisher = [[SciDev.Net]] }}</ref>  It is managed by the Islamic World Science Citation Center, located in [[Shiraz]].

In 2009, ISC partnered with [[Scopus]] that allows ISC's publications to be indexed in Scopus.<ref>{{cite journal | journal = [[Library Connect]] | title = The Islamic World Science Citation Database partnership with Scopus brings greater visibility to Islamic researchers | url = http://libraryconnect.elsevier.com/lcn/0703/lcn070319.html | author = Ahmed Rostom | volume = 7 | issue = 3 | date = August 2009 | issn = 1549-3725 }}</ref>

== References ==
{{Reflist}}

==See also==
* [[Academic publishing]]
* [[List of academic databases and search engines]]
* [[Impact factor]]

== External links ==
* {{Official website|http://isc.gov.ir/Default.aspx?lan=en}}

[[Category:Bibliographic databases and indexes]]
[[Category:Online databases]]
[[Category:Citation indices]]
[[Category:Research management]]
[[Category:Databases in Iran]]
[[Category:Science and technology in Iran]]


{{science-journal-stub}}
{{islam-stub}}
{{iran-stub}}
<=====doc_Id=====>:434
<=====title=====>:
Russian Science Citation Index
<=====text=====>:
{{primary sources|date=March 2012}}
'''Russian Science Citation Index''' is a [[bibliographic database]] of [[scientific publication]]s in Russian. It accumulates more than 2 million publications of Russian authors, as well as information about citing these publications from more than 2000 Russian journals. The Russian Science Citation Index has been developed since 2009 by the Scientific Electronic Library. The information-analytical system Science Index is a search engine of this database; it offers a wide range of services for authors, research institutions and scientific publishers. It is designed not only for operational search for relevant bibliographic information, but is also as a powerful tool to assess the impact and effectiveness of research organizations, scientists, and the level of scientific journals, etc.

== Purpose ==
From 3000 Russian scientific journals only about 150 are presented in foreign databases (i.e. not more than 5%). Those are mainly translated journals. So far, the vast majority of Russian scientific publications remain "invisible" and not available online.  Russian Science Citation Index makes it real to objectively compare Russian journals with  the best international journals and brings them closer to researchers all over the world.

== Functionality ==
In Russia, this database is one of the main sources of information for evaluating the effectiveness of organizations involved in research. It allows to appraise: 
* Scientific capacity and effectiveness of research, and
* Publication activity
through the following indicators:
* The number of publications (including foreign scientific and technical journals, and local publications from the list of [[Higher Attestation Commission]]) of researchers from a particular scientific organization, divided by the number of researchers,
* The number of publications (registered in the Russian Science Citation Index) of researchers from a particular scientific organization, divided by the number of researchers, and
* Citation of researchers (registered in the Russian Science Citation Index) from a particular scientific organization, divided by the number of researchers.

== See also ==
*[[List of academic databases and search engines]]
*[[Science Citation Index]]
*[[Scopus]]

==External links==
* [http://elibrary.ru/ Scientific Electronic Library]


[[Category:Citation indices]]
[[Category:Russian-language journals| ]]
[[Category:Science and technology in Russia]]
<=====doc_Id=====>:437
<=====title=====>:
Citation index
<=====text=====>:
{{distinguish|Citation metric}}

A '''citation index''' is a kind of [[bibliographic index]], an index of [[citation]]s between publications, allowing the user to easily establish which later documents cite which earlier documents. A form of citation index is first found in 12th-century Hebrew religious literature. Legal citation indexes are found in the 18th century and were made popular by [[citator]]s such as [[Shepard's Citations]] (1873). In 1960, [[Eugene Garfield]]'s [[Institute for Scientific Information]] (ISI) introduced the first citation index for papers published in [[academic journal]]s, first the ''[[Science Citation Index]]'' (SCI), and later the ''[[Social Sciences Citation Index]]'' (SSCI) and the ''[[Arts and Humanities Citation Index]]'' (AHCI). The first automated citation indexing was done by [[CiteSeer]] in 1997. Other sources for such data include [[Google Scholar]] and Elsevier's [[Scopus]].

==History==
The earliest known citation index is an index of biblical citations in [[rabbinic literature]], the ''Mafteah ha-Derashot'', attributed to [[Maimonides]] and probably dating to the 12th century. It is organized alphabetically by biblical phrase. Later biblical citation indexes are in the order of the canonical text. These citation indices were used both for general and for legal study. The Talmudic citation index ''En Mishpat'' (1714) even included a symbol to indicate whether a Talmudic decision had been overridden, just as in the 19th-century ''Shepard's Citations''.<ref>Bella Hass Weinberg, "The Earliest Hebrew Citation Indexes" in Trudi Bellardo Hahn, Michael Keeble Buckland, eds., ''Historical Studies in Information Science'', 1998, p. 51''ff''</ref><ref>Bella Hass Weinberg, "Predecessors of Scientific Indexing Structures in the Domain of Religion" in W. Boyden Rayward, Mary Ellen Bowden, ''The History and Heritage of Scientific and Technological Information Systems'', Proceedings of the 2002 Conference, 2004, p. 126''ff''</ref> Unlike modern scholarly citation indexes, only references to one work, the Bible, were indexed.

In English legal literature, volumes of judicial reports included lists of cases cited in that volume starting with ''Raymond's Reports'' (1743) and followed by ''Douglas's Reports'' (1783). Simon Greenleaf (1821) published an alphabetical list of cases with notes on later decisions affecting the precedential authority of the original decision.<ref name='shapiro'/>

The first true citation index dates to the 1860 publication of Labatt's ''Table of Cases...California...'', followed in 1872 by Wait's ''Table of Cases...New York...''. But the most important and best-known citation index came with the 1873 publication of [[Shepard's Citations]].<ref name='shapiro'>Fred R. Shapiro, "Origins of Bibliometrics, Citation Indexing, and Citation Analysis: The Neglected Legal Literature" ''Journal of the American Society of Information Science'' '''43''':5:337-339 (1992)</ref>

==Major citation indexing services==
{{main article|Indexing and abstracting service}}
{{main cat|Citation indices}}
General-purpose academic citation indexes include:
*[[Web of Science]] by [[Clarivate Analytics]] (previously the Intellectual Property and Science business of [[Thomson Reuters]])
*[[Scopus]] by [[Elsevier]], available online only, which similarly combines subject searching with citation browsing and tracking in the sciences and [[social sciences]].
*[[Indian Citation Index (ICI)|Indian Citation Index]] is an online citation data which covers [[peer review]]ed journals published from India. It covers major subject areas such as scientific, technical, medical, and [[social sciences]] and includes arts and humanities. The citation database is the first of its kind in India.
Each of these offer an index of citations between publications and a mechanism to establish which documents cite which other documents. They differ widely in cost: Web of Science and Scopus are available by subscription (generally to libraries).

In addition, [[CiteSeer]] and [[Google Scholar]] are freely available online.

==See also==
* [[Microsoft Academic Search]]
* [[Google Scholar]]
* [[Scopus]]
* [[Semantic Scholar]]
* [[Citation analysis]]
* [[Acknowledgment index]]
* [[CiteSeer]]
* [[CiteSeerX]]
* [[Scientific journal]]
* [[Science Citation Index]]
* [[Indian Citation Index]]
* [[Journal Citation Reports]]
* [[Emerging Sources Citation Index (ESCI)]]
* [[SciELO]]
* [[Redalyc]]
* [[Index Copernicus]]

==References==
{{Reflist}}

{{DEFAULTSORT:Citation Index}}
[[Category:Academic publishing]]
[[Category:Bibliometrics]]
[[Category:Bibliographic databases and indexes]]
[[Category:Reputation management]]
[[Category:Citation indices| ]]
<=====doc_Id=====>:440
<=====title=====>:
ScienceOpen
<=====text=====>:
{{Multiple issues|
{{notability|Organizations|date=February 2016}}
{{refimprove|date=February 2016}}
}}

{{Infobox organization
| name                = ScienceOpen
| native_name         = 
| native_name_lang    = 
| named_after         = 
| image               =
| image_size          = 300px
| alt                 =  ScienceOpen's logo
| caption             = 
| map                 = 
| map_size            = 
| map_alt             = 
| map_caption         = 
| map2                = 
| map2_size           = 
| map2_alt            = 
| map2_caption        = 
| abbreviation        = 
| motto               = 
| predecessor         = 
| merged              = 
| successor           = 
| formation           = <!-- use {{start date|YYYY|MM|DD|df=y}} -->
| founder             = 
| founding_location   = 
| extinction          = <!-- use {{end date and age|YYYY|MM|DD}} -->
| merger              = 
| type                = 
| tax_id              = <!-- or | vat_id = (for European organizations) -->
| registration_id     = <!-- for non-profit org -->
| status              = 
| purpose             = 
| headquarters        = [[Berlin]], [[Germany]]
| location            = 
| coords              = <!-- {{coord|LAT|LON|display=inline,title}} -->
| region              = 
| services            = 
| products            = 
| methods             = 
| fields              = 
| membership          = 
| membership_year     = 
| language            = 
| owner               = <!-- or | owners = -->
| sec_gen             = 
| leader_title        = 
| leader_name         = 
| leader_title2       = 
| leader_name2        = 
| leader_title3       = 
| leader_name3        = 
| leader_title4       = 
| leader_name4        = 
| board_of_directors  = 
| key_people          = 
| main_organ          = 
| parent_organization = 
| subsidiaries        = 
| secessions          = 
| affiliations        = 
| budget              = 
| budget_year         = 
| revenue             = 
| revenue_year        = 
| disbursements       = 
| expenses            = 
| expenses_year       = 
| endowment           = 
| staff               = 
| staff_year          = 
| volunteers          = 
| volunteers_year     = 
| slogan              = 
| mission             = 
| website             = {{URL|scienceopen.com}}
| remarks             = 
| formerly            = 
| footnotes           = 
}}
'''ScienceOpen''' is a privately owned discovery and research network with three roles:  aggregation, [[Open access|open access publishing]] and the evaluation of scholarly literature in all scholarly disciplines.

== History ==
ScienceOpen began in 2013<ref>{{Cite web|title = OA interviews: Alexander Grossmann, ScienceOpen - Open-access publishing - Research Information|url = http://www.researchinformation.info/features/feature.php?feature_id=473|website = www.researchinformation.info|accessdate = 2015-11-19}}</ref> when Alexander Grossmann, a professor of Publishing Management at the [[Leipzig University of Applied Sciences]] and former publishing director at scientific house [http://www.degruyter.com/dg/page/15/the-publishing-house De Gruyter], and Tibor Tscheke, president and CEO of content management systems company [http://ovitas.com/ Ovitas], decided to start a platform. Their idea was to allow researchers to share scientific information, both formally by publishing articles and participating in [https://futureofscipub.wordpress.com/open-post-publication-peer-review/ post-publication peer review], and informally by reviewing their colleagues’ work, providing endorsements and comments, and by updating their own papers.

Its beta version was introduced in November 2013, and release 1.0 launched in May 2014.<ref>{{Cite web|title = ScienceOpen: the next wave of Open Access? - EuroScientist Webzine|url = http://www.euroscientist.com/scienceopen-next-wave-open-access|website = EuroScientist Webzine|accessdate = 2015-11-19|language = en-US}}</ref> As of September 2015 the site has 10 million articles and records<ref>{{Cite web|title = Open and Shut?: The OA Interviews: ScienceOpen’s Alexander Grossmann|url = http://poynder.blogspot.com/2015/11/the-oa-interviews-scienceopens.html|website = Open and Shut?|date = 2015-11-16|accessdate = 2015-11-19|first = Richard|last = Poynder}}</ref> from [[PubMed Central]], [[ArXiv]], [[PubMed]] and ScienceOpen, and a publicly available citation index which is free for researchers to use wherever they are and is provided at no cost to libraries, which in February 2016 was dubbed the Open Citation Index.<ref>http://blog.scienceopen.com/2016/02/the-open-citation-index/</ref> All content on the platform is available for post-publication peer review by scientific members with five or more peer-reviewed publications on their [[ORCID]], and all articles can be publicly commented on by members with one or more items.

ScienceOpen appoints members of the research community as Collection Editors<ref>{{Cite web|title = ScienceOpen Collections|url = http://about.scienceopen.com/scienceopen-collections/#more-390|website = About ScienceOpen|accessdate = 2015-11-19|language = en-US}}</ref> who curate articles from multiple publishers in any topic. [[Thieme Medical Publishers|Thieme]], a German medical publisher, mirrors three open access journals [https://www.scienceopen.com/collection/Thieme on the platform].

The organization is based in Berlin and has a technical office in Boston. It is a member of [[CrossRef]], [[ORCID]], the [[Open Access Scholarly Publishers Association]]<ref>{{Cite web|url=http://oaspa.org/member/scienceopen/|title=Member Record: ScienceOpen|last=|first=|date=|website=|publisher=|access-date=}}</ref> and the [[Directory of Open Access Journals]]. The company was designated as one of  “10 to Watch” by research advisory firm Outsell in its report “[http://img.en25.com/Web/CopyrightClearanceCenterInc/%7Bfc9f07ac-b2c9-4cd7-b763-2f21e0c6e94b%7D_Outsell_2015_Open_Access_Report.pdf Open Access 2015: Market Size, Share, Forecast, and Trends].”

In 2015, Tscheke provided further clarification of ScienceOpen’s focus on aggregation and filtering content.<ref>{{Cite web|title = There’s more to Open Access than APCs, right? – ScienceOpen Blog|url = http://blog.scienceopen.com/2015/10/theres-more-to-open-access-than-apcs-right/|website = blog.scienceopen.com|accessdate = 2015-11-19}}</ref>

== Business model ==
ScienceOpen publishes articles of almost any type and from any research field, including the social sciences and humanities. This includes primary research articles, opinion papers, posters, case studies, negative results, and data publications. To fund article publication, ScienceOpen charges a publication fee ($800 as of this time of writing, in 2015) to be paid by the author or the author’s employer, funder or library. This is for a post-publication peer review process and publication after editorial control,<ref>{{Cite web|url=http://blog.scienceopen.com/2016/06/review-instructions-for-scienceopen/|title=Review Instructions for ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> and is facilitated through integration with [[ORCID]].<ref>{{Cite web|url=http://blog.scienceopen.com/2016/06/orcid-integration-at-scienceopen/|title=ORCID integration at ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> Additionally, authors can opt to use the 'Peer Review by Endorsement' model,<ref>{{cite journal |last= Jan Velterop |date= 29 September 2015 |title= Peer review – issues, limitations, and future development |url= https://www.scienceopen.com/document?15&vid=1dcfbe69-c30c-4eaa-a003-948c9700da40 |journal= ScienceOpen Research |doi= 10.14293/S2199-1006.1.SOR-EDU.AYXIPS.v1 |access-date=14 June 2016}}</ref> in which peer review is conducted prior to submission, and for a fee of $400. Included in this fee are up to two article revisions within 12 months, with full version control. Revised versions have a new DOI so that it is easier to link back to cited versions. A partial or full fee waiver is available to those who demonstrate need. [[Poster session]] publishing is free. All published articles are published via a [[Creative Commons]] Attribution 4.0 International Public License.<ref>{{Cite web|url=http://about.scienceopen.com/open-access-explanation-of-cc-by-license/|title=Open Access License Agreement|last=|first=|date=|website=|publisher=|access-date=}}</ref>

From 2016, ScienceOpen has started partnering with publishers to offer advanced indexing services. In June 2016 they partnered with [[SciELO]], the largest publisher in Latin America{{Cite web|title = ScienceOpen helps to put scientific research in a global context with more than 15 million article records – ScienceOpen Blog|url = http://blog.scienceopen.com/2016/06/scienceopen-helps-to-put-scientific-research-in-a-global-context-with-more-than-15-million-article-records/|website = blog.scienceopen.com|accessdate = 2016-06-14}}. Additional publishing partners include Higher Education Press<ref>{{Cite web|url=http://blog.scienceopen.com/2016/04/higher-education-press-indexing-partnership-with-scienceopen/|title=Higher Education Press indexing partnership with ScienceOpen|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> and The Italian Society of Victimology.<ref>{{Cite web|url=http://blog.scienceopen.com/2016/06/welcome-to-the-italian-society-of-victimology/|title=Welcome to the Italian Society of Victimology|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> In September 2015, ScienceOpen hit the 10 million article record mark,<ref>{{Cite web|url=http://www.prnewswire.com/news-releases/scienceopen-hits-the-10-million-article-mark-527671151.html|title=ScienceOpen Hits the 10 Million Article Mark|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> and as of 27 June 2016 has more than 13 million records.

Every research article on ScienceOpen has a traceable genealogy through citations, a public peer review process, and social interaction tracked by [[altmetrics]], which they call research "context".<ref>{{Cite web|url=http://blog.scienceopen.com/2016/05/why-context-is-important-for-research/|title=Why ‘context’ is important for research|last=|first=|date=|website=|publisher=|access-date=27 June 2016}}</ref> The technology behind the ScienceOpen platform is provided by Ovitas.

ScienceOpen are findable on [https://www.youtube.com/user/ScienceOpen YouTube], [https://www.linkedin.com/company/scienceopen-inc- LinkedIn], [https://www.facebook.com/scienceopen/ Facebook], and [https://twitter.com/science_open Twitter].

== Publications ==
* [https://www.scienceopen.com/collection/scienceopen_research ScienceOpen Research], ISSN [https://www.worldcat.org/search?q=n2%3A2199-1006&qt=results_page 2199-1006]
* [https://www.scienceopen.com/collection/scienceopen_posters ScienceOpen Posters], ISSN [https://www.worldcat.org/search?q=n2%3A2199-8442&qt=results_page 2199-8442]

== Headquarters ==
ScienceOpen has its headquarters located at Pappelallee 78-79, 10437 Berlin and its technical hub is at 60 Mall Rd., Burlington, Mass.

== See also ==
* [[Open Access Scholarly Publishers Association]] (OASPA)
* [[Directory of Open Access Journals]] (DOAJ)
* [[Registry of Open Access Repositories Mandatory Archiving Policies]] (ROARMAP)

== References ==
{{Reflist}}

== External links ==
* {{official website|http://www.scienceopen.com}}

[[Category:Open access publishers]]
[[Category:Open access (publishing)]]
[[Category:Scholarly communication]]
[[Category:Citation indices]]
<=====doc_Id=====>:443
<=====title=====>:
Thomson Directories
<=====text=====>:
{{Advert|date=July 2009}}
{{Refimprove|date=July 2009}}
'''Thomson Directories''', more commonly referred to as Thomson Local, is a local business directory company based in [[Farnborough, Hampshire|Farnborough]], [[Hampshire]], [[England]], and offers business listings both in print and online following the launch of ThomsonLocal.com in 2003.

174 regional editions of the Thomson Local are produced and delivered free of charge to residential and commercial addresses throughout the UK.
 
The Chief Executive Officer is currently Elio Shiavo.<ref>CEO http://www.answers.com/topic/thomson-directories-1</ref> 

The company was purchased by [[US West]], a telecommunications company in the United States, in 1994.<ref>{{cite news|url=http://www.independent.co.uk/news/business/us-west-pays-70m-pounds-for-thomson-directories-american-telephone-company-continues-to-develop-multimedia-in-uk-1437180.html|title=US West pays 70m pounds for Thomson Directories: American telephone company continues to develop multimedia in UK |last=Fagan|first=Mary|date=20 May 1994|work=The Independent|accessdate=2009-08-15}}</ref> In 1999, the company was sold by [[3i]] to [[TDL Infomedia]], a subsidiary of [[Apax Partners]].<ref>{{cite news|url=http://www.independent.co.uk/news/business/3i-sells-thomson-guides-for-pounds-220m-1109799.html|title=  3i sells Thomson guides for pounds 220m |last=Baker|first=Lucy|date=31 July 1999|work=The Independent|accessdate=2009-08-15}}</ref>

The company was placed in [[Administration (law)]] in August 2013, and acquired by Corporate Media Partners.<ref>{{cite news|url=http://www.bbc.co.uk/news/uk-england-hampshire-23710958|title= Thomson Local directory firm goes into administration|work=BBC News|accessdate=2013-09-11}}</ref>

==References==
{{Reflist}}

==External links==
*[http://www.thomsondirectories.com/ Official Thomson Directory Site]

[[Category:Companies of the United Kingdom]]
[[Category:Directories]]
[[Category:Apax Partners companies]]
[[Category:3i Group companies]]


{{UK-company-stub}}
<=====doc_Id=====>:446
<=====title=====>:
World Radio TV Handbook
<=====text=====>:
{{redirect|WRTH|the radio station|WRTH (FM)}}
The '''''World Radio TV Handbook''''', also known as '''''WRTH''''', is a [[directory (databases)|directory]] of virtually every [[Radio station|radio]] and [[TV station]] on Earth, published yearly. It was started in 1947 by [[Oluf Lund Johansen]] (1891–1975) as the ''World Radio Handbook'' (WRH).<ref>[http://oz6gh.byethost33.com/lund_johansen.htm O. Lund-Johansen], presented by OZ6GH.</ref> The word "TV" was added to the title in 1965, when [[Jens M. Frost]] (1919–1999) took over as editor.<ref>[http://www.dswci.org/specials/membersofhonour/jens_frost.html DSWCI Member of Honour:  Jens M. Frost]</ref> It had then already included data for [[television broadcasting]] for some years. After the 40th edition in 1986, Frost handed over editorship to [[Andrew G. Sennitt|Andrew G. (Andy) Sennitt]].<ref>[http://www.agsmedia.nl/body_who.html Andy Sennitt], own presentation.</ref>

The first edition that bears an edition number is the 4th edition, published in 1949. The three previous editions appear to have been:
* the 1st edition, marked "Winter Ed. 1947" on the cover and completed in November 1947
* the 2nd edition, marked "1948 (May-November)" on the cover and completed in May 1948
* the 3rd edition, marked "1948-49" on the cover and completed in November 1948.

Summer Supplements appear to have been issued from 1959 through 1971. From 1959 through 1966 they were called the Summer Supplement. From 1967 through 1971 they were called the Summer Edition.

Through the 1969 edition, the WRTH indicated the date on which the manuscript was completed.

Issues with covers in Danish are known to have been available for the years 1948 May-November (2d ed.), 1950-51 (5th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), 1952 (6th ed.; cover and 1st page in Danish, rest in English, most ads in Danish), and probably others. The 1952 English ed., which is completely in English, has an extra page with world times and agents, and ads in English which are sometimes different from the ads in the Danish edition. Also, the 1953 ed. mentions the availability of a German edition.

[[Oluf Lund Johansen]] published, in conjunction with [[Libreria Hispanoamericana]] of [[Barcelona]], Spain, a [[softbound]] Spanish-language version of the 1960 WRTH. The book was printed in Spain and called ''Guia Mundial de Radio y Television'', and carried the WRTH logo at the time as well as all the editorial references contained in the English-language version. 

Hardbound editions are known to have been available for the years 1963 through 1966, 1968, 1969, and 1975-1978, and probably others.

== Publications ==
* Gilbert, Sean; Nelson, John; Jacobs, George, [https://books.google.com/books?id=IBu8NHvC4fMC&printsec=frontcover ''World Radio TV Handbook 2007''], Watson-Guptill, 2006. ISBN 0-9535864-9-9.

==References==
{{reflist}}

== External links ==
* http://www.wrth.com/

[[Category:Radio organizations]]
[[Category:Television organizations]]
[[Category:International broadcasting]]
[[Category:Directories]]
[[Category:1945 introductions]]
<=====doc_Id=====>:449
<=====title=====>:
Oregon Blue Book
<=====text=====>:
{{Infobox book 
| name          = Oregon Blue Book
| image         = OrBlueBookCover.png
| caption       = Cover of the 2005 edition
| editor        = [[Oregon Secretary of State]]
| country       = United States
| language      = English
| subject       = Oregon history, government
| genre         = Reference
| published     = Biennially, 1911–present
| media_type    = Print, online
| isbn          = 
| external_url  = http://bluebook.state.or.us/
}}
The '''''Oregon Blue Book''''' is the official directory and fact book for the U.S. state of [[Oregon]] prepared by the [[Oregon Secretary of State]]<ref name="ORS">{{cite web|url = https://www.oregonlegislature.gov/bills_laws/ors/ors177.html|title = ORS 177.120|publisher = [[Oregon Legislative Counsel]]|accessdate = February 16, 2015}}</ref> and published by the Office of the Secretary's [[Oregon State Archives|Archives Division]].

The ''Blue Book'' comes in both print and online editions. The [[Oregon Revised Statutes]] require the Secretary of State to publish the print edition "biennially on or about February 15 of the same year as the regular sessions of the [[Oregon Legislative Assembly|Legislative Assembly]],"<ref name=ORS/> which are during odd-numbered years; it has been so published since 1911.  The online edition is updated regularly.<ref name=About>{{cite web |url= http://bluebook.state.or.us/misc/about/about.htm |title= About the Oregon Blue Book |publisher= Oregon Secretary of State |accessdate= February 16, 2015}}</ref>

==Contents==
The book contains information on the state, city, county, and federal governments in Oregon, educational institutions, finances, the economy, resources, population figures and demographics.<ref name=ERG83>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19830410&id=jP5VAAAAIBAJ&sjid=UeIDAAAAIBAJ&pg=3537,2331038 |title= New Oregon Blue Book Published |author= [[United Press International]] |date= April 10, 1983 |newspaper= [[The Register-Guard]] |accessdate= February 16, 2015}}</ref>

The 1919 edition contained a "statement of registered motor vehicles, chauffeurs, and dealers from 1905 to 1919", and "a general summary of in the taxable property in Oregon from 1858 to 1918".<ref name=Received>{{cite news |url= https://news.google.com/newspapers?nid=1243&dat=19190911&id=0NgsAAAAIBAJ&sjid=HCAEAAAAIBAJ&pg=3731,5589153 |date= September 11, 1919 |title= Blue Book is Received Here |newspaper= [[The Bulletin (Bend)|The Bulletin]] |accessdate= February 17, 2015}}</ref>

==History==
Secretary of State [[Ben Olcott]] published the first edition in 1911 in response to an "increased demand for information of a general character concerning Oregon".<ref name=Indispensable>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19950525&id=4ERWAAAAIBAJ&sjid=7OoDAAAAIBAJ&pg=3777,5960904 |title= Blue Book Indispensable |newspaper= The Register-Guard |date= May 25, 1995 |accessdate= February 16, 2015}}</ref>

Early editions of the book were available free from the State.<ref name=Received/> By 1937, copies cost 25; in 1981 the book cost $4.<ref>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19370921&id=dMpYAAAAIBAJ&sjid=Q-gDAAAAIBAJ&pg=2452,1169200 |title= Oregon Blue Book Being Distributed |author= United Press International |date= September 21, 1937 |newspaper= The Register-Guard |accessdate= February 17, 2015}}</ref><ref>{{cite news |url= https://news.google.com/newspapers?nid=1243&dat=19810427&id=tl0zAAAAIBAJ&sjid=EPcDAAAAIBAJ&pg=3755,5317981 |title= Oregon Blue Book Makes Biennial Appearance |author= United Press International |date= April 27, 1981 |newspaper= The Bulletin |accessdate= February 17, 2015}}</ref>

In 1953, a legislative ways and means subcommittee, headed by Representative [[Francis Ziegler]], was going to confer with Secretary of State [[Earl T. Newbry]] about how to improve the ''Blue Book''.<ref name=Revision>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19530323&id=URZWAAAAIBAJ&sjid=veIDAAAAIBAJ&pg=5600,1181962 |title= Legislative Group to Study Revision of 'Blue Book' |date= March 23, 1953 |newspaper= The Register-Guard |author= United Press International |accessdate= February 16, 2015}}</ref> This was following complaints by Representative [[Monroe Sweetland]] that the book was "obsolete, carelessly edited, and only of limited use."<ref name=Revision/> Calling the book "an inferior job", Sweetland criticized the timing of book's publication long after elections, as well as the map in the back.<ref name=Revision/> As a result, the [[47th Oregon Legislative Assembly|1953 Legislative Assembly]] passed a law requiring the book be published soon after the legislature convenes.<ref>{{cite news |url= https://news.google.com/newspapers?nid=1310&dat=19550125&id=nPlVAAAAIBAJ&sjid=p-IDAAAAIBAJ&pg=6704,3150227 |title= Oregon Blue Book Printed But It Isn't Blue Any More |author= [[Associated Press]] |date= January 25, 1955 |newspaper= The Register-Guard |accessdate= February 17, 2015}}</ref>

The 1993–94 edition of the book contained a four-page [[errata]].<ref name=Indispensable/> When [[Norma Paulus]] was Secretary of State, she would send a free copy of the book to the first person to find a mistake in each new edition.<ref name=Indispensable/> The 1995–96 edition was reduced in size from its predecessors.<ref name=Indispensable/>

==Reviews==
A 1995 ''[[Register-Guard]]'' editorial called the book "indispensable".<ref name=Indispensable/>

==See also==
*''[[The Oregon Encyclopedia]]''

==References==
{{reflist}}

==External links==
*[http://bluebook.state.or.us/ Oregon Blue Book] (official website)

[[Category:1911 books]]
[[Category:Government of Oregon|Blue Book]]
[[Category:Directories]]
[[Category:1911 establishments in Oregon]]
<=====doc_Id=====>:452
<=====title=====>:
Vsya Rossiya
<=====text=====>:
{{italic title}}
'''''Vsya Rossiya''''' (literally translated "''All Russia''" or "''The whole Russia''") was the title of a series of directories of the [[Russian Empire]] published by [[Aleksei Sergeevich Suvorin]] on a yearly basis from 1895 to 1923 and was continued under the name '''''Ves SSSR''''' (Literally translated ''All of the USSR'' or ''The whole USSR'') from 1924 to 1931. Each volume was anywhere between 500 and 1500 pages long. The directories contained detailed lists of government offices, public services and medium and large businesses present in major cities across Russia including [[Kiev]], [[Minsk]], . These directories are often used by [[genealogists]] today to trace family members who were living in pre-revolutionary Russia and the early [[Soviet Union|Soviet]] period when [[vital records]] are missing or prove difficult to find. [[Historians]] use them to research the [[social histories]] of late 19th century and early 20th century Russia.

==Contents==

The following information can be found in most editions:
*a surname index of over 100,000 names and thousands of companies
*a directory of prominent landowners
*Lists members of the Imperial House of Russia and government officials 
*statistical information about the Russian Empire
*Population figures
*information and guidelines about trade and industry in Russia
*Lists of joint-stock companies
*Sub-sections detailing a directory of each district of each province, listing administrative officials, merchants, industrial and commercial manufacturers
*Original advertising

== Availability ==

Many original directories in the series (or [[microfiche]] copies thereof) can be found in libraries across the U.S., Europe (including the [[Baltic countries]], Finland the United Kingdom and Germany) however most only have an incomplete collection.

==Other city directories in Russia ==
Suvorin also published city directories for [[Saint Petersburg]] under the title ''[[Ves Petersburg]]'' (Literally translated ''All Petersburg'' or ''The Whole Saint Petersburg'') for the years 1894 to 1940 and for [[Moscow]] under the title ''[[Vsia Moskva]]'' (Literally translated ''All Moscow'' or ''The Whole Moscow'') for the years 1875 to 1936.

== External links ==

*[http://surname.litera-ru.ru/ A Russian website offering a search engine in Cyrillic for some city directories.]

[[Category:Directories]]
[[Category:Russian non-fiction books]]
[[Category:Russian Empire]]
[[Category:1895 books]]
<=====doc_Id=====>:455
<=====title=====>:
Category:Personal information managers
<=====text=====>:
{{Cat main|personal information manager}}

[[Category:Directories]]
[[Category:Planning]]
[[Category:Personal life|Information managers]]
[[Category:Application software]]
<=====doc_Id=====>:458
<=====title=====>:
Yellowikis
<=====text=====>:
{{Infobox Website
| name = Yellowikis
| favicon =
| logo = [[Image:Yelwiki.png]]
| screenshot =
| url = [http://yellowikis.wikia.com/wiki/Main_Page yellowikis.wikia.com]
| commercial =
| alexa =
| type = [[MediaWiki]]
| registration = Optional
| owner = [[Wikia]]
| author =
| launch date = 2005
| current status = Inactive
| revenue =
}}
'''Yellowikis''' was a [[MediaWiki]] [[website]] collecting basic information about businesses. This information included basic contact details such as company name, address, websites, and telephone numbers, as well as internal Yellowiki [[wikilink]]s to competitors. Yellowikis was launched in January 2005. {{As of|2011|3}}, the Yellowikis main page had been translated into more than 25 different languages.{{citation needed|date=April 2015}}

Some users also entered a number of codes including a two letter country code as well as an [[International Standard Industrial Classification]], [[North American Industry Classification System|North American Industry Classification]] or US [[Standard Industrial Classification]]. Some users are also adding [[geocode]]s and [[Skype]] ids.

==Legal issues==
A commercial business listing company, [[Yell Limited]], requested that the founders of Yellowikis, Paul Youlten and Rosa Blaus, amend their site, claiming that Yellowikis was "passing itself off" as being associated with Yell.com and that people would confuse the two organisations.<ref>{{cite news
 | title =Legal threat to wiki listing site
 | work =BBC News
 | date = 12 July 2006
 | url =http://news.bbc.co.uk/1/hi/technology/5169674.stm
 | accessdate =2006-07-12 }}</ref><ref>{{cite news|title=Teenager faces action over listings website|author=Bobbie Johnson|date=2006-08-02|work=[[The Guardian]]|url=https://www.theguardian.com/uk_news/story/0,,1835233,00.html|publisher=Guardian News and Media Ltd | location=London}}</ref><ref>{{cite news|title=Yell threatens to sue wiki rival|author=Jane Hoskyn|work=vnunet.com|date=2006-07-14|url=http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|publisher=VNU Business Publications Ltd|archiveurl=https://web.archive.org/web/20070930195439/http://www.vnunet.com/vnunet/news/2160380/yell-threatens-sue-wiki-rival|archivedate=2007-09-30}}</ref> This might be considered to be anti-competitive behaviour/anti-competitive in the eyes of certain commentators, however, such claim is unlikely to carry water from a legal perspective. Yell's claim is given considerable weight by the slogan on Yellowiki's front page that they are "Yellow Pages for the 21st Century" although in their public protestations, Yellowikis claim that they are not trying to create association between themselves and Yellow Pages.<ref>{{cite web
 | last =The Yellowikis Community
 | title =Response to Yell
 | work =
 | publisher =Yellowikis
 | year =2006
 | url =http://www.yellowikis.org/wiki/index.php/Response_to_Yell
 | accessdate =2006-07-14 }}</ref>

[[Yellow Pages]] is a registered [[trade mark]] in many countries including the UK. In some territories, however, the mark has lost its distinctiveness as a source of origin of goods and services.

From 9 to 14 October 2006, the domain address redirected to the new [http://www.owikis.org.uk/ Owikis] website, which stated "The trademark dispute between Yell Limited and Paul Youlten concerning the Yellowikis website has been satisfactorily resolved".

On 15 October 2006, the Yellowikis website reappeared, with the explanation that [[United Kingdom]] users would have to use Owikis, with the word ''Yell'' from the domain name and the color yellow from the logo; international users could continue to use Yellowikis. {{As of|2008|5}}, the Owikis site is not yet available.

As of at least May 2014, the [http://yellowikis.wikia.com/wiki/Main_Page Wikia page] is dead.<ref>{{cite web|url=http://yellowikis.wikia.com/wiki/Main_Page |title=Web Archive |accessdate=2014-05-28 |deadurl=yes |archiveurl=https://web.archive.org/web/20140109015130/http://yellowikis.wikia.com/wiki/Main_Page |archivedate=January 9, 2014 }}</ref>

==References==
{{Reflist}}

==Further reading==
* {{cite news|url=http://www.researchbuzz.org/2005/06/business_information_in_wiki_f.shtml|title=Business Information in Wiki Format|date=2005-06-22|publisher=ResearchBuzz}}
* {{cite news|url=http://competia.com/competia_w/site/fiche/1954|title=Yellowikis|date=2005-07-26|publisher=Competia}}
* {{cite web|url=http://alina_stefanescu.typepad.com/totalitarianism_today/2005/05/a_wiki_worth_wa.html|title=A wiki worth watching|work=totalitarianism today|accessdate=2005-10-07}}
* {{cite web|url=http://www.stabani.com/index.php?s=yellowikis|title=Why I think Yellowikis is a good idea |work=site spotlight|accessdate=2006-01-16|author=S.Tabani}}
* [[n:Emily Chang|Emily Chang]] {{cite web|url=http://www.emilychang.com/go/ehub/|title=Emily Chang's eHub|work=eHub|accessdate=2005-10-09}} 
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=58|title=Yellowikis - A Case Study of a Web 2.0 Business, Part 1|date=2005-10-15}}
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=59|title=Yellowikis: a Web 2.0 Case Study, Part 2 - Industry Disruption and The Competition|date=2005-10-16}}
* {{cite web|author=Richard MacManus|url=http://blogs.zdnet.com/web2explorer/?p=62|title=Yellowikis: Demonstrating Web 2.0 principles|date=2005-10-17}}
* {{cite news|url=http://news.independent.co.uk/media/article1096343.ece|title=New Media: Who are the real winners now we've all gone Wiki-crazy?|date=2006-06-26|publisher=[[The Independent]] | location=London | first=Kathy | last=Marks}}
* [[n:Yell threatens to shut down Yellowikis|Yell threatens to shut down Yellowikis]] from [[Wikinews]], 2006-07-05

==External links==
* [http://yellowikis.wikia.com/wiki/Main_Page Yellowikis] on [[Wikia]]

[[Category:Directories]]
[[Category:MediaWiki websites]]
[[Category:Internet properties established in 2005]]
[[Category:Yellow pages]]
<=====doc_Id=====>:461
<=====title=====>:
Thacker's Indian Directory
<=====text=====>:
{{italic title}}
{{Refimprove|date=September 2014}}
'''''Thacker, Spink & Co.''''' was a well-known [[Kolkata]] publishing company. ''Thacker's Bengal Directory'' was published from 1864 to 1884 and covered the [[Bengal Presidency]] – which included the present day [[Myanmar]] and [[Bangladesh]]. From 1885 the ''Directory'' covered the whole of [[British India]] and was renamed '''''Thacker's Indian Directory'''''.  It was later owned by [[Kameshwar Singh|Maharaja of Darbhanga]].<ref>{{cite book|title=Appendices|date=1982|publisher=India. Second Press Commissior Controller of Publications|pages=266, 343|url=https://books.google.com/books?id=tBwuAAAAMAAJ&q=darbhanga+Thacker+Spink&dq=darbhanga+Thacker+Spink&hl=en&sa=X&ei=Ul0sU5_BH46zrgfjsoCYBw&ved=0CEkQ6AEwBQ}}</ref>   It continued to be published until 1960.

The directory was essentially an [[almanac]] which listed British and Foreign Merchants and Manufacturers, Commercial Industries, Army, railway and government departments and office holders, European residents, and separately, prominent non-European residents.   Earlier editions of ''Thacker'' had street directories of major cities, such as Kolkata and [[Yangon]], together with the name of the residents of each house.

Similar directories published included:
*''Thacker's Bombay Directory'', city and island (together with a directory of the chief industries of Bombay, etc.);
*Thacker's medical directory of India, Burma and Ceylon;
*''Thacker's Directory of the Chief Industries of India, Burma and Ceylon''.

== References ==
<references />

[[Category:Almanacs]]
[[Category:Directories]]
[[Category:Books about British India]]
[[Category:Bengal Presidency]]
<=====doc_Id=====>:464
<=====title=====>:
Reverse telephone directory
<=====text=====>:
A '''reverse telephone directory''' (also known as a '''gray pages''' directory, criss-cross directory or '''reverse phone lookup''') is a collection of telephone numbers and associated customer details. However, unlike a standard [[telephone directory]], where the user uses customer's details (such as name and address) in order to retrieve the telephone number of that person or business, a reverse telephone directory allows users to search by a telephone service number in order to retrieve the customer details for that service.

Reverse telephone directories are used by law enforcement and other emergency services in order to determine the origin of any request for assistance, however these systems include both publicly accessible (listed) and private (unlisted) services. As such, these directories are restricted to internal use only. Some forms of [[city directory|city directories]] provide this form of lookup for listed services by phone number, along with address cross-referencing.

Publicly accessible reverse telephone directories may be provided as part of the standard directory services from the telecommunications carrier in some countries. In other countries these directories are often created by [[phreaking|phone phreaker]]s by collecting the information available via the publicly accessible directories and then providing a search function which allows users to search by the telephone service details.

==History==
Printed reverse phone directories have been produced by the telephone companies (in the United States) for decades, and were distributed to the phone companies, law enforcement, and [[public library|public libraries]].<ref>{{cite news | url=https://news.google.com/newspapers?nid=1454&dat=19720102&id=87osAAAAIBAJ&sjid=vgkEAAAAIBAJ&pg=3122,379459 | title=Clinton Directory Issued | date=Jan 2, 1972 | accessdate=9 February 2014 | location=Page 16}}</ref> In the early 1990s, businesses started offering reverse telephone lookups for fees, and by the early 2000s advertising-based reverse directories were available online, prompting occasional alarms about privacy concerns.

==Australia==
In 2001, a legal case ''[[Telstra|Telstra Corporation Ltd]] v Desktop Marketing Systems Pty Ltd'' was heard in the Australian Federal Court.<ref>{{cite web|url=http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html|title=Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd (2001) FCA 612 (25 May 2001)|author=[[Federal Court of Australia]]|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref><ref name=austliiPP>{{cite web|url=http://www.austlii.edu.au/au/journals/PLPR/2001/25.html|title=Private parts - PLPR 25; (2001) 8 PLPR 24|publisher=Australasian Legal Information Institute|accessdate=2008-01-03}}</ref> gave Telstra, the predominant carrier within Australia and the maintainer of the publicly accessible [[White Pages]] (residential) and [[Yellow Pages]] (commercial) directories, [[copyright]] over the content of these directories.

In February 2010 a Federal Court of Australia case ''[[Telstra|Telstra Corporation Ltd]] v Phone Directories Company Pty Ltd'' determined that Telstra does not hold copyright in the White Pages or the Yellow Pages.<ref>{{cite news|url=http://www.smh.com.au/business/copyright-to-enter-a-new-dimension-20101215-18y9o.html|title=Copyright to enter a new dimension|newspaper=[[The Sydney Morning Herald]]| first=Malcolm|last=Maiden|date=16 December 2010|accessdate=20 December 2012}}</ref>

As it currently{{when|date=October 2014}} stands there is no legal way to ensure a particular number is not listed in the directories currently available.

==United States==

In United States, landline phone subscribers can pay a small fee to exclude their number from the directory. This service is usually called "Your Listing Not Published" and the cost ranges between $0.80 and $1.50 for residential customers.

As [[cellular phones]] become more popular, there has been debate about releasing cell phone numbers into public [[4-1-1|411]] and reverse number directories. (S. 1963, the "Wireless 411 Privacy Act" 9/2004). However, opposition led by leading consumer-protection organization [[Consumers Union]] presented several privacy concerns in their congressional [http://www.consumersunion.org/pub/wireless%20411%20senate%20testimony%20final.pdf testimony]. Right now,{{when|date=October 2014}} cell phone numbers are not available in any public 411 or reverse-number directories. However, several information companies provide reverse cell phone lookups that are obtained from utility resources, and are available online. Because there is no central database of cell phone numbers, reverse phone directories that claim to be free cannot return information on those numbers.<ref>{{cite journal | title=Evaluating the utility and accuracy of a reverse telephone directory to identify the location of survey respondents. | work=2005 Feb |vauthors=Schootman M, Jeffe D, Kinman E, Higgs G, Jackson-Thompson J | pmid=15652722 | doi=10.1016/j.annepidem.2004.06.005 | volume=15 | pages=160-6}}</ref>

In recent years{{when|date=October 2014}} community web based services offer a reverse telephone directory of known telemarketers, debt collectors, fund raisers, and other solicitors which contact consumers by telephone.  Users of these services can perform a search of the telephone number which showed up on their caller ID and read through user comments to find the identity of the calling company or individual.

==United Kingdom==
In the United Kingdom proper, reverse directory information is not publicly available.<ref>{{cite web | url=http://ico.org.uk/for_organisations/privacy_and_electronic_communications/the_guide/directories_of_subscribers | title=Directories of subscribers | publisher=Information Commissioner's Office | accessdate=9 February 2014}}</ref> However, in the [[Channel Islands]] it is provided in the printed telephone directories.

Although the information is, of necessity, available to emergency services, for other agencies it is treated as 'communication data' in the [[RIPA]] regime and subject to the same controls as requests for lists of and content of calls.

==References==
{{reflist}}

==External links==
<!-- Do not delete these comments. -->
<!-- Do not put commercial links into this list. Doing so can get you blocked with no further warning. --> 
*[https://web.archive.org/web/20010721175437/http://blackpages.2600.org.au/ Wayback Machine (21 July 2001) archive of http://blackpages.2600.org.au]
*[http://www.austlii.edu.au/au/cases/cth/federal_ct/2001/612.html Federal Court of Australia Case 612 (25 May 2001): Telstra Corporation Limited v Desktop Marketing Systems Pty Ltd]

[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:Information retrieval systems]]
<=====doc_Id=====>:467
<=====title=====>:
Bulmer (directories)
<=====text=====>:
'''Bulmer''' was a [[Victorian era|Victorian]] [[historian]], [[surveying|surveyor]], [[cartographer]] and compiler of [[Yellow Pages|directories]]. His directories provided a history and geography of a particular area. The directories listed and described all parishes; listed trades and professions and provided a helpful street index with the names of residents, together with other local information. Data CDs of Bulmer Directories are available from publishers in the UK.<ref>[[S&N Genealogy Supplies|S&N Publishing]]</ref>

==List of directories==
* Bulmer's History, Topography and Directory of East Cumberland, 1883
* Bulmer's History, Topography and Directory of West Cumberland, 1884.
* Bulmer's History, Directory and Topography of Westmorland, 1885
* Bulmer's History, Topography and Directory of Northumberland (Hexham Division), 1886
* Bulmer's History, Topography and Directory of Northumberland (Tyneside, Wansbeck and Berwick Divisions), 1887
* Bulmer's History, and Directory of Newcastle upon Tyne, 1887
* Bulmer's History, and Directory of North Yorkshire, 1890 (two Volumes)
* Bulmer's History, Topography and Directory of East Yorkshire and Hull, 1892
* Bulmer's Directory of Cumberland, 1901
* T. Bulmer: History, Topography and Directory of Westmorland, 1906
* T Bulmer: History, Topography and Directory of Furness and Cartmel, 1911
* Bulmer's History, Topography and Directory of Furness, Cartmel and Egremont division of Cumbria, 1911
* T Bulmer: History, Topography and Directory of Lancaster and district, 1912
* J Bulmer: History, Topography and Directory of Lancaster and district, 1913

== References ==

{{reflist}}

{{DEFAULTSORT:Bulmer (Directories)}}
[[Category:Directories]]


{{UK-hist-stub}}
{{Ref-book-stub}}
<=====doc_Id=====>:470
<=====title=====>:
Pamyatnaya Knizhka
<=====text=====>:
{{unreferenced|date=April 2010}}
'''Pamyatnaya knizhka''' Memorial Book Памятная книжка is the title of official reference books of regions and towns in [[Russian empire]].

== History ==
The books were annually published by local authorities in 89 [[gubernia]]s and regions of [[Russian Empire]] starting from the mid-1830s till 1917. They provide information on population and businesses in the course of over 60 years. Over two thousand books have been found.

== Composition of Pamyatnaya knizhka ==
The books had some peculiarities in some [[gubernia]]s and not always comprised 4 main sections:
* [[address-calendar]] (index of all local official institutions and their staff),
* administrative reference book (information on administrative units in a [[gubernia]], post offices, roads, industrial and commercial enterprises, hospitals and chemists’, educational institutions, museums and libraries, book stores and print shops, periodicals, list of towns, major landowners etc.),
* statistic data (statistic tables on population, farming, education, incomes, fires etc.);
* historical background.

== Research project ==
The [[Russian National Library]] is carrying out a research project devoted to Pamyatnaya knizhka. The project is supervised by Mrs. Nadezhda Balatskaya Надежда Михайловна Балацкая in the department Bibliografia and krayevedeniye of the [[Russian National Library]].
The project comprises official memorial books Pamyatnay knizhkas of all gubernias and regions of [[Russia]], including areas that are no longer within the [[Russian Federation]].
The main result of the project will be the publication of 15 volume bibliographic index book titled “Памятные книжки губерний и областей Российской империи: Указатель содержания”. Some of materials in work are available at [http://givc.ru national computer center]. Рабочие материалы проекта представляют огромный интерес для людей, занимающихся генеалогией. That is an important source for genealogic researchers.

==External links==
* [http://www.nlr.ru/pro/inv/mem_buks.htm the site of the project of the Russia’s National Library «Памятные книжки губерний и областей Российской империи»]
* [http://chigirin.narod.ru/book.html#spravochniki some memorial books of Russian Empire in pdf at Russian national computer center]
* [http://russian-family.ru/index.php?option=com_jdownloads&task=viewcategory&catid=23&Itemid=27 Memorial books of some gubernias]
{{use dmy dates|date=December 2010}}

[[Category:Russian Empire]]
[[Category:Directories]]
[[Category:Genealogy publications]]
<=====doc_Id=====>:473
<=====title=====>:
Category:Directory assistance services
<=====text=====>:
[[Category:Directories]]
[[Category:Telephone services]]
[[Category:Telephone service enhanced features]]
[[Category:Information by telephone]]
<=====doc_Id=====>:476
<=====title=====>:
Pigot's Directory
<=====text=====>:
[[File:PigotDirectory1839Kent.jpg|thumb|An example page from Pigot's 1839 directory of Kent, Surrey and Sussex. This page has information on Bromley and Canterbury]]
'''Pigot's Directory''' was a major British [[Trade directory|directory]] started in 1814 by [[James Pigot]].<ref>{{cite web | url=http://www.hertfordshire-genealogy.co.uk/data/directories/directories-pigot.htm | title=Trade directories in Hertfordshire}}</ref>

Pigot's Directories covered England, Scotland, and Wales in the period before official Civil Registration began and are a valuable source of information regarding all major professions, nobility, gentry, clergy, trades and occupations including taverns and public houses and much more are listed. There are even timetables of the coaches and carriers that served a town.

Parishes are listed for each area with useful information including the number of inhabitants, a geographical description and the main trades and industries of the area or town.

{{TOC right}}

==List of Pigot’s Trade Directories by date==
* {{cite book |url=https://books.google.com/books?id=4llGAAAAYAAJ |title= Commercial Directory for 1818-19-20 |location=Manchester |publisher=James Pigot |year=1818 }}
* {{Citation |publication-place = London |publisher = J. Pigot & Co. |url =https://books.google.com/books?id=zBEHAAAAQAAJ |title = Pigot & Co.'s metropolitan guide & book of reference to every street, court, lane, passage, alley and public building, in the cities of London & Westminster, the borough of Southwark, and their respective suburbs |publication-date = 1824 }}
* {{cite book |url= https://books.google.com/books?id=hdMHAAAAQAAJ |title= Pigot & Co.'s National Commercial Directory for 1828-9 |location=London |publisher=James Pigot }}
*{{Citation |url = http://openlibrary.org/books/ia:pigotcosnational1837dire/Pigot_and_Co.'s_national_commercial_directory_for_the_whole_of_Scotland_and_of_the_Isle_of_Man_..._t |title = Pigot and Co.'s National Commercial Directory for the Whole of Scotland and of the Isle of Man, ... Manchester, Liverpool, Leeds, Hull, Birmingham, Sheffiled, Carlisle, and Newcastle-upon-Tyne |publication-date = 1837 |location = London |publisher =J. Pigot & Co. }}

==List of Pigot’s Trade Directories by geographic coverage==
*[[Bedfordshire]] 1839
*[[Cambridgeshire]] 1839
*[[Cambridgeshire]] 1830
*[[Derbyshire]] 1835
*[[Durham, England|Durham]] 1834
*[[Essex]] 1839
*[[Herefordshire]] 1835
*[[Hertfordshire]] 1839
*[[Huntingdonshire]] 1830
*[[Huntingdonshire]] 1839
*[[Kent]] 1839
*[[Leicestershire]] 1835
*[[Lincolnshire]] 1835
*[[London]] 1839
*[[Middlesex]] 1839
*[[Monmouthshire (historic)|Monmouthshire]] 1835
*[[Norfolk]] 1839
*[[North Wales]] 1835
*[[Northumberland]] 1828
*[[Northumberland]] 1834
*[[Nottinghamshire]] 1835
*[[Rutlandshire]] 1835
*[[Shropshire]] 1835
*[[South Wales]] 1835
*[[Staffordshire]] 1835
*[[Suffolk]] 1830
*[[Suffolk]] 1839
*[[Surrey]] 1839
*[[Sussex]] 1839
*[[Sussex]] 1840
*[[Warwickshire]] 1835
*[[Worcestershire]] 1835

==References==
{{reflist}}

==External links==
* {{citation |title=Historical Directories - England & Wales |publisher=[[University of Leicester]] |location=UK |url=http://specialcollections.le.ac.uk/cdm/landingpage/collection/p16445coll4}}. Includes digitized Pigot's & Slater's directories for England & Wales, various dates
* {{citation |title=Historical Directories - Scotland |publisher=[[National Library of Scotland]] |location=UK |url=http://www.nls.uk/family-history/directories/post-office/index.cfm?place=Scotland }}. Includes digitized Pigot's & Slater's directories for Scotland, various dates

[[Category:Directories]]


{{ref-book-stub}}
<=====doc_Id=====>:479
<=====title=====>:
Army List
<=====text=====>:
The '''''Army List''''' is a list (or more accurately seven series of lists) of serving regular, [[militia]] or territorial [[British Army]] officers, kept in one form or another, since 1702.

Manuscript lists of army officers were kept from 1702–1752, the first official list being published in 1740.

==Regular army==
* Annual Army Lists, 1754–1879 (WO 65)
* Quarterly Army Lists (First Series), 1879–1922
* Half-Yearly Army Lists, 1923 - Feb 1950 (From 1947, annual, despite the name)
* Modern Army Lists, 1951-Ongoing
** Part 1; serving officers.
** Part 2; retired officers, as of 2011 four-yearly
** Part 3; the Gradation List, a short biography of officers, a restricted publication  not generally available.

==Other lists==
* Monthly Army Lists, 1798-June 1940. Officers of colonial, militia and territorial units are included.
* Quarterly Army Lists (Second Series), July 1940-December 1950. These superseded the Monthly Army Lists, and, for the remainder of [[World War II]] were not published but  produced as confidential documents, monthly or bi-monthly until December 1943 and quarterly until April 1947, then three times a year, April, August and December.
* [[British Home Guard|Home Guard]] List, 1939–1945
* Militia Lists - various militia lists pertaining to the eighteenth and nineteenth centuries are extant.
* ''[[Hart's Army List]]'', an unofficial list, produced between 1839 and 1915, containing details of war service which the official lists started covering only in 1881.

==Further reading and bibliography==
* ''The army lists of the [[Roundheads and Cavaliers]]: containing the names of the officers in the Royal and Parliamentary armies of 1642'', [[Edward Peacock (antiquary)|Edward Peacock]] (ed) (1874)
* ''English army lists and commission registers, 1661–1714'', [[Charles Dalton]] (ed) (1892–1904)
* [[Henry George Hart]], ''Hart's army list : the new army list exhibiting the rank, standing, and various services of every officer in the Army on full pay'' (1839-)
* William Spencer, ''Army service records of the First World War'' (seventh edition, 2006)

==See also==
* [[Navy List]]
* [[Crockford's Clerical Directory]]
*

==References==
{{Reflist}}

==External links==
* [https://books.google.com/books?id=p_BfsBzDzWYC The 1740 Army List] at [[google books]]
* [http://discovery.nationalarchives.gov.uk/SearchUI/Collection/Display?iaid=C14273&parentiaid=C543 War Office: Printed Annual Army Lists 1754-1879 (WO 65) - download for free]
* Digitised copies of 'Quarterly army lists' from [http://digital.nls.uk/97136046 1913 to 1919] and from [http://digital.nls.uk/97136048 1940 to 1946] at [[National Library of Scotland]]
* Digitised copies of 'Half-yearly army lists' from [http://digital.nls.uk/97136047 1938 to 1941] at National Library of Scotland
* [http://www.nationalarchives.gov.uk/records/research-guides/british-army-lists.htm British Army Lists] ([[National Archives]]' Research Guide)
* [https://archive.org/details/nlsarmylists Hart's Army List] at the [[Internet Archive]]

[[Category:Directories]]
[[Category:British Army]]
<=====doc_Id=====>:482
<=====title=====>:
Corporate Technology Directory
<=====text=====>:
The '''Corporate Technology Directory''' also known as the '''CorpTech directory of technology companies''' was a [[Trade directory|directory]] of technology companies published from 1986<!-- maybe 1987 -->-2004 by [[CorpTech]]. It listed thousands of technology companies including software, services, and hardware as well as developers.

The directory was later made available in digital form as a cd<ref name="SterlingBracken1998">{{cite book|last1=Sterling|first1=Christopher H.|last2=Bracken|first2=James K.|last3=Hill|first3=Susan M.|title=Mass communications research resources: an annotated guide|url=https://books.google.com/books?id=kwOo6BiWiFkC&pg=PA10|accessdate=27 September 2011|year=1998|publisher=Psychology Press|isbn=978-0-8058-2024-9|pages=10–}}</ref> and subsequently database subscription.

==See also==
* [[Major Information Technology Companies of the World]]

==References==
{{reflist}}
<!-- https://books.google.com/books?id=96XpVBf6pvAC&pg=PA246 -->

{{technology-stub}}

[[Category:Directories]]
<=====doc_Id=====>:485
<=====title=====>:
Royal Almanac
<=====text=====>:
{{unreferenced|date=May 2011}}

The '''Royal Almanac''' is a French administrative directory founded in 1683 by the [[bookseller]] Laurent d'Houry, which appeared under this title from 1700 to 1792, and under other titles until 1919.

He presented each year in the official [[order of precedence]], the list of members of the [[royal family]] of [[France]], the princes of blood, and the main body of the kingdom, great crown officers, senior clerics, abbots of large abbeys (with income of each abbey), marshals of France, colonels and general officers, ambassadors and consuls of France, presidents of the main courts, state councilors, bankers, etc..

Despite the fact that he could present indigestible because of the many lists that he was composed, he enjoyed a wide circulation with a readership consisting primarily of financial, political and all persons who had an interest in knowing the administrative organization of France.

Although his edition is due to the initiative of a private publisher, included in the lists of the Almanac was a royal official and abuse were therefore punished. Thus, a [[Poitou|Poitevin]], Pierre Joly, was interned in the [[Bastille]] at the end of the eighteenth century to have usurped the banking profession by being registered as such in the Almanach Royal.

His edition was in regular format in-8 o editor with a binder leather adorned with a sprinkling of [[fleur de lys]] gold.

==History==
===Founded at the request of King===
Laurent d'Houry imagines a calendar or [[Almanac]] 1683. The first edition contained only a few pages with a calendar and omens for the coming year. The last edition in this form, in 1699, already shows some lists that foreshadow the upcoming Royal Almanac. Thus we find lists of counselors of state with their ordinary homes, the commissioners of the Board, auditors general and stewards of finances, the Chancellor, archbishops and bishops of France, universities, and the list of major exhibitions, sessions of the courts of [[Paris]] and the log of the Palace, and finally addresses the messengers and items indicating the day of departure.

In 1699, [[Louis XIV]] asked him what the author describes in detail his work. Here as his widow explains these beginnings :

:"Louis XIV, who wanted this glorious memory Almanac, made him ask the author, who had the honor to present to Her Majesty's what induced him to give the title of Royal Almanac, & to make it his principal occupation of this work. "

The same year [[Louis Tribouillet]], chaplain of the king and canon of [[Meaux]], publishes its State of France. This book describes in detail the functioning of the Court of the King, all his ministers, the treatments they receive, the various expenses of the state, clergy, etc..

The first edition of the Almanac Royal appears in 1700, at the same time as another book, Calendar of the Court of John Colombat, one of the printers of the King. Parisians have a choice between three books with similar content: the Almanach Royal Houry, Calendar of the Court of Colombat and the State of France Tribouillet. At that time, the yard around Louis XIV is highly stratified and since the expansion of the [[Palace of Versailles]] in 1684, it continues to grow. In this context, recognition of peers is a valuable asset "if someone has just placed a new post is an overflow of praise in its favor during the floods and the Chapel (...) but c is that while envy and jealousy talk like adulation.. One can understand the need to maintain directories so that everyone can follow the evolution of all these people. The multiplicity of these publications so says the king's will want to "officially" referencing his [[courtier]]s to charges created to keep beside him at [[Palace of Versailles|Versailles]], and maintain the jealousies of each other.

Even if the king gave his approval, publishing such a book is not without risk. In December 1708, Laurent d'Houry is being prosecuted for having established a [[printing press]] in his house and forced to sell its equipment two months later. Then, in February 1716, he was imprisoned in the Bastille on complaint of the [[Earl of Stair]], the British Ambassador, "to be disrespectful in his almanac, the King George by not naming him not as king of England, or rather from Great Britain, and mention the king as son of Jacques II Stuart, exiled to St. Germain.

===Affirmation of a monopoly===
The Almanac and the Royal Court Calendar coexist peacefully for ten years and a lot of money to their authors, but from 1710, Laurent d'Houry integrates more and more topics like the book Colombat Biblio. The abscess broke out in 1717 when Houry Almanac releases its Abstract that will follow the format of the Calendar of the Court and simultaneously filing a [[lawsuit]] against its competitor. A Judgement of Solomon is made: if it is allowed to Houry now to continue the publication of its abstract and to counterfeit the Court Calendar, Colombat is obliged to freeze its calendar format and forbidden 'expand content. This stops any changes in the calendar of the Court, leaving the way open to the Almanac.

Upon the death of Laurent Houry in 1725, his family is [[destitute]]. Revenues from sales of the Almanac are not sufficient to cover expenses of printing and bookselling. In these circumstances, his widow, Elizabeth Dubois, took over the business. Their son Charles-Maurice, who had hitherto been a mere [[proofreader]] of the Almanac, is trying to evict her mother and she is suing cons. It prepares the edition of 1726 but a ruling forbade him to publish it in his name alone. The ruling of 11 December 1726 forbids even disturb the affairs of his mother and to participate directly or indirectly to the development of the Almanac. That is why Charles-Maurice is mentioned as editor of the Almanac on the edition of 1726.

In 1731, she filed a new lawsuit against Colombat which increased its schedule despite the prohibition of 1717. Unsuccessful, she resumed the publication of the Abstract and Colombat complaint in turn, she then accepts to abandon the publication of the Abstract "if returned to Colombat format from 1718. "The disputes have become extinct with the death of the parties.

The privilege of the Almanac is about to expire, Charles-Maurice d'Houry tries one last time to seize it, but a royal letter of 27 March 1744 confirmed definitively André-François Le Breton as sole heir.

==A family of hegemony 131 years==
===Directorate of André Le Breton===
In 1728, the widow of his grandson, Houry combines son André-François Le Breton, who was 18 years old and an orphan under the [[guardianship]] of Charles-Maurice d'Houry. Andrew Francis had inherited, according to the will of the estate of Lawrence Houry, half of the rights of the Almanac, and his widow, the rest.

Under his leadership, the Almanac takes a new breath and adds new sections, which sometimes does not go without punishment. For example, in 1768, he has trouble with [[Voltaire]], who sent him a letter incendiary:

:"I say as much to Le Breton, the Almanach royal printer: I'll pay him Almanac point that sold me this year. He had the rudeness to say that Mr. President... Mr. advisor..., remains in the cul-de-sac to Menard, in the cul-de-sac of the White Coats in the ass -de-sac de l'Orangerie. (...) How can you say that president remains in a serious ass?"

In 1773, Le Breton moved his print shop in a wing of the former Hotel d'Alegre, at 13 rue de Hautefeuille, he acquired William Louis Joly de Fleury and was previously occupied by Ambassador Portugal.

In the late eighteenth century, the weather is bad and bad [[wheat]] harvests. The price of this staple increases disproportionately. Recently, a rumor that the government would have the [[monopoly]] wheat, thus perpetuating the high cost of food. This rumor became official when accidentally in his edition of 1774, Le Breton added a "Treasurer's grain account of the King" in the person of Sr. Mirlavaud. The edition of the Almanac had yet been proofread and approved by the Chancellor, but was still sentenced to close his shop for 3 months and publish a revised edition, without the line in question,.

In 1777, Le Breton was again accused of inserting information deemed subversive. It has, according to its critics, cited "the Floral & Pranks Vergès & Vaucresson, among the prosecutors and attorneys general of the [[Parliament of Paris]], who had been involved in a reform of parliament made by Maupeou against the venality of Parliament desired by [[Louis XV]], but annulled by [[Louis XVI]]. In rebuke, Le Breton was sentenced to " carton "section on the Almanacs that had not yet been sold, and replacement cost, the Almanac issue of those who so request it."

He died on 4 October 1779 and his cousin, Charles-Laurent Houry, son of Charles-Maurice d'Houry, took over the business.

===French Revolution===
The privilege granted to the family of Houry for the Almanac has been threatened in 1789 when [[Camille Desmoulins]], in his speech at the Lantern to the Parisians, says it will cease in favor of Baldwin, another Parisian publisher. This threat has not been brought into effect since the Almanac remained in Houry. Looking at the publications of the time, we can nevertheless see that Baldwin got the impressions of the [[National Assembly (French Revolution)|National Assembly]] and other organs of state.

===Last generation Houry===
Following the death of Joan Nera, widow of Laurent-Charles Houry, the Almanac is echoed by Jean-François-Noël Debure, husband of Anne-Charlotte d'Houry, their daughter. Debure is from a prominent and wealthy family of Parisian booksellers, especially combined with the Didot family. It is a printer since 1784 with the title's printer [[Duke of Orleans]].

Debure takes time printing of the family of Houry, but his other business is in trouble and he is forced to file for [[bankruptcy]]. To keep the property inherited from his family, Anne-Charlotte d'Houry hires a separation procedure. In November 1791, the bankruptcy is declared and it is opposed to the [[creditor]]s to preserve his legacy, and this opposition is futile and a ruling allows creditors to seize his furniture, but that does not appear to have been necessary because a subsequent decision allows him to recover property that creditors have not taken her husband.

Francois-Jean-Noel Debure dies 1802 in [[Loiret]]. However, it is focused died from 1795 through various sources. Maybe it was just left without leaving an address.

Stephen Lawrence Testu worked as a clerk in the family home Debure since 1788, and had gradually won the confidence of the household. Because of the absence of Mr. Debure, Anne-Charlotte is alone with his two son. Despite their age difference, he is 20 years younger than she, she married Testu July 1795. Testu few highlights its knowledge in the profession to convince him to transfer the management of printing. It accepts in 1797 and offered him priority over the rights of the Almanac in exchange for a perpetual [[Annuity (European financial arrangements)|annuity]] of 800 francs, then she completely abandons the Almanac. This influx of money seems to turn the head Testu who play games and learn to enjoy the easy life, neglects the direction of its establishment and constantly running out of money, he contracted many loans that gradually ruining his business. Relations were strained with his wife because he left the marital home in September 1801 and the only ties that bind the couple are now linked to multiple trials they s'intentent.

In 1810, Testu secretly sold the rights of the Almanac in which he partnered with Guyot. Anne-Charlotte d'Houry opposes this sale she saw as a usurpation, but loses the case in 1812. She gets in return a pension of 1 200 francs Testu does not pay. Indeed, a decree of 1820 declares the debtor more than 90 000 francs... In 1814, due to the large sums invested by Guyot in the case, an order confirming the owner of the Almanac, a copy of this order is also printed at the end of the following books. Testu still gets Guyot repayment of its debts and an annuity of 2,400 francs.

Guyot dismisses Testu business in 1820 and continues even to pay his annuity. The latter, again running out of money turned in 1823 against his wife, calls it reaches the marital home and she pays all household expenses, or alternatively, that she pays rent of 6000 francs. Judges déboutent Testu the marital home, since he himself had deserted 22 years earlier and has no housing to offer his wife even though she already lives in a very beautiful but still require his wife, yet very rich, to pay him a pension of 1,800 francs by invoking the solidarity between spouses.

The [[hegemony]] of the family of Houry on the Almanac established in 1683 has finally ceased in 1814 when, by order, the company is transferred to the association Guyot-Testu. Anne-Charlotte d'Houry died 22 July 1828 aged about 83 years.

===Judgement of publishing===
In 1867, edition of the Almanac is transferred to the widow Berger-Levrault, who had already published the Yearbook of the French empire diplomatic, and military Yearbook of the French empire, both published as the Almanac according to documents provided by the administration.

The edition of the Almanac stops definitively in 1919 after four years of interruption due to war, the latter number includes the years 1915 to 1919. Not found in the literature the reasons that prompted the shutdown of publication, but it can be assumed that the combination of very large volume of the book (more than 1650 pages in 1900) and the hard times that the [[Economics]] and French policy at that time was to make the management of such a volume of information extremely complex and unprofitable for the publisher. It is also possible that the new government formed after elections in 1919 no longer supported the development of the Almanac.

==Changing content==
===The topics in the Almanac===
The Almanac or Calendar, as he was called in its early editions, was just a simple calendar which were associated topics on astronomical events, the days of fairs, the newspaper of the Palace, the residence of messengers The departure of the mails, the price of currencies and the list of collectors' offices. After his presentation to the king in 1699, many items are constantly being added including the clergy, the royal family of France, then the families of other sovereign nations, officers, ambassadors, etc..

In 1705, Houry added to the list of knights of the Holy Spirit and peer and marshal of France. In 1707, it is the state of the clergy and, in 1712, the birth of kings, princes and princesses of Europe. After the death of Louis XIV, the Duke of Orleans, became the Regent, is added to the list of members of the royal family of the members of the [[House of Orleans]]. Later, he put more of his own, the full house of the Queen and princes.

It is not possible to describe all items contained in an almanac as there, so the contents of 1780 covers ten pages:

The Almanac also stands abreast of scientific advances. In the middle of the eighteenth century, improving the accuracy of clocks and many wealthy fans begin to observe and study the stars. It is indispensable to know precisely the difference between true solar time of [[sundial]]s, and mean solar time clocks, especially since the advent of clocks seconds. This is the equation of the pendulum, also called the equation of time, the table is added shortly before 1750.

With the [[French Revolution]], the Almanac exchange of title and its content is modified to match the new institutions.

The abolition of all distinctions requires overhaul the topics, timing of the vulgar era is replaced by the [[French Republican Calendar|Republican calendar]], the place reserved for kings and princes of Europe is replaced by a note on the friendly powers of France, the administrative organs of royal power are replaced by new ministries.

The content changes again with the reforms of the Consulate and the Empire, the Restoration, the [[Hundred Days]], the [[July Monarchy]], the [[French Second Republic|Second Republic]] followed the [[Second French Empire|Second Empire]] and the [[French Third Republic|Third Republic]] who sees the end of the edition of Almanac. In each case, the bindings are supplied with the times.

As the number of entries is increasing, the number of pages follows the same trend: they numbered one hundred in 1699, nearly five-hundred in 1760, and seven-hundred just before the French Revolution. The course of a thousand pages is taken in 1840 to over 1000-6-cents in 1900. On average, about thirty names are listed per page, the total number of people or places listed annually in the tens of thousands, but no table patronymic does quickly find a particular name.

All changes in the Almanac makes it a very useful book for historians that may follow, year after year, ministries and other administrative bodies, movements of people in these offices, and retail organization public services to a resident of Paris (such as places of mailboxes, timetables and fares for ticks and royal messengers ).

===Chronology of the 237 years of publishing Almanacs===
After the death of Laurent Houry, his descendants continued his work until 1814. The edition was continued until 1919. It would be tedious to describe in words the evolution of the Almanac of the 237 years that have elapsed since the first edition by Laurent d'Houry in 1683, hence the choice of this table layout.

Throughout its existence, the Almanac has crossed 11 schemes political editor changed the title 14 times and 9 times.

==Publication==
===Collecting information===
Since its inception in 1700, following a royal demand, the Almanac invented by Laurent d'Houry aims to be an official handbook.

Until the French Revolution, contributors are cordially invited to provide information to the bookseller, as pointed note of the printer in the first pages of the Almanac. In 1771, for example, we read in the Journal History of the Revolution that the Bar Association in the person of a certain Gerbier, asserted that "there would be no change in the order of the table, and that it would be printed in the Almanac as Royal was last year, leaving out only the dead."

With the French Revolution, the order was given to government to provide all information to the publisher. In 1802, Testu gets even exclusivity.

Later, the collection of information for the Almanac is even part of the operating budget of the ministries and can be seen in order of December 31, 1844 signed by [[Louis-Philippe I]] "on the organization of the Ministry Administration Navy "Article 6 of which list the items in the budget" the formation of the Royal Almanac.

====Typography====
The print quality improves significantly when Laurent d'Houry became printer. It multiplies the bands and tail-lights to decorate and titles for sections. The Almanac is still very poor prints because the image is not its goal. Only that the reader can find are patterns explaining the oppositions of the planets and eclipses are present every year, and the map of [[departments of France]] editions of 1791 and 1792.

Despite the short time to prepare the book, the printer treats the presentation and uses in the case of many variations in size and shape of characters for easy reading of long lists, special characters to emphasize certain lines, compositions tables or columns and clusters in braces.

That Le Breton, grand son of Laurent Houry, who brings more to the book. It increases greatly and restructures the Almanac, and also improves its presentation in order to preserve readability. Many notes are added to guide the reader and help in understanding the operation of certain administrative bodies.

The Almanacs modern nineteenth-century advantage of technological advances. Cartoon characters are modernized and the use of fonts to graphics customizable multiplies, sometimes to excess: we can count at least 7 in 11 fonts fonts differ on the cover page of the National Almanac 1850 printed by Guyot et Scribe!

Announcements, the ancestors of [[advertising]], are introduced by the publisher Berger-Levrault in the late nineteenth century.

====Well-to-shoot====
The deadline for submitting this information to the editor is set to "first ten days of October (or November). The last-minute changes are incorporated in an erratum end of the book. When they are too large, they may even delay the release. In late December, an event is sent to the administration for approving the content. This approval is required before the sale.

It leaves only two months to integrate the information of the year in the text of the previous edition and call all of the pages before submitting the book for the right to shoot. The editing step, at least for the test in 1706, has not been done with great care as can be seen by very many shells and mistakes which have crept into the [[table of contents]] presented in thumbnail to the right.

Once the administrative agreement obtained, it is inserted end of the book, the Almanac is stapled or bound and is then distributed to customers at the end of the year.

===Printing===
Early Almanacs were not printed by Laurent d'Houry. The Almanac of 1706 and is printed by Jacques Vincent, installed Huchette street, at the sign of the Angel. November 15, 1712, Laurent Houry became printer and immediately began printing his work. Then all the almanacs will be printed by their publishers.

====Draw====
There is no source that explains the draw of the Almanac. The only figures available are the annual rents generated by sales.

In 1782, Mercier said a pension of more than 40 000 francs. Diderot, at the same time, puts the figure at 30,000 pounds. For a price of sale of 5 to 6 pounds, the draw must necessarily be greater than about 7500 almanacs.

Around 1820, during the trials that have brought the widow and Debure Testu, income of the Almanac was estimated between 25 and 30 000 francs. In 1834, another almanac, the Almanac of France, said that its cost is 35 cents for a sale price of 50 cents. Booksellers then purchase the item at prices of 38 cents, to resell a suggested retail price of 50 cents. The publisher earns so 3 cents per pound sold, the bookseller earns 12 (minus shipping costs, dependent). If one considers - arbitrarily - a four Almanac is sold directly into the library Testu (priced at 10 francs 50) the remainder being passed through intermediaries, we can prorate that to generate an annual income of 30 000 francs Testu must sell approximately 25 000.

In the absence of more precise information, we can only estimate at about 15,000, the number of copies sold per year between the late eighteenth and early nineteenth century.

====Binding====
The almanac is sold either stapled or bound by the printer. The [[paperback]] version allows the purchaser to connect his book as he wishes, and so it is possible to find books with bindings very ornate, with lace, weapons of families, many colors brightened or gilding Biblio 24, etc..

The bound version provided by the printer is usually presented in a bound in calf or Morocco,{{clarify|date=August 2012}} full, and lilies in the boxes back. With the revolution, the lilies are replaced with Phrygian caps in roundels Library 25.

====Distribution====
The Almanac is normally available from the bookseller, but it can also be found in the province in other bookstores that serve as intermediaries, for example in 1816, at Pesche, bookseller at Le Mans Ref 17, or by correspondence through the [[Sorbonne]], as did Voltaire Ref 18.

===Readership===
The Almanac has a very great interest because of the number of subjects it addresses the organization of the French administration. In 1785, Mairobert wrote that "the Almanach Royal is in the hands of everyone and is among the Princes, the King's office, the foreign ministers would cater Ref 19. Louis-Sebastien Mercier in a pamphlet, the Tableau de Paris that stands in 1782 Ref 20 explains that "Those who are thrown into the paths of ambition, study Almanac Royal with serious attention," "more a beautiful royal consult the almanac to see if her lover is a lieutenant or sergeant,... ", that" everyone is buying the almanac to find out exactly where they stand. "And finally" even Fontenelle said, that it was the book which contained the greatest truths."

Adages use the almanac as a reference. According to Jean-François de La Harpe is "the only book to read to get rich is the Royal Almanac Ref 21, [[Jean-Joseph Regnault-Warin]] uses the phrase" having the memory of a Royal Almanac Ref 22 " or the Memoirs of the Academy of hawkers Ref 23 explains that "it is enough to read the Almanac for education."

In the eyes of [[justice]], the book can be used as a basis for comparison: during a police investigation in 1824 Ref 24, a defendant defends himself by explaining that the volume of documents he was accused of having carried "could be equal to that of a royal Almanac almanac or a related trade."

Whether to have a certain level of resources to purchase this item, the customer extended beyond the financial and political world.

==Competition==
The Royal Almanac is competing at its inception with the Almanac of the Court of Colombat who can not make it evolve since 1717. In fact, bibliographers consider that the Royal Almanac is one of the "oldest and most helpful Ref 25". If it essentially describes the royal court and the Parisian institutions, other major cities also have their almanacs, such as the city of Lyon equally voluminous Ref 26. The Almanac is however considered a reference book. In 1780, a notice of a bookseller named Desnos inserted at the end of the Gazette of 27 offers for courts Ref 8 pounds to "the statesman, letters, and generally all persons attached to the service of the King (... ) Almanach Royal, Calendar of the Court, said Colombat, Mignone Strennas-Note 22, Ref 28, the State Military Note 23, the four connected units, with shelves & stylus to write, which makes the closure ": the Royal Almanac ranks first in the collection.

===The Court Calendar===
Since 1717, the Calendar of the Court can not change, its sections are limited to an ephemeris of the celestial motions (30 years) increased by astronomical tables with sky conditions, and timing of the court to the family and royal house, lists of boards, departments and secretaries of state finances, births and deaths of kings, queens, princes and princesses of Europe, the knights of various orders, the archbishops and bishops of the kingdom and Cardinals of the Sacred College.

It is primarily sought for its ephemeris of the celestial motions and astronomical tables of events

===The Almanac of Commerce===
The Almanac of Business, published by Sébastien Bottin in the eighteenth century contains, besides the addresses of shops in Paris, many useful statistics financiers. It is supplementary to the Almanach royal, which concerns only the French administration.

===The State of France===
Some have criticized the Almanach Royal of being a [[plagiarism]] of the State of France, another administrative directory, the first publication seems to have been made in 1619 and is still published in the middle of the eighteenth century Ref 30. However, the edition of 1736 of the State of France said it was a "periodical whose audience has applied for renewal from time to time, and had been published until 1699, 1702, 1708, 1712, 1718 and 1727 Ref 31. The latest editions of 1727 and 1736 five volumes contain over 500 pages each. Offices are described down to the smallest detail Note 25, the state of France is a companion volume of the Almanach royal use by those who wish to deepen their knowledge on the functioning of the French administration.

==Examples of information contained in the Almanac==
Further details concerning the organization of the administration of the French state, and persons who occupied positions, many other topics are discussed in the Almanacs, for example in the eighteenth century:

===The cost of construction in Paris===
This section is only found in the Almanacs of the early eighteenth century, and stops just before 1726.

There are prices for masonry, carpentry and joinery, roofing, locksmithing, painting and glazing that are usually in Paris, for example:

:"Walls of circular pits, with layers of stone studded with low excess moilon quilted 18 inch thick, 22 pounds fquare fathom, and more in proportion to the depth of the wells, or other difficulties that may encounter."

With these data, the historian is able to quantify fully the construction of a building in Paris at that time.

===The official ceremonies===
The Almanac explains in great detail some official ceremonies:
* Opening Ceremony of the Annual Courthouse
:"The Entry of Parliament is the day after the S. Martin, 12 November, which day Presidents in red dresses holding their furs & Mortar Note 26, & Gowns Gentlemen Consultants red, after attending the solemn Mass that are usually said by a Bishop in the grand hall of the Palace, receive oaths of Lawyers & Counsel. The first president made this day a speech to thank those who celebrated the Mass, which responds to him by another harangue Note 27."
* Procession of the University, whose description takes three pages of the Almanac Note 28
:"The Rector of the University at the end of its Rector, who regularly is only three months, indicating a general procession which assists the whole body. It is a ceremony that deserves to be seen. We will mark the place here What the doctors take the four faculties Note 29 that comprise the university, all the graduates of these faculties, with the Religious Orders Note 30. Procession from the Church of Religious Trinitarians, otherwise known as Maturin. (...) The procession is closed by the booksellers, papermakers, bookbinders, Parcherminiers, illuminators, writers swear by the University."

The detailed description of the ceremonies to stop mid-eighteenth century to make room for a still more comprehensive directory. A reference is then made towards the end of the book "guides for all kinds of ceremonies to be observed in the receipt of any office or employment whether in dress or in the Sword."

===Transportation===
Transportation of persons is ensured by the coaches, carriages, wagons and other carriages. Found in the Almanac schedules and rates of major roads.

In 1715, a passenger wishing to travel from Paris to [[Caen]] will go rue Saint-Denis on Monday at six o'clock in the morning. He has previously "sent his clothes the night before early." Fifteen years later, the starting time is advanced to 5 am in summer and in 1750, the departure is 5 hours throughout the year. In 1780, two flights are scheduled Tuesdays and Fridays at 23:30 and the journey takes two days. A van, slower, except Sunday at noon and made the trip in four and a half days in summer and five days in winter. In 1790, transportation is now provided by the General Department of stagecoaches and mail royal France. Three coaches liaise on Tuesday, Thursday and Sunday and the van on Sunday. The departure is now Notre-Dame-des-Victoires.

Rates are rarely reported but in 1725 and 1761 is 18 pounds per person tournaments. He is 21 pounds in 1770 to reach 42 pounds in 1790 (fortunately for the traveler, it is stated that the "sleeping bag weighing 10 pounds is" free").

===Company guards of the King Pumps===
In 1716, the king appoints François Perrier Dumouriez as Director General of public pump to remedy fire, without the audience is obliged to pay anything. In 1722, he founded the Compagnie des Gardes Pumps du Roy, under the direction of the same. This company later became the Brigade of firefighters in Paris Note 32.

The Almanac of 1719 lists these pumps and their wardens and deputy wardens. We then learned that a brigade is made up of four guards and four sub-custodians who are responsible for maintaining the material deposited in each district. What became three years later the Society of King's Guards Pumps were not at that time that 41 people, 17 pumps distributed in groups of 8 men and 4 or 3 pumps in the City Hall, the convent of the Grands Augustins The Carmelite convent in the Place Maubert, Convent of Mercy, and the Fathers of Little Place des Victoires, in addition to a pump at the Director General of the pumps, Rue Mazarine. Except Dumouriez guards pumps are not professional fire but shoemakers, carpenters, locksmiths, etc..

==Considerations bibliophiles==
===Availability===
Almanacs are found regularly in auctions and in the antiquarian booksellers. Given their importance documentary and the fact that there are beautiful copies, these books are particularly sought after by historians, writers, bibliophiles and enthusiasts.

Volumes in the first round of the seventeenth century often exceed several thousand euros Ref 32, the other is generally negotiated between a few tens and five hundred euros, sometimes more, depending on their rarity, condition and quality bookbinding Note 33. Just over half are however available for free download on Gallica or Google Books:

===Notes handwritten readers===
Almanacs contain some handwritten notes left by their readers. The value of the book can then be influenced upward or downward depending on the quality and content of these notes, and especially the person who wrote them - when you can identify it. They are usually found on page intentionally left blank for the ephemeris. Some of these notes can provide very interesting information, such as notes written on the page in August 1715 a copy of the BNF. It relates the circumstances of the death of Louis XIV, who was suffering from gangrene Note 34:

:"We thought the death dez Roy Lundy 25. He marched
:better a day or two quoyque hopeless. It
:died after having suffered much and with great
:Patience on Sunday September 1, r t is 8 pm Morning
:M r le Duc d'Orleans went to Parl t and was declared
:Regent on 2. September e"

==References==
{{Reflist}}

[[Category:Historiography of France]]
[[Category:18th-century books]]
[[Category:Directories]]
<=====doc_Id=====>:488
<=====title=====>:
Mobile social address book
<=====text=====>:

A '''mobile social address book''' is a [[phonebook]] on a [[mobile device]] that enables [[subscribers]] to build and grow their [[social networks]]. The mobile social address book transforms the phone book on any standard mobile phone into a social networking platform that makes it easier for subscribers to exchange contact information.<ref>[http://www.wirelessweek.com/article.aspx?id=163626 Wireless Week, retrieved 2008-12-29]</ref> The mobile social address book is the convergence of [[personal information management]] (PIM) and social networking on a mobile device. While standard mobile phonebooks force users to manually enter contacts, mobile social address books automate this process by enabling subscribers to exchange contact information following a call or SMS.<ref>[http://www.computerworld.com/action/article.do?command=viewArticleBasic&articleId=9115165 Computerworld, retrieved 2008-11-5]</ref> The contact information exchange occurs instantaneously and the user’s phonebook updates automatically. Mobile social address books also provide dynamic updates of contacts if their numbers change over time.

== History ==
Mobile social address books began appearing in 2007 as a parallel social trend to the emergence of Internet-based social networking sites like [[Facebook]], [[MySpace]] and [[LinkedIn]], establishing a new paradigm for interpersonal contact and communication. Mobile social address books sought to bring the connectivity of social networking to the in-the-moment experience of the mobile phone. Users can easily exchange contact information regardless of their handset, mobile carrier, or social networking application they use.<ref>[http://latestgeeknews.blogspot.com/2008/02/social-address-booknext-killer-app-part.html Latest Geek News, retrieved 2008-11-5]</ref>

Examples of emerging companies providing technology to support mobile social address books include: [[PicDial]] (which dynamically augments the existing address book with pictures and status from Facebook, MySpace and Twitter, integrates with the call screen so during every call you see the latest picture and status of whoever is calling.  It is a network address book so everything can be managed from Windows or Mac as well and lastly you can also set your one callerID picture and status for your friends to see when you call them) FusionOne (whose backup and synchronization solutions lets users easily transfer and update mobile content, including contact information, among different devices); [[Loopt]] (whose Loopt service provides a social compass alerting users when friends are near); OnePIN (whose CallerXchange person-to-person contact exchange service lets users share contact info with one click on the mobile phone); and VoxMobili (whose Phone Backup and Synchronized Address Book solutions let users safeguard and synchronize their contact information among different devices).

== References ==
<references />

==External links==
* [http://www.loopt.com Loopt website]
* [http://www.onepin.com OnePIN website]
* [http://www.voxmobili.com VoxMobili website]
* [http://www.picdial.com PicDial website]

{{Mobile phones}}

[[Category:Social networks]]
[[Category:Directories]]
<=====doc_Id=====>:491
<=====title=====>:
Almanach de Bruxelles (defunct)
<=====text=====>:
The '''''Almanach de Bruxelles''''' is a now defunct [[France|French]] social register that listed [[royal family|royal]] and [[nobility|noble]] [[dynasties]] of [[Europe]]. It was established in 1918 during the [[Second World War]] to compete against the prominent German [[Almanach de Gotha]].<ref>March 17, 1918. [http://query.nytimes.com/mem/archive-free/pdf?res=9E0DEED6143AEF33A25754C1A9659C946996D6CF Almanach de Gotha has a french rival] at ''[[New York Times]]''</ref>

==See also==
* ''[[Almanach de Gotha]]''

==Sources==
{{reflist}}

==External links==
*[http://www.worldcat.org/title/almanach-de-bruxelles-annuaire-genealogique-historique-heraldique-des-maisons-souverains-princieres-et-ducales/oclc/06083750 ''Almanach de Bruxelles'' (1918-] at [[WorldCat]]

[[Category:Genealogy publications]]
[[Category:Directories]]
[[Category:Biographical dictionaries]]
[[Category:Defunct periodicals of France]]
[[Category:European nobility]]
[[Category:French royalty]]
[[Category:1918 establishments in France]]
[[Category:Publications established in 1918]]
 

{{royal-bio-book-stub}}
{{bio-dict-stub}}
<=====doc_Id=====>:494
<=====title=====>:
Spotlight (Casting Services Company)
<=====text=====>:
{{Use dmy dates|date=September 2015}}
{{Use British English|date=September 2015}}
{{multiple issues|
{{no footnotes|date=January 2014}}
{{Primary sources|date=April 2013}}
}}
[[File:Cinema Museum, London object 15.JPG|thumb|Spotlight volumes preserved at the Cinema Museum, London.]]
	
'''Spotlight''' was founded in 1927 and is the [[United Kingdom|UK's]] largest casting resource. Over 60,000 performers appear in Spotlight, including [[actor]]s and actresses, [[child actor|child artist]]s, [[presenter]]s, [[dancer]]s, and [[stuntman|stunt artists]]. Thousands of [[production company|production companies]], [[broadcasting|broadcasters]], [[advertising agency|ad agencies]], and independent casting directors use Spotlight as a casting resource. Their clients range from large organisations such as the [[BBC]], [[ITV (TV network)|ITV]], and [[Channel 4]], to small production companies and individual casting [[Television director|directors]].

Spotlight also publishes the [[handbook]] Contacts both in hard copy and as an [[e-book]]. It includes listings for over 5000 [[company|companies]], services and individuals across all branches of [[television]], [[theatre|stage]], [[film industry|film]], and [[radio]].

==External links==
* [http://www.spotlight.com Spotlight website]
* [http://www.contactshandbook.com Contacts website]

{{UK-stub}}
{{DEFAULTSORT:Spotlight}}
[[Category:Directories]]
[[Category:1927 establishments in the United Kingdom]]
[[Category:Casting companies]]

{{advertising-stub}}
<=====doc_Id=====>:497
<=====title=====>:
Polk's Directory
<=====text=====>:
#REDIRECT [[R.L. Polk & Company]]

[[Category:Histories of cities in the United States]]
[[Category:Publications established in the 1870s]]
[[Category:Directories]]
<=====doc_Id=====>:500
<=====title=====>:
Search.ch
<=====text=====>:
'''search.ch''' is a [[search engine]] and [[web portal]] for [[Switzerland]]. It was founded in 1995 <ref name="founding" /> by Rudolf Räber and Bernhard Seefeld as a regional search engine. In the following years many other services were added, such as a phonebook in 1999, a free [[Short message service|SMS]] service in 2000 (now reduced to only one free SMS per week).

The search technology is home grown. The user can restrict his search to regions of Switzerland, such as a [[cantons of Switzerland|canton]] or a [[city]]. The [[web crawler]] looks only at sites in the [[.ch]] and [[.li]] [[top-level domain]]s and a number of automatically and manually updated list of Swiss websites on other domains. The index is updated weekly.

== External links ==
* [http://www.search.ch/ search.ch]
* [http://tel.search.ch/ tel.search.ch] phonebook
* [http://map.search.ch/ map.search.ch] Swiss maps
* [http://meteo.search.ch/ meteo.search.ch] Swiss weather
* [http://news.search.ch/ news.search.ch] Swiss news
* [http://timetable.search.ch/ timetable.search.ch] Swiss public transport timetable
* [http://tv.search.ch/ tv.search.ch] Swiss TV programme
* [http://kino.search.ch/ kino.search.ch] Swiss cinema programme
* [http://sms.search.ch/ sms.search.ch] sms service
* [http://immo.search.ch/ immo.search.ch] immo portal search service
* [http://www.post.ch/ Swiss Post] <ref name="post" /> Swiss Post acquired search.ch
* [http://www.tamedia.ch Tamedia] <ref name="tamedia" /> Tamedia akquired a 75% stake from Swiss Post

==References==
{{Reflist|refs=
<ref name="founding">http://www.moneyhouse.ch/en/u/search_ch_ag_CH-130.0.009.911-2.htm</ref>
<ref name="tamedia">http://about.search.ch/archives/2004/06/04/post-kauft-search-ch/</ref>
<ref name="post">http://www.post.ch/post-startseite/post-konzern/post-medien/post-archive/2009/post-mm09-fruehzustellung/post-medienmitteilungen.htm</ref>
}}

{{DEFAULTSORT:Search.Ch}}
[[Category:Web portals]]
[[Category:Internet search engines]]
[[Category:Directories]]
<=====doc_Id=====>:503
<=====title=====>:
Trow's Directory
<=====text=====>:
#REDIRECT[[John Fowler Trow]]

[[Category:Directories]]
[[Category:Books about New York City]]
[[Category:History of New York City]]
<=====doc_Id=====>:506
<=====title=====>:
Address book
<=====text=====>:
{{redirect|Address Book|the Apple Inc. software|Address Book (application)}}
{{unreferenced|date=February 2012}}
[[File:Address book 1.jpg|thumb|A blank page in a typical paper address book]]
[[File:Jack L. Warner's address book - National Museum of American History - DSC06088.JPG|thumb|[[Jack L. Warner]]'s address book on display at the [[National Museum of American History]]]]
An '''address book''' or a '''name and address book''' ('''NAB''') is a [[book]] or a [[database]] used for storing entries called '''contacts'''. Each contact entry usually consists of a few standard [[Field (computer science)|fields]] (for example: first name, last name, company name, [[address (geography)|address]], [[telephone]] number, [[e-mail]] address, [[fax]] number, [[mobile phone]] number). Most such systems store the details in alphabetical order of people's names, although in [[paper]]-based address books entries can easily end up out of order as the owner inserts details of more individuals or as people move. Many address books use small [[ring binder]]s that allow adding, removing and shuffling of pages to make room.

== Little black book ==
A related term that has entered the popular [[lexicon]] is '''little black book''' (or simply '''black book'''). Such books are used as [[courtship|dating]] guides, listing people who the owner has dated in the past or hopes to in the future, and details of their various relationships. More explicit variations are guides for [[sexual partner]]s. It is unclear how prevalent this is in practice or when it originated, though such books have been mentioned in many pieces of [[popular culture]]. For example, the 1953 film version of ''[[Kiss Me, Kate]]'' features a musical scene in which [[Howard Keel]]'s character laments the loss of the social life he enjoyed before marriage, naming numerous female romantic encounters while perusing a miniature black book. More recently, the mid-2000s [[Guinness Brewmasters]] advertising campaign features the "little black book" as an invention of one of the brewmasters.

== Software address book ==
[[File:X-office-address-book.svg|thumb|A digital address book icon]]
Address books can also appear as [[software]] designed for this purpose, such as the [[Address Book (application)|"Address Book"]] application included with [[Apple Inc.]]'s [[Mac OS X]]. Simple address books have been incorporated into [[e-mail]] software for many years, though more advanced versions have emerged in the 1990s and beyond; and also in [[mobile phone]]s.

A [[personal information manager]] (PIM) integrates an address book, [[calendar]], task list, and sometimes other features.

Entries can be imported and exported from the software in order to transfer them between programs or computers. The common file formats for these operations are:
* [[LDAP Data Interchange Format|LDIF]] (*.ldif, *.ldi)
* Tab delimited (*.tab, *.txt)
* [[Comma-separated values|Comma-separated]] (*.csv)
* [[vCard]] (*.vcf)

Individual entries are frequently transferred as [[vCard]]s (*.vcf), which are roughly comparable to physical [[business card]]s. And some software applications like [[Lotus Notes]] and Open Contacts can handle a vCard file containing multiple vCard records.

== Online address book ==
An online address book typically enables users to create their own web page (or profile page) which is then indexed by search engines like Google and Yahoo. This in turn enables users to be found by other people via a search of their name and then contacted via their web page containing their personal information. Ability to find people registered with online address books via search engine searches usually varies according to the commonness of the name and the amount of results for the name. Typically users of such systems can synchronize their contact details with other users that they know to ensure that their contact information is kept up to date.

== Network address book ==
Currently, most people have many different address books: their email accounts, their mobile phone, and the "friends lists" on their social networks. A network address book allows them to organize and manage all of their address books through a single interface and share their contacts across their different address books and social networks.

== See also ==
{{colbegin|3}}
* [[Calendaring software]]
* [[Contact list]]
* [[Mobile social address book]]
* [[Personal information manager]]
* [[Rolodex]]
* [[Suvorin directories]]
* [[Telephone directory]]
* [[Windows Address Book]]
{{colend}}

{{Authority control}}
[[Category:Office equipment]]
[[Category:Directories]]
<=====doc_Id=====>:509
<=====title=====>:
Category:Domain name system
<=====text=====>:
{{Cat main|Domain Name System}}
{{Commonscat|Domain name system}}

[[Category:Internet governance]]
[[Category:Internet Standards]]
[[Category:Internet architecture]]
[[Category:Network addressing]]
[[Category:Application layer protocols]]
[[Category:Directories]]

[[ms:Kategori:Sistem nama domain]]
<=====doc_Id=====>:512
<=====title=====>:
Category:File system directories
<=====text=====>:
[[Category:Computer file systems]]
[[Category:Directories]]
<=====doc_Id=====>:515
<=====title=====>:
World Leaders
<=====text=====>:
{{italic title}}
[[Image:ChiefsCover.jpg|right|200px|thumb|Cover of ''World Leaders'']]
'''''World Leaders''''', also known as '''''Chiefs of State and Cabinet Members of Foreign Governments''''', is a [[public domain]] directory published weekly by the United States [[Central Intelligence Agency]]. It lists different state officials for each country of the world: the [[head of state]] and/or [[head of government]] and other [[cabinet minister]]s, the chief of the [[central bank]], and the [[ambassador]]s to the [[United Nations]] and the United States.

==See also==
*[[World-Check]]
*[[List of current heads of state and government]]
*[[National Security Agency academic publications]]
*''[[International Who's Who]]''

==External links==
*[https://www.cia.gov/library/publications/world-leaders-1/ ''World Leaders'']

[[Category:Central Intelligence Agency publications]]
[[Category:Heads of state]]
[[Category:Heads of government]]
[[Category:Directories]]
[[Category:Public domain databases]]

{{US-gov-stub}}
<=====doc_Id=====>:518
<=====title=====>:
Telephone directory
<=====text=====>:
{{Redirect2|Phone book|White pages|a contact list|Contact list|other uses|White pages (disambiguation)}}
{{Use dmy dates|date=June 2012}}
{{refimprove|date=November 2008}}
[[File:Telefonkatalog 1928.jpg|thumb|[[Gothenburg]] telephone directory, 1928.]]

A '''telephone directory''', also known as a '''telephone book''', '''telephone address book''', '''phone book''', or the '''white/yellow pages''', is a listing of telephone [[subscriber]]s in a geographical area or subscribers to services provided by the organization that publishes the directory. Its purpose is to allow the telephone number of a subscriber identified by name and address to be found.

The advent of the Internet and [[smart phones]] in the 21st century greatly reduced the need for a paper phone book.  Some communities, such as [[Seattle]] and [[San Francisco]], sought to ban their unsolicited distribution as wasteful, unwanted and harmful to the environment.<ref name=SF>[http://www.sfgate.com/bayarea/article/Yellow-Pages-ruling-endangers-SF-ban-3951477.php Yellow Pages ruling endangers SF ban], Heather Knight, ''[[San Francisco Chronicle]]'', 15 October 2012; accessed 19 March 2013</ref><ref>[http://seattletimes.com/html/localnews/2019441687_yellowpages16m.html Appeals court rules against Seattle's curbs on yellow pages], Emily Heffter, ''[[Seattle Times]]'', 15 October 2012; accessed 19 March 2013</ref>

== Content ==
Subscriber names are generally listed in alphabetical order, together with their postal or street address and [[telephone number]].  In principle every subscriber in the geographical coverage area is listed, but subscribers may request the exclusion of their number from the directory, often for a fee; their number is then said to be "unlisted" ([[American English]]), "ex-directory" ([[British English]]), "private" or '''private number''' (Australia and New Zealand), or "non-published" (Canada).<ref>{{cite web|url=http://support.bell.ca/Home_phone/Phone_line/How_to_unlist_my_Bell_Home_phone_number |title=How to get a non-published Bell Home phone number |publisher=Support.bell.ca |date=2013-06-17 |accessdate=2014-04-16}}</ref>

A telephone directory may also provide instructions: how to use the [[Local telephone service|telephone service]], how to dial a particular number, be it local or international, what numbers to access important and [[emergency services]], utilities, hospitals, doctors, and organizations who can provide support in times of crisis. It may also have [[civil defense]] or [[emergency management]] information. There may be transit maps, postal code/zip code guides, international dialing codes or stadium seating charts, as well as advertising.

In the US, under current rules and practices, mobile phone and [[voice over IP]] listings are not included in telephone directories.  Efforts to create cellular directories have met stiff opposition from several fronts, including those who seek to avoid [[telemarketer]]s.{{Citation needed|date=January 2011}}

== Types ==
[[File:Telefonbog ubt-1.JPG|thumb|White pages.]]
A telephone directory and its content may be known by the color of the paper it is printed on.
* White pages<!--redirects here--> generally indicates personal or alphabetic listings.
* [[Yellow pages]], golden pages, A2Z, or classified directory is usually a "business directory", where businesses are listed alphabetically within each of many classifications (e.g., "lawyers"), almost always with paid advertising.
* [[Reverse telephone directory|Grey pages]], sometimes called a "reverse telephone directory", allowing subscriber details to be found for a given number. Not available in all jurisdictions.{{citation needed|date=March 2014}}  (These listings are often published separately, in a city directory, [[Polk directory]], or under another name, for a price, and made available to commercial and government agencies.)

Other colors may have other meanings; for example, information on [[government agencies]] is often printed on [[blue pages]] or green pages.{{Citation needed|date=September 2011}}

== Publication ==
[[File:New haven directory 1878.jpg|thumb|upright|New Haven directory, November, 1878.]]Telephone directories can be published in [[hard copy]] or in electronic form. In the latter case, the directory can be provided as an online service through proprietary terminals or over the Internet, or on physical media such as CD-ROM. In many countries directories are both published in book form and also available over the Internet. Printed directories were usually supplied free of charge.

== History ==
[[File:Unused Phonebooks.JPG|thumb|Recently delivered 2013–2014 phone books in the trash unopened; in the 21st century some communities have tried to stop the unsolicited distribution of the books<ref name=SF/>]]
{{Expand section|date=September 2011}}
Telephone directories are a type of [[city directory]]. Books listing the inhabitants of an entire city were widely published starting in the 18th century, before the invention of the telephone. 

The first telephone directory, consisting of a single piece of cardboard, was issued on 21 February 1878; it listed 50 individuals, businesses, and other offices in [[New Haven, Connecticut]] that had telephones.<ref>{{cite web| title= The Phone Book | url= http://failuremag.com/feature/article/the_phone_book/ | author=Jason Zasky | work=Failure Magazine |accessdate=2013-12-31}}</ref>

The first British telephone directory was published on 15 January 1880  by The Telephone Company. It contained 248 names and addresses of individuals and businesses in London; telephone numbers were not used at the time as subscribers were asked for by name at the exchange.<ref>Records of the Telephone Company Limited (Bell's Patents), BT Archives reference TPA</ref> The directory is preserved as part of the British phone book collection by [[BT Archives]].

In 1938, AT&T commissioned the creation of a new type font, known as [[Bell Gothic|BELL GOTHIC]], the purpose of which was to be readable at very small font sizes when printed on newsprint where small imperfections were common.

In 1981 France was the first country to have an electronic directory<ref>{{cite web|url=http://whitepages.fr/minitel/ |title=Telephone History in France by |publisher=Whitepages.fr |date= |accessdate=2014-04-16}}</ref> on a system called [[Minitel]]. The directory is called "11" after its telephone access number.

In 1991 the [[U.S. Supreme Court]] ruled (in ''[[Feist v. Rural]]'') that telephone companies do not have a [[copyright]] on telephone listings, because copyright protects creativity and not the mere labor of collecting existing information.

1996 is the year the first telephone directories go online in the USA. [[Yellowpages.com]] and [[Whitepages.com]] both see their start in April.<ref>[http://www.whitepages.fr/telecom-history-ft-late-with-internet.html Telephone Directory History by Whitepages.fr]</ref>

In 1999, the first online telephone directories and people finding sites such as [[LookupUK.com]] go online in the UK. In 2003, more advanced UK searching including Electoral Roll become available on [[LocateFirst.com]].

In the 21st century, printed telephone directories are increasingly criticized as waste. In 2012, after some North American cities passed laws banning the distribution of telephone books, an industry group sued and obtained a court ruling permitting the distribution to continue.<ref name=SF/> Manufacture and distribution of telephone directories produces over 1,400,000 metric tons of [[greenhouse gases]] and consumes over 600,000 tons of paper annually.<ref>{{cite web|last= Paster |first= Pablo |url=http://www.treehugger.com/culture/ask-pablo-what-is-the-impact-of-all-those-unwanted-phone-books.html |title=Ask Pablo: What Is The Impact Of All Those Unwanted Phone Books? |publisher=TreeHugger |date=2010-01-11 |accessdate=2014-04-16}}</ref>

== Reverse directories ==
{{main|Reverse telephone directory}}
A reverse telephone directory is sorted by number, which can be looked up to give the name and address of the subscriber.

== In popular culture ==
Ripping phone books in half has often been considered a [[Feats of strength|feat of strength]]. The Guinness World Record for ripping the most telephone directories is 27; the record for French telephone directories is 29, held by [[Georges Christen]].{{citation needed|date=October 2012}}

== See also ==
* [[Domain Name System|DNS]]
* [[Lightweight Directory Access Protocol|LDAP]]
* [[Silent number]]
* [[Whois]]
* [[City directory]]

== References ==
{{reflist|colwidth=30em}}

== Further reading ==
* {{cite book|title=The Phone Book: The Curious History of the Book That Everyone Uses But No One Reads| last= Shea|first=Ammon|publisher=Perigee Trade|year=2010|ISBN=978-0-399-53593-2}}

== External links ==
*{{Commonscat-inline|Phone books}}
*{{wikt-inline}}
* {{dmoz|Reference/Directories/Address_and_Phone_Numbers}}

{{Authority control}}

{{DEFAULTSORT:Telephone Directory}}
[[Category:Telephone numbers]]
[[Category:Directories]]
[[Category:History of the telephone]]
[[Category:American inventions]]
[[Category:1878 introductions]]
<=====doc_Id=====>:521
<=====title=====>:
Directory of Open Access Journals
<=====text=====>:
{{Infobox website
|name            = Directory of Open Access Journals
|logo            = DOAJ logo.jpg
|logocaption     =
|screenshot      =
|collapsible     =
|collapsetext    =
|caption         =
|url             = {{URL|https://doaj.org/}}
|slogan          =
|commercial      =No
|type            =
|registration    =
|language        =English
|content license =
|owner           =
|author          =
|launch date     = <!--{{Start date and age|YYYY|MM|DD|df=yes/no}}-->
|alexa           = 58,591 (as of October 2015)<ref>{{cite web|title=Ranking for Doaj.org|url=http://www.alexa.com/siteinfo/doaj.org|accessdate=2015-10-20|work=[[Alexa.com]]}}</ref>
|revenue         =
|current status  =Online
|footnotes       =
}}

The '''Directory of Open Access Journals''' ('''DOAJ''') is a [[website]] that lists [[open access journal]]s and is maintained by Infrastructure Services for Open Access (IS4OA).<ref>{{cite web |url=http://www.is4oa.org/ |title=Infrastructure Services for Open Access |publisher=Infrastructure Services for Open Access C.I.C. |accessdate=2013-03-05}}</ref> The project defines open access journals as [[scientific journal|scientific]] and [[academic journal|scholarly journal]]s that meet high quality standards by exercising [[peer review]] or editorial quality control and "use a funding model that does not charge readers or their institutions for access."<ref name=aboutdoaj/> The [[Budapest Open Access Initiative]]'s definition of [[Open access (publishing)|open access]] is used to define required rights given to users, for the journal to be included in the DOAJ, as the rights to "read, download, copy, distribute, print, search, or link to the full texts of these articles".<ref name=aboutdoaj/><ref>The BOAI definition is at "[http://www.earlham.edu/~peters/fos/boaifaq.htm#openaccess Budapest Open Access Initiative: Frequently Asked Questions]".</ref> The aim of DOAJ is to "increase the visibility and ease of use of open access scientific and scholarly journals thereby promoting their increased usage and impact."<ref name=aboutdoaj>{{cite web |url= http://doaj.org/about |title=About |work=Directory of Open Access Journals |accessdate=2015-04-14}}</ref>

As of March 2015, the database contained records for 10,000 journals.<ref>{{cite web |url= http://sparc.arl.org/blog/doaj-introduces-new-standards |title=Directory of Open Access Journals introduces new standards to help community address quality concerns |first=Caralee |last=Adams |date=5 March 2015 |publisher=SPARC |accessdate=2015-04-14}}</ref> An average of four journals were being added each day in 2012.<ref>{{cite web|title=DOAJ Statistics |url= http://www.doaj.org/doaj?func=newTitles&uiLanguage=en&fromDate=1970-01-01+00:00:00&orderedBy=J.first_added |archiveurl= https://web.archive.org/web/20120404125652/http://www.doaj.org/doaj?func=newTitles&uiLanguage=en&fromDate=1970-01-01+00:00:00&orderedBy=J.first_added |archivedate= 2012-04-04 |accessdate=2013-01-06 |work=Directory of Open Access Journals}}</ref>

In May 2016, DOAJ announced that they had removed approximately 3,300 journals from their database to provide better reliability on the content listed on it.<ref>{{cite journal |last=Marchitelli|first= Andrea|last2=Galimberti |first2=Paola |last3=Bollini |first3=Andrea|last4=Mitchell |first4=Dominic|date= January 2017 |title=
Helping journals to improve their publishing standards: a data analysis of DOAJ new criteria effects |url=http://leo.cineca.it/index.php/jlis/article/view/12052|journal= JLIS.it|volume=8 |issue=1 |pages= 39-49|doi=10.4403/jlis.it-12052|access-date=2017-01-22 }}</ref>

The journals that were removed can reapply as part of an ongoing procedure. <ref>{{Cite web|url=https://doajournals.wordpress.com/2016/05/09/doaj-to-remove-approximately-3300-journals/|title=DOAJ to remove approximately 3300 journals|last=DOAJ|date=2016-05-09|website=News Service|access-date=2016-09-24}}</ref> As of September 2016, the database now contains 9,216 journals. <ref>{{Cite web|url=https://doaj.org/|title=Directory of Open Access Journals|last=DOAJ|access-date=2016-09-24}}</ref>

==History==
The [[Open Society Institute]] funded various open access related projects after the Budapest Open Access Initiative; the Directory was one of those projects.<ref>{{cite book|last=Crawford|first=Walt|title=Open access : what you need to know now|publisher=American Library Association|location=Chicago|isbn=9780838911068|page=13}}</ref> The idea for the DOAJ came out of discussions at the first Nordic Conference on
Scholarly Communication in 2002, [[Lund University]] became the organization to set up and maintain the DOAJ.<ref>{{Cite journal | last1 = Hedlund | first1 = T. | last2 = Rabow | first2 = I. | doi = 10.1087/2009303 | title = Scholarly publishing and open access in the Nordic countries | journal = Learned Publishing | volume = 22 | issue = 3 | pages = 177-186| year = 2009 | pmid =  | pmc = }}</ref> It continued to do so  until January 2013, when Infrastructure Services for Open Access (IS4OA) took over.

== See also ==
* [[List of open-access journals]]
*[[Open Access Scholarly Publishers Association]]

== References ==
<references/>

== External links ==
* {{official website|https://doaj.org/}}
{{Open access navbox}}

[[Category:Open access (publishing)]]
[[Category:Open access journals| ]]
[[Category:Directories]]
<=====doc_Id=====>:524
<=====title=====>:
Western Australia Post Office Directory
<=====text=====>:
{{Use Australian English|date=March 2015}}
{{Use dmy dates|date=March 2015}}

The '''''Western Australia Post Office Directory''''', also known as ''Wise Directories'' or ''Wise Street Directories'' were published in Perth from 1893-1949.

They were published by H. Pierssené<ref>{{Citation | author1=Pierssené, Herbert | title=The Western Australian directory | publication-date=1893 | publisher=H. Pierssene | url=http://trove.nla.gov.au/work/28621466 | accessdate=6 March 2015 }}</ref> and later by H. Wise & Co.<ref>{{Citation | author1=Wise & Co | title=Wise's Western Australia post office directory | publication-date=1938 | publisher=H. Wise & Co. Pty Ltd | url=http://trove.nla.gov.au/work/19293522 | accessdate=6 March 2015 }}</ref>  They listed household, business, society, and Government contacts in [[Perth]], [[Freemantle]], [[Kalgoorlie, Western Australia|Kalgoorlie]], [[Boulder, Western Australia|Boulder]] and [[Coolgardie, Western Australia|Coolgardie]] including some rural areas of [[Western Australia]].

==Publishers==
The ''Western Australian Directory'' was published by H. Pierssene between 1893-1895. Herbert Pierssene was a merchant and importer of English Continental and Ceylonese goods. He was an agent for McCulluch Carrying Company and a bottler of West Australian wines.<ref>{{cite web|title=Thomas Herbert Pierssené|url=http://www.territorystories.nt.gov.au/handle/10070/244383|website=Territory Stories|publisher=Northern Territory Department of Arts and Museums|accessdate=6 March 2015}}</ref>

The ''Western Australia Post Office Directory'' was published by Wise & Co. between the years 1895-1949 with the exception of 1943 and 1948.

==Wise Directories== 	
The directories provide information by locality, individual surname, government service, and by trade or profession. The addresses of householders and businesses throughout Western Australia are included.<ref>{{cite news |url=http://nla.gov.au/nla.news-article77356821 |title=POST-OFFICE DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=27 April 1909 |accessdate=6 March 2015 |page=2 Edition: THIRD EDITION |publisher=National Library of Australia}}</ref>  Maps were sometimes published with an edition of the directory.<ref> {{cite news |url=http://nla.gov.au/nla.news-article77329156 |title=WESTERN AUSTRALIA DIRECTORY. |newspaper=[[Daily_News_(Perth,_Western_Australia)|The Daily News (Perth, WA : 1882 - 1950)]] |location=Perth, WA |date=5 March 1908 |accessdate=6 March 2015 |page=6 Edition: THIRD EDITION |publisher=National Library of Australia}}</ref>  The towns section of the directories normally contained separate street directories of Perth and suburbs, Fremantle and Suburbs, Kalgoorlie, Boulder and Coolgardie.<ref>{{Citation | author1=Wise's Directories | author2=Archive CD Books Australia | title=Western Australia Post Office directory (Wise's) 1905 | publication-date=2004 | publisher=Archive CD Books Australia | isbn=978-1-920978-23-5 }}</ref>

Known colloquially to users and  book collectors as 'Wise Directories' or 'Wise Street Directories' the red covered directories were published between 1893 and 1949.  Due to the annual changes, the directories are valuable historical documents for Western Australian History.  They are scarce in the Australian rare book market.  

The directories have been invaluable referent points for such projects as the [[Dictionary of Western Australians]] and others where the street lists in the directory provide details of inhabitants and houses in some streets in the more built-up residential areas.  Country towns in the directory have name lists only. 

They have been available in microfilm form in [[J S Battye Library]], and more recently have become online (see link below) in one of the J S Battye Library digitization projects.

==References==
{{reflist}}

==External links==
* http://www.slwa.wa.gov.au/find/wa_resources/post_office_directories

==See also==
* [[Australia Post]]
*[[Australian Dictionary of Biography]]
*[[Cyclopedia of Western Australia]]
*[[Dictionary of Australian Biography]]
*[[J S Battye Library]]
*[[State Records Office of Western Australia]]

[[Category:Books about Western Australia]]
[[Category:History of Western Australia|Western Australia Post Office Directory]]
[[Category:Australian directories]]
[[Category:Directories]]
[[Category:Gazetteers]]
[[Category:Postal system of Australia]]
<=====doc_Id=====>:527
<=====title=====>:
Bengali film directory
<=====text=====>:
{{italic title}}
{{Infobox book
| name = Bengali Film Directory
| image = Bengali film directory.jpg
| caption =Cover page of ''Bengali Film Directory''
| author = [[Ansu Sur]]
| country = India
| language = English
| cover_artist =
| genre = [[Trade directory|Directory]]
| publisher = [[Nandan (Kolkata)|Nandan]]<br /> [[West Bengal]] Film Centre([[Calcutta]])
| release_date = 1999
| media_type = Print ([[Hardback]])
| pages =319
| preceded_by =
| followed_by =
}}

'''''Bengali Film Directory''''' is an archive of [[Cinema of West Bengal|Bengali film]]s (in English).<ref>{{cite web |url=http://www.rosland.freeserve.co.uk/filmbooks5.htm |title=The Howard Summers Cinema Website-National Filmographies-Asian Cinema |publisher=www.rosland.freeserve.co.uk |accessdate=2008-10-23 |archiveurl = https://web.archive.org/web/20080616195358/http://www.rosland.freeserve.co.uk/filmbooks5.htm <!-- Bot retrieved archive --> |archivedate = 2008-06-16}}</ref> Published in March 1999 by [[Nandan (Kolkata)|Nandan]], [[West Bengal]] Film Centre ([[Calcutta]]), this directory was edited by Ansu Sur and was compiled by Abhijit Goswami. It includes all [[Bengali people|Bengali]] [[feature film]]s released from 1917 to 1998, described briefly, but including detailed cast and crew, director name, release date and release theater name.<ref>{{cite web |url=https://openlibrary.org/b/OL171681M |title=Bengali film directory (Open Library) |publisher=openlibrary.org |accessdate=2008-10-23}}
</ref>

==Contents==
* Acknowledgements iv
* A Note from the Editor v
* Reference vi
* Abbreviations vii
* Filmography
** Silent era 1
** Sound era 9
* Studios and Post-Production Centres 267
* Production Companies 269
* Distributions 271
* Show-houses 274
* Useful Addresses 277
* Film Societies 278
* Film Journals 280
* Film Books 281
* Index
** Films 285
** Directors 298
** Actors and Actresses 303
* Appendix: Films released in 1998 315<ref>{{cite web |url=http://www.cscsarchive.org/MediaArchive/Library.nsf/(docid)/5BD370ADB88260A6652571AF0037A748?OpenDocument&StartKey=Bengali&Count=100 |title=Bengali film directory |publisher=www.cscsarchive.org |accessdate=2008-10-23}} {{Dead link|date=October 2010|bot=H3llBot}}</ref>

==References==
{{Reflist}}

==External links==

{{Cinema of West Bengal}}
{{Cinema of Bangladesh}}
{{Cinema of India}}
{{World cinema}}

[[Category:Cinema of Bengal]]
[[Category:Bengali language]]
[[Category:1999 books]]
[[Category:Bengali-language media]]
[[Category:English-language media]]
[[Category:Film guides]]
[[Category:Archives in India]]
[[Category:Directories]]


{{film-org-stub}}
<=====doc_Id=====>:530
<=====title=====>:
Prospective search
<=====text=====>:
{{one source|date=April 2014}}
'''Prospective search''', or '''persistent search''', is a method of [[Search engine technology|searching]] on the [[Internet]] where the query is given first and the information for the results are then acquired. This differs from traditional, or "retrospective", search such as [[search engines]], where the information for the results is acquired and then queried.<ref name=globalpr2005>{{cite web|url=http://www.globalprblogweek.com/2005/09/21/wyman-reputation-management/|title=Blogs & Prospective Search Technology for Corporate Reputation Management|year=2005|author=Bob Wyman|publisher=Global PR Blog Week website}}</ref>

== Comparison to retrospective search ==
Retrospective search starts by gathering the information, indexing it, then letting users query the information. The results don't change until the index is rebuilt, often months apart. Prospective search starts with the user's queries, gathers the information in a targeted way, indexing it and then providing the results as they arrive. Sometimes [[Ping blog|Ping Servers]] are used to gather notification of changes to websites so that the information received is as fresh as possible. Users can be notified in a number of ways of new results.

Prospective search is well suited to queries where the results change over time, such as the current news, [[blog]]s and trends.

== See also ==
* [[PubSub]]
* [[Google Alerts]]
* Google AppEngine Prospective Search Service<ref>https://code.google.com/appengine/docs/python/prospectivesearch/</ref> (deprecated as of December 1st 2015<ref>https://cloud.google.com/appengine/docs/deprecations/prospective_search?hl=en</ref>)
* [[Selective dissemination of information]]
* [[Superfeedr]] ('tracker' API<ref>http://blog.superfeedr.com/full-text-trackers/</ref>)

== Quotes ==
{{quote|Prospective search is emerging as a way of keeping up-to-date on any subject of interest. This technology constantly monitors relevant blogs and Web feeds for matches to users’ subscriptions and delivers results in real time. Thus, users are notified whenever something new appears on their subject of choice|Global PR Blog Week<ref name=globalpr2005/>}}

==References==
{{reflist}}

[[Category:Internet search]]


{{compu-prog-stub}}
<=====doc_Id=====>:533
<=====title=====>:
Web query classification
<=====text=====>:
{{Cleanup|date=March 2011}}
''' 
A Web query topic classification/categorization is a problem in [[information science]]. The task is to assign a [[Web search query]] to one or more predefined [[Categorization|categories]], based on its topics. The importance of query classification is underscored by many services provided by Web search. A direct application is to provide better search result pages for users with interests of different categories. For example, the users issuing a Web query “''apple''” might expect to see Web pages related to the fruit apple, or they may prefer to see products or news related to the computer company. Online advertisement services can rely on the query classification results to promote different products more accurately. Search result pages can be grouped according to the categories predicted by a query classification algorithm. However, the computation of query classification is non-trivial. Different from the [[document classification]] tasks, queries submitted by Web search users are usually short and ambiguous; also the meanings of the queries are evolving over time. Therefore, query topic classification is much more difficult than traditional document classification tasks.

== KDDCUP 2005 ==

KDDCUP 2005 competition<ref>[http://www.kdd.org/kdd-cup/view/kdd-cup-2005 KDDCUP 2005 dataset]</ref> highlighted the interests in query classification. The objective of this competition is to classify 800,000 real user queries into 67 target categories. Each query can belong to more than one target category. As an example of a QC task, given the query “''apple''”, it should be classified into ranked categories: “''Computers \ Hardware''; ''Living \ Food & Cooking''”.

{| class="wikitable"
|-
! Query
! Categories
|-
| apple
| Computers \ Hardware<br />Living \ Food & Cooking
|-
| FIFA 2006
| Sports \ Soccer<br />Sports \ Schedules & Tickets<br />Entertainment \ Games & Toys
|-
| cheesecake recipes
| Living \ Food & Cooking<br />Information \ Arts & Humanities
|-
| friendships poem
| Information \ Arts & Humanities<br />Living \ Dating & Relationships
|}

[[Image:Web query length.gif]]
[[Image:Web query meaning.gif]]

== Difficulties ==

Web query topic classification is to automatically assign a query to some predefined categories. Different from the traditional document classification tasks, there are several major difficulties which hinder the progress of Web query understanding:

=== How to derive an appropriate feature representation for Web queries? ===

Many queries are short and query terms are noisy. As an example, in the KDDCUP 2005 dataset, queries containing 3 words are most frequent (22%). Furthermore, 79% queries have no more than 4 words. A user query often has multiple meanings. For example, "''apple''" can mean a kind of fruit or a computer company. "''Java''" can mean a programming language or an island in Indonesia. In the KDDCUP 2005 dataset, most of the queries contain more than one meaning. Therefore, only using the keywords of the query to set up a [[vector space model]] for classification is not appropriate.

* Query-enrichment based methods<ref>Shen et al.  [http://www.sigkdd.org/sites/default/files/issues/7-2-2005-12/KDDCUP2005Report_Shen.pdf "Q2C@UST: Our Winning Solution to Query Classification"]. ''ACM SIGKDD Exploration, December 2005, Volume 7, Issue 2''.</ref><ref>Shen et al. [http://portal.acm.org/ft_gateway.cfm?id=1165776 "Query Enrichment for Web-query Classification"]. ''ACM TOIS, Vol. 24, No. 3, July 2006''.</ref> start by enriching user queries to a collection of text documents through [[search engines]]. Thus, each query is represented by a pseudo-document which consists of the snippets of top ranked result pages retrieved by search engine. Subsequently, the text documents are classified into the target categories using synonym based classifier or statistical classifiers, such as [[Naive Bayes]] (NB) and [[Support Vector Machines]] (SVMs).

How about disadvantages and advantages??
give the answers:

=== How to adapt the changes of the queries and categories over time? ===

The meanings of queries may also evolve over time. Therefore, the old labeled training queries may be out-of-data and useless soon. How to make the classifier adaptive over time becomes a big issue. For example, the word "''Barcelona''" has a new meaning of the new micro-processor of AMD, while it refers to a city or football club before 2007. The distribution of the meanings of this term is therefore a function of time on the Web.

* Intermediate taxonomy based method<ref>Shen et al.  [http://portal.acm.org/ft_gateway.cfm?id=1148196 "Building bridges for web query classification"]. ''ACM SIGIR, 2006''.</ref> first builds a bridging classifier on an intermediate taxonomy, such as [[Open Directory Project]] (ODP), in an offline mode. This classifier is then used in an online mode to map user queries to the target categories via the intermediate taxonomy. The advantage of this approach is that the bridging classifier needs to be trained only once and is adaptive for each new set of target categories and incoming queries.

=== How to use the unlabeled query logs to help with query classification? ===

Since the manually labeled training data for query classification is expensive, how to use a very large web search engine query log as a source of unlabeled data to aid in automatic query classification becomes a hot issue. These logs record the Web users' behavior when they search for information via a search engine. Over the years, query logs have become a rich resource which contains Web users' knowledge about the World Wide Web.

* Query clustering method<ref>Wen et al. [http://portal.acm.org/ft_gateway.cfm?id=503108 "Query Clustering Using User Logs"], ''ACM TOIS, Volume 20, Issue 1, January 2002''.</ref> tries to associate related queries by clustering “session data”, which contain multiple queries and click-through information from a single user interaction. They take into account terms from result documents that a set of queries has in common. The use of query keywords together with session data is shown to be the most effective method of performing query clustering.
* Selectional preference based method<ref>Beitzel et al. [http://portal.acm.org/ft_gateway.cfm?id=1229183 "Automatic Classification of Web Queries Using Very Large Unlabeled Query Logs"], ''ACM TOIS, Volume 25, Issue 2, April 2007''.</ref> tries to exploit some [[association rules]] between the query terms to help with the query classification. Given the training data, they exploit several classification approaches including exact-match using labeled data, N-Gram match using labeled data and classifiers based on perception. They emphasize on an approach adapted from computational linguistics named selectional preferences. If x and y form a pair (x; y) and y belongs to category c, then all other pairs (x; z) headed by x belong to c. They use unlabeled query log data to mine these rules and validate the effectiveness of their approaches on some labeled queries.

== Applications ==

* '''[[metasearch|Metasearch engines]]''' send a user's query to multiple search engines and blend the top results from each into one overall list. The search engine can organize the large number of Web pages in the search results, according to the potential categories of the issued query, for the convenience of Web users' navigation.
* '''[[Vertical search]]''', compared to general search, focuses on specific domains and addresses the particular information needs of niche audiences and professions. Once the search engine can predict the category of information a Web user is looking for, it can select a certain vertical search engine automatically, without forcing the user to access the vertical search engine explicitly.
* '''[[Online advertising]]'''<ref>[http://www.kdd2007.com/workshops.html#adkdd Data Mining and Audience Intelligence for Advertising (ADKDD'07)], KDD workshop 2007</ref><ref>[http://research.yahoo.com/workshops/troa-2008/ Targeting and Ranking for Online Advertising (TROA'08)], WWW workshop 2008</ref> aims at providing interesting advertisements to Web users during their search activities. The search engine can provide relevant advertising to Web users according to their interests, so that the Web users can save time and effort in research while the advertisers can reduce their advertising costs.
All these services rely on the understanding Web users' search intents through their Web queries.

== See also ==

* [[Document classification]]
* [[Web search query]]
* [[Information retrieval]]
* [[Query expansion]]
* [[Naive Bayes classifier]]
* [[Support vector machines]]
* [[Meta search]]
* [[Vertical search]]
* [[Online advertising]]

== References ==

{{reflist}}

== Further reading ==
* Shen.  [http://lbxml.ust.hk/th/th_search.pl?smode=VIEWBYCALLNUM&skeywords=CSED%202007%20Shen "Learning-based Web Query Understanding"]. ''Phd Thesis'', ''HKUST'', June 2007.
{{Internet search}}

{{DEFAULTSORT:Web Query Classification}}
[[Category:Information retrieval techniques]]
[[Category:Internet search]]
<=====doc_Id=====>:536
<=====title=====>:
Search link optimization
<=====text=====>:
{{orphan|date =August 2009}}

'''Search Link Optimization''' ('''SLO''') is a process by which internal and external incoming links are optimized for [[search engine]] algorithms to determine the relevance of [[web page]] content.  Relevant [[anchor text]] integration, text that contains keywords for optimizing a web page, is key to this process.

==Inbound links, outbound links, internal links==

Inbound and outbound links are those that hyperlink two independent web pages together whereas inbound links would hyperlink domain “A” to domain “B” and outbound links would hyperlink domain “B” to domain “A.”

Inbound and outbound links are essential to web page visibility often enhancing web page relevance, ranking, & placement.  There are few instances where inbound links would be discouraged.  Outbound links however should be given sparingly and should only link material to other material of same or similar relevance.  Often, developers will utilize a [[nofollow]] tag used mostly to further optimize hyperlinks by “instructing” search engines not to distribute any [[PageRank]] from the hyperlink.  An example of a nofollow tag might be:

<syntaxhighlight lang="html5">
<a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia" rel="nofollow">Wikipedia, Online Encyclopedia</a>
</syntaxhighlight>

[[Internal link]]s are those that hyperlink within a single domain. Hyperlinks listed higher within the source code typically gain greater relevance. Some developers use a  practice known as PageRank Sculpting by using the nofollow tag to adjust the flow of PageRank.

==Proper coding of hyperlinks==

The mere presence of hyperlinks within a web page may not yield desired optimization results. For example, when coding a web page about "blue widgets," anchor text containing links referencing "red widgets" may alter relevance which can result in gain/loss ranking scenario where “blue widgets” gains while “red widgets” actually loses position.  Moreover, another result can be a complete loss of overall web page ranking altogether for either keyword.

A properly coded keyword contains these elements: Relevant anchor text, relevant keyword titling, and compliance-based [[Character encodings in HTML|HTML code]] structures.  Additionally, use of titles that are linked within the [[HTML element|title tag]] is also a recommended practice provided the title tag for the web page has also been properly optimized for the desired keyword(s).  Below is the proper coding of a hyperlink:

<syntaxhighlight lang="html5">
<a href="http://www.wikipedia.com" title="Wikipedia Online Encyclopedia">Wikipedia, Online Encyclopedia</a>
</syntaxhighlight>

==External links and references==
The footnotes below are given in support of the statements above. Because some facts are proprietary secrets held by private companies and therefore not documented in journals, such facts are reasoned from facts that are public.
* [http://www.mattcutts.com/blog/pagerank-sculpting/ PageRank Sculpting]
* [http://googleblog.blogspot.com/2005/01/preventing-comment-spam.html Prevent Comment Spam]
* [http://www.textlinks2u.com/search_engine_optimization.html Search Engine Optimization]

{{DEFAULTSORT:Search Link Optimization}}
[[Category:Internet search]]
<=====doc_Id=====>:539
<=====title=====>:
Hyper Search
<=====text=====>:
'''Hyper Search''' has been the first{{cn|date=May 2013}} published technique to introduce [[link analysis]] for search engines. It was created by Italian researcher [[Massimo Marchiori]].

==Bibliography==

* [[Massimo Marchiori]], [http://www.w3.org/People/Massimo/papers/WWW6/ "The Quest for Correct Information on the Web: Hyper Search Engines"], ''Proceedings of the Sixth International World Wide Web Conference (WWW6)'', 1997.
* [[Sergey Brin]] and [[Lawrence Page]], [http://www-db.stanford.edu/~backrub/google.html "The anatomy of a large-scale hypertextual Web search engine"], ''Proceedings of the Seventh International World Wide Web Conference (WWW7)'', 1998. 

== See also ==
* [[PageRank]]
* [[Spamdexing]]

[[Category:Internet search]]

{{web-stub}}
<=====doc_Id=====>:542
<=====title=====>:
Search/Retrieve via URL
<=====text=====>:
{{MOS|date=September 2015|reason=OMG! =)}}
'''Search/Retrieve via URL''' ('''SRU''') is a standard search protocol for [[Internet search]] queries, utilizing [[Contextual Query Language]] (CQL), a standard query syntax for representing queries.

Samplecode of a complete answer for this SRU Query-URL:
http://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&operation=searchRetrieve&query=dc.title=Darwinism 

<pre>
<?xml version="1.0"?>
<sru:searchRetrieveResponse xmlns:sru="http://www.loc.gov/zing/srw/" xmlns:diag="http://www.loc.gov/zing/srw/diagnostic/" xmlns:xcql="http://www.loc.gov/zing/cql/xcql/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <sru:version>1.1</sru:version>
  <sru:numberOfRecords>4</sru:numberOfRecords>
  <sru:records>
    <sru:record>
      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>
      <sru:recordPacking>XML</sru:recordPacking>
      <sru:recordData>
        <sru:dc>
          <sru:title>Darwinism</sru:title>
          <sru:creator>Dennett</sru:creator>
          <sru:subject>The rule of the Local is a basic principle of Darwinism -  it corresponds to the principle that there is no Creator, no intelligent foresight. I 262</sru:subject>
        </sru:dc>
      </sru:recordData>
      <sru:recordNumber>1</sru:recordNumber>
    </sru:record>
    <sru:record>
      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>
      <sru:recordPacking>XML</sru:recordPacking>
      <sru:recordData>
        <sru:dc>
          <sru:title>Darwinism</sru:title>
          <sru:creator>McGinn</sru:creator>
          <sru:subject>Design argument/William Paley: organisms have a brilliant design: We have not designed them, so we have to assume that a foreign intelligence did it. Let s call this intelligence "God". So God exists. II 98
DarwinVsPaley: intelligent design does not require a Creator. Selection is sufficient. II 98
Mind/consciousness/evolution/McGinn: evolution does not explain consciousness! nor sensations. II 99
 Reason: sensation and consciousness cannot be explained through the means of Darwinian principles and physics, because if selection were to explain how sensations are supposed to be created by it, it must be possible to mold the mind from matter. II 100
 (s) Consciousness or sensations would have to be visible for selection! (Similar GouldVsDawkins)</sru:subject>
        </sru:dc>
      </sru:recordData>
      <sru:recordNumber>2</sru:recordNumber>
    </sru:record>
    <sru:record>
      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>
      <sru:recordPacking>XML</sru:recordPacking>
      <sru:recordData>
        <sru:dc>
          <sru:title>Darwinism</sru:title>
          <sru:creator>Putnam</sru:creator>
          <sru:subject>Rorty: Darwinism / Putnam: he does noit like the image of man as a more complicated animal  (scientistic and reductionist physicalism).
Rorty VI 63</sru:subject>
        </sru:dc>
      </sru:recordData>
      <sru:recordNumber>3</sru:recordNumber>
    </sru:record>
    <sru:record>
      <sru:recordSchema>info:srw/schema/1/dc-v1.1</sru:recordSchema>
      <sru:recordPacking>XML</sru:recordPacking>
      <sru:recordData>
        <sru:dc>
          <sru:title>Darwinism</sru:title>
          <sru:creator>Rorty</sru:creator>
          <sru:subject>Darwinism/Rorty provides a useful vocabulary. "Darwinism": for me is a fable about human beings as animals with special skills and organs. But these organs and skills are just as little in a representational relation to the world as the anteater s snout. VI 69 ff
Darwinism/Rorty: it demands that we consider our doing and being as part of the same continuum, which also includes the existence of amoebae, spiders and squirrels. One way to do that is to say that our experience is just more complex. VI 424</sru:subject>
        </sru:dc>
      </sru:recordData>
      <sru:recordNumber>4</sru:recordNumber>
    </sru:record>
  </sru:records>
</sru:searchRetrieveResponse>
</pre>
==See also==
* [[Search/Retrieve Web Service]]

==External links==
* [http://www.loc.gov/standards/sru/ Search/Retrieve via URL] at [[Library of Congress]]
* http://www.philosophy-science-humanities-controversies.com/XML/index.php Sample Page from the Lexicon of Arguments.
* http://philosophy-science-humanities-controversies.com/XML/sru.php?version=1.1&operation=searchRetrieve&query=dc.title=Darwinism This is a complete example with query and answer.
{{Internet search}}

{{DEFAULTSORT:Search Retrieve via URL}}
[[Category:Internet search]]
[[Category:Information retrieval techniques]]
[[Category:Uniform Resource Locator]]

{{web-stub}}
<=====doc_Id=====>:545
<=====title=====>:
Notey
<=====text=====>:
{{Orphan|date=August 2016}}

{{Infobox Website
| name           = Notey
| logo           = [[Image:Notey (logo).png|220px]]
| screenshot     = 
| caption        = 
| url            = [http://www.notey.com/ notey.com]
| commercial     = 
| type           = Blog discovery platform
| language       = English
| registration   = 
| owner          = 
| launch date    = February 2015
| current status = active
| revenue        = 
| slogan         = 
| alexa          =  
}}

'''Notey''' is a [[:Category:Blog search engines|blog search]] and discovery platform founded in 2015 that helps users discover non-mainstream content and blogs. The platform ranks and features both bloggers and independent publishers on various topics including [[technology]], [[weddings]], [[sneakers]] and more than 500,000 others. Users can upvote articles they like, save them in notebooks and see what the community is reading.<ref>{{cite web | url=http://techcrunch.com/2015/02/17/notey | title=Notey Raises $1.6 Million For Its Topic-Focused Blog Directory  | publisher=Techcrunch | date=February 17, 2015 | accessdate=2016-01-21}}</ref>

In April 2015, Business Insider named Notey “one of the fastest growing startups in the world still flying under the radar”.<ref>{{cite web | url= http://uk.businessinsider.com/15-of-the-fastest-growing-b2b-startups-2015-4?op=1 | title=15 of the fastest growing startups in the world still flying under the radar  | publisher=Business Insider | date=April 17, 2015 | accessdate=2016-01-21}}</ref>

The company is based in Hong Kong and San Francisco.<ref>{{cite web | url= https://www.crunchbase.com/organization/notey | title=Notey &#124; CrunchBase | year=2015 | publisher=CrunchBase | accessdate=2016-01-21}}</ref> Its investors include [[Hugo Barra]], [[Ryan Holmes]], Shakil Khan and [[Steve Kirsch]].

==References==
{{reflist}}

==External links==
* [http://www.notey.com/ Notey Home Page]

{{Commons category|Internet search engines}}

[[Category:Blog search engines]]
[[Category:Websites|Search engines]]
[[Category:Internet search]]
[[Category:Aggregation websites]]
[[Category:Search engine software]]


{{searchengine-website-stub}}
{{US-company-stub}}
<=====doc_Id=====>:548
<=====title=====>:
User intent
<=====text=====>:
'''User intent''' or '''query intent''' is the identification and categorization of what a user online intended or wanted when they typed their [[web search query|search terms]] into an online [[web search engine]] for the purpose of [[search engine optimization]] or [[conversion rate optimization]].<ref name="Understanding Sponsored Search: Core Elements of Keyword Advertising">{{cite book|last1=Jansen|first1=Jim|title=Understanding Sponsored Search: Core Elements of Keyword Advertising|date=July 2011|publisher=Cambridge University Press|location=New York, NY, USA|isbn=9781107011977|page=44|url=https://books.google.com/books?id=L4LIGyLOwDoC&pg=PA44&dq=what+is+user+intent&hl=en&gl=us&sa=X&redir_esc=y#v=onepage&q=what%20is%20user%20intent&f=false}}</ref> When a user goes online, there is always a purpose, an intent. The goal can be fact-checking, comparison shopping, filling downtime, or any other activity online.<ref name="The Different Types of User Intent">{{cite web|last1=Shih|first1=Joseph|title=The Different Types of User Intent|url=https://www.twinword.com/blog/understanding-different-types-user-intent/|website=Twinword Blog|publisher=Twinword, Inc.|accessdate=26 December 2016}}</ref>

==Types==
Though there are various ways of classifying or naming the categories of the different types of user intent, overall they seem to follow the same clusters. In general and up until the rise and explosion<ref name="The Rise of Mobile Search: From 2012 to 2015">{{cite web|title=The Rise of Mobile Search: From 2012 to 2015|url=http://www.texodesign.com.au/the-rise-of-mobile-search/|website=Texo Design|publisher=Texo Design|accessdate=26 December 2016}}</ref> of [[mobile search]], there are and were [[Web search query#Types|three very broad categories]]: informational, transactional, and navigational.<ref>{{cite journal|last1=Broder|first1=Andrei|title=A Taxonomy of Web Search|journal=SIGIR Forum|date=Fall 2002|volume=36|issue=2|pages=5–6|url=http://www.cis.upenn.edu/~nenkova/Courses/cis430/p3-broder.pdf|accessdate=27 December 2016}}</ref> However over time and with the rise<ref name="The Rise of Mobile Search: From 2012 to 2015" /> of [[mobile search]], other categories have appeared or categories have segmented into more specific categorization. The following is a table showing how different organizations have categorize the different types.

{| class="wikitable" style="text-align: center;"
|+ style="text-align: left;" | The Different Types of User Intents<ref name="The Different Types of User Intent" />
|-
! !! Type 1 !! colspan="2" | Type 2 (a/b) !! Type 3 !! Type 4
|-
| || "who wrote the Matrix" || "online IQ test" || "office supplies" || "google play store" || "restaurants near me"
|-
| [[Microsoft]]<ref>{{cite journal|last1=KhudaBukhsh|first1=Ashiqur|last2=Bennett|first2=Paul|last3=White|first3=Ryen|title=Building Effective Query Classifiers: A Case Study in Self-harm Intent Detection|journal=CIKM '15 Proceedings of the 24th ACM International on Conference on Information and Knowledge Management|date=2015|pages=1735–1738|url=http://research.microsoft.com/en-us/um/people/pauben/papers/cikm-2015-KhudaBukhsh-et-al.pdf|accessdate=26 December 2016|format=PDF}}</ref> || Informational || colspan="2" | Transactional || Navigational || --
|-
| [[Google]]<ref>{{cite book|title=Search Quality Evaluator Guidelines|date=28 March 2016|publisher=Google|pages=61–74|url=http://static.googleusercontent.com/media/www.google.com/en//insidesearch/howsearchworks/assets/searchqualityevaluatorguidelines.pdf|accessdate=26 December 2016}}</ref> || Know || colspan="2" | Do || Website || Visit-in-person
|-
| [[Hubspot]]<ref>{{cite web|title=Keyword Development: Without a computer!|url=https://cdn2.hubspot.net/hub/137828/file-331703896-pdf/docs/hubspot_keyword_development_worksheet.pdf|website=Hubspot|publisher=Hubspot|accessdate=26 December 2016}}</ref> || Problem based || colspan="2" | Solution based || Brand based || --
|-
| [[SEMRush]]<ref>{{cite web|title=Types of keywords: commercial, informational, navigational, transactional|url=https://www.semrush.com/blog/types-of-keywords-commercial-informational-navigational-transactional/|website=SEMRush Blog|publisher=SEMRush|accessdate=26 December 2016}}</ref> || Informational || Commercial || Transactional || Navigational || --
|-
| [[Web Analytics World]]<ref>{{cite web|last1=Levitt|first1=Dean|title=Using Intent, Demographics and Micro-Moments to Better Understand your Web Traffic|url=http://www.webanalyticsworld.net/2016/09/intent-demographics-and-micro-moments-in-analytics.html|website=Web Analytics World|publisher=Jump Digital|accessdate=26 December 2016}}</ref> || Know || Do || Buy || -- || Go
|-
| Summary || Know || Do || Buy || Web || Local
|}

* Know - An informational search query looking for facts or other information (e.g. "who wrote the Matrix")
* Do - A transactional search query wanting to fulfill a task online (e.g. "online IQ test")
* Buy - A transactional search query wanting to buy something (e.g. "office supplies")
* Web - A navigational search query wanting to visit to a specific web site or page (e.g. "google play store")
* Local - A search query wanting to visit-in-person a physical location (e.g. "restaurants near me")

Please note that many search queries may be ambiguous and thus may be classified into multiple intents. For example, a user who typed a query "matrix" into a search bar may want to purchase the [[The Matrix|1999 American-Australian philosophical sci-fi film]] or may want to learn more about the [[Matrix (mathematics)|matrices in mathematics]].

==Importance==
With the prevalence of search engines being the first starting point of many online sessions,<ref>{{cite web|last1=Purcell|first1=Kristen|title=Search and email still top the list of most popular online activities|url=https://searchenginewatch.com/sew/study/2101282/search-engines-92-adult-internet-users-study|website=Pew Research Center Internet, Science & Tech|publisher=Pew Research Center|accessdate=26 December 2016}}</ref> search engines are tasked with surfacing the [[Search engine results page|best results]] or best [[Online advertising|ads]] that will satisfy the various user intents. Because [[Web search engine#Search engine bias|search engines do not actually read and understand]] web pages and ad copy completely, [[Digital marketing|digital marketers]] have to align their [[Keyword research|target keywords]] to the correct user intent that they are trying to satisfy<ref name="The Different Types of User Intent" /> if they want to rank high on [[Search engine result page|SERPs]] and improve their [[Conversion rate optimization|conversion rate]].

Take for example, a company selling colored contact lenses who wants their ad to show up for relevant searches may target the keyword "blue eyes". However, this may not be the most effective strategy as users who search "blue eyes" may want to learn biological facts about blue eyes. Instead, the company can target keywords that clearly indicates that the user is looking to buy colored contact lenses (i.e. "blue contact lenses" most likely implies "buy blue contact lenses"). With the correct keyword intent targeting, studies have shown that conversion rates increase significantly.<ref>{{cite web|last1=daSilva|first1=Tiffany|title=Why Ignoring User Intent is Costing You Money in AdWords|url=http://unbounce.com/ppc/ignoring-user-intent-costs-you-money-in-adwords/|website=unbounce Pay Per Click|publisher=unbounce|accessdate=26 December 2016}}</ref>

==See also==
* [[Web search query]]
* [[Keyword research]]
* [[Intent marketing]]
* [[Search engine optimization]]
* [[Conversion rate optimization]]
* [[Search engine result page]]
* [[Principle of least astonishment]]

==References==
{{reflist}}

[[Category:Internet search]]
<=====doc_Id=====>:551
<=====title=====>:
ZyLAB Technologies
<=====text=====>:
{{advert|date=April 2012}}
{{Infobox company |
  name   = ZyLAB |
  logo   = <!--  Commented out because image was deleted: [[Image:zylab logo.jpg|center]] --> |
  slogan = "eDiscovery & Information Risk Management" |
  type   = Private |
  foundation     = 1983 |
  location       = [[McLean, Virginia]]<br>[[Amsterdam]] |
  key_people     = [[Pieter Varkevisser]], president & CEO<br>[[Dr. Johannes C. Scholtes]], chairman & chief strategy officer | Mary Mack, Enterprise Technology Counsel
  num_employees  = 140 |
  industry       = [[Software]], eDiscovery and Information Risk Management, Records Management, Email Archiving, SharePoint Archiving |
  products       = ZyLAB Information Management Platform and various bundles for eDiscovery, email & SharePoint archiving, text-analytics, visualization, contract management, and workflow. |

  homepage       = [http://www.zylab.com/ www.zylab.com]
}}

'''ZyLAB''' is a developer of software for [[Electronic discovery|e-discovery]], information risk management, email management, records, contract, and document management, knowledge management, and workflow. The company is headquartered in [[McLean, Virginia]] and in [[Amsterdam]], [[Netherlands]]. ZyLAB’s most important products are ZyLAB eDiscovery & Production System, the ZyLAB Information Management Platform and bundles that build systems for deployments.

== History ==
In 1983 ZyLAB was the first company providing a [[Full text search|full-text]] search program for electronic files stored in the file system of [[IBM PC compatible|IBM-compatible PCs]]. The program was called ZyINDEX. The first version of ZyINDEX was written in [[Pascal (programming language)|Pascal]] and worked on [[MS-DOS]]. Subsequent programs were written in [[C (programming language)|C]], [[C++]] and [[C Sharp (programming language)|C#]] and work on a variety of Microsoft operating systems.

In 1991, ZyLAB integrated ZyINDEX with an optical character recognition ([[Optical character recognition|OCR]]) program, Calera Wordscan, which was a spin-off from [[Raymond Kurzweil]]’s first OCR implementation. This integration was called ZyIMAGE. ZyIMAGE was the first PC program to include a [[Fuzzy string searching|fuzzy string search]] algorithm to overcome scanning and OCR errors.

In 1998, the company developed support to full-text search email, including attachments.

In 2000, ZyLAB embraced the new [[XML]] standard and created a full content management and records management system based on the XML standard and build a full solution for e-discovery, historical archives, records management, document management, email archiving, contract management, and professional back-office solutions.

In 2003, the company invested in expanding the ZyIMAGE product suite with advanced [[text analytics]], [[text mining]], [[data visualization]], [[computational linguistics]], and [[Machine translation|automatic translation]].

2005: ZyIMAGE Information Access Platform was released, an integrated solution to address information access problems.

Platforms for ZyIMAGE e-Discovery and legal production, historical archiving, compliance, back-office records management and [[COMINT#COMINT|COMINT]] were launched in 2007.

2010: ZyLAB Information Management Platform was released, an integrated solution to address e-Discovery and information management problems.

==Customers==
Initial customers of ZyINDEX were organizations such as the [[FBI]] and other law enforcement agencies to investigate electronic data from seized PCs, the [[United States Navy|U.S. Navy]] for on-board manuals, and law firms around the world for [[Electronic discovery|e-Discovery]]. Over the years, ZyLAB received grants from the European Union (DG13).

Other well-known ZyLAB customers were [[O. J. Simpson murder case|O.J. Simpson's defense team]], war crime tribunals such as the [[trial of Slobodan Milošević]], the [[Special Court for Sierra Leone]], the [[Extraordinary Chambers in the Courts of Cambodia|UN-AKRT-ECCC Cambodia Khmer Rouge trials]] and the [[International Criminal Tribunal for Rwanda|Rwanda tribunal]]. In 2007, the U.S. [[Executive Office of the President of the United States|Executive Office of the President]] selected ZyLAB for email archiving, basically for its open XML structures, which is endorsed by organizations such as the [[National Archives and Records Administration]]. ZyLAB’s software was used for many other high-profile investigations such as the [[Oklahoma City bombing]].

Public websites also use the ZyLAB Webserver.

[[Gartner]] positioned ZyLAB in the "Leaders" quadrant in its 2007, 2008 and 2009 Magic Quadrant for Information Access Solutions, gave it a strong positive rating in its 2007, 2008 and 2009 e-Discovery Marketscope and a Positive Rating in its 2007 and 2008 Records Management MarketScope.

ZyLAB’s chief strategy officer, Dr. Johannes C. Scholtes, is professor in [[text mining]] at [[Maastricht University|the University of Maastricht]] faculty of Humanities and Sciences and director in the board of AIIM.

==System overview and compatibility==
According to the company’s website it delivers systems for deployments, product bundles and the core components is the ZyLAB Information Management platform include:

Systems:
*ZyLAB eDiscovery and Production
*ZyLAB Compliance and Litigation readiness
*ZyLAB Law Enforcement and Investigations
*ZyLAB Communications Intelligence
*ZyLAB Digital Print and Media Archiving
*ZyLAB Enterprise Information Management

Bundles:
*E-Mail Archiving Bundle
*Microsoft SharePoint Bundle
*Analytics Bundle
*eDiscovery EDRM Processing bundle
*DoD and Sox Compliant RMA Bundle
*TIFF Archiving and Production Bundle
*WebPublishing Bundle
*Commercial Publishing Bundle
*Business Process Automation Bundle
*Development and Integrators Bundle
*Scanning Bundle
*Digital Copier Bundle
*Professional Text Mining
*Machine translation

===Supported configurations===
*'''Server OS''': Windows 2003, Windows 2008
*'''Databases''': XML, MS SQL Server 2005, MS SQL Server 2008, Oracle 10g, Oracle 11g, mySQL
*'''Web Servers''': IIS
*'''Client OS''': Windows XP, Windows Vista, Windows 7
*'''Clustering''': Support for Active/Passive Failover.
*'''Authentication''': Active Directory, LDAP, XML, NTFS, IBM Tripoli.
*'''Virtualization''': VMware Infrastructure, VMware Workstation, VMware Server, VMware Fusion.

===Languages supported===
*'''Unicode'''. Support for documents in all languages.
*'''Internationalization'''. ZyLAB offers translated products for English, German, French, Dutch, Spanish, Italian, Danish, Swedish, Norwegian, Finnish, Portuguese, Arabic and [[Persian language|Persian]]. In addition to these languages, over 400 languages are supported by ZyLAB's recognition and full-text indexing technology, including all Western-European, Eastern European, Baltic, African, Asian and South American languages. ZyLAB's technical ability for broad language and character recognition enhances the accuracy of stored information searches and helps diminish the costs incurred by incorrect searches or text correction.

==Zy-IMAGE-nation Annual Conference==
The annual Zy-IMAGE-nation Conference is sponsored by ZyLAB. During this conference, seminars and interactive sessions from leading professionals about the advanced technologies and procedural enhancements that are driving new levels of operational efficiency in private and public sectors. The focus of the conference is on technologies that provide integrated capabilities for managing the accumulated knowledge of an organization, especially records and e-mail, as well as other business-critical processes. Related topics to be covered include best practices for e-discovery preparation and implementation, records management, email archiving, and knowledge management.

==See also==
* [[Electronic discovery|e-Discovery]]
* [[Optical character recognition|Optical Character Recognition (OCR)]]
* [[Document Imaging]]
* [[E-mail archiving|E-mail Archiving]]
* [[Knowledge Management]]
* [[Document management system|Document Management (System)]]
* [[Enterprise content management|Enterprise Content Management]]
* [[Records management|Records Management]]
* [[Contract management|Contract Management]]
* [[Workflow]]
* [[Text mining|Text Mining]]
* [[Text analytics|Text Analytics]]
* [[Machine translation|Automatic Machine Translation]]
* [[Data visualization|Data Visualization]]

==References==
{{Reflist}}
*[http://www.pcmag.com/encyclopedia_term/0,,t=zyindex&i=55248,00.asp Definition of ZyINDEX] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[http://www.pcmag.com/encyclopedia_term/0,2542,t=ZyIMAGE&i=55247,00.asp Definition of ZyIMAGE] in [[PC Magazine|''PCMAG.com'']]'s encyclopedia
*[https://web.archive.org/web/20090108050805/http://www.informationweek.com/777/knowledge.htm Review] of ZyImage 3.0 in ''[[InformationWeek]]''
*[http://www.accessmylibrary.com/coms2/summary_0286-9201794_ITM Mac version of ZyINDEX made its debut on Comdex]
*[http://query.nytimes.com/gst/fullpage.html?res=940DE6DA1730F93AA35751C0A96E948260 Review] of ZyINDEX in the ''[[New York Times]]''
*[https://web.archive.org/web/20090224230803/http://www.computerwoche.de/heftarchiv/1988/26/1155611/ Review] of ZyINDEX on ''Computerwoche.de'' (article in German)
*[https://web.archive.org/web/20110717125042/http://www.computerwoche.de/index.cfm?pid=2123&pk=1096333 Review] of ZyIMAGE's webserver on ''Computerwoche.de'' (article in German)
*[http://nl.newsbank.com/nl-search/we/Archives?p_product=MH&s_site=miami&p_multi=MH&p_theme=realcities&p_action=search&p_maxdocs=200&p_topdoc=1&p_text_direct-0=0EB367D56736E685&p_field_direct-0=document_id&p_perpage=10&p_sort=YMD_date: Review] of ZyINDEX in the ''[[Miami Herald]]''
*[http://www.usdoj.gov/oig/special/0203/chapter3.htm ZyINDEX used in the Investigation of the Belated Production of Documents in the Oklahoma City Bombing Case]
*[http://www.fcw.com/print/6_31/news/70014-1.html Review]{{dead link|date=July 2016 |bot=InternetArchiveBot |fix-attempted=yes }} of ZyIMAGE on ''Federal Computer Week (FCW.com)''
*Zylab retrieval engine optimized for CD-ROM; Zylab, Progressive Technologies merge," Seybold Report on Desktop Publishing. vol. 8, No. 10, Jun. 6, 1994, p. 40.
*Knibbe, "ZyImage 2 boosts, OCR, batch duties," InfoWorld, vol. 15, Issue 51, Dec. 20, 1993, p.&nbsp;20.
*Knibbe, "ZyImage 3.0 will facilitate distribution on CD-ROMs; Boasts integration with WordScan OCR software," InfoWorld, vol. 16, No. 38, Sep. 19, 1994, p.&nbsp;22.
*Marshall, "Text retrieval alternatives: 10 more ways to pinpoint important information," Infoworld, vol. 14, No. 12, Mar. 23, 1992, pp.&nbsp;88–89.
*Marshall, "ZyImage adds scanning access to ZyIndex," InfoWorld, vol. 16, No. 15, Apr. 11, 1994, pp.&nbsp;73, 76, and 77.
*Marshall, "ZyImage is ZyIndex plus a scan interface integrated," InfoWorld. vol. 15, Issue 10, Mar. 8, 1993, p.&nbsp;100.
*Marshall et al., "ZyIndex for Windows, Version 5.0," InfoWorld, v. 15, n. 21, May 1993, pp.&nbsp;127, 129, 133 and 137.
*Simon, "ZyImage: A Winning Combination of OCR And Text Indexing," PC Magazine. vol. 12, No. 6, Mar. 30, 1993, p.&nbsp;56.
*Rooney, "Text-retrieval veterans prepare Windows attack," PC Week, v. 9, n. 24, Jun. 1992, p.&nbsp;46.
*Rooney, "ZyLab partners with Calera: firms roll out document-image system," PC Week, vol. 10, No. 3, Jan. 25, 1993, p.&nbsp;22.
*Torgan, "ZyImage: Document Imaging and Retrieval System," PC Magazine. vol. 12, No. 3, Feb. 9, 1993, p.&nbsp;62.

===Gartner reports===
*Introduction to Investigative Case Management Products (18 April 2007)
*Hype Cycle for Legal and Regulatory Information Governance, 2007 (16 July 2007)
*MarketScope for Contract Management, 2007 (16 July 2007)
*Choosing an E-Discovery Solution in 2007 and 2008 (18 July 2007)
*Magic Quadrant for Information Access Technology, 2007 (5 September 2007)
*Magic Quadrant for Information Access Technology, 2008
*Magic Quadrant for Information Access Technology, 2009
*The Expanding Enterprise E-Discovery Marketplace (12 November 2007)
*MarketScope for E-Discovery and Litigation Support Vendors, 2007 (14 December 2007)
*MarketScope for E-Discovery Product Vendors, 2008
*MarketScope for E-Discovery Product Vendors, 2009
*MarketScope for Records Management (20 May 2008)
*Hype Cycle for Content Management, 2008 (8 July 2008)
*Using the Electronic Discovery Reference Model to Identify, Collect and Preserve Digital Evidence (11 July 2008)
*Using the Electronic Discovery Reference Model to Process, Review and Analyze Digital Evidence (11 July 2008)
*Hype Cycle for Governance, Risk and Compliance Technologies, 2009 (17 July 2009)

==External links==
*[http://www.zylab.com/ ZyLAB official website]
*[http://www.edrm.net/ The Electronic Discovery Reference Model (EDRM)]
*[http://www.aiim.org/ AIIM]

[[Category:Companies established in 1983]]
[[Category:Software companies of the United States]]
[[Category:Information retrieval organizations]]
<=====doc_Id=====>:554
<=====title=====>:
International Society for Music Information Retrieval
<=====text=====>:
{{Infobox non-profit
| Non-profit_name   = The International Society for Music Information Retrieval
| Non-profit_logo   = [[Image:LogoInternationalSocietyMIR.png|250px]]
| Non-profit_type   = Non-profit organization
| founded_date      = 2008
| founder           = 
| location          = [[Canada]]
| origins           = International Symposium for Music Information Retrieval
| key_people        = 
| area_served       = Worldwide
| focus             = [[Music information retrieval|Music Information Retrieval (MIR)]]
| method            = Conferences, publications
| revenue           = 
| endowment         = 
| num_volunteers    = 
| num_employees     = 
| num_members       = 
| owner             = 
| Non-profit_slogan = The world's leading research forum on processing, searching, organising and accessing music-related data
| homepage          = {{URL|http://www.ismir.net/}}
| tax_exempt        = 
| dissolved         = 
| footnotes         = 
}}

The '''[http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]''' is an international forum for research on the organization of music-related data. It started as an informal group steered by an ''ad hoc'' committee in 2000<ref>[http://www.ismir.net/texts/Byrd02.html Donald Byrd and Michael Fingerhut: ''The History of ISMIR - A Short Happy Tale''. D-Lib Magazine, Vol. 8 No. 11], {{ISSN|1082-9873}}.</ref> which established a yearly symposium - whence "ISMIR", which meant '''International Symposium on Music Information Retrieval'''. It was turned into a conference in 2002 while retaining the acronym. ISMIR was incorporated in Canada on July 4, 2008.<ref>[http://www.ismir.net/ISMIR-Letters-Patent.pdf ISMIR Letters Patent. Canada, July 4, 2008.]</ref>

==Purpose==
Given the tremendous growth of digital music and music metadata in recent years, methods for effectively extracting, searching, and organizing music information have received widespread interest from academia and the information and entertainment industries. The purpose of ISMIR is to provide a venue for the exchange of news, ideas, and results through the presentation of original theoretical or practical work. By bringing together researchers and developers, educators and librarians, students and professional users, all working in fields that contribute to this multidisciplinary domain, the conference also serves as a discussion forum, provides introductory and in-depth information on specific domains, and showcases current products.

As the term [[Music Information Retrieval|Music Information Retrieval (MIR)]]  indicates, this research is motivated by the desire to provide music lovers, music professionals and music industry with robust, effective and usable methods and tools to help them locate, retrieve and experience the music they wish to have access to. MIR is a truly interdisciplinary area, involving researchers from the disciplines of musicology, cognitive science, library and information science, computer science, electrical engineering and many others.

==Annual conferences==
Since its inception in 2000, ISMIR has been the world’s leading forum for research on the modelling, creation, searching, processing and use of musical data. Researchers across the globe meet at the annual conference conducted by the society. It is known by the same acronym as the society, ISMIR. Following is the list of previous conferences held by the society.

* ISMIR 2019, Delft (The Netherlands)
* ISMIR 2018, Paris (France)
* ISMIR 2017, Suzhou (China)
* [http://ismir2016.ismir.net ISMIR 2016], 8–12 August 2016, New York City (USA) [http://dblp.uni-trier.de/db/conf/ismir/ismir2016.html proceedings]
* [http://ismir2015.ismir.net ISMIR 2015], 26–30 October 2015, Malaga (Spain) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2015.html proceedings]
* [http://ismir2014.ismir.net ISMIR 2014], 27–31 October 2014, Taipei (Taiwan) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2014.html proceedings]
* [http://ismir2013.ismir.net ISMIR 2013], 4–8 November 2013, Curitiba (Brazil) [http://www.informatik.uni-trier.de/~ley/db/conf/ismir/ismir2013.html proceedings]
* [http://ismir2012.ismir.net ISMIR 2012], 8–12 October 2012, Porto (Portugal) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2012'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2011.ismir.net ISMIR 2011], 24–28 October 2011, Miami (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2011'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2010.ismir.net ISMIR 2010], 9–13 August 2010, Utrecht (The Netherlands) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2010'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2009.ismir.net ISMIR 2009], 26–30 October 2009, Kobe (Japan) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2009'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2008.ismir.net ISMIR 2008], 14–18 September 2008, Philadelphia (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2008'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2007.ismir.net ISMIR 2007], 23–30 September 2007, Vienna (Austria) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2007'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2006.ismir.net ISMIR 2006], 8–12 October 2006, Victoria, BC (Canada) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2006'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2005.ismir.net ISMIR 2005], 11–15 September 2005, London (UK) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2005'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2004.ismir.net ISMIR 2004], 10–15 October 2004, Barcelona (Spain) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2004'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2003.ismir.net ISMIR 2003], 26–30 October 2003, Baltimore, Maryland (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2003'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2002.ismir.net ISMIR 2002], 13–17 October 2002, Paris (France) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2002'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2001.ismir.net ISMIR 2001], 15–17 October 2001, Bloomington, Indiana (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='2001'&page=0&order=Authors&order_type=ASC proceedings]
* [http://ismir2000.ismir.net ISMIR 2000], 23–25 October 2000, Plymouth, Massachusetts (USA) [http://www.ismir.net/proceedings/index.php?table_name=papers&function=search&where_clause=`papers`.`Year`='200'&page=0&order=Authors&order_type=ASC proceedings]

The [http://www.ismir.net/ official webpage] provides a more up-to-date information on past and future conferences and provides access to all past websites and to the [http://www.ismir.net/proceedings cumulative database] of all papers, posters and tutorials presented at these conferences. An overview of all papers published at ISMIR can be found at [http://dblp.uni-trier.de/db/conf/ismir/index.html DBLP].

==Research areas and topics==
The following list gives an overview of the main research areas and topics that are within the scope of 
[[Music Information Retrieval]].

===MIR data and fundamentals===
*    music signal processing
*    symbolic music processing
*    metadata, linked data and semantic web
*    social tags and user generated data
*    natural language processing, text and web mining
*    multi-modal approaches to MIR

===Methodology===
*    methodological issues and philosophical foundations
*    evaluation methodology
*    corpus creation
*    legal, social and ethical issues

===Domain knowledge===
*    representation of musical knowledge and meaning
*    music perception and cognition
*    computational music theory
*    computational musicology and ethnomusicology

===Musical features and properties===
*    melody and motives
*    harmony, chords and tonality
*    rhythm, beat, tempo
*    structure, segmentation and form
*    timbre, instrumentation and voice
*    musical style and genre
*    musical affect, emotion and mood
*    expression and performative aspects of music

===Music processing===
*    sound source separation
*    music transcription and annotation
*    optical music recognition
*    alignment, synchronization and score following
*    music summarization
*    music synthesis and transformation
*    fingerprinting
*    automatic classification
*    indexing and querying
*    pattern matching and detection
*    similarity metrics

===Application===
*    user behavior and modelling
*    user interfaces and interaction
*    digital libraries and archives
*    music retrieval systems
*    music recommendation and playlist generation
*    music and health, well-being and therapy
*    music training and education
*    MIR applications in music composition, performance and production
*    music and gaming
*    MIR in business and marketing

==MIREX==
The ''Music Information Retrieval Evaluation eXchange'' (MIREX) is an annual evaluation campaign for MIR algorithms, coupled to the ISMIR conference. Since it started in 2005, MIREX has fostered advancements both in specific areas of MIR and in the general understanding of how MIR systems and algorithms are to be evaluated.<ref name=DownieEBJ10>
{{citation 
|author1=J. Stephen Downie |author2=Andreas F. Ehmann |author3=Mert Bay |author4=M. Cameron Jones |title=The Music Information Retrieval Evaluation eXchange: Some Observations and Insights
|journal=Advances in Music Information Retrieval, Springer
|year=2010
|pages=93–115
|doi=10.1007/978-3-642-11674-2_5}}
</ref><ref name=DownieEEV05_ISMIR>
{{cite journal
|last1=Downie
|first1=J. Stephen
|last2=West
|first2=Kris 
|last3=Ehmann
|first3=Andreas F.
|last4=Vincent
|first4=Emmanuel
|title=The 2005 Music Information retrieval Evaluation Exchange (MIREX 2005): Preliminary Overview
|journal=Proceedings of the International Conference on Music Information Retrieval
|year=2005
|pages=320–323}}
</ref> MIREX is to the MIR community what the [[Text Retrieval Conference]] (TREC) is to the text information retrieval community: A set of community-defined formal evaluations through which a wide variety of state-of-the-art systems, algorithms and techniques are evaluated under controlled conditions. MIREX is managed by the International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) at the University of Illinois at Urbana-Champaign (UIUC).<ref name="DownieIMIRSEL">{{cite web|last1=Downie|first1=J. Stephen|title=The International Music Information Retrieval Systems Evaluation Laboratory (IMIRSEL) Project|url=http://www.music-ir.org/evaluation/|publisher=University of Illinois|accessdate=22 April 2016}}</ref>

==Related conferences==
* [[ACM Multimedia]]
* [[International Computer Music Conference|International Computer Music Conference (ICMC)]]
* [[International Conference on Acoustics, Speech, and Signal Processing|International Conference on Acoustics, Speech, and Signal Processing (ICASSP)]]
* [[International Conference on Digital Audio Effects|International Conference on Digital Audio Effects (DAFx)]]
* [[New Interfaces for Musical Expression|International Conference on New Interfaces for Musical Expression (NIME)]]
* International Symposium on Computer Music Modeling and Retrieval (CMMR)
* [[Sound and Music Computing Conference|Sound and Music Computing Conference (SMC)]]

==Related journals==
* [[Computer Music Journal|Computer Music Journal (CMJ)]]
* [http://asmp.eurasipjournals.springeropen.com/ EURASIP Journal on Audio, Speech, and Music Processing]
* [http://www.signalprocessingsociety.org/publications/periodicals/taslp/ IEEE/ACM Transactions on Audio, Speech, and Language Processing (TASLP)]
* [http://www.signalprocessingsociety.org/tmm/ IEEE Transactions on Multimedia (TMM)]
* [http://mp.ucpress.edu/ Music Perception]
* [[Journal of New Music Research|Journal of New Music Research (JNMR)]]

==Further links==
* [[Audio Engineering Society]]
* [http://www.signalprocessingsociety.org/technical-committees/list/audio-tc/ Audio and Acoustic Signal Processing]
* [[Music Technology]]
* [http://www.ismir.net International Society for Music Information Retrieval (ISMIR)]
* [[Sound and music computing|Sound and Music Computing]]

==References==
{{Reflist}}

[[Category:Music information retrieval]]
[[Category:Computer science conferences]]
[[Category:Music technology]]
[[Category:Multimedia]]
[[Category:Information retrieval organizations]]
[[Category:Music search engines]]
<=====doc_Id=====>:557
<=====title=====>:
DtSearch
<=====text=====>:
{{Lowercase}}

{{Infobox company |
  name   = dtSearch Corp. |
  slogan = "The Smart Choice for Text Retrieval since 1991" |
  type   =  Private company |
  foundation     = 1991 |
  location       = [[Bethesda, Maryland|Bethesda]], [[Maryland]], [[United States|US]] |
  key_people     = David Thede, President |
  industry       = [[Software]] |

  homepage       = [http://www.dtsearch.com/ www.dtsearch.com]
}}

'''dtSearch Corp.''' is a [[software company]] which specializes in [[text retrieval]] software. It was founded in 1991, and is headquartered in [[Bethesda, Maryland|Bethesda]], [[Maryland]]. Its current range of software includes products for enterprise [[desktop search]], Intranet/Internet [[spidering]] and search, and [[search engines]] for developers ([[Software development kit|SDK]]) to integrate into other software applications.

==History==
dtSearch Corp was founded by David Thede;<ref>[http://www.lets-talk-computers.com/guests/dtsearch/6.2/index.htm Lets talk computers - Interview May 31, 2003]</ref><ref>[https://www.google.com/patents/US6782380 Method and system for indexing and searching contents of extensible mark-up language(XML) documents US 6782380 B1]</ref><ref>[https://www.google.com/patents/US7464098 Method for rapidly searching elements or attributes or for rapidly filtering fragments in binary representations of structured, for example, XML-based documents US 7464098 B2]</ref> the company started research and development in text retrieval in 1988  and incorporated in Virginia in 1991 as D T Software. Marketing of dtSearch 1.0 a DOS Text Retrieval software product began in the first quarter of 1991. Initially it was distributed as [[Association of Shareware Professionals]]-approved [[shareware]]. The product was featured in an article entitled "Text Retrieval Software" in an early edition of ''[[PC Magazine]]''<ref>"Text Retrieval Software". (July 1992). [[PC Magazine]] (UK ed)</ref> as a shareware alternative to the commercial products reviewed; these included [[ISYS Search Software|ISYS]], [[ZyLAB Technologies|ZyIndex]], Strix, [[askSam]], [[ideaList]], Assassin PC, [[Folio Corporation|Folio Views]] and Lotus SmartText.

In the first few years after its initial release, dtSearch was an end-user application only. Then, in 1994, [[Symantec]] approached dtSearch about including its search technology into one of the first applications for 32-bit Windows; the dtSearch end-user application was developed into a [[Dynamic-link library]] (DLL) which Symantec embedded in Norton Navigator, which was released alongside Microsoft’s initial release of its 32-bit Windows operating system, [[Windows 95]].<ref>[http://www.processor.com/editorial/article.asp?article=articles%2Fp3012%2F11p12%2F11p12.asp dtSearch Performs Incredible Feats. Processor Mag. March 21, 2008]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref>

In 2007 the company was listed in the [[EContent]] 100 list, a list of companies that matter most in the digital content industry.<ref>[http://www.econtentmag.com/Articles/ArticleReader.aspx?ArticleID=40160&PageNum=22007 EContent 100 list]</ref>

==Products==
The current (v 7.7) product range is [[Unicode]]-based and has an index that can handle over 1 [[terabyte|TB]] of data per index.

*dtSearch Desktop with Spider -  Windows client Desktop search software (32 and 64 bit indexers)
*dtSearch Network with Spider -  as dtSearch Desktop but licensed for Network use (32 and 64 bit indexers)
*dtSearch Web with Spider -  browser based search-only client for Intranet/Internet usage based on Microsoft IIS (32 and 64 bit indexers)
*dtSearch Engine with Spider - SDK with C++, .NET, COM, Java, Delphi APIs (32-bit and 64-bit versions)
*dtSearch Engine for Linux - SDK with C++ and Java APIs
*dtSearch Publish <ref>[http://www.law.com/jsp/lawtechnologynews/PubArticleLTN.jsp?id=1202463957873&slreturn=1&hbxlogin=1 dtSearch Publish for EDD Production Law Technology News July 29, 2010]</ref> - a search front-end for CD\DVD publishing (32 and 64 bit indexers)

==Licensing Partners==
* COMPANY:  PRODUCT
* Docupoint, LLC:  DrawingSearcher
* FileHold Systems Inc.: FileHold Document Management System
* ...

==See also==
* [[Enterprise search]]
* [[List of enterprise search vendors]]

==References==
{{Reflist}}

==External links==
*[http://www.dtsearch.com/ Company Website]
*[http://www.searchtools.com/tools/dtsearch.html Product description on SearchTools.com ]
*[http://www.windowsitpro.com/article/desktop-management/dtsearch-7-desktop-with-spider.aspx The index is mightier than the sword - Windows IT Pro. August 27, 2008]
*[http://www.infoworld.com/t/platforms/desktop-search-gets-down-business-610 Desktop search gets down to business - InfoWorld. September 01, 2005]
*[http://www.ncbi.nlm.nih.gov/pmc/articles/PMC150357/ Integrating Query of Relational and Textual Data in Clinical Databases - J Am Med Inform Assoc. 2003 Jan–Feb]
*[http://radiographics.rsna.org/content/29/5/1233.full.pdf Informatics in Radiology. Render: An Online Searchable Radiology Study Repository - RadioGraphics 2009; 29:1233–1246] 
*[http://jms.ndmctsgh.edu.tw/fdarticlee%5C2606199.pdf Use Of Intelligent Computer Search for the Patterns of Abnormal Lymphatic Uptake by F-18 FDG PET in Primary Lung Cancers - J Med Sci 2006;26(6):199-204]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}

{{DEFAULTSORT:Dtsearch Corp.}}
[[Category:Desktop search engines]]
[[Category:Information retrieval organizations]]
[[Category:Software companies based in Maryland]]
<=====doc_Id=====>:560
<=====title=====>:
Concept Searching Limited
<=====text=====>:
{{Infobox company |
  name   = Concept Searching Limited |
  logo = [[Image:conceptSearching.jpg]] |
  slogan = "Retrieval Just Got Smarter" |
  type   =  [[Privately held company|Private]] |
  foundation     = 2002|
  location       = UK, United States |
  area_served    = Global |
  industry       = [[Information retrieval]] |
  products       = conceptSearch<br/>conceptClassifier<br/>conceptClassifier for SharePoint<br/>conceptClassifier for SharePoint Online<br/>Taxonomy Manager<br/>Taxonomy Workflow |
  homepage       = [http://www.conceptsearching.com/ www.conceptsearching.com]
}}

'''Concept Searching Limited''' is a [[software company]] which specializes in [[information retrieval]] software. It has products for [[Enterprise search]], Taxonomy Management and  [[Statistical classification]].

==History==
Concept Searching was founded in 2002 in the UK and now has offices in the USA and South Africa. In August 2003 the company introduced the idea of using [[Compound term processing]].<ref>[http://direct.bl.uk/bld/PlaceOrder.do?UIN=138451913&ETOC=RN Lateral thinking in information retrieval] ''Information Management and Technology.'' 2003. vol 36; part 4, pp 169-173</ref><ref>[http://www.conceptsearching.com/Web/UserFiles/File/Concept%20Searching%20Lateral%20Thinking.pdf] Lateral Thinking in Information Retrieval</ref>

Compound term processing allows statistical information retrieval applications to perform matching using multi-word concepts. This can improve the quality of search results and also allows unstructured information to be automatically classified with semantic metadata.<ref>[http://airforcemedicine.afms.mil/711hswom/InterSymp2008/AFMS%20-%20InterSymp%202008.html] US Air Force Medical Service presentation at InterSymp-2008</ref>

The company's products run on the Microsoft [[.NET Framework|.NET]] platform. The products integrate with Microsoft [[SharePoint]] and many other platforms.<ref>[http://pinpoint.microsoft.com/en-US/partners/Concept-Searching-Inc-4297066101] Microsoft Partner Profile</ref>

Concept Searching has developed the '''Smart Content Framework''', which is a toolset that provides an enterprise framework to mitigate risk, automate processes, manage information, protect privacy, and address compliance issues. The Smart Content Framework is used by many large organizations including 23,000 users at the [[NASA]] Safety Center <ref>[http://www.aiim.org/About/News/CS-NASA-Safety] NASA Safety Center using Smart Content Framework</ref>

== Awards ==
* 100 Companies that Matter in Knowledge Management 2009/2010/2011/2012/2013/2014/2015 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-100-COMPANIES-That-Matter-in-Knowledge-Management-102189.aspx |title=KMWorld Magazine}}</ref>
* KMWorld Trend-Setting Products of 2009/2010/2011/2012/2013/2014/2015 <ref>{{cite web |url=http://www.kmworld.com/Articles/Editorial/Features/KMWorld-Trend-Setting-Products-of-2015-105783.aspx |title=Trend-Setting Products}}</ref>

==See also==
* [[Compound term processing]]
* [[Enterprise search]]
* [[Full text search]]
* [[Information retrieval]]
* [[Concept Search]]

==References==
{{Reflist}}

==External links==
*[http://www.conceptsearching.com/ Company Website]

[[Category:Information retrieval organizations]]
[[Category:Privately held companies of the United Kingdom]]
<=====doc_Id=====>:563
<=====title=====>:
Category:Yandex
<=====text=====>:
{{commonscat|Yandex}}
{{Cat main|Yandex}}

[[Category:Wikipedia categories named after information technology companies]]
[[Category:Wikipedia categories named after companies of Russia]]
[[Category:Web portals]]
[[Category:Internet search engines]]
[[Category:Internet companies of Russia]]
[[Category:Companies listed on NASDAQ]]
[[Category:Internet in Russia]]
[[Category:Internet properties established in 1997]]
[[Category:Russian websites]]
[[Category:Information retrieval organizations]]
<=====doc_Id=====>:566
<=====title=====>:
Special Interest Group on Information Retrieval
<=====text=====>:
{{Infobox organization
|name           = ACM Special Interest Group on Information Retrieval
|image          = sig-information-retrieval-logo.png
|size           = 140px
|alt            = ACM SIGIR
|parent_organization = [[Association for Computing Machinery]]
|website        = {{URL|sigir.org}}
}}

'''SIGIR''' is the [[Association for Computing Machinery]]'s '''Special Interest Group on Information Retrieval'''. The scope of the group's specialty is the theory and application of computers to the acquisition, organization, storage, [[Information retrieval|retrieval]] and distribution of information; emphasis is placed on working with non-numeric information, ranging from natural language to highly structured data bases.

== Conferences ==
The annual international SIGIR conference, which began in 1978, is considered the most important in the field of information retrieval. SIGIR also sponsors the annual [[Joint Conference on Digital Libraries]] (JCDL) in association with [[ACM SIGWEB|SIGWEB]], the [[Conference on Information and Knowledge Management]] (CIKM), and the [[International Conference on Web Search and Data Mining]] (WSDM) in association with [[SIGKDD]], [[SIGMOD]], and [[ACM SIGWEB|SIGWEB]].

=== SIGIR conference locations ===
{| class="wikitable" border="1"
|-
! Number
! Year
! Location
|-
| 22
| 1999
| [[Berkeley, California]]
|-
| 23
| 2000
| [[Athens]]
|-
| 24
| 2001
| [[New Orleans]]
|-
| 25
| 2002
| [[Tampere]]
|-
| 26
| 2003
| [[Toronto]]
|-
| 27
| 2004
| [[Sheffield]]
|-
| 28
| 2005
| [[Salvador, Bahia]]
|-
| 29
| 2006
| [[Seattle]]
|-
| 30
| 2007
| [[Amsterdam]]
|-
| 31
| 2008
| [[Singapore]]
|-
| 32
| 2009
| [[Boston]]
|-
| 33
| 2010
| [[Geneva]]
|-
| 34
| 2011
| [[Beijing]]
|-
| 35
| 2012
| [[Portland, Oregon]]
|-
| 36
| 2013
| [[Dublin]]
|-
| 37
| 2014
| [[Gold Coast, Queensland]]
|-
| 38
| 2015
| [[Santiago]]
|-
| 39
| 2016
| [[Pisa]]
|-
| 40
| 2017
| [[Tokyo]]
|-
| 41
| 2018
| [[Ann Arbor]]
|}

== Awards ==
The group gives out several awards to contributions to the field of information retrieval. The most important award is the [[Gerard Salton Award]] (named after the computer scientist [[Gerard Salton]]), which is awarded every three years to an individual who has made "significant, sustained and continuing contributions to research in information retrieval". Additionally, SIGIR presents a Best Paper Award <ref>{{cite web | url=http://sigir.org/awards/awards.html#bestpaper | title=SIGIR Conference Best Paper Awards | accessdate=2012-08-29 }}</ref> to recognize the highest quality paper at each conference. "Test of time" Award <ref>{{cite web | url=http://sigir.org/awards/test-of-time-awards/ | title=SIGIR Conference Test of Time Awards | accessdate=2015-12-29 }}</ref> is a recent award that is given to a paper that  has had "long-lasting influence, including impact on a subarea of information retrieval research, across subareas of information retrieval research, and outside of the information retrieval research community". This award is selected from a set of full papers presented at the main SIGIR conference 10-12 years before.

==See also==
* [[Conference on Information and Knowledge Management]]

==References==

{{Reflist}}
==External links==
* {{official website|http://www.sigir.org/}}

{{Authority control}}

[[Category:Association for Computing Machinery Special Interest Groups]]
[[Category:Information retrieval organizations]]
<=====doc_Id=====>:569
<=====title=====>:
Artificial Solutions
<=====text=====>:
{{Infobox company
|name= Artificial Solutions
|logo=[[Image:Artificial Solutions Logo.png]]
|type=[[Private company]]
|foundation=(2001)
|founder=Johan Åhlund, Johan Gustavsson and Michael Söderström 
|location=[[Barcelona]], [[Spain]]
|locations=Offices worldwide with R&D centers in [[Barcelona]], [[Hamburg]], [[London]], [[Mountain View, California|Mountain View]], [[Milan]], [[Utrecht]] and [[Stockholm]] 
|industry=[[Computer Software]], [[Natural language]], [[Intelligent software assistant]], 
|products= Teneo platform
|homepage=[http://www.artificial-solutions.com/ www.artificial-solutions.com]
}}

'''Artificial Solutions''' is a multinational [[software company]] that develops and sells natural language interaction products for enterprise and consumer use.<ref>{{cite web|last=Ion |first=Florence |url=http://arstechnica.com/gadgets/2013/06/review-indigo-brings-siri-like-conversation-to-the-android-platform/ |title=Review: Indigo wants to bring Siri-like conversation to the Android platform |publisher=Ars Technica |date=2013-06-05 |accessdate=2013-09-08}}</ref> The company's natural language solutions have been deployed in a wide range of industries including finance,<ref>{{cite web|last=Thompson|first=Scott|title=Agria working with Artificial Solutions|url=http://www.fstech.co.uk/fst/AgriaDjurf%C3%B6rs%C3%A4kring_ArtificialSolutions.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Savvas|first=Antony|title=Co-operative Bank uses Mia to speed up contact centre calls|url=http://www.computerworlduk.com/news/it-business/3316914/co-operative-bank-uses-mia-to-speed-up-contact-centre-calls/|work=Computerworld UK|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Thompson|first=Scott|title=2012 FStech Awards: winners announced|url=http://www.fstech.co.uk/fst/2012_FStechAwards_Winners.php|work=FStech|publisher=Perspective Publishing|accessdate=12 September 2013}}</ref> telecoms,<ref>{{cite web|last=Westerholm|first=Joel|title=Telenors elektroniska kundtjänst pressar kostnaderna|url=http://computersweden.idg.se/2.2683/1.143425|work=ComputerSweden|publisher=IDG|accessdate=12 September 2013}}</ref><ref>{{cite web|title=Artificial Solutions Powers Online IVA for Vodafone|url=http://langtechnews.hivefire.com/articles/262940/artificial-solutions-powers-online-iva-for-vodafon/|work=LangTechNews|accessdate=12 September 2013}}</ref> the public sector,<ref>{{cite web|last=Brax|first=Sofia|title=Digitala kolleger alltid till tjänst|url=http://www.publikt.se/artikel/digitala-kolleger-alltid-till-tjanst-38087|work=Publik|publisher=Fackforbundet ST|accessdate=12 September 2013}}</ref><ref>{{cite web|last=Nilsson|first=Orjan|title=Cyber-damene husker deg|url=http://www.nettavisen.no/innenriks/ibergen/article1609734.ece|work=Nettavisen|publisher=iBergen}}</ref> retail<ref>{{cite web|author=Aaron Travis |url=http://techcrunch.com/2013/01/05/in-defense-of-the-humble-walkthrough/ |title=In Defense Of The Humble App Walkthrough |publisher=TechCrunch |date=2013-01-05 |accessdate=2013-09-08}}</ref> and travel.<ref>{{cite web|last=Fox|first=Linda|title=CWT brings virtual face to mobile service|url=http://www.tnooz.com/2013/04/16/news/cwt-brings-virtual-face-to-mobile-service/|work=Tnooz|accessdate=12 September 2013}}</ref>

==History==
Artificial Solutions was founded in Stockholm in 2001 by friends Johan Åhlund, Johan Gustavsson and Michael Söderström to create interactive web assistants using a combination of artificial intelligence and natural language processing. Though Åhlund initially took some persuading, he thought it sounded ridiculous to be talking to a virtual agent on the internet.<ref>{{cite web|url=http://it24.idg.se/2.2275/1.143922 |title=Löjlig affärside vinstlott för Artificial Solutions |publisher=IT24 |date= |accessdate=2013-09-08}}</ref>

The company expanded with the development of online customer service optimization products and by 2005 it had several offices throughout Europe supporting the development and sales of its online virtual assistants.<ref>{{cite web|url=http://www.elnuevolunes.es/historico/2008/1294/1294%20al%20grano.html |title=Al grano |publisher=Elnuevolunes.es |date= |accessdate=2013-09-08}}</ref> Artificial Solutions was placed as visionary in the latest Gartner Magic Quadrant for CRM Web Customer Service Applications.<ref>{{cite web|author=Barry Levine |url=http://www.cmswire.com/cms/customer-experience/gartner-mq-for-crm-web-customer-service-kana-moxie-software-oraclerightnow-among-leaders-019626.php |title=Gartner MQ for CRM Web Customer Service: Kana, Moxie Software, Oracle-RightNow Among Leaders |publisher=Cmswire.com |date= |accessdate=2013-09-08}}</ref>

In 2006 Artificial Solutions acquired Kiwilogic, a German software house creating its own virtual assistants.<ref>{{cite web|url=http://www.earlybird.com/en/companies/tech/exited/kiwilogic.html |title=Venture Capital: KIWILOGIC.COM AG |publisher=Earlybird |date= |accessdate=2013-09-08}}</ref>
[[Elbot]], Artificial Solutions’ test-bed to explore the psychology of human-machine communication, won the [[Loebner Prize]] in 2008 and is the closest contestant of the annual competition based on the [[Turing Test]] to reach the 30% threshold by fooling 25% of the human judges.<ref>[[Loebner Prize]]</ref><ref>{{cite web|url=http://news.bbc.co.uk/2/hi/uk_news/england/berkshire/7666246.stm |title=UK &#124; England &#124; Berkshire &#124; Test explores if robots can think |publisher=BBC News |date=2008-10-13 |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Robson|first=David|title=Almost human: Interview with a chatbot|url=http://www.newscientist.com/article/dn14925-almost-human-interview-with-a-chatbot.html#.UjHKzTdBuM9|work=New Scientist|publisher=Reed Business Information Ltd}}</ref>

With a change in management in 2010 the company started to focus the basis of its technology on Natural Language Interaction and launched the Teneo Platform, which allows people to hold humanlike, intelligent conversations with applications and services running on electronic devices.<ref>{{cite web|author=[[Mike Elgan]] |url=http://www.computerworld.com/s/article/9237448/Smart_apps_think_so_you_don_t_have_to_ |title=Smart apps think (so you don't have to) |publisher=Computerworld |date=2013-03-09 |accessdate=2013-09-08}}</ref><ref>{{cite web|url=http://www.speechtechmag.com/Articles/News/Industry-News/Artificial-Solutions-Unveils-a-Software-Toolkit-for-Adding-Speech-to-Mobile-Apps-80015.aspx |title=Artificial Solutions Unveils a Software Toolkit for Adding Speech to Mobile Apps |publisher=SpeechTechMag.com |date=2012-01-17 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://www.computerworld.dk/art/220859/saa-effektiv-er-ikeas-chat-robot-har-vaeret-paa-efteruddannelse |title=Så effektiv er Ikeas chat-robot: Har været på 'efteruddannelse' - Computerworld |publisher=Computerworld.dk |date= |accessdate=2013-09-08}}</ref>
In 2013 Artificial Solutions launched [[Indigo (virtual assistant)|Indigo]], a mobile personal assistant that is able to operate and remember the context of the conversation across different platforms and operating systems.<ref>{{cite web|last=Hoyle |first=Andrew |url=http://reviews.cnet.com/8301-13970_7-57570960-78/indigo-brings-siri-like-assistance-to-android-for-free-hands-on/ |title=Indigo brings Siri-like assistance to Android for free (hands-on) &#124; Mobile World Congress - CNET Reviews |publisher=Reviews.cnet.com |date=2013-02-24 |accessdate=2013-09-08}}</ref><ref>{{cite web|author= |url=http://lifehacker.com/indigo-wants-to-be-your-personal-assistant-across-devic-484924277 |title=Indigo Wants to Be Your Personal Assistant Across Devices |publisher=Lifehacker.com |date= |accessdate=2013-09-08}}</ref><ref>{{cite web|last=Wollman |first=Dana |url=http://www.engadget.com/2013/02/26/indigo-personal-assistant-hands-on/ |title=Indigo is a cloud-based, cross-platform personal assistant for Android and Windows Phone 8 (hands-on) |publisher=Engadget.com |date=2013-02-26 |accessdate=2013-09-08}}</ref>
A new round of funding was announced in June 2013. The $9.4m will be used to support expansion in the US market.<ref>{{cite web|url=http://www.altassets.net/private-equity-news/by-news-type/deal-news/artificial-solutions-raises-9-4m-in-scope-led-round-for-us-expansion.html |title=Artificial Solutions raises $9.4m in Scope-led round for US expansion &#124; AltAssets Private Equity News |publisher=Altassets.net |date=2013-06-25 |accessdate=2013-09-08}}</ref>

In February 2014 Artificial Solutions announced the Teneo Network of Knowledge, a patented intelligent framework that enables users to interact using natural language with private, shared and public ecosystem of devices, also known as the [[Internet of Things]].<ref>{{cite web|last1=Trenholm|first1=Rich|title=Next generation of personal assistant takes a step towards 'Her'-style super-Siri|url=http://www.cnet.com/news/next-generation-of-personal-assistant-takes-a-step-towards-her-style-super-siri/|website=Cnet|publisher=CBS Interactive}}</ref>

==References==
{{Reflist|30em}}

==External links==
*[http://www.hello-indigo.com Indigo]
*[http://www.elbot.com Elbot]

[[Category:Natural language processing software]]
[[Category:Intelligent software assistants]]
[[Category:User interfaces]]
[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval organizations]]
<=====doc_Id=====>:572
<=====title=====>:
Information Retrieval Facility
<=====text=====>:
{{Advert|date=May 2012}}

[[Image:IRF logo 350x350.png|thumb|200px|right|IRF logo]]

The '''Information Retrieval Facility''' ('''IRF'''), founded 2006 and located in [[Vienna]], [[Austria]], was a research platform for networking and collaboration for professionals in the field of [[information retrieval]]. It ceased operations in 2012.

The IRF had members in the following categories:

* Researchers in [[information retrieval]] (IR) or related scientific areas
* Industrial/corporate information management professionals
* Patent authorities and governmental institutions
* Students of one of the above

==The Scientific Board==
'''Maristella Agosti''', Professor, [http://www.dei.unipd.it/wdyn/?IDsezione=1 Department of Information Engineering, University of Padova]

'''Gerhard Budin''', Director of the [http://transvienna.univie.ac.at/forschung/professuren/dr-gerhard-budin/ Center of Translation Studies at the University of Vienna],
Director of the [http://www.oeaw.ac.at/icltt/ Department of Corpuslinguistics and Text Technology, Austrian Academy of Sciences]

'''Jamie Callan''', Professor, [http://www.cs.cmu.edu/~callan/Bio.html Language Technologies Institute, CMU, Carnegie Mellon University]

'''Yves Chiaramella''', Professor Emeritus, [http://www-clips.imag.fr/mrim/User/yves.chiaramella/ Department of Computer Science and Applied Mathematics, Joseph Fourier University]

'''Kilnam Chon''', Professor, Computer Science Department, [http://cosmos.kaist.ac.kr/salab/professor/index02.html Korea Advanced Institute of Science and Technology (KAIST)]

'''W. Bruce Croft''', Distinguished Professor, [http://ciir.cs.umass.edu/personnel/croft.html Department of Computer Science and Director Center for Intelligent IR University of Massachusetts Amherst]

'''Hamish Cunningham''', Research Professor, [http://www.dcs.shef.ac.uk/~hamish/ Computer Science Department University Sheffield]

'''Norbert Fuhr''', Chairman of the Scientific Board, Professor, [http://www.is.informatik.uni-duisburg.de/staff/fuhr.html Institute of Informatics and Interactive Systems University Duisburg-Essen]

'''David Hawking''', Science Leader, Project Leader, [http://es.csiro.au/people/Dave/ CSIRO ICT Centre]

'''Noriko Kando''', Professor, [http://www.nii.ac.jp/index.shtml.en Software Engineering Research, Software Research Division, National Institute of Informatics (NII)]

'''Arcot Desai Narasimhalu''', Associate Dean, [http://www.sis.smu.edu.sg/faculty/infosys/arcotdesai.asp School of Information Systems Singapore Management University]

'''John Tait''', Chief Scientific Officer of the IRF, [http://www.johntait.net/ Until July 2007 Professor of Intelligent Information Systems and Associate Dean of the School of Computing and Technology]

'''Benjamin T'sou''', Director, [http://www.cityu.edu.hk/ Language Information Sciences Research Centre, City University of Hong Kong]

'''[[C. J. van Rijsbergen|C.J. van Rijsbergen]]''',
[http://www.dcs.gla.ac.uk/~keith/ Dept. Computer Science at the University of Glasgow]

==Scientific goals==

* Modelling innovative and specialised information retrieval systems for global patent document collections.
* Investigating and developing an adequate technical infrastructure that allows interactive experimentation with formal, mathematical retrieval concepts for very large-scale document collections.<
* Studying the usability of multi modal user-interfaces to very large-scale information retrieval systems.
* Integrating real users with actual information needs into the research process of modelling information retrieval systems to allow accurate performance evaluation.
* Ability to create different views of patent data depending on the focus of the information need.
* Defining standardised methods for benchmarking the information retrieval process in patent document collections.
* Ability to handle text and non-text parts of a patent in a coherent manner.
* Designing, experimenting and evaluating search engines able to retrieve structured and semi-structured documents in very large-scale patent collections.
* Integrating the temporal dimension of patent documents in retrieval strategies.
* Improving effectiveness and precision of patent retrieval, based on ontologies and natural-language understanding techniques.
* Refining IR methods that allow unstructured querying by exploiting available structure within the patent documents.
* Formal (mathematical) identification and specification of relevant business information needs in the field of intellectual property information.
* Investigating efficient scaling mechanisms for information retrieval taking into account the characteristics of patent data.
* Investigating and experimenting with computing architectures for very high-capacity information management.
* Establishing an open [[eScience]] platform that enables a standardised and easy way of creating and performing IR experiments on a common research infrastructure.
* Discovering and investigating novel use cases and business applications deriving from intellectual property information.
* Enabling the formal information retrieval, natural language and semantic processing research to grow into the field of applied sciences in the global, industrial context.
* Development and integration of different information access methods.
* Research on effective methods for interactive information retrieval.

==Semantic supercomputing==
Current technologies to extract concepts from unstructured documents are extremely computational intensive. To allow interactive experimentation with rich and huge text corpora, the IRF has built a high performance computing environment, into which the latest technological advances have been implemented:

* multi-node clusters (currently 80 cores, up to 1024)
* highest speed interconnect technology
* single system image with large compound memory (currently 320 GB, up to 4 TB)
* fully integrated configurable computing (currently 4 FPGA cores, up to 256)

The combination of these HPC features to accelerate text mining represents the IRF implementation of semantic supercomputing.

==The World Patent Corpus==
The IRF aims to bring state-of-the-art information retrieval technology to the community of patent information professionals. We expect information retrieval (IR) technology to become the focus of information technology very soon. All industry sectors can profit from applying modern and future text mining processes to the special requirements of patent research. Although all ideas and concepts are universally applicable to all sorts of intellectual property information, patents require the most sophistication, and confront us with challenging technical and organisational problems. 
The entire body of patent-related documents possibly constitutes the largest corpus of compound documents, making it a rewarding target for text mining scientists and end-users alike. What’s more, patents have become a crucial issue, in particular for large global corporations and universities. The industrial users of patent data are among the most demanding and important information professionals. As a consequence, they could benefit the most from technology that relieves the burden of researching the large body of patent information.

== Research collections ==
The IRF provides a number of test data collections that have either been developed by the IRF, by one of its members or by third parties. These data collections can be used freely for scientific experimentations.

The MAtrixware REsearch Collection ([[MAREC]]) is the first standardised patent data corpus for research purposes. It consists of 19 million patent documents in different languages, normalised to a highly specific XML format. The collection has been developed by Matrixware for the IRF.

The ClueWeb09 collection is a 25 terabyte dataset of about 1 billion web pages crawled in January and February, 2009. It has been created by the Language Technologies Institute at [[Carnegie Mellon University]] to support research on information retrieval and related human language technologies.

==References==
* [http://www.iwr.co.uk/information-world-review/analysis/2231880/patent-medicine-info-retrievers?page=2 Patent medicine for information retrievers, Information World Review]
* [http://ecir2008.dcs.gla.ac.uk/industry.html The IRF and its Role in Professional Information Research, ECIR 2008]

==External links==
* [http://www.ir-facility.org/ Official site: ir-facility.org]
* [https://www.youtube.com/watch?v=XpXtRu0XfeA YouTube: The future of information retrieval Part1] 
* [https://www.youtube.com/watch?v=dRaTeTaHBsI YouTube: The future of information retrieval Part2]

[[Category:Organizations established in 2006]]
[[Category:Computer science organizations]]
[[Category:Information retrieval organizations]]
[[Category:Education in Vienna]]
<=====doc_Id=====>:575
<=====title=====>:
Category:Waymo
<=====text=====>:
{{Commons category|Google}}
{{Cat main|Waymo}}

[[Category:Technology companies based in the San Francisco Bay Area]]
[[Category:Companies based in Mountain View, California]]
[[Category:Web portals]]
[[Category:Information retrieval organizations]]
[[Category:Alphabet Inc.]]
[[Category:Wikipedia categories named after websites]]
<=====doc_Id=====>:578
<=====title=====>:
ChemRefer
<=====text=====>:
{{Orphan|date=February 2009}}
{{Infobox website
| name = Chemrefer
| logo = [[Image:Chemrefer.png]]
| screenshot = 
| caption = 
| url = http://www.chemrefer.com
| commercial = Yes
| type = [[Search engine]]
| language = English
| registration = Not Applicable
| owner = ChemRefer Limited
| author = William James Griffiths
| launch date = 2006
| current status = Offline
| revenue = 
}}
'''ChemRefer''' is a service that allows searching of freely available and full-text chemical and pharmaceutical literature that is published by authoritative sources.<ref>{{citation|journal=Science Articles |title= Science News Forum|publisher= SciScoop |date=May 19, 2006|url= http://www.sciscoop.com/story/2006/5/19/95844/6293}}</ref>

Features include basic and advanced search options, [[mouseover]] detailed view, an integrated chemical structure drawing and search tool, downloadable [[toolbar]], customized [[RSS]] feeds, and newsletter.

ChemRefer is primarily of use to readers who do not have subscriptions for accessing restricted chemical literature, and to publishers who offer either [[Open access (publishing)|open access]] or [[hybrid open access journal]]s and seek to attract further subscriptions by publicly releasing part of their archive.

==See also==
*[[Google Scholar]]
*[[Windows Live Academic]]
*[[BASE (search engine)|BASE]]
*[[PubMed]]

==References==
{{reflist}}

==External links==
===Recommendations & reviews===
*[https://web.archive.org/web/20060902072725/http://www.rowland.harvard.edu/resources/library/lnn_archive/031706.php Cited as an "Internet Site of the Week"] by the library of the [[Rowland Institute for Science]] at [[Harvard University]]
*[https://web.archive.org/web/20070804051550/http://infoweb.nrl.navy.mil:80/index.cfm?i=156 Recommended in the list of chemical literature databases] by the library of the [[United States Naval Research Laboratory]]
*[https://web.archive.org/web/20070212122105/http://www.mta.ca:80/library/subject_chemistry.html Recommended in the list of chemical literature databases] by the library of [[Mount Allison University]]
*[http://depth-first.com/articles/2007/01/15/chemrefer-free-direct-access-to-the-primary-literature Review of ChemRefer] at Depth-First chemoinformatics magazine
*[https://web.archive.org/web/20080917155607/http://recherche-technologie.wallonie.be:80/fr/particulier/menu/revue-athena/l-annuaire-de-liens/internet/moteurs-de-recherche/www-chemrefer-com.html?PROFIL=PART Recommended in the list of chemical literature databases] by the Technology Research Portal, Belgium
*[http://www.certh.gr/0E9BF53C.en.aspx Recommended in the list of chemical literature databases] by the Centre for Research and Technology, Thessaloniki

===Background===
*[http://www.reactivereports.com/56/56_0.html Interview with William James Griffiths] at Reactive Reports chemistry magazine
*[http://www.earlham.edu/~peters/fos/overview.htm Open access overview] by Professor Peter Suber, Earlham College

[[Category:Scholarly search services]]
[[Category:Chemistry literature]]
[[Category:Information retrieval systems]]
[[Category:Open access projects]]

{{searchengine-website-stub}}
<=====doc_Id=====>:581
<=====title=====>:
Category:Music search engines
<=====text=====>:
[[Category:Information retrieval systems]]
[[Category:Music software|Search engines]]
[[Category:Internet search engines]]
[[Category:Online music and lyrics databases]]
[[Category:Music information retrieval]]
<=====doc_Id=====>:584
<=====title=====>:
Ptx (Unix)
<=====text=====>:
{{Unreferenced stub|auto=yes|date=December 2009}}
{{Lowercase|title=ptx}}
'''ptx''' is a [[Unix utility]], named for the ''[[permuted index]]'' which can perform the function of the [[Keyword in Context]] (KWIC) search mode. There is a corresponding [[IBM mainframe]] utility which performs the same function. permuted indexes are often used in such places as bibliographic or medical databases, [[thesaurus]]es, or web sites to aid in locating entries of interest.

==See also==
* [[Concordancer]]

[[Category:Information retrieval systems]]
[[Category:Unix text processing utilities]]


{{Unix-stub}}
<=====doc_Id=====>:587
<=====title=====>:
Reverse DNS lookup
<=====text=====>:
{{Redirect|Reverse DNS|Java-like naming convention|Reverse domain name notation}}
{{Refimprove|date=September 2016}}

In [[computer networking]], '''reverse DNS lookup''' or '''reverse DNS resolution''' ('''rDNS''') is the determination of a [[domain name]] associated with an [[IP address]] via querying [[Domain Name System|DNS]] – the reverse of the usual "forward" DNS lookup of an IP from a domain name.

The process of reverse resolving an IP address uses [[PTR record]]s. The reverse DNS database of the Internet is rooted in the [[.arpa|arpa]] [[top-level domain]].

Although the informational RFC 1912 (Section 2.1) specifies that "Every Internet-reachable host should have a name" and that "For every IP address, there should be a matching PTR record...", it is not an [[Internet Standard]] requirement, and not all IP addresses have a reverse entry.

== Implementation details ==
===IPv4 reverse resolution===
Reverse DNS lookups for [[IPv4]] addresses use the special domain <code>in-addr.arpa</code>. In this domain, an IPv4 address is represented as a concatenated sequence of four decimal numbers, separated by dots, to which is appended the second level domain suffix <code>.in-addr.arpa</code>. The four decimal numbers are obtained by splitting the 32-bit IPv4 address into four [[octet (computing)|octet]]s and converting each octet into a decimal number. These decimal numbers are then concatenated in the order: least significant octet first (leftmost), most significant octet last (rightmost). It is important to note that this is the reverse order to the usual dotted-decimal convention for writing IPv4 addresses in textual form.

For example, to do a reverse lookup of the IP address <code>8.8.4.4</code> the PTR record for the domain name <code>4.4.8.8.in-addr.arpa</code> would be looked up, and found to point to <code>google-public-dns-b.google.com</code>. 

If the [[A record]] for <code>google-public-dns-b.google.com</code> in turn pointed back to <code>8.8.4.4</code> then it would be said to be [[Forward-confirmed reverse DNS|forward-confirmed]].

====Classless reverse DNS method====
Historically, Internet registries and Internet service providers allocated IP addresses in blocks of 256 (for Class C) or larger octet-based blocks for classes B and A. By definition, each block fell upon an octet boundary. The structure of the reverse DNS domain was based on this definition. However, with the introduction of [[Classless Inter-Domain Routing]], IP addresses were allocated in much smaller blocks, and hence the original design of pointer records was impractical, since autonomy of administration of smaller blocks could not be granted. RFC 2317 devised a methodology to address this problem by using [[CNAME record]]s.

===IPv6 reverse resolution===
Reverse DNS lookups for [[IPv6]] addresses use the special domain <code>ip6.arpa</code> (previously <code>ip6.int</code><ref>RFC 4159</ref>). An IPv6 address appears as a name in this domain as a sequence of [[nibble]]s in reverse order, represented as hexadecimal digits as subdomains. For example, the pointer domain name corresponding to the IPv6 address <code>2001:db8::567:89ab</code> is <code>b.a.9.8.7.6.5.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.8.b.d.0.1.0.0.2.ip6.arpa</code>.

===Multiple pointer records===
While most rDNS entries only have one PTR record, DNS does not restrict the number. However, having multiple PTR records for the same IP address is generally not recommended{{by whom|date=August 2016}}, unless there is a specific need. For example, if a web server supports many [[virtual host]]s, there may be one PTR record for each host and some versions of name server software will allocate this automatically. Multiple PTR records can cause problems, however, including triggering bugs in programs that only expect single PTR records.<ref>[http://sources.redhat.com/bugzilla/show_bug.cgi?id=5790 glibc bug #5790]</ref> In the case of a large web server, having hundreds of PTR records can cause the DNS packets to be much larger than normal, which can cause the query to be requested over TCP when they exceed the DNS 512 byte UDP message limit.

===Records other than PTR records===
Record types other than PTR records may also appear in the reverse DNS tree. For example, encryption keys may be placed there for [[IPsec]], [[Secure Shell|SSH]] and [[Internet Key Exchange|IKE]]. [[Zero-configuration networking#DNS-SD|DNS-Based Service Discovery]] uses specially-named records in the reverse DNS tree to provide hints to clients about subnet-specific service discovery domains.<ref>{{Citation | publisher = IETF | title = RFC 6763 | url = http://tools.ietf.org/html/rfc6763#section-11}}</ref> Less standardized usages include comments placed in [[TXT record]]s and [[LOC record]]s to identify the geophysical location of an IP address.

==Uses==
The most common uses of the reverse DNS include:

* The original use of the rDNS: network troubleshooting via tools such as [[traceroute]], [[Ping (networking utility)|ping]], and the "Received:" trace header field for [[SMTP]] e-mail, web sites tracking users (especially on [[Internet forum]]s), etc.
* One [[anti-spam techniques (e-mail)#PTR.2Freverse DNS checks|e-mail anti-spam technique]]: checking the domain names in the rDNS to see if they are likely from dialup users, or dynamically assigned addresses unlikely to be used by legitimate mail servers. Owners of such IP addresses typically assign them generic rDNS names such as "1-2-3-4-dynamic-ip.example.com." Some anti-spam filters assume that email that originates from such addresses is likely to be spam, and may refuse connection.<ref>[http://www.spamhaus.org/faq/answers.lasso?section=ISP%20Spam%20Issues#131 spamhaus's FAQ]</ref><ref>[http://postmaster.aol.com/info/rdns.html reference page from AOL] {{webarchive |url=https://web.archive.org/web/20061210223820/http://postmaster.aol.com/info/rdns.html |date=December 10, 2006 }}</ref>
* A [[forward-confirmed reverse DNS]] (FCrDNS) verification can create a form of authentication showing a valid relationship between the owner of a domain name and the owner of the server that has been given an IP address. While not very thorough, this validation is strong enough to often be used for [[whitelist]]ing purposes, since [[Spam (electronic)|spammers]] and [[Phishing|phishers]] usually cannot achieve forward validation when they use [[zombie computer]]s to forge domain records.
* System logging or monitoring tools often receive entries with the relevant devices specified only by IP addresses. To provide more human-usable data, these programs often perform a reverse lookup before writing the log, thus writing a name rather than the IP address.

==References==
{{reflist}}

==External links==
* {{dmoz|Computers/Internet/Protocols/DNS/Web_Tools|Web-based DNS lookup tools}}
* [http://dns.icann.org ICANN DNS Operations]
* [https://tools.ietf.org/html/rfc3596 RFC 3596 DNS Extensions to Support IP Version 6]
* RDNS policies: [https://web.archive.org/web/20121106162649/http://postmaster.aol.com:80/Postmaster.Errors.php#554rlyb1#whatisrdns AOL], [http://customer.comcast.com/help-and-support/internet/fix-a-554-error/ Comcast], [http://www.craigslist.org/about/help/rdns_failure Craigslist], [https://www.misk.com/kb/reverse-dns Misk.com]

[[Category:Information retrieval systems]]
[[Category:Domain name system]]

[[nl:Domain Name System#Omgekeerde lookups]]
<=====doc_Id=====>:590
<=====title=====>:
Contextual Query Language
<=====text=====>:
'''Contextual Query Language''' (CQL), previously known as '''Common Query Language''',<ref>[http://www.loc.gov/standards/sru/cql/spec.html CQL: the Contextual Query Language: Specifications] SRU: Search/Retrieval via URL, Standards, Library of Congress</ref> is a [[formal language]] for representing queries to [[information retrieval]] systems such as [[search engine]]s, [[bibliography|bibliographic catalogs]] and [[museum]] collection information. Based on the [[semantics]] of [[Z39.50]], its design objective is that queries be human readable and writable, and that the language be intuitive while maintaining the expressiveness of more complex [[query language]]s. It is being developed and maintained by the Z39.50 Maintenance Agency, part of the [[Library of Congress]].

== Examples of query syntax ==

Simple queries:

<blockquote><tt>dinosaur<br/>
"complete dinosaur"<br/>
title = "complete dinosaur"<br/>
title exact "the complete dinosaur"</tt></blockquote>

Queries using [[Boolean logic]]:

<blockquote><tt>dinosaur or bird<br/>
Palomar assignment and "ice age"<br/>
dinosaur not reptile<br/>
dinosaur and bird or dinobird<br/>
(bird or dinosaur) and (feathers or scales)<br/>
"feathered dinosaur" and (yixian or jehol)</tt></blockquote>

Queries accessing [[index (publishing)|publication indexes]]:

<blockquote><tt>publicationYear < 1980<br/>
lengthOfFemur > 2.4<br/>
bioMass >= 100</tt></blockquote>

Queries based on the proximity of words to each other in a document:

<blockquote><tt>ribs prox/distance<=5 chevrons<br/>
ribs prox/unit=sentence chevrons<br/>
ribs prox/distance>0/unit=paragraph chevrons</tt></blockquote>

Queries across multiple [[Dimension (data warehouse)|dimensions]]:

<blockquote><tt>date within "2002 2005"<br/>
dateRange encloses 2003</tt></blockquote>

Queries based on [[Relevance (information retrieval)|relevance]]:

<blockquote><tt>subject any/relevant "fish frog"<br/>
subject any/rel.lr "fish frog"</tt></blockquote>

The latter example specifies using a specific [[algorithm]] for [[logistic regression]].<ref>[http://srw.cheshire3.org/contextSets/rel/ Relevance Ranking Context Set version 1.1]</ref>

== References ==
{{Reflist}}

== External links ==
* [http://www.loc.gov/standards/sru/cql/ CQL home page]
* [http://www.loc.gov/z3950/agency/ Z39.50 Maintenance Agency]
* [http://zing.z3950.org/cql/intro.html A Gentle Introduction to CQL]

{{Query languages}}

{{USGovernment|sourceURL=http://www.loc.gov/standards/sru/cql/}}
{{LOC-stub}}

[[Category:Information retrieval systems]]
[[Category:Library science]]
[[Category:Library of Congress]]
[[Category:Query languages]]
[[Category:Knowledge representation languages]]
<=====doc_Id=====>:593
<=====title=====>:
Wolfram Alpha
<=====text=====>:
{{Use mdy dates|date=October 2014}}
{{Infobox website
| name = Wolfram Alpha
| logo =  Wolfram Alpha December 2016.svg
| caption = Wolfram Alpha is based on the computational platform [[Mathematica]], written by British scientist [[Stephen Wolfram]] in 1988.
| url = {{URL|http://www.wolframalpha.com/}}.
| slogan             = Making the world’s knowledge computable.<ref>[http://www.wolframalpha.com/about.html Wolfram Alpha About page]</ref>
| commercial = Yes
| type =  [[Answer engine]]
| registration = Optional
| owner = Wolfram Alpha LLC
| author = [[Wolfram Research]]
| alexa  = {{Decrease}} 1,932 ({{as of|2015|26|31|alt=March 2015}})<ref name="alexa">{{cite web|url= http://www.alexa.com/siteinfo/wolframalpha.com |title= Wolframalpha.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2015-03-31}}</ref>
| num_employees ≈ 200 (as of 2012)
| programming_language = [[Wolfram Language]]
| launch date = {{start date and age|2009|5|18}}<ref name="launch date">{{cite web |author=The Wolfram&#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |work=Wolfram&#124;Alpha Blog |publisher=Wolfram Alpha |date=May 8, 2009 |accessdate=2013-02-09}}</ref> (official launch)<br>{{start date|2009|5|15}}<ref name="updated launch detail">{{cite web |author=The Wolfram&#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/12/going-live-and-webcasting-it/ |work=Wolfram&#124;Alpha Blog |title=Going Live—and Webcasting It |publisher=Wolfram Alpha |date=May 12, 2009 |accessdate=2013-02-09}}</ref> (public launch)
| current status = Active
}}

'''Wolfram Alpha''' (also styled '''WolframAlpha''' and '''Wolfram|Alpha''') is a computational knowledge engine<ref name=Guardiandatasource>{{cite news |title=Where does Wolfram Alpha get its information? |author=Bobbie Johnson |publisher=The Guardian |date=May 21, 2009 |accessdate=2013-03-08 |url=https://www.theguardian.com/technology/2009/may/21/1 }}</ref> or [[answer engine]] developed by [[Wolfram Research]], which was founded by [[Stephen Wolfram]]. It is an online service that answers factual queries directly by computing the answer from externally sourced "curated data",<ref>{{Cite web|title = About Wolfram{{!}}Alpha: Making the World's Knowledge Computable|url = http://www.wolframalpha.com/about.html|website=wolframalpha.com|accessdate = 2015-11-25}}</ref> rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] might.<ref>{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=March 9, 2009 |work=The Guardian |publisher=Guardian News and Media |location=UK |accessdate=2013-02-09}}</ref>

Wolfram Alpha, which was released on May 18, 2009, is based on Wolfram's earlier flagship product [[Wolfram Mathematica]], a computational platform or toolkit that encompasses computer algebra, symbolic and numerical computation, visualization, and statistics capabilities.<ref name="launch date" /> Additional data is gathered from both academic and commercial websites such as the CIA's ''[[The World Factbook]]'', the United States Geological Survey, a Cornell University Library publication called ''All About Birds'', ''Chambers Biographical Dictionary'', [[Dow Jones]], the ''Catalogue of Life'',<ref name=Guardiandatasource /> [[CrunchBase]],<ref name=techcrunch>{{cite news |last=Dillet |first=Romain |title=Wolfram Alpha Makes CrunchBase Data Computable Just In Time For Disrupt SF |url=http://techcrunch.com/2012/09/07/wolfram-alpha-makes-crunchbase-data-computable-just-in-time-for-disrupt/ |publisher=TechCrunch |date=September 7, 2012 |accessdate=2013-02-09}}</ref> [[Best Buy]],<ref>{{cite news |last=Golson |first=Jordan |title=Wolfram Delivers Siri-Enabled Shopping Results From Best Buy |url=http://www.macrumors.com/2011/12/16/wolfram-delivers-siri-enabled-shopping-results-from-best-buy/ |publisher=MacRumors |date=December 16, 2011 |accessdate=2013-02-09}}</ref> the [[Federal Aviation Administration|FAA]]<ref>{{cite news |last=Barylick |first=Chris |title=Wolfram Alpha search engine now tracks flight paths, trajectory information |url=http://www.engadget.com/2011/11/19/wolfram-alpha-search-engine-now-tracks-flight-paths-trajectory/ |publisher=Engadget |date=November 19, 2011 |accessdate=2013-02-09}}</ref> and optionally a user's Facebook account.

== Overview ==
Users submit queries and computation requests via a text field.  Wolfram Alpha then computes answers and relevant visualizations from a [[knowledge base]] of [[Data curation|curated]], [[structured data]] that come from other sites and books. The site "use[s] a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review."<ref>{{cite web |title=Data in Wolfram&#124;Alpha |url=http://www.wolframalpha.com/faqs5.html |website=Wolfram Alpha |accessdate=4 August 2015}}</ref> The curated data makes Alpha different from [[semantic search]] engines, which index a large number of answers and then try to match the question to one.

Wolfram Alpha can only provide robust query results based on computational facts, not queries on the social sciences, cultural studies or even many questions about history where responses require more subtlety and complexity. It is able to respond to particularly-phrased [[natural language understanding|natural language]] fact-based questions such as "Where was [[Mary Robinson]] born?" or more complex questions such as "How old was [[Queen Elizabeth II]] in 1974?" It displays its "Input interpretation" of such a question, using standardized phrases such as "age | of Queen Elizabeth II (royalty) | in 1974", the answer of which is "Age at start of 1974: 47 years", and a biography link. Wolfram Alpha does not answer queries which require a narrative response such as "What is the difference between the Julian and the Gregorian calendars?" but will answer factual or computational questions such as "June 1 in Julian calendar".

Mathematical symbolism can be parsed by the engine, which typically responds with more than the numerical results. For example, "lim(x->0) (sin x)/x" yields the correct [[limit (functions)|limit]]ing value of 1, as well as a plot, up to 235 terms ({{as of|2013|lc=y}}) of the [[Taylor series]], and (for registered users) a possible derivation using [[L'Hôpital's rule]]. It is also able to perform calculations on data using more than one source. For example, "What is the [[List of countries by GDP (nominal) per capita|fifty-second smallest]] country by [[GDP per capita]]?" yields [[Nicaragua]], $1160 per year.

== Technology ==
Wolfram Alpha is written in 15 million lines of [[Wolfram Language]] code<ref>{{cite web |author=WolframResearch |url=https://www.youtube.com/watch?v=56ISaies6Ws#t=927s |title=Stephen Wolfram: The Background and Vision of Mathematica |publisher=Youtube.com |date=October 10, 2011 |accessdate=2013-02-09}}</ref> and runs on more than 10,000 CPUs.<ref>{{cite news |first=Frederic |last=Lardinois |url=http://readwrite.com/2009/04/25/wolframalpha_our_first_impressions |title=Wolfram&#124;Alpha: Our First Impressions |date=April 25, 2009 |publisher=ReadWriteWeb |accessdate=2013-02-09}}</ref><ref>{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2009/05/15/wolframalpha-is-launching-made-possible-by-mathematica/ |title=Wolfram&#124;Alpha Is Launching: Made Possible by ''Mathematica'' |work=WolframAlpha Blog |publisher=Wolfram Alpha |date=May 15, 2009 |accessdate=2013-02-09}}</ref> The database currently includes hundreds of datasets, such as "All Current and Historical Weather." The datasets have been accumulated over several years.<ref>{{cite web |title=Taking a first bite out of Wolfram Alpha | first=Jane Fae | last=Ozimek |work=The Register |date=May 18, 2009 |url=http://www.theregister.co.uk/2009/05/18/wolfram_alpha/ |accessdate=2013-02-09}}</ref> The curated (as distinct from auto-generated) datasets are checked for quality either by a scientist or other expert in a relevant field, or someone acting in a clerical capacity who simply verifies that the datasets are "acceptable".<ref name=semanticabyss>{{cite web |title=The Semantic Abyss - Plumbing the Semantic Web: Exploring the depths of the semantic gap between the Semantic Web and real world users and consumers |url=http://semanticabyss.blogspot.ca/2009/03/what-is-curated-data.html |year=2009 |author=Jack Krupansky}}</ref>{{unreliable source?|date=September 2015}}

One example of a live dataset that Wolfram Alpha can use is the profile of a [[Facebook]] user, through inputting the "facebook report" query. If the user authorizes Facebook to share his or her account details with the Wolfram site, Alpha can generate a "personal analytics" report containing the age distribution of friends, the frequency of words used in status updates and other detailed information.<ref name=techland>{{cite news |first=Thomas E. |last=Weber |url=http://techland.time.com/2012/09/05/wolfram-alphas-facebook-analytics-tool-digs-deep-into-your-social-life/ |title=Wolfram Alpha's Facebook Analytics Tool Digs Deep into Your Social Life |work=Tech |publisher=Time Magazine |date=September 5, 2012 |accessdate=2013-02-09}}</ref> Within two weeks of launching the Facebook analytics service, 400,000 users had used it.<ref>{{cite news |last=R. |first=A. |title=Visualising Facebook Who am I? |url=http://www.economist.com/blogs/graphicdetail/2012/09/visualising-facebook |publisher=The Economist |work=Graphic detail |date=September 21, 2012 |accessdate=2013-02-09}}</ref> Downloadable query results are behind a pay wall but summaries are accessible to free accounts.<ref name=publiclibraries>{{cite web |url=http://publiclibrariesonline.org/2013/03/a-wolf-or-a-ram-what-is-wolfram-alpha/ |title=A Wolf or a Ram? What is Wolfram Alpha? |author=Joanna Nelson |date=March 4, 2013 |publisher=Public Libraries Online }}</ref>

== Licensing partners ==
Wolfram Alpha has been used to power some searches in the [[Microsoft]] [[Bing (search engine)|Bing]] and [[DuckDuckGo]] search engines.<ref>{{cite news |first=Tom |last=Krazit |url=http://news.cnet.com/8301-30684_3-10315117-265.html |title=Bing strikes licensing deal with Wolfram Alpha |publisher=CNET |date=August 21, 2009 |accessdate=2013-02-09}}</ref><ref>{{cite web |author=The Wolfram&#124;Alpha Team |date=April 18, 2011 |url=http://blog.wolframalpha.com/2011/04/18/wolframalpha-and-duckduckgo-partner-on-api-binding-and-search-integration/ |title=Wolfram&#124;Alpha and DuckDuckGo Partner on API Binding and Search Integration |work=Wolfram&#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}</ref> For factual [[question answering]], it is also queried by Apple's [[Siri (software)|Siri]], Samsung's [[S Voice]], as well as Dexetra's [[speech recognition]] software for the [[Android (operating system)|Android]] platform, Iris, and the voice control software on [[BlackBerry 10]].<ref>{{cite web|url=http://www.berryreview.com/2013/10/21/blackberry-teams-up-with-wolfram-alpha-for-blackberry-10-voice-control/|title=BlackBerry Teams Up with Wolfram Alpha For BlackBerry 10 Voice Control|work=BerryReview}}</ref>

== History ==
Launch preparations began on May 15, 2009 at 7&nbsp;pm [[Central Daylight Time (North America)#Central Daylight Time|CDT]] and were broadcast live on [[Justin.tv]]. The plan was to publicly launch the service a few hours later, with expected issues due to extreme load. The service was officially launched on May 18, 2009.<ref name="BBC">{{cite news |url=http://news.bbc.co.uk/1/hi/technology/8052798.stm |title=Wolfram 'search engine' goes live |publisher=BBC News |date=May 18, 2009 |accessdate=2013-02-09}}</ref>

Wolfram Alpha has received mixed reviews.<ref name="spivack">{{cite web |first=Nova |last=Spivack |title=Wolfram Alpha is Coming – and It Could be as Important as Google |date=March 7, 2009 |url=http://www.novaspivack.com/uncategorized/wolfram-alpha-is-coming-and-it-could-be-as-important-as-google |accessdate=2013-02-09 |publisher=Nova Spivack – Minding the Planet}}</ref><ref>{{cite news |first=Ryan |last=Singel |title=Wolfram&#124;Alpha Fails the Cool Test |date=May 18, 2009 |url=http://www.wired.com/epicenter/2009/05/wolframalpha-fails-the-cool-test/ |publisher=Wired |accessdate=2013-02-09}}</ref> Wolfram Alpha advocates point to its potential, some even stating that how it determines results is more important than current usefulness.<ref name="spivack"/>

On December 3, 2009, an [[iPhone]] app was introduced. Some users<ref name="ios-price">{{cite web |first=MG |last=Siegler |url=http://techcrunch.com/2009/12/03/wolfram-alpha-iphone-app/ |title=Nice Try, Wolfram Alpha. Still Not Paying $50 For Your App. |publisher=TechCrunch |date=December 3, 2009 |accessdate=2013-02-09}}</ref> considered the initial $50 price of the [[iOS]] app unnecessarily high, since the same features could be freely accessed by using a web browser instead. They also complained about the simultaneous removal of the mobile formatting option for the site.<ref name="mobile-format">{{cite news |url=http://www.tuaw.com/2009/12/03/wolframalpha-iphone-formatted-web-page-no-longer-available/ |first=TJ |last=Luoma |title=WolframAlpha iPhone-formatted web page no longer available |publisher=TUAW |date=December 3, 2009 |accessdate=2013-02-09}}</ref> Wolfram responded by lowering the price to $2, offering a refund to existing customers<ref name="refund">{{cite web|last=Broida |first=Rick |url=http://reviews.cnet.com/8301-19512_7-10471978-233.html |title=Get Wolfram Alpha app for $1.99-and a refund if you paid more |publisher=CNET |date=April 1, 2010 |accessdate=2012-02-28}}</ref> and re-instating the mobile site.

On October 6, 2010 an Android version of the app was released<ref>{{cite news |url=http://techcrunch.com/2010/10/06/wolframalphas-android-app-now-available/ |title=Wolfram Alpha's Android app now available |first=Leena |last=Rao |publisher=TechCrunch |date=October 6, 2010 |accessdate=2013-02-09}}</ref> and it is now available for Kindle Fire and Nook. (The Nook version is not available outside the US). A further 71 apps are available which use the Wolfram Alpha engine for specialized tasks.<ref>{{cite web |url=http://products.wolframalpha.com/mobile/ |title=Wolfram&#124;Alpha: Mobile & Tablet Apps |year=2013 |accessdate=2013-02-09 |publisher=Wolfram Alpha}}</ref>

== Wolfram Alpha Pro ==
On February 8, 2012, Wolfram Alpha Pro was released,<ref name="WAProAnnounce">{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2012/02/08/announcing-wolframalpha-pro/ |title=Announcing Wolfram&#124;Alpha Pro |date=February 8, 2012 |work=Wolfram&#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}</ref> offering users additional features for a monthly subscription fee. A key feature is the ability to upload many common file types and data—including raw tabular data, images, audio, XML, and dozens of specialized scientific, medical, and mathematical formats—for automatic analysis. Other features include an extended keyboard, interactivity with [[Computable Document Format|CDF]], data downloads, in-depth step by step solution, the ability to customize and save graphical and tabular results<ref name="Hachman">{{cite news |last=Hachman |first=Mark |title=Data Geeks, Meet Wolfram Alpha Pro |publisher=[[PC Magazine]] |date=February 7, 2012 |url=http://www.pcmag.com/article2/0,2817,2399911,00.asp |accessdate=2012-02-15}}</ref> and extra computation time.<ref name="WAProAnnounce" />

Along with new premium features, Wolfram Alpha Pro has led to some changes in the free version of the site:
* An increase in advertisements on the free site.
* Text and PDF export options now require the user to set up a free account<ref name="WAProAnnounce" /> even though they existed before the introduction of Wolfram Alpha accounts.<ref>{{cite web|url=http://hplusmagazine.com/2009/06/24/users-guide-wolframalpha/|title=A User's Guide to Wolfram Alpha|first=Surfdaddy|last=Orca|publisher=H+ Magazine|date=2009-06-24|accessdate=2013-04-24}}</ref>
* The option to request extra time for a long calculation used to be free<ref name="extra-time-before">{{cite web|url=http://web.mst.edu/~jkmq53/school/Fall_2011/English_160/files/Marlowe_Usability_Test.docx|title=Wolfram Alpha Usability Test Survey|first=James|last=Marlowe|year=2011|accessdate=2013-04-24}}</ref> but is now only available to subscribers.<ref name="WAProAnnounce" />
* Step-by-Step limited to 3 for free users (previously uncapped)(no longer available).<ref name="StepByStep">{{cite web|url=http://blog.wolframalpha.com/2009/12/01/step-by-step-math/|title=Step-by-Step Math}}</ref>

== Copyright claims ==
''[[InfoWorld]]'' published an article<ref name="copyright">{{cite web |last=McAllister |first=Neil |url=http://www.infoworld.com/d/developer-world/how-wolfram-alpha-could-change-software-248 |title=How Wolfram Alpha could change software |publisher=InfoWorld |date=July 29, 2009 |accessdate=2012-02-28}}</ref> warning readers of the potential implications of giving an automated website proprietary rights to the data it generates. [[Free software movement|Free software]] advocate [[Richard Stallman]] also opposes the idea of recognizing the site as a copyright holder and suspects that Wolfram would not be able to make this case under existing copyright law.<ref name="fsf">{{cite mailing list |url=http://lists.essential.org/pipermail/a2k/2009-August/004865.html |title=How Wolfram Alpha's Copyright Claims Could Change Software |date=August 4, 2009 |accessdate=2012-02-17 |mailinglist=[http://lists.essential.org/mailman/listinfo/a2k Access 2 Knowledge] |archiveurl=https://web.archive.org/web/20130428041345/http://lists.essential.org/pipermail/a2k/2009-August/004865.html |archivedate=April 28, 2013 |last=Stallman |first=Richard |authorlink=Richard Stallman}}</ref>

== See also ==
* [[Commonsense knowledge problem]]
* [[Artificial general intelligence|Strong AI]]
* [[Watson (computer)]]

== References ==
{{Reflist|colwidth=30em}}

== Further reading ==
* [http://www.businessweek.com/the_thread/techbeat/archives/2009/03/wolfram_alpha_a.html Wolfram Alpha: A New Way To Search?], Stephen Wildstrom, ''BusinessWeek'', March 9, 2009.
* [http://www.informationweek.com/news/internet/search/showArticle.jhtml?articleID=215801388&subSection=News Stephen Wolfram's Answer To Google: If Wolfram/Alpha works as advertised, it will be able to do something Google can't: provide answers that don't already exist in indexed documents.] by Thomas Claburn, ''InformationWeek'', March 10, 2009.
* [http://bits.blogs.nytimes.com/2009/03/09/better-search-doesnt-mean-beating-google/ Better Search Doesn’t Mean Beating Google] by Saul Hansell, ''The New York Times'', March 9, 2009.
* [http://www.pcworld.com/article/160904/wolfram_alpha_will_take_your_questions_any_questions.html Wolfram Alpha will Take Your Questions – Any Questions], Ian Paul, ''PC World'', March 9, 2009.
* [http://www.hplusmagazine.com/articles/ai/wolframalpha-searching-truth Wolfram Alpha: Searching for Truth: Stephen Wolfram talks with Rudy Rucker about his Upcoming Release] by [[Rudy Rucker]], ''H+ Magazine''.
*  [http://www.boston.com/business/technology/articles/2009/05/05/a_hungry_little_number_cruncher/ "A hungry little number cruncher: Wolfram Alpha search tool mines databases to yield math-based replies"] by [[Hiawatha Bray]], ''[[The Boston Globe]]'', May 5, 2009
* [http://newsbreaks.infotoday.com/NewsBreaks/Wolfram-Alpha-Semantic-Search-Is-Born-53892.asp "Wolfram Alpha: Semantic Search is Born" by [[Woody Evans]], May 21, 2009.]

== External links ==
* {{official website}}

{{Wolfram Research|state=uncollapsed}}
{{computable knowledge}}
{{Intelligent personal assistant software}}

[[Category:Agent-based software]]
[[Category:Computer algebra systems]]
[[Category:Educational math software]]
[[Category:Educational websites]]
[[Category:Information retrieval systems]]
[[Category:Intelligent software assistants]]
[[Category:Internet properties established in 2009]]
[[Category:Mathematics education]]
[[Category:Natural language processing software]]
[[Category:Open educational resources]]
[[Category:Physics education]]
[[Category:Semantic Web]]
[[Category:Software calculators]]
[[Category:Web analytics]]
[[Category:Websites which mirror Wikipedia]]
[[Category:Wolfram Research]]
<=====doc_Id=====>:596
<=====title=====>:
Phynd
<=====text=====>:
'''Phynd''' (Find) is a LAN-indexing [[search engine]] used to facilitate [[peer-to-peer]] [[file sharing]] over a [[local-area network]].  It was developed by [[Rensselaer Polytechnic Institute]] student researcher Jesse Jordan to solve various problems experienced by Microsoft browsers and networks when trying to index files within a large network.  

One of the results of Jordan's file indexing exercise was that large numbers of downloaded music files were found on other users' local systems.  Jordan was relatively unconcerned with the nature of the content he was indexing.  His objective was enabling a network to index all its files without crashing any elements of the network. <ref>Lessig 2004, p. 48</ref>

Although Jordan's search engine, Phynd, merely indexed public data that users elected to share through an integrated sharing feature in [[Microsoft Windows]], Jordan was sued by [[RIAA]] for copyright infringement. The original Phynd search engine, rpi.phynd.net (defunct), existing years before and months after Jesse's lawsuit was shut down by the enormous pressure that the [[RIAA]] in November 2003 brought upon Jordan and his family. The RIAA was demanding $15,000,000 to settle.<ref>Lessig 2004, p. 51</ref>  As a student researcher, Jordan had only modest life savings of approximately $12,000, and his family had only modest assets.  His limited options were to fight the RIAA at enormous personal expense, or to settle. Jordan, chose to settle outside of court for $12,000, his entire life savings from student employment. He subsequently raised $12,005.67 via contributions on a personal web site in July 2003.

==References==
*Lessig, Lawrence (2004) . "Free Culture" . ISBN 1-59420-006-8 . The Penguin Press . New York 

==Notes==
{{reflist}} 


==External links==
* [http://poly.rpi.edu/old/article_view.php3?view=2599&part=1 Phynd server shut down by threat of lawsuit]
* {{webarchive |date=2013-01-20 |url=http://archive.is/20130120043253/http://news.com.com/2100-1027-995429.html?tag=fd_lede1_hed |title=RIAA sues campus file-swappers}}
* [http://www.isp-planet.com/news/2003/riaa_030505.html Students to Pay in RIAA Song-Swapping Suit]
* [http://articles.chicagotribune.com/2003-07-07/news/0307080008_1_recording-industry-donations-settlement $12,005.67: Amount Jesse Jordan, sued by the recording...]

[[Category:Information retrieval systems]]

{{compu-network-stub}}
<=====doc_Id=====>:599
<=====title=====>:
TREX search engine
<=====text=====>:
'''TREX''' is a search engine in the [[NetWeaver|SAP NetWeaver]] integrated technology platform produced by [[SAP AG]] using [[columnar storage]].<ref>{{cite journal|url=http://db.csail.mit.edu/pubs/abadi-column-stores.pdf|doi=10.1561/1900000024|title=The Design and Implementation of Modern Column-Oriented Database Systems|author1=Daniel Abadi|author2=Peter Boncz|author3=Stavros Harizopoulos|author4=Stratos Idreos|author5=Samuel Madden|journal=Foundations and Trends in Databases|volume=5|issue=3|year=2012|pages=197–280}}</ref> The TREX engine is a standalone component that can be used in a range of system environments but is used primarily as an integral part of such SAP products as Enterprise Portal, Knowledge Warehouse, and '''Business Intelligence (BI, formerly [[SAP Business Information Warehouse]]).''' In SAP NetWeaver BI, the TREX engine powers the BI Accelerator, which is a plug-in appliance for enhancing the performance of [[online analytical processing]]. The name "TREX" stands for '''Text Retrieval and information EXtraction''', but it is not a registered trade mark of SAP and is not used in marketing collateral.

==Search functions==

TREX supports various kinds of text search, including exact search, boolean search, wildcard search, linguistic search (grammatical variants are normalized for the index search) and fuzzy search (input strings that differ by a few letters from an index term are normalized for the index search). Result sets are ranked using term frequency-inverse document frequency ([[tf-idf]]) weighting, and results can include snippets with the search terms highlighted.

TREX supports text mining and classification using a [[vector space model]]. Groups of documents can be classified using query based classification, example based classification, or a combination of these plus keyword management.

TREX supports structured data search not only for document metadata but also for mass business data and data in SAP [[Business Objects]]. Indexes for structured data are implemented compactly using [[data compression]] and the data can be aggregated in linear time, to enable large volumes of data to be processed entirely in memory.

Recent developments include:
* A join engine to join structured data from different fields in business objects
* A fast update capability to write a delta index beside a main index and to merge them offline while a second delta index takes updates
* A [[data mining]] feature pack for advanced mathematical analysis

==History==

The first code for the engine was written in 1998 and TREX became an SAP component in 2000. The SAP NetWeaver BI Accelerator was first rolled out in 2005. As of Q1 2013, the current release of TREX is SAP NW 7.1.

==References==
{{Reflist}}

==External links==
* [http://www.sap.com/platform/netweaver/index.epx SAP NetWeaver]
* [http://www.sap.com/platform/netweaver/components/bi/index.epx SAP NetWeaver Business Intelligence]
* [http://www.sap.com/platform/netweaver/businessinformation.epx SAP NetWeaver Business Information Management]
* [http://scn.sap.com/docs/DOC-8489 Search and Classification (TREX) on SAP Community Network]

[[Category:SAP NetWeaver]]
[[Category:Information retrieval systems]]
[[Category:Business intelligence]]
<=====doc_Id=====>:602
<=====title=====>:
Pleade
<=====text=====>:
{{Infobox software
| name                   = Pleade-infoxbox
| title                  = Pleade
| logo                   = [[File:Pleade-logo.png]]
| logo caption           = Logo de Pleade
| screenshot             = <!-- [[File: ]] -->
| caption                = 
| collapsible            = 
| author                 = AJLSM
| developer              = AJLSM
| released               = <!-- {{Start date|YYYY|MM|DD|df=yes/no}} -->
| discontinued           = 
| latest release version = 3.4
| latest release date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| latest preview version = <!-- 3.5 -->
| latest preview date    = <!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} -->
| frequently updated     = <!-- DO NOT include this parameter unless you know what it does -->
| programming language   = [[Java]], [[XSLT]], [[Apache Cocoon|Cocoon]]
| operating system       = [[Unix-like]], [[Microsoft Windows]]
| platform               = 
| size                   = 
| language               = French, English, German, Chinese
| language count         = <!-- DO NOT include this parameter unless you know what it does -->
| language footnote      = 
| status                 = Active
| genre                  = Digital Library
| license                = GNU General Public License
| alexa                  = 
| website                = {{URL|http://www.pleade.com/}}
}}

'''Pleade''' is an open source [[search engine]] and browser for [[Finding aid|archival finding aids]] encoded in [[Encoded Archival Description|EAD]] (an XML standard for encoding archival finding aids). Based on the [[Secure Document Exchange|SDX]] platform, it is a very flexible web application.

== History ==
The software was jointly started by the companies AJLSM and Anaphore and was originally intended for publication and dissemination only of archival research tools like EAD finding aids, but it has become a library portal and a medium for digital libraries.<ref>[http://www.digicult.info/downloads/dc_info_issue6_december_20031.pdf DigiCult.Info issue #6, page 16]</ref>

==Technologies==
Pleade is published in GPL 3. It is based on the [[Apache Cocoon|Apache Cocoon framework]] and it works with the search engine SDX.

It is able to publish and distribute the following format : [[Encoded Archival Description|EAD]], [[Comma-separated values|CSV]] (internally converted to XML), [[XMLMarc]], [[Text Encoding Initiative|TEI]], [[Dublin Core]]. Support for [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]] is under active development.<ref>[http://pleade.com/ Pleade 2012 : les imprimés numérisés et les formats XML METS / ALTO]</ref>

== Features ==
* Customizable publication ;
* Customizable index creation ;
* Customizable search form ;
* Simple and advanced search among publish documents ;
* Federate search among different bases (e.g. EAD, METS) ;
* basket (for database and for images), a search history, printing, etc. ;
* document viewer supporting : [[JPEG]], [[TIFF]] and for high resolution TIFF and [[JPEG2000]] it use [http://iipimage.sourceforge.net/ IIPImage image server] ;
* [[OAI-PMH]] repositories and expose them, by default, the format EAD, Dublin Core and [[Dublin Core#Qualified Dublin Core|Qualified DC]] ;
* The viewer has a Pleade indexing module (paleographic) that can be used to permit correction of the OCR. This tool is a TEI export of data input. A workflow management allows annotators and validation records seized ;
* Printing resulting and finding aids as PDF documents (with embedded images) ;
* Compatible with standard archival format : [[Text Encoding Initiative|TEI]], [[BiblioML]] ;
* Ability to import metadata from an [[Integrated library system|ILS]].

=== Pleade-Entreprise ===
* Pleade-Entreprise extended features to others XML format, such as [[Metadata Encoding and Transmission Standard|METS]] and [[ALTO (XML)|ALTO]].

== Examples ==
These are examples of websites based on Pleade:
{{columns-list|2|
* Archival portals
** [http://archives-inventaires.loire-atlantique.fr/ Departmental records of Loire-Atlantique (AD 44) (AD 44)]
** [http://gael.gironde.fr/ GAEL : GAEL: Gironde archives online]
** [http://odysseo.org/ Odysseo: Resources for the history of immigration]
** [http://taubira.anaphore.org/ Parliamentary work of Christiane Taubira]
** [http://archivesetmanuscrits.bnf.fr/ Archives and manuscrits of the BNF French National Library]
** [http://jubilotheque.upmc.fr/ Jubilothèque, UPMC's scientific digital library]
** [http://lbf-ehess.ens-lyon.fr/pages/fonds.html Michel Foucault's Library "les Mots et les Choses" ENS]
* Portals documentary
** [http://www.michael-culture.org/fr/home Michael]
** [http://www.numerique.culture.fr/mpf/pub-fr/index.html Digital Heritage]
* Digital Libraries
** Digital Library of Lille
** Lille III
** [http://archivesetmanuscrits.bnf.fr/ BNF: Archives and manuscripts (French National Library)]
}}

== Related resources ==
* {{Official website|http://pleade.com}}
* [http://demo.pleade.com Official demo]
* [http://www.pleadeenpratique.org/ Pleade in practice]
* [http://www.ajlsm.com/produits/sdx SDX]
* [http://www.ajlsm.com AJLSM company]

== References ==
<references/>

[[Category:Digital library software]]
[[Category:Free software]]
[[Category:Information retrieval systems]]
[[Category:Archival science]]
<=====doc_Id=====>:605
<=====title=====>:
Dynatext
<=====text=====>:
{{primary sources|date=October 2011}}
'''DynaText''' is an [[SGML]] publishing tool. It was introduced in 1990, and was the first system to handle arbitrarily large SGML documents, and to render them according to multiple style-sheets that could be switched at will.

DynaText and its Web sibling DynaWeb won multiple [[Seybold]] and other awards [http://xml.coverpages.org/ebt-award.html][http://xml.coverpages.org/dynaweb3-dvi.html], and there are eleven US Patents related to the DynaText technology: 5,557,722; 5,644,776; 5,708,806; 5,893,109; 5,983,248; 6,055,544; 6,101,511; 6,101,512; 6,105,044; 6,167,409; and 6,546,406.

DynaText was developed by Electronic Book Technologies, Incorporated, of [[Providence, Rhode Island]]. EBT was founded by [[Louis Reynolds]], [[Steven DeRose]], [[Jeffrey Vogel]], and [[Andries van Dam]], and was sold to [[Inso]] corporation in 1996, when it had about 150 employees.

DynaText heavily influenced stylesheet technologies such as [[DSSSL]] and [[CSS]], and [[XML]] chairman [[Jon Bosak]] cites EBT chief architect [[Steven DeRose]] as one of the originators of the notion of [[well-formed document|well-formedness]] formalized in [[XML]], as well as DynaText for influencing the design of Web browsers in general [http://www.ibiblio.org/bosak/cv.htm].

[[Inso]] corporation went out of business in 2002.

==Technology==

DynaText accepted [[SGML]] as input, and built a binary representation of the structure (similar to [[Document Object Model|DOM]] for [[XML]], but persistent), as well as a full-text [[inverted index]] of the text, elements, and attributes. Customers typically distributed such compiled e-books on CD-ROM or via network servers. Later versions of DynaText could also read SGML on the fly, providing exactly the same interface.

Unlike many prior systems, DynaText was not limited to any particular [[Document type definition|DTD]] (or [[XML schema|schema]]). Rather, customers could build style sheets in a simple language (also SGML-based), using properties very much like the later [[DSSSL]], [[CSS]], and [[XSL-FO]]. However, every property could have an expression as its value, which would be evaluated (if necessary) for each element the style applied to. Graphics, tables, formulae, and plug-ins could be included in documents.

Unlike nearly all prior SGML systems, DynaText was not limited to documents that could fit in [[RAM]] on the viewing or serving computer system. Users commonly created documents in the tens to hundreds of MB. DynaText customers included aerospace, workstation and other computer industry firms, government, literary and technical publishers, and others.

Full-text searches were based on an inverted index of words and other tokens (except for Japanese text, which was handled specially). Dynatext could report the number of "hits" for a given search, that occur within each section in the table of contents (by default, the table of contents appeared in a separate pane as an expandable outline, and clicking on any entry scrolled the full-text pane to the start of the corresponding section). Searches could also restrict hits to particular SGML element types, or sequences of types; refer to attributes; and use Boolean operators and parentheses. The "and" operator restricted its operands to occurring near each other, by default in the same paragraph or comparable element.

==References==
*[http://www.w3.org/History/19921103-hypertext/hypertext/Products/DynaText/Overview.html DynaText Notes] by [[Tim Berners-Lee]] (this note refers to a pre-release or very early release of DynaText).
*{{cite journal
 | id = MS
 | last = Smith
 | first = MacKenzie
 | title = Review: DynaText: An Electronic Publishing System
 | journal = Computers and the Humanities
 | volume = 27
 | issue = 5/6
 | pages = 415–420
 | publisher = Springer
 | location =
 | date = 1993
 | jstor = http://www.jstor.org/stable/30204569
 | issn = 0010-4817
 }}

*{{cite book
 | url = http://techpubs.sgi.com/library/dynaweb_docs/0630/SGI_EndUser/books/IIDWeb_UG/sgi_html/ch05.html
 | title = IRIS InSight™ DynaWeb™ User's Guide: Chapter 5. Introduction to the DynaText Search Language
 | publisher = Silicon Graphics, Inc.
}} Document Number: 007-3229-001

*{{cite journal
 | url = http://www.w3.org/Conferences/WWW4/ora_951122/112.html
 | title = DynaWeb: Interfacing Large SGML Repositories and the WWW
 | journal = Fourth International World Wide Web Conference: ``The Web Revolution''
 | date = 1995
 | location = Boston
 | first = Gavin Thomas 
 | last = Nicol
}}

[[Category:Information retrieval systems]]
<=====doc_Id=====>:608
<=====title=====>:
Relevance (information retrieval)
<=====text=====>:
{{other uses|Relevance (disambiguation)}}

In [[information science]] and [[information retrieval]], '''relevance''' denote how well a retrieved document or set of documents meets the [[information need]] of the user. Relevance may include concerns such as timeliness, authority or novelty of the result.

== History ==

The concern with the problem of finding relevant information dates back at least to the first publication of scientific journals in the 17th century.{{citation needed|date=June 2015}}

The formal study of relevance began in the 20th Century with the study of what would later be called [[bibliometrics]]. In the 1930s and 1940s, S. C. Bradford used the term "relevant" to characterize articles relevant to a subject (cf., [[Bradford's law]]). In the 1950s, the first information retrieval systems emerged, and researchers noted the retrieval of irrelevant articles as a significant concern. In 1958, B. C. Vickery made the concept of relevance explicit in an address at the International Conference on Scientific Information.<ref>Mizzaro, S. (1997). Relevance: The Whole History. Journal of the American Society for Information Science. 48, 810‐832.</ref>

Since 1958, information scientists have explored and debated definitions of relevance. A particular focus of the debate was the distinction between "relevance to a subject" or "topical relevance" and "user relevance".{{citation needed|date=June 2015}}

== Evaluation ==
{{main article|Information retrieval#Performance and correctness measures}}

The information retrieval community has emphasized the use of test collections and benchmark tasks to measure topical relevance, starting with the [[Cranfield Experiments]] of the early 1960s and culminating in the [[Text Retrieval Conference|TREC]] evaluations that continue to this day as the main evaluation framework for information retrieval research.{{citation needed|date=June 2015}}

In order to evaluate how well an [[information retrieval]] system retrieved topically relevant results, the relevance of retrieved results must be quantified. In [[Cranfield Experiments|Cranfield]]-style evaluations, this typically involves assigning a ''relevance level'' to each retrieved result, a process known as ''relevance assessment''. Relevance levels can be binary (indicating a result is relevant or that it is not relevant), or graded (indicating results have a varying degree of match between the topic of the result and the information need).   Once relevance levels have been assigned to the retrieved results, [[Information retrieval#Performance measures|information retrieval performance measures]] can be used to assess the quality of a retrieval system's output.

In contrast to this focus solely on topical relevance, the information science community has emphasized user studies that consider user relevance.{{citation needed|date=June 2015}} These studies often focus on aspects of [[human-computer interaction]] (see also [[human-computer information retrieval]]).

== Clustering and relevance ==

The [[cluster hypothesis]], proposed by [[C. J. van Rijsbergen]] in 1979, asserts that two documents that are similar to each other have a high likelihood of being relevant to the same information need. With respect to the embedding similarity space, the cluster hypothesis can be interpreted globally or locally.<ref name=diazthesis>F. Diaz, Autocorrelation and Regularization of Query-Based Retrieval Scores. PhD thesis, University of Massachusetts Amherst, Amherst, MA, February 2008, Chapter 3.</ref>    The global interpretation assumes that there exist some fixed set of underlying topics derived from inter-document similarity. These global clusters or their representatives can then be used to relate relevance of two documents (e.g. two documents in the same cluster should both be relevant to the same request). Methods in this spirit include:
* cluster-based information retrieval<ref name=croftcbir>W. B. Croft, “A model of cluster searching based on classification,” Information Systems, vol. 5, pp. 189–195, 1980.</ref><ref name=griffithscbir>A. Griffiths, H. C. Luckhurst, and P. Willett, “Using interdocument similarity information in document retrieval systems,” Journal of the American Society for Information Science, vol. 37, no. 1, pp. 3–11, 1986.</ref>
* cluster-based document expansion such as [[latent semantic analysis]] or its language modeling equivalents.<ref name=lmcbir>X. Liu and W. B. Croft, “Cluster-based retrieval using language models,” in SIGIR ’04: Proceedings of the 27th annual international conference on Research and development in information retrieval, (New York, NY, USA), pp. 186–193, ACM Press, 2004.</ref>    It is important to ensure that clusters – either in isolation or combination – successfully model the set of possible relevant documents.

A second interpretation, most notably advanced by Ellen Voorhees,<ref name=voorheescbir>E. M. Voorhees, “The cluster hypothesis revisited,” in SIGIR ’85: Proceedings of the 8th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 188–196, ACM Press, 1985.</ref>    focuses on the local relationships between documents. The local interpretation avoids having to model the number or size of clusters in the collection and allow relevance at multiple scales. Methods in this spirit include,
* multiple cluster retrieval<ref name=griffithscbir/><ref name=voorheescbir/>
* spreading activation<ref name=preece>S. Preece, A spreading activation network model for information retrieval. PhD thesis, University of Illinois, Urbana-Champaign, 1981.</ref> and relevance propagation<ref name=relprop>T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma, “A study of relevance propagation for web search,” in SIGIR ’05: Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 408–415, ACM Press, 2005.</ref> methods
* local document expansion<ref name=docexpansion>A. Singhal and F. Pereira, “Document expansion for speech retrieval,” in SIGIR ’99: Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, (New York, NY, USA), pp. 34–41, ACM Press, 1999.</ref>
* score regularization<ref name=diazreg>F. Diaz, “Regularizing query-based retrieval scores,” Information Retrieval, vol. 10, pp. 531–562, December 2007.</ref>
Local methods require an accurate and appropriate document similarity measure.

==Problems and alternatives==

The documents which are most relevant are not necessarily those which are most useful to display in the first page of search results.  For example, two duplicate documents might be individually considered quite relevant, but it is only useful to display one of them.  A measure called "maximal marginal relevance" (MMR) has been proposed to overcome this shortcoming. It considers the relevance of each document only in terms of how much new information it brings given the previous results.<ref>{{cite journal|last1=Carbonell|first1=Jaime|last2=Goldstein|first2=Jade|title=The use of MMR, diversity-based reranking for reordering documents and producing summaries|journal=Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval|date=1998|doi=10.1145/290941.291025|url=http://dl.acm.org/citation.cfm?id=291025}}</ref>

In some cases, a query may have an ambiguous interpretation, or a variety of potential responses.  Providing a diversity of results can be a consideration when evaluating the utility of a result set.<ref>http://www.dcs.gla.ac.uk/workshops/ddr2012/</ref>

==References==
 {{reflist}}

==Additional reading==
*Hjørland, B. (2010). The foundation of the concept of relevance. Journal of the American Society for Information Science and Technology, 61(2), 217-237.
*Relevance : communication and cognition. by Dan Sperber; Deirdre Wilson. 2nd ed. Oxford; Cambridge, MA: Blackwell Publishers, 2001. ISBN 978-0-631-19878-9
*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part II: nature and manifestations of relevance. Journal of the American Society for Information Science and Technology, 58(3), 1915-1933. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20II%20JASIST%20%2707.pdf pdf])
*Saracevic, T. (2007). Relevance: A review of the literature and a framework for thinking on the notion in information science. Part III: Behavior and effects of relevance. Journal of the American Society for Information Science and Technology, 58(13), 2126-2144. ([http://www.scils.rutgers.edu/~tefko/Saracevic%20relevance%20pt%20III%20JASIST%20%2707.pdf pdf])
*Saracevic, T. (2007). Relevance in information science. Invited Annual Thomson Scientific Lazerow Memorial Lecture at School of Information Sciences, University of Tennessee. September 19, 2007. ([http://www.sis.utk.edu/lazerow2007 video])
*Introduction to Information Retrieval: Evaluation. Stanford. ([http://web.stanford.edu/class/cs276/handouts/EvaluationNew-handout-6-per.pdf presentation in PDF])

[[Category:Information retrieval evaluation]]
<=====doc_Id=====>:611
<=====title=====>:
Precision and recall
<=====text=====>:
[[File:Precisionrecall.svg|thumb|350px|Precision and recall]]
In [[pattern recognition]] and [[information retrieval]] with [[binary classification]], '''precision''' (also called [[positive predictive value]]) is the fraction of retrieved instances that are relevant, while '''recall''' (also known as [[Sensitivity and specificity|sensitivity]]) is the fraction of relevant instances that are retrieved. Both precision and recall are therefore based on an understanding and measure of [[relevance]].

Suppose a computer program for recognizing dogs in scenes from a video identifies 7 dogs in a scene containing 9 dogs and some cats. If 4 of the identifications are correct, but 3 are actually cats, the program's precision is 4/7 while its recall is 4/9.  When a [[Search engine (computing)|search engine]] returns 30 pages only 20 of which were relevant while failing to return 40 additional relevant pages, its precision is 20/30 = 2/3 while its recall is 20/60 = 1/3.  So, in this case, precision is "how useful the search results are", and recall is "how complete the results are".

In [[statistics]], if the [[null hypothesis]] is that all and only the relevant items are retrieved, absence of [[type I and type II errors]] corresponds respectively to maximum precision (no false positive) and maximum recall (no false negative).  The above pattern recognition example contained 7 &minus; 4 = 3 type I errors and 9 &minus; 4 = 5 type II errors.  Precision can be seen as a measure of exactness or ''quality'', whereas recall is a measure of completeness or ''quantity''.

In simple terms, high precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.

==Introduction==
In an information retrieval scenario, the instances are documents and the task is to return a set of relevant documents given a search term; or equivalently, to assign each document to one of two categories, "relevant" and "not relevant".  In this case, the "relevant" documents are simply those that belong to the "relevant" category.  Recall is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of existing relevant documents'', while precision is defined as the ''number of relevant documents'' retrieved by a search ''divided by the total number of documents retrieved'' by that search.

In a [[classification (machine learning)|classification]] task, the precision for a class is the ''number of true positives'' (i.e. the number of items correctly labeled as belonging to the positive class) ''divided by the total number of elements labeled as belonging to the positive class'' (i.e. the sum of true positives and [[Type I and type II errors|false positives]], which are items incorrectly labeled as belonging to the class).  Recall in this context is defined as the ''number of true positives divided by the total number of elements that actually belong to the positive class'' (i.e. the sum of true positives and [[Type I and type II errors|false negatives]], which are items which were not labeled as belonging to the positive class but should have been).

In information retrieval, a perfect precision score of 1.0 means that every result retrieved by a search was relevant (but says nothing about whether all relevant documents were retrieved) whereas a perfect recall score of 1.0 means that all relevant documents were retrieved by the search (but says nothing about how many irrelevant documents were also retrieved).

In a classification task, a precision score of 1.0 for a class C means that every item labeled as belonging to class C does indeed belong to class C (but says nothing about the number of items from class C that were not labeled correctly) whereas a recall of 1.0 means that every item from class C was labeled as belonging to class C (but says nothing about how many other items were incorrectly also labeled as belonging to class C).

Often, there is an inverse relationship between precision and recall, where it is possible to increase one at the cost of reducing the other. Brain surgery provides an illustrative example of the tradeoff.  Consider a brain surgeon tasked with removing a cancerous tumor from a patient’s brain. The surgeon needs to remove all of the tumor cells since any remaining cancer cells will regenerate the tumor. Conversely, the surgeon must not remove healthy brain cells since that would leave the patient with impaired brain function. The surgeon may be more liberal in the area of the brain she removes to ensure she has extracted all the cancer cells. This decision increases recall but reduces precision.  On the other hand, the surgeon may be more conservative in the brain she removes to ensure she extracts only cancer cells. This decision increases precision but reduces recall. That is to say, greater recall increases the chances of removing healthy cells (negative outcome) and increases the chances of removing all cancer cells (positive outcome).  Greater precision decreases the chances of removing healthy cells (positive outcome) but also decreases the chances of removing all cancer cells (negative outcome).

Usually, precision and recall scores are not discussed in isolation. Instead, either values for one measure are compared for a fixed level at the other measure (e.g. ''precision at a recall level of 0.75'') or both are combined into a single measure. Examples for measures that are a combination of precision and recall are the [[Precision and recall#F-measure|F-measure]] (the weighted [[harmonic mean]] of precision and recall), or the [[Matthews correlation coefficient]], which is a [[geometric mean]] of the chance-corrected variants: the [[regression coefficient]]s [[Informedness]] (DeltaP') and [[Markedness]] (DeltaP).<ref name="Powers2011" /><ref>{{cite journal |first1=P. |last1=Perruchet |first2=R. |last2=Peereman |year=2004 |title=The exploitation of distributional information in syllable processing |journal=J. Neurolinguistics |volume=17 |pages=97–119 |doi=10.1016/s0911-6044(03)00059-9}}</ref> [[Accuracy and precision#In binary classification|Accuracy]] is a weighted arithmetic mean of Precision and Inverse Precision (weighted by Bias) as well as a weighted arithmetic mean of Recall and Inverse Recall (weighted by Prevalence).<ref name="Powers2011"/> Inverse Precision and Recall are simply the Precision and Recall of the inverse problem where positive and negative labels are exchanged (for both real classes and prediction labels).  Recall and Inverse Recall, or equivalently true positive rate and false positive rate, are frequently plotted against each other as [[Receiver operating characteristic|ROC]] curves and provide a principled mechanism to explore operating point tradeoffs. Outside of Information Retrieval, the application of Recall, Precision and F-measure are argued to be flawed as they ignore the true negative cell of the contingency table, and they are easily manipulated by biasing the predictions.<ref name="Powers2011"/>  The first problem is 'solved' by using [[Accuracy and precision#In binary classification|Accuracy]] and the second problem is 'solved' by discounting the chance component and renormalizing to [[Cohen's kappa]], but this no longer affords the opportunity to explore tradeoffs graphically. However, [[Informedness]] and [[Markedness]] are Kappa-like renormalizations of Recall and Precision,<ref>{{cite conference |first=David M. W. |last=Powers |date=2012 |title=The Problem with Kappa |booktitle=Conference of the European Chapter of the Association for Computational Linguistics (EACL2012) Joint ROBUS-UNSUP Workshop}}</ref> and their geometric mean [[Matthews correlation coefficient]] thus acts like a debiased F-measure.

== Definition (information retrieval context) ==

In [[information retrieval]] contexts, precision and recall are defined in terms of a set of ''retrieved documents'' (e.g. the list of documents produced by a [[web search engine]] for a query) and a set of ''relevant documents'' (e.g. the list of all documents on the internet that are relevant for a certain topic), cf. [[relevance]]. The measures were defined in {{harvtxt|Perry|Kent|Berry|1955}}.

===Precision===

In the field of [[information retrieval]], precision is the fraction of retrieved documents that are [[Relevance (information retrieval)|relevant]] to the query:

<math display="block"> \text{precision}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{retrieved documents}\}|} </math>

Precision takes all retrieved documents into account, but it can also be evaluated at a given cut-off rank, considering only the topmost results returned by the system. This measure is called ''precision at n'' or ''P@n''.

For example for a text search on a set of documents precision is the number of correct results divided by the number of all returned results.

Precision is also used with [[recall (information retrieval)|recall]], the percent of ''all'' relevant documents that is returned by the search. The two measures are sometimes used together in the [[F1 Score]] (or f-measure) to provide a single measurement for a system.

Note that the meaning and usage of "precision" in the field of information retrieval differs from the definition of [[accuracy and precision]] within other branches of science and technology.

===Recall===

Recall in information retrieval is the fraction of the documents that are relevant to the query that are successfully retrieved.

<math display="block"> \text{recall}=\frac{|\{\text{relevant documents}\}\cap\{\text{retrieved documents}\}|}{|\{\text{relevant documents}\}|} </math>

For example for text search on a set of documents recall is the number of correct results divided by the number of results that should have been returned.

In binary classification, recall is called [[Sensitivity and specificity#Sensitivity|sensitivity]]. So it can be looked at as the probability that a relevant document is retrieved by the query.

It is trivial to achieve recall of 100% by returning all documents in response to any query. Therefore, recall alone is not enough but one needs to measure the number of non-relevant documents also, for example by computing the precision.

== Definition (classification context) ==
For classification tasks, the terms ''true positives'', ''true negatives'', ''false positives'', and ''false negatives'' (see [[Type I and type II errors]] for definitions) compare the results of the classifier under test with trusted external judgments.  The terms ''positive'' and ''negative'' refer to the classifier's prediction (sometimes known as the ''expectation''), and the terms ''true'' and ''false'' refer to whether that prediction corresponds to the external judgment (sometimes known as the ''observation'').

Let us define an experiment from ''P'' positive instances and ''N'' negative instances for some condition. The four outcomes can be formulated in a 2×2 [[contingency table]] or [[confusion matrix]], as follows:

{{DiagnosticTesting_Diagram}}
{{Confusion matrix terms}}

<!--
{| border="0" align="center" style="text-align: center; background: #FFFFFF;"
|+
!
! colspan="2" style="background: #ddffdd;"|actual class <br/> (observation)
|-
!
|-----
|+
! rowspan="2" style="background: #ffdddd;"|predicted class <br/> (expectation)
| '''tp''' <br> (true positive) <br/> Correct result
| '''fp''' <br> (false positive) <br/> Unexpected result
|-bgcolor="#EFEFEF"
| '''fn''' <br> (false negative) <br/> Missing result
| '''tn''' <br> (true negative) <br/> Correct absence of result
|+
|}

-->

Precision and recall are then defined as:<ref name="OlsonDelen">Olson, David L.; and Delen, Dursun (2008); ''Advanced Data Mining Techniques'', Springer, 1st edition (February 1, 2008), page 138, ISBN 3-540-76916-1</ref>

<math display="block">\text{Precision}=\frac{tp}{tp+fp} \, </math>

<math display="block">\text{Recall}=\frac{tp}{tp+fn} \, </math>

Recall in this context is also referred to as the true positive rate or [[Sensitivity and specificity|sensitivity]], and precision is also referred to as [[positive predictive value]] (PPV); other related measures used in classification include true negative rate and [[Accuracy and precision#In binary classification|accuracy]].<ref name="OlsonDelen" /> True negative rate is also called [[Specificity (tests)#Specificity|specificity]].

<math display="block">\text{True negative rate}=\frac{tn}{tn+fp} \, </math>

<math display="block">\text{Accuracy}=\frac{tp+tn}{tp+tn+fp+fn} \, </math>

== Probabilistic interpretation ==

It is possible to interpret precision and recall not as ratios but as probabilities:

* Precision is the probability that a (randomly selected) retrieved document is relevant.
* Recall is the probability that a (randomly selected) relevant document is retrieved in a search.

Note that the random selection refers to a uniform distribution over the appropriate pool of documents; i.e. by ''randomly selected retrieved document'', we mean selecting a document from the set of retrieved documents in a random fashion. The random selection should be such that all documents in the set are equally likely to be selected.

Note that, in a typical classification system, the probability that a retrieved document is relevant depends on the document. The above interpretation extends to that scenario also (needs explanation).

Another interpretation for precision and recall is as follows. Precision is the average probability of relevant retrieval. Recall is the average probability of complete retrieval. Here we average over multiple retrieval queries.

== F-measure ==
{{main article|F1 score}}
A measure that combines precision and recall is the [[harmonic mean]] of precision and recall, the traditional F-measure or balanced F-score:

<math display="block">F = 2 \cdot \frac{\mathrm{precision} \cdot \mathrm{recall}}{ \mathrm{precision} + \mathrm{recall}}</math>

This measure is approximately the average of the two when they are close, and is more generally the [[harmonic mean]], which, for the case of two numbers, coincides with the square of the [[geometric mean]] divided by the [[arithmetic mean]]. There are several reasons that the F-score can be criticized in particular circumstances due to its bias as an evaluation metric.<ref name="Powers2011" /> This is also known as the <math>F_1</math> measure, because recall and precision are evenly weighted.

It is a special case of the general <math>F_\beta</math> measure (for non-negative real values of&nbsp;<math>\beta</math>):

<math display="block">F_\beta = (1 + \beta^2) \cdot \frac{\mathrm{precision} \cdot \mathrm{recall} }{ \beta^2 \cdot \mathrm{precision} + \mathrm{recall}}</math>

Two other commonly used <math>F</math> measures are the <math>F_2</math> measure, which weights recall higher than precision, and the <math>F_{0.5}</math> measure, which puts more emphasis on precision than recall.

The F-measure was derived by van Rijsbergen (1979) so that <math>F_\beta</math> "measures the effectiveness of retrieval with respect to a user who attaches <math>\beta</math> times as much importance to recall as precision".  It is based on van Rijsbergen's effectiveness measure <math>E_{\alpha} = 1 - \frac{1}{\frac{\alpha}{P} + \frac{1-\alpha}{R}}</math>, the second term being the weighted harmonic mean of precision and recall with weights <math>(\alpha, 1-\alpha)</math>.  Their relationship is <math>F_\beta = 1 - E_{\alpha}</math> where <math>\alpha=\frac{1}{1 + \beta^2}</math>.

==Limitations as goals==
There are other parameters and strategies for performance metric of information retrieval system, such as the area under the precision-recall curve (AUC).<ref>Zygmunt Zając. What you wanted to know about AUC.  http://fastml.com/what-you-wanted-to-know-about-auc/</ref>

For [[web document]] retrieval, if the user's objectives are not clear, the  precision and recall can't be optimized. As summarized by Lopresti,<ref>Lopresti, Daniel (2001); [http://www.csc.liv.ac.uk/~wda2001/Panel_Presentations/Lopresti/Lopresti_files/v3_document.htm ''WDA 2001 panel'']</ref>
{{quote|[[Browsing]] is a comfortable and powerful paradigm (the [[Serendipity|serendipity effect]]).
* Search results don't have to be very good.
* Recall?    Not important (as long as you get at least some good hits).
* Precision? Not important (as long as at least some of the hits on the first page you return are good).}}

==See also==
* [[Uncertainty coefficient]], also called ''proficiency''
* [[Sensitivity and specificity]]

== References ==
{{Reflist}}
{{refbegin}}
* Baeza-Yates, Ricardo; Ribeiro-Neto, Berthier (1999). ''Modern Information Retrieval''. New York, NY: ACM Press, Addison-Wesley, Seiten 75 ff. ISBN 0-201-39829-X
* Hjørland, Birger (2010); ''The foundation of the concept of relevance'', Journal of the American Society for Information Science and Technology, 61(2), 217-237
* Makhoul, John; Kubala, Francis; Schwartz, Richard; and Weischedel, Ralph (1999); [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.27.4637 ''Performance measures for information extraction''], in ''Proceedings of DARPA Broadcast News Workshop, Herndon, VA, February 1999''
* {{cite journal |title=Machine literature searching X. Machine language; factors underlying its design and development |year=1955 |doi=10.1002/asi.5090060411}}
* van Rijsbergen, Cornelis Joost "Keith" (1979); ''Information Retrieval'', London, GB; Boston, MA: Butterworth, 2nd Edition, ISBN 0-408-70929-4
{{refend}}

== External links ==
* [http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval – C. J. van Rijsbergen 1979]
* [http://www.text-analytics101.com/2014/10/computing-precision-and-recall-for.html Computing Precision and Recall for a Multi-class Classification Problem]

[[Category:Information retrieval evaluation]]
[[Category:Information science]]
[[Category:Bioinformatics]]

[[de:Beurteilung eines Klassifikators#Anwendung im Information Retrieval]]
<=====doc_Id=====>:614
<=====title=====>:
Mean reciprocal rank
<=====text=====>:
{{Refimprove|date=June 2007}}
The '''mean reciprocal rank''' is a [[statistic]] measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the [[multiplicative inverse]] of the rank of the first correct answer. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q:<ref>{{cite conference | title=Proceedings of the 8th Text Retrieval Conference | booktitle=TREC-8 Question Answering Track Report | author=E.M. Voorhees |year=1999 | pages=77&ndash;82}}</ref>

:<math> \text{MRR} = \frac{1}{|Q|} \sum_{i=1}^{|Q|} \frac{1}{\text{rank}_i}. \!</math>

where <math> \text{rank}_i</math> refers to the rank position of the ''first'' relevant document for the ''i''-th query.

The reciprocal value of the mean reciprocal rank corresponds to the [[harmonic mean]] of the ranks.

== Example ==
For example, suppose we have the following three sample queries for a system that tries to translate English words to their plurals.  In each case, the system makes three guesses, with the first one being the one it thinks is most likely correct:

{| class="wikitable"
|-
! Query
! Results
! Correct response
! Rank
! Reciprocal rank
|-
| cat
| catten, cati, '''cats'''
| cats
| 3
| 1/3
|-
|tori
| torii, '''tori''', toruses
| tori
| 2
| 1/2
|-
| virus
| '''viruses''', virii, viri
| viruses
| 1
| 1
|}

Given those three samples, we could calculate the mean reciprocal rank as (1/3&nbsp;+&nbsp;1/2&nbsp;+&nbsp;1)/3 = 11/18 or about 0.61.

This basic definition does not specify what to do if none of the proposed results are correct, though reciprocal rank 0 could be used in this situation.  It also does not specify what do to if there are multiple correct answers in the list. In this case, [[Information retrieval#Mean average precision|mean average precision]] is a potential alternative metric.

==See also==
* [[Information retrieval]]
* [[Question answering]]

==References==
{{Reflist}}

==External links==
* {{cite conference | title=Evaluating web-based question answering systems | booktitle=Proceedings of LREC |author1=D. R. Radev |author2=H. Qi |author3=H. Wu |author4=W. Fan |year=2002 }}

[[Category:Summary statistics]]
[[Category:Information retrieval evaluation]]
<=====doc_Id=====>:617
<=====title=====>:
Queries per second
<=====text=====>:
{{distinguish-redirect|Query rate|Query throughput}}
'''Queries per second''' (QPS) is a common measure of the amount of search traffic an [[information retrieval]] system, such as a [[search engine]] or a [[database]], receives during one second.<ref>[http://www.microsoft.com/enterprisesearch/en/us/search-glossary.aspx#Q Microsoft's search glossary]</ref> The term is used more broadly for any [[request–response]] system, more correctly called [[requests per second]] (RPS).

High-traffic systems must watch their QPS in order to know when to scale the system to handle more load.

== References ==
{{reflist}}

[[Category:Units of frequency]]
[[Category:Information retrieval evaluation]]

{{computer-stub}}
<=====doc_Id=====>:620
<=====title=====>:
Cranfield experiments
<=====text=====>:
The '''Cranfield experiments''' were computer information retrieval experiments conducted by [[Cyril W. Cleverdon]] at [[Cranfield University]] in the 1960s, to evaluate the efficiency of indexing systems.<ref>Cleverdon, C. W. (1960). ASLIB Cranfield research project on the comparative efficiency of indexing systems. ASLIB Proceedings, XII, 421-431.</ref><ref>Cleverdon, C. W. (1967). The Cranfield tests on index language devices. Aslib Proceedings, 19(6), 173-194.</ref><ref>Cleverdon, C. W., & Keen, E. M. (1966). Factors determining the performance of indexing systems. Vol. 1: Design, Vol. 2: Results. Cranfield, UK: Aslib Cranfield Research Project. 
</ref>

They represent the prototypical evaluation model of [[information retrieval]] systems, and this model has been used in large-scale information retrieval evaluation efforts such as the [[Text Retrieval Conference]] (TREC).

==See also==
*[[ASLIB]]
*[[Information history]]

==References==
{{Reflist}}

==External links==
* [http://ir.dcs.gla.ac.uk/resources/test_collections/cran/ Cranfield 1400 corpus]

[[Category:Experiments]]
[[Category:Information retrieval evaluation]]


{{database-stub}}
<=====doc_Id=====>:623
<=====title=====>:
Query likelihood model
<=====text=====>:
The '''query likelihood model''' is a [[language model]] used in [[information retrieval]]. A language model is constructed for each document in the collection.  It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the [[Likelihood function|likelihood]] of a document being relevant given a query.

==Calculating the likelihood==
Using [[Bayes' theorem|Bayes' rule]], the probability <math>P</math> of a document <math>d</math>, given a query <math>q</math> can be written as follows:

:<math>
 P(d|q) = \frac{P(q|d) P(d)}{P(q)}
</math>

Since the probability of the query P(q) is the same for all documents, this can be ignored. Further, it is typical to assume that the probability of documents is uniform. Thus, P(d) is also ignored.

:<math>
 P(d|q) = P(q|d)
</math>

Documents are then ranked by the probability that a query is observed as a random sample from the document model. The multinomial unigram language model is commonly used to achieve this. We have:
:<math>
 P(q|M_d) = K_q \prod_{t \in V} P(t|M_d)^{tf_{t,q}}
</math>,where the multinomial coefficient is <math>K_q = L_q!/(tf_{t1,q}!tf_{t2,q}!...tf_{tN,q}!)</math> for query {{math|q}}, 

and <math>L_q = \sum_{1 \leq i \leq N}tf_{t_i,q}</math> is the length of query {{math|q}} given the term frequencies {{math|tf}} in the query vocabulary {{math|N}}.

In practice the multinomial coefficient is usually removed from the calculation. The reason is that it is a constant for a given bag of words (such as all the words from a specific document <math>d</math>). The language model <math>M_d</math> should be the true language model calculated from the distribution of words underlying each retrieved document. In practice this language model is unknown, so it is usually approximated by considering each term (unigram) from the retrieved document together with its probability of appearance. So <math>P(t|M_d)</math> is the probability of term <math>t</math> being generated by the language model <math>M_d</math> of document <math>d</math>. This probability is multiplied for all terms from query <math>q</math> to get a rank for document <math>d</math> in the interval <math>[0,1]</math>. The calculation is repeated for all documents to create a ranking of all documents in the document collection.

<ref>Christopher D. Manning, Prabhakar Raghavan, Hinrich Schütze: An Introduction to Information Retrieval, page 241. Cambridge University Press, 2009</ref>

==References==
 <references/>

[[Category:Information retrieval techniques]]
<=====doc_Id=====>:626
<=====title=====>:
Divergence-from-randomness model
<=====text=====>:
In the field of [[information retrieval]], '''divergence from randomness''' is one type of [[probabilistic]] model.

Term weights are computed by measuring the divergence between a term distribution produced by a random process and the actual term distribution.

==External links==
*[http://terrier.org/docs/v3.5/dfr_description.html Terrier's DFR Web page]
*[http://ir.dcs.gla.ac.uk/wiki/DivergenceFromRandomness Glasgow IR group Wiki DFR page]

[[Category:Ranking functions]]
[[Category:Information retrieval techniques]]
[[Category:Probabilistic models]]


{{comp-sci-stub}}
<=====doc_Id=====>:629
<=====title=====>:
Stemming
<=====text=====>:
{{about||the skiing technique|Stem (skiing)|the climbing technique|Glossary of climbing terms#stem}}
{{Expert needed|date=October 2010}}
In [[linguistic morphology]] and [[information retrieval]], '''stemming''' is the process of reducing inflected (or sometimes derived) words to their [[word stem]], base or [[root (linguistics)|root]] form—generally a written word form. The stem need not be identical to the [[morphological root]] of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. [[Algorithm]]s for stemming have been studied in [[computer science]] since the 1960s. Many [[search engine]]s treat words with the same stem as [[synonym]]s as a kind of [[query expansion]], a process called conflation.

Stemming programs are commonly referred to as stemming algorithms or stemmers.

==Examples==
A stemmer for English, for example, should identify the [[string literal|string]] "cats" (and possibly "catlike", "catty" etc.) as based on the root "cat", and "stems", "stemmer", "stemming", "stemmed" as based on "stem". A stemming algorithm reduces the words "fishing", "fished", and "fisher" to the root word, "fish". On the other hand, "argue", "argued", "argues", "arguing", and "argus" reduce to the stem "argu" (illustrating the case where the stem is not itself a word or root) but "argument" and "arguments" reduce to the stem "argument".<!-- using the Porter algorithm -->

==History==
The first published stemmer was written by [[Julie Beth Lovins]] in 1968.<ref>{{cite journal |first=Julie Beth |last=Lovins |year=1968 |title=Development of a Stemming Algorithm |journal=Mechanical Translation and Computational Linguistics |volume=11 |pages=22–31 }}</ref> This paper was remarkable for its early date and had great influence on later work in this area.

A later stemmer was written by [[Martin Porter]] and was published in the July 1980 issue of the journal ''Program''. This stemmer was very widely used and became the de facto standard algorithm used for English stemming. Dr. Porter received the [[Tony Kent Strix award]] in 2000 for his work on stemming and information retrieval.

Many implementations of the Porter stemming algorithm were written and freely distributed; however, many of these implementations contained subtle flaws. As a result, these stemmers did not match their potential. To eliminate this source of error, Martin Porter released an official [[free software]] (mostly [[BSD licenses|BSD]]-licensed) implementation<ref>http://tartarus.org/~martin/PorterStemmer/</ref> of the algorithm around the year 2000. He extended this work over the next few years by building [[Snowball programming language|Snowball]], a framework for writing stemming algorithms, and implemented an improved English stemmer together with stemmers for several other languages.

==Algorithms==
There are several types of stemming algorithms which differ in respect to performance and accuracy and how certain stemming obstacles are overcome.

A simple stemmer looks up the inflected form in a [[lookup table]]. The advantages of this approach are that it is simple, fast, and easily handles exceptions. The disadvantages are that all inflected forms must be explicitly listed in the table: new or unfamiliar words are not handled, even if they are perfectly regular (e.g. iPads ~ iPad), and the table may be large. For languages with simple morphology, like English, table sizes are modest, but highly inflected languages like Turkish may have hundreds of potential inflected forms for each root.

A lookup approach may use preliminary part-of-speech tagging to avoid overstemming.<ref>Yatsko, V. A.; [http://yatsko.zohosites.com/y-stemmer.html ''Y-stemmer'']</ref>

===The production technique===

The lookup table used by a stemmer is generally produced semi-automatically. For example, if the word is "run", then the inverted algorithm might automatically generate the forms "running", "runs", "runned", and "runly". The last two forms are valid constructions, but they are unlikely.

===Suffix-stripping algorithms===
Suffix stripping algorithms do not rely on a lookup table that consists of inflected forms and root form relations. Instead, a typically smaller list of "rules" is stored which provides a path for the algorithm, given an input word form, to find its root form. Some examples of the rules include:
* if the word ends in 'ed', remove the 'ed'
* if the word ends in 'ing', remove the 'ing'
* if the word ends in 'ly', remove the 'ly'

Suffix stripping approaches enjoy the benefit of being much simpler to maintain than brute force algorithms, assuming the maintainer is sufficiently knowledgeable in the challenges of linguistics and morphology and encoding suffix stripping rules. Suffix stripping algorithms are sometimes regarded as crude given the poor performance when dealing with exceptional relations (like 'ran' and 'run'). The solutions produced by suffix stripping algorithms are limited to those [[lexical category|lexical categories]] which have well known suffixes with few exceptions. This, however, is a problem, as not all parts of speech have such a well formulated set of rules. [[Lemmatisation]] attempts to improve upon this challenge.

Prefix stripping may also be implemented. Of course, not all languages use prefixing or suffixing.

====Additional algorithm criteria====
Suffix stripping algorithms may differ in results for a variety of reasons. One such reason is whether the algorithm constrains whether the output word must be a real word in the given language. Some approaches do not require the word to actually exist in the language lexicon (the set of all words in the language). Alternatively, some suffix stripping approaches maintain a database (a large list) of all known morphological word roots that exist as real words. These approaches check the list for the existence of the term prior to making a decision. Typically, if the term does not exist, alternate action is taken. This alternate action may involve several other criteria. The non-existence of an output term may serve to cause the algorithm to try alternate suffix stripping rules.

It can be the case that two or more suffix stripping rules apply to the same input term, which creates an ambiguity as to which rule to apply. The algorithm may assign (by human hand or stochastically) a priority to one rule or another. Or the algorithm may reject one rule application because it results in a non-existent term whereas the other overlapping rule does not. For example, given the English term ''friendlies'', the algorithm may identify the ''ies'' suffix and apply the appropriate rule and achieve the result of ''friendl''. ''friendl'' is likely not found in the lexicon, and therefore the rule is rejected.

One improvement upon basic suffix stripping is the use of suffix substitution. Similar to a stripping rule, a substitution rule replaces a suffix with an alternate suffix. For example, there could exist a rule that replaces ''ies'' with ''y''. How this affects the algorithm varies on the algorithm's design. To illustrate, the algorithm may identify that both the ''ies'' suffix stripping rule as well as the suffix substitution rule apply. Since the stripping rule results in a non-existent term in the lexicon, but the substitution rule does not, the substitution rule is applied instead. In this example, ''friendlies'' becomes ''friendly'' instead of ''friendl''.

Diving further into the details, a common technique is to apply rules in a cyclical fashion (recursively, as computer scientists would say). After applying the suffix substitution rule in this example scenario, a second pass is made to identify matching rules on the term ''friendly'', where the ''ly'' stripping rule is likely identified and accepted. In summary, ''friendlies'' becomes (via substitution) ''friendly'' which becomes (via stripping) ''friend''.

This example also helps illustrate the difference between a rule-based approach and a brute force approach. In a brute force approach, the algorithm would search for ''friendlies'' in the set of hundreds of thousands of inflected word forms and ideally find the corresponding root form ''friend''. In the rule-based approach, the three rules mentioned above would be applied in succession to converge on the same solution. Chances are that the rule-based approach would be slower, as lookup algorithms have a direct access to the solution, while rule-based should try several options, and combinations of them, and then choose which result seems to be the best.

===Lemmatisation algorithms===
A more complex approach to the problem of determining a stem of a word is [[lemmatisation]]. This process involves first determining the [[part of speech]] of a word, and applying different normalization rules for each part of speech. The part of speech is first detected prior to attempting to find the root since for some languages, the stemming rules change depending on a word's part of speech.

This approach is highly conditional upon obtaining the correct lexical category (part of speech). While there is overlap between the normalization rules for certain categories, identifying the wrong category or being unable to produce the right category limits the added benefit of this approach over suffix stripping algorithms. The basic idea is that, if the stemmer is able to grasp more information about the word being stemmed, then it can apply more accurate normalization rules (which unlike suffix stripping rules can also modify the stem).

===Stochastic algorithms===
[[Stochastic]] algorithms involve using probability to identify the root form of a word. Stochastic algorithms are trained (they "learn") on a table of root form to inflected form relations to develop a probabilistic model. This model is typically expressed in the form of complex linguistic rules, similar in nature to those in suffix stripping or lemmatisation. Stemming is performed by inputting an inflected form to the trained model and having the model produce the root form according to its internal ruleset, which again is similar to suffix stripping and lemmatisation, except that the decisions involved in applying the most appropriate rule, or whether or not to stem the word and just return the same word, or whether to apply two different rules sequentially, are applied on the grounds that the output word will have the highest probability of being correct (which is to say, the smallest probability of being incorrect, which is how it is typically measured).

Some lemmatisation algorithms are stochastic in that, given a word which may belong to multiple parts of speech, a probability is assigned to each possible part. This may take into account the surrounding words, called the context, or not. Context-free grammars do not take into account any additional information. In either case, after assigning the probabilities to each possible part of speech, the most likely part of speech is chosen, and from there the appropriate normalization rules are applied to the input word to produce the normalized (root) form.

===''n''-gram analysis===
Some stemming techniques use the [[n-gram]] context of a word to choose the correct stem for a word.<ref name="CEUR Proceedings">{{cite journal|last1=McNamee|first1=Paul|title=Exploring New Languages with HAIRCUT at CLEF 2005|journal=CEUR Workshop Proceedings|date=September 21–22, 2005|volume=1171|url=http://ceur-ws.org/Vol-1171/CLEF2005wn-adhoc-McNamee2005.pdf|accessdate=3/6/15}}</ref>

===Hybrid approaches===
Hybrid approaches use two or more of the approaches described above in unison. A simple example is a suffix tree algorithm which first consults a lookup table using brute force. However, instead of trying to store the entire set of relations between words in a given language, the lookup table is kept small and is only used to store a minute amount of "frequent exceptions" like "ran => run". If the word is not in the exception list, apply suffix stripping or lemmatisation and output the result.

===Affix stemmers===
In [[linguistics]], the term [[affix]] refers to either a [[prefix]] or a [[suffix]]. In addition to dealing with suffixes, several approaches also attempt to remove common prefixes. For example, given the word ''indefinitely'', identify that the leading "in" is a prefix that can be removed. Many of the same approaches mentioned earlier apply, but go by the name '''affix stripping'''. A study of affix stemming for several European languages can be found here.<ref>Jongejan, B.; and Dalianis, H.; ''Automatic Training of Lemmatization Rules that Handle Morphological Changes in pre-, in- and Suffixes Alike'', in the ''Proceeding of the ACL-2009, Joint conference of the 47th Annual Meeting of the Association for Computational Linguistics and the 4th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Singapore, August 2–7, 2009'', pp. 145-153
[http://www.aclweb.org/anthology/P/P09/P09-1017.pdf]</ref>

===Matching algorithms===
Such algorithms use a stem database (for example a set of documents that contain stem words). These stems, as mentioned above, are not necessarily valid words themselves (but rather common sub-strings, as the "brows" in "browse" and in "browsing"). In order to stem a word the algorithm tries to match it with stems from the database, applying various constraints, such as on the relative length of the candidate stem within the word (so that, for example, the short prefix "be", which is the stem of such words as "be", "been" and "being", would not be considered as the stem of the word "beside").

==Language challenges==
While much of the early academic work in this area was focused on the English language (with significant use of the Porter Stemmer algorithm), many other languages have been investigated.<ref>Dolamic, Ljiljana; and Savoy, Jacques; [http://clef.isti.cnr.it/2007/working_notes/DolamicCLEF2007.pdf ''Stemming Approaches for East European Languages (CLEF 2007)'']</ref><ref>Savoy, Jacques; [http://portal.acm.org/citation.cfm?doid=1141277.1141523 ''Light Stemming Approaches for the French, Portuguese, German and Hungarian Languages''], ACM Symposium on Applied Computing, SAC 2006, ISBN 1-59593-108-2</ref><ref>Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp. 384–390</ref><ref>[http://staff.science.uva.nl/~mdr/Publications/Files/clef2005-proc-adhoc.pdf ''Stemming in Hungarian at CLEF 2005'']</ref><ref>Viera, A. F. G. & Virgil, J. (2007); [http://InformationR.net/ir/12-3/paper315.html ''Uma revisão dos algoritmos de radicalização em língua portuguesa''], Information Research, 12(3), paper 315</ref>

Hebrew and Arabic are still considered difficult research languages for stemming. English stemmers are fairly trivial (with only occasional problems, such as "dries" being the third-person singular present form of the verb "dry", "axes" being the plural of "axe" as well as "axis"); but stemmers become harder to design as the morphology, orthography, and character encoding of the target language becomes more complex. For example, an Italian stemmer is more complex than an English one (because of a greater number of verb inflections), a Russian one is more complex (more noun [[declension]]s), a Hebrew one is even more complex (due to [[nonconcatenative morphology]], a writing system without vowels, and the requirement of prefix stripping: Hebrew stems can be two, three or four characters, but not more), and so on.

===Multilingual stemming===
Multilingual stemming applies morphological rules of two or more languages simultaneously instead of rules for only a single language when interpreting a search query. Commercial systems using multilingual stemming exist{{Citation needed|date=October 2013}}.

==Error metrics==
There are two error measurements in stemming algorithms, overstemming and understemming. Overstemming is an error where two separate inflected words are stemmed to the same root, but should not have been—a [[false positive]]. Understemming is an error where two separate inflected words should be stemmed to the same root, but are not—a [[false negative]]. Stemming algorithms attempt to minimize each type of error, although reducing one type can lead to increasing the other.

For example, the widely used Porter stemmer stems "universal", "university", and "universe" to "univers". This is a case of overstemming: though these three words are etymologically related, their modern meanings are in widely different domains, so treating them as synonyms in a search engine will likely reduce the relevance of the search results.

An example of understemming in the Porter stemmer is "alumnus" → "alumnu", "alumni" → "alumni", "alumna"/"alumnae" → "alumna".  This English word keeps Latin morphology, and so these near-synonyms are not conflated.

==Applications==
Stemming is used as an approximate method for grouping words with a similar basic meaning together. For example, a text mentioning "daffodils" is probably closely related to a text mentioning "daffodil" (without the s). But in some cases, words with the same morphological stem have [[idiom]]atic meanings which are not closely related: a user searching for "marketing" will not be satisfied by most documents mentioning "markets" but not "marketing".

===Information retrieval===
Stemmers are common elements in [[Information Retrieval|query systems]] such as [[World Wide Web|Web]] [[search engine]]s. The effectiveness of stemming for English query systems were soon found to be rather limited, however, and this has led early [[information retrieval]] researchers to deem stemming irrelevant in general.<ref>Baeza-Yates, Ricardo; and Ribeiro-Neto, Berthier (1999); ''Modern Information Retrieval'', ACM Press/Addison Wesley</ref> An alternative approach, based on searching for [[n-gram]]s rather than stems, may be used instead. Also, stemmers may provide greater benefits in other languages than English.<ref>Kamps, Jaap; Monz, Christof; de Rijke, Maarten; and Sigurbjörnsson, Börkur (2004); ''Language-Dependent and Language-Independent Approaches to Cross-Lingual Text Retrieval'', in Peters, C.; Gonzalo, J.; Braschler, M.; and Kluck, M. (eds.); ''Comparative Evaluation of Multilingual Information Access Systems'', Springer Verlag, pp. 152–165</ref><ref>Airio, Eija (2006); ''Word Normalization and Decompounding in Mono- and Bilingual IR'', Information Retrieval '''9''':249–271</ref>

===Domain Analysis===
Stemming is used to determine domain vocabularies in [[domain analysis]].
<ref>Frakes, W.; Prieto-Diaz, R.; & Fox, C. (1998); ''DARE: Domain Analysis and Reuse Environment'', Annals of Software Engineering (5), pp. 125-141</ref>

===Use in commercial products===
Many commercial companies have been using stemming since at least the 1980s and have produced algorithmic and lexical stemmers in many languages.<ref>[http://www.dtsearch.co.uk/language.htm ''Language Extension Packs''], dtSearch</ref><ref>[http://technet2.microsoft.com/Office/en-us/library/87065c9d-d39d-479d-909b-02160ec6d7791033.mspx?mfr=true ''Building Multilingual Solutions by using Sharepoint Products and Technologies''], Microsoft Technet</ref>

The [[Snowball (programming language)|Snowball]] stemmers have been compared with commercial lexical stemmers with varying results.<ref>[http://clef.isti.cnr.it/2003/WN_web/19.pdf CLEF 2003: Stephen Tomlinson compared the Snowball stemmers with the Hummingbird lexical stemming (lemmatization) system]</ref><ref>[http://clef.isti.cnr.it/2004/working_notes/WorkingNotes2004/21.pdf CLEF 2004: Stephen Tomlinson "Finnish, Portuguese and Russian Retrieval with Hummingbird SearchServer"]</ref>

[[Google search]] adopted word stemming in 2003.<ref>[http://www.google.com/support/bin/static.py?page=searchguides.html&ctx=basics#stemming ''The Essentials of Google Search''], Web Search Help Center, [[Google|Google Inc.]]</ref> Previously a search for "fish" would not have returned "fishing". Other software search algorithms vary in their use of word stemming. Programs that simply search for substrings obviously will find "fish" in "fishing" but when searching for "fishes" will not find occurrences of the word "fish".

==See also==
* [[Root (linguistics)]] - linguistic definition of the term "root"
* [[Stem (linguistics)]] - linguistic definition of the term "stem"
* [[Morphology (linguistics)]]
* [[Lemma (morphology)]] - linguistic definition
* [[Lemmatization]]
* [[Lexeme]]
* [[Inflection]]
* [[Derivation (linguistics)|Derivation]] - stemming is a form of reverse derivation
* [[Natural language processing]] - stemming is generally regarded as a form of NLP
* [[Text mining]] - stemming algorithms play a major role in commercial NLP software
* [[Computational linguistics]]
* [[Snowball (programming language)]] - designed for creating stemming algorithms

{{Natural Language Processing}}

==References==
{{reflist|2}}

==Further reading==
{{refbegin|2}}
* Dawson, J. L. (1974); ''Suffix Removal for Word Conflation'', Bulletin of the Association for Literary and Linguistic Computing, 2(3): 33–46
* Frakes, W. B. (1984); ''Term Conflation for Information Retrieval'', Cambridge University Press
* Frakes, W. B. & Fox, C. J. (2003); ''Strength and Similarity of Affix Removal Stemming Algorithms'', SIGIR Forum, 37: 26–30
* Frakes, W. B. (1992); ''Stemming algorithms, Information retrieval: data structures and algorithms'', Upper Saddle River, NJ: Prentice-Hall, Inc.
* Hafer, M. A. & Weiss, S. F. (1974); ''Word segmentation by letter successor varieties'', Information Processing & Management 10 (11/12), 371–386
* Harman, D. (1991); ''How Effective is Suffixing?'', Journal of the American Society for Information Science 42 (1), 7–15
* Hull, D. A. (1996); ''Stemming Algorithms&nbsp;– A Case Study for Detailed Evaluation'', JASIS, 47(1): 70–84
* Hull, D. A. & Grefenstette, G. (1996); ''A Detailed Analysis of English Stemming Algorithms'', Xerox Technical Report
* Kraaij, W. & Pohlmann, R. (1996); ''Viewing Stemming as Recall Enhancement'', in Frei, H.-P.; Harman, D.; Schauble, P.; and Wilkinson, R. (eds.); ''Proceedings of the 17th ACM SIGIR conference held at Zurich, August 18–22'', pp.&nbsp;40–48
* Krovetz, R. (1993); ''Viewing Morphology as an Inference Process'', in ''Proceedings of ACM-SIGIR93'', pp.&nbsp;191–203
* Lennon, M.; Pierce, D. S.; Tarry, B. D.; & Willett, P. (1981); ''An Evaluation of some Conflation Algorithms for Information Retrieval'', Journal of Information Science, 3: 177–183
* Lovins, J. (1971); ''[http://www.eric.ed.gov/sitemap/html_0900000b800c571a.html Error Evaluation for Stemming Algorithms as Clustering Algorithms]'', JASIS, 22: 28–40
* Lovins, J. B. (1968); ''Development of a Stemming Algorithm'', Mechanical Translation and Computational Linguistics, 11, 22—31
* Jenkins, Marie-Claire; and Smith, Dan (2005); [http://www.uea.ac.uk/polopoly_fs/1.85493!stemmer25feb.pdf ''Conservative Stemming for Search and Indexing'']
* Paice, C. D. (1990); ''[http://www.comp.lancs.ac.uk/computing/research/stemming/paice/article.htm Another Stemmer]'', SIGIR Forum, 24: 56–61
* Paice, C. D. (1996) ''[http://www3.interscience.wiley.com/cgi-bin/abstract/57804/ABSTRACT Method for Evaluation of Stemming Algorithms based on Error Counting]'', JASIS, 47(8): 632–649
* Popovič, Mirko; and Willett, Peter (1992); [http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291097-4571%28199206%2943:5%3C384::AID-ASI6%3E3.0.CO;2-L/abstract ''The Effectiveness of Stemming for Natural-Language Access to Slovene Textual Data''], Journal of the [[American Society for Information Science]], Volume 43, Issue 5 (June), pp.&nbsp;384–390
* Porter, Martin F. (1980); ''[http://telemat.det.unifi.it/book/2001/wchange/download/stem_porter.html An Algorithm for Suffix Stripping]'', Program, 14(3): 130–137
* Savoy, J. (1993); ''[http://www3.interscience.wiley.com/cgi-bin/abstract/10049824/ABSTRACT?CRETRY=1&SRETRY=0 Stemming of French Words Based on Grammatical Categories]'' Journal of the American Society for Information Science, 44(1), 1–9
* Ulmschneider, John E.; & Doszkocs, Tamas (1983); ''[http://www.eric.ed.gov/sitemap/html_0900000b8007ea83.html A Practical Stemming Algorithm for Online Search Assistance]'', Online Review, 7(4), 301–318
* Xu, J.; & Croft, W. B. (1998); ''[http://portal.acm.org/citation.cfm?doid=267954.267957 Corpus-Based Stemming Using Coocurrence of Word Variants]'', ACM Transactions on Information Systems, 16(1), 61–81
{{refend}}

==External links==
*[http://opennlp.apache.org/index.html Apache OpenNLP] includes Porter and Snowball stemmers
* [http://smile-stemmer.appspot.com SMILE Stemmer] - free online service, includes Porter and Paice/Husk' Lancaster stemmers (Java API)
* [http://code.google.com/p/ir-themis/ Themis] - open source IR framework, includes Porter stemmer implementation (PostgreSQL, Java API)
* [http://snowballstem.org Snowball] - free stemming algorithms for many languages, includes source code, including stemmers for five romance languages
* [http://www.iveonik.com/blog/2011/08/snowball-stemmers-on-csharp-free-download/ Snowball on C#] - port of Snowball stemmers for C# (14 languages)
* [http://snowball.tartarus.org/wrappers/guide.html Python bindings to Snowball API]
* [http://locknet.ro/archive/2009-10-29-ann-ruby-stemmer.html Ruby-Stemmer] - Ruby extension to Snowball API
* [http://pecl.php.net/package/stem/ PECL] - PHP extension to the Snowball API
* [http://www.oleandersolutions.com/stemming.html Oleander Porter's algorithm] - stemming library in C++ released under BSD
* [http://www.cs.waikato.ac.nz/~eibe/stemmers/index.html Unofficial home page of the Lovins stemming algorithm] - with source code in a couple of languages
* [http://www.tartarus.org/~martin/PorterStemmer/index.html Official home page of the Porter stemming algorithm] - including source code in several languages
* [http://www.comp.lancs.ac.uk/computing/research/stemming/index.htm Official home page of the Lancaster stemming algorithm] - Lancaster University, UK
* [https://www.uea.ac.uk/computing/word-stemming/ Official home page of the UEA-Lite Stemmer ] - University of East Anglia, UK
* [http://www.comp.lancs.ac.uk/computing/research/stemming/general/index.htm Overview of stemming algorithms]
* [http://code.google.com/p/ptstemmer/ PTStemmer] - A Java/Python/.Net stemming toolkit for the Portuguese language
* [http://mazko.github.com/jssnowball/ jsSnowball] - open source JavaScript implementation of Snowball stemming algorithms for many languages
* [http://trimc-nlp.blogspot.com/2013/08/snowball-stemmer-for-java.html Snowball Stemmer] - implementation for Java
* [http://hlt.di.fct.unl.pt/luis/hindi_stemmer/ hindi_stemmer] - open source stemmer for Hindi
* [http://hlt.di.fct.unl.pt/luis/czech_stemmer/ czech_stemmer] - open source stemmer for Czech
* [http://www.comp.leeds.ac.uk/eric/sawalha08coling.pdf Comparative Evaluation of Arabic Language Morphological Analysers and Stemmers]
* [https://github.com/rdamodharan/tamil-stemmer Tamil Stemmer]

{{FOLDOC}}

[[Category:Linguistic morphology]]
[[Category:Natural language processing]]
[[Category:Tasks of natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:632
<=====title=====>:
Type-1 OWA operators
<=====text=====>:
{{Underlinked|date=July 2016}}

The [[Ordered weighted averaging aggregation operator|Yager's OWA (ordered weighted averaging) operators]]{{R|yagerOWA}} are used to aggregate the crisp values in decision making schemes (such as multi-criteria decision making, multi-expert decision making and  multi-criteria/multi-expert decision making).{{R|Yager|YagerBeliakov}} It is widely accepted that [[Fuzzy set]]s{{R|Zadeh}} are more suitable for representing preferences of criteria in decision making.

The type-1 OWA operators{{R|fssT1OWA|kdeT1OWA}} have been proposed for this purpose. The type-1 OWA operators provides a technique for directly aggregating uncertain information with uncertain weights via OWA mechanism in soft decision making and [[data mining]], where these uncertain objects are modelled by fuzzy sets.

The two definitions for type-1 OWA operators are based on Zadeh's Extension Principle and <math>\alpha</math>-cuts of fuzzy sets. The two definitions lead to equivalent results.

==Definitions==

===Definition 1===
Let <math>F(X)</math> be the set of fuzzy sets with domain of discourse <math>X</math>, a type-1 OWA operator is defined as follows:{{R|kdeT1OWA}}

Given n linguistic weights <math>\left\{ {W^i} \right\}_{i = 1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,1]</math>, a type-1 OWA operator is a mapping, <math>\Phi</math>,

:<math>\Phi \colon F(X)\times \cdots \times F(X)  \longrightarrow  F(X)</math>
:<math>(A^1 , \cdots ,A^n)  \mapsto   Y</math>

such that

:<math>\mu _{Y} (y) =\displaystyle \sup_{\displaystyle \sum_{k =1}^n \bar {w}_i a_{\sigma (i)}  = y }\left({\begin{array}{*{1}l}\mu _{W^1 } (w_1 )\wedge \cdots \wedge \mu_{W^n } (w_n )\wedge \mu _{A^1 } (a_1 )\wedge \cdots \wedge \mu _{A^n } (a_n )\end{array}}\right)</math>

where <math>\bar {w}_i = \frac{w_i }{\sum_{i = 1}^n {w_i } }</math>,and <math>\sigma \colon \{1, \cdots ,n\} \longrightarrow \{1, \cdots ,n\}</math> is a permutation function such that <math>a_{\sigma (i)} \geq a_{\sigma (i + 1)},\ \forall i = 1, \cdots ,n - 1</math>, i.e., <math>a_{\sigma(i)} </math> is the <math>i</math>th highest element in the set <math>\left\{ {a_1 , \cdots ,a_n } \right\}</math>.

===Definition 2===
Using the alpha-cuts of fuzzy sets:{{R|kdeT1OWA}}

Given the n linguistic weights <math>\left\{ {W^i} \right\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\;\;1]</math>, then for each <math>\alpha \in [0,\;1]</math>, an <math>\alpha </math>-level type-1 OWA operator with <math>\alpha </math>-level sets <math>\left\{ {W_\alpha ^i } \right\}_{i = 1}^n </math> to aggregate the <math>\alpha </math>-cuts of fuzzy sets <math>\left\{ {A^i} \right\}_{i =1}^n </math> is:

: <math>
\Phi_\alpha \left( {A_\alpha ^1 , \ldots ,A_\alpha ^n } \right) =\left\{ {\frac{\sum\limits_{i = 1}^n {w_i a_{\sigma (i)} } }{\sum\limits_{i = 1}^n {w_i } }\left| {w_i \in W_\alpha ^i ,\;a_i } \right. \in A_\alpha ^i ,\;i = 1, \ldots ,n} \right\}</math>

where  <math>W_\alpha ^i= \{w| \mu_{W_i }(w) \geq \alpha \}, A_\alpha ^i=\{ x| \mu _{A_i }(x)\geq \alpha \}</math>, and <math>\sigma :\{\;1, \cdots ,n\;\} \to \{\;1, \cdots ,n\;\}</math> is a permutation function such that <math>a_{\sigma (i)} \ge a_{\sigma (i + 1)} ,\;\forall \;i = 1, \cdots ,n - 1</math>, i.e., <math>a_{\sigma (i)} </math> is the <math>i</math>th largest
element in the set <math>\left\{ {a_1 , \cdots ,a_n } \right\}</math>.

== Representation theorem of Type-1 OWA operators==

Given the ''n'' linguistic weights <math>\left\{ {W^i} \right\}_{i =1}^n </math> in the form of fuzzy sets defined on the domain of discourse <math>U = [0,\;\;1]</math>, and the fuzzy sets <math>A^1, \cdots ,A^n</math>, then we have that{{R|kdeT1OWA}}
:<math>Y=G</math>

where <math>Y</math> is the aggregation result obtained by Definition 1, and <math>G</math> is the result obtained by in Definition 2.

==Programming problems for Type-1 OWA operators==

According to the Representation Theorem of Type-1 OWA Operators, a general type-1 OWA operator can be decomposed into a series of <math>\alpha</math>-level type-1 OWA operators. In practice, this series of  <math>\alpha</math>-level type-1 OWA operators is used to construct the resulting aggregation fuzzy set. So we only need to compute the left end-points and right end-points of the intervals <math>\Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)</math>. Then, the resulting aggregation fuzzy set is constructed with the membership function as follows:

:<math>\mu _{G} (x) = \operatorname{ \bigvee} \limits_{\alpha :x \in \Phi _\alpha \left( {A_\alpha ^1 , \cdots
,A_\alpha ^n } \right)_\alpha } \alpha </math>

For the left end-points, we need to solve the following programming problem:
:<math> \Phi _\alpha \left( {A_\alpha ^1 , \cdots ,A_\alpha ^n } \right)_{-} = \operatorname {\min }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i = 1}^n {w_i } } </math>

while for the right end-points, we need to solve the following programming problem:
:<math>\Phi _\alpha \left( {A_\alpha ^1 , \cdots , A_\alpha ^n } \right)_{+} = \operatorname {\max }\limits_{\begin{array}{l} W_{\alpha - }^i \le w_i \le W_{\alpha + }^i  A_{\alpha - }^i \le a_i \le A_{\alpha + }^i  \end{array}} \sum\limits_{i = 1}^n {w_i a_{\sigma (i)} / \sum\limits_{i =
1}^n {w_i } } </math>

A fast method has been presented to solve two programming problem so that the type-1 OWA aggregation operation can be performed efficiently, for details, please see the paper.{{R|kdeT1OWA}}

==Alpha-level approach to Type-1 OWA operation==

Three-step process:{{R|kdeT1OWA}}
* Step 1&mdash;To set up the <math>\alpha </math>- level resolution in [0, 1].
* Step 2&mdash;For each <math>\alpha \in [0,1]</math>,
:*Step 2.1&mdash;To calculate <math>\rho _{\alpha +} ^{i_0^\ast } </math>
# Let <math>i_0 = 1</math>;
# If <math>\rho _{\alpha +} ^{i_0 } \ge A_{\alpha + }^{\sigma (i_0 )} </math>, stop, <math>\rho _{\alpha +} ^{i_0 } </math> is the solution; otherwise go to Step 2.1-3.
# <math>i_0 \leftarrow i_0 + 1</math>, go to Step 2.1-2.

:*Step 2.2 To calculate<math>\rho _{\alpha -} ^{i_0^\ast } </math>
# Let <math>i_0 = 1</math>;
# If <math>\rho _{\alpha -} ^{i_0 } \ge A_{\alpha - }^{\sigma (i_0 )} </math>, stop, <math>\rho _{\alpha -} ^{i_0 } </math> is the solution; otherwise go to Step 2.2-3.
#<math>i_0 \leftarrow i_0 + 1</math>, go to step Step 2.2-2.

* Step 3&mdash;To construct the aggregation resulting fuzzy set <math>G</math> based on all the available intervals <math>\left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]</math>:

:<math>\mu _{G} (x) = \operatorname \bigvee \limits_{\alpha :x \in \left[ {\rho _{\alpha -} ^{i_0^\ast } ,\;\rho _{\alpha +} ^{i_0^\ast } } \right]} \alpha </math>

==Special cases==
* Any OWA operators, like maximum, minimum, mean operators;<ref name="yagerOWA" />
* Join operators of (type-1) fuzzy sets,{{R|MT}} i.e., fuzzy maximum operators;
* Meet operators of (type-1) fuzzy sets,{{R|MT|zadehJ}} i.e., fuzzy minimum operators;
* Join-like operators of (type-1) fuzzy sets;{{R|kdeT1OWA|bookT1OWA}}
* Meet-like operators of (type-1) fuzzy sets.{{R|kdeT1OWA|bookT1OWA}}

==Generalizations==
Type-2 OWA operators{{R|Zhou}} have been suggested to aggregate the [[Type-2 fuzzy sets and systems|type-2 fuzzy sets]] for soft decision making.

==References==
{{reflist|30em|refs=

<ref name="yagerOWA">{{cite journal|last=Yager|first=R.R|title=On ordered weighted averaging aggregation operators in multi-criteria decision making|journal=IEEE Transactions on Systems, Man and Cybernetics|year=1988|volume=18|pages=183–190|doi=10.1109/21.87068}}</ref>

<ref name=Yager>{{cite book|last=Yager|first=R. R. and Kacprzyk, J|title=The Ordered Weighted Averaging Operators: Theory and Applications|year=1997|publisher=Kluwer: Norwell, MA}}</ref>

<ref name=YagerBeliakov>{{cite book|last=Yager|first=R.R, Kacprzyk, J. and Beliakov, G|title=Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|publisher=Springer}}</ref>

<ref name=Zadeh>{{cite journal|last=Zadeh|first=L.A|title=Fuzzy sets|journal=Information and Control |year=1965|volume=8 |pages=338–353|doi=10.1016/S0019-9958(65)90241-X}}</ref>

<ref name="kdeT1OWA">{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Alpha-level aggregation: a practical approach to type-1 OWA operation for aggregating uncertain information with applications to breast cancer treatments|journal=IEEE Transactions on Knowledge and Data Engineering|year=2011|volume=23|issue=10|pages=1455–1468|doi=10.1109/TKDE.2010.191}}</ref>

<ref name="fssT1OWA">{{cite journal|last=Zhou|first=S. M. |author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi|title=Type-1 OWA operators for aggregating uncertain information with uncertain weights induced by type-2 linguistic quantifiers|journal=Fuzzy Sets and Systems|year=2008|volume=159|issue=24|pages=3281–3296|doi=10.1016/j.fss.2008.06.018}}</ref>

<ref name="MT">{{cite journal|last=Mizumoto|first=M.|author2=K. Tanaka |title=Some Properties of fuzzy sets of type 2|journal=Information and Control|year=1976|volume=31|pages=312–40|doi=10.1016/s0019-9958(76)80011-3}}</ref><ref name="zadehJ">{{cite journal|last=Zadeh|first=L. A.|title=The concept of a linguistic variable and its application to approximate reasoning-1|journal=Information Sciences|year=1975|volume=8|pages=199–249|doi=10.1016/0020-0255(75)90036-5}}</ref>

<ref name="bookT1OWA">{{cite journal|last=Zhou|first=S. M.|author2=F. Chiclana |author3=R. I. John |author4=J. M. Garibaldi |title=Fuzzificcation of the OWA Operators in Aggregating Uncertain Information|journal=R. R. Yager, J. Kacprzyk and G. Beliakov (ed): Recent Developments in the Ordered Weighted Averaging Operators-Theory and Practice|year=2011|volume=Springer|pages=91–109|doi=10.1007/978-3-642-17910-5_5}}</ref>

<ref name=Zhou>{{cite journal|last=Zhou|first=S.M. |author2=R. I. John |author3=F. Chiclana |author4=J. M. Garibaldi|title=On aggregating uncertain information by type-2 OWA operators for soft decision making|journal=International Journal of Intelligent Systems|year=2010|volume=25|issue=6|pages=540–558|doi=10.1002/int.20420}}</ref>
}}

[[Category:Artificial intelligence]]
[[Category:Fuzzy logic]]
[[Category:Information retrieval techniques]]
[[Category:Logic in computer science]]
<=====doc_Id=====>:635
<=====title=====>:
Tag (metadata)
<=====text=====>:
{{hatnote|Not to be confused with [[Markup language]] or [[HTML element]] tags.}}
[[File:Web 2.0 Map.svg|thumb|right|250px|A [[tag cloud]] with terms related to [[Web 2.0]]]]

In [[information system]]s, a '''tag''' is a non-hierarchical [[index term|keyword or term]] assigned to a piece of information (such as an [[Bookmark (World Wide Web)|Internet bookmark]], digital image, or [[computer file]]). This kind of [[metadata]] helps describe an item and allows it to be found again by browsing or searching. Tags are generally chosen informally and personally by the item's creator or by its viewer, depending on the system.

Tagging was popularized by websites associated with [[Web 2.0]] and is an important feature of many Web 2.0 services. It is now also part of some desktop software.

==History==

Labeling and tagging are carried out to perform functions such as aiding in [[Classification (machine learning)|classification]], marking ownership, noting boundaries, and indicating [[online identity]]. They may take the form of words, images, or other identifying marks. An analogous example of tags in the physical world is museum object tagging. In the organization of information and objects, the use of textual keywords as part of identification and classification long  predates computers. However, computer based searching made the use of keywords a rapid way of exploring records.

[[File:A Description of the Equator and Some Otherlands, collaborative hypercinema portal Upload page.jpg|thumb|A Description of the Equator and Some Otherlands, collaborative hypercinema portal, produced by documenta X, 1997. User upload page associating user contributed media with the term ''Tag''.]] Online and Internet databases and early websites deployed them as a way for publishers to help users find content. In 1997, the collaborative portal "A Description of the Equator and Some Other Lands" produced by [[documenta]] X, Germany, coined the folksonomic term ''Tag'' for its co-authors and guest authors on its Upload page. In "The Equator" the term ''Tag'' for user-input was described as an ''abstract literal or keyword'' to aid the user. Turned out in Web 1.0 days, all "Otherlands" users defined singular ''Tags'', and did not share ''Tags'' at that point.

In 2003, the [[social bookmarking]] website [[Delicious (website)|Delicious]] provided a way for its users to add "tags" to their bookmarks (as a way to help find them later); Delicious also provided browseable aggregated views of the bookmarks of all users featuring a particular tag.<ref>[http://flickr.com/photos/joshu/765809051/in/set-72157600740166824/ Screenshot of tags on del.icio.us] in 2004 and [http://flickr.com/photos/joshu/765817375/in/set-72157600740166824/ Screenshot of a tag page on del.icio.us], also in 2004, both published by [[Joshua Schachter]] on July 9, 2007.</ref> [[Flickr]] allowed its users to add their own text tags to each of their pictures, constructing flexible and easy metadata that made the pictures highly searchable.<ref>[http://www.adaptivepath.com/ideas/essays/archives/000519.php "An Interview with Flickr's Eric Costello"] by Jesse James Garrett, published on August 4, 2005. Quote: "Tags were not in the initial version of Flickr. Stewart Butterfield...liked the way they worked on del.icio.us, the social bookmarking application. We added very simple tagging functionality, so you could tag your photos, and then look at all your photos with a particular tag, or any one person’s photos with a particular tag."</ref> The success of Flickr and the influence of Delicious popularized the concept,<ref>An example is [http://www.adammathes.com/academic/computer-mediated-communication/folksonomies.html "Folksonomies - Cooperative Classification and Communication Through Shared Metadata"] by Adam Mathes, December 2004. It focuses on tagging in Delicious and Flickr.</ref> and other [[social software]] websites&nbsp;– such as [[YouTube]], [[Technorati]], and [[Last.fm]]&nbsp;– also implemented tagging. Other traditional and web applications have incorporated the concept such as "Labels" in [[Gmail]] and the ability to add and edit tags in [[iTunes]] or [[Winamp]].

Tagging has gained wide popularity due to the growth of social networking, photography sharing and bookmarking sites. These sites allow users to create and manage labels (or “tags”) that categorize content using simple keywords. The use of keywords as part of an identification and classification system long predates computers. In the early days of the web keywords meta tags were used by web page designers to tell search engines what the web page was about. Today's tagging takes the meta keywords concept and re-uses it. The users add the tags. The tags are clearly visible, and are themselves links to other items that share that keyword tag.

Knowledge tags are an extension of [[Index term|keyword]] tags. They were first used by [[Jumper 2.0]], an [[open source]] [[Web 2.0]] software platform released by Jumper Networks on 29 September 2008.<ref>{{Citation|url=http://www.jumpernetworks.com/ NEWS-Jumper_Networks_Releases_Jumper_2.0_Platform.pdf|title=Jumper Networks Press Release for Jumper 2.0|publisher=Jumper Networks, Inc.|date=29 September 2008}}</ref> Jumper 2.0 was the first [[collaborative search engine]] platform to use a method of expanded tagging for [[knowledge capture]].

Websites that include tags often display collections of tags as [[tag cloud]]s. A user's tags are useful both to them and to the larger community of the website's users.

Tags may be a "bottom-up" type of classification, compared to [[hierarchy|hierarchies]], which are "top-down". In a traditional hierarchical system ([[Taxonomy (general)|taxonomy]]), the designer sets out a limited number of terms to use for classification, and there is one correct way to classify each item. In a tagging system, there are an unlimited number of ways to classify an item, and there is no "wrong" choice. Instead of belonging to one category, an item may have several different tags. Some researchers and applications have experimented with combining structured hierarchy and "flat" tagging to aid in information retrieval.<ref>[http://infolab.stanford.edu/~heymann/taghierarchy.html Tag Hierarchies], research notes by Paul Heymann.</ref>

==Examples==

===Within a blog===
Many [[blog]] systems allow authors to add free-form tags to a post, along with (or instead of) placing the post into categories. For example, a post may display that it has been tagged with ''baseball'' and ''tickets''. Each of those tags is usually a [[web link]] leading to an index page listing all of the posts associated with that tag. The blog may have a sidebar listing all the tags in use on that blog, with each tag leading to an index page. To reclassify a post, an author edits its list of tags. All connections between posts are automatically tracked and updated by the blog software; there is no need to relocate the page within a complex hierarchy of categories.

===For an event===
An official tag is a keyword adopted by events and conferences for participants to use in their web publications, such as blog entries, photos of the event, and presentation slides. Search engines can then index them to make relevant materials related to the event searchable in a uniform way. In this case, the tag is part of a [[controlled vocabulary]].

===In research===
A researcher may work with a large collection of items (e.g. press quotes, a bibliography, images) in digital form. If he/she wishes to associate each with a small number of themes (e.g. to chapters of a book, or to sub-themes of the overall subject), then a group of tags for these themes can be attached to each of the items in the larger collection. In this way, free form [[categorization|classification]] allows the author to manage what would otherwise be unwieldy amounts of information. Commercial, as well as some free computer applications are readily available to do this.

==Special types==

===Triple tags===
{{see also|Microformat}}
A '''triple tag''' or '''machine tag''' uses a special [[syntax]] to define extra [[semantic]] information about the tag, making it easier or more meaningful for interpretation by a computer program. Triple tags comprise three parts: a [[namespace]], a [[wikt:predicate|predicate]], and a value. For example, "<nowiki>geo:long=50.123456</nowiki>" is a tag for the geographical [[longitude]] coordinate whose value is 50.123456. This triple structure is similar to the [[Resource Description Framework]] model for information.

The triple tag format was first devised for geolicious<ref>{{cite web |url=http://brainoff.com/weblog/2004/11/05/124 |title=geo.lici.us: geotagging hosted services |first1=Mikel |last1=Maron |date=November 5, 2004}}</ref> in November 2004, to map [[Delicious (website)|Delicious]] bookmarks, and gained wider acceptance after its adoption by [http://stamen.com/projects/mappr Mappr] and GeoBloggers<ref>[http://web.archive.org/web/20071011024028/http://geobloggers.com/archives/2006/01/11/advanced-tagging-and-tripletags/ Advanced Tagging and TripleTags] by Reverend Dan Catt, ''Geobloggers'', January 11, 2006.</ref> to map [[Flickr]] photos. In January 2007, [[Aaron Straup Cope]] at [[Flickr]] introduced the term ''machine tag'' as an alternative name for the triple tag, adding some questions and answers on purpose, syntax, and use.<ref>[https://www.flickr.com/groups/api/discuss/72157594497877875/ Machine tags], a post by Aaron Straup Cope in the Flickr API group, January 24, 2007.</ref>

Specialized metadata for geographical identification is known as ''[[geotagging]]''; machine tags are also used for other purposes, such as identifying photos taken at a specific event or naming species using [[binomial nomenclature]].<ref>[https://www.flickr.com/groups/encyclopedia_of_life/rules/ Encyclopedia of Life use of machine tag], The Encyclopedia of Life project rules including the required use of a taxonomy machine tag, September 19, 2009.</ref>

===Hashtags===
{{main|Hashtag}}
A hashtag is a kind of metadata tag marked by the prefix <code>#</code>, sometimes known as a "hash" symbol. This form of tagging is used on [[microblogging]] and [[social networking service]]s such as [[Twitter]], [[Facebook]], [[Google+]], [[VK (social network)|VK]] and [[Instagram]].

===Knowledge tags===
A knowledge tag is a type of [[metadata|meta-information]] that describes or defines some aspect of an information resource (such as a [[document]], [[digital image]], [[database table|relational table]], or [[web page]]). Knowledge tags are more than traditional non-hierarchical [[index term|keywords or terms]]. They are a type of [[metadata]] that captures knowledge in the form of descriptions, categorizations, classifications, [[semantics]], comments, notes, annotations, [[hyperdata]], [[hyperlinks]], or references that are collected in tag profiles. These tag profiles reference an information resource that resides in a distributed, and often heterogeneous, storage repository. Knowledge tags are a [[knowledge management]] discipline that leverages [[Enterprise 2.0]] methodologies for users to capture insights, expertise, attributes, dependencies, or relationships associated with a data resource. It generally allows greater flexibility than other [[knowledge management]] classification systems.

Capturing knowledge in tags takes many different forms, there is factual knowledge (that found in books and data), conceptual knowledge (found in perspectives and concepts), expectational knowledge (needed to make judgments and hypothesis), and methodological knowledge (derived from reasoning and strategies).<ref>
{{Citation
 | last=Wiig | first=K. M.
 | year= 1997
 | title=Knowledge Management: An Introduction and Perspective
 | journal=Journal of Knowledge Management
 | volume=1 | issue=1
 | pages=6–14
 | url=http://www.mendeley.com/c/67997727/Wiig-1997-Knowledge-Management-An-Introduction-and-Perspective/
 | doi=10.1108/13673279710800682
}}
</ref> These forms of [[knowledge]] often exist outside the data itself and are derived from personal experience, insight, or expertise.

Knowledge tags, in fact, manifest themselves in any number of ways – conceptual knowledge tags describe procedures, lessons learned, and facts that are related to the information resource. [[Tacit knowledge]] tags, manifests itself through skills, habits or learning by doing and represent experience or organizational intelligence. Anecdotal knowledge, is a memory of a particular case or event that may not surface without context.<ref>
{{citation
 | last=Getting | first=Brian
 | year= 2007
 | title=What Are "Tags" And What Is "Tagging?
 | publisher=Practical eCommerce
 | url=http://www.practicalecommerce.com/articles/589
}}
</ref>

Knowledge can best be defined as information possessed in the mind of an individual: it is personalized or subjective information related to facts, procedures, concepts, interpretations, ideas, observations and judgments (which may or may not be unique, useful, accurate, or structurable). Knowledge tags are considered an expansion of the information itself that adds additional value, context, and meaning to the information. Knowledge tags are valuable for preserving organizational intelligence that is often lost due to turn-over, for sharing knowledge stored in the minds of individuals that is typically isolated and unharnessed by the organization, and for connecting knowledge that is often lost or disconnected from an information resource.<ref>
{{Citation
 | last=Alavi | first=Maryam
 | last2=Leidner
 | year= 1999
 | title=Knowledge Management Systems: Issues, Challenges, and Benefits
 | journal=Communications of the Association for Information Systems
 | volume=1 | issue=7
 | url=http://www.belkcollege.uncc.edu/jpfoley/Readings/artic07.pdf
}}
</ref>

==Advantages and disadvantages==
{{procon|date=November 2012}}

In a typical tagging system, there is no explicit information about the meaning or [[semantics]] of each tag, and a user can apply new tags to an item as easily as applying older tags. Hierarchical classification systems can be slow to change, and are rooted in the culture and era that created them.<ref name="Smith2008">Smith, Gene (2008). Tagging: People-Powered Metadata for the Social Web. Berkeley, CA: New Riders. ISBN 0-321-52917-0</ref> The flexibility of tagging allows users to classify their collections of items in the ways that they find useful, but the personalized variety of terms can present challenges when searching and browsing.

When users can freely choose tags (creating a [[folksonomy]], as opposed to selecting terms from a [[controlled vocabulary]]), the resulting metadata can include [[homonym]]s (the same tags used with different meanings) and [[synonym]]s (multiple tags for the same concept), which may lead to inappropriate connections between items and inefficient searches for information about a subject.<ref>Golder, Scott A. Huberman, Bernardo A. (2005).
"[http://arxiv.org/abs/cs.DL/0508082 The Structure of Collaborative Tagging Systems]." Information Dynamics Lab, HP Labs. Visited November 24, 2005.</ref> For example, the tag "orange" may refer to the [[Orange (fruit)|fruit]] or the [[Orange (colour)|color]], and items related to a version of the [[Linux kernel]] may be tagged "Linux", "kernel", "Penguin", "software", or a variety of other terms. Users can also choose tags that are different [[inflection]]s of words (such as singular and plural),<ref>[http://keithdevens.com/weblog/archive/2004/Dec/24/SvP.tags Singular vs. plural tags in a tag-based categorization system] by Keith Devens, December 24, 2004.</ref> which can contribute to navigation difficulties if the system does not include [[stemming]] of tags when searching or browsing. Larger-scale folksonomies address some of the problems of tagging, in that users of tagging systems tend to notice the current use of "tag terms" within these systems, and thus use existing tags in order to easily form connections to related items. In this way, folksonomies collectively develop a partial set of tagging conventions.

===Complex system dynamics===

Despite the apparent lack of control, research has shown that a simple form of shared vocabularies emerges in social bookmarking systems. Collaborative tagging exhibits a form of [[complex system]]s dynamics,<ref name="WWW07-ref">Harry Halpin, Valentin Robu, Hana Shepherd [http://portal.acm.org/citation.cfm?id=1242572.1242602 The Complex Dynamics of Collaborative Tagging], Proceedings of the 16th International Conference on the World Wide Web (WWW'07), Banff, Canada, pp. 211-220, ACM Press, 2007. Downloadable on [http://www2007.org/papers/paper635.pdf the conference's website]</ref> (or [[Self-organization|self organizing]] dynamics). Thus, even if no central controlled vocabulary constrains the actions of individual users, the distribution of tags that describe different resources (e.g., websites) converges over time to stable [[power law]] distributions.<ref name="WWW07-ref"/> Once such stable distributions form, simple vocabularies can be extracted by examining the [[correlation]]s that form between different tags.  This informal collaborative system of tag creation and management has been called a [[folksonomy]].

===Spamming===

Tagging systems open to the public are also open to tag spam, in which people apply an excessive number of tags or unrelated tags to an item (such as a [[YouTube]] video) in order to attract viewers. This abuse can be mitigated using human or statistical identification of spam items.<ref>[http://heymann.stanford.edu/tagspam.html Tag Spam], research notes by Paul Heymann.</ref> The number of tags allowed may also be limited to reduce spam.

==Syntax==
Some tagging systems provide a single [[text box]] to enter tags, so to be able to [[tokenize]] the string, a [[Wiktionary:separator|separator]] must be used. Two popular separators are the [[Space (punctuation)|space character]] and the [[comma]]. To enable the use of separators in the tags, a system may allow for higher-level separators (such as [[quotation mark]]s) or [[escape character]]s. Systems can avoid the use of separators by allowing only one tag to be added to each input [[Web widget|widget]] at a time, although this makes adding multiple tags more time-consuming.

A syntax for use within [[HTML]] is to use the '''rel-tag''' [[microformat]] which uses the [[Rel attribute|''rel'' attribute]] with value "tag" (i.e., <code>rel="tag"</code>) to indicate that the linked-to page acts as a tag for the current context.<ref>[http://microformats.org/wiki/rel-tag rel tag microformat specification], Microformats Wiki, January 10, 2005.</ref>

==See also==
{{colbegin||27em}}
* [[Collective intelligence]]
* [[Concept map]]
* [[Enterprise 2.0]]
* [[Enterprise bookmarking]]
* [[Explicit knowledge]]
* [[Faceted classification]]
* [[Folksonomy]]
* [[Information ecology]]
* [[Knowledge representation]]
* [[Knowledge transfer]]
* [[Metaknowledge]]
* [[Ontology (information science)]]
* [[Organisational memory]]
* [[Semantic web]]
* [[SciCrunch]]
* [[Tag cloud]]
* [[Web 2.0]]
{{colend}}
'''Others'''
{{colbegin||27em}}
* [[Collective unconscious]]
* [[Human-computer interaction]]
* [[Social network aggregation]]
* [[Enterprise social software]]
* [[Expert system]]
* [[Knowledge]]
* [[Knowledge base]]
* [[Knowledge worker]]
* [[Management information system]]
* [[Microformats]]
* [[Social network]]
* [[Social software]]
* [[Sociology of knowledge]]
* [[Tacit knowledge]]
{{colend}}

==References==
{{reflist|30em}}

'''General'''
{{refbegin}}
*{{Citation
 | surname1=Nonaka | given1=Ikujiro
 | year=1994
 | title= A dynamic theory of organizational knowledge creation
 | journal= Organization Science |volume=5 |issue=1
 | pages=14–37
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
 | doi=10.1287/orsc.5.1.14
}}
*{{Citation
 | surname1=Wigg | given1=Karl M
 | year=1993
 | title= Knowledge Management Foundations: Thinking About Thinking: How People and Organizations Create, Represent and Use Knowledge
 | journal= Arlington: Schema Press
 | pages=153
 | url=http://papers.ssrn.com/sol3/papers.cfm?abstract_id=889992
}}
*{{Citation
 | surname1=Alavi | given1=Maryam
 | surname2=Leidner | given2=Dorothy E.
 | year=1999
 | title=Knowledge management systems: issues, challenges, and benefits
 | journal=Communications of the AIS
 | volume=1| issue=2 | url=http://portal.acm.org/citation.cfm?id=374117
}}
*{{Citation
 | surname1=Kemsley | given1=Sandy
 | year=2009
 | title=Models, Social Tagging and Knowledge Management #BPM2009 #BPMS2’09
 | journal=BPM, Enterprise 2.0 and technology trends in business
 | url=http://www.column2.com/2009/09/models-social-tagging-and-knowledge-management-bpm2009-bpms209/
}}
{{refend}}

==External links==
* [http://www.inc.com/tech-blog/twitter-hashtag-techniques-for-businesses.html Hashtag Techniques for Businesses], Curt Finch. Inc Magazine. May 26, 2011.
* [http://www.tbray.org/tmp/tag-urn.html A Uniform Resource Name (URN) Namespace for Tag Metadata].  Tim Bray.  Internet draft, expired August 5, 2007.

{{Web syndication}}

{{DEFAULTSORT:Tag (Metadata)}}
[[Category:Collective intelligence]]
[[Category:Computer jargon]]
[[Category:Information retrieval techniques]]
[[Category:Knowledge representation]]
[[Category:Metadata]]
[[Category:Reference]]
[[Category:Web 2.0]]
<=====doc_Id=====>:638
<=====title=====>:
Category:String similarity measures
<=====text=====>:
{{Cat main|String metrics}}

[[Category:Algorithms on strings|Similarity]]
[[Category:Information retrieval techniques]]
[[Category:Metric geometry]]
[[Category:Information theory]]
[[Category:String (computer science)]]
<=====doc_Id=====>:641
<=====title=====>:
Category:Vector space model
<=====text=====>:
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:644
<=====title=====>:
Learning to rank
<=====text=====>:
{{machine learning bar}}
'''Learning to rank'''<ref name="liu">{{citation
|author=Tie-Yan Liu
|title=Learning to Rank for Information Retrieval
|series=Foundations and Trends in Information Retrieval
|year=2009
|isbn=978-1-60198-244-5
|doi=10.1561/1500000016
|pages=225–331
|journal=Foundations and Trends in Information Retrieval
|volume=3
|issue=3
}}. Slides from Tie-Yan Liu's talk at [[World Wide Web Conference|WWW]] 2009 conference are [http://www2009.org/pdf/T7A-LEARNING%20TO%20RANK%20TUTORIAL.pdf available online]
</ref> or '''machine-learned ranking''' (MLR) is the application of [[machine learning]], typically [[Supervised learning|supervised]], [[Semi-supervised learning|semi-supervised]] or [[reinforcement learning]], in the construction of [[ranking function|ranking models]] for [[information retrieval]] systems.<ref>[[Mehryar Mohri]], Afshin Rostamizadeh, Ameet Talwalkar (2012) ''Foundations of Machine Learning'', The
MIT Press ISBN 9780262018258.</ref> [[Training data]] consists of lists of items with some [[partial order]] specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. "relevant" or "not relevant") for each item. The ranking model's purpose is to rank, i.e. produce a [[permutation]] of items in new, unseen lists in a way which is "similar" to rankings in the training data in some sense.

== Applications ==

=== In information retrieval ===
[[File:MLR-search-engine-example.png|250px|thumb|A possible architecture of a machine-learned search engine.]]
Ranking is a central part of many [[information retrieval]] problems, such as [[document retrieval]], [[collaborative filtering]], [[sentiment analysis]], and [[online advertising]].

A possible architecture of a machine-learned search engine is shown in the figure to the right.

Training data consists of queries and documents matching them together with relevance degree of each match. It may be prepared manually by human ''assessors'' (or ''raters'', as [[Google]] calls them),
<!-- "assessor" is the more standard term, used e.g. by TREC conference -->
who check results for some queries and determine [[Relevance (information retrieval)|relevance]] of each result. It is not feasible to check relevance of all documents, and so typically a technique called [[pooling (information retrieval)|pooling]] is used — only the top few documents, retrieved by some existing ranking models are checked. <!--
  TODO: write something about selection bias caused by pooling
--> Alternatively, training data may be derived automatically by analyzing ''clickthrough logs'' (i.e. search results which got clicks from users),<ref name="Joachims2002">{{citation
 | author=Joachims, T.
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
 | url=http://www.cs.cornell.edu/people/tj/publications/joachims_02c.pdf
 | title=Optimizing Search Engines using Clickthrough Data
 | year=2002
}}</ref> ''query chains'',<ref>{{citation
 |author1=Joachims T. |author2=Radlinski F. | title=Query Chains: Learning to Rank from Implicit Feedback
 | url=http://radlinski.org/papers/Radlinski05QueryChains.pdf
 | year=2005
 | journal=Proceedings of the ACM Conference on [[SIGKDD|Knowledge Discovery and Data Mining]]
}}</ref> or such search engines' features as Google's [[Google SearchWiki|SearchWiki]].

Training data is used by a learning algorithm to produce a ranking model which computes relevance of documents for actual queries.

Typically, users expect a search query to complete in a short time (such as a few hundred milliseconds for web search), which makes it impossible to evaluate a complex ranking model on each document in the corpus, and so a two-phase scheme is used.<ref>{{citation
 |author1=B. Cambazoglu |author2=H. Zaragoza |author3=O. Chapelle |author4=J. Chen |author5=C. Liao |author6=Z. Zheng |author7=J. Degenhardt. | title=Early exit optimizations for additive machine learned ranking systems
 | journal=WSDM '10: Proceedings of the Third ACM International Conference on Web Search and Data Mining, 2010. 
 | url=http://olivier.chapelle.cc/pub/wsdm2010.pdf
}}</ref> First, a small number of potentially relevant documents are identified using simpler retrieval models which permit fast query evaluation, such as the [[vector space model]], [[Standard Boolean model|boolean model]], weighted AND,<ref>{{citation
 |author1=Broder A. |author2=Carmel D. |author3=Herscovici M. |author4=Soffer A. |author5=Zien J. | title=Efficient query evaluation using a two-level retrieval process
 | journal=Proceedings of the twelfth international conference on Information and knowledge management
 | year=2003
 | pages=426–434
 | isbn=1-58113-723-0
 | url=http://cis.poly.edu/westlab/papers/cntdstrb/p426-broder.pdf
 }}</ref> and [[Okapi BM25|BM25]]. This phase is called ''top-<math>k</math> document retrieval'' and many heuristics were proposed in the literature to accelerate it, such as using a document's static quality score and tiered indexes.<ref name="manning-q-eval">{{citation
 |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. | title=Introduction to Information Retrieval
 | publisher=Cambridge University Press
 | year=2008}}. Section [http://nlp.stanford.edu/IR-book/html/htmledition/efficient-scoring-and-ranking-1.html 7.1]</ref> In the second phase, a more accurate but computationally expensive machine-learned model is used to re-rank these documents.

=== In other areas ===
Learning to rank algorithms have been applied in areas other than information retrieval:
* In [[machine translation]] for ranking a set of hypothesized translations;<ref name="Duh09">{{citation
 | author=Kevin K. Duh
 | title=Learning to Rank with {{sic|hide=y|Partially|-}}Labeled Data
 | year=2009
 | url=http://ssli.ee.washington.edu/people/duh/thesis/uwthesis.pdf
}}</ref>
* In [[computational biology]] for ranking candidate 3-D structures in protein structure prediction problem.<ref name="Duh09" />
* In [[Recommender system]]s for identifying a ranked list of related news articles to recommend to a user after he or she has read a current news article.<ref>Yuanhua Lv, Taesup Moon, Pranam Kolari, Zhaohui Zheng, Xuanhui Wang, and Yi Chang, [http://sifaka.cs.uiuc.edu/~ylv2/pub/www11-relatedness.pdf ''Learning to Model Relatedness for News Recommendation''], in International Conference on World Wide Web (WWW), 2011.</ref>

== Feature vectors ==
For convenience of MLR algorithms, query-document pairs are usually represented by numerical vectors, which are called ''[[feature vector]]s''. Such an approach is sometimes called ''bag of features'' and is analogous to the [[bag of words]] model and [[vector space model]] used in information retrieval for representation of documents.

Components of such vectors are called ''[[feature (machine learning)|feature]]s'', ''factors'' or ''ranking signals''. They may be divided into three groups (features from [[document retrieval]] are shown as examples):
* ''Query-independent'' or ''static'' features — those features, which depend only on the document, but not on the query. For example, [[PageRank]] or document's length. Such features can be precomputed in off-line mode during indexing. They may be used to compute document's ''static quality score'' (or ''static rank''), which is often used to speed up search query evaluation.<ref name="manning-q-eval" /><ref>
{{cite conference
 | first=M. |last=Richardson |author2=Prakash, A. |author3=Brill, E.
 | title=Beyond PageRank: Machine Learning for Static Ranking
 | booktitle=Proceedings of the 15th International World Wide Web Conference
 | pages=707–715
 | publisher=
 | year=2006
 | url=http://research.microsoft.com/en-us/um/people/mattri/papers/www2006/staticrank.pdf
 | accessdate=
 }}</ref>
* ''Query-dependent'' or ''dynamic'' features — those features, which depend both on the contents of the document and the query, such as [[TF-IDF]] score or other non-machine-learned ranking functions.
* ''Query level features'' or ''query features'', which depend only on the query. For example, the number of words in a query. ''Further information: [[query level feature]]''

Some examples of features, which were used in the well-known [[LETOR]] dataset:<ref name="letor3">[http://research.microsoft.com/en-us/people/taoqin/letor3.pdf LETOR 3.0. A Benchmark Collection for Learning to Rank for Information Retrieval]</ref>
* TF, [[TF-IDF]], [[Okapi BM25|BM25]], and [[language modeling]] scores of document's [[Zone (information retrieval)|zone]]s (title, body, anchors text, URL) for a given query;
* Lengths and [[Inverse document frequency|IDF]] sums of document's zones;
* Document's [[PageRank]], [[HITS algorithm|HITS]] ranks and their variants.

Selecting and designing good features is an important area in machine learning, which is called [[feature engineering]].

== Evaluation measures ==
There are several measures (metrics) which are commonly used to judge how well an algorithm is doing on training data and to compare performance of different MLR algorithms. Often a learning-to-rank problem is reformulated as an optimization problem with respect to one of these metrics.

Examples of ranking quality measures:
* [[Information retrieval#Mean average precision|Mean average precision]] (MAP);
* [[Discounted cumulative gain|DCG]] and [[Normalized discounted cumulative gain|NDCG]];
* [[Precision (information retrieval)|Precision]]@''n'', NDCG@''n'', where "@''n''" denotes that the metrics are evaluated only on top ''n'' documents;
* [[Mean reciprocal rank]];
* [[Kendall's tau]]
* [[Spearman's rank correlation coefficient|Spearman's Rho]]

DCG and its normalized variant NDCG are usually preferred in academic research when multiple levels of relevance are used.<ref>http://www.stanford.edu/class/cs276/handouts/lecture15-learning-ranking.ppt</ref> Other metrics such as MAP, MRR and precision, are defined only for binary judgements.

Recently, there have been proposed several new evaluation metrics which claim to model user's satisfaction with search results better than the DCG metric:
* [[Expected reciprocal rank]] (ERR);<ref>{{citation
|author1=Olivier Chapelle |author2=Donald Metzler |author3=Ya Zhang |author4=Pierre Grinspan |title=Expected Reciprocal Rank for Graded Relevance
|url=https://web.archive.org/web/20120224053008/http://research.yahoo.com/files/err.pdf
|journal=CIKM
|year=2009
|pages=
}}</ref>
* [[Yandex]]'s pfound.<ref>{{citation
|author1=Gulin A. |author2=Karpovich P. |author3=Raskovalov D. |author4=Segalovich I. |title=Yandex at ROMIP'2009: optimization of ranking algorithms by machine learning methods
|url=http://romip.ru/romip2009/15_yandex.pdf
|journal=Proceedings of ROMIP'2009
|year=2009
|pages=163–168
}} (in Russian)</ref>
Both of these metrics are based on the assumption that the user is more likely to stop looking at search results after examining a more relevant document, than after a less relevant document.

== Approaches ==
{{Expand section|date=December 2009}}
Tie-Yan Liu of [[Microsoft Research Asia]] has analyzed existing algorithms for learning to rank problems in his paper "Learning to Rank for Information Retrieval".<ref name="liu" /> He categorized them into three groups by their input representation and [[loss function]]:

=== Pointwise approach ===
In this case it is assumed that each query-document pair in the training data has a numerical or ordinal score. Then learning-to-rank problem can be approximated by a regression problem — given a single query-document pair, predict its score.

A number of existing [[Supervised learning|supervised]] machine learning algorithms can be readily used for this purpose. [[Ordinal regression]] and [[classification (machine learning)|classification]] algorithms can also be used in pointwise approach when they are used to predict score of a single query-document pair, and it takes a small, finite number of values.

=== Pairwise approach ===
In this case learning-to-rank problem is approximated by a classification problem — learning a [[binary classifier]] that can tell which document is better in a given pair of documents. The goal is to minimize average number of [[Permutation#Inversions|inversions]] in ranking.

=== Listwise approach ===
These algorithms try to directly optimize the value of one of the above evaluation measures, averaged over all queries in the training data. This is difficult because most evaluation measures are not continuous functions with respect to ranking model's parameters, and so continuous approximations or bounds on evaluation measures have to be used.

=== List of methods ===
A partial list of published learning-to-rank algorithms is shown below with years of first publication of each method:
:{|class="wikitable sortable"
! Year || Name || Type || Notes
|-
| 1989 || OPRF <ref name="Fuhr1989">{{citation
 | last=Fuhr
 | first=Norbert
 | journal=ACM Transactions on Information Systems
 | title=Optimum polynomial retrieval functions based on the probability ranking principle
 | volume=7
 | number=3
 | pages=183–204 
 | year=1989
 | doi=10.1145/65943.65944
}}</ref> || <span style="display:none">2</span> pointwise || Polynomial regression (instead of machine learning, this work refers to pattern recognition, but the idea is the same)
|-
| 1992 || SLR <ref name="Cooperetal1992">{{citation
 |author1=Cooper, William S. |author2=Gey, Frederic C. |author3=Dabney, Daniel P. | journal=SIGIR '92 Proceedings of the 15th annual international ACM SIGIR conference on Research and development in information retrieval 
 | title=Probabilistic retrieval based on staged logistic regression
 | pages=198–210 
 | year=1992
 | doi=10.1145/133160.133199
}}</ref>   || <span style="display:none">2</span> pointwise || Staged logistic regression
|-
| 2000 || [http://research.microsoft.com/apps/pubs/default.aspx?id=65610 Ranking SVM] (RankSVM) || <span style="display:none">2</span> pairwise ||  A more recent exposition is in,<ref name="Joachims2002" /> which describes an application to ranking using clickthrough logs.
|-
| 2002 || Pranking<ref>{{cite journal | citeseerx = 10.1.1.20.378 | title = Pranking }}</ref> || <span style="display:none">1</span> pointwise || Ordinal regression.
|-
| 2003 <!-- or 1998? --> || [http://jmlr.csail.mit.edu/papers/volume4/freund03a/freund03a.pdf RankBoost] || <span style="display:none">2</span> pairwise ||
|-
| 2005 || [http://research.microsoft.com/en-us/um/people/cburges/papers/ICML_ranking.pdf RankNet] || <span style="display:none">2</span> pairwise ||
|-
| 2006 || [http://research.microsoft.com/en-us/people/tyliu/cao-et-al-sigir2006.pdf IR-SVM] || <span style="display:none">2</span> pairwise || Ranking SVM with query-level normalization in the loss function.
|-
| 2006 || [http://research.microsoft.com/en-us/um/people/cburges/papers/lambdarank.pdf LambdaRank] || pairwise/listwise || RankNet in which pairwise loss function is multiplied by the change in the IR metric caused by a swap.
|-
| 2007 || [http://research.microsoft.com/en-us/people/junxu/sigir2007-adarank.pdf AdaRank] || <span style="display:none">3</span> listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70364 FRank] || <span style="display:none">2</span> pairwise || Based on RankNet, uses a different loss function - fidelity loss.
|-
| 2007 || [http://www.cc.gatech.edu/~zha/papers/fp086-zheng.pdf GBRank] || <span style="display:none">2</span> pairwise || 
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=70428 ListNet] || <span style="display:none">3</span> listwise ||
|-
| 2007 || [http://research.microsoft.com/apps/pubs/default.aspx?id=68128 McRank] || <span style="display:none">1</span> pointwise ||
|-
| 2007 || [http://www.stat.rutgers.edu/~tzhang/papers/nips07-ranking.pdf QBRank] || <span style="display:none">2</span> pairwise ||
|-
| 2007 || [http://research.microsoft.com/en-us/people/hangli/qin_ipm_2008.pdf RankCosine] || <span style="display:none">3</span> listwise ||
|-
| 2007 || RankGP<ref>{{cite journal | citeseerx = 10.1.1.90.220 | title = RankGP }}</ref> || <span style="display:none">3</span> listwise ||
|-
| 2007 || [http://staff.cs.utu.fi/~aatapa/publications/inpPaTsAiBoSa07a.pdf RankRLS] || <span style="display:none">2</span> pairwise ||
Regularized least-squares based ranking. The work is extended in
<ref name=pahikkala2009efficient>{{Citation|last=Pahikkala|first=Tapio |author2=Tsivtsivadze, Evgeni |author3=Airola, Antti |author4=Järvinen, Jouni |author5=Boberg, Jorma |title=An efficient algorithm for learning to rank from preference graphs|journal=Machine Learning|year=2009|volume=75|issue=1|pages=129–165|doi=10.1007/s10994-008-5097-z|postscript=.}}</ref> to learning to rank from general preference graphs.
|-
| 2007 || [http://www.cs.cornell.edu/People/tj/publications/yue_etal_07a.pdf SVM<sup>map</sup>] || <span style="display:none">3</span> listwise ||
|-
| 2008 || [http://research.microsoft.com/pubs/69536/tr-2008-109.pdf LambdaMART] || pairwise/listwise || Winning entry in the recent Yahoo Learning to Rank competition used an ensemble of LambdaMART models.<ref>C. Burges. (2010). [http://research.microsoft.com/en-us/um/people/cburges/tech_reports/MSR-TR-2010-82.pdf From RankNet to LambdaRank to LambdaMART: An Overview].</ref>
|-
| 2008 || [http://research.microsoft.com/en-us/people/tyliu/icml-listmle.pdf ListMLE] || <span style="display:none">3</span> listwise || Based on ListNet.
|-
| 2008 || [http://research.microsoft.com/en-us/people/junxu/sigir2008-directoptimize.pdf PermuRank] || <span style="display:none">3</span> listwise ||
|-
| 2008 || [http://research.microsoft.com/apps/pubs/?id=63585 SoftRank] || <span style="display:none">3</span> listwise ||
|-
| 2008 || [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf Ranking Refinement]<ref>Rong Jin, Hamed Valizadegan, Hang Li, [http://www.cs.pitt.edu/~valizadegan/Publications/ranking_refinement.pdf ''Ranking Refinement and Its Application for Information Retrieval''], in International Conference on World Wide Web (WWW), 2008.</ref> || <span style="display:none">2</span> pairwise || A semi-supervised approach to learning to rank that uses Boosting.
|-
| 2008 || [http://www-connex.lip6.fr/~amini/SSRankBoost/ SSRankBoost]<ref>Massih-Reza Amini, Vinh Truong, Cyril Goutte, [http://www-connex.lip6.fr/~amini/Publis/SemiSupRanking_sigir08.pdf ''A Boosting Algorithm for Learning Bipartite Ranking Functions with Partially Labeled Data''], International ACM SIGIR conference, 2008. The [http://www-connex.lip6.fr/~amini/SSRankBoost/ code] is available for research purposes.</ref>  || <span style="display:none">2</span> pairwise|| An extension of RankBoost to learn with partially labeled data (semi-supervised learning to rank)
|-
| 2008 || [http://phd.dii.unisi.it/PosterDay/2009/Tiziano_Papini.pdf SortNet]<ref>Leonardo Rigutini, Tiziano Papini, Marco Maggini, Franco Scarselli, [http://research.microsoft.com/en-us/um/beijing/events/lr4ir-2008/PROCEEDINGS-LR4IR%202008.PDF "SortNet: learning to rank by a neural-based sorting algorithm"], SIGIR 2008 workshop: Learning to Rank for Information Retrieval, 2008</ref> || <span style="display:none">2</span> pairwise|| SortNet, an adaptive ranking algorithm which orders objects using a neural network as a comparator. 
|-
| 2009 || [http://itcs.tsinghua.edu.cn/papers/2009/2009031.pdf MPBoost] || <span style="display:none">2</span> pairwise || Magnitude-preserving variant of RankBoost. The idea is that the more unequal are labels of a pair of documents, the harder should the algorithm try to rank them.
|-
| 2009 || [http://www.machinelearning.org/archive/icml2009/papers/498.pdf BoltzRank] || <span style="display:none">3</span> listwise || Unlike earlier methods, BoltzRank produces a ranking model that looks during query time not just at a single document, but also at pairs of documents.
|-
| 2009 || [http://www.iis.sinica.edu.tw/papers/whm/8820-F.pdf BayesRank] || <span style="display:none">3</span> listwise || A method combines Plackett-Luce Model and neural network to minimize the expected Bayes risk, related to NDCG, from the decision-making aspect.
|-
| 2010 || [https://people.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf NDCG Boost]<ref>Hamed Valizadegan, Rong Jin, Ruofei Zhang, Jianchang Mao, [http://www.cs.pitt.edu/~valizadegan/Publications/NDCG_Boost.pdf ''Learning to Rank by Optimizing NDCG Measure''], in Proceeding of Neural Information Processing Systems (NIPS), 2010.</ref> || <span style="display:none">3</span> listwise || A boosting approach to optimize NDCG.
|-
| 2010 || [http://arxiv.org/abs/1001.4597 GBlend] || <span style="display:none">2</span> pairwise || Extends GBRank to the learning-to-blend problem of jointly solving multiple learning-to-rank problems with some shared features.
|-
| 2010 || [http://wume.cse.lehigh.edu/~ovd209/wsdm/proceedings/docs/p151.pdf IntervalRank] || <span style="display:none">2</span> pairwise & listwise || 
|-
| 2010 || [http://www.eecs.tufts.edu/~dsculley/papers/combined-ranking-and-regression.pdf CRR] || <span style="display:none">2</span> pointwise & pairwise || Combined Regression and Ranking. Uses [[stochastic gradient descent]] to optimize a linear combination of a pointwise quadratic loss and a pairwise hinge loss from Ranking SVM.
|}

Note: as most [[supervised learning]] algorithms can be applied to pointwise case, only those methods which are specifically designed with ranking in mind are shown above.

== History ==
[[Norbert Fuhr]] introduced the general idea of MLR in 1992, describing learning approaches in information retrieval as a generalization of parameter estimation;<ref name="Fuhr1992">{{citation
 | last=Fuhr
 | first=Norbert
 | journal=Computer Journal
 | title=Probabilistic Models in Information Retrieval
 | volume=35
 | number=3
 | pages=243–255
 | year=1992
 | doi=10.1093/comjnl/35.3.243
}}</ref> a specific variant of this approach (using [[polynomial regression]]) had been published by him three years earlier.<ref name="Fuhr1989" /> Bill Cooper proposed [[logistic regression]] for the same purpose in 1992 <ref name="Cooperetal1992" /> and used it with his  [[University of California at Berkeley|Berkeley]] research group to train a successful ranking function for [[Text Retrieval Conference|TREC]].  Manning et al.<ref>{{citation |author1=Manning C. |author2=Raghavan P. |author3=Schütze H. |title=Introduction to Information Retrieval |publisher=Cambridge University Press |year=2008}}. Sections [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-7.html 7.4] and [http://nlp.stanford.edu/IR-book/html/htmledition/references-and-further-reading-15.html 15.5]</ref>  suggest that these early works achieved limited results in their time due to little available training data and poor machine learning techniques.

Several conferences, such as [[Neural Information Processing Systems|NIPS]], [[Special Interest Group on Information Retrieval|SIGIR]] and [[International Conference on Machine Learning|ICML]] had workshops devoted to the learning-to-rank problem since mid-2000s (decade).

=== Practical usage by search engines ===
Commercial [[web search engine]]s began using machine learned ranking systems since the 2000s (decade). One of the first search engines to start using it was [[AltaVista]] (later its technology was acquired by [[Overture Services, Inc.|Overture]], and then [[Yahoo]]), which launched a [[gradient boosting]]-trained ranking function in April 2003.<ref>Jan O. Pedersen. [http://jopedersen.com/Presentations/The_MLR_Story.pdf The MLR Story]</ref><ref>{{US Patent|7197497}}</ref>

[[Bing (search engine)|Bing]]'s search is said to be powered by [[RankNet]] algorithm,<ref>[http://www.bing.com/community/blogs/search/archive/2009/06/01/user-needs-features-and-the-science-behind-bing.aspx?PageIndex=4 Bing Search Blog: User Needs, Features and the Science behind Bing]</ref>{{when|date=February 2014}} which was invented at [[Microsoft Research]] in 2005.

In November 2009 a Russian search engine [[Yandex]] announced<ref name="snezhinsk">[http://webmaster.ya.ru/replies.xml?item_no=5707&ncrnd=5118 Yandex corporate blog entry about new ranking model "Snezhinsk"] (in Russian)</ref> that it had significantly increased its [[search quality]] due to deployment of a new proprietary [[MatrixNet]] algorithm, a variant of [[gradient boosting]] method which uses [[oblivious decision tree]]s.<ref>The algorithm wasn't disclosed, but a few details were made public in [http://download.yandex.ru/company/experience/GDD/Zadnie_algoritmy_Karpovich.pdf] and [http://download.yandex.ru/company/experience/searchconf/Searchconf_Algoritm_MatrixNet_Gulin.pdf].</ref> Recently they have also sponsored a machine-learned ranking competition "Internet Mathematics 2009"<ref>[http://imat2009.yandex.ru/academic/mathematic/2009/en/ Yandex's Internet Mathematics 2009 competition page]</ref> based on their own search engine's production data. Yahoo has announced a similar competition in 2010.<ref>[http://learningtorankchallenge.yahoo.com/ Yahoo Learning to Rank Challenge]</ref>

As of 2008, [[Google]]'s [[Peter Norvig]] denied that their search engine exclusively relies on machine-learned ranking.<ref>{{cite web
  | url = http://anand.typepad.com/datawocky/2008/05/are-human-experts-less-prone-to-catastrophic-errors-than-machine-learned-models.html
  | archiveurl = http://www.webcitation.org/5sq8irWNM
  | archivedate = 2010-09-18
  | title = Are Machine-Learned Models Prone to Catastrophic Errors?
  | date = 2008-05-24
  | last = Rajaraman
  | first = Anand
  | authorlink = Anand Rajaraman}}</ref> [[Cuil]]'s CEO, [[Tom Costello (businessman)|Tom Costello]], suggests that they prefer hand-built models because they can outperform machine-learned models when measured against metrics like click-through rate or time on landing page, which is because machine-learned models "learn what people say they like, not what people actually like".<ref>{{cite web
  | url = http://www.cuil.com/info/blog/2009/06/26/so-how-is-bing-doing
  | archiveurl = http://www.webcitation.org/5sq7DX3Pj
  | archivedate = 2010-09-15
  | title = Cuil Blog: So how is Bing doing?
  | date = 2009-06-26
  | last = Costello
  | first = Tom}}</ref>

== References ==
{{reflist|2}}

== External links ==
; Competitions and public datasets
* [http://research.microsoft.com/en-us/um/people/letor/ LETOR: A Benchmark Collection for Research on Learning to Rank for Information Retrieval]
* [http://imat2009.yandex.ru/en/ Yandex's Internet Mathematics 2009]
* [http://learningtorankchallenge.yahoo.com/ Yahoo! Learning to Rank Challenge]
* [http://research.microsoft.com/en-us/projects/mslr/default.aspx Microsoft Learning to Rank Datasets]

; Open Source code
* [https://mloss.org/software/view/332/ Parallel C++/MPI implementation of Gradient Boosted Regression Trees for ranking, released September 2011]
* [https://sites.google.com/site/rtranking/ C++ implementation of Gradient Boosted Regression Trees and Random Forests for ranking]
* [http://dlib.net/ml.html#svm_rank_trainer C++ and Python tools for using the SVM-Rank algorithm]

[[Category:Information retrieval techniques]]
[[Category:Machine learning]]
[[Category:Ranking functions]]
<=====doc_Id=====>:647
<=====title=====>:
Semantic technology
<=====text=====>:
{{no footnotes|date=March 2013}}
[[File:SemanticNetExample.jpg|thumb|Simplistic example of the sort of semantic net used in Semantic Web technology]]
In [[software]], '''semantic technology''' encodes meanings separately from data and content files, and separately from application code. 

This enables machines as well as people to understand, share and reason with them at execution time. With semantic technologies, adding, changing and implementing new relationships or interconnecting programs in a different way can be just as simple as changing the external model that these programs share.

With traditional [[information technology]], on the other hand, meanings and relationships must be predefined and “hard wired” into data formats and the application program code at design time. This means that when something changes, previously unexchanged information needs to be exchanged, or two programs need to interoperate in a new way, the humans must get involved.

Off-line, the parties must define and communicate between them the knowledge needed to make the change, and then recode the data structures and program logic to accommodate it, and then apply these changes to the database and the application. Then, and only then, can they implement the changes.

Semantic technologies are “meaning-centered.” They include tools for:

* autorecognition of topics and concepts, 
* information and meaning extraction, and
* categorization. 

Given a question, semantic technologies can directly search topics, concepts, associations that span a vast number of sources.

Semantic technologies provide an abstraction layer above existing IT technologies that enables bridging and interconnection of data, content, and processes. Second, from the portal perspective, semantic technologies can be thought of as a new level of depth that provides far more intelligent, capable, relevant, and responsive interaction than with information technologies alone.

== See also ==
* [[Business Intelligence 2.0]] (BI 2.0)
* [[Metadata]]
* [[Ontology (computer science)]]
* [[Semantic web]]

==References==

* J.T. Pollock, R. Hodgson. ''Adaptive Information: Improving Business Through Semantic Interoperability, Grid Computing, and Enterprise Integration.'' [[J. Wiley and Sons]], October 2004
* R. Guha, R. McCool, and E. Miller. Semantic search. In ''WWW2003 — Proc. of the 12th international conference on World Wide Web'', pp 700–709. [[ACM Press]], 2003.
* I. Polikoff and D. Allemang. [https://lists.oasis-open.org/archives/regrep-semantic/200402/pdf00000.pdf Semantic technology.] ''TopQuadrant Technology Briefing'' v1.1, September 2003.
* [[Tim Berners-Lee|T. Berners-Lee]], J. Hendler, and O. Lassila. The Semantic Web: A new form of Web content that is meaningful to computers will unleash a revolution of new possibilities. ''[[Scientific American]]'', May 2001.
* A.P. Sheth, C. Ramakrishnan. [http://corescholar.libraries.wright.edu/knoesis/970Technology%20In%20Action:%20Ontology%20Driven%20Information%20Systems%20For%20Search,%20Integration%20and%20Analysis. Semantic (Web) Technology In Action: Ontology Driven Information Systems For Search, Integration and Analysis.] ''[[IEEE Data Engineering Bulletin]]'', 2003.
* Steffen Staab, Rudi Studer  (Ed.), Handbook on Ontologies, Springer, 
* Mills Davis. The Business Value of Semantic Technologies. Presentation and Report. Semantic Technologies for E-Government, September
2004.
* P. Hitzler, M. Krötzsch, S. Rudolph, Foundations of Semantic Web Technologies, Chapman&Hall/CRC, 2009, ISBN 978-1-4200-9050-5

== External links ==
* [http://semtech2010.semanticuniverse.com Semantic Technology Conference]

[[Category:Information retrieval techniques]]
[[Category:Semantics]]
<=====doc_Id=====>:650
<=====title=====>:
Literature-based discovery
<=====text=====>:
'''Literature-based discovery''' refers to the use of papers and other [[Academic publishing|academic publications]] (the "literature") to find new relationships between existing knowledge (the "discovery"). The technique was pioneered by [[Don R. Swanson]] in the 1980s and has since seen widespread use. 

Literature-based discovery does not generate new knowledge through laboratory experiments, as is customary for [[empirical]] sciences. Instead it seeks to connect existing knowledge from empirical results by bringing to light relationships that are implicated and "neglected".<ref>{{cite journal | last1 = Swanson | first1 = Don | year = 1988 | title = Migraine and Magnesium: Eleven Neglected Connections | url = | journal = Perspectives in Biology and Medicine | volume = 31 | issue = 4| pages = 526–557 | doi=10.1353/pbm.1988.0009}}</ref> It is marked by [[empiricism]] and [[rationalism]] in concert or [[consilience]].

==Swanson linking==
[[File:Swanson linking.jpg|thumb|Swanson linking example diagram]]
''Swanson linking'' is a term proposed in 2003<ref>Stegmann J, Grohmann G. Hypothesis generation guided by co-word clustering. Scientometrics. 2003;56:111–135. As quoted by Bekhuis</ref> that refers to connecting two pieces of knowledge previously thought to be unrelated.<ref>{{cite journal|last=Bekhuis|first=Tanja|title=Conceptual biology, hypothesis discovery, and text mining: Swanson's legacy|publisher=BioMed Central Ltd.|year=2006|pmc=1459187|pmid=16584552|doi=10.1186/1742-5581-3-2|volume=3|journal=Biomed Digit Libr|pages=2}}</ref> For example, it may be known that illness A is caused by chemical B, and that drug C is known to reduce the amount of chemical B in the body. However, because the respective articles were published separately from one another (called "disjoint data"), the relationship between illness A and drug C may be unknown. ''Swanson linking'' aims to find these relationships and report them.

==See also==
*[[Arrowsmith System]]
*[[Implicature]]
*[[Latent semantic indexing]]
*[[Metaphor]]

==References==
* Chen, Ran; Hongfei Lin & Zhihao Yang (2011). "Passage retrieval based hidden knowledge discovery from biomedical literature." ''Expert Systems with Applications: An International Journal'' (August, 2011), vol. 38, no. 8, pp.&nbsp;9958–9964.
*:  '''Abstract''': [...] automatic extraction of the implicit biological relationship from biomedical literature contributes to building the biomedical hypothesis that can be explored further experimentally. This paper presents a passage retrieval based method which can explore the hidden connection from MEDLINE records. [...] Experimental results show this method can significantly improve the hidden knowledge discovery performance. @ [http://portal.acm.org/citation.cfm?id=1967763.1968003&coll=DL&dl=GUIDE&CFID=23143258&CFTOKEN=52033794 ACM DL]

; Further readings
* [[Patrick Wilson (librarian)|Wilson, Patrick]] (1977). ''Public Knowledge, Private Ignorance: Toward a Library and Information Policy''. Greenwood Publishing Group. p.&nbsp;156. ISBN 0-8371-9485-7.

; Footnotes
{{reflist}}

[[Category:Information retrieval techniques]]
[[Category:Medical research]]


{{science-stub}}
<=====doc_Id=====>:653
<=====title=====>:
Index term
<=====text=====>:
An '''index term''', '''subject term''', '''subject heading''', or '''descriptor''', in [[information retrieval]], is a term that captures the essence of the topic of a document. Index terms make up a [[controlled vocabulary]] for use in [[bibliographic record]]s. They are an integral part of bibliographic control, which is the function by which libraries collect, organize and disseminate documents. They are used as keywords to retrieve documents in an information system, for instance, a catalog or a [[search engine]].  A popular form of keywords on the web are [[tag (metadata)|tags]] which are directly visible and can be assigned by non-experts. Index terms can consist of a word, phrase, or alphanumerical term. They are created by analyzing the document either manually with [[subject indexing]] or automatically with [[Index (search engine)|automatic indexing]] or more sophisticated methods of keyword extraction. Index terms can either come from a controlled vocabulary or be freely assigned.

Keywords are stored in a [[Index (search engine)|search index]]. Common words like [[article (grammar)|articles]] (a, an, the) and conjunctions (and, or, but) are not treated as keywords because it's inefficient. Almost every English-language site on the Internet has the article "''the''", and so it makes no sense to search for it. The most popular search engine, [[Google]] removed [[stop words]] such as "the" and "a" from its indexes for several years, but then re-introduced them, making certain types of precise search possible again.

The term "descriptor" was coined by [[Calvin Mooers]] in 1948. It is in particular used about a preferred term from a [[Thesaurus (information retrieval)|thesaurus]].

The [[Simple Knowledge Organization System]] language (SKOS) provides a way to express index terms with [[Resource Description Framework]] for use in the context of [[Semantic Web]].<ref name="auto">{{cite book|last=Svenonius|first=Elaine|author-link=Elaine Svenonius|title=The intellectual foundation of information organization|date=2009|publisher=MIT Press|location=Cambridge, Mass.|isbn=9780262512619|edition=1st MIT Press pbk.}}</ref>

==In web search engines==
Most [[web search engine]]s are designed to search for words anywhere in a document—the title, the body, and so on. This being the case, a keyword can be any term that exists within the document. However, priority is given to words that occur in the title, words that recur numerous times, and words that are explicitly assigned as keywords within the coding.<ref>Cutts, Matt. (2010, March 4). ''How search works.'' Retrieved from https://www.youtube.com/watch?v=BNHR6IQJGZs</ref> Index terms can be further refined using [[Boolean algebra|Boolean operators]] such as "AND, OR, NOT." "AND" is normally unnecessary as most search engines infer it. "OR" will search for results with one search term or another, or both. "NOT" eliminates a word or phrase from the search, getting rid of any results that include it. Multiple words can also be enclosed in quotation marks to turn the individual index terms into a specific index ''phrase''. These modifiers and methods all help to refine search terms, to better maximize the accuracy of search results.<ref>CLIO. ''Keyword search''. Columbia University Libraries. Retrieved from http://www.columbia.edu/cu/lweb/help/clio/keyword.html</ref>

==Author keywords==
Many journals and databases provides access (also) to index terms made by authors to the articles being published or represented. The relative quality of indexer-provided index terms and author provided index terms is of interest to research in information retrieval. The quality of both kinds of indexing terms depends, of course, on the qualifications of provider. In general authors have difficulties providing indexing terms that characterizes his document ''relative'' to the other documents in the database. Author keywords are an integral part of literature.<ref name="auto"/>

==Examples==
*[[Canadian Subject Headings]] (CSH)
*[[Library of Congress Subject Headings]] (LCSH)
*[[Medical Subject Headings]] (MeSH)
*[[Polythematic Structured Subject Heading System]] (PSH)
*[[Subject Headings Authority File]] (SWD)

==See also==
*[[Dynamic keyword insertion]]
*[[Tag cloud]]
*[[Keyword density]]
*[[Search engine optimization]]
*[[Tag (metadata)]]
*[[Subject (documents)]]

==References==
{{reflist}}

{{Authority control}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:656
<=====title=====>:
Fuzzy retrieval
<=====text=====>:
'''Fuzzy retrieval''' techniques are based on the [[Extended Boolean model]] and the [[Fuzzy set]] theory. There are two classical fuzzy retrieval models: Mixed Min and Max (MMM) and the Paice model. Both models do not provide a way of evaluating query weights, however this is considered by the [[Extended Boolean model|P-norms]] algorithm.

==Mixed Min and Max model (MMM)==

In fuzzy-set theory, an element has a varying degree of membership, say ''d<sub>A</sub>'', to a given set ''A'' instead of the traditional membership choice (is an element/is not an element).<br />
In MMM<ref>{{citation | last1=Fox | first1=E. A. | author2=S. Sharat | year=1986 | title=A Comparison of Two Methods for Soft Boolean Interpretation in Information Retrieval | publisher=Technical Report TR-86-1, Virginia Tech, Department of Computer Science}}</ref> each index term has a fuzzy set associated with it. A document's weight with respect to an index term ''A'' is considered to be the degree of membership of the document in the fuzzy set associated with ''A''. The degree of membership for union and intersection are defined as follows in Fuzzy set theory:<br/>
:<math>d_{A\cap B}= min(d_A, d_B)</math>
:<math>d_{A\cup B}= max(d_A,d_B)</math>

According to this, documents that should be retrieved for a query of the form ''A or B'', should be in the fuzzy set associated with the union of the two sets ''A'' and ''B''. Similarly, the documents that should be retrieved for a query of the form ''A and B'', should be in the fuzzy set associated with the intersection of the two sets. Hence, it is possible to define the similarity of a document to the ''or'' query to be ''max(d<sub>A</sub>, d<sub>B</sub>)'' and the similarity of the document to the ''and'' query to be ''min(d<sub>A</sub>, d<sub>B</sub>)''. The MMM model tries to soften the Boolean operators by considering the query-document similarity to be a linear combination of the ''min'' and ''max'' document weights.

Given a document ''D'' with index-term weights ''d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>'' for terms ''A<sub>1</sub>, A<sub>2</sub>, ..., A<sub>n</sub>'', and the queries:

''Q<sub>or</sub> = (A<sub>1</sub> or A<sub>2</sub> or ... or A<sub>n</sub>)''<br />
''Q<sub>and</sub> = (A<sub>1</sub> and A<sub>2</sub> and ... and A<sub>n</sub>)''

the query-document similarity in the MMM model is computed as follows:

''SlM(Q<sub>or</sub>, D) = C<sub>or1</sub> * max(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>or2</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>)''<br />
''SlM(Q<sub>and</sub>, D) = C<sub>and1</sub> * min(d<sub>A1</sub>, d<sub>A2</sub>, ..., d<sub>An</sub>) + C<sub>and2</sub> * max(d<sub>A1</sub>, d<sub>A2</sub> ..., d<sub>An</sub>)''

where ''C<sub>or1</sub>, C<sub>or2</sub>'' are "softness" coefficients for the ''or'' operator, and ''C<sub>and1</sub>, C<sub>and2</sub>'' are softness coefficients for the ''and'' operator. Since we would like to give the maximum of the document weights more importance while considering an ''or'' query and the minimum more importance while considering an ''and'' query, generally we have ''C<sub>or1</sub> > C<sub>or2</sub> and C<sub>and1</sub> > C<sub>and2</sub>''. For simplicity it is generally assumed that ''C<sub>or1</sub> = 1 - C<sub>or2</sub>'' and ''C<sub>and1</sub> = 1 - C<sub>and2</sub>''.

Lee and Fox<ref name="leefox">{{citation | last1=Lee | first1=W. C. | author2=E. A. Fox | year=1988 | title=Experimental Comparison of Schemes for Interpreting Boolean Queries}}</ref> experiments indicate that the best performance usually occurs with ''C<sub>and1</sub>'' in the range [0.5, 0.8] and with ''C<sub>or1</sub>'' > 0.2. In general, the computational cost of MMM is low, and retrieval effectiveness is much better than with the [[Standard Boolean model]].

==Paice model==

The Paice model<ref>{{citation | last=Paice | first=C. P. | year=1984 | title=Soft Evaluation of Boolean Search Queries in Information Retrieval Systems | publisher=Information Technology, Res. Dev. Applications, 3(1), 33-42 }}</ref> is a general extension to the MMM model. In comparison to the MMM model that considers only the minimum and maximum weights for the index terms, the Paice model incorporates all of the term weights when calculating the similarity:

:<math>S(D,Q) = \sum_{i=1}^n\frac{r^{i-1}*w_{di}}{\sum_{j=1}^n r^{j-1}}</math>

where ''r'' is a constant coefficient and ''w<sub>di</sub>'' is arranged in ascending order for ''and'' queries and descending order for ''or'' queries. When n = 2 the Paice model shows the same behavior as the MMM model.

The experiments of Lee and Fox<ref name="leefox"/> have shown that setting the ''r'' to 1.0 for ''and'' queries and 0.7 for ''or'' queries gives good retrieval effectiveness. The computational cost for this model is higher than that for the MMM model. This is because the MMM model only requires the determination of ''min'' or ''max'' of a set of term weights each time an ''and'' or ''or'' clause is considered, which can be done in ''O(n)''. The Paice model requires the term weights to be sorted in ascending or descending order, depending on whether an ''and'' clause or an ''or'' clause is being considered. This requires at least an ''0(n log n)'' sorting algorithm. A good deal of floating point calculation is needed too.

==Improvements over the Standard Boolean model==
Lee and Fox<ref name="leefox"/> compared the Standard Boolean model with MMM and Paice models with three test collections, CISI, CACM and INSPEC. These are the reported results for average mean precision improvement:
{| class="wikitable"
|-
!
! CISI
! CACM
! INSPEC
|-
! MMM
| 68%
| 109%
| 195%
|-
! Paice
| 77%
| 104%
| 206%
|}

These are very good improvements over the Standard model. MMM is very close to Paice and P-norm results which indicates that it can be a very good technique, and is the most efficient of the three.

==Recent work==

Recently '''Kang ''et al.'''.<ref>{{citation | title=Fuzzy Information Retrieval Indexed by Concept Identification | url=http://www.springerlink.com/content/ac96v4qf4f8adatp/ | last1=Kang | first1=Bo-Yeong | author2=Dae-Won Kim |author3=Hae-Jung Kim | publisher=Springer Berlin / Heidelberg | year=2005}}</ref> have devised a fuzzy retrieval system indexed by concept identification.

If we look at documents on a pure [[Tf-idf]] approach, even eliminating stop words, there will be words more relevant to the topic of the document than others and they will have the same weight because they have the same term frequency. If we take into account the user intent on a query we can better weight the terms of a document. Each term can be identified as a concept in a certain lexical chain that translates the importance of that concept for that document.<br />
They report improvements over Paice and P-norm on the average precision and recall for the Top-5 retrieved documents.

Zadrozny<ref>{{citation | title=Fuzzy information retrieval model revisited | doi=10.1016/j.fss.2009.02.012 | first1=Sławomir | last1=Zadrozny | last2=Nowacka | first2=Katarzyna | year=2009 | publisher=Elsevier North-Holland, Inc.}}</ref> revisited the fuzzy information retrieval model. He further extends the fuzzy extended Boolean model by:
* assuming linguistic terms as importance weights of keywords also in documents
* taking into account the uncertainty concerning the representation of documents and queries
* interpreting the linguistic terms in the representation of documents and queries as well as their matching in terms of the Zadeh’s fuzzy logic (calculus of linguistic statements)
* addressing some pragmatic aspects of the proposed model, notably the techniques of indexing documents and queries

The proposed model makes it possible to grasp both imprecision and uncertainty concerning the textual information representation and retrieval.

==See also==
*[[Information retrieval]]

==Further reading==
* {{citation | title=Information Retrieval: Algorithms and Data structures; Extended Boolean model | last1=Fox | first1=E. | author2=S. Betrabet | author3=M. Koushik | author4=W. Lee | year=1992 | publisher=Prentice-Hall, Inc. | url=http://www.scribd.com/doc/13742235/Information-Retrieval-Data-Structures-Algorithms-William-B-Frakes}}

==References==
{{reflist}}

{{DEFAULTSORT:Fuzzy Retrieval}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:659
<=====title=====>:
Negative search
<=====text=====>:
{{Multiple issues|
{{unreferenced|date=March 2009}}
{{orphan|date=February 2009}}
{{confusing|date=March 2009}}
}}

'''Negative Search''' is the elimination of information which is not relevant from a mass of content in order to present to a user a range of relevant content.

Negative Search is different from both Positive Search and Discovery Search. Positive Search uses the selection of relevant content as its primary mechanism. Discovery calculates relatedness (between user intent and content) to present users with relevant alternatives of which they may not have been aware.

Negative Search applies to those forms of searches where the user has the intention of finding a specific, actionable piece information but lacks the knowledge of what that specific information is or might be.

Negative Search can also apply to searches where the user has a clear understanding of '''Negative Intent''' (what they don't want) rather than what they do.

Examples of Negative Intent are:

- Job searching: someone knows they want a new job but they have no idea what it might be. They just know what they don't want.

- Online dating: someone is looking for a dating partner, but cannot identify what criteria they are looking for. They just know what they don't want.

- An investigator is looking for a car but has no other information on that car on which to base a search.

==Negative Search Classifiers==

If there are two forms of search (positive and negative) it follows that there are two forms of classifier models: '''Inclusive Classifiers''' and '''Exclusive Classifiers'''.

[[List of countries|Countries of the World]] are a good example of a MECE list. A positive search for the country Kenya would identify content referencing Kenya and present it. A Negative Search for the country Kenya would exclude all content relating to other countries in the world leaving the user with content of some relevance to Kenya.

==Irrelevancy as a Desirable Construct==

Positive Search tends to view Irrelevancy as undesirable. Having a system actively identify and pursue irrelevant content for the purpose of elimination from a [[user experience]] may prove a highly powerful mechanism.

It follows that Positive and Negative Search are not mutually exclusive and that a more powerful search may result from the combination of selection and elimination as tools to empower user experience in Negative Searches.

==Degrees of Passivity==

Positive Search involves an active search by a user with no degree of passivity (or openness). For example: "I am only interested in the Hilton Hotel in Vientiane on [[New Year\'s Eve|New Years Eve]]."

Discovery involves a simultaneous secondary more Passive search by the user while they are involved in a Positive search. For example: "I am interested in the Hilton Hotel in Vientiane on New Years Eve but if there's a better hotel, let me know"

Negative Search also involves an Active search but with a much higher degree of Passivity (or openness to discovery). For example: "I need a holiday and really don't care where as long as its good."

Searchers can be active in one dimension (Positive Search) while simultaneously being passive to alternatives or what they don't know they're looking for in many dimensions. In Discovery they are Passive in a small number of dimensions but in Negative Search they are Passive in many or all dimensions.

==References==
{{Reflist}}

[[Category:Information retrieval techniques]]
<=====doc_Id=====>:662
<=====title=====>:
Proximity search (text)
<=====text=====>:
In [[natural language processing|text processing]], a '''proximity search''' looks for documents where two or more separately matching term occurrences are within a specified [[string distance|distance]], where distance is the number of intermediate words or characters. In addition to proximity, some implementations may also impose a constraint on the word order, in that the order in the searched text must be identical to the order of the search query. Proximity searching goes beyond the simple matching of words by adding the constraint of proximity and is generally regarded as a form of advanced search.

For example, a search could be used to find "red brick house", and match phrases such as "red house of brick" or "house made of red brick". By limiting the proximity, these phrases can be matched while avoiding documents where the words are scattered or spread across a page or in unrelated articles in an anthology.

== Rationale ==
The basic linguistic assumption of proximity searching is that the proximity of the words in a document implies a [[semantic relation|relationship]] between the words. Given that authors of documents try to formulate sentences which contain a single idea, or cluster of related ideas within neighboring sentences or organized into paragraphs, there is an inherent, relatively high, probability within the document structure that words used together are related. On the other hand, when two words are on the opposite ends of a book, the probability of a relationship between the words is relatively weak. By limiting search results to only include matches where the words are within the specified maximum proximity, or distance, the search results are assumed to be of higher relevance than the matches where the words are scattered.

Commercial internet search engines tend to produce too many matches (known as recall) for the average search query. Proximity searching is one method of reducing the number of pages matches, and to improve the relevance of the matched pages by using word proximity to assist in ranking. As an added benefit, proximity searching helps combat [[spamdexing]] by avoiding webpages which contain dictionary lists or shotgun lists of thousands of words, which would otherwise rank highly if the search engine was heavily biased toward [[word frequency]].

== Boolean syntax and operators ==
Note that a proximity search can designate that only some keywords must be within a specified distance. Proximity searching can be used with other search syntax and/or controls to allow more articulate search queries. Sometimes query operators like NEAR, NOT NEAR, FOLLOWED BY, NOT FOLLOWED BY, SENTENCE or FAR are used to indicate a proximity-search limit between specified keywords: for example, "brick NEAR house".

== Usage in commercial search engines ==
In regards to implicit/automatic versus explicit proximity search, as of November 2008, most Internet [[search engine]]s only implement an implicit proximity search functionality. That is, they automatically rank those search results higher where the user keywords have a good "overall proximity score" in such results. If only two keywords are in the search query, this has no difference from an explicit proximity search which puts a NEAR operator between the two keywords. However, if three or more than three keywords are present, it is often important for the user to specify which subsets of these keywords expect a proximity in search results. This is useful if the user wants to do a [[prior art]] search (e.g. finding an existing approach to complete a specific task, finding a document that discloses a system that exhibits a procedural behavior collaboratively conducted by several components and links between these components).

[[Web search engine]]s which support proximity search via an explicit proximity operator in their query language include  [[Walhello]], [[Exalead]], [[Yandex]], [[Yahoo!]], [[Altavista]], and [[Bing (search engine)|Bing]]:
* When using the [[Walhello]] search-engine, the proximity can be defined by the number of characters between the keywords.<ref>[http://www.walhello.com/aboutgl.html "About Walhello"], visited 23 December 2009</ref>
* The search engine Exalead allows the user to specify the required proximity, as the maximum number of words between keywords. The syntax is <tt>(keyword1 NEAR/n keyword2)</tt> where n is the number of words.<ref>[http://www.exalead.com/search/web/search-syntax/#proximity_search "Web Search Syntax"], visited 23 December 2009</ref>
* [[Yandex]] uses the syntax <tt>keyword1 /n keyword2</tt> to search for two keywords separated by at most <math>n - 1</math> words, and supports a few other variations of this syntax.<ref>[http://help.yandex.ru/search/?id=481939 Yandex help page on query language] (in Russian)</ref>
* [[Yahoo!]] and [[Altavista]] both support an undocumented NEAR operator.<ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+additional "Successful Yahoo! proximity query"] (22 Feb 2010)</ref><ref>[http://search.yahoo.com/search?p=site%3Awww.rfc-editor.org+inurl%3Arfc2606+guidance+NEAR+unused "Unsuccessful Yahoo! proximity query"] (22 Feb 2010)</ref> The syntax is <tt>keyword1 NEAR keyword2</tt>.
* [[Google Search]] supports AROUND(#).<ref>[http://www.guidingtech.com/16116/google-search-little-known-around-operator/ "GuidingTech: Meet Google Search's Little Known AROUND Operator"]</ref><ref>[http://www.netforlawyers.com/content/google-offers-proximity-search-around-connector-0015/ "Google Offers Proximity Search"] (8 Feb 2011)</ref>
* [[Bing (search engine)|Bing]] supports NEAR.<ref>[http://www.howtogeek.com/106751/how-to-use-bings-advanced-search-operators-8-tips-for-better-searches/ "How to Use Bing’s Advanced Search Operators"]</ref> The syntax is <tt>keyword1 near:n keyword2</tt> where n=the number of maximum separating words.

Ordered search within the [[Google]] and [[Yahoo!]] search engines is possible using the asterisk (*) full-word [[Wildcard character|wildcard]]s: in Google this matches one or more words,<ref>[http://www.google.com/support/websearch/bin/answer.py?answer=136861 "More Google Search Help" visited 23 December 2009]</ref> and an in Yahoo! Search this matches exactly one word.<ref>[http://www.searchengineshowdown.com/features/yahoo/review.html "Review of Yahoo! Search", by Search Engine Showdown, visited 23 December 2009]</ref>  (This is easily verified by searching for the following phrase in both Google and Yahoo!: "addictive * of biblioscopy".)

To emulate unordered search of the NEAR operator can be done using a combination of ordered searches.  For example, to specify a close co-occurrence of "house" and "dog", the following search-expression could be specified: "house dog" OR "dog house" OR "house * dog" OR "dog * house" OR "house * * dog" OR "dog * * house".

== See also ==
* [[Compound term processing]]
* [[Edit distance]]
* [[Information retrieval]]
* [[Search engine]]
* [[Search engine indexing]] - how texts are indexed to support proximity search
* [[Semantic proximity]]

== Notes ==
{{Reflist}}

[[Category:Information retrieval techniques]]
[[Category:Internet search algorithms]]
<=====doc_Id=====>:665
<=====title=====>:
Cluster labeling
<=====text=====>:
In [[natural language processing]] and [[information retrieval]], '''cluster labeling''' is the problem of picking descriptive, human-readable labels for the clusters produced by a [[document clustering]] algorithm; standard clustering algorithms do not typically produce any such labels. Cluster labeling algorithms examine the contents of the documents per cluster to find a labeling that summarize the topic of each cluster and distinguish the clusters from each other.

==Differential cluster labeling==
Differential cluster labeling labels a cluster by comparing term [[probability distribution|distributions]] across clusters, using techniques also used for [[feature selection]] in [[document classification]], such as [[mutual information]] and [[Pearson's chi-squared test|chi-squared feature selection]].  Terms having very low frequency are not the best in representing the whole cluster and can be omitted in labeling a cluster.  By omitting those rare terms and using a differential test, one can achieve the best results with differential cluster labeling.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Cluster Labeling''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/cluster-labeling-1.html>.</ref>

===Pointwise mutual information===

{{Main article|Pointwise mutual information}}

In the fields of [[probability theory]] and [[information theory]], mutual information measures the degree of dependence of two [[random variables]].  The mutual information of two variables {{mvar|X}} and {{mvar|Y}} is defined as:

<math>I(X, Y) = \sum_{x\in X}{ \sum_{y\in Y} {p(x, y)log_2\left(\frac{p(x, y)}{p_1(x)p_2(y)}\right)}}</math>

where ''p(x, y)'' is the [[joint probability|joint probability distribution]] of the two variables, ''p<sub>1</sub>(x)'' is the probability distribution of X, and ''p<sub>2</sub>(y)'' is the probability distribution of Y.

In the case of cluster labeling, the variable X is associated with membership in a cluster, and the variable Y is associated with the presence of a term.<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Mutual Information''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html>.</ref>  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

<math>I(C, T) = \sum_{c\in {0, 1}}{ \sum_{t\in {0, 1}} {p(C = c, T = t)log_2\left(\frac{p(C = c, T = t)}{p(C = c)p(T = t)}\right)}}</math>

In this case, ''p(C = 1)'' represents the probability that a randomly selected document is a member of a particular cluster, and ''p(C = 0)'' represents the probability that it isn't.  Similarly, ''p(T = 1)'' represents the probability that a randomly selected document contains a given term, and ''p(T = 0)'' represents the probability that it doesn't.  The [[joint probability|joint probability distribution function]] ''p(C, T)'' represents the probability that two events occur simultaneously.  For example, ''p(0, 0)'' is the probability that a document isn't a member of cluster ''c'' and doesn't contain term ''t''; ''p(0, 1)'' is the probability that a document isn't a member of cluster ''c'' and does contain term ''t''; and so on.

===Chi-Squared Selection===
{{Main article|Pearson's chi-squared test}}
The Pearson's chi-squared test can be used to calculate the probability that the occurrence of an event matches the initial expectations.  In particular, it can be used to determine whether two events, A and B, are [[statistically independent]].  The value of the chi-squared statistic is:

<math>X^2 = \sum_{a \in A}{\sum_{b \in B}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>

where ''O<sub>a,b</sub>'' is the ''observed'' frequency of a and b co-occurring, and ''E<sub>a,b</sub>'' is the ''expected'' frequency of co-occurrence.

In the case of cluster labeling, the variable A is associated with membership in a cluster, and the variable B is associated with the presence of a term.  Both variables can have values of 0 or 1, so the equation can be rewritten as follows:

<math>X^2 = \sum_{a \in {0,1}}{\sum_{b \in {0,1}}{\frac{(O_{a,b} - E_{a, b})^2}{E_{a, b}}}}</math>

For example, ''O<sub>1,0</sub>'' is the observed number of documents that are in a particular cluster but don't contain a certain term, and ''E<sub>1,0</sub>'' is the expected number of documents that are in a particular cluster but don't contain a certain term.
Our initial assumption is that the two events are independent, so the expected probabilities of co-occurrence can be calculated by multiplying individual probabilities:<ref>Manning, Christopher D., Prabhakar Raghavan, and Hinrich Schutze. ''Introduction to Information Retrieval''. Cambridge: Cambridge UP, 2008. ''Chi2 Feature Selection''. Stanford Natural Language Processing Group. Web. 25 Nov. 2009. <http://nlp.stanford.edu/IR-book/html/htmledition/feature-selectionchi2-feature-selection-1.html>.</ref>

''E<sub>1,0</sub> = N * P(C = 1) * P(T = 0)''

where N is the total number of documents in the collection.

==Cluster-Internal Labeling==
Cluster-internal labeling selects labels that only depend on the contents of the cluster of interest. No comparison is made with the other clusters.
Cluster-internal labeling can use a variety of methods, such as finding terms that occur frequently in the centroid or finding the document that lies closest to the centroid.

===Centroid Labels===
{{Main article|Vector space model}}
A frequently used model in the field of [[information retrieval]] is the vector space model, which represents documents as vectors.  The entries in the vector correspond to terms in the [[vocabulary]]. Binary vectors have a value of 1 if the term is present within a particular document and 0 if it is absent. Many vectors make use of weights that reflect the importance of a term in a document, and/or the importance of the term in a document collection. For a particular cluster of documents, we can calculate the [[centroid]] by finding the [[arithmetic mean]] of all the document vectors.  If an entry in the centroid vector has a high value, then the corresponding term occurs frequently within the cluster.  These terms can be used as a label for the cluster.
One downside to using centroid labeling is that it can pick up words like "place" and "word" that have a high frequency in written text, but have little relevance to the contents of the particular cluster.

===Contextualized centroid labels===
A simple, cost-effective way of overcoming the above limitation is to embed the centroid terms with the highest weight in a graph structure that provides a context for their interpretation and selection.<ref>Francois Role, Moahmed Nadif. [http://dl.acm.org/citation.cfm?id=2574675 Beyond cluster labeling: Semantic interpretation of clusters’ contents using a graph representation.] Knowledge-Based Systems, Volume 56, January, 2014: 141-155</ref>
In this approach, a term-term co-occurrence matrix referred as <math>T_k</math> is first built for each cluster <math>S_k</math>. Each cell represents the number of times term <math>i</math> co-occurs with term <math>j</math> within a certain window of text (a sentence, a paragraph, etc.)
In a second stage, a similarity matrix <math>T_k^{sim}</math> is obtained by multiplying <math>T_k</math> with its transpose. We have <math>T_k^{sim}=T_k' T_k=(t_{{sim}_{ij}})</math>. Being the dot product of two normalized vectors <math>\tilde{t}_{i}</math> and <math>\tilde{t}_{j}</math>, <math>t_{{sim}_{ij}}</math> denotes the cosine similarity between terms <math>i</math> and <math>j</math>. The so obtained <math>T_k^{sim}</math> can then be used as the weighted adjacency matrix of a term similarity graph. The centroid terms are part of this graph, and they thus can be interpreted and scored by inspecting the terms that surround them in the graph.

===Title labels===
An alternative to centroid labeling is title labeling.  Here, we find the document within the cluster that has the smallest [[Euclidean distance]] to the centroid, and use its title as a label for the cluster.  One advantage to using document titles is that they provide additional information that would not be present in a list of terms.  However, they also have the potential to mislead the user, since one document might not be representative of the entire cluster.

===External knowledge labels===
Cluster labeling can be done indirectly using external knowledge such as pre-categorized knowledge such as the one of Wikipedia.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> In such methods, a set of important cluster text features are first extracted from the cluster documents. These features then can be used to retrieve the (weighted) K-nearest categorized documents from which candidates for cluster labels can be extracted. The final step involves the ranking of such candidates. Suitable methods are such that are based on a voting or a fusion process which is determined using the set of categorized documents and the original cluster features.

=== Combining Several Cluster Labelers ===
The cluster labels of several different cluster labelers can be further combined to obtain better labels. 
For example, [[Linear Regression]] can be used to learn an optimal combination of labeler scores.<ref>David Carmel, Haggai Roitman, Naama Zwerdling. [http://portal.acm.org/citation.cfm?doid=1571941.1571967 Enhancing cluster labeling using wikipedia.] SIGIR 2009: 139-146</ref> A more sophisticated technique is based on a [[wikt:fusion|fusion]] approach and analysis of the cluster labels decision stability of various labelers.<ref>Haggai Roitman, Shay Hummel, Michal Shmueli-Scheuer. [http://dl.acm.org/citation.cfm?id=2609465 A fusion approach to cluster labeling.] SIGIR 2014: 883-886</ref>

==External links==
* [http://nlp.stanford.edu/IR-book/html/htmledition/hierarchical-clustering-1.html Hierarchical Clustering]
* [http://www.cs.cmu.edu/~callan/Papers/dgo06-puck.pdf Automatically Labeling Hierarchical Clusters]

==References==
<references/>

{{DEFAULTSORT:Cluster Labeling}}
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:668
<=====title=====>:
Policy framework
<=====text=====>:
{{refimprove|date=March 2009}}
{{globalize|date=June 2015}}
A '''policy framework''' is a logical structure that is established to organize policy documentation into groupings and categories that make it easier for employees to find and understand the contents of various [[policy]] documents. Policy frameworks can also be used to help in the planning and development of the policies for an organization.

==Principles==
[[State Services Commission]] of [[New Zealand]] outlines eleven principles of policy framework as below.<ref>http://www.ssc.govt.nz/Documents/policy_framework_for_Government_.htm</ref>

===Availability===
Government departments should make information available easily, widely and equitably to the people of New Zealand (except where reasons preclude such availability as specified in legislation).....

===Coverage===
Government departments should make the following information increasingly available on an electronic basis:
* all published material or material already in the public domain
* all policies that could be released publicly
* all information created or collected on a statutory basis (subject to commercial sensitivity and privacy considerations)
* all documents that the public may be required to complete
* corporate documentation in which the public would be interested

===Pricing=== 
a) Free dissemination of Government-held information is appropriate where:
* dissemination to a target audience is desirable for a public policy purpose, or
* a charge to recover the cost of dissemination is not feasible or cost-effective

b) Pricing to recover the cost of dissemination is appropriate where:
* there is no particular public policy reason to disseminate the information, and 
* a charge to recover the cost of dissemination is both feasible and cost effective

c) Pricing to recover the cost of transformation is appropriate where:
* pricing to recover the cost of dissemination is appropriate, and
* there is an avoidable cost involved in transforming the information from the form in which it is held into a form preferred by the recipient, where it is feasible and cost-effective to recover in addition to the cost of dissemination

d) Pricing to recover the full costs of information production and dissemination is appropriate where:
* the information is created for the commercial purpose of sale at a profit, and 
* to do so would not breach the other pricing principles

===Ownership===
Government-held information, created or collected by any person employed or engaged by the Crown is a strategic resource 'owned' by the Government as a steward on behalf of the public.

===Stewardship===
Government departments are stewards of Government-held information, and it is their responsibility to implement good information management.

===Collection===
Government departments should only collect information for specified public policy, operational business or legislative purposes.

===Copyright===
Information created by departments is subject to Crown copyright but where wide dissemination is desirable, the Crown should permit use of its copyrights subject to acknowledgement of source.
 
===Preservation===
Government-held information should be preserved only where a public business need, legislative or policy requirement, or a historical or archival reason, exists.

===Quality===
The key qualities underpinning Government-held information include accuracy, relevancy, timeliness, consistency and collection without bias so that the information supports the purposes for which it is collected.

===Integrity===
The integrity of Government-held information will be achieved when:
* all guarantees and conditions surrounding the information are met
* the principles are clear and communicated
* any situation relating to Government-held information is handled openly and consistently
* those affected by changes to Government-held information are consulted on those changes
* those charged as independent guardians of the public interest  (e.g. the Ombudsman) have confidence in the ability of departments to manage the information well
* there are minimum exceptions to the principles.

===Privacy===
The principles of the Privacy Act 1993 apply.

==References==
{{reflist}}

{{DEFAULTSORT:Policy Framework}}
[[Category:Information retrieval techniques]]
[[Category:Government of New Zealand]]
<=====doc_Id=====>:671
<=====title=====>:
Subject indexing
<=====text=====>:
'''Subject indexing''' is the act of describing or [[document classification|classifying]] a [[document]] by [[index term]]s or other symbols in order to indicate what the document is '''[[aboutness|about]],''' to summarize its [[content (media and publishing)|content]] or to increase its [[findability]]. In other words, it is about identifying and describing the '''[[Subject (documents)|subject]]''' of documents. Indexes are constructed, separately, on three distinct levels: terms in a document such as a book; objects in a collection such as a library; and documents (such as books and articles) within a field of knowledge.

Subject indexing is used in [[information retrieval]] especially to create [[bibliographic index]]es to retrieve documents on a particular subject. Examples of academic indexing services are [[Zentralblatt MATH]], [[Chemical Abstracts]] and [[PubMed]]. The index terms were mostly assigned by experts but author keywords are also common.

The process of indexing begins with any analysis of the subject of the document. The indexer must then identify terms which appropriately identify the subject either by extracting words directly from the document or assigning words from a [[controlled vocabulary]].<ref name="Lancaster2003a">F. W. Lancaster (2003): "Indexing and abstracting in theory and practise". Third edition. London, Facet ISBN 1-85604-482-3. page 6</ref> The terms in the index are then presented in a systematic order.

Indexers must decide how many terms to include and how specific the terms should be. Together this gives a depth of indexing.

== Subject analysis ==
The first step in indexing is to decide on the subject matter of the document. In manual indexing, the indexer would consider the subject matter in terms of answer to a set of questions such as "Does the document deal with a specific product, condition or phenomenon?".<ref name="Chowdhury2004">G.G. Chowdhury (2004): "Introduction to modern information retrieval". Third Edition. London, Facet. ISBN 1-85604-480-7. page 71</ref> As the analysis is influenced by the knowledge and experience of the indexer, it follows that two indexers may analyze the content differently and so come up with different index terms. This will impact on the success of retrieval.

=== Automatic vs. manual subject analysis ===
Automatic indexing follows set processes of analyzing frequencies of word patterns and comparing results to other documents in order to assign to subject categories. This requires no understanding of the material being indexed. This therefore leads to more uniform indexing but this is at the expense of the true meaning being interpreted. A computer program will not understand the meaning of statements and may therefore fail to assign some relevant terms or assign incorrectly. Human indexers focus their attention on certain parts of the document such as the title, abstract, summary and conclusions, as analyzing the full text in depth is costly and time consuming <ref name="Lancaster2003b">F. W. Lancaster (2003): "Indexing and abstracting in theory and practice". Third edition. London, Facet ISBN 1-85604-482-3. page 24</ref> An automated system takes away the time limit and allows the entire document to be analyzed, but also has the option to be directed to particular parts of the document.

== Term selection ==
The second stage of indexing involves the translation of the subject analysis into a set of [[keyword (search)|index terms]]. This can involve extracting from the document or assigning from a [[controlled vocabulary]]. With the ability to conduct a [[full text search]] widely available, many people have come to rely on their own expertise in conducting information searches and [[full text search]] has become very popular. Subject indexing and its experts, professional indexers, [[catalogers]], and [[librarians]], remains crucial to information organization and retrieval. These experts understand [[controlled vocabularies]] and are able to find information that cannot be located by [[full text search]]. The cost of expert analysis to create subject indexing is not easily compared to the cost of hardware, software and labor to manufacture a comparable set of full-text, fully searchable materials. With new web applications that allow every user to annotate documents, [[social tagging]] has gained popularity especially in the Web.<ref name="Voss2007">{{cite conference
|last1=Voss |first1=Jakob
|title=Tagging, Folksonomy & Co - Renaissance of Manual Indexing?
|booktitle=Proceedings of the International Symposium of Information Science
|pages=234–254
|year=2007
|arxiv=cs/0701072
}}</ref>

One application of indexing, the [[Index (publishing)|book index]], remains relatively unchanged despite the information revolution.

=== Extraction/Derived indexing ===
Extraction indexing involves taking words directly from the document. It uses [[natural language]] and lends itself well to automated techniques where word frequencies are calculated and those with a frequency over a pre-determined threshold are used as index terms. A stop-list containing common words (such as "the", "and") would be referred to and such [[stop words]] would be excluded as index terms.

Automated extraction indexing may lead to loss of meaning of terms by indexing single words as opposed to phrases. Although it is possible to extract commonly occurring phrases, it becomes more difficult if key concepts are inconsistently worded in phrases. Automated extraction indexing also has the problem that, even with use of a stop-list to remove common words, some frequent words may not be useful for allowing discrimination between documents. For example, the term glucose is likely to occur frequently in any document related to diabetes. Therefore use of this term would likely return most or all the documents in the database. Post-co-ordinated indexing where terms are combined at the time of searching would reduce this effect but the onus would be on the searcher to link appropriate terms as opposed to the information professional. In addition terms that occur infrequently may be highly significant for example a new drug may be mentioned infrequently but the novelty of the subject makes any reference significant. One method for allowing rarer terms to be included and common words to be excluded by automated techniques would be a relative frequency approach where frequency of a word in a document is compared to frequency in the database as a whole. Therefore a term that occurs more often in a document than might be expected based on the rest of the database could then be used as an index term, and terms that occur equally frequently throughout will be excluded.
Another problem with automated extraction is that it does not recognise when a concept is discussed but is not identified in the text by an indexable keyword.<ref name="Lamb2008">J. Lamb (2008): ''[http://www.indexers.org.uk/index.php?id=463 Human or computer produced indexes?]'' [online] Sheffield, Society of Indexers. Accessed 15 January 2009.</ref>

=== Assignment indexing ===
An alternative is assignment indexing where index terms are taken from a controlled vocabulary. This has the advantage of controlling for [[synonym]]s as the preferred term is indexed and synonyms or related terms direct the user to the preferred term. This means the user can find articles regardless of the specific term used by the author and saves the user from having to know and check all possible synonyms.<ref name="Tenopir">C. Tenopir (1999): "Human or automated, indexing is important". ''Library Journal'' '''124'''(18) pages 34-38.</ref> It also removes any confusion caused by [[homograph]]s by inclusion of a qualifying term. A third advantage is that it allows the linking of related terms whether they are linked by hierarchy or association, e.g. an index entry for an oral medication may list other oral medications as related terms on the same level of the hierarchy but would also link to broader terms such as treatment. Assignment indexing is used in manual indexing to improve inter-indexer consistency as different indexers will have a controlled set of terms to choose from. Controlled vocabularies do not completely remove inconsistencies as two indexers may still interpret the subject differently.<ref name="Chowdhury2004" />

== Index presentation ==
The final phase of indexing is to present the entries in a systematic order. This may involve linking entries. In a pre-coordinated index the indexer determines the order in which terms are linked in an entry by considering how a user may formulate their search. In a post-coordinated index, the entries are presented singly and the user can link the entries through searches, most commonly carried out by computer software. Post-coordination results in a loss of precision in comparison to pre-coordination <ref name="Bodoff1998">D. Bodoff and A. Kambil, (1998): "Partial coordination. I. The best of pre-coordination and post-coordination." ''Journal of the American Society for Information Science'', '''49'''(14), 1254-1269.</ref>

== Depth of Indexing ==
Indexers must make decisions about what entries should be included and how many entries an index should incorporate. The depth of indexing describes the thoroughness of the indexing process with reference to exhaustivity and specificity <ref name="Cleveland2001">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 105</ref>

=== Exhaustivity ===
An exhaustive index is one which lists all possible index terms. Greater exhaustivity gives a higher [[Recall (information retrieval)|recall]], or more likelihood of all the relevant articles being retrieved, however, this occurs at the expense of [[Precision (information retrieval)|precision]]. This means that the user may retrieve a larger number of irrelevant documents or documents which only deal with the subject in little depth. In a manual system a greater level of exhaustivity brings with it a greater cost as more man hours are required. The additional time taken in an automated system would be much less significant. At the other end of the scale, in a selective index only the most important aspects are covered.<ref name="Weinberg1999">B.H. Weinberg (1990): "Exhaustivity of indexes: Books, journals, and electronic full texts; Summary of a workshop presented at the 1999 ASI Annual Conference". ''Key Words'', '''7'''(5), pages 1+.</ref> Recall is reduced in a selective index as if an indexer does not include enough terms, a highly relevant article may be overlooked. Therefore indexers should strive for a balance and consider what the document may be used. They may also have to consider the implications of time and expense.

=== Specificity ===
The specificity describes how closely the index terms match the topics they represent <ref name="Anderson1997">J.D. Anderson (1997): ''[http://www.niso.org/publications/tr/ Guidelines for indexes and related information retrieval devices]'' [online]. Bethesda, Maryland, Niso Press. 10 December 2008.</ref> An index is said to be specific if the indexer uses parallel descriptors to the concept of the document and reflects the concepts precisely.<ref name="Cleveland2001b">D.B. Cleveland and A.D. Cleveland (2001): "Introduction to indexing and abstracting". 3rd Ed. Englewood, libraries Unlimited, Inc. ISBN 1-56308-641-7. page 106</ref> Specificity tends to increase with exhaustivity as the more terms you include, the narrower those terms will be.

==Indexing theory==
[[Birger Hjørland|Hjørland]] (2011)<ref>Hjørland, Birger (2011). The Importance of Theories of Knowledge: Indexing and Information retrieval as an example. ''Journal of the American Society for Information Science and Technology'', 62(1,), 72-77.</ref> found that theories of indexing is at the deepest level connected to different theories of knowledge:

'''Rationalist theories of indexing''' (such as Ranganathan's theory) suggest that subjects are constructed logically from a fundamental set of categories. The basic method of subject analysis is then "analytic-synthetic", to isolate a set of basic categories (=analysis) and then to construct the subject of any given document by combining those categories according to some rules (=synthesis). '''Empiricist theories of indexing''' are based on selecting similar documents based on their properties, in particular by applying numerical statistical techniques. '''Historicist and hermeneutical theories of indexing''' suggest that the subject of a given document is relative to a given discourse or domain, why the indexing should reflect the need of a particular discourse or domain. According to hermeneutics is a document always written and interpreted from particular horizon. The same is the case with systems of knowledge organization and with all users searching such systems. Any question put to such a system is put from a particular horizon. All those horizons may be more or less in consensus or in conflict. To index a document is to try to contribute to the retrieval of “relevant” documents by knowing about those different horizons. '''Pragmatic and critical theories of indexing''' (such as Hjørland, 1997)<ref>Hjørland, B. (1997). Information Seeking and Subject Representation. An Activity-theoretical approach to Information Science. Westport & London: Greenwood Press.</ref> is in agreement with the historicist point of view that subjects are relative to specific discourses but emphasizes that subject analysis should support given goals and values and should consider the consequences of indexing one way or another. These theories believe that indexing cannot be neutral and that it is a wrong goal to try to index in a neutral way. Indexing is an act (and computer based indexing is acting according to the programmers intentions). Acts serve human goals. Libraries and information services also serve human goals, why their indexing should be done in a way that supports these goals as much as possible. At a first glance this looks strange because the goals of libraries and information services is to identify any document or piece of information. Nonetheless is any specific way of indexing always supporting some kind of uses at the expense of other. The documents to be indexed intend to serve some specific purposes in a community. Basically the indexing should intend serving the same purposes. Primary and secondary documents and information services are parts of the same overall social system. In such a system different theories, epistemologies, worldviews etc. may be at play and users need to be able to orient themselves and to navigate among those different views. This calls for a mapping of the different epistemologies in the field and classification of the single document into such a map. Excellent examples of such different paradigms and their consequences for indexing and classification systems are provided in the domain of art by Ørom (2003)<ref>Ørom, Anders (2003). Knowledge Organization in the domain of Art Studies - History, Transition and Conceptual Changes. Knowledge Organization. 30(3/4), 128-143.</ref> and in music by Abrahamsen (2003).<ref>Abrahamsen, Knut T. (2003). Indexing of Musical Genres. An Epistemological Perspective. Knowledge Organization, 30(3/4), 144-169.</ref>

The core of indexing is, as stated by Rowley & Farrow<ref name=rowley2000>Rowley, J. E. & Farrow, J. (2000). Organizing Knowledge: An Introduction to Managing Access to Information. 3rd. Alderstot: Gower Publishing Company</ref> to evaluate a papers contribution to knowledge and index it accordingly. Or, with the words of Hjørland (1992,<ref>Hjørland, Birger (1992). The Concept of "Subject" in Information Science. Journal of Documentation. 48(2), 172-200. http://iva.dk/bh/Core%20Concepts%20in%20LIS/1992JDOC%5FSubject.PDF</ref> 1997) to index its informative potentials.

"In order to achieve good consistent indexing, the indexer must have a thorough appreciation of the structure of the subject and the nature of the contribution that the document is making to the advancement of knowledge." (Rowley & Farrow, 2000,<ref name=rowley2000/> p.&nbsp;99).

== See also ==
{{Commons category|Subject indexing}}
* [[Indexing and abstracting service]]
* [[Document classification]]
* [[Metadata]]
* [[Overcategorization]]
* [[Thomas of Ireland]], a medieval pioneer in subject indexing

== References ==
{{reflist}}
* {{cite book|author=Fugman, Robert|year=1993|title=Subject analysis and indexing. Theoretical foundation and practical advice|place=Frankfurt/Main|publisher=Index Verlag}}
* {{cite journal|author=Frohmann, B.|year=1990|title=Rules of Indexing: A Critique of [[Mentalism]] in Information Retrieval Theory|journal=Journal of Documentation|volume=46|issue=2|pages=81–101|doi=10.1108/eb026855}}

[[Category:Index (publishing)]]
[[Category:Information science]]
[[Category:Information retrieval techniques]]
<=====doc_Id=====>:674
<=====title=====>:
Latent semantic analysis
<=====text=====>:
{{semantics}}
'''Latent semantic analysis''' ('''LSA''') is a technique in [[natural language processing]], in particular [[distributional semantics]], of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text.  A matrix containing word counts per paragraph (rows represent unique words and columns represent each paragraph) is constructed from a large piece of text and a mathematical technique called [[singular value decomposition]] (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Words are then compared by taking the cosine of the angle between the two vectors (or the [[dot product]] between the [[Unit vector|normalizations]] of the two vectors) formed by any two rows.  Values close to 1 represent very similar words while values close to 0 represent very dissimilar words.<ref>{{cite journal | title=Latent Semantic Analysis | author=Susan T. Dumais |year=2005 | doi=10.1002/aris.1440380105 | journal=Annual Review of Information Science and Technology | volume=38 | pages=188–230}}</ref>

An information retrieval technique using latent semantic structure was patented in 1988 ([http://patft.uspto.gov/netacgi/nph-Parser?patentnumber=4839853 US Patent 4,839,853], now expired) by [[Scott Deerwester]], [[Susan Dumais]], [[George Furnas]], [[Richard Harshman]], [[Thomas Landauer]], [[Karen Lochbaum]] and [[Lynn Streeter]]. In the context of its application to [[information retrieval]], it is sometimes called [[Latent semantic indexing|Latent Semantic Indexing '''(LSI)''']].<ref>{{cite web | url=http://lsa.colorado.edu/ | title=The Latent Semantic Indexing home page}}</ref>

== Overview ==

=== Occurrence matrix ===
LSA can use a [[term-document matrix]] which describes the occurrences of terms in documents; it is a [[sparse matrix]] whose rows correspond to [[terminology|terms]] and whose columns correspond to documents. A typical example of the weighting of the elements of the matrix is [[tf-idf]] (term frequency–inverse document frequency): the weight of an element of the matrix is proportional to the number of times the terms appear in each document, where rare terms are upweighted to reflect their relative importance.

This matrix is also common to standard semantic models, though it is not necessarily explicitly expressed as a matrix, since the mathematical properties of matrices are not always used.

=== Rank lowering ===
After the construction of the occurrence matrix, LSA finds a [[low-rank approximation]]<ref>Markovsky I. (2012) Low-Rank Approximation: Algorithms, Implementation, Applications, Springer, 2012, ISBN 978-1-4471-2226-5 {{page needed|date=January 2012}}</ref> to the [[term-document matrix]]. There could be various reasons for these approximations:

* The original term-document matrix is presumed too large for the computing resources; in this case, the approximated low rank  matrix is interpreted as an ''approximation'' (a "least and necessary evil").
* The original term-document matrix is presumed ''noisy'': for example, anecdotal instances of terms are to be eliminated. From this point of view, the approximated matrix is interpreted as a ''de-noisified matrix'' (a better matrix than the original).
* The original term-document matrix is presumed overly [[Sparse matrix|sparse]] relative to the "true" term-document matrix.  That is, the original matrix lists only the words actually ''in'' each document, whereas we might be interested in all words ''related to'' each document—generally a much larger set due to [[synonymy]].

The consequence of the rank lowering is that some dimensions are combined and depend on more than one term:

:: {(car), (truck), (flower)} -->  {(1.3452 * car + 0.2828 * truck), (flower)}

This mitigates the problem of identifying synonymy, as the rank lowering is expected to merge the dimensions associated with terms that have similar meanings. It also mitigates the problem with [[polysemy]], since components of polysemous words that point in the "right" direction are added to the components of words that share a similar meaning. Conversely, components that point in other directions tend to either simply cancel out, or, at worst, to be smaller than components in the directions corresponding to the intended sense.

=== Derivation ===
Let <math>X</math> be a matrix where element <math>(i,j)</math> describes the occurrence of term <math>i</math> in document <math>j</math> (this can be, for example, the frequency). <math>X</math> will look like this:

:<math>
\begin{matrix} 
 & \textbf{d}_j \\
 & \downarrow \\
\textbf{t}_i^T \rightarrow &
\begin{bmatrix} 
x_{1,1} & \dots & x_{1,n} \\
\vdots & \ddots & \vdots \\
x_{m,1} & \dots & x_{m,n} \\
\end{bmatrix}
\end{matrix}
</math>

Now a row in this matrix will be a vector corresponding to a term, giving its relation to each document:

:<math>\textbf{t}_i^T = \begin{bmatrix} x_{i,1} & \dots & x_{i,n} \end{bmatrix}</math>

Likewise, a column in this matrix will be a vector corresponding to a document, giving its relation to each term:

:<math>\textbf{d}_j = \begin{bmatrix} x_{1,j} \\ \vdots \\ x_{m,j} \end{bmatrix}</math>

Now the [[dot product]] <math>\textbf{t}_i^T \textbf{t}_p</math> between two term vectors gives the [[correlation]] between the terms over the set of documents. The [[matrix product]] <math>X X^T</math> contains all these dot products. Element <math>(i,p)</math> (which is equal to element <math>(p,i)</math>) contains the dot product <math>\textbf{t}_i^T \textbf{t}_p</math> (<math> = \textbf{t}_p^T \textbf{t}_i</math>). Likewise, the matrix <math>X^T X</math> contains the dot products between all the document vectors, giving their correlation over the terms: <math>\textbf{d}_j^T \textbf{d}_q = \textbf{d}_q^T \textbf{d}_j</math>.

Now, from the theory of linear algebra, there exists a decomposition of <math>X</math> such that <math>U</math> and <math>V</math> are [[orthogonal matrix|orthogonal matrices]] and <math>\Sigma</math> is a [[diagonal matrix]]. This is called a [[singular value decomposition]] (SVD):

:<math>
\begin{matrix}
X = U \Sigma V^T
\end{matrix}
</math>

The matrix products giving us the term and document correlations then become

:<math>
\begin{matrix}
X X^T &=& (U \Sigma V^T) (U \Sigma V^T)^T = (U \Sigma V^T) (V^{T^T} \Sigma^T U^T) = U \Sigma V^T V \Sigma^T U^T = U \Sigma \Sigma^T U^T = U \Sigma^2 U^T \\
X^T X &=& (U \Sigma V^T)^T (U \Sigma V^T) = (V^{T^T} \Sigma^T U^T) (U \Sigma V^T) = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T = V \Sigma^2 V^T
\end{matrix}
</math>

Since <math>\Sigma \Sigma^T</math> and <math>\Sigma^T \Sigma</math> are diagonal we see that <math>U</math> must contain the [[eigenvector]]s of <math>X X^T</math>, while <math>V</math> must be the eigenvectors of <math>X^T X</math>. Both products have the same non-zero eigenvalues, given by the non-zero entries of <math>\Sigma \Sigma^T</math>, or equally, by the non-zero entries of <math>\Sigma^T\Sigma</math>. Now the decomposition looks like this:

:<math>
\begin{matrix} 
 & X & & & U & & \Sigma & & V^T \\
 & (\textbf{d}_j) & & & & & & & (\hat{\textbf{d}}_j) \\
 & \downarrow & & & & & & & \downarrow \\
(\textbf{t}_i^T) \rightarrow 
&
\begin{bmatrix} 
x_{1,1} & \dots & x_{1,n} \\
\\
\vdots & \ddots & \vdots \\
\\
x_{m,1} & \dots & x_{m,n} \\
\end{bmatrix}
&
=
&
(\hat{\textbf{t}}_i^T) \rightarrow
&
\begin{bmatrix} 
\begin{bmatrix} \, \\ \, \\ \textbf{u}_1 \\ \, \\ \,\end{bmatrix} 
\dots
\begin{bmatrix} \, \\ \, \\ \textbf{u}_l \\ \, \\ \, \end{bmatrix}
\end{bmatrix}
&
\cdot
&
\begin{bmatrix} 
\sigma_1 & \dots & 0 \\
\vdots & \ddots & \vdots \\
0 & \dots & \sigma_l \\
\end{bmatrix}
&
\cdot
&
\begin{bmatrix} 
\begin{bmatrix} & & \textbf{v}_1 & & \end{bmatrix} \\
\vdots \\
\begin{bmatrix} & & \textbf{v}_l & & \end{bmatrix}
\end{bmatrix}
\end{matrix}
</math>

The values <math>\sigma_1, \dots, \sigma_l</math> are called the singular values, and <math>u_1, \dots, u_l</math> and <math>v_1, \dots, v_l</math> the left and right singular vectors.
Notice the only part of <math>U</math> that contributes to <math>\textbf{t}_i</math> is the <math>i\textrm{'th}</math> row.
Let this row vector be called <math>\hat{\textrm{t}}^T_i</math>.
Likewise, the only part of <math>V^T</math> that contributes to <math>\textbf{d}_j</math> is the <math>j\textrm{'th}</math> column, <math>\hat{ \textrm{d}}_j</math>.
These are ''not'' the eigenvectors, but ''depend'' on ''all'' the eigenvectors.

It turns out that when you select the <math>k</math> largest singular values, and their corresponding singular vectors from <math>U</math> and <math>V</math>, you get the rank <math>k</math> approximation to <math>X</math> with the smallest error ([[Frobenius norm]]). This approximation has a minimal error.  But more importantly we can now treat the term and document vectors as a "semantic space". The row "term" vector <math>\hat{\textbf{t}}^T_i</math> then has <math>k</math> entries mapping it to a lower-dimensional space dimensions. These new dimensions do not relate to any comprehensible concepts. They are a lower-dimensional approximation of the higher-dimensional space. Likewise, the "document" vector <math>\hat{\textbf{d}}_j</math> is an approximation in this lower-dimensional space. We write this approximation as

:<math>X_k = U_k \Sigma_k V_k^T</math>

You can now do the following:
* See how related documents <math>j</math> and <math>q</math> are in the low-dimensional space by comparing the vectors <math>\Sigma_k \hat{\textbf{d}}_j </math> and <math>\Sigma_k \hat{\textbf{d}}_q </math> (typically by [[vector space model|cosine similarity]]).
* Comparing terms <math>i</math> and <math>p</math> by comparing the vectors <math>\Sigma_k \hat{\textbf{t}}_i</math> and <math>\Sigma_k \hat{\textbf{t}}_p</math>. Note that <math>\hat{\textbf{t}}</math> is now a column vector.
* Documents and term vector representations can be clustered using traditional clustering algorithms like k-means using similarity measures like cosine.
* Given a query, view this as a mini document, and compare it to your documents in the low-dimensional space.

To do the latter, you must first translate your query into the low-dimensional space. It is then intuitive that you must use the same transformation that you use on your documents:

:<math>\hat{\textbf{d}}_j = \Sigma_k^{-1} U_k^T \textbf{d}_j</math>

Note here that the inverse of the diagonal matrix <math>\Sigma_k</math> may be found by inverting each nonzero value within the matrix.

This means that if you have a query vector <math>q</math>, you must do the translation <math>\hat{\textbf{q}} = \Sigma_k^{-1} U_k^T \textbf{q}</math> before you compare it with the document vectors in the low-dimensional space. You can do the same for pseudo term vectors:

:<math>\textbf{t}_i^T = \hat{\textbf{t}}_i^T \Sigma_k V_k^T</math>

:<math>\hat{\textbf{t}}_i^T = \textbf{t}_i^T V_k^{-T} \Sigma_k^{-1} = \textbf{t}_i^T V_k \Sigma_k^{-1}</math>

:<math>\hat{\textbf{t}}_i = \Sigma_k^{-1}  V_k^T \textbf{t}_i</math>

== Applications ==

The new low-dimensional space typically can be used to:
* Compare the documents in the low-dimensional space ([[data clustering]], [[document classification]]).
* Find similar documents across languages, after analyzing a base set of translated documents ([[cross language retrieval]]).
* Find relations between terms ([[synonymy]] and [[polysemy]]).
* Given a query of terms, translate it into the low-dimensional space, and find matching documents ([[information retrieval]]).
* Find the best similarity between small groups of terms, in a semantic way (i.e. in a context of a knowledge corpus), as for example in multi choice questions [[Multiple choice question|MCQ]] answering model.<ref name="Alain2009">{{cite journal | url=http://hal.archives-ouvertes.fr/docs/00/38/41/43/PDF/eLSA1-brm20.pdf |format=PDF| title=Effect of tuned parameters on an LSA multiple choice questions answering model |author1=Alain Lifchitz |author2=Sandra Jhean-Larose |author3=Guy Denhière | journal=Behavior Research Methods | volume=41 | issue=4 | pages=1201–1209 | year=2009  | doi=10.3758/BRM.41.4.1201 | pmid=19897829 }}</ref>
* Expand the feature space of machine learning / text mining systems <ref name="Galvez2017">{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. Gálvez |author2=Agustín Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}</ref>

Synonymy and polysemy are fundamental problems in [[natural language processing]]: 
* Synonymy is the phenomenon where different words describe the same idea. Thus, a query in a search engine may fail to retrieve a relevant document that does not contain the words which appeared in the query. For example, a search for "doctors" may not return a document containing the word "[[physicians]]", even though the words have the same meaning.
* Polysemy is the phenomenon where the same word has multiple meanings. So a search may retrieve irrelevant documents containing the desired words in the wrong meaning. For example, a botanist and a computer scientist looking for the word "tree" probably desire different sets of documents.

=== Commercial applications ===

LSA has been used to assist in performing [[prior art]] searches for [[patents]].<ref name="Gerry2007">{{Cite journal | author=Gerry J. Elman | title=Automated Patent Examination Support - A proposal | journal=Biotechnology Law Report | date=October 2007 | doi=10.1089/blr.2007.9896 | volume=26 | issue=5 | pages=435–436 | postscript=<!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. -->{{inconsistent citations}}}}</ref>

=== Applications in human memory ===

The use of Latent Semantic Analysis has been prevalent in the study of human memory, especially in areas of [[free recall]] and memory search.  There is a positive correlation between the semantic similarity of two words (as measured by LSA) and the probability that the words would be recalled one after another in free recall tasks using study lists of random common nouns. They also noted that in these situations, the inter-response time between the similar words was much quicker than between dissimilar words.  These findings are referred to as the [[Semantic Proximity Effect]].<ref>{{cite journal | url=http://psycnet.apa.org/journals/xlm/25/4/923.pdf |format=PDF| title=Contextual Variability and Serial Position Effects in Free Recall |author1=Marc W. Howard |author2=Michael J. Kahana |year=1999}}</ref>

When participants made mistakes in recalling studied items, these mistakes tended to be items that were more semantically related to the desired item and found in a previously studied list.  These prior-list intrusions, as they have come to be called, seem to compete with items on the current list for recall.<ref>{{cite journal | url=https://memory.psych.upenn.edu/files/pubs/ZaroEtal06.pdf |format=PDF| title=Temporal Associations and Prior-List Intrusions in Free Recall | author=Franklin M. Zaromb| booktitle=Interspeech'2005|year=2006|display-authors=etal}}</ref>

Another model, termed [[Word Association Spaces]] (WAS) is also used in memory studies by collecting free association data from a series of experiments and which includes measures of word relatedness for over 72,000 distinct word pairs.<ref>{{cite web|last=Nelson|first=Douglas|title=The University of South Florida Word Association, Rhyme and Word Fragment Norms|url=http://w3.usf.edu/FreeAssociation/Intro.html|accessdate=May 8, 2011}}</ref>

== Implementation ==

The [[Singular Value Decomposition|SVD]] is typically computed using large matrix methods (for example, [[Lanczos method]]s) but may also be computed incrementally and with greatly reduced resources via a [[neural network]]-like approach, which does not require the large, full-rank matrix to be held in memory.<ref name="Genevi2005">{{cite conference | url=http://www.dcs.shef.ac.uk/~genevieve/gorrell_webb.pdf |format=PDF| title=Generalized Hebbian Algorithm for Latent Semantic Analysis |author1=Geneviève Gorrell |author2=Brandyn Webb | booktitle=Interspeech'2005 |year=2005}}</ref>
A fast, incremental, low-memory, large-matrix SVD algorithm has recently been developed.<ref name="brand2006">{{cite journal | url=http://www.merl.com/reports/docs/TR2006-059.pdf |format=PDF| title=Fast Low-Rank Modifications of the Thin Singular Value Decomposition | author=Matthew Brand | journal=Linear Algebra and Its Applications | volume=415 | pages=20–30 | year=2006 | doi=10.1016/j.laa.2005.07.021 }}</ref> [http://web.mit.edu/~wingated/www/resources.html MATLAB] and [http://radimrehurek.com/gensim Python] implementations of these fast algorithms are available. Unlike Gorrell and Webb's (2005) stochastic approximation, Brand's algorithm (2003) provides an exact solution.
In recent years progress has been made to reduce the computational complexity of SVD; for instance, by using a parallel ARPACK algorithm to perform parallel eigenvalue decomposition it is possible to speed up the SVD computation cost while providing comparable prediction quality.<ref>{{cite journal | doi = 10.1109/ICCSNT.2011.6182070 | title=A parallel implementation of Singular Value Decomposition based on Map-Reduce and PARPACK | journal=Proceedings of 2011 International Conference on Computer Science and Network Technology}}</ref>

== Limitations ==
Some of LSA's drawbacks include:

* The resulting dimensions might be difficult to interpret. For instance, in
:: {(car), (truck), (flower)} ↦  {(1.3452 * car + 0.2828 * truck), (flower)}
:the (1.3452 * car + 0.2828 * truck) component could be interpreted as "vehicle". However, it is very likely that cases close to
:: {(car), (bottle), (flower)} ↦  {(1.3452 * car + 0.2828 * '''bottle'''), (flower)}
:will occur. This leads to results which can be justified on the mathematical level, but have no interpretable meaning in natural language.

* LSA cannot capture [[polysemy]] (i.e., multiple meanings of a word) because each occurrence of a word is treated as having the same meaning due to the word being represented as a single point in space.  For example, the occurrence of "chair" in a document containing "The Chair of the Board" and in a separate document containing "the chair maker" are considered the same.  The behavior results in the vector representation being an ''average'' of all the word's different meanings in the corpus, which can make it difficult for comparison.  However, the effect is often lessened due to words having a [[word sense disambiguation|predominant sense]] throughout a corpus (i.e. not all meanings are equally likely).
* Limitations of [[bag of words model]] (BOW), where a text is represented as an unordered collection of words. To address some of the limitation of [[bag of words model]] (BOW), [[N-gram|multi-gram]] dictionary can be used to find direct and indirect association as well as [[Higher-order statistics|higher-order]] [[co-occurrence]]s among terms.<ref>{{cite journal|url=http://www.translational-medicine.com/content/12/1/324|title=Empirical study using network of semantically related associations in bridging the knowledge gap|first1=Vida|last1=Abedi|first2=Mohammed|last2=Yeasin|first3=Ramin|last3=Zand|date=27 November 2014|publisher=|volume=12|issue=1|doi=10.1186/s12967-014-0324-9|pmid=25428570|pmc=4252998|journal=Journal of Translational Medicine}}</ref>
* The [[probabilistic model]] of LSA does not match observed data: LSA assumes that words and documents form a joint [[normal distribution|Gaussian]] model ([[ergodic hypothesis]]), while a [[Poisson distribution]] has been observed.  Thus, a newer alternative is [[probabilistic latent semantic analysis]], based on a [[multinomial distribution|multinomial]] model, which is reported to give better results than standard LSA.<ref name="Thomas1999">{{cite conference | url=http://www.cs.brown.edu/people/th/papers/Hofmann-UAI99.pdf |format=PDF| title=Probabilistic Latent Semantic Analysis | author=Thomas Hofmann | booktitle=Uncertainty in Artificial Intelligence |year=1999}}</ref>

==Alternative methods==

===Semantic hashing===
In semantic hashing <ref>Salakhutdinov, Ruslan, and Geoffrey Hinton. "Semantic hashing." RBM 500.3 (2007): 500.</ref> documents are mapped to memory addresses by means of a [[neural network]] in such a way that semantically similar documents are located at nearby addresses. [[Deep learning|Deep neural network]] essentially builds a [[graphical model]] of the word-count vectors obtained from a large set of documents. Documents similar to a query document can then be found by simply accessing all the addresses that differ by only a few bits from the address of the query document. This way of extending the efficiency of hash-coding to approximate matching is much faster than [[locality sensitive hashing]], which is the fastest current method.

== Latent semantic indexing ==
'''Latent semantic indexing''' ('''LSI''') is an indexing and retrieval method that uses a mathematical technique called [[singular value decomposition]] (SVD) to identify patterns in the relationships between the [[terminology|term]]s and [[concept]]s contained in an unstructured collection of text.  LSI is based on the principle that words that are used in the same contexts tend to have similar meanings.  A key feature of LSI is its ability to extract the conceptual content of a [[Text corpus|body of text]] by establishing associations between those terms that occur in similar [[context (language use)|context]]s.<ref name=deerwester1988>Deerwester, S., et al, Improving Information Retrieval with Latent Semantic Indexing, Proceedings of the 51st Annual Meeting of the American Society for Information Science 25, 1988, pp. 36–40.</ref>

LSI is also an application of [[correspondence analysis]], a multivariate statistical technique developed by [[Jean-Paul Benzécri]]<ref>{{ cite book
 | author = Benzécri, J.-P.
 | publisher=Dunod |location= Paris, France
 | year = 1973
 | title = L'Analyse des Données. Volume II. L'Analyse des Correspondences
 }}</ref> in the early 1970s, to a [[contingency table]] built from word counts in documents.

Called Latent Semantic Indexing because of its ability to correlate semantically related terms that are latent in a collection of text, it was first applied to text at [[Bellcore]] in the late 1980s.   The method, also called latent semantic analysis (LSA), uncovers the underlying latent semantic structure in the usage of words in a body of text and how it can be used to extract the meaning of the text in response to user queries, commonly referred to as concept searches.  Queries, or concept searches, against a set of documents that have undergone LSI will return results that are conceptually similar in meaning to the search criteria even if the results don’t share a specific word or words with the search criteria.

== Benefits of LSI ==

LSI overcomes two of the most problematic constraints of Boolean [[keyword search|keyword queries]]:  multiple words that have similar meanings ([[synonymy]]) and words that have more than one meaning ([[polysemy]]).  Synonymy is often the cause of mismatches in the vocabulary used by the authors of documents and the users of [[information retrieval]] systems.<ref>{{Cite journal | last1 = Furnas | first1 = G. W. | last2 = Landauer | first2 = T. K. | last3 = Gomez | first3 = L. M. | last4 = Dumais | first4 = S. T. | title = The vocabulary problem in human-system communication | doi = 10.1145/32206.32212 | journal = Communications of the ACM | volume = 30 | issue = 11 | pages = 964–971 | year = 1987 | pmid =  | pmc = }}</ref>  As a result, Boolean or keyword queries often return irrelevant results and miss information that is relevant.

LSI is also used to perform automated [[document categorization]].  In fact, several experiments have demonstrated that there are a number of correlations between the way LSI and humans process and categorize text.<ref name=landauer2008>Landauer, T., et al., Learning Human-like Knowledge by Singular Value Decomposition: A Progress Report, M. I. Jordan, M. J. Kearns & S. A. Solla (Eds.), Advances in Neural Information Processing Systems 10, Cambridge: MIT Press, 1998, pp. 45–51.</ref>    Document categorization is the assignment of documents to one or more predefined categories based on their similarity to the conceptual content of the categories.<ref>{{Cite book | last1 = Dumais | first1 = S. | last2 = Platt | first2 = J. | last3 = Heckerman | first3 = D. | last4 = Sahami | first4 = M. | chapter = Inductive learning algorithms and representations for text categorization | doi = 10.1145/288627.288651 | title = Proceedings of the seventh international conference on Information and knowledge management  - CIKM '98 | pages = 148 | year = 1998 | isbn = 1581130619 | url = http://research.microsoft.com/en-us/um/people/jplatt/cikm98.pdf| pmid =  | pmc = }}</ref>   LSI uses ''example'' documents to establish the conceptual basis for each category.  During categorization processing, the concepts contained in the documents being categorized are compared to the concepts contained in the example items, and a category (or categories) is assigned to the documents based on the similarities between the concepts they contain and the concepts that are contained in the example documents.

Dynamic clustering based on the conceptual content of documents can also be accomplished using LSI.  Clustering is a way to group documents based on their conceptual similarity to each other without using example documents to establish the conceptual basis for each cluster.  This is very useful when dealing with an unknown collection of unstructured text.

Because it uses a strictly mathematical approach, LSI is inherently independent of language.  This enables LSI to elicit the semantic content of information written in any language without requiring the use of auxiliary structures, such as dictionaries and thesauri.  LSI can also perform cross-linguistic concept searching and example-based categorization.  For example, queries can be made in one language, such as English, and conceptually similar results will be returned even if they are composed of an entirely different language or of multiple languages.{{Citation needed|date=July 2015}}

LSI is not restricted to working only with words.  It can also process arbitrary character strings.  Any object that can be expressed as text can be represented in an LSI vector space.<ref>Zukas, Anthony, Price, Robert J., Document Categorization Using Latent Semantic Indexing, White Paper, [[Content Analyst Company]], LLC</ref>   For example, tests with MEDLINE abstracts have shown that LSI is able to effectively classify genes based on conceptual modeling of the biological information contained in the titles and abstracts of the MEDLINE citations.<ref>{{Cite journal | last1 = Homayouni | first1 = R. | last2 = Heinrich | first2 = K. | last3 = Wei | first3 = L. | last4 = Berry | first4 = M. W. | title = Gene clustering by Latent Semantic Indexing of MEDLINE abstracts | doi = 10.1093/bioinformatics/bth464 | journal = Bioinformatics | volume = 21 | issue = 1 | pages = 104–115 | year = 2004 | pmid =  15308538| pmc = }}</ref>

LSI automatically adapts to new and changing terminology, and has been shown to be very tolerant of noise (i.e., misspelled words, typographical errors, unreadable characters, etc.).<ref>{{Cite book | last1 = Price | first1 = R. J. | last2 = Zukas | first2 = A. E. | chapter = Application of Latent Semantic Indexing to Processing of Noisy Text | doi = 10.1007/11427995_68 | title = Intelligence and Security Informatics | series = Lecture Notes in Computer Science | volume = 3495 | pages = 602 | year = 2005 | isbn = 978-3-540-25999-2 | pmid =  | pmc = }}</ref>   This is especially important for applications using text derived from Optical Character Recognition (OCR) and speech-to-text conversion.  LSI also deals effectively with sparse, ambiguous, and contradictory data.

Text does not need to be in sentence form for LSI to be effective.  It can work with lists, free-form notes, email, Web-based content, etc.  As long as a collection of text contains multiple terms, LSI can be used to identify patterns in the relationships between the important terms and concepts contained in the text.

LSI has proven to be a useful solution to a number of conceptual matching problems.<ref>Ding, C., A Similarity-based Probability Model for Latent Semantic Indexing, Proceedings of the 22nd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 59–65.</ref><ref>Bartell, B., Cottrell, G., and Belew, R., Latent Semantic Indexing is an Optimal Special Case of Multidimensional Scaling, Proceedings, ACM SIGIR Conference on Research and Development in Information Retrieval, 1992, pp. 161–167.</ref>  The technique has been shown to capture key relationship information, including causal, goal-oriented, and taxonomic information.<ref>{{cite journal|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.5444&rep=rep1&type=pdf|author1=Graesser, A. |author2=Karnavat, A.|title=Latent Semantic Analysis Captures Causal, Goal-oriented, and Taxonomic Structures|journal=Proceedings of CogSci 2000|pages=184–189}}</ref>

== LSI timeline ==

*'''Mid-1960s''' – Factor analysis technique first described and tested (H. Borko and M. Bernick)
*'''1988''' – Seminal paper on LSI technique published <ref name=deerwester1988/>
*'''1989''' – Original patent granted <ref name=deerwester1988/>
*'''1992''' – First use of LSI to assign articles to reviewers<ref>{{cite journal|last1=Dumais |first1=S. |last2=Nielsen |first2=J. |title=Automating the Assignment of Submitted Manuscripts to Reviewers|journal=Proceedings of the Fifteenth Annual International Conference on Research and Development in Information Retrieval|year=1992|pages=233–244|doi=10.1145/133160.133205|isbn=0897915232 }}</ref>
*'''1994''' – Patent granted for the cross-lingual application of LSI (Landauer et al.)
*'''1995''' – First use of LSI for grading essays (Foltz, et al., Landauer et al.)
*'''1999''' – First implementation of LSI technology for intelligence community for analyzing unstructured text ([[Science Applications International Corporation|SAIC]]).
*'''2002''' – LSI-based product offering to intelligence-based government agencies (SAIC)
*'''2005''' – First vertical-specific application – publishing – EDB (EBSCO, [[Content Analyst Company]])

== Mathematics of LSI ==

LSI uses common linear algebra techniques to learn the conceptual correlations in a collection of text.  In general, the process involves constructing a weighted term-document matrix, performing a [[Singular value decomposition|'''Singular Value Decomposition''']] on the matrix, and using the matrix to identify the concepts contained in the text.

=== Term-document matrix ===

LSI begins by constructing a term-document matrix, <math>A</math>, to identify the occurrences of the <math>m</math> unique terms within a collection of <math>n</math> documents.  In a term-document matrix, each term is represented by a row, and each document is represented by a column, with each matrix cell, <math>a_{ij}</math>, initially representing the number of times the associated term appears in the indicated document, <math>\mathrm{tf_{ij}}</math>.  This matrix is usually very large and very sparse.

Once a term-document matrix is constructed, local and global weighting functions can be applied to it to condition the data.  The weighting functions transform each cell, <math>a_{ij}</math> of <math>A</math>, to be the product of a local term weight, <math>l_{ij}</math>, which describes the relative frequency of a term in a document, and a global weight, <math>g_i</math>, which describes the relative frequency of the term within the entire collection of documents.

Some common local weighting functions <ref>
Berry, M. W., and Browne, M., Understanding Search Engines: Mathematical Modeling and Text Retrieval, Society for Industrial and Applied Mathematics, Philadelphia, (2005).</ref> are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
|  style="width:22%" | '''Binary''' ||
| <math>l_{ij} = 1</math> if the term exists in the document, or else <math>0</math>
|-
|  style="width:22%" | '''TermFrequency''' ||
| <math>l_{ij} = \mathrm{tf}_{ij}</math>, the number of occurrences of term <math>i</math> in document <math>j</math>
|-
|  style="width:22%" | '''Log''' ||
| <math>l_{ij} = \log(\mathrm{tf}_{ij} + 1)</math>
|-
|  style="width:22%" | '''Augnorm''' ||
| <math>l_{ij} = \frac{\Big(\frac{\mathrm{tf}_{ij}}{\max_i(\mathrm{tf}_{ij})}\Big) + 1}{2}</math>
|}

Some common global weighting functions are defined in the following table.

{| style="width:60%" cellpadding="25" cellspacing="5" align="center"
|-
| style="width:22%" | '''Binary''' ||
| <math>g_i = 1</math>
|-
| style="width:22%" | '''Normal''' ||
| <math>g_i = \frac{1}{\sqrt{\sum_j \mathrm{tf}_{ij}^2}}</math>
|-
| style="width:22%" | '''GfIdf''' ||
| <math>g_i = \mathrm{gf}_i / \mathrm{df}_i</math>, where <math>\mathrm{gf}_i</math> is the total number of times term <math>i</math> occurs in the whole collection, and <math>\mathrm{df}_i</math> is the number of documents in which term <math>i</math> occurs.
|-
| style="width:22%" | '''[[Tf–idf#Inverse document frequency 2|Idf (Inverse Document Frequency)]]''' ||
| <math>g_i = \log_2 \frac{n}{1+ \mathrm{df}_i}</math>
|-
| style="width:22%" | '''Entropy''' ||
| <math>g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}</math>, where <math>p_{ij} = \frac{\mathrm{tf}_{ij}}{\mathrm{gf}_i}</math>
|}

Empirical studies with LSI report that the Log and Entropy weighting functions work well, in practice, with many data sets.<ref>Landauer, T., et al., Handbook of Latent Semantic Analysis, Lawrence Erlbaum Associates, 2007.</ref>  In other words, each entry <math>a_{ij}</math> of <math>A</math> is computed as:

:<math>g_i = 1 + \sum_j \frac{p_{ij} \log p_{ij}}{\log n}</math>

:<math>a_{ij} = g_i \ \log (\mathrm{tf}_{ij} + 1)</math>

=== Rank-reduced singular value decomposition ===

A rank-reduced, [[singular value decomposition]] is performed on the matrix to determine patterns in the relationships between the terms and concepts contained in the text.  The SVD forms the foundation for LSI.<ref>Berry, Michael W., Dumais, Susan T., O'Brien, Gavin W., Using Linear Algebra for Intelligent Information Retrieval, December 1994, SIAM Review 37:4 (1995), pp. 573–595.</ref>   It computes the term and document vector spaces by approximating the single term-frequency matrix, <math>A</math>, into three other matrices— an '''''m''''' by '''''r'''''  term-concept vector matrix <math>T</math>, an '''''r''''' by '''''r''''' singular values matrix <math>S</math>, and a '''''n''''' by '''''r''''' concept-document vector matrix, <math>D</math>, which satisfy the following relations:

<math>A \approx TSD^T</math>

<math>T^T T = I_r \quad D^T D = I_r </math>

<math>S_{1,1} \geq S_{2,2} \geq \ldots \geq  S_{r,r} > 0 \quad S_{i,j} = 0 \; \text{where} \; i \neq j</math>

In the formula, '''A''' is the supplied '''''m''''' by '''''n''''' weighted matrix of term frequencies in a collection of text where '''''m''''' is the number of unique terms, and '''''n''''' is the number of documents.  '''T''' is a computed '''''m''''' by '''''r''''' matrix of term vectors where '''''r''''' is the rank of '''A'''—a measure of its unique dimensions '''≤ min(''m,n'')'''.  '''S''' is a computed '''''r''''' by '''''r''''' diagonal matrix of decreasing singular values, and '''D''' is a computed '''''n''''' by '''''r''''' matrix of document vectors.

The SVD is then [[Singular value decomposition#Truncated SVD|truncated]] to reduce the rank by keeping only the largest '''''k''''' « '''''r''''' diagonal entries in the singular value matrix '''S''',
where '''''k''''' is typically on the order 100 to 300 dimensions.
This effectively reduces the term and document vector matrix sizes to '''''m''''' by '''''k''''' and '''''n''''' by '''''k''''' respectively.  The SVD operation, along with this reduction, has the effect of preserving the most important semantic information in the text while reducing noise and other undesirable artifacts of the original space of '''A'''.  This reduced set of matrices is often denoted with a modified formula such as:

:::::::'''A ≈ A''<sub>k''</sub> = T''<sub>k''</sub> S''<sub>k''</sub> D''<sub>k''</sub><sup>T</sup>'''

Efficient LSI algorithms only compute the first '''''k''''' singular values and term and document vectors as opposed to computing a full SVD and then truncating it.

Note that this rank reduction is essentially the same as doing [[Principal Component Analysis]] (PCA) on the matrix '''A''', except that PCA subtracts off the means.  PCA loses the sparseness of the '''A''' matrix, which can make it infeasible for large lexicons.

== Querying and augmenting LSI vector spaces ==

The computed '''T''<sub>k''</sub>''' and '''D''<sub>k''</sub>''' matrices define the term and document vector spaces, which with the computed singular values, '''S''<sub>k''</sub>''', embody the conceptual information derived from the document collection.  The similarity of terms or documents within these spaces is a factor of how close they are to each other in these spaces, typically computed as a function of the angle between the corresponding vectors.

The same steps are used to locate the vectors representing the text of queries and new documents within the document space of an existing LSI index.  By a simple transformation of the '''A = T S D<sup>T</sup>''' equation into the equivalent '''D = A<sup>T</sup> T S<sup>−1</sup>''' equation, a new vector, '''''d''''', for a query or for a new document can be created by computing a new column in '''A''' and then multiplying the new column by '''T S<sup>−1</sup>'''.  The new column in '''A''' is computed using the originally derived global term weights and applying the same local weighting function to the terms in the query or in the new document.

A drawback to computing vectors in this way, when adding new searchable documents, is that terms that were not known during the SVD phase for the original index are ignored.  These terms will have no impact on the global weights and learned correlations derived from the original collection of text.  However, the computed vectors for the new text are still very relevant for similarity comparisons with all other document vectors.

The process of augmenting the document vector spaces for an LSI index with new documents in this manner is called ''folding in''.  Although the folding-in process does not account for the new semantic content of the new text, adding a substantial number of documents in this way will still provide good results for queries as long as the terms and concepts they contain are well represented within the LSI index to which they are being added.  When the terms and concepts of a new set of documents need to be included in an LSI index, either the term-document matrix, and the SVD, must be recomputed or an incremental update method (such as the one described in <ref name="brand2006"/>) be used.

== Additional uses of LSI ==

It is generally acknowledged that the ability to work with text on a semantic basis is essential to modern information retrieval systems.  As a result, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.

LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, 2004, Chapter 4.</ref>   Below are some other ways in which LSI is being used:

* Information discovery<ref>Best Practices Commentary on the Use of Search and Information Retrieval Methods in E-Discovery, the Sedona Conference, 2007, pp. 189–223.</ref>  ([[Electronic Discovery|eDiscovery]], Government/Intelligence community, Publishing)
* Automated document classification (eDiscovery, Government/Intelligence community, Publishing)<ref>Foltz, P. W. and Dumais, S. T. Personalized Information Delivery:  An analysis of information filtering methods, Communications of the ACM, 1992, 34(12), 51-60.</ref>
* Text summarization<ref>Gong, Y., and Liu, X., Creating Generic Text Summaries, Proceedings, Sixth International Conference on Document Analysis and Recognition, 2001, pp. 903–907.</ref>  (eDiscovery, Publishing)
* Relationship discovery<ref>Bradford, R., Efficient Discovery of New Information in Large Text Databases, Proceedings, IEEE International Conference on Intelligence and Security Informatics, Atlanta, Georgia, LNCS Vol. 3495, Springer, 2005, pp. 374–380.</ref>  (Government, Intelligence community, Social Networking)
* Automatic generation of link charts of individuals and organizations<ref>Bradford, R., Application of Latent Semantic Indexing in Generating Graphs of Terrorist Networks, in: Proceedings, IEEE International Conference on Intelligence and Security Informatics, ISI 2006, San Diego, CA, USA, May 23–24, 2006, Springer, LNCS vol. 3975, pp. 674–675.</ref>  (Government, Intelligence community)
* Matching technical papers and grants with reviewers<ref>Yarowsky, D., and Florian, R., Taking the Load off the Conference Chairs: Towards a Digital Paper-routing Assistant, Proceedings of the 1999 Joint SIGDAT Conference on Empirical Methods in NLP and Very-Large Corpora, 1999, pp. 220–230.</ref>  (Government)
* Online customer support<ref>Caron, J., Applying LSA to Online Customer Support: A Trial Study, Unpublished Master's Thesis, May 2000.</ref>  (Customer Management)
* Determining document authorship<ref>Soboroff, I., et al, Visualizing Document Authorship Using N-grams and Latent Semantic Indexing,   Workshop on New Paradigms in Information Visualization and Manipulation, 1997, pp. 43–48.</ref>  (Education)
* Automatic keyword annotation of images<ref>Monay, F., and Gatica-Perez, D., On Image Auto-annotation with Latent Space Models, Proceedings of the 11th ACM international conference on Multimedia, Berkeley, CA, 2003, pp. 275–278.</ref>
* Understanding software source code<ref>{{cite journal|author1=Maletic, J. |author2=Marcus, A.|title=Using Latent Semantic Analysis to Identify Similarities in Source Code to Support Program Understanding|journal=Proceedings of 12th IEEE International Conference on Tools with Artificial Intelligence|location=Vancouver, British Columbia|date=November 13–15, 2000|pages= 46–53|doi=10.1109/TAI.2000.889845|isbn=0-7695-0909-6}}</ref>  (Software Engineering)
* Filtering [[Spam (electronic)|spam]]<ref>Gee, K., Using Latent Semantic Indexing to Filter Spam, in: Proceedings, 2003 ACM Symposium on Applied Computing, Melbourne, Florida, pp. 460–464.</ref>  (System Administration)
* Information visualization<ref name=landauer2004>Landauer, T., Laham, D., and Derr, M., From Paragraph to Graph: Latent Semantic Analysis for Information Visualization, Proceedings of the National Academy of Sciences, 101, 2004, pp. 5214–5219.</ref>
* [[Automated essay scoring|Essay scoring]]<ref>Foltz, Peter W., Laham, Darrell, and Landauer, Thomas K., Automated Essay Scoring: Applications to Educational Technology, Proceedings of EdMedia,  1999.</ref>  (Education)
* [[Literature-based discovery]]<ref>Gordon, M., and Dumais, S., Using Latent Semantic Indexing for Literature Based Discovery, Journal of the American Society for Information Science, 49(8), 1998, pp. 674–685.</ref>
* Stock resturns prediction<ref name="Galvez2017">{{cite journal | url=http://www.sciencedirect.com/science/article/pii/S1877750317300091 | title=Assessing the usefulness of online message board mining in automatic stock prediction systems |author1=Ramiro H. Gálvez |author2=Agustín Gravano | journal=Journal of Computational Science | volume=19 | pages=1877-7503 | year=2017  | doi=10.1016/j.jocs.2017.01.001}}</ref>

LSI is increasingly being used for electronic document discovery (eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is essential.  Concept-based searching using LSI has been applied to the eDiscovery process by leading providers as early as 2003.<ref>There Has to be a Better Way to Search, 2008, White Paper, Fios, Inc.</ref>

== Challenges to LSI ==

Early challenges to LSI focused on scalability and performance.  LSI requires relatively high computational performance and memory in comparison to other information retrieval techniques.<ref>Karypis, G., Han, E., Fast Supervised Dimensionality Reduction Algorithm with Applications to Document Categorization and Retrieval, Proceedings of CIKM-00, 9th ACM Conference on Information and Knowledge Management.</ref>  However, with the implementation of modern high-speed processors and the availability of inexpensive memory, these considerations have been largely overcome.  Real-world applications involving more than 30 million documents that were fully processed through the matrix and SVD computations are common in some LSI applications. A fully scalable (unlimited number of documents, online training) implementation of LSI is contained in the open source [[gensim]] software package.<ref name="rehurek2011">{{cite journal | url=http://dx.doi.org/10.1007/978-3-642-20161-5_29 |format=PDF| title=Subspace Tracking for Latent Semantic Analysis | author=Radim Řehůřek | journal=Advances in Information Retrieval - 33rd European Conference on IR Research, ECIR 2011 | volume=6611 | pages=289–300 | year=2011 | doi=10.1007/978-3-642-20161-5_29 |series=Lecture Notes in Computer Science|isbn=978-3-642-20160-8}}</ref>

Another challenge to LSI has been the alleged difficulty in determining the optimal number of dimensions to use for performing the SVD.  As a general rule, fewer dimensions allow for broader comparisons of the concepts contained in a collection of text, while a higher number of dimensions enable more specific (or more relevant) comparisons of concepts.  The actual number of dimensions that can be used is limited by the number of documents in the collection.  Research has demonstrated that around 300 dimensions will usually provide the best results with moderate-sized document collections (hundreds of thousands of documents) and perhaps 400 dimensions for larger document collections (millions of documents).<ref>Bradford, R., An Empirical Study of Required Dimensionality for Large-scale Latent Semantic Indexing Applications, Proceedings of the 17th ACM Conference on Information and Knowledge Management, Napa Valley, California, USA, 2008, pp. 153–162.</ref>   However, recent studies indicate that 50-1000 dimensions are suitable depending on the size and nature of the document collection.<ref name=landauer2008>Landauer, Thomas K., and Dumais, Susan T., Latent Semantic Analysis, Scholarpedia, 3(11):4356, 2008.</ref>

Checking the amount of variance in the data after computing the SVD can be used to determine the optimal number of dimensions to retain.  The variance contained in the data can be viewed by plotting the singular values (S) in a [[scree plot]].  Some LSI practitioners select the dimensionality associated with the knee of the curve as the cut-off point for the number of dimensions to retain.  Others argue that some quantity of the variance must be retained, and the amount of variance in the data should dictate the proper dimensionality to retain.  Seventy percent is often mentioned as the amount of variance in the data that should be used to select the optimal dimensionality for recomputing the SVD.<ref>Cangelosi, R., Goriely A., Component Retention In Principal Component Analysis With Application to Cdna Microarray Data, BMC Biology Direct 2(2) (2007).</ref><ref>Jolliffe, L. T., Principal Component Analysis, Springer-Verlag, New York, (1986).</ref><ref>Hu, X., Z. Cai, et al., LSA: First Dimension and Dimensional Weighting, 25th Annual Meeting of the Cognitive Science Society, Boston, MA.</ref>

== See also ==
* [[Compound term processing]]
* [[Explicit semantic analysis]]
* [[Latent semantic mapping]]
* [[Latent Semantic Structure Indexing]]
* [[Principal components analysis]]
* [[Probabilistic latent semantic analysis]]
* [[Spamdexing]]
* [[Topic model]]
** [[Latent Dirichlet allocation]]
* [[Distributional semantics]]
* [[Coh-Metrix]]

== References ==
{{Reflist|30em}}

==Further reading==
* {{cite journal
 | url=http://lsa.colorado.edu/papers/dp1.LSAintro.pdf
 | format=PDF
 | title=Introduction to Latent Semantic Analysis
 | author-link1= Thomas Landauer |first1=Thomas |last1=Landauer |first2=Peter W. |last2=Foltz |first3=Darrell |last3=Laham
 | journal=Discourse Processes
 | volume=25
 | pages=259–284
 | year=1998
 | doi=10.1080/01638539809545028
 | issue=2–3
}}
* {{cite journal
 | url=http://lsi.research.telcordia.com/lsi/papers/JASIS90.pdf 
 | format=PDF| title=Indexing by Latent Semantic Analysis
 | first1=Scott |last1=Deerwester |first2=Susan T. |last2=Dumais |first3=George W. |last3=Furnas |first4=Thomas K. |last4=Landauer |first5=Richard |last5=Harshman
 | author-link1=Scott Deerwester |author-link2=Susan Dumais |author-link3=George Furnas |author-link4=Thomas Landauer |author-link5=Richard Harshman
 | journal=Journal of the American Society for Information Science
 | volume=41
 | issue=6
 | pages=391–407
 | year=1990 
 | doi=10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9
}} Original article where the model was first exposed.
* {{cite journal
 | url=http://citeseer.ist.psu.edu/berry95using.html
 | title=Using Linear Algebra for Intelligent Information Retrieval
 | first1=Michael |last1= Berry |first2=Susan T. |last2=Dumais  |first3=Gavin W. |last3=O'Brien
 | author-link1=Susan Dumais
 |year=1995
}} [http://lsirwww.epfl.ch/courses/dis/2003ws/papers/ut-cs-94-270.pdf (PDF)]. Illustration of the application of LSA to document retrieval.
* {{cite web
 | url=http://iv.slis.indiana.edu/sw/lsa.html
 | title=Latent Semantic Analysis
 | publisher=InfoVis
}}
* {{cite web
 | url=http://cran.at.r-project.org/web/packages/lsa/index.html
 | title=An Open Source LSA Package for R
 | publisher=CRAN
 | author=Fridolin Wild
 | date=November 23, 2005
 | accessdate=November 20, 2006
}}
* {{ cite web
 | url=http://www.welchco.com/02/14/01/60/96/02/2901.HTM
 | title=A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge
 | author=[[Thomas Landauer]], [[Susan Dumais|Susan T. Dumais]]
 | accessdate=2007-07-02
}}

==External links==

===Articles on LSA===
* [http://www.scholarpedia.org/article/Latent_semantic_analysis Latent Semantic Analysis], a scholarpedia article on LSA written by Tom Landauer, one of the creators of LSA.

===Talks and demonstrations===
* [http://videolectures.net/slsfs05_hofmann_lsvm/ LSA Overview], talk by Prof. [http://www.cs.brown.edu/~th/ Thomas Hofmann] describing LSA, its applications in Information Retrieval, and its connections to [[probabilistic latent semantic analysis]].
* [http://www.semanticquery.com/archive/semanticsearchart/researchLSA.html Complete LSA sample code in C# for Windows]. The demo code includes enumeration of text files, filtering stop words, stemming, making a document-term matrix and SVD.

===Implementations===

Due to its cross-domain applications in [[Information Retrieval]], [[Natural Language Processing]] (NLP), [[Cognitive Science]] and [[Computational Linguistics]], LSA has been implemented to support many different kinds of applications.
* [http://www.d.umn.edu/~tpederse/senseclusters.html Sense Clusters], an Information Retrieval-oriented perl implementation of LSA
* [http://code.google.com/p/airhead-research/ S-Space Package], a Computational Linguistics and Cognitive Science-oriented Java implementation of LSA
* [http://code.google.com/p/semanticvectors/ Semantic Vectors] applies Random Projection, LSA, and Reflective Random Indexing to [[Lucene]] term-document matrices
* [http://infomap-nlp.sourceforge.net/ Infomap Project], an NLP-oriented C implementation of LSA (superseded by semanticvectors project)
* [http://scgroup20.ceid.upatras.gr:8000/tmg/index.php/Main_Page Text to Matrix Generator], A MATLAB Toolbox for generating term-document matrices from text collections, with support for LSA
* [[Gensim]] contains a Python implementation of LSA for matrices larger than RAM.

[[Category:Information retrieval techniques]]
[[Category:Natural language processing]]
[[Category:Latent variable models]]
<=====doc_Id=====>:677
<=====title=====>:
XML retrieval
<=====text=====>:
{{Multiple issues|
{{expert-subject|Computer science|date=January 2015}}
{{COI|date=February 2009}}
}}

'''XML retrieval''', or XML Information Retrieval, is the content-based retrieval of documents structured with [[XML]] (eXtensible Markup Language). As such it is used for computing [[Relevance (information retrieval)|relevance]] of XML documents.<ref>{{Cite web|url=ftp://ftp.tm.informatik.uni-frankfurt.de/pub/papers/ir/An%20Architecture%20for%20XML%20Information%20Retrieval%20in%20a%20Peer-to-Peer%20Environment_2007.pdf|title=An Architecture for XML Information Retrieval in a Peer-to-Peer Environment|last=Winter|first=Judith|author2=Drobnik, Oswald |date=November 9, 2007|publisher=ACM|accessdate=2009-02-10}}</ref>

==Queries==
Most XML retrieval approaches do so based on techniques from the [[information retrieval]] (IR) area, e.g. by computing the similarity between a query consisting of keywords (query terms) and the document. However, in XML-Retrieval the query can also contain [[Data structure|structural]] [[Hint (SQL)|hints]]. So-called "content and structure" (CAS) queries enable users to specify what structure the requested content can or must have.

==Exploiting XML structure==
Taking advantage of the [[Self-documenting|self-describing]] structure of XML documents can improve the search for XML documents significantly. This includes the use of CAS queries, the weighting of different XML elements differently and the focused retrieval of subdocuments.

==Ranking==
Ranking in XML-Retrieval can incorporate both content relevance and structural similarity, which is the resemblance between the structure given in the query and the structure of the document. Also, the retrieval units resulting from an XML query may not always be entire documents, but can be any deeply nested XML elements, i.e. dynamic documents. The aim is to find the smallest retrieval unit that is highly relevant. Relevance can be defined according to the notion of specificity, which is the extent to which a retrieval unit focuses on the topic of request.<ref name="INEX2006">{{Cite web|url=http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |title=Overview of INEX 2006 |last=Malik |first=Saadia |author2=Trotman, Andrew |author3=Lalmas, Mounia |author4=Fuhr, Norbert |year=2007 |work=Proceedings of the Fifth Workshop of the INitiative for the Evaluation of XML Retrieval |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081016101202/http://www.cs.otago.ac.nz/homepages/andrew/2006-10.pdf |archivedate=October 16, 2008 }}</ref>

==Existing XML search engines==
An overview of two potential approaches is available.<ref>{{Cite web|url=http://www.sigmod.org/record/issues/0612/p16-article-yahia.pdf|title=XML Search: Languages, INEX and Scoring|last=Amer-Yahia|first=Sihem|author2=Lalmas, Mounia |year=2006|publisher=SIGMOD Rec. Vol. 35, No. 4|accessdate=2009-02-10}} {{Dead link|date=October 2010|bot=H3llBot}}</ref><ref>{{Cite paper|citeseerx = 10.1.1.109.5986|title=XML Retrieval: A Survey|last=Pal|first=Sukomal|date=June 30, 2006|publisher=Technical Report, CVPR }}</ref> The INitiative for the Evaluation of XML-Retrieval (''INEX'') was founded in 2002 and provides a platform for evaluating such [[algorithm]]s.<ref name="INEX2006" /> Three different areas influence XML-Retrieval:<ref name="INEX2002">{{Cite web|url=http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |title=INEX: Initiative for the Evaluation of XML Retrieval |last=Fuhr |first=Norbert |author2=Gövert, N. |author3=Kazai, Gabriella |author4=Lalmas, Mounia |year=2003 |work=Proceedings of the First INEX Workshop, Dagstuhl, Germany, 2002 |publisher=ERCIM Workshop Proceedings, France |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081121135758/http://www.is.informatik.uni-duisburg.de/bib/pdf/ir/Fuhr_etal:02a.pdf |archivedate=November 21, 2008 }}</ref>

===Traditional XML query languages===
[[Query language]]s such as the [[W3C]] standard [[XQuery]]<ref>{{Cite web|url=http://www.w3.org/TR/2007/REC-xquery-20070123/|title=XQuery 1.0: An XML Query Language|last=Boag|first=Scott|author2=Chamberlin, Don |author3=Fernández, Mary F. |author4=Florescu, Daniela |author5=Robie, Jonathan |author6= Siméon, Jérôme |date=23 January 2007|work=W3C Recommendation|publisher=World Wide Web Consortium|accessdate=2009-02-10}}</ref> supply complex queries, but only look for exact matches. Therefore, they need to be extended to allow for vague search with relevance computing. Most XML-centered approaches imply a quite exact knowledge of the documents' [[Database schema|schemas]].<ref name="Schlieder2002">{{Cite journal|url=http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |title=Querying and Ranking XML Documents |last=Schlieder |first=Torsten |author2=Meuss, Holger |year=2002 |work=Journal of the American Society for Information Science and Technology, Vol. 53, No. 6 |accessdate=2009-02-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20070610002349/http://www.cis.uni-muenchen.de/people/Meuss/Pub/JASIS02.ps.gz |archivedate=June 10, 2007 }}</ref>

===Databases===
Classic [[database]] systems have adopted the possibility to store [[Semi-structured model|semi-structured data]]<ref name="INEX2002" /> and resulted in the development of [[XML database]]s. Often, they are very formal, concentrate more on searching than on ranking, and are used by experienced users able to formulate complex queries.

===Information retrieval===
Classic information retrieval models such as the [[vector space model]] provide relevance ranking, but do not include document structure; only flat queries are  supported. Also, they apply a static document concept, so retrieval units usually are entire documents.<ref name="Schlieder2002"/> They can be extended to consider structural information and dynamic document retrieval. Examples for approaches extending the vector space models are available: they use document [[subtree]]s (index terms plus structure) as dimensions of the vector space.<ref>{{Cite web|url=http://www.cobase.cs.ucla.edu/tech-docs/sliu/SIGIR04.pdf|title=Configurable Indexing and Ranking for XML Information Retrieval|last=Liu|first=Shaorong|author2=Zou, Qinghua |author3=Chu, Wesley W. |year=2004|work=SIGIR'04|publisher=ACM|accessdate=2009-02-10}}</ref>

==See also==
*[[Document retrieval]]
*[[Information retrieval applications]]

==References==
{{Reflist}}

{{DEFAULTSORT:Xml-Retrieval}}
[[Category:XML]]
[[Category:Information retrieval genres]]
<=====doc_Id=====>:680
<=====title=====>:
Enterprise search
<=====text=====>:
{{original research|date=November 2015}}
'''Enterprise search''' is the practice of making content from multiple enterprise-type sources, such as [[database]]s and [[intranet]]s, searchable to a defined audience.

"Enterprise search" is used to describe the software of search information within an enterprise (though the search function and its results may still be public).<ref>{{cite web|url=http://www.aiim.org/What-is-Enterprise-Search|title=What is Enterprise Search?|publisher=}}</ref> Enterprise search can be contrasted with [[web search]], which applies search technology to documents on the open web, and [[desktop search]], which applies search technology to the content on a single computer.

Enterprise search systems index data and documents from a variety of sources such as: [[file systems]], [[intranets]], [[document management system]]s, [[e-mail]], and [[databases]]. Many enterprise search systems integrate structured and unstructured data in their collections.<ref>[http://www.arma.org/bookstore/files/Delgado.pdf The New Face of Enterprise Search: Bridging Structured and Unstructured Information]</ref> Enterprise search systems also use access controls to enforce a security policy on their users.<ref>{{cite web|url=http://www.ideaeng.com/tabId/98/itemId/118/Mapping-Security-Requirements-to-Enterprise-Search.aspx|title=Security Requirements to Enterprise Search: part 1 - New Idea Engineering|publisher=}}</ref>

Enterprise search can be seen as a type of [[vertical search]] of an enterprise.

==Components of an enterprise search system==
In an enterprise search system, content goes through various phases from source repository to search results:

=== Content awareness ===
Content awareness (or "content collection") is usually either a push or pull model. In the push model, a source system is integrated with the search engine in such a way that it connects to it and pushes new content directly to its [[API]]s. This model is used when realtime indexing is important. In the pull model, the software gathers content from sources using a connector such as a [[web crawler]] or a [[database]] connector. The connector typically polls the source with certain intervals to look for new, updated or deleted content.<ref>{{cite web|url=http://www.information-management.com/issues/20_7/content_management_data_integration_indexing_metadata-10019105-1.html|title=Understanding Content Collection and Indexing|publisher=}}</ref>

=== Content processing and analysis ===
Content from different sources may have many different formats or document types, such as XML, HTML, Office document formats or plain text. The content processing phase processes the incoming documents to plain text using document filters. It is also often necessary to normalize content in various ways to improve [[Recall (information retrieval)|recall]] or [[Precision (information retrieval)|precision]]. These may include [[stemming]], [[lemmatization]], [[synonym]] expansion, [[entity extraction]], [[part of speech]] tagging.

As part of processing and analysis, [[tokenization (lexical analysis)|tokenization]] is applied to split the content into [[Lexical analysis#Token|tokens]] which is the basic matching unit. It is also common to normalize tokens to lower case to provide case-insensitive search, as well as to normalize accents to provide better recall.

=== Indexing ===
The resulting text is stored in an [[Index (search engine)|index]], which is optimized for quick lookups without storing the full text of the document. The index may contain the dictionary of all unique words in the corpus as well as information about ranking and [[term frequency]].

=== Query processing ===
Using a web page, the user issues a [[Web search query|query]] to the system. The query consists of any terms the user enters as well as navigational actions such as [[faceted search|faceting]] and paging information.

=== Matching ===
The processed query is then compared to the stored index, and the search system returns results (or "hits") referencing source documents that match. Some systems are able to present the document as it was indexed.

==Differences from web search==
{{unreferenced section|date=November 2015}}
Beyond the difference in the kinds of materials being indexed, enterprise search systems also typically include functionality that is not associated with the mainstream [[web search engine]]s. These include:
*Adapters to index content from a variety of repositories, such as [[databases]] and [[content management systems]].
*[[Federated search]], which consists of
# transforming a query and broadcasting it to a group of disparate databases or external content sources with the appropriate syntax,
# merging the results collected from the databases,
# presenting them in a succinct and unified format with minimal duplication, and
# providing a means, performed either automatically or by the portal user, to sort the merged result set.
*[[Enterprise bookmarking]], collaborative [[tag (metadata)|tagging]] systems for capturing knowledge about structured and semi-structured enterprise data.
*[[Entity extraction]] that seeks to locate and classify elements in text into predefined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.
*[[Faceted search]], a technique for accessing a collection of information represented using a [[faceted classification]], allowing users to explore by filtering available information.
*Access control, usually in the form of an [[Access control list]] (ACL), is often required to restrict access to documents based on individual user identities. There are many types of access control mechanisms for different content sources making this a complex task to address comprehensively in an enterprise search environment (see below).
*[[Text clustering]], which groups the top several hundred search results into topics that are computed on the fly from the search-results descriptions, typically titles, excerpts (snippets), and meta-data.  This technique lets users navigate the content by topic rather than by the meta-data that is used in faceting. Clustering compensates for the problem of incompatible meta-data across multiple enterprise repositories, which hinders the usefulness of faceting.
*[[User interfaces]], which in web search are deliberately kept simple in order not to distract the user from clicking on ads, which generates the revenue.  Although the business model for enterprise search could include showing ads, in practice this is not done.  To enhance end user productivity, enterprise vendors continually experiment with rich UI functionality which occupies significant screen space, which would be problematic for web search.

==Relevance factors for enterprise search==
{{unreferenced section|date=November 2015}}
The factors that determine the relevance of search results within the context of an enterprise overlap with but are different from those that apply to web search. In general, enterprise search engines cannot take advantage of the rich [[hyperlink|link structure]] as is found on the web's [[hypertext]] content, however, a new breed of Enterprise search engines based on a bottom-up [[Web 2.0]] technology are providing both a contributory approach and [[hyperlink]]ing within the enterprise. Algorithms like [[PageRank]] exploit hyperlink structure to assign authority to documents, and then use that authority as a query-independent relevance factor. In contrast, enterprises typically have to use other query-independent factors, such as a document's recency or popularity, along with query-dependent factors traditionally associated with [[information retrieval]] algorithms.  Also, the rich functionality of enterprise search UIs, such as clustering and faceting, diminish reliance on ranking as the means to direct the user's attention.

==Access control: early binding vs late binding==
Security and restricted access to documents is an important matter in enterprise search. There are two main approaches to apply restricted access: early binding vs late binding.<ref>[http://enterprisesearch.co/enterprise-search-document-access-control/ Enterprise Search: document access control]</ref>

===Late binding===
Permissions are analyzed and assigned to documents at query stage. Query engine generates a document set and before returning it to a user this set is filtered based on user access rights. It is costly process but accurate (based on user permissions at the moment of query).

===Early binding===
Permissions are analyzed and assigned to documents at indexing stage. It is much more effective than late binding, but could be inaccurate (user might be granted or revoked permissions between in the period between indexing and querying).

==Search relevance testing options==
Search application relevance can be determined by following relevance testing options like<ref>[http://searchhub.org/2009/09/02/debugging-search-application-relevance-issues/  Debugging Search Application Relevance Issues]</ref>
*Focus groups
*Reference evaluation protocol (based on relevance judgements of results from agreed-upon queries performed against common document corpuses)
*Empirical testing
*[[A/B testing]]
*Log analysis on a Beta production site
*Online ratings

==See also==
*[[Collaborative search engine]]
*[[Comparison of enterprise search software]]
*[[Data defined storage]] 
*[[Enterprise bookmarking]]
*[[Enterprise information access]]
*[[Faceted search]]
*[[Information extraction]]
*[[Knowledge management]]
*[[List of enterprise search vendors]]
*[[List of search engines]]
*[[Text mining]]
*[[Vertical search]]

==References==
{{Reflist}}

{{DEFAULTSORT:Enterprise Search}}
[[Category:Information retrieval genres]]
<=====doc_Id=====>:683
<=====title=====>:
Multimodal search
<=====text=====>:
'''Multimodal search''' is a type of [[search engines|search]] that uses different methods to get relevant results. They can use any kind of search, [[keyword search|search by keyword]], [[concept search|search by concept]], [[Query by Example|search by example]],etc.

== Introduction ==

A multimodal search engine is designed to imitate the flexibility and agility of how the [[mind|human mind]] works to create, process and refuse irrelevant ideas. So, the more elements you have in the input of the search engine to can compare, the more [[Arithmetic precision|accurate]] the results can be.
Multimodal search engines use different inputs of different nature and methods of search at the same time with the possibility of combining the results by merging all of the input elements of the search. There are also engines that can use a feedback of the results with the evaluation of the user to perform a more appropriate and relevant search.

[[File:Schema of a simple search.jpg|thumb|Schema of a Simple Search]]

Nowadays, mobile devices have been developed to a point that they can perform infinite functions from any place at any time, thanks to the [[internet]] and [[GPS]] connections. Touch screens, motion sensors and voice recognition are now featured on mobile devices called [[smartphone]]s. All the features and functions make possible to can execute multimodal searches from any place of the world at any time.

=== Search elements ===

The use of text is an option, as well as [[multimedia search]]ing, [[image]]s, [[video]]s, [[Content (media)|audio]], [[voice]], [[document]]s. Even the location of the user can help the search engine to perform a more effective search, adaptable to every situation.
Nowadays, different ways to [[Human–computer interaction|interact]] with a search engine are being discovered, in terms of input elements of the search and in the variety of results obtained.

=== Personal context ===

Many queries from mobiles are [[location-based service|location-based]] (LBS), that use the location of the user to interact with the applications. If available, the browser uses the device GPS, or computes an approximate location based on cell tower triangulation, with the permission of the user, who must be agree to share his/her location with the application in the download.
Therefore, multimodal searches use not only audiovisual content that the user provides directly, but also the context where the user is, like his/her location, language, time at the moment, web site or document where the user is surfing, or other elements that can help to improve of a search in every situation.
[[File:Contextual query.jpg|Example of Contextual Query]]

== Classification of the results ==

The multimodal search engine works in parallel, whilst at the same time, performs a search of more to less relevance of every element introduced directly or indirectly (personal context). Afterwards, it provides a combination of all the results, merging every element with its associated weight for every descriptor.

The engine analyzes every element and tags them, so a comparison of the tags can be made with existent indexed information in databases. A classification of the results proceeds, to show them from more to less relevance.

[[File:Framework of Multimodal Search.jpg|thumb|Framework of a Multimodal Search]]

It’s necessary to define the importance of every input element. There are search engines that do this automatically, however there are also engines where the user can do it manually, giving more or less weight to every element of the search.
It’s also important that the user provides the appropriate and essential information for the search; too much information can confuse the system and provide unsatisfactory results.
With multimodal searches users can get better results than with a simple search, but multimodal searches must process more input information. It can also spend more time to process it and require more memory space.

An efficient search engine interprets the query of the users, realizes his/her intention and applies a strategy to use an appropriate search, i.e. the engine adapts to every input query and also to the combination of the elements and methods.

== Applications ==

Nowadays, existing multimodal search engines are not very complex, and some of them are in an experimental phase. Some of the more simple engines are [[Google Images]] [http://images.google.es/] or [[Bing (search engine)|Bing]] [http://www.bing.com], web interfaces that use text and images as inputs to find images in the output.

MMRetrieval [http://www.aviarampatzis.com/publications/p117-zagoris.pdf] is a multimodal experimental search engine that uses multilingual and multimedia information through a web interface. The engine searches the different inputs in parallel and merges all the results by different chosen methods. The engine also provides different multistage retrieval, as well as a single text index baseline to be able to compare all the different phases of search.

There are a lot of applications for mobile devices, using the context of the user, like based-location services, and using also text, images, audios or videos that the user provides at the moment or with saved files, or even interacting with the voice.

== References ==

* Query-Adaptive Fusion for Multimodal Search,Lyndon Kennedy, Student Member IEEE, Shih-Fu Chang, Fellow IEEE, and Apostol Natsev [http://www.ee.columbia.edu/~lyndon/pubs/pieee2008-queryadaptive.pdf]
* Context-aware Querying for Multimodal Search Engines, Jonas Etzold, Arnaud Brousseau, Paul Grimm and Thomas Steiner [http://www.lsi.upc.edu/~tsteiner/papers/2012/context-aware-querying-mmm2012.pdf]
* Apply Multimodal Search and Relevance Feedback In a Digital Video Library, Thesis of Yu Zhong [http://www.informedia.cs.cmu.edu/documents/zhong_thesis_may00.pdf]
* Aplicació rica d’internet per a la consulta amb text i imatge al repositori de vídeos de la Corporació Catalana de Mitjans Audiovisuals, Ramon Salla, Universitat Politècnica de Catalunya [http://upcommons.upc.edu/pfc/bitstream/2099.1/8766/1/PFC.pdf]

== External links ==
* MMRetrieval [http://www.mmretrieval.net]
* Google Images [http://images.google.es/]
* Bing [http://www.bing.com]

<!--- Categories --->
[[Category:Information retrieval genres]]
[[Category:Internet search engines]]
[[Category:Multimedia]]
<=====doc_Id=====>:686
<=====title=====>:
Concept search
<=====text=====>:
A '''concept search''' (or '''conceptual search''') is an automated [[information retrieval]] method that is used to search electronically stored [[unstructured data|unstructured text]] (for example, [[digital archive]]s, email, scientific literature, etc.) for information that is conceptually similar to the information provided in a search query.  In other words, the ''ideas'' expressed in the information retrieved in response to a [[concept]] search query are relevant to the ideas contained in the text of the query.

__TOC__

==Development==
Concept search techniques were developed because of limitations imposed by classical Boolean [[Search algorithm|keyword search]] technologies when dealing with large, unstructured digital collections of text.  Keyword searches often return results that include many non-relevant items ([[false positive]]s) or that exclude too many relevant items (false negatives) because of the effects of [[synonymy]] and [[polysemy]].  Synonymy means that one of two or more words in the same language have the same meaning, and polysemy means that many individual words have more than one meaning.

Polysemy is a major obstacle for all computer systems that attempt to deal with human language.  In English, most frequently used terms have several common meanings.  For example, the word fire can mean: a combustion activity; to terminate employment; to launch, or to excite (as in fire up).  For the 200 most-polysemous terms in English, the typical verb has more than twelve common meanings, or senses.  The typical noun from this set has more than eight common senses.  For the 2000 most-polysemous terms in English, the typical verb has more than eight common senses and the typical noun has more than five.<ref>Bradford, R. B., Word Sense Disambiguation, [[Content Analyst Company]], LLC, U.S. Patent 7415462, 2008.</ref>

In addition to the problems of polysemous and synonymy, keyword searches can exclude inadvertently [[misspelled]] words as well as the variations on the [[Stemming|stems]] (or roots) of words (for example, strike vs. striking).  Keyword searches are also susceptible to errors introduced by [[optical character recognition]] (OCR) scanning processes, which can introduce [[random error]]s into the text of documents (often referred to as [[noisy text]]) during the scanning process.

A concept search can overcome these challenges by employing [[word sense disambiguation]] (WSD),<ref>R. Navigli, [http://www.dsi.uniroma1.it/~navigli/pubs/ACM_Survey_2009_Navigli.pdf Word Sense Disambiguation: A Survey], ACM Computing Surveys, 41(2), 2009.</ref> and other techniques, to help it derive the actual meanings of the words, and their underlying concepts, rather than by simply matching character strings like keyword search technologies.

==Approaches==
In general, [[information retrieval]] research and technology can be divided into two broad categories: semantic and statistical. Information retrieval systems that fall into the semantic category will attempt to implement some degree of syntactic and [[Semantic analysis (machine learning)|semantic analysis]] of the [[natural language]] text that a human user would provide (also see [[computational linguistics]]).  Systems that fall into the statistical category will find results based on statistical measures of how closely they match the query.  However, systems in the semantic category also often rely on statistical methods to help them find and retrieve information.<ref>Greengrass, E., Information Retrieval: A Survey, 2000.</ref>

Efforts to provide information retrieval systems with semantic processing capabilities have basically used three different approaches:

* Auxiliary structures
* Local [[co-occurrence]] statistics
* Transform techniques (particularly [[matrix decomposition]]s)

===Auxiliary structures===
A variety of techniques based on [[artificial intelligence]] (AI) and [[natural language processing]] (NLP) have been applied to semantic processing, and most of them have relied on the use of auxiliary structures such as [[controlled vocabularies]] and [[Ontology (information science)|ontologies]].  Controlled vocabularies (dictionaries and thesauri), and ontologies allow broader terms, narrower terms, and related terms to be incorporated into queries.<ref>Dubois, C., The Use of Thesauri in Online Retrieval, Journal of Information Science, 8(2), 1984 March, pp. 63-66.</ref> Controlled vocabularies are one way to overcome some of the most severe constraints of Boolean keyword queries.  Over the years, additional auxiliary structures of general interest, such as the large synonym sets of [[WordNet]], have been constructed.<ref>Miller, G., Special Issue, [http://www.mit.edu/~6.863/spring2009/readings/5papers.pdf WordNet: An On-line Lexical Database], Intl. Journal of Lexicography, 3(4), 1990.</ref>  It was shown that concept search that is based on auxiliary structures, such as WordNet, can be efficiently implemented by reusing retrieval models and data structures of classical information retrieval.<ref>Fausto Giunchiglia, Uladzimir Kharkevich, and Ilya Zaihrayeu. [http://www.ulakha.com/concept-search-eswc2009.html Concept Search], In Proceedings of European Semantic Web Conference, 2009.</ref>  Later approaches have implemented grammars to expand the range of semantic constructs.  The creation of data models that represent sets of concepts within a specific domain (''domain ontologies''), and which can incorporate the relationships among terms, has also been implemented in recent years.

Handcrafted controlled vocabularies contribute to the efficiency and comprehensiveness of information retrieval and related text analysis operations, but they work best when topics are narrowly defined and the terminology is standardized.  Controlled vocabularies require extensive human input and oversight to keep up with the rapid evolution of language.  They also are not well suited to the growing volumes of unstructured text covering an unlimited number of topics and containing thousands of unique terms because new terms and topics need to be constantly introduced.  Controlled vocabularies are also prone to capturing a particular world view at a specific point in time, which makes them difficult to modify if concepts in a certain topic area change.<ref name="Bradford, R. B. 2008">Bradford, R. B., Why LSI? [[Latent Semantic Indexing]] and Information Retrieval, White Paper, [[Content Analyst Company]], LLC, 2008.</ref>

===Local co-occurrence statistics===
Information retrieval systems incorporating this approach count the number of times that groups of terms appear together (co-occur) within a [[sliding window]] of terms or sentences (for example, ± 5 sentences or ± 50 words) within a document.  It is based on the idea that words that occur together in similar contexts have similar meanings.  It is local in the sense that the sliding window of terms and sentences used to determine the co-occurrence of terms is relatively small.

This approach is simple, but it captures only a small portion of the semantic information contained in a collection of text.  At the most basic level, numerous experiments have shown that approximately only ¼ of the information contained in text is local in nature.<ref>Landauer, T., and Dumais, S., A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge, Psychological Review, 1997, 104(2), pp. 211-240.</ref>   In addition, to be most effective, this method requires prior knowledge about the content of the text, which can be difficult with large, unstructured document collections.<ref name="Bradford, R. B. 2008"/>

===Transform techniques===
Some of the most powerful approaches to semantic processing are based on the use of mathematical transform techniques.  [[Matrix decomposition]] techniques have been the most successful.  Some widely used matrix decomposition techniques include the following:<ref>Skillicorn, D., Understanding Complex Datasets: Data Mining with Matrix Decompositions, CRC Publishing, 2007.</ref>

* [[Independent component analysis]]
* Semi-discrete decomposition
* [[Non-negative matrix factorization]]
* [[Singular value decomposition]]

Matrix decomposition techniques are data-driven, which avoids many of the drawbacks associated with auxiliary structures.  They are also global in nature, which means they are capable of much more robust information extraction and representation of semantic information than techniques based on local co-occurrence statistics.<ref name="Bradford, R. B. 2008"/>

Independent component analysis is a technique that creates sparse representations in an automated fashion,<ref>Honkela, T., Hyvarinen, A. and Vayrynen, J. WordICA - Emergence of linguistic representations for words by independent component analysis. Natural Language Engineering, 16(3):277-308, 2010</ref> and the semi-discrete and non-negative matrix approaches sacrifice accuracy of representation in order to reduce computational complexity.<ref name="Bradford, R. B. 2008"/>

Singular value decomposition (SVD) was first applied to text at Bell Labs in the late 1980s. It was used as the foundation for a technique called [[latent semantic indexing]] (LSI) because of its ability to find the semantic meaning that is latent in a collection of text.  At first, the SVD was slow to be adopted because of the resource requirements needed to work with large datasets.  However, the use of LSI has significantly expanded in recent years as earlier challenges in scalability and performance have been overcome.  LSI is being used in a variety of information retrieval and text processing applications, although its primary application has been for concept searching and automated document categorization.<ref>Dumais, S., Latent Semantic Analysis, ARIST Review of Information Science and Technology, vol. 38, Chapter 4, 2004.</ref>

==Uses==
* '''[[eDiscovery]]''' – Concept-based search technologies are increasingly being used for Electronic Document Discovery (EDD or eDiscovery) to help enterprises prepare for litigation.  In eDiscovery, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis is much more efficient than traditional linear review techniques.  Concept-based searching is becoming accepted as a reliable and efficient search method that is more likely to produce relevant results than keyword or Boolean searches.<ref>Magistrate Judge John M. Facciola of the U.S. District Court for the District of Washington, D.C.
Disability Rights Council v. Washington Metropolitan Transit Authority, 242 FRD 139 (D. D.C. 2007), citing George L. Paul & Jason R. Baron, "Information Inflation: Can the Legal System Adapt?" 13 Rich. J.L. & Tech. 10 (2007).</ref>

* '''[[Enterprise Search]] and Enterprise Content Management (ECM)''' – Concept search technologies are being widely used in enterprise search.  As the volume of information within the enterprise grows, the ability to cluster, categorize, and search large collections of unstructured text on a conceptual basis has become essential.  In 2004 the Gartner Group estimated that professionals spend 30 percent of their time searching, retrieving, and managing information.<ref name="Laplanche, R. 2004">Laplanche, R., Delgado, J., Turck, M., Concept Search Technology Goes Beyond Keywords, Information Outlook, July 2004.</ref>  The research company IDC found that a 2,000-employee corporation can save up to $30 million per year by reducing the time employees spend trying to find information and duplicating existing documents.<ref name="Laplanche, R. 2004"/>
* '''[[Content-based image retrieval|Content-Based Image Retrieval (CBIR)]]''' – Content-based approaches are being used for the semantic retrieval of digitized images and video from large visual corpora.  One of the earliest content-based image retrieval systems to address the semantic problem was the ImageScape search engine.  In this system, the user could make direct queries for multiple visual objects such as sky, trees, water, etc. using spatially positioned icons in a WWW index containing more than ten million images and videos using keyframes.  The system used information theory to determine the best features for minimizing uncertainty in the classification.<ref name="Lew, M. S. 2006">Lew, M. S., Sebe, N., Djeraba, C., Jain, R., Content-based Multimedia Information Retrieval: State of the Art and Challenges, ACM Transactions on Multimedia Computing, Communications, and Applications, February 2006.</ref>  The semantic gap is often mentioned in regard to CBIR.  The semantic gap refers to the gap between the information that can be extracted from visual data and the interpretation that the same data have for a user in a given situation.<ref>Datta R., Joshi, D., Li J., Wang, J. Z., [http://infolab.stanford.edu/~wangz/project/imsearch/review/JOUR/datta.pdf Image Retrieval: Ideas, Influences, and Trends of the New Age], ACM Computing Surveys, Vol. 40, No. 2, April 2008.</ref>  The [http://www.liacs.nl/~mir ACM SIGMM Workshop on Multimedia Information Retrieval] is dedicated to studies of CBIR.
* '''Multimedia and Publishing''' – Concept search is used by the multimedia and publishing industries to provide users with access to news, technical information, and subject matter expertise coming from a variety of unstructured sources.  Content-based methods for multimedia information retrieval (MIR) have become especially important when text annotations are missing or incomplete.<ref name="Lew, M. S. 2006"/>
* '''Digital Libraries and Archives''' – Images, videos, music, and text items in digital libraries and digital archives are being made accessible to large groups of users (especially on the Web) through the use of concept search techniques.  For example, the Executive Daily Brief (EDB), a business information monitoring and alerting product developed by EBSCO Publishing, uses concept search technology to provide corporate end users with access to a digital library containing a wide array of business content.  In a similar manner, the [[Music Genome Project]] spawned Pandora, which employs concept searching to spontaneously create individual music libraries or ''virtual'' radio stations.
* '''Genomic Information Retrieval (GIR)''' – Genomic Information Retrieval (GIR) uses concept search techniques applied to genomic literature databases to overcome the ambiguities of scientific literature.
* '''Human Resources Staffing and Recruiting''' – Many human resources staffing and recruiting organizations have adopted concept search technologies to produce highly relevant resume search results that provide more accurate and relevant candidate resumes than loosely related keyword results.

==Effective searching==
The effectiveness of a concept search can depend on a variety of elements including the dataset being searched and the search engine that is used to process queries and display results. However, most concept search engines work best for certain kinds of queries:

* Effective queries are composed of enough text to adequately convey the intended concepts.  Effective queries may include full sentences, paragraphs, or even entire documents.  Queries composed of just a few words are not as likely to return the most relevant results.
* Effective queries do not include concepts in a query that are not the object of the search.  Including too many unrelated concepts in a query can negatively affect the relevancy of the result items.  For example, searching for information about ''boating on the Mississippi River'' would be more likely to return relevant results than a search for ''boating on the Mississippi River on a rainy day in the middle of the summer in 1967.''
* Effective queries are expressed in a full-text, natural language style similar in style to the documents being searched.  For example, using queries composed of excerpts from an introductory science textbook would not be as effective for concept searching if the dataset being searched is made up of advanced, college-level science texts.  Substantial queries that better represent the overall concepts, styles, and language of the items for which the query is being conducted are generally more effective.

As with all search strategies, experienced searchers generally refine their queries through multiple searches, starting with an initial ''seed'' query to obtain conceptually relevant results that can then be used to compose and/or refine additional queries for increasingly more relevant results.  Depending on the search engine, using query concepts found in result documents can be as easy as selecting a document and performing a ''find similar'' function.  Changing a query by adding terms and concepts to improve result relevance is called ''[[query expansion]]''.<ref>[[Stephen Robertson (computer scientist)|Robertson, S. E.]], [[Karen Spärck Jones|Spärck Jones, K.]], Simple, Proven Approaches to Text Retrieval, Technical Report, University of Cambridge Computer Laboratory, December 1994.</ref> The use of [[ontology (information science)|ontologies]] such as WordNet has been studied to expand queries with conceptually-related words.<ref>Navigli, R., Velardi, P. [http://www.dcs.shef.ac.uk/~fabio/ATEM03/navigli-ecml03-atem.pdf An Analysis of Ontology-based Query Expansion Strategies]. ''Proc. of Workshop on Adaptive Text Extraction and Mining (ATEM 2003)'', in the ''14th European Conference on Machine Learning (ECML 2003)'', Cavtat-Dubrovnik, Croatia, September 22-26th, 2003, pp.&nbsp;42–49</ref>

==Relevance feedback==
[[Relevance feedback]] is a feature that helps users determine if the results returned for their queries meet their information needs.  In other words, relevance is assessed relative to an information need, not a query.  A document is relevant if it addresses the stated information need, not because it just happens to contain all the words in the query.<ref name="Manning, C. D. 2008">Manning, C. D., Raghavan P., Schütze H., Introduction to Information Retrieval, Cambridge University Press, 2008.</ref>   It is a way to involve users in the retrieval process in order to improve the final result set.<ref name="Manning, C. D. 2008"/> Users can refine their queries based on their initial results to improve the quality of their final results.

In general, concept search relevance refers to the degree of similarity between the concepts expressed in the query and the concepts contained in the results returned for the query.  The more similar the concepts in the results are to the concepts contained in the query, the more relevant the results are considered to be.  Results are usually ranked and sorted by relevance so that the most relevant results are at the top of the list of results and the least relevant results are at the bottom of the list.

Relevance feedback has been shown to be very effective at improving the relevance of results.<ref name="Manning, C. D. 2008"/>   A concept search decreases the risk of missing important result items because all of the items that are related to the concepts in the query will be returned whether or not they contain the same words used in the query.<ref name="Laplanche, R. 2004"/>

[[Ranking]] will continue to be a part of any modern information retrieval system.  However, the problems of heterogeneous data, scale, and non-traditional discourse types reflected in the text, along with the fact that search engines will increasingly be integrated components of complex information management processes, not just stand-alone systems, will require new kinds of system responses to a query.  For example, one of the problems with ranked lists is that they might not reveal relations that exist among some of the result items.<ref name="Callan, J. 2007">Callan, J., Allan, J., Clarke, C. L. A., Dumais, S., Evans, D., A., Sanderson, M., Zhai, C., Meeting of the MINDS: An Information Retrieval Research Agenda, ACM, SIGIR Forum, Vol. 41 No. 2, December 2007.</ref>

==Guidelines for evaluating a concept search engine==
# Result items should be relevant to the information need expressed by the concepts contained in the query statements, even if the terminology used by the result items is different from the terminology used in the query.
# Result items should be sorted and ranked by relevance.
# Relevant result items should be quickly located and displayed.  Even complex queries should return relevant results fairly quickly.
# Query length should be ''non-fixed'', i.e., a query can be as long as deemed necessary.  A sentence, a paragraph, or even an entire document can be submitted as a query.
# A concept query should not require any special or complex syntax.  The concepts contained in the query can be clearly and prominently expressed without using any special rules.
# Combined queries using concepts, keywords, and metadata should be allowed.
# Relevant portions of result items should be usable as query text simply by selecting the item and telling the search engine to ''find similar'' items.
# Query-ready indexes should be created relatively quickly.
# The search engine should be capable of performing Federated searches.  Federated searching enables concept queries to be used for simultaneously searching multiple datasources for information, which are then merged, sorted, and displayed in the results.
# A concept search should not be affected by misspelled words, typographical errors, or OCR scanning errors in either the query text or in the text of the dataset being searched.

==Conferences and forums==
Formalized search engine evaluation has been ongoing for many years.  For example, the [[Text Retrieval Conference|Text REtrieval Conference (TREC)]] was started in 1992 to support research within the information retrieval community by providing the infrastructure necessary for large-scale evaluation of text retrieval methodologies.  Most of today's commercial search engines include technology first developed in TREC.<ref>Croft, B., Metzler, D., Strohman, T., Search Engines, Information Retrieval in Practice, Addison Wesley, 2009.</ref>

In 1997, a Japanese counterpart of TREC was launched, called National Institute of Informatics Test Collection for IR Systems (NTCIR).  NTCIR conducts a series of evaluation workshops for research in information retrieval, question answering, text summarization, etc.  A European series of workshops called the Cross Language Evaluation Forum (CLEF) was started in 2001 to aid research in multilingual information access.  In 2002, the Initiative for the Evaluation of XML Retrieval (INEX) was established for the evaluation of content-oriented XML retrieval systems.

Precision and recall have been two of the traditional performance measures for evaluating information retrieval systems.  Precision is the fraction of the retrieved result documents that are relevant to the user's information need.  Recall is defined as the fraction of relevant documents in the entire collection that are returned as result documents.<ref name="Manning, C. D. 2008"/>

Although the workshops and publicly available test collections used for search engine testing and evaluation have provided substantial insights into how information is managed and retrieved, the field has only scratched the surface of the challenges people and organizations face in finding, managing, and, using information now that so much information is available.<ref name="Callan, J. 2007"/>   Scientific data about how people use the information tools available to them today is still incomplete because experimental research methodologies haven’t been able to keep up with the rapid pace of change. Many challenges, such as contextualized search, personal information management, information integration, and task support, still need to be addressed.<ref name="Callan, J. 2007"/>

==See also==
{{div col|3}}
* [[Approximate string matching]]
* [[Compound term processing]]
* [[Concept mining]]
* [[Information extraction]]
* [[Latent semantic analysis]]
* [[Semantic network]]
* [[Semantic search]]
* [[Semantic Web]]
* [[Statistical semantics]]
* [[Text mining]]
{{div col end}}

==References==
{{Reflist|2}}

==External links==
* [http://trec.nist.gov/ Text Retrieval Conference (TREC)]
* [http://research.nii.ac.jp/ntcir/ National Institute of Informatics Test Collection for IR Systems (NTCIR)]
* [http://www.clef-campaign.org/ Cross Language Evaluation Forum (CLEF)]
* [http://inex.is.informatik.uni-duisburg.de/ Initiative for the Evaluation of XML Retrieval (INEX)]

[[Category:Information retrieval genres]]
<=====doc_Id=====>:689
<=====title=====>:
Question answering
<=====text=====>:
{{other uses|question|answer}}
{{multiple issues|
{{cleanup|date=January 2012|reason='''appearance of plagiarised text (now in extensive footnote), use of draft rather than published sources, extensive appearance of text violating [[WP:VERIFY]] and/or [[WP:OR]], use of jargon to define jargon, etc.'''}}
{{cleanup rewrite|date=January 2012}}
{{more footnotes|date=February 2014}}
{{citation style|date=January 2016}}
}}

'''Question Answering''' ('''QA''') is a computer science discipline within the fields of [[information retrieval]] and [[natural language processing]] (NLP), which is concerned with building systems that automatically answer questions posed by humans in a [[natural language]].

A QA implementation, usually a computer program, may construct its answers by querying a structured [[database]] of knowledge or information, usually a [[knowledge base]]. More commonly, QA systems can pull answers from an unstructured collection of natural language documents.

Some examples of natural language document collections used for QA systems include:
* a local collection of reference texts
* internal organization documents and web pages
* compiled [[newswire]] reports
* a set of [[Wikipedia]] pages
* a subset of [[World Wide Web]] pages

 with a wide range of question types including: fact, list, [[definition]], ''How'', ''Why'', hypothetical, semantically constrained, and cross-lingual questions.

* ''Closed-domain'' question answering deals with questions under a specific domain (for example, medicine or automotive maintenance), and can be seen as an easier task because NLP systems can exploit domain-specific knowledge frequently formalized in [[Ontology (computer science)|ontologies]]. Alternatively, ''closed-domain'' might refer to a situation where only a limited type of questions are accepted, such as questions asking for [[descriptive knowledge|descriptive]] rather than [[procedural knowledge|procedural]] information. QA systems in the context of machine reading applications have also been constructed in the medical domain, for instance related to Alzheimers disease <ref>Roser Morante , Martin Krallinger , Alfonso Valencia and  Walter Daelemans. Machine Reading of Biomedical Texts about Alzheimer's Disease. CLEF 2012 Evaluation Labs and Workshop. September 17, 2012</ref>
* ''[[Open domain#References|Open-domain]]'' question answering deals with questions about nearly anything, and can only rely on general ontologies and world knowledge. On the other hand, these systems usually have much more data available from which to extract the answer.

==History==
{{unreferenced section|date=January 2016}}
Two early QA systems were BASEBALL<ref>{{cite journal|last1=GREEN JR|first1=Bert F|title=Baseball: an automatic question-answerer.|journal=western joint IRE-AIEE-ACM computer conference|date=1961|pages=219–224|display-authors=etal}}</ref> and LUNAR.<ref>{{cite journal|last1=Woods|first1=William A|last2=Kaplan|first2=R.|title=Lunar rocks in natural English: Explorations in natural language question answering|journal=Linguistic structures processing 5|date=1977|volume=5|pages=521–569}}</ref> BASEBALL answered questions about the US baseball league over a period of one year. LUNAR, in turn, answered questions about the geological analysis of rocks returned by the Apollo moon missions. Both QA systems were very effective in their chosen domains. In fact, LUNAR was demonstrated at a lunar science convention in 1971 and it was able to answer 90% of the questions in its domain posed by people untrained on the system. Further restricted-domain QA systems were developed in the following years. The common feature of all these systems is that they had a core database or knowledge system that was hand-written by experts of the chosen domain. The language abilities of BASEBALL and LUNAR used techniques similar to [[ELIZA]] and [[DOCTOR]], the first [[chatterbot]] programs.

[[SHRDLU]] was a highly successful question-answering program developed by [[Terry Winograd]] in the late 60s and early 70s. It simulated the operation of a robot in a toy world (the "blocks world"), and it offered the possibility to ask the robot questions about the state of the world. Again, the strength of this system was the choice of a very specific domain and a very simple world with rules of physics that were easy to encode in a computer program.

In the 1970s, [[knowledge base]]s were developed that targeted narrower domains of knowledge. The QA systems developed to interface with treport[[expert system|<nowiki/>]]s produced more repeatable and valid responses to questions within an area of knowledge. These [[expert systems]] closely resembled modern QA systems except in their internal architecture. Expert systems rely heavily on expert-constructed and organized [[knowledge base]]s, whereas many modern QA systems rely on statistical processing of a large, unstructured, natural language text corpus.

The 1970s and 1980s saw the development of comprehensive theories in [[computational linguistics]], which led to the development of ambitious projects in text comprehension and question answering. One example of such a system was the Unix Consultant (UC), developed by [[Robert Wilensky]] at [[U.C. Berkeley]] in the late 1980s. The system answered questions pertaining to the [[Unix]] operating system. It had a comprehensive hand-crafted knowledge base of its domain, and it aimed at phrasing the answer to accommodate various types of users. Another project was LILOG, a text-understanding system that operated on the domain of tourism information in a German city. The systems developed in the UC and LILOG projects never went past the stage of simple demonstrations, but they helped the development of theories on computational linguistics and reasoning.

Recently, specialized natural language QA systems have been developed, such as [http://bitem.hesge.ch/content/eagli-eagle-eye EAGLi] for health and life scientists.

==Architecture==
{{refimprove section|date=January 2016}}
Most modern QA systems use [[natural language]] text documents as their underlying knowledge source.{{citation needed|date=January 2016}}  [[Natural language processing]] techniques are used to both process the question and index or process the text [[Text corpus|corpus]] from which answers are extracted.{{citation needed|date=January 2016}} An increasing number of QA systems use the [[World Wide Web]] as their corpus of text and knowledge; however, many of these tools do not produce a human-like answer, but rather employ "shallow" methods (keyword-based techniques, templates, etc.) to produce a list of documents or a list of document excerpts containing the probable answer highlighted.{{citation needed|date=January 2016}}

In an alternative QA implementation, human users assemble knowledge in a structured database, called a [[knowledge base]], similar to those employed in the [[expert systems]] of the 1970s.{{citation needed|date=January 2016}} It is also possible to employ a combination of structured databases and natural language text documents in a hybrid QA system.{{citation needed|date=January 2016}} Such a hybrid system may employ data mining algorithms to populate a structured knowledge base that is also populated and edited by human contributors.{{citation needed|date=January 2016}} An example hybrid QA system is the [[Wolfram Alpha]] QA system which employs natural language processing to transform human questions into a form that is processed by a curated knowledge base.{{citation needed|date=January 2016}}

As of 2001, QA systems typically included a ''question classifier'' module that determines the type of question and the type of answer.<ref>Hirschman, L. & Gaizauskas, R. (2001) [http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=96167 Natural Language Question Answering. The View from Here]. Natural Language Engineering (2001), 7:4:275-300 Cambridge University Press.</ref> After the question is analysed, the system typically uses several modules that apply increasingly complex NLP techniques on a gradually reduced amount of text; thus, a ''document retrieval module'' uses [[search engine]]s to identify the documents or paragraphs in the document set that are likely to contain the answer, and a ''filter'' preselects small text fragments that contain strings of the same type as the expected answer.{{citation needed|date=January 2016}} For example, if the question is "Who invented
penicillin?", the filter returns text that contain names of people. Finally, an ''answer extraction'' module looks for further clues in the text to determine if the answer candidate can indeed answer the question.{{citation needed|date=January 2016}}

A ''multiagent'' question-answering architecture has been proposed, where each domain is represented by an agent which tries to answer questions taking into account its specific knowledge; a meta–agent controls the cooperation between question answering agents and chooses the most relevant answer(s).<ref>{{vcite journal |author=Galitsky B, Pampapathi R|title=Can many agents answer questions better than one|journal=First Monday |volume = 10| Number=1 |date=2005 | url = http://firstmonday.org/ojs/index.php/fm/article/view/1204/1124 |doi=10.5210/fm.v10i1.1204
}}</ref>

==Question answering methods==
QA is very dependent on a good search [[text corpus|corpus]] - for without documents containing the answer, there is little any QA system can do. It thus makes sense that larger collection sizes generally lend well to better QA performance, unless the question domain is orthogonal to the collection. The notion of [[data redundancy]] in massive collections, such as the web, means that nuggets of information are likely to be phrased in many different ways in differing contexts and documents,<ref>Lin, J. (2002). The Web as a Resource for Question Answering: Perspectives and Challenges. In Proceedings of the Third International Conference on Language Resources and Evaluation (LREC 2002).</ref> leading to two benefits:
# By having the right information appear in many forms, the burden on the QA system to perform complex NLP techniques to understand the text is lessened.
# Correct answers can be filtered from [[false positive]]s by relying on the correct answer to appear more times in the documents than instances of incorrect ones.

Question answering heavily relies on [[reasoning]]. There are a number of question answering systems designed in [[Prolog]],<ref>{{cite book |last=Galitsky |first=Boris |title=Natural Language Question Answering System: Technique of Semantic Headers |url=https://books.google.com/books?id=LkNmAAAACAAJ |series=International Series on Advanced Intelligence |volume=Volume 2 |year=2003 |publisher=Advanced Knowledge International |location=Australia |isbn=978-0-86803-979-4}}</ref> a [[logic programming]] language associated with [[artificial intelligence]].

===Open domain question answering===
{{unreferenced section|date=January 2016}}
In [[information retrieval]], an open domain question answering system aims at returning an answer in response to the user's question. The returned answer is in the form of short texts rather than a list of relevant documents. The system uses a combination of techniques from [[computational linguistics]], [[information retrieval]] and [[knowledge representation]] for finding answers.

The system takes a [[natural language]] question as an input rather than a set of keywords, for example, "When is the national day of China?" The sentence is then transformed into a query through its [[logical form]]. Having the input in the form of a natural language question makes the system more user-friendly, but harder to implement, as there are various question types and the system will have to identify the correct one in order to give a sensible answer. Assigning a question type to the question is a crucial task, the entire answer extraction process relies on finding the correct question type and hence the correct answer type.

Keyword [[Data extraction|extraction]] is the first step for identifying the input question type. In some cases, there are clear words that indicate the question type directly. i.e. "Who", "Where" or "How many", these words tell the system that the answers should be of type "Person", "Location", "Number" respectively. In the example above, the word "When" indicates that the answer should be of type "Date". POS (Part of Speech) tagging and syntactic parsing techniques can also be used to determine the answer type. In this case, the subject is "Chinese National Day", the predicate is "is" and the adverbial modifier is "when", therefore the answer type is "Date". Unfortunately, some interrogative words like "Which", "What" or "How" do not give clear answer types. Each of these words can represent more than one type. In situations like this, other words in the question need to be considered. First thing to do is to find the words that can indicate the meaning of the question. A lexical dictionary such as [[WordNet]] can then be used for understanding the context.

Once the question type has been identified, an [[Information retrieval]] system is used to find a set of documents containing the correct key words. A tagger and NP/Verb Group chunker can be used to verify whether the correct entities and relations are mentioned in the found documents. For questions such as "Who" or "Where", a Named Entity Recogniser is used to find relevant "Person" and "Location" names from the retrieved documents. Only the relevant paragraphs are selected for ranking.

A [[vector space model]] can be used as a strategy for classifying the candidate answers. Check if the answer is of the correct type as determined in the question type analysis stage. Inference technique can also be used to validate the candidate answers. A score is then given to each of these candidates according to the number of question words it contains and how close these words are to the candidate, the more and the closer the better. The answer is then translated into a compact and meaningful representation by parsing. In the previous example, the expected output answer is "1st Oct."

==Issues==
In 2002, a group of researchers presented an unpublished and largely unsourced report as a funding support document, in which they describe a 5-year roadmap of research current to the state of the question answering filed at that time.<ref>Burger, J., Cardie, C., Chaudhri, V., Gaizauskas, R., Harabagiu, S., Israel, D., Jacquemin, C., Lin, C-Y., Maiorano, S., Miller, G., Moldovan, D., Ogden, B., Prager, J., Riloff, E., Singhal, A., Shrihari, R., Strzalkowski, T., Voorhees, E., Weishedel, R., date unknown, "Tasks and Program Structures to Roadmap Research in Question Answering (QA)," at [http://www-nlpir.nist.gov/projects/duc/papers/qa.Roadmap-paper_v2.doc Issues] [DRAFT DOCUMENT], accessed 1 January 2016.</ref><ref>Here is some content taken verbatim from that roadmap (see preceding citation): "[1] Question classes: Different types of questions (e.g., "What is the capital of [[Liechtenstein]]?" vs. "Why does a [[rainbow]] form?" vs. "Did [[Marilyn Monroe]] and [[Cary Grant]] ever appear in a movie together?") require the use of different strategies to find the answer. Question classes are arranged hierarchically in taxonomies.{{example needed|date=February 2011}} [2] Question processing: The same information request can be expressed in various ways, some interrogative ("Who is the King of Lesotho?") and some assertive ("Tell me the name of the King of Lesotho."). A semantic model of question understanding and processing would recognize equivalent questions, regardless of how they are presented. This model would enable the translation of a complex question into a series of simpler questions, would identify ambiguities and treat them in context or by interactive clarification. [3] Context and QA : Questions are usually asked within a context and answers are provided within that specific context. The context can be used to clarify a question, resolve ambiguities or keep track of an investigation performed through a series of questions. (For example, the question, "Why did Joe Biden visit Iraq in January 2010?" might be asking why Vice President Biden visited and not President Obama, why he went to Iraq and not Afghanistan or some other country, why he went in January 2010 and not before or after, or what Biden was hoping to accomplish with his visit. If the question is one of a series of related questions, the previous questions and their answers might shed light on the questioner's intent.) [4] Data sources for QA: Before a question can be answered, it must be known what knowledge sources are available and relevant. If the answer to a question is not present in the data sources, no matter how well the question processing, information retrieval and answer extraction is performed, a correct result will not be obtained. [4] Answer extraction: Answer extraction depends on the complexity of the question, on the answer type provided by question processing, on the actual data where the answer is searched, on the search method and on the question focus and context.{{example needed|date=February 2011}} [5] Answer formulation: The result of a QA system should be presented in a way as natural as possible. In some cases, simple extraction is sufficient. For example, when the question classification indicates that the answer type is a name (of a person, organization, shop or disease, etc.), a quantity (monetary value, length, size, distance, etc.) or a date (e.g. the answer to the question, "On what day did Christmas fall in 1989?") the extraction of a single datum is sufficient. For other cases, the presentation of the answer may require the use of fusion techniques that combine the partial answers from multiple documents. [6] Real time question answering: There is need for developing Q&A systems that are capable of extracting answers from large data sets in several seconds, regardless of the complexity of the question, the size and multitude of the data sources or the ambiguity of the question. [7] Multilingual (or cross-lingual) question answering: The ability to answer a question posed in one language using an answer corpus in another language (or even several). This allows users to consult information that they cannot use directly. (See also [[Machine translation]].) [8] Interactive QA: It is often the case that the information need is not well captured by a QA system, as the question processing part may fail to classify properly the question or the information needed for extracting and generating the answer is not easily retrieved. In such cases, the questioner might want not only to reformulate the question, but to have a dialogue with the system. In addition, system may also use previously answered questions. (For example, the system might ask for a clarification of what sense a word is being used, or what type of information is being asked for.) [9] Advanced reasoning for QA: More sophisticated questioners expect answers that are outside the scope of written texts or structured databases. To upgrade a QA system with such capabilities, it would be necessary to integrate reasoning components operating on a variety of knowledge bases, encoding world knowledge and common-sense reasoning mechanisms, as well as knowledge specific to a variety of domains. [[Evi (software)|Evi]] is an example of such as system. [10] Information clustering for QA: Information clustering for question answering systems is a new trend that originated to increase the accuracy of question answering systems through search space reduction. In recent years this was widely researched through development of question answering systems which support information clustering in their basic flow of process. [11] User profiling for QA: The user profile captures data about the questioner, comprising context data, domain of interest, reasoning schemes frequently used by the questioner, common ground established within different dialogues between the system and the user, and so forth. The profile may be represented as a predefined template, where each template slot represents a different profile feature. Profile templates may be nested one within another.{{example needed|date=February 2011}} [12] Deep Question Answering: Deep QA complement traditional Question Answering by adding some machine learning capabilities within a standard factoid question answering pipeline. The idea is to leverage curated data repositories or knowledge bases, which can be general ones such as Wikipedia, or domain-specific (e.g. molecular biology) in order to provide more accurate answers to the end-users.</ref>
<ref>On the subject of interactive QA, see also Perera, R. and Nand, P. (2014). "Interaction History Based Answer Formulation for Question Answering," at [http://rivinduperera.com/publications/kesw2014.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}</ref>{{full citation needed|date=January 2016}}
<ref>On the subject of information clustering for QA, see also Perera, R. (2012). "IPedagogy: Question Answering System Based on Web Information Clustering," at [http://rivinduperera.com/publications/t4e2012.html] [DRAFT DOCUMENT], accessed 1 January 2015.{{full citation needed|date=January 2016}}</ref>{{full citation needed|date=January 2016}}
<ref>On the subject of deep question answering, see the following citation.</ref><ref>{{cite journal | pmc = 4572360 | pmid=26384372 | doi=10.1093/database/bav081 | volume=2015 | title=Deep Question Answering for protein annotation | year=2015 | journal=Database (Oxford) |vauthors=Gobeill J, Gaudinat A, Pasche E, Vishnyakova D, Gaudet P, Bairoch A, Ruch P }}</ref>
<!-- 
Because much of the text in this section was copied and pasted from the "roadmap" document, which itself is a draft and unpublished document, the text was moved into a footnote. 
-->

==Progress==
QA systems have been extended in recent years to encompass additional domains of knowledge<ref>Maybury, M. T. editor. 2004. [http://www.mitpressjournals.org/doi/pdf/10.1162/089120105774321055 New Directions in Question Answering.] AAAI/MIT Press.</ref>  For example, systems have been developed to automatically answer temporal and geospatial questions, questions of definition and terminology, biographical questions, multilingual questions, and questions about the content of audio, images, and video. Current QA research topics include:

* interactivity—clarification of questions or answers
* answer reuse or caching
* knowledge representation and reasoning
* social media analysis with QA systems
* [[sentiment analysis]]<ref>{{webarchive |url=https://web.archive.org/web/20121027153311/http://totalgood.com/bitcrawl/ |date=October 27, 2012 |title=BitCrawl by Hobson Lane }}</ref>
* utilization of thematic roles<ref>Perera, R. and Perera, U. 2012. [http://rivinduperera.com/publications/qacd_coling2012.html Towards a thematic role based target identification model for question answering.]</ref>
* semantic resolution: to bridge the gap between syntactically different questions and answer-bearing texts<ref>{{cite conference |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes | year=2008 | conference=The 30th European Conference on Information Retrieval (ECIR'08)| pages= 430–437 | publisher=Springer Berlin Heidelberg | url=http://link.springer.com/chapter/10.1007/978-3-540-78646-7_40 | title=The impact of semantic class identification and semantic role labeling on natural language answer extraction}}</ref>
* utilization of linguistic resources,<ref>{{cite journal |author1=Bahadorreza Ofoghi |author2=John Yearwood |author3=Liping Ma  |last-author-amp=yes |title=The impact of frame semantic annotation levels, frame‐alignment techniques, and fusion methods on factoid answer processing | journal=Journal of the American Society for Information Science and Technology |volume=60 |issue=2 |pages=247–263 |year =2009 |url=http://onlinelibrary.wiley.com/doi/10.1002/asi.20989/abstract;jsessionid=099F3D167FD0511A48FB1C19C1060676.f02t02?deniedAccessCustomisedMessage=&userIsAuthenticated=false |doi=10.1002/asi.20989}}</ref> such as [[WordNet]], [[FrameNet]], and the similar

IBM's question answering system, [[Watson (computer)|Watson]], defeated the two greatest Jeopardy champions, Brad Rutter and Ken Jennings, by a significant margin.
<ref>http://www.nytimes.com/2011/02/17/science/17jeopardy-watson.html?_r=0</ref>

==References==
{{reflist}}

==Further reading==
{{citation style|section|date=January 2016}}
* Dragomir R. Radev, John Prager, and Valerie Samn. [http://clair.si.umich.edu/~radev/papers/anlp00.pdf Ranking suspected answers to natural language questions using predictive annotation]. In Proceedings of the 6th Conference on Applied Natural Language Processing, Seattle, WA, May 2000.
* John Prager, Eric Brown, Anni Coden, and Dragomir Radev. [http://clair.si.umich.edu/~radev/papers/sigir00.pdf Question-answering by predictive annotation]. In Proceedings, 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Athens, Greece, July 2000.
*{{cite book | last = Hutchins | first = W. John | authorlink = John Hutchins |author2=Harold L. Somers  | year = 1992 | title = An Introduction to Machine Translation | url = http://www.hutchinsweb.me.uk/IntroMT-TOC.htm | publisher = Academic Press | location = London | isbn = 0-12-362830-X}}
* L. Fortnow, Steve Homer (2002/2003).   [http://people.cs.uchicago.edu/~fortnow/papers/history.pdf A Short History of Computational Complexity].  In D. van Dalen, J. Dawson, and A. Kanamori, editors, ''The History of Mathematical Logic''. North-Holland, Amsterdam.

==External links==
* [http://aclia.lti.cs.cmu.edu/ntcir8 Question Answering Evaluation at NTCIR]
* [http://trec.nist.gov/data/qamain.html Question Answering Evaluation at TREC]
* [http://nlp.uned.es/clef-qa/ Question Answering Evaluation at CLEF]
* [http://www.gyanibano.com Quiz Question Answers]

{{Computable knowledge}}
{{Natural Language Processing}}


[[Category:Artificial intelligence applications]]
[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Information retrieval genres]]
<=====doc_Id=====>:692
<=====title=====>:
Exploratory search
<=====text=====>:
'''Exploratory search''' is a specialization of information exploration which represents the activities carried out by searchers who are:<ref>Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.</ref>
* unfamiliar with the domain of their goal (i.e. need to learn about the topic in order to understand how to achieve their goal)
* or unsure about the ways to achieve their goals (either the technology or the process)
* or unsure about their goals in the first place.

Consequently, exploratory search covers a broader class of activities than typical [[information retrieval]], such as investigating, evaluating, comparing, and synthesizing, where new information is sought in a defined conceptual area; [[exploratory data analysis]] is another example of an information exploration activity. Typically, therefore, such users generally combine querying and browsing strategies to foster learning and investigation.

==History==
Exploratory search is a topic that has grown from the fields of [[information retrieval]] and [[information seeking]] but has become more concerned with alternatives to the kind of search that has received the majority of focus (returning the most relevant documents to a [[Google]]-like keyword search). The research is motivated by questions like "what if the user doesn't know which keywords to use?" or "what if the user isn't looking for a single answer?". Consequently, research has begun to focus on defining the broader set of ''information behaviors'' in order to learn about the situations when a user is, or feels, limited by only having the ability to perform a keyword search.

In the last few years,{{When|date=April 2016}} a series of workshops has been held at various related and key events. In 2005, the Exploratory Search Interfaces workshop focused on beginning to define some of the key challenges in the field.<ref>{{cite web|url=http://research.microsoft.com/~ryenw/xsi/index.html|title=HCIL SOH 2005 Workshop on Exploratory Search Interfaces|publisher=Microsoft|accessdate=8 April 2016}}</ref> Since then a series of other workshops has been held at related conferences: Evaluating Exploratory Search<ref>{{cite web|url=http://research.microsoft.com/~ryenw/eess/index.html|title=SIGIR 2006 Workshop - Evaluating Exploratory Search Systems|publisher=Microsoft|accessdate=8 April 2016}}</ref> at SIGIR06<ref>{{cite web|url=http://www.sigir2006.org|title=Sigir 2006|publisher=|accessdate=8 April 2016}}</ref> and Exploratory Search and HCI<ref>{{cite web|url=http://research.microsoft.com/~ryenw/esi/index.html|title=CHI 2007 Workshop - Exploratory Search and HCI|publisher=Microsoft|accessdate=8 April 2016}}</ref> at CHI07<ref>{{cite web|url=http://www.chi2007.org|title=CHI 2007 Reach Beyond - welcome|publisher=|accessdate=8 April 2016}}</ref> (in order to meet with the experts in [[human–computer interaction]]).

In March 2008, an ''Information Processing and Management'' special issue<ref>{{cite web|url=http://www.sciencedirect.com/science/journal/03064573|title=Information Processing & Management|publisher=|accessdate=8 April 2016}}</ref><ref>Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&nbsp;433–436</ref> focused particularly on the challenges of evaluating exploratory search, given the reduced assumptions that can be made about scenarios of use.

In June 2008, the [[National Science Foundation]] sponsored an invitational workshop to identify a research agenda for exploratory search and similar fields for the coming years.<ref>{{cite web|url=http://www.ils.unc.edu/ISSS_workshop/|title=Moved|publisher=|accessdate=8 April 2016}}</ref>

==Research challenges==

===Important scenarios===
With the majority of research in the [[information retrieval]] community focusing on typical keyword search scenarios, one challenge for exploratory search is to further understand the scenarios of use for when keyword search is not sufficient. An example scenario, often used to motivate the research by mSpace,<ref>[http://mspace.fm mSpace]</ref> states: if a user does not know much about classical music, how should they even begin to find a piece that they might like.

===Designing new interfaces===
With one of the motivations being to support users when keyword search is not enough, some research has focused on identifying alternative user interfaces and interaction models that support the user in different ways. An example is [[Faceted classification|faceted search]] which presents diverse category-style options to the users, so that they can choose from a list instead of guess a possible keyword query.

Many of the [[human–computer information retrieval|interactive forms of search]], including [[faceted browser]]s, are being considered for their support of exploratory search conditions.

Computational cognitive models of exploratory search have been developed to capture the cognitive complexities involved in exploratory search. Model-based dynamic presentation of information cues are proposed to facilitate exploratory search performance.<ref>Fu, W.-T., Kannampalill, T. G., & Kang, R. (2010). [http://portal.acm.org/citation.cfm?id=1719970.1719998 Facilitating exploratory search by model-based navigational cues.] In Proceedings of the ACM International conference on Intelligent User Interface. 199–208. </ref>

===Evaluating interfaces===
As the tasks and goals involved with exploratory search are largely undefined or unpredictable, it is very hard to evaluate systems with the measures often used in information retrieval. Accuracy was typically used to show that a user had found a correct answer, but when the user is trying to summarize a domain of information, the ''correct'' answer is near impossible to identify, if not entirely subjective (for example: possible hotels to stay in Paris). In exploration, it is also arguable that spending more time (where time efficiency is typically desirable) researching a topic shows that a system provides increased support for investigation. Finally, and perhaps most importantly, giving study participants a well specified task could immediately prevent them from exhibiting exploratory behavior.{{cn|date=April 2016}}

===Models of exploratory search behavior===
There have been recent{{When|date=April 2016}} attempts to develop a process model of exploratory search behavior, especially in social information system (e.g., see [[models of collaborative tagging]].<ref>{{Citation
  | doi = 10.1145/1460563.1460600
  | last1 = Fu  | first1 = Wai-Tat
  | title = The Microstructures of Social Tagging: A Rational Model
  | journal = Proceedings of the ACM 2008 conference on Computer Supported Cooperative Work.
  | pages = 66–72
  | date = April 2008
  | url = http://portal.acm.org/citation.cfm?id=1460600
  | isbn = 978-1-60558-007-4 }}
</ref>
<ref>{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | title = A Semantic Imitation Model of Social Tagging
  | journal = Proceedings of the IEEE conference on Social Computing
  | pages = 66–72
  | date = Aug 2009
  | url = http://www.humanfactors.illinois.edu/Reports&PapersPDFs/IEEESocialcom09/A%20Semantic%20Imitation%20Model%20of%20Social%20Tag%20Choices%20(2).pdf }}</ref> The process model assumes that user-generated information cues, such as social tags, can act as navigational cues that facilitate exploration of information that others have found and shared with other users on a social information system (such as [[social bookmarking]] system). These models provided extension to existing process model of information search that characterizes information-seeking behavior in traditional fact-retrievals using search engines.<ref>
{{Citation
  | last1 = Fu  | first1 = Wai-Tat
  | last2 = Pirolli  | first2 = Peter
  | title = SNIF-ACT: a cognitive model of user navigation on the world wide web
  | journal = Human-Computer Interaction
  | pages = 335–412
  | year = 2007
  | url = http://portal.acm.org/citation.cfm?id=1466608
  | volume = 22}}</ref><ref>Kitajima, M., Blackmon, M. H., & Polson, P. G. (2000). A comprehension-based model of Web navigation and its application to Web usability analysis. In S. Mc-Donald, Y. Waern, & G. Cockton (Eds.), People and computers XIV—Usability or else!
New York: Springer-Verlag.</ref><ref>Miller, C. S., & Remington, R.W. (2004). Modeling information navigation: Implications for information architecture. Human Computer Interaction, 19, 225–271.</ref>
Recent{{When|date=April 2016}} development in exploratory search is often concentrated in predicting users' search intents in interaction with the user.<ref>
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Athukorala  | first2 = Kumaripaba
  | last3 = Glowacka  | first3 = Dorota
  | last4 = Konuyshkova  | first4 = Ksenia
  | last5 = Oulasvrita  | first5 = Antti
  | last6 = Kaipiainen  | first6 = Samuli
  | last7 = Kaski  | first7 = Samuel
  | last8 = Jacucci  | first8 = Giulio
  | title = Supporting exploratory search tasks with interactive user modeling
  | journal = Proceedings of the 76th Annual Meeting of the American Society for Information Science and Technology ASIS&T
  | year = 2013}}
</ref>
Such predictive user modeling, also referred as intent modeling, can help users to get accustomed to a body of domain knowledge and help users to make sense of the potential directions to be explored around their initial, often vague, expression of information needs.<ref>
{{Citation
  | last1 = Ruotsalo  | first1 = Tuukka
  | last2 = Peltonen  | first2 = Jaakko
  | last3 = Eugster | first3 = Manuel J.A.
  | last4 = Glowacka  | first4 = Dorota
  | last5 = Konuyshkova  | first5 = Ksenia
  | last6 = Athukorala  | first6 = Kumaripaba
  | last7 = Kosunen | first7 = Ilkka   
  | last8 = Reijonen  | first8 = Aki
  | last9 = Myllymäki | first9 = Petri
  | last10 = Kaski  | first10 = Samuel
  | last11 = Jacucci  | first11 = Giulio
  | title = Directing Exploratory Search with Interactive Intent Modeling
  | journal = Proceedings of the ACM Conference of Information and Knowledge Management CIKM
  | year = 2013}}
</ref>
<ref>
{{Citation
  | last1 = Glowacka  | first1 = Dorota
  | last2 = Ruotsalo  | first2 = Tuukka
  | last3 = Konuyshkova  | first3 = Ksenia
  | last4 = Athukorala  | first4 = Kumaripaba
  | last5 = Kaski  | first5 = Samuel
  | last6 = Jacucci  | first6 = Giulio
  | title = Directing exploratory search: Reinforcement learning from user interactions with keywords
  | journal = Proceedings of the ACM Conference of Intelligent User Interfaces IUI
  | url = http://dl.acm.org/citation.cfm?id=2449413
  | pages = 117–128
  | year = 2013}}
</ref>

==Major figures==
Key figures, including experts from both [[information seeking]] and [[human–computer interaction]], are:{{Says who|date=April 2016}}
* [[Marcia Bates]]
* Nicholas Belkin<ref>{{cite web|url=http://comminfo.rutgers.edu/~belkin/|title=Nick's home page|publisher=|accessdate=17 April 2016}}</ref>
* Gary Marchionini<ref>{{cite web|url=http://ils.unc.edu/~march|title=Gary's Home Page|publisher=|accessdate=8 April 2016}}</ref>
* m.c. schraefel<ref>{{cite web|url=http://users.ecs.soton.ac.uk/mc|title=m.c. schraefel: design for innovation, creativity, discovery|publisher=|accessdate=8 April 2016}}</ref>
* Ryen White<ref>{{cite web|url=http://research.microsoft.com/~ryenw|title=Ryen W. White|publisher=Microsoft|accessdate=8 April 2016}}</ref>

==References==
<references />

==Sources==
# White, R.W., Kules, B., Drucker, S.M., and schraefel, m.c. (2006). ''Supporting Exploratory Search'', Introduction to Special Section of Communications of the ACM, Vol. 49, Issue 4, (2006), pp.&nbsp;36–39.
# Ryen W. White, Gary Marchionini, Gheorghe Muresan (2008). ''Evaluating exploratory search systems: Introduction to special topic issue of information processing and management'' Vol. 44, Issue 2, (2008), pp.&nbsp;433–436
# Ryen W. White and Resa A. Roth (2009). ''Exploratory Search: Beyond the Query-Response Paradigm'', San Rafael, CA: Morgan and Claypool.
# P. Papadakos, S. Kopidaki, N. Armenatzoglou and Y. Tzitzikas (2009). ''Exploratory Web Searching with Dynamic Taxonomies and Results Clustering'',13th European Conference on Digital Libraries (ECDL'09), Corfu, Greece, Sep-Oct 2009

{{DEFAULTSORT:Exploratory Search}}
[[Category:Human–computer interaction]]
[[Category:Information retrieval genres]]
[[Category:Information science]]
<=====doc_Id=====>:695
<=====title=====>:
Information retrieval applications
<=====text=====>:
Areas where [[information retrieval]] techniques are employed include (the entries are in alphabetical order within each category):

==General applications of information retrieval==
* [[Digital libraries]]
*  [[Information filtering]]
** [[Recommender systems]]
*  Media search
** Blog search
** [[Image retrieval]]
** [[3D retrieval]]
** [[Music information retrieval|Music retrieval]]
** News search
** Speech retrieval
** Video retrieval
* [[Search engines]]
** [[Site search]]
** [[Desktop search]]
** [[Enterprise search]]
** [[Federated search]]
** [[Mobile search]]
** [[Social search]]
** [[Web search engine|Web search]]

==Domain specific applications of information retrieval==
* Expert search finding
* Genomic information retrieval
* [[Geographic information retrieval]]
*  Information retrieval for chemical structures
* Information retrieval in [[software engineering]]
* [[Legal information retrieval]]
* [[Vertical search]]

==Other retrieval methods==
Methods/Techniques in which [[information retrieval]] techniques are employed include:
* [[Adversarial information retrieval]]
* [[Automatic summarization]]
**[[Multi-document summarization]]
* [[Compound term processing]]
* [[Cross-language information retrieval|Cross-lingual retrieval]]
* [[Document classification]]
* [[Spam filtering]]
* [[Question answering]]

== See also ==
* [[Information retrieval]]

{{DEFAULTSORT:Information Retrieval Applications}}
[[Category:Information retrieval genres|*]]
<=====doc_Id=====>:698
<=====title=====>:
Multi-document summarization
<=====text=====>:
{{Refimprove|date=January 2016}}
'''Multi-document summarization''' is an automatic procedure aimed at [[information extraction|extraction of information]] from multiple texts written about the same topic. The resulting summary report allows individual users, such as professional information consumers, to quickly familiarize themselves with information contained in a large cluster of documents. In such a way, multi-document summarization systems are complementing the [[news aggregators]] performing the next step down the road of coping with [[information overload]].

==Key benefits==
Multi-[[document summarization]] creates information reports that are both concise and comprehensive.
With different opinions being put together & outlined, every topic is described from multiple perspectives within a single document.
While the goal of a brief summary is to simplify information search and cut the time by pointing to the most relevant source documents, comprehensive multi-document summary should itself contain the required information, hence limiting the need for accessing original files to cases when refinement is required.
Automatic summaries present information extracted from multiple sources algorithmically, without any editorial touch or subjective human intervention, thus making it completely unbiased.

==Technological challenges==
The multi-document summarization task has turned out to be much more complex than [[automatic summarization|summarizing a single document]], even a very large one. This difficulty arises from inevitable thematic diversity within a large set of documents. A good summarization technology aims to combine the main themes with completeness, readability, and conciseness. Document Understanding Conferences,<ref>{{cite web|url=http://www-nlpir.nist.gov/projects/duc/index.html |title=Document Understanding Conferences |website=Nlpir.nist.gov |date=2014-09-09 |accessdate=2016-01-10}}</ref> conducted annually by [[NIST]], have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge.

An ideal multi-document summarization system does not simply shorten the source texts but presents information organized around the key aspects to represent a wider diversity of views on the topic. When such quality is achieved, an automatic multi-document summary is perceived more like an overview of a given topic. The latter implies that such text compilations should also meet other basic requirements for an overview text compiled by a human. The multi-document summary quality criteria are as follows:
*clear structure, including an outline of the main content, from which it is easy to navigate to the full text sections
*text within sections is divided into meaningful paragraphs
*gradual transition from more general to more specific thematic aspects
*good [[readability]]

The latter point deserves additional note - special care is taken in order to ensure that the automatic overview shows:
*no paper-unrelated "[[communication noise|information noise]]" from the respective documents (e.g., web pages)
*no dangling references to what is not mentioned or explained in the overview
*no text breaks across a sentence
*no semantic [[Redundancy (information theory)|redundancy]].

==Real-life systems==
The multi-document summarization technology is now coming of age - a view supported by a choice of advanced web-based systems that are currently available.
* Ultimate Research Assistant<ref>{{cite web|url=http://ultimate-research-assistant.com/ |title=Generate Research Report |publisher=Ultimate Research Assistant |date= |accessdate=2016-01-10}}</ref> - performs text mining on Internet search results to help summarize and organize them and make it easier for the user to perform online research. Specific text mining techniques used by the tool include concept extraction, text summarization, hierarchical concept clustering (e.g., automated taxonomy generation), and various visualization techniques, including tag clouds and mind maps. 
* iResearch Reporter<ref>{{cite web|url=http://www.iresearch-reporter.com/ |title=iResearch Reporter service |website=Iresearch-reporter.com |date= |accessdate=2016-01-10}}</ref> - Commercial Text Extraction and Text Summarization system, free demo site accepts user-entered query, passes it on to Google search engine, retrieves multiple relevant documents, produces categorized, easily  readable natural language summary reports covering multiple documents in retrieved set, all extracts linked to original documents on the Web, post-processing, entity extraction, event and relationship extraction, text extraction, extract clustering, linguistic analysis, multi-document, full text, natural language processing, categorization rules, clustering, linguistic analysis, text summary construction tool set.
* Newsblaster<ref>[http://newsblaster.cs.columbia.edu]  {{webarchive |url=https://web.archive.org/web/20130416065538/http://newsblaster.cs.columbia.edu |date=April 16, 2013 }}</ref> is a system that helps users find news that is of the most interest to them. The system automatically collects, clusters, categorizes, and summarizes news from several sites on the web ([[CNN]], [[Reuters]], [[Fox News]], etc.) on a daily basis, and it provides users an interface to browse the results.
* NewsInEssence<ref>[http://www.newsinessence.com]  {{webarchive |url=https://web.archive.org/web/20110411005726/http://www.newsinessence.com |date=April 11, 2011 }}</ref> may be used to retrieve and summarize a cluster of articles from the web. It can start from a [[Uniform Resource Locator|URL]] and retrieve documents that are similar, or it can retrieve documents that match a given set of keywords. NewsInEssence also downloads news articles daily and produces news clusters from them.
* NewsFeed Researcher<ref>{{cite web|url=http://newsfeedresearcher.com |title=News Feed Researcher &#124; General Stuff |website=Newsfeedresearcher.com |date= |accessdate=2016-01-10}}</ref> is a news portal performing continuous [[automatic summarization]] of documents initially clustered by the [[news aggregators]] (e.g., [[Google News]]). NewsFeed Researcher is backed by a free online engine covering major events related to business, technology, U.S. and international news. This tool is also available in on-demand mode allowing a user to build a summaries on selected topics.
* Scrape This<ref>[http://www.scrapethis.com]  {{webarchive |url=https://web.archive.org/web/20090919054723/http://www.scrapethis.com |date=September 19, 2009 }}</ref> is like a search engine, but instead of providing links to the most relevant websites based on a query, it scrapes the pertinent information off of the relevant websites and provides the user with a consolidated multi-document summary, along with dictionary definitions, images, and videos.
* JistWeb<ref>[http://www.jastatechnologies.com/productList.html]  {{webarchive |url=https://web.archive.org/web/20130529112318/http://www.jastatechnologies.com/productList.html |date=May 29, 2013 }}</ref> is a query specific multiple document summariser.
* The Simplish Simplifying & Summarizing tool<ref>{{cite web|url=http://simplish.org/ |title=Simplish Basic english Tool |publisher=The Goodwill Consortium |date= |accessdate=2016-02-12}}</ref> - performs automatic multi-lingual multi-document summarization. This tool does not need training of any kind and works by generating ideograms that represent the meaning of each sentence and then summarizes using two user-supplied parameters: equivalence (when are two sentences to be considered equivalent) and relevance (how long is the desired summary). 

As auto-generated multi-document summaries increasingly resemble the overviews written by a human, their use of extracted text snippets may one day face [[copyright]] issues in relation to the [[fair use]] copyright concept.

==Bibliography==
* Günes Erkan and Dragomir R. Radev. Lexrank: Graph-based centrality as salience in text summarization. Journal of Artificial Intelligence Research (JAIR), 2004. [http://clair.si.umich.edu/~radev/papers/lprj.pdf]
* Dragomir R. Radev, Hongyan Jing, Malgorzata Styś, and Daniel Tam. Centroid-based summarization of multiple documents. Information Processing and Management, 40:919–938, December 2004. [http://clair.si.umich.edu/~radev/papers/centroid.pdf]
* Kathleen R. McKeown and Dragomir R. Radev. Generating summaries of multiple news articles. In Proceedings, ACM Conference on Research and Development in Information Retrieval SIGIR'95, pages 74–82, Seattle, Washington, July 1995. [http://clair.si.umich.edu/~radev/papers/sigir95.pdf]
* C.-Y. Lin, E. Hovy, "From single to multi-document summarization: A prototype system and its evaluation", In "Proceedings of the ACL", pp.&nbsp;457–464, 2002
*Kathleen McKeown, Rebecca J. Passonneau, David K. Elson, Ani Nenkova, Julia Hirschberg, "Do Summaries Help? A Task-Based Evaluation of Multi-Document Summarization", SIGIR’05, Salvador, Brazil, August 15–19, 2005 [http://www.cs.columbia.edu/~ani/papers/f98-mckeown.pdf]
*R. Barzilay, N. Elhadad, K. R. McKeown, "Inferring strategies for sentence ordering in multidocument news summarization", Journal of Artificial Intelligence Research, v. 17, pp.&nbsp;35–55, 2002
*M. Soubbotin, S. Soubbotin, "Trade-Off Between Factors Influencing Quality of the Summary", Document Understanding Workshop (DUC), Vancouver, B.C., Canada, October 9–10, 2005 [http://duc.nist.gov/pubs/2005papers/freetext.sergei.pdf]
* C Ravindranath Chowdary, and P. Sreenivasa Kumar. "Esum: an efficient system for query-specific multi-document summarization." In ECIR (Advances in Information Retrieval), pp.&nbsp;724–728. Springer Berlin Heidelberg, 2009.

==See also==
* [[Automatic summarization]]
* [[Text mining]]
* [[News aggregators]]

==References==
{{reflist}}

==External links==
*[http://www-nlpir.nist.gov/projects/duc/index.html Document Understanding Conferences]
*[http://www1.cs.columbia.edu/nlp/projects.html Columbia NLP Projects]
*[http://lada.si.umich.edu:8080/clair/nie1/nie.cgi NewsInEssence: Web-based News Summarization]

{{Natural Language Processing}}

{{DEFAULTSORT:Multi-Document Summarization}}
[[Category:Natural language processing]]
[[Category:Information retrieval genres]]
<=====doc_Id=====>:701
<=====title=====>:
Karen Spärck Jones
<=====text=====>:
{{Infobox scientist
| name = Karen Spärck Jones
| image = Karen Spärck.jpg
| caption = Karen Spärck Jones in 2002
| birth_date = {{birth date|1935|8|26|df=y}}
| birth_place = [[Huddersfield]], [[Yorkshire]]
| death_date = {{death date and age|2007|4|4|1935|8|26|df=y}}
| death_place = [[Willingham, Cambridgeshire]]
| residence = United Kingdom
| nationality = British
| field = Computer science
| work_institution = [[University of Cambridge Computer Laboratory]]
| alma_mater = University of Cambridge
| doctoral_advisor = Richard Braithwaite<ref name=odnb>{{cite web|title=Jones, Karen Ida Boalth Spärck (1935–2007), Computer Scientist|url=http://www.oxforddnb.com/view/article/98729|work=Oxford Dictionary of National Biography|publisher=Oxford University Press|accessdate=5 October 2014}}</ref>
| thesis_title = Synonymy and Semantic Classiﬁcation
| thesis_year = 1964<ref>{{cite book|author=Karen Spärck Jones|title=Synonymy and Semantic Classification (thesis published as a book)|publisher=Edinburgh University Press|series=Edinburgh Information Technology series|volume=1|year=1986}}</ref>
| doctoral_students = 
| known_for  = work on information retrieval and natural language processing, in particular her probabilistic model of document and text retrieval
| prizes = ACL Lifetime Achievement Award, BCS Lovelace Medal, ACM-AAAI Allen Newell Award, ACM SIGIR Salton Award, American Society for Information Science and Technology’s Award of Merit
| religion = 
| spouse = [[Roger Needham]]
| website = {{URL|http://www.cl.cam.ac.uk/archive/ksj21}}
}}

'''Karen Spärck Jones''' [[Fellow of the British Academy|FBA]] (26 August 1935 – 4 April 2007) was a [[United Kingdom|British]] computer scientist.<ref>{{Cite journal | last1 = Tait | first1 = J. I. | title = Karen Spärck Jones | doi = 10.1162/coli.2007.33.3.289 | journal = Computational Linguistics | volume = 33 | issue = 3 | pages = 289–291 | year = 2007 | pmid =  | pmc = }}</ref><ref>{{Cite journal | last1 = Robertson | first1 = S. | last2 = Tait | first2 = J. | doi = 10.1002/asi.20784 | title = Karen Spärck Jones | journal = Journal of the American Society for Information Science and Technology | volume = 59 | issue = 5 | pages = 852 | year = 2008 | pmid =  | pmc = }}</ref>

==Personal life==
Karen Ida Boalth Spärck Jones was born in [[Huddersfield]], [[Yorkshire]], [[England]]. Her father was Owen Jones, a lecturer in chemistry, and her mother was Ida Spärck, a [[Norway|Norwegian]] who moved to Britain during [[World War II]].  They left Norway on one of the last boats out after the German invasion in 1940.<ref name="odnb" /> Spärck Jones was educated at a grammar school in Huddersfield and then [[Girton College, Cambridge]] from 1953 to 1956, reading History, with an additional final year in Moral Sciences (philosophy). She briefly became a school teacher, before moving into Computer Science.  During her career in Computer Science, she campaigned hard for more women to enter computing.<ref name="odnb" />   She was married to fellow Cambridge computer scientist [[Roger Needham]] until his death in 2003. She died 4 April 2007 at [[Willingham, Cambridgeshire|Willingham]] in [[Cambridgeshire]].

==Career==
She worked at the Cambridge Language Research Unit from the late 1950s,<ref>{{cite web|url=http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/|title=Computer Laboratory obituary}}</ref> then at [[University of Cambridge|Cambridge's]] [[Cambridge University Computer Laboratory|Computer Laboratory]] from 1974, and retired in 2002, holding the post of Professor of Computers and Information, which she was awarded in 1999.<ref name="odnb" /> She continued to work in the Computer Laboratory until shortly before her death. Her main research interests, since the late 1950s, were [[natural language processing]] and [[information retrieval]].<ref name="doi10.1108/eb026526">{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| doi = 10.1108/eb026526 | title = A Statistical Interpretation of Term Specificity and Its Application in Retrieval | journal = Journal of Documentation | volume = 28 | pages = 11–21 | year = 1972 | url = http://www.emeraldinsight.com/doi/abs/10.1108/eb026526| pmid =  | pmc = }}</ref><ref>{{Cite journal | editor1-last = Tait | editor1-first = John I. | title = Charting a New Course: Natural Language Processing and Information Retrieval, Essays in Honour of Karen Spärck Jones| doi = 10.1007/1-4020-3467-9 | series = The Kluwer International Series on Information Retrieval | volume = 16 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}</ref> One of her most important contributions was the concept of [[inverse document frequency]] (IDF) weighting in information retrieval, which she introduced in a 1972 paper.<ref name="doi10.1108/eb026526"/><ref name="idf">{{Cite journal | last1 = Spärck Jones | first1 = K. | authorlink1 = Karen Spärck Jones| title = Index term weighting | doi = 10.1016/0020-0271(73)90043-0 | journal = Information Storage and Retrieval | volume = 9 | issue = 11 | pages = 619–633 | year = 1973 | pmid =  | pmc = }}</ref> IDF is used in most search engines today, usually as part of the [[tf-idf]] weighting scheme.<ref>{{Cite book | last1 = Maybury | first1 = M. T. | chapter = Karen Spärck Jones and Summarization | doi = 10.1007/1-4020-3467-9_7 | title = Charting a New Course: Natural Language Processing and Information Retrieval | series = The Kluwer International Series on Information Retrieval | volume = 16 | pages = 99–10 | year = 2005 | isbn = 1-4020-3343-5 | pmid =  | pmc = }}</ref>

There is an annual [[British Computer Society|BCS]] lecture named in her honour.<ref>{{cite web|title=Karen Spärck Jones lecture|url=http://academy.bcs.org/ksj|work=BCS Academy of Computing|publisher=British Computer Society|accessdate=3 October 2013}}</ref>

===Honours===
* Fellow of the [[British Academy]], of which she was Vice-President in 2000–02
* Fellow of [[AAAI]]
* Fellow of [[ECCAI]]
* President of the [[Association for Computational Linguistics]] in 1994

===Awards===
* [[Gerard Salton Award]] (1988)
* [[ASIS&T]] Award of Merit (2002)
* [[Association for Computational Linguistics|ACL]] Lifetime Achievement Award (2004) <ref>{{cite web|title=ACL Lifetime Achievement Award Recipients|url=http://aclweb.org/aclwiki/index.php?title=ACL_Lifetime_Achievement_Award_Recipients|website=ACL wiki|publisher=[[Association for Computational Linguistics|ACL]]|accessdate=16 August 2014}}</ref>
* [[British Computer Society|BCS]] [[Lovelace Medal]] (2007)
* [[ACM - AAAI Allen Newell Award]] (2006)

==Karen Spärck Jones Award==
To commemorate her achievements, the Karen Spärck Jones Award was created in 2008 by the [[British Computer Society|BCS]] and its Information Retrieval Specialist Group (BCS IRSG), which is sponsored by [[Microsoft Research]].<ref>[http://irsg.bcs.org/ksjaward.php Microsoft BCS/BCS IRSG Karen Spärck Jones Award An Award to Commemorate Karen Spärck Jones]</ref>

The recipients are:
* 2016, [[Jaime Teevan]]
* 2015, [[Jordan Boyd-Graber]], [[Emine Yilmaz]]
* 2014, [[Ryen White]]
* 2013, [[Eugene Agichtein]]
* 2012, [[Diane Kelly(computer scientist)]]
* 2011, No award was made
* 2010, [[Evgeniy Gabrilovich]]
* 2009, [[Mirella Lapata]]

==References==
{{reflist}}

==Further reading==
* [http://spectrum.ieee.org/may07/5063 Computer Science, A Woman's Work], IEEE Spectrum, May 2007

==External links==
*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/video/ Video: Natural Language and the Information Layer, Karen Spärck Jones, March 2007]
*[http://www.cl.cam.ac.uk/misc/obituaries/sparck-jones/ University of Cambridge obituary]
*[http://news.independent.co.uk/people/obituaries/article2441969.ece Obituary], ''[[The Independent]]'', 12 April 2007 {{dead link|date=April 2014}}
*[http://www.telegraph.co.uk/news/main.jhtml?view=DETAILS&grid=&xml=/news/2007/04/12/db1201.xml  Obituary], ''[[The Daily Telegraph]]'', 12 April 2007 {{dead link|date=April 2014}}
*[http://www.timesonline.co.uk/tol/comment/obituaries/article1968942.ece Obituary], ''[[The Times]]'', 22 June 2007 {{subscription required}}

{{s-start}}
{{s-ach}}
{{succession box |
 before=[[Makoto Nagao]] |
 title=ACL Lifetime Achievement Award |
 after=[[Martin Kay]] |
 years=2004}}
{{s-end}}

{{Authority control}}

{{DEFAULTSORT:Sparck Jones, Karen}}
[[Category:1935 births]]
[[Category:2007 deaths]]
[[Category:Alumni of Girton College, Cambridge]]
[[Category:British computer scientists]]
[[Category:Women computer scientists]]
[[Category:Fellows of the British Academy]]
[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]
[[Category:Fellows of Newnham College, Cambridge]]
[[Category:Fellows of Wolfson College, Cambridge]]
[[Category:Members of the University of Cambridge Computer Laboratory]]
[[Category:People from Huddersfield]]
[[Category:Deaths from cancer in England]]
[[Category:Information retrieval researchers]]
[[Category:British women scientists]]
[[Category:Artificial intelligence researchers]]
[[Category:20th-century women scientists]]
<=====doc_Id=====>:704
<=====title=====>:
Cyril Cleverdon
<=====text=====>:
{{Infobox scientist
| name = Cyril Cleverdon
| image =
| caption = 
| birth_date = {{birth date|1914|9|9|df=y}}
| birth_place = [[Bristol]], [[United Kingdom|UK]]
| death_date = {{death date and age|1997|12|4|1914|9|9|df=y}}
| death_place = [[Cranfield]], [[United Kingdom|UK]]
| residence = United Kingdom
| nationality = British
| field = Computer Science
| work_institution = [[Cranfield Institute of Technology]]
| known_for  = work on the evaluation of information retrieval systems
| prizes = Professional Award of the Special Libraries Association (1962), Award of Merit of the American Society for Information Science (1971), The [[Gerard Salton Award]] of the Special Interest Group on Information Retrieval of the Association for Computing Machinery (1991)
}}
'''Cyril Cleverdon''' (9 September 1914 – 4 December 1997) was a [[United Kingdom|British]] librarian and computer scientist who is best known for his work on the evaluation of [[information retrieval]] systems.

Cyril Cleverdon was born in [[Bristol]], [[England]]. He worked at the Bristol Libraries from 1932 to 1938, and from 1938 to 1946 he was the librarian of the Engine Division of the Bristol Aeroplane Co. Ltd. In 1946 he was appointed librarian of the College of Aeronautics at Cranfield (later the [[Cranfield Institute of Technology]] and [[Cranfield University]]), where he served until his retirement in 1979, the last two years as professor of Information Transfer Studies.

With the help of NSF funding, Cleverdon started a series of projects in 1957 that lasted for about 10 years in which he and his colleagues set the stage for information retrieval research. In the Cranfield project, retrieval experiments were conducted on test databases in a controlled, laboratory-like setting. The aim of the research was to improve the retrieval effectiveness of information retrieval systems, by developing better indexing languages and methods. The components of the experiments were:
# a collection of documents,
# a set of user requests or queries, and 
# a set of relevance judgments—that is, a set of documents judged to be [[Relevance (information retrieval)|relevant]] to each query. 
Together, these components form an information retrieval test collection. The test collection serves as a standard for testing retrieval approaches, and the success of each approach is measured in terms of two measures: [[Precision (information retrieval)|precision]] and [[Recall (information retrieval)|recall]]. Test collections and evaluation measures based on precision and recall are driving forces behind modern research on search systems. Cleverdon's approach formed a blueprint for the successful [[Text Retrieval Conference]] series that began in 1992.

Not only did Cleverdon's Cranfield studies introduce experimental research into computer science, the outcomes of the project also established the basis of the [[automatic indexing]] as done in today's [[search engine]]s. Essentially, Cleverdon found that the use of single terms from the documents achieved the best retrieval performance, as opposed to manually assigned thesaurus terms, synonyms, etc. These results were very controversial at the time. In the Cranfield 2 Report, Cleverdon said:

''This conclusion is so controversial and so unexpected that it is bound to throw considerable doubt on the methods which have been used (...) A complete recheck has failed to reveal any discrepancies (...) there is no other course except to attempt to explain the results which seem to offend against every canon on which we were trained as librarians.'' 

Cyril Cleverdon also ran, for many years, the Cranfield conferences, which provided a major international forum for discussion of ideas and research in information retrieval. This function was taken over by the [[Special Interest Group on Information Retrieval|SIGIR]] conferences in the 1970s.

==References==
* {{cite journal|author=Cyril Cleverdon|title=Report on the Testing and Analysis of an Investigation into the Comparative Efficiency of Indexing Systems|publisher=The College of Aeronautics, Cranfield|year=1960|url=http://www.sigir.org/museum/pdfs/Report_on_the_Testing_and_Analysis_of_an_Investigation_Into_the_Comparative_Efficiency_of_Indexing_Systems/pdfs/frontmatter.pdf}}
* Cyril Cleverdon and Michael Keen, Factors Determining the Performance of Indexing Systems, Volume 2, ''The College of Aeronautics, Cranfield'', 1966
* Stephen Robertson, In Memoriam Cyril W. Cleverdon, ''Journal of the American Society for Information Science 49''(10):866, 1998

{{DEFAULTSORT:Cleverdon, Cyril}}
[[Category:1914 births]]
[[Category:1997 deaths]]
[[Category:British computer scientists]]
[[Category:English librarians]]
[[Category:People associated with Cranfield University]]
[[Category:People from Bristol]]
[[Category:Information retrieval researchers]]
<=====doc_Id=====>:707
<=====title=====>:
C. J. van Rijsbergen
<=====text=====>:
{{Use dmy dates|date=March 2014}}
{{Use British English|date=March 2014}}
{{multiple issues|
{{BLP sources|date=January 2012}}
{{More footnotes|date=January 2012}}
}}

{{Infobox scientist
|name              = Cornelis Joost van Rijsbergen
|image             = C J van Rijsbergen.jpg
|image_size        =
|caption           = C. J. "Keith" van Rijsbergen
|birth_date        = {{birth year and age|1943}}
|birth_place       = [[Rotterdam]]
|residence         = 
|citizenship       =
|nationality       = 
|fields            = [[Information Retrieval]]
|workplaces        = [[Monash University]], [[University of Glasgow]]
|alma_mater        = [[University of Western Australia]], [[University of Cambridge]]
|doctoral_advisor  = 
|academic_advisors =
|doctoral_students =
|notable_students  =
|known_for         = 
|author_abbrev_bot =
|author_abbrev_zoo =
|influences        =
|influenced        =
|awards            =
|signature         = <!--(filename only)-->
|footnotes         =
}}

'''C. J. "Keith" van Rijsbergen''' [[FREng]]<ref name=fellow>{{cite web|title=List of Fellows|url=http://www.raeng.org.uk/about-us/people-council-committees/the-fellowship/list-of-fellows}}</ref> ('''Cornelis Joost van Rijsbergen''') (born 1943) is a professor of [[computer science]] and the leader of the Glasgow Information Retrieval Group based at the [[University of Glasgow]]. He is one of the founders of modern [[Information Retrieval]] and the author of the seminal monograph ''Information Retrieval'' and of the textbook ''The Geometry of Information Retrieval''.

He was born in [[Rotterdam]], and educated in the [[Netherlands]], [[Indonesia]], [[Namibia]] and [[Australia]].
His first degree is in mathematics from the [[University of Western Australia]], and in 1972 he completed a
PhD in computer science at the [[University of Cambridge]].
He spent three years lecturing in information retrieval and artificial intelligence at [[Monash University]]
before returning to [[University of Cambridge|Cambridge]] to hold a [[Royal Society]] Information Research Fellowship. 
In 1980 he was appointed to the chair of computer science at [[University College Dublin]];
from there he moved in 1986 to [[Glasgow University]].
Since 2007 he has been Chairman of the Scientific Board of the [[Information Retrieval Facility]].

==Awards and honors==
In 2003 he was inducted as a Fellow of the [[Association for Computing Machinery]]. In 2004 he was awarded the [[Tony Kent Strix award]].
In 2004 he was appointed a [[Fellow]]<ref name=fellow /> of the [[Royal Academy of Engineering]].<ref name=fellow />
In 2006, he was awarded the [[Gerard Salton Award]] for ''Quantum haystacks''.

==See also==
*[[F1 score]]

==References==
{{Reflist}}

==External links==
*[http://www.dcs.gla.ac.uk/~keith/ C. J. "Keith" van Rijsbergen - The University of Glasgow]
*[http://ir.dcs.gla.ac.uk/ Glasgow Information Retrieval Group]
*[http://www.dcs.gla.ac.uk/Keith/Preface.html Information Retrieval book - C. J. van Rijsbergen 1979]
*[http://www.ir-facility.org/ Information Retrieval Facility]
*{{worldcat id|id=lccn-n83-236586}}
* [http://www.alanmacfarlane.com/ancestors/rijsbergen.htm Keith van Rijsbergen interviewed by Alan Macfarlane 15 July 2009 (film)]

{{Authority control}}

{{DEFAULTSORT:Rijsbergen, C. J. van}}
[[Category:1943 births]]
[[Category:Living people]]
[[Category:Dutch computer scientists]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:People from Rotterdam]]
[[Category:University of Western Australia alumni]]
[[Category:Information retrieval researchers]]


{{Netherlands-scientist-stub}}
{{compu-scientist-stub}}
<=====doc_Id=====>:710
<=====title=====>:
Jaime Teevan
<=====text=====>:
{{Infobox scientist
| name        = Jaime Teevan
| image       = 
| caption     =
| birth_date  = {{Birth year and age|1976}}
| birth_place = 
| death_date  =
| death_place =
| nationality = 
| residence   = 
| fields      = [[Computer science]]<br/ >[[Human-Computer Interaction]]<br/ >[[Information Retrieval]]
| work_institution = [[Microsoft Research]]
| alma_mater  = [[Massachusetts Institute of Technology]]<br/>[[Yale University]]
| known_for   =
| doctoral_advisor = [[David Karger]]
| awards = [[TR35]] (2009)<br/ >Borg Early Career Award (2014)<br/ >[[Karen Spärck Jones]] Award (2016)
| website           = {{URL|http://teevan.org}} 
}}

'''Jaime Teevan''' is an American [[computer scientist]] known for her research in [[human-computer interaction]] and [[information retrieval]]. She is particularly known for the work she has done on [[personalized search]]. According to the [[Technology Review]], Teevan "is a leader in using data about people's knowledge, preferences, and habits to help them manage information.<ref name="tr35">{{cite news|last=Kleiner|first=Kurt|title=TR35: Jaime Teevan, 32|url=http://www.technologyreview.com/tr35/Profile.aspx?Cand=T&TRID=778|accessdate=10 March 2011|newspaper=Technology Review|date=August 2009}}</ref>"

==Biography==
Teevan received and a [[Bachelor of Science|B.S.]] in [[Computer Science]] from [[Yale University]] and a Ph.D. and S.M. from [[MIT]].<ref>http://www.csail.mit.edu/~teevan/work/publications/theses/phd/thesis.pdf</ref><ref>http://www.csail.mit.edu/~teevan/work/publications/theses/masters/thesis.pdf</ref>

She is currently a researcher at [[Microsoft Research]] and an affiliate professor at the [[University of Washington]]. There she co-authored the first book on [[collaborative information seeking]],.<ref>{{cite book|last=Morris|first=Meredith Ringel and Teevan, Jaime|title=Collaborative Search: Who, What, Where, When, Why, and How|year=2010|publisher=Morgan and Claypool Publishers|isbn=1-60845-121-6|url=http://www.amazon.com/dp/1608451216/}}</ref> She also edited a book on [[Personal Information Management]] (PIM),<ref>{{cite book|editor=Jones, William |editor2=Teevan, Jaime|title=Personal Information Management|year=2007|publisher=University of Washington Press|isbn=0-295-98737-5|url=http://www.amazon.com/dp/0295987375}}</ref> 
edited a special issue of Communications of the ACM on the topic, and organized workshops on [[PIM (software)|PIM]] and query log analysis. She has published numerous technical papers, including several best papers, and was chair of the Web Search and Data Mining (WSDM) 2012 conference.

==Awards==

Teevan was named a Technology Review ([[TR35]]) 2009 Young Innovator for her research on [[personalized search]]<ref name="tr35" /> and received the [[CRA-W]] Borg Early Career Award (BECA) in 2014.<ref name="borg">{{cite news|last=Knies|first=Rob|title=Researcher Teevan Wins Borg Early Career Award|url=http://blogs.technet.com/b/inside_microsoft_research/archive/2014/04/22/researcher-teevan-wins-borg-early-career-award.aspx|accessdate=28 April 2014|newspaper=Inside Microsoft Research|date=April 2014}}</ref> In 2016 she received the [[Karen Spärck Jones]] award from the [[British Computer Society]] for her "technically strong and exceptionally creative contributions to the intersection of information retrieval, user experience and social media." <ref>http://irsg.bcs.org/ksjaward.php</ref>

==Personal==
Teevan is married to Alexander Hehmeyer.<ref>{{cite news|title=WEDDINGS; Jaime Teevan, Alexander Hehmeyer|url=http://www.nytimes.com/2002/06/16/style/weddings-jaime-teevan-alexander-hehmeyer.html|accessdate=14 September 2015|newspaper=New York Times|date=June 16, 2002}}</ref>
The couple live in [[Bellevue, Washington]]
and have four children.<ref>{{cite news|last=Vanderkam|first=Laura|title=Women with Big Jobs and Big Families: Balancing Really Isn't That Hard|url=http://fortune.com/2015/06/06/women-with-big-jobs-and-big-families-balancing-really-isnt-that-hard/|accessdate=14 September 2015|newspaper=Fortune|date=6 June 2015}}</ref>
Teevan is an advocate for helping researchers successfully integrate parenthood and academic efforts.<ref name="borg" />

==References==
<references />

==External links==
* [http://teevan.org/ Professional home page]

{{DEFAULTSORT:Teevan, Jaime}}
[[Category:People in information technology]]
[[Category:Information retrieval researchers]]
[[Category:Human–computer interaction researchers]]
[[Category:Women computer scientists]]
[[Category:Microsoft employees]]
[[Category:Living people]]
[[Category:Yale University alumni]]
[[Category:Massachusetts Institute of Technology alumni]]
[[Category:University of Washington faculty]]
[[Category:1976 births]]
<=====doc_Id=====>:713
<=====title=====>:
Meta Content Framework
<=====text=====>:
'''Meta Content Framework''' ('''MCF''') is a specification of a [[content format]] for structuring [[metadata]] about [[web site]]s and other [[data]].

==History==
MCF was developed by [[Ramanathan V. Guha]] at [[Apple Advanced Technology Group|Apple Computer's Advanced Technology Group]] between 1995 and 1997. Rooted in [[Knowledge representation and reasoning | knowledge-representation]] systems such as [[CycL]], [[KRL (programming language)| KRL]], and [[Knowledge Interchange Format|KIF]], it sought to describe objects, their attributes, and the relationships between them.<ref name=hammersley>{{Cite book| publisher = O'Reilly| isbn = 978-0-596-00383-8| last = Hammersley| first = Ben| title = Content Syndication with RSS| location = Sebastopol| date = 2003| page=2}}</ref>

One application of MCF was [[HotSauce]], also developed by Guha while at Apple. It generated a [[3D computer graphics|3D]] [[visualization (graphic)|visualization]] of a web site's table of contents, based on MCF descriptions. By late 1996, a few hundred sites were creating MCF files and Apple HotSauce allowed users to browse these MCF representations in 3D.<ref name=hammersley />

When the research project was discontinued, Guha left Apple for [[Netscape Communications Corporation|Netscape]], where, in collaboration with [[Tim Bray]], he adapted MCF to use [[XML]]<ref>{{Cite conference
| publisher = W3C
| last = Guha
| first = R V
|author2=Tim Bray
 | title = Meta Content Framework Using XML
| accessdate = 2014-09-14
| date = 1997-06-06
| url = http://www.w3.org/TR/NOTE-MCF-XML/
}}</ref><ref>{{Cite web|last1=Guha |first1=R.V. |last2=Bray |first2=Tim |title=Meta Content Framework Using XML |work=Netscape |accessdate=2015-12-12 |date=1997-06-13 |url=http://developer.netscape.com/mcf.html |deadurl=yes |archiveurl=https://web.archive.org/web/19970615144715/http://developer.netscape.com/mcf.html |archivedate=June 15, 1997 }}</ref> and created the first version of the [[Resource Description Framework]] (RDF).<ref>{{Cite web|last=Andreessen |first=Marc |title=Innovators of the Net: R.V. Guha and RDF |work=Netscape |accessdate=2014-09-14 |date=1999-01-08 |url=http://wp.netscape.com/columns/techvision/innovators_rg.html |deadurl=yes |archiveurl=https://web.archive.org/web/20080205163659/http://wp.netscape.com/columns/techvision/innovators_rg.html |archivedate=February 5, 2008 }}</ref>

==References==
{{reflist}}

==External links==
*[http://www.textuality.com/mcf/MCF-tutorial.html MCF Tutorial] (using XML syntax)
*[http://www.guha.com/mcf/ Guha MCF site]
*[http://downlode.org/Etext/MCF/towards_a_theory_of_metacontent.html The metacontent concept]

[[Category:Knowledge representation]]
[[Category:Apple Inc. software]]

{{compu-AI-stub}}
<=====doc_Id=====>:716
<=====title=====>:
Logic form
<=====text=====>:
'''Logic forms''' are simple, [[first-order logic]] [[knowledge representation]]s of [[natural language]] sentences formed by the conjunction of concept predicates related through shared arguments. Each noun, verb, adjective, adverb, pronoun, preposition and conjunction generates a predicate. Logic forms can be decorated with [[word sense]]s to [[Word sense disambiguation|disambiguate]] the semantics of the word. There are two types of predicates: events are marked with ''e'', and entities are marked with ''x''. The shared arguments connect the subjects and objects of verbs and prepositions together. Example input/output might look like this:
 Input:  '''The Earth provides the food we eat every day.'''
 Output: '''Earth''':n_#1(<span style="color:#008800;">x1</span>) '''provide''':v_#2(<span style="color:#888800;">e1</span>, <span style="color:#008800;">x1</span>, <span style="color:#880000;">x2</span>) '''food''':n_#1(<span style="color:#880000;">x2</span>) '''we'''(<span style="color:#000088;">x3</span>) '''eat''':v_#1(<span style="color:#880088;">e2</span>, <span style="color:#000088;">x3</span>, <span style="color:#880000;">x2</span>; <span style="color:#008888;">x4</span>) '''day''':n_#1(<span style="color:#008888;">x4</span>)

Logic forms are used in some [[natural language processing]] techniques, such as [[question answering]], as well as in [[inference]] both for [[database]] systems and QA systems.

==Evaluations==
[http://www.senseval.org/ SENSEVAL-3] in 2004 introduced a {{webarchive |url=https://web.archive.org/web/20050902115653/site=http://www.cs.iusb.edu/~vasile/logic/indexLF.html |date=September 2, 2005 |title=Logic Form Identification task }}.

==References==
*{{cite book | author=Vasile Rus | title=Logic Form for WordNet Glosses  | url=http://www.engr.smu.edu/~vasile/rus02.PhDThesis.ps | publisher=Ph.D. thesis, Southern Methodist University | year=2002 }} <!-- Most information in the article derived from Vasile's work -->
*{{cite journal | author=Vasile Rus and Dan Moldovan | title=High performance logic form transformation | journal=International Journal for Tools with Artificial Intelligence. IEEE Computer Society, IEEE Press |date=September 2002 | volume=11| issue =  3 | pages=437–454 | url=http://www.worldscinet.com/ijait/11/1103/S0218213002000976.html}}
*{{cite conference | author=Dan Moldovan and Vasile Rus | url=http://engr.smu.edu/~vasile/acl2001.ps | title=Logic Form transformation of wordNet and its Applicability to question answering | booktitle=Proceedings of ACL 2001, Toulouse, France | year=2001 | pages=}}
*{{cite conference | author=Jerry R. Hobbs | title=Overview of the TACITUS project | booktitle=Computational Linguistics| year=1986 | pages=12(3)}}
*{{cite conference | author=Vasile Rus | url=http://acl.ldc.upenn.edu/acl2004/senseval/pdf/rus.pdf | title=A First Evaluation of Logic Form Identification Systems | booktitle=SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text | year=2004 | pages=|format=PDF}}

[[Category:Natural language processing]]
[[Category:Computational linguistics]]
[[Category:Knowledge representation]]
{{ling-stub}}
<=====doc_Id=====>:719
<=====title=====>:
Nippon Decimal Classification
<=====text=====>:
The '''Nippon Decimal Classification''' ('''NDC''', also called the '''Nippon Decimal System''') is a system of [[library classification]] developed for mainly Japanese language books maintained by the Japan Library Association since 1956. It is based on the [[Dewey Decimal Classification|Dewey Decimal System]]. The system is based upon using each successive digit to divide into nine divisions with the digit zero used for those not belonging to any of the divisions.

== Main classes ==
The system is made up of ten categories:

* 000 General
* 100 [[Philosophy]]
* 200 [[History]]
* 300 [[Social sciences]]
* 400 [[Natural sciences]]
* 500 [[Technology]] and [[engineering]]
* 600 [[Industry]] and [[commerce]]
* 700 [[Arts]]
* 800 [[Language]]
* 900 [[Literature]]

== Description of the classes ==

*000 General
**010 [[Libraries]], [[Library and information science|Library & information science]]
**020 [[Books]], [[Bibliography]]
**030 [[Encyclopaedia]]s
**040 General collected [[essays]]
**050 General [[Periodical literature|serial publications]]
**060 [[Organizations]]
**070 [[Journalism]], [[Newspapers]]
**080 General collections
**090 [[Rare books]], Local collections, [[Special collections]]
*100 Philosophy
**110 Special treatises on philosophy
**120 [[Oriental philosophy]]
**130 [[Western philosophy]]
**140 [[Psychology]]
**150 [[Ethics]] & morals
**160 [[Religion]]
**170 [[Shintoism]]
**180 [[Buddhism]]
**190 [[Christianity]]
*200 History
**210 [[History of Japan]]
**220 [[History of Asia]] and the Orient
**230 [[History of Europe]] and the West
**240 [[History of Africa]]
**250 [[History of North America]]
**260 [[History of South America]]
**270 [[History of Oceania]] & [[Polar region]]s
**280 [[Biography]]
**290 [[Geography]], [[Topography]], [[Travel]]
*300 Social Sciences
**310 [[Politics]]
**320 [[Law]]
**330 [[Economics]]
**340 [[Finance]]
**350 [[Statistics]]
**360 [[Sociology]]
**370 [[Education]]
**380 [[Customs]], [[Folklore]], [[Ethnology]]
**390 National defence, [[Military science]]
*400 Natural Sciences
**410 [[Mathematics]]
**420 [[Physics]]
**430 [[Chemistry]]
**440 [[Astronomy]], [[Space science]]
**450 [[Earth science]]
**460 [[Biology]]
**470 [[Botany]]
**480 [[Zoology]]
**490 [[Medicine]], [[Pharmacology]]
*500 Technology & Engineering
**510 [[Construction]], [[Civil engineering]]
**520 [[Architecture]]
**530 [[Mechanical engineering]], [[Nuclear engineering]]
**540 [[Electrical engineering|Electrical]] & [[Electronic engineering]]
**550 Maritime & [[Naval engineer]]ing
**560 Metal & [[Mining engineering]]
**570 [[Chemical technology]]
**580 [[Manufacturing]]
**590 [[Domestic science|Domestic arts and sciences]]
*600 Industry and Commerce
**610 [[Agriculture]]
**620 [[Horticulture]]
**630 [[Sericulture|Silk industry]]
**640 [[Animal husbandry]]
**650 [[Forestry]]
**660 [[Fishing]]
**670 [[Commerce]]
**680 [[Transportation]] & [[Traffic]]
**690 [[Communications]]
*700 Arts
**710 [[Plastic arts]] (sculpture)
**720 [[Painting]] & [[Calligraphy]]
**730 [[Engraving]]
**740 [[Photography]] & [[Printing]]
**750 [[Craft]]
**760 [[Music]] & [[Dance]]
**770 [[Theatre]], [[Motion Pictures]]
**780 [[Sports]], [[Physical Education]]
**790 [[Recreation]], Amusements
*800 Language
**810 [[Japanese language|Japanese]]
**820 [[Chinese language|Chinese]], other [[oriental languages]]
**830 [[English language|English]]
**840 [[German language|German]]
**850 [[French language|French]]
**860 [[Spanish language|Spanish]]
**870 [[Italian language|Italian]]
**880 [[Russian language|Russian]]
**890 Other languages
*900 Literature
**910 [[Japanese literature]]
**920 [[Chinese literature]], Other [[Oriental literature]]
**930 [[English literature|English]] & [[American literature]]
**940 [[German literature]]
**950 [[French literature]]
**960 [[Spanish literature]]
**970 [[Italian literature]]
**980 [[Russian literature|Russian]] & [[Soviet literature]]
**990 Other language literature

== External links ==
*[http://www.jla.or.jp/index-e.html Japan Library Association]
*[http://www.asahi-net.or.jp/~ax2s-kmtn/ref/ndc/e_ndc.html CyberLibrarian]

{{Library classification systems}}

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]
<=====doc_Id=====>:722
<=====title=====>:
Resource Description Framework
<=====text=====>:
{{Infobox technology standard
| title             = RDF 1.1 Concepts and Abstract Syntax
| status            = Published, W3C Recommendation
| year_started      = 1997
| editors           = Richard Cyganiak, David Wood, Markus Lanthaler
| base_standards    = [[URI]]
| related_standards = [[RDFS]], [[Web Ontology Language|OWL]], [[Rule Interchange Format|RIF]], [[RDFa]]
| domain            = [[Semantic Web]]
| abbreviation      = RDF
| website           = {{url|http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}
}}

The '''Resource Description Framework''' ('''RDF''') is a family of [[World Wide Web Consortium]] (W3C) [[specification]]s<ref>{{cite web|url=http://www.dblab.ntua.gr/~bikakis/XMLSemanticWebW3CTimeline.pdf |title=XML and Semantic Web W3C Standards Timeline
|date=2012-02-04}}</ref> originally designed as a [[metadata]] [[data model]]. It has come to be used as a general method for conceptual description or modeling of information that is implemented in [[web resource]]s, using a variety of syntax notations and [[data serialization]] formats. It is also used in [[knowledge management]] applications.

RDF was adopted as a W3C recommendation in 1999. The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014.

== Overview ==
The RDF data model<ref>http://www.w3.org/TR/PR-rdf-syntax/ "Resource Description Framework
(RDF) Model and Syntax Specification"</ref>  is similar to classical conceptual modeling approaches (such as [[entity–relationship model|entity–relationship]] or [[class diagram]]s). It is based upon the idea of making [[statement (programming)|statement]]s about [[resource (computer science)|resource]]s (in particular [[web resource]]s) expressions, known as ''[[Semantic triple|triples]]''.  Triples are so named because they follow a <var>subject</var>–<var>predicate</var>–<var>object</var> structure. The <var>subject</var> denotes the resource, and the <var>predicate</var> denotes traits or aspects of the resource, and expresses a relationship between the <var>subject</var> and the <var>object</var>. 

For example, one way to represent the notion "The sky has the color blue" in RDF is as the triple: a [[Subject (grammar)|subject]] denoting "the sky", a [[Predicate (grammar)|predicate]] denoting "has the color", and an [[Object (grammar)|object]] denoting "blue". Therefore, RDF swaps <var>object</var> for <var>subject</var> in contrast to the typical approach of an [[entity–attribute–value model]] in [[object-oriented design]]: entity (sky), attribute (color), and value (blue).  

RDF is an abstract model with several [[Serialization|serialization formats]] (i.e. file formats), so the particular encoding for resources or triples varies from format to format.

This mechanism for describing resources is a major [[software componentry|component]] in the W3C's [[Semantic Web]] activity: an evolutionary stage of the [[World Wide Web]] in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and [[certainty]]. RDF's simple data model and ability to model disparate, abstract concepts has also led to its increasing use in [[knowledge management]] applications unrelated to Semantic Web activity.

A collection of RDF statements intrinsically represents a [[Glossary of graph theory|labeled, directed multi-graph]]. This theoretically makes an RDF [[data model]] better suited to certain kinds of [[knowledge representation]] than other [[relational model|relational]] or [[Ontology (computer science)|ontological]] models. However, in practice, RDF data is often persisted in [[RDBMS|relational database]] or native representations (also called [[Triplestore]]s—or Quad stores, if context (i.e. the [[named graph]]) is also persisted for each RDF triple).<ref>[http://sw.deri.org/2005/02/dexa/yars.pdf Optimized Index Structures for Querying RDF from the Web] Andreas Harth, Stefan Decker, 3rd Latin American Web Congress, Buenos Aires, Argentina, October 31 to November 2, 2005, pp. 71–80</ref> 

ShEX, or Shape Expressions,<ref>[http://www.w3.org/2001/sw/wiki/ShEx]  Shape Expressions language</ref> is a language for expressing constraints on RDF graphs. It includes the cardinality constraints from [[Open Services for Lifecycle Collaboration|OSLC]] Resource Shapes and [[Dublin Core]] Description Set Profiles, as well as logical connectives for disjunction and polymorphism. 

As [[RDFS]] and [[Web Ontology Language|OWL]] demonstrate, one can build additional [[ontology language]]s upon RDF.

== History ==
The initial RDF design, intended to "build a vendor-neutral and operating system-independent system of metadata,"<ref name="press-release-1997">{{Cite news| last = | first = | title = World Wide Web Consortium Publishes Public Draft of Resource Description Framework| work = W3C| location = Cambridge, MA| date = 1997-10-03| url = http://www.w3.org/Press/RDF}}</ref>  derived from the W3C's [[Platform for Internet Content Selection]] (PICS), an early web content labelling system,<ref name="lash" /> but the project was also  shaped by ideas from [[Dublin Core]], and from the [[Meta Content Framework]] (MCF),<ref name="press-release-1997" /> which had been developed during 1995–1997 by [[Ramanathan V. Guha]] at [[Apple Computer|Apple]] and [[Tim Bray]] at [[Netscape Communications Corporation|Netscape]].<ref>{{Cite book| publisher = O’Reilly| isbn = 0-596-00881-3| last = Hammersley| first = Ben| title = Developing Feeds with RSS and Atom| pages=2–3|location = Sebastopol| date = 2005}}</ref>

A first public draft of RDF appeared in October 1997,<ref>{{Cite web| last1 = Lassila| first1 = Ora| last2 = Swick| first2 = Ralph R.| title = Resource Description Framework (RDF): Model and Syntax| work = W3C| accessdate = 2015-11-24| date = 1997-10-02| url = http://www.w3.org/TR/WD-rdf-syntax-971002/}}</ref><ref>{{Cite web|last=Swick |first=Ralph |title=Resource Description Framework (RDF) |work=W3C |accessdate=2015-11-24 |date=1997-12-11 |url=http://www13.w3.org/RDF/Overview.html |deadurl=yes |archiveurl=https://web.archive.org/web/19980214043631/http://www13.w3.org/RDF/Overview.html |archivedate=February 14, 1998 }}</ref> issued by a W3C working group that included representatives from [[IBM]], [[Microsoft]], [[Netscape]], [[Nokia]], [[Reuters]], [[SoftQuad Software|SoftQuad]], and the [[University of Michigan]].<ref name="lash">{{Cite news|last=Lash |first=Alex |title=W3C takes first step toward RDF spec |work=CNET News |accessdate=2015-11-28 |date=1997-10-03 |url=http://news.cnet.com/2100-1001-203893.html |deadurl=yes |archiveurl=https://web.archive.org/web/20110616023126/http://news.cnet.com/2100-1001-203893.html |archivedate=June 16, 2011 }}</ref>

The W3C published a specification of RDF's data model and an [[XML]] serialization as a recommendation in February 1999.<ref>{{cite web|url=http://www.w3.org/TR/1999/REC-rdf-syntax-19990222|title=Resource Description Framework (RDF) Model and Syntax Specification| date=22 Feb 1999|accessdate=5 May 2014}}</ref>

Two persistent misunderstandings developed around RDF at this time: firstly, from the MCF influence and the RDF "Resource Description" acronym, the idea that RDF was specifically for use in representing metadata. Secondly that RDF was an XML format, rather than RDF being a data model and only the RDF/XML serialisation being XML-based. RDF saw little take-up in this period, but there was significant work carried out in [[Bristol]], around ILRT at [[Bristol University]] and [[HP Labs]], and also in Boston at [[MIT]]. [[RSS 1.0]] and [[FOAF (ontology)|FOAF]] became exemplar applications for RDF in this period.

The recommendation of 1999 was replaced in 2004 by a set of six specifications: "The RDF Primer",<ref>{{Citation| publisher = W3C| last1 = Manola| first1 = Frank| last2 = Miller| first2 = Eric| title = RDF Primer| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-primer-20040210/}}</ref> "RDF Concepts and Abstract",<ref>{{Citation| publisher = W3C| last1 = Klyne| first1 = Graham| last2 = Carroll| first2 = Jeremy J.| title = Resource Description Framework (RDF): Concepts and Abstract Syntax| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/}}</ref> "RDF/XML Syntax Specification (revised)",<ref>{{Citation| publisher = W3C| last = Beckett| first = Dave| title = RDF/XML Syntax Specification (Revised)| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-syntax-grammar-20040210/}}</ref> "RDF Semantics",<ref>{{Citation| last = Hayes| first = Patrick| title = RDF Semantics| accessdate = 2015-11-21| date = 2014-02-10| url = http://www.w3.org/TR/2004/REC-rdf-mt-20040210/}}</ref> "RDF Vocabulary Description Language 1.0",<ref>{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Vocabulary Description Language 1.0: RDF Schema: W3C Recommendation 10 February 2004| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-schema-20040210/}}</ref> and "The RDF Test Cases".<ref>{{Citation| publisher = W3C| last1 = Grant| first1 = Jan| last2 = Beckett| first2 = Dave| title = RDF Test Cases| accessdate = 2015-11-21| date = 2004-02-10| url = http://www.w3.org/TR/2004/REC-rdf-testcases-20040210/}}</ref>

This series was superseded in 2014 by the following six "RDF 1.1" documents: "RDF 1.1 Primer,"<ref>{{Citation| publisher = W3C| last1 = Schreiber| first1 = Guus| last2 = Raimond| first2 = Yves| title = RDF 1.1 Primer| accessdate = 2015-11-22| date = 2014-06-24| url = http://www.w3.org/TR/2014/NOTE-rdf11-primer-20140624/}}</ref> "RDF 1.1 Concepts and Abstract Syntax,"<ref>{{Citation| publisher = W3C| last1 = Cyganiak| first1 = Richard| last2 = Wood| first2 = David| last3 = Lanthaler| first3 = Markus| title = RDF 1.1 Concepts and Abstract Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/}}</ref> "RDF 1.1 XML Syntax,"<ref>{{Citation| publisher = W3C| last1 = Gandon| first1 = Fabien| last2 = Schreiber| first2 = Guus| title = RDF 1.1 XML Syntax| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-syntax-grammar/}}</ref>
"RDF 1.1 Semantics,"<ref>{{Citation| publisher = W3C| last1 = Hayes| first1 = Patrick J.| last2 = Patel-Schneider| first2 = Peter F.| title = RDF 1.1 Semantics| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/REC-rdf11-mt-20140225/}}</ref> "RDF Schema 1.1,"<ref>{{Citation| publisher = W3C| last1 = Brickley| first1 = Dan| last2 = Guha| first2 = R.V.| title = RDF Schema 1.1| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/rdf-schema/}}</ref> and "RDF 1.1 Test Cases".<ref>{{Citation| publisher = W3C| last1 = Kellogg| first1 = Gregg| last2 = Lanthaler| first2 = Markus| title = RDF 1.1 Test Cases| accessdate = 2015-11-22| date = 2014-02-25| url = http://www.w3.org/TR/2014/NOTE-rdf11-testcases-20140225/}}</ref>

==RDF topics==

===RDF vocabulary===
The vocabulary defined by the RDF specification is as follows:<ref name="rdfschema">{{cite web|url=http://www.w3.org/TR/rdf-schema/|title=RDF Vocabulary Description Language 1.0: RDF Schema|publisher=[[W3C]]|date=2004-02-10|accessdate=2011-01-05}}</ref>

====Classes====

===== rdf =====
* '''<code>rdf:XMLLiteral</code>''' – the class of XML literal values
* '''<code>rdf:Property</code>''' – the class of properties
* '''<code>rdf:Statement</code>''' – the class of RDF statements
* '''<code>rdf:Alt</code>''', '''<code>rdf:Bag</code>''', '''<code>rdf:Seq</code>''' – containers of alternatives, unordered containers, and ordered containers (<code>rdfs:Container</code> is a super-class of the three)
* '''<code>rdf:List</code>''' – the class of RDF Lists
* '''<code>rdf:nil</code>''' – an instance of <code>rdf:List</code> representing the empty list

===== rdfs =====
* '''<code>rdfs:Resource</code>''' – the class resource, everything
* '''<code>rdfs:Literal</code>''' – the class of literal values, e.g. [[string literal|string]]s and [[integer]]s
* '''<code>rdfs:Class</code>''' – the class of classes
* '''<code>rdfs:Datatype</code>''' – the class of RDF datatypes
* '''<code>rdfs:Container</code>''' – the class of RDF containers
* '''<code>rdfs:ContainerMembershipProperty</code>''' – the class of container membership properties, <code>rdf:_1</code>, <code>rdf:_2</code>, ..., all of which are sub-properties of <code>rdfs:member</code>

====Properties====

=====rdf=====
* '''<code>rdf:type</code>''' – an instance of <code>rdf:Property</code> used to state that a resource is an instance of a class
* '''<code>rdf:first</code>''' – the first item in the subject RDF list
* '''<code>rdf:rest</code>''' – the rest of the subject RDF list after <code>rdf:first</code>
* '''<code>rdf:value</code>''' – idiomatic property used for structured values
* '''<code>rdf:subject</code>''' – the subject of the subject RDF statement
* '''<code>rdf:predicate</code>''' – the predicate of the subject RDF statement
* '''<code>rdf:object</code>''' – the object of the subject RDF statement

<code>rdf:Statement</code>, <code>rdf:subject</code>, <code>rdf:predicate</code>, <code>rdf:object</code> are used for [[reification (knowledge representation)|reification]] (see [[#Statement reification and context|below]]).

=====rdfs=====
* '''<code>rdfs:subClassOf</code>''' – the subject is a subclass of a class
* '''<code>rdfs:subPropertyOf</code>''' – the subject is a subproperty of a property
* '''<code>rdfs:domain</code>''' – a domain of the subject property
* '''<code>rdfs:range</code>''' – a range of the subject property
* '''<code>rdfs:label</code>''' – a human-readable name for the subject
* '''<code>rdfs:comment</code>''' – a description of the subject resource
* '''<code>rdfs:member</code>''' – a member of the subject resource
* '''<code>rdfs:seeAlso</code>''' – further information about the subject resource
* '''<code>rdfs:isDefinedBy</code>''' – the definition of the subject resource

This vocabulary is used as a foundation for [[RDF Schema]] where it is extended.

=== Serialization formats ===
{{Infobox file format
| name = RDF 1.1 Turtle serialization
| icon = 
| extension = .ttl
| mime = text/turtle<ref>{{cite web |url=http://www.w3.org/TR/turtle/#h2_sec-mediaReg |title=RDF 1.1 Turtle: Terse RDF Triple Language |publisher=W3C |date=9 Jan 2014 |accessdate=2014-02-22}}</ref>
| owner = [[World Wide Web Consortium]]
| standard = [http://www.w3.org/TR/turtle/ RDF 1.1 Turtle: Terse RDF Triple Language] {{release date and age|2014|01|09}}
| free = Yes
}}

{{Infobox file format
| name = RDF/XML serialization
| icon = [[Image:XML.svg|100px]]
| extension = .rdf
| mime = application/rdf+xml<ref>{{cite web |url=http://tools.ietf.org/html/rfc3870 |title=application/rdf+xml Media Type Registration |page=2 |publisher=IETF |date=September 2004 |accessdate=2011-01-08}}</ref>
| owner = [[World Wide Web Consortium]]
| standard = [http://www.w3.org/TR/2004/REC-rdf-concepts-20040210/ Concepts and Abstract Syntax] {{release date and age|2004|02|10}}
| free = Yes
}}

Several common [[Serialization|serialization formats]] are in use, including:
* '''[[Turtle (syntax)|Turtle]],'''<ref name="turtle">{{cite web
  |title=RDF 1.1 Turtle: Terse RDF Triple Language
  |url=http://www.w3.org/TR/turtle/
  |date=9 January 2014
  |publisher=W3C
}}</ref> a compact, human-friendly format.
* '''[[N-Triples]],'''<ref name="n-triples" >{{cite web
  |title=RDF 1.1 N-Triples: A line-based syntax for an RDF graph
  |date=9 January 2014
  |url=http://www.w3.org/TR/n-triples/
  |publisher=[[W3C]]
}}</ref> a very simple, easy-to-parse, line-based format that is not as compact as Turtle.
* '''[[N-Quads]],'''<ref>{{cite web
  |title=N-Quads: Extending N-Triples with Context
  |date=2012-06-25 
  |url=http://sw.deri.org/2008/07/n-quads/
}}</ref><ref name="n-quads" >{{cite web
  |title=RDF 1.1 N-Quads
  |date=January 2014
  |url=http://www.w3.org/TR/n-quads/
  |publisher=[[W3C]]
}}</ref> a superset of N-Triples, for serializing multiple RDF graphs.
* '''[[JSON-LD]],'''<ref name="json-ld">{{cite web|title=
JSON-LD 1.0: A JSON-based Serialization for Linked Data|url=http://www.w3.org/TR/json-ld/|publisher=W3C}}</ref> a [[JSON]]-based serialization.
* '''N3''' or [[Notation3]], a non-standard serialization that is very similar to Turtle, but has some additional features, such as the ability to define inference rules.
* '''[[RDF/XML]]''',<ref name="rdf-xml" >{{cite web
  |title=RDF 1.1 XML Syntax
  |date=25 February 2014
  |url=http://www.w3.org/TR/rdf-syntax-grammar/
  |publisher=[[W3C]]
}}</ref> an XML-based syntax that was the first standard format for serializing RDF.

RDF/XML is sometimes misleadingly called simply RDF because it was introduced among the other W3C specifications defining RDF and it was historically the first W3C standard RDF serialization format. However, it is important to distinguish the RDF/XML format from the abstract RDF model itself. Although the RDF/XML format is still in use, other RDF serializations are now preferred by many RDF users, both because they are more human-friendly,<ref name="rdf-xml-syntax-criticism">{{cite web|title=
Problems of the RDF syntax|url=http://milicicvuk.com/blog/2011/07/21/problems-of-the-rdf-syntax/|publisher=Vuk Miličić}}</ref>  and because some RDF graphs are not representable in RDF/XML due to restrictions on the syntax of XML [[QName]]s.

With a little effort, virtually any arbitrary [[XML]] may also be interpreted as RDF using [[GRDDL]] (pronounced 'griddle'), Gleaning Resource Descriptions from Dialects of Languages.

RDF triples may be stored in a type of database called a [[triplestore]].

=== Resource identification ===
The subject of an RDF statement is either a [[uniform resource identifier]] (URI) or a [[blank node]], both of which denote  [[web resource|resource]]s. Resources indicated by [[blank node]]s are called anonymous resources. They are not directly identifiable from the RDF statement. The predicate is a URI which also indicates a resource, representing a relationship. The object is a URI, blank node or a [[Unicode]] [[string literal]]. 
As of RDF 1.1 resources are identified by IRI's. IRI is a generalization of URI.<ref>RDF 1.1 Concepts and Abstract Syntax https://www.w3.org/TR/rdf11-concepts/</ref>

In Semantic Web applications, and in relatively popular applications of RDF like [[RSS (file format)|RSS]] and [[FOAF (software)|FOAF]] (Friend of a Friend), resources tend to be represented by URIs that intentionally denote, and can be used to access, actual data on the World Wide Web. But RDF, in general, is not limited to the description of Internet-based resources. In fact, the URI that names a resource does not have to be dereferenceable at all. For example, a URI that begins with "http:" and is used as the subject of an RDF statement does not necessarily have to represent a resource that is accessible via [[HTTP]], nor does it need to represent a tangible, network-accessible resource — such a URI could represent absolutely anything. However, there is broad agreement that a bare URI (without a # symbol) which returns a 300-level coded response when used in an HTTP GET request should be treated as denoting the internet resource that it succeeds in accessing.

Therefore, producers and consumers of RDF statements must agree on the semantics of resource identifiers. Such agreement is not inherent to RDF itself, although there are some controlled vocabularies in common use, such as [[Dublin Core]] Metadata, which is partially mapped to a URI space for use in RDF. The intent of publishing RDF-based ontologies on the Web is often to establish, or circumscribe, the intended meanings of the resource identifiers used to express data in RDF. For example, the URI:
<blockquote>
<code>
<nowiki>
http://www.w3.org/TR/2004/REC-owl-guide-20040210/wine#Merlot
</nowiki>
</code>
</blockquote>
is intended by its owners to refer to the class of all [[Merlot]] red wines by vintner (i.e., instances of the above URI each represent the class of all wine produced by a single vintner), a definition which is expressed by the OWL ontology — itself an RDF document — in which it occurs.  Without careful analysis of the definition, one might erroneously conclude that an instance of the above URI was something physical, instead of a type of wine.

Note that this is not a 'bare' resource identifier, but is rather a [[Uniform Resource Identifier#URI reference|URI reference]], containing the '#' character and ending with a [[fragment identifier]].

=== Statement reification and context ===
The body of knowledge modeled by a collection of statements may be subjected to [[Reification (knowledge representation)|reification]], in which each ''statement'' (that is each triple ''subject-predicate-object'' altogether) is assigned a URI and treated as a resource about which additional statements can be made, as in "''Jane says that'' John is the author of document X". Reification is sometimes important in order to deduce a level of confidence or degree of usefulness for each statement.

In a reified RDF database, each original statement, being a resource, itself, most likely has at least three additional statements made about it: one to assert that its subject is some resource, one to assert that its predicate is some resource, and one to assert that its object is some resource or literal. More statements about the original statement may also exist, depending on the application's needs.

Borrowing from concepts available in [[logic]] (and as illustrated in graphical notations such as [[conceptual graphs]] and [[topic map]]s), some RDF model implementations acknowledge that it is sometimes useful to group statements according to different criteria, called ''situations'', ''contexts'', or ''scopes'', as discussed in articles by RDF specification co-editor [[Graham Klyne]].<ref>[http://www.ninebynine.org/RDFNotes/RDFContexts.html Contexts for RDF Information Modelling]</ref><ref>[http://www.ninebynine.org/RDFNotes/UsingContextsWithRDF.html Circumstance, Provenance and Partial Knowledge]</ref> For example, a statement can be associated with a context, named by a URI, in order to assert an "is true in" relationship. As another example, it is sometimes convenient to group statements by their source, which can be identified by a URI, such as the URI of a particular RDF/XML document. Then, when updates are made to the source, corresponding statements can be changed in the model, as well.

Implementation of scopes does not necessarily require fully reified statements. Some implementations allow a single scope identifier to be associated with a statement that has not been assigned a URI, itself.<ref>[http://uche.ogbuji.net/tech/akara/nodes/2003-01-01/scopes The Concept of 4Suite RDF Scopes]</ref><ref>[http://librdf.org/notes/contexts.html Redland RDF Library – Contexts]</ref>  Likewise ''named graphs'' in which a set of triples is named by a URI can represent context without the need to reify the triples.<ref>[http://www.w3.org/2004/03/trix/ Named Graphs]</ref>

=== Query and inference languages ===
{{main|RDF query language}}
The predominant query language for RDF graphs is [[SPARQL]]. SPARQL is an [[SQL]]-like language, and a [[W3C recommendation|recommendation]] of the [[W3C]] as of January 15, 2008.

An example of a SPARQL query to show country capitals in Africa, using a fictional ontology.
<source lang="sparql">
PREFIX ex: <http://example.com/exampleOntology#>
SELECT ?capital ?country
WHERE {
  ?x ex:cityname ?capital ;
     ex:isCapitalOf ?y .
  ?y ex:countryname ?country ;
     ex:isInContinent ex:Africa .
}
</source>

Other non-standard ways to query RDF graphs include:
* [[RDQL]], precursor to [[SPARQL]], SQL-like
* Versa, compact syntax (non–SQL-like), solely implemented in [[4Suite]] ([[Python (programming language)|Python]])
* RQL, one of the first declarative languages for uniformly querying RDF schemas and resource descriptions, implemented in RDFSuite.<ref name=RQL>{{cite web|title=The RDF Query Language (RQL)|url=http://139.91.183.30:9090/RDF/RQL/index.html|work=The ICS-FORTH RDFSuite|publisher=ICS-FORTH}}</ref>
* [[SeRQL]], part of [[Sesame (framework)|Sesame]]
* [[XUL]] has a template element in which to declare rules for matching data in RDF. XUL uses RDF extensively for databinding.

== Examples ==

=== Example 1: RDF Description of a person named Eric Miller<ref name="rdf-primer">{{cite web|url= http://www.w3.org/TR/rdf-primer/|title= RDF Primer |publisher=[[W3C]]|accessdate=2009-03-13}}</ref> ===

The following  example is taken from the W3C website<ref name="rdf-primer" /> describing a resource with statements "there is a Person identified by <nowiki>http://www.w3.org/People/EM/contact#me</nowiki>, whose name is Eric Miller, whose email address is e.miller123(at)example (changed for security purposes), and whose title is Dr. [[Image:Rdf graph for Eric Miller.png|thumb|An RDF Graph Describing Eric Miller<ref name="rdf-primer" />]]

The resource "<nowiki>http://www.w3.org/People/EM/contact#me</nowiki>" is the subject.

The objects are:
* "Eric Miller" (with a predicate "whose name is"),
* <nowiki>mailto:e.miller123</nowiki>(at)example (with a predicate "whose email address is"), and
* "Dr." (with a predicate "whose title is").

The subject is a URI.

The predicates also have URIs. For example, the URI for each predicate:
* "whose name is" is <nowiki>http://www.w3.org/2000/10/swap/pim/contact#fullName</nowiki>,
* "whose email address is" is <nowiki>http://www.w3.org/2000/10/swap/pim/contact#mailbox</nowiki>,
* "whose title is" is <nowiki>http://www.w3.org/2000/10/swap/pim/contact#personalTitle</nowiki>.

In addition, the subject has a type (with URI <nowiki>http://www.w3.org/1999/02/22-rdf-syntax-ns#type</nowiki>), which is person (with URI <nowiki>http://www.w3.org/2000/10/swap/pim/contact#Person</nowiki>).

Therefore, the following "subject, predicate, object" RDF triples can be expressed:
* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#fullName,</nowiki> "Eric Miller"
* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#mailbox, mailto:e.miller123(at)example</nowiki>
* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/2000/10/swap/pim/contact#personalTitle,</nowiki> "Dr."
* <nowiki>http://www.w3.org/People/EM/contact#me, http://www.w3.org/1999/02/22-rdf-syntax-ns#type, http://www.w3.org/2000/10/swap/pim/contact#Person</nowiki>

In standard N-Triples format, this RDF can be written as:
<source lang="turtle">
<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#fullName> "Eric Miller" .
<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#mailbox> <mailto:e.miller123(at)example> .
<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/2000/10/swap/pim/contact#personalTitle> "Dr." .
<http://www.w3.org/People/EM/contact#me> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type> <http://www.w3.org/2000/10/swap/pim/contact#Person> .
</source>

Equivalently, it can be written in standard Turtle (syntax) format as:

<source lang="turtle">
@prefix eric:    <http://www.w3.org/People/EM/contact#> .
@prefix contact: <http://www.w3.org/2000/10/swap/pim/contact#> .
@prefix rdf:     <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .

eric:me contact:fullName "Eric Miller" .
eric:me contact:mailbox <mailto:e.miller123(at)example> .
eric:me contact:personalTitle "Dr." .
eric:me rdf:type contact:Person .
</source>

Or, it can be written in RDF/XML format as:
<source lang="xml" enclose="div">
<?xml version="1.0" encoding="utf-8"?>
<rdf:RDF xmlns:contact="http://www.w3.org/2000/10/swap/pim/contact#" xmlns:eric="http://www.w3.org/People/EM/contact#" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#">
  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">
    <contact:fullName>Eric Miller</contact:fullName>
  </rdf:Description>
  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">
    <contact:mailbox rdf:resource="mailto:e.miller123(at)example"/>
  </rdf:Description>
  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">
    <contact:personalTitle>Dr.</contact:personalTitle>
  </rdf:Description>
  <rdf:Description rdf:about="http://www.w3.org/People/EM/contact#me">
    <rdf:type rdf:resource="http://www.w3.org/2000/10/swap/pim/contact#Person"/>
  </rdf:Description>
</rdf:RDF>
</source>

=== Example 2: The postal abbreviation for New York ===

Certain concepts in RDF are taken from [[logic]] and [[linguistics]], where subject-predicate and subject-predicate-object structures have meanings similar to, yet distinct from, the uses of those terms in RDF. This example demonstrates:

In the [[English language]] statement '' 'New York has the postal abbreviation NY' '','' 'New York' '' would be the subject, '' 'has the postal abbreviation' '' the predicate and '' 'NY' '' the object.

Encoded as an RDF triple, the subject and predicate would have to be resources named by URIs. The object could be a resource or literal element. For example, in the N-Triples form of RDF, the statement might look like:

<source lang="turtle">
<urn:x-states:New%20York> <http://purl.org/dc/terms/alternative> "NY" .
</source>

In this example, "<nowiki>urn:x-states:New%20York</nowiki>" is the URI for a resource that denotes the US state [[New York (state)|New York]], "<nowiki>http://purl.org/dc/terms/alternative</nowiki>" is the URI for a predicate (whose human-readable definition can be found at here <ref>[http://dublincore.org/documents/dcmi-terms/index.shtml#terms-alternative DCMI Metadata Terms]. Dublincore.org. Retrieved on 2014-05-30.</ref>), and "NY" is a literal string.  Note that the URIs chosen here are not standard, and don't need to be, as long as their meaning is known to whatever is reading them.

=== Example 3: A Wikipedia article about Tony Benn ===
In a like manner, given that <nowiki>"http://en.wikipedia.org/wiki/Tony_Benn"</nowiki> identifies a particular resource (regardless of whether that URI could be traversed as a hyperlink, or whether the resource is ''actually'' the [[Wikipedia]] article about [[Tony Benn]]), to say that the title of this resource is "Tony Benn" and its publisher is "Wikipedia" would be two assertions that could be expressed as valid RDF statements. In the N-Triples form of RDF, these statements might look like the following:

<source lang="turtle">
<http://en.wikipedia.org/wiki/Tony_Benn> <http://purl.org/dc/elements/1.1/title> "Tony Benn" .
<http://en.wikipedia.org/wiki/Tony_Benn> <http://purl.org/dc/elements/1.1/publisher> "Wikipedia" .
</source>

To an English-speaking person, the same information could be represented simply as:
<blockquote>The title of this resource, which is published by Wikipedia, is 'Tony Benn'</blockquote>
However, RDF puts the information in a formal way that a machine can understand. The purpose of RDF is to provide an [[Semantics encoding|encoding]] and interpretation mechanism so that [[Resource (computer science)|resources]] can be described in a way that particular [[software]] can understand it; in other words, so that software can access and use information that it otherwise couldn't use.

Both versions of the statements above are wordy because one requirement for an RDF resource (as a subject or a predicate) is that it be unique. The subject resource must be unique in an attempt to pinpoint the exact resource being described. The predicate needs to be unique in order to reduce the chance that the idea of [[Title]] or [[Publisher]] will be ambiguous to software working with the description. If the software recognizes  ''<nowiki>http://purl.org/dc/elements/1.1/title</nowiki>'' (a specific [[definition]] for the [[concept]] of a title established by the [[Dublin Core]] Metadata Initiative), it will also know that this title is different from a land title or an honorary title or just the letters t-i-t-l-e put together.

The following example, written in Turtle, shows how such simple claims can be elaborated on, by combining multiple RDF vocabularies. Here, we note that the primary topic of the Wikipedia page is a "Person" whose name is "Tony Benn":

<source lang="turtle">
@prefix rdf:  <http://www.w3.org/1999/02/22-rdf-syntax-ns#> .
@prefix foaf: <http://xmlns.com/foaf/0.1/> .
@prefix dc:   <http://purl.org/dc/elements/1.1/> .

<http://en.wikipedia.org/wiki/Tony_Benn>
    dc:publisher "Wikipedia" ;
    dc:title "Tony Benn" ;
    foaf:primaryTopic [
        a foaf:Person ;
        foaf:name "Tony Benn"
    ] .
</source>

== Applications ==
* [[DBpedia]] – Extracts facts from Wikipedia articles and publishes them as RDF data.
* [[Creative Commons]] – Uses RDF to embed license information in web pages and mp3 files.
* [[FOAF (software)|FOAF (Friend of a Friend)]] – designed to describe [[person|people]], their interests and interconnections.
* [[Haystack (PIM)|Haystack client]] – Semantic web browser from MIT CS & AI lab.<ref>[http://groups.csail.mit.edu/haystack/ Haystack]</ref>
* [[IDEAS Group]] – developing a formal [[Ontology components|4D ontology]] for [[Enterprise Architecture]] using RDF as the encoding.<ref>[http://www.ideasgroup.org The IDEAS Group Website]</ref>
* Microsoft shipped a product, Connected Services Framework,<ref>[http://www.microsoft.com/serviceproviders/solutions/connectedservicesframework.mspx Connected Services Framework]</ref> which provides RDF-based Profile Management capabilities.
* [[MusicBrainz]] – Publishes information about Music Albums.<ref>[http://wiki.musicbrainz.org/RDF RDF on MusicBrainz Wiki]</ref>
* [[NEPOMUK (framework)|NEPOMUK]], an open-source software specification for a Social Semantic desktop uses RDF as a storage format for collected metadata. NEPOMUK is mostly known because of its integration into the [[KDE Software Compilation 4|KDE SC 4]] desktop environment.
* [[Press Association]] is a news agency in the UK. They use ontologies to dynamically identify and link their NoSQL data to do [[semantic publishing]] but in a dynamic, rules based way that creates custom content on the fly.<ref>[http://www.datalanguage.com/blog/2012/05/17/ontology-driven-software-engineering/]</ref>
* RDF Site Summary – one of several "[[RSS (file format)|RSS]]" languages for publishing information about updates made to a web page; it is often used for disseminating news article summaries and sharing [[weblog]] content.
* [[Simple Knowledge Organization System]] (SKOS) – a KR representation intended to support vocabulary/thesaurus applications
* [[SIOC|SIOC (Semantically-Interlinked Online Communities)]] – designed to describe online communities and to create connections between Internet-based discussions from message boards, weblogs and mailing lists.<ref>[http://sioc-project.org/ SIOC (Semantically-Interlinked Online Communities)]</ref>
* [[Smart-M3]] – provides an infrastructure for using RDF and specifically uses the ontology agnostic nature of RDF to enable heterogeneous mashing-up of information<ref>Oliver Ian, Honkola Jukka, Ziegler Jurgen (2008). “Dynamic, Localized Space Based Semantic Webs”. IADIS WWW/Internet 2008. Proceedings, p.426, IADIS Press, ISBN 978-972-8924-68-3</ref>

Some uses of RDF include research into social networking. It will also help people in business fields understand better their relationships with members of industries that could be of use for product placement.<ref>An RDF Approach for Discovering the Relevant Semantic Associations in a Social Network By Thushar A.K, and P. Santhi Thilagam</ref>  It will also help scientists understand how people are connected to one another.

RDF is being used to have a better understanding of road traffic patterns.  This is because the information regarding traffic patterns is  on different websites, and RDF is used to integrate information from different sources on the web. Before, the common methodology was using keyword searching, but this method is problematic because it does not  consider synonyms. This is why ontologies are useful in this situation. But one of the issues that comes up when trying to efficiently study traffic is that to fully understand traffic,  concepts related to people, streets, and roads must be well understood. Since these are human  concepts, they require the addition of [[fuzzy logic]]. This is because values that are useful  when describing roads, like slipperiness, are not precise concepts and cannot be measured. This would imply that the best solution would incorporate both fuzzy logic and ontology.<ref>Traffic Information Retrieval Based on Fuzzy Ontology and RDF on the Semantic Web By Jun Zhai, Yi Yu, Yiduo Liang, and Jiatao Jiang (2008)</ref>

== See also ==
;Notations for RDF
* [[TriG (syntax)|TRiG]]
* [[TriX (syntax)|TRiX]]
* [[RDF/XML]]
* [[RDFa]]
* [[JSON-LD]]
;Similar concepts
* [[Entity-attribute-value model]]
* [[Graph theory]] – An RDF model is a labeled, directed multi-graph.
* [[Website Parse Template]]
* [[Tag (metadata)|Tagging]]
* [[SciCrunch]]
* [[Semantic network]]
; Other (unsorted):
*[[Associative model of data]]
*[[Business Intelligence 2.0]] (BI 2.0)
*DataPortability
* [[EU Open Data Portal]]
*[[Folksonomy]]
*[[LSID|Life Science Identifiers]]
*[[Swoogle]]
*[[Universal Networking Language]] (UNL)

== References ==
{{Reflist|2}}

== Further reading ==
* [http://www.w3.org/RDF/ W3C's RDF at W3C]: specifications, guides, and resources
* [http://www.w3.org/TR/2004/REC-rdf-mt-20040210/ RDF Semantics]: specification of semantics, and complete systems of inference rules for both RDF and RDFS

== External links ==
{{Commons category|Resource Description Framework}}
*{{DMOZ|Reference/Libraries/Library_and_Information_Science/Technical_Services/Cataloguing/Metadata/RDF/}}
{{Semantic Web}}
{{W3C Standards}}
{{Data Exchange}}

{{Authority control}}

[[Category:Resource Description Framework| ]]
[[Category:Knowledge representation]]
[[Category:World Wide Web Consortium standards]]
[[Category:XML]]
[[Category:XML-based standards]]
[[Category:Metadata]]
[[Category:Semantic Web]]
[[Category:Bibliography file formats]]
<=====doc_Id=====>:725
<=====title=====>:
John F. Sowa
<=====text=====>:
{{Infobox person
 | name             = John F. Sowa
 | image            =
 | image_size       = 
 | caption          = 
 | birth_name       = John Florian Sowa
 | birth_date       = {{Birth date and age|mf=yes|1940|1|1}}
 | birth_place      = 
 | death_date       = 
 | death_place      = 
 | death_cause      = 
 | resting_place    = 
 | residence        = [[Croton-on-Hudson, New York]]
 | nationality      = 
 | other_names      =
 | known_for        = [[Conceptual graph]]s
 | education        = [[Massachusetts Institute of Technology]] BS 1962, [[Harvard University]] MA 1966, [[Vrije Universiteit Brussel]] PhD 1999
 | alma_mater       = 
 | employer         = 
 | occupation       = Computer Scientist
 | boards           = 
 | religion         = 
 | spouse           = [[Cora Angier Sowa]]
 | children         = 
 | parents          = 
 | relations        =
 | callsign         = 
 | awards           = 
 | signature        =
 | website          = {{URL|http://www.jfsowa.com/|JFSowa.com}}
| 
}}
'''John Florian Sowa''' (born 1940) is an American [[computer scientist]], an expert in [[artificial intelligence]] and [[computer design]], and the inventor of [[conceptual graph]]s.<ref>[[Kecheng Liu]] (2000) ''Semiotics in Information Systems Engineering''. p.54 states: ''Conceptual graphs are devised as a language of knowledge representation by Sowa (1984), based on philosophy, psychology and linguistics. Knowledge in conceptual graph form is highly structured by modelling specialised facts that can be subjected to generalised reasoning.</ref><ref>Marite Kirikova (2002) ''Information Systems Development: Advances in Methodologies, Components, and Management''. p.194. states: ''The original theory of conceptual graphs was introduced by Sowa (Sowa, 1984 ). A conceptual graph is a finite, connected, bipartite graph. It includes notions of concepts, relations, and actors...''</ref>

== Biography ==
Sowa received a BS in mathematics from [[Massachusetts Institute of Technology]] in 1962, an MA in applied mathematics from [[Harvard University]] in 1966, and a PhD in [[computer science]] from the [[Vrije Universiteit Brussel]] in 1999 on a dissertation titled "Knowledge Representation: Logical, Philosophical, and Computational Foundations".<ref>Andreas Tolk, Lakhmi C. Jain (2011) ''Intelligent-Based Systems Engineering''. p.xxi</ref>

Sowa spent most of his professional career at [[International Business Machines|IBM]], which started in 1962 at IBM's applied mathematics group. Over the decades he has researched and developed emerging fields of [[computer science]] from compiler, programming languages, and system architecture<ref name="SoZa92">John F. Sowa and [[John Zachman]] (1992). [http://www.research.ibm.com/journal/sj/313/sowa.pdf "Extending and Formalizing the Framework for Information Systems Architecture"] In: ''IBM Systems Journal'', Vol 31, no.3, 1992. p. 590-616.</ref> to artificial intelligence and knowledge representation. In the 1990s Sowa was associated with IBM Educational Center in New York. Over the years he taught courses at the IBM Systems Research Institute, [[Binghamton University]], [[Stanford University]], [[Linguistic Society of America]] and [[Université du Québec à Montréal]]. He is a fellow of the [[Association for the Advancement of Artificial Intelligence]].

After early retirement at IBM Sowa in 2001 cofounded VivoMind Intelligence, Inc. with [[Arun K. Majumdar]]. With this company he was developing data-mining and database technology, more specific high-level "[[ontology|ontologies]]" for [[artificial intelligence]] and automated [[natural language understanding]]. Currently Sowa is working with [http://kyndi.com/ Kyndi Inc.], also founded by Majumdar. 

John Sowa is married to the philologist Cora Angier Sowa,<ref>Cora Angier Sowa (1984) ''Traditional themes and the Homeric hymns''. p.iv</ref> and they live in [[Croton-on-Hudson, New York]].

== Work ==
Sowa's research interest since the 1970s were in the field of [[artificial intelligence]], [[expert systems]] and [[database query]] linked to natural languages.<ref name="SoZa92"/> In his work he combines ideas from numerous disciplines and eras modern and ancient, for example, applying ideas from [[Aristotle]], the medieval [[Scholastics]] to [[Alfred North Whitehead]] and including [[logical schema|database schema]] theory, and incorporating the model of analogy of Islamic scholar [[Ibn Taymiyyah]] in his works.<ref>[http://www.jfsowa.com/pubs/analog.htm Analogical Reasoning]</ref>

=== Conceptual graph ===
{{main|Conceptual graph}}
Sowa invented conceptual graphs, a graphic notation for logic and natural language, based on the structures in [[semantic network]]s and on the [[existential graph]]s of [[Charles Sanders Peirce|Charles S. Peirce]]. He published the concept in the 1976 article "Conceptual graphs for a data base interface" in the ''IBM Journal of Research and Development''.<ref>{{cite journal |last=Sowa |authorlink = |first= John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336–357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}</ref> He further explained in the 1983 book ''Conceptual structures: information processing in mind and machine''.

In the 1980s this theory has "been adopted by a number of research and development groups throughout the world.<ref name="SoZa92"/> International conferences on conceptual graphs have been held for over a decade since before 1992.{{citation needed|date=November 2012}}

==={{anchor|law of standards}}Sowa's law of standards===
In 1991, Sowa first stated his ''Law of Standards'': 
: "Whenever a major organization develops a new system as an official [[Technical standard|standard]] for X, the primary result is the widespread adoption of some simpler system as a [[de facto]] standard for X."<ref>[http://www.jfsowa.com/computer/standard.htm Law of Standards]</ref> 
Like [[Gall's law]], The Law of Standards is essentially an argument in favour of underspecification. Examples include:

*The introduction of [[PL/I]] resulting in [[COBOL]] and [[FORTRAN]] becoming the de facto standards for scientific and business programming
*The introduction of [[Algol-68]] resulting in [[Pascal (programming language)|Pascal]] becoming the de facto standard for academic programming
*The introduction of the [[Ada (programming language)|Ada language]] resulting in [[C (programming language)|C]] becoming the de facto standard for [[United States Department of Defense|DoD]] programming
*The introduction of [[OS/2]] resulting in [[Microsoft Windows|Windows]] becoming the de facto standard for [[desktop OS]]
*The introduction of [[X.400]] resulting in [[SMTP]] becoming the de facto standard for [[electronic mail]]
*The introduction of [[X.500]] resulting in [[LDAP]] becoming the de facto standard for [[directory services]]

== Publications ==
* 1984. ''Conceptual Structures - Information Processing in Mind and Machine''. The Systems Programming Series, Addison-Wesley<ref>[http://conceptualstructures.org/ Conceptual Structures Home Page]. Retrieved Nov 23, 2012.</ref>
* 1991. ''Principles of Semantic Networks''. Morgan Kaufmann.
* {{Cite journal| editor1-last = Mineau | editor1-first = Guy W| editor2-last = Moulin | editor2-first = Bernard| editor3-last = Sowa | editor3-first = John F | editor3-link = John F. Sowa| title = Conceptual Graphs for Knowledge Representation| doi = 10.1007/3-540-56979-0| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 699| year = 1993| isbn = 978-3-540-56979-4}}
* 1994. ''International Conference on Conceptual Structures (2nd : 1994 : College Park, Md.)	Conceptual structures, current practices : Second International Conference on Conceptual Structures, ICCS'94, College Park, Maryland, USA, August 16–20, 1994 : proceedings''. William M. Tepfenhart, Judith P. Dick, John F. Sowa, eds.
*{{Cite journal| editor1-last = Ellis | editor1-first = Gerard| editor2-last = Levinson | editor2-first = Robert| editor3-last = Rich | editor3-first = William| editor4-last = Sowa | editor4-first = John F | editor4-link = John F. Sowa| doi = 10.1007/3-540-60161-9| title = Conceptual Structures: Applications, Implementation and Theory| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 954| year = 1995| isbn = 978-3-540-60161-6}}
*{{Cite journal| editor1-last = Lukose | editor1-first = Dickson| editor2-last = Delugach | editor2-first = Harry| editor3-last = Keeler | editor3-first = Mary| editor4-last = Searle | editor4-first = Leroy| editor5-last = Sowa | editor5-first = John | editor5-link = John F. Sowa| doi = 10.1007/BFb0027865| title = Conceptual Structures: Fulfilling Peirce's Dream| series = [[Lecture Notes in Computer Science|LNCS]]| volume = 1257| year = 1997| isbn = 3-540-63308-1}}
* 2000. ''Knowledge representation : logical, philosophical, and computational foundations'', Brooks Cole Publishing Co., Pacific Grove<ref>[http://www.jfsowa.com/krbook/ Knowledge Representation: Logical, Philosophical, and Computational Foundations] at jfsowa.com. Retrieved Nov 23, 2012.</ref>

;Articles, a selection<ref>{{DBLP|name=John F. Sowa}}</ref>
*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| title = Conceptual Graphs for a Data Base Interface| doi = 10.1147/rd.204.0336| journal = IBM Journal of Research and Development| volume = 20| issue = 4| pages = 336–357| date=July 1976 }}
*{{Cite journal| last1 = Sowa | first1 = J. F. | author1-link = John F. Sowa| last2 = Zachman | first2 = J. A.| doi = 10.1147/sj.313.0590| title = Extending and formalizing the framework for information systems architecture| journal = IBM Systems Journal| volume = 31| issue = 3| pages = 590–616| year = 1992}}
* 1992. "[http://www.jfsowa.com/cg/cgif.htm Conceptual Graph Summary]"; In: T.E. Nagle et. al. (Eds.). ''Conceptual Structures: Current Research and Practice''. Chichester: Ellis Horwood. 
* 1995. "Top-level ontological categories." in: ''International journal of human-computer studies''. Vol. 43, Iss. 5–6, Nov. 1995, pp.&nbsp;669–685
* 2006. "Semantic Networks". In: ''Encyclopedia of Cognitive Science.''. John Wiley & Sons.

== References ==
{{reflist}}

== External links ==
{{Wikiquote}}
* [http://www.jfsowa.com/ John F. Sowa] homepage

{{Authority control}}

{{DEFAULTSORT:Sowa, John}}
[[Category:1940 births]]
[[Category:Artificial intelligence researchers]]
[[Category:Knowledge representation]]
[[Category:Living people]]
[[Category:People from Croton-on-Hudson, New York]]
[[Category:Harvard University alumni]]
[[Category:Binghamton University faculty]]
<=====doc_Id=====>:728
<=====title=====>:
Conceptual graph
<=====text=====>:
'''Conceptual graphs''' ('''CGs''') are a formalism for [[knowledge representation]]. In the first published paper  on CGs, [[John F. Sowa]] {{harv|Sowa|1976}} used them to represent the [[conceptual schema]]s used in [[database system]]s. The first book on CGs {{harv|Sowa|1984}} applied them to a wide range of topics in [[artificial intelligence]], [[computer science]], and [[cognitive science]].

Since 1984, the model has been developed along three main directions.

== A graphical interface for first-order logic ==
In this approach, a formula in [[first-order logic]] (predicate calculus) is represented by a labeled graph.

A linear notation, called the '''Conceptual Graph Interchange Format (CGIF)''', has been standardized in the ISO standard for [[common logic]].

[[Image:Cat-on-mat.svg|thumb|250px|Elsie the cat is sitting on a mat]]
The diagram on the right is an example of the '''display form''' for a conceptual graph.  Each box is called a '''concept node''', and each oval is called a '''relation node'''.  In CGIF, this CG would be represented by the following statement:

    [Cat Elsie] [Sitting *x] [Mat *y] (agent ?x Elsie) (location ?x ?y)
In CGIF, brackets enclose the information inside the concept nodes, and parentheses enclose the information inside the relation nodes.  The letters x and y, which are called '''coreference labels''', show how the concept and relation nodes are connected.  In the '''Common Logic Interchange Format (CLIF)''', those letters are mapped to variables, as in the following statement:

    (exists ((x Sitting) (y Mat)) (and (Cat Elsie) (agent x Elsie) (location x y)))

As this example shows, the asterisks on the coreference labels *x and *y in CGIF map to existentially quantified variables in CLIF, and the question marks on ?x and ?y map to bound variables in CLIF.  A universal quantifier, represented '''@every*z''' in CGIF, would be represented '''forall (z)''' in CLIF.

Reasoning can be done by translating graphs into logical formulas, then applying a logical inference engine.

== Diagrammatic calculus of logics ==
Another research branch continues the work on [[existential graph]]s of [[Charles Sanders Peirce]], which were one of the origins of conceptual graphs as proposed by Sowa.
In this approach, developed in particular by Dau {{harv|Dau|2003}}, conceptual graphs are conceptual [[diagram]]s rather than graphs in the sense of [[graph theory]], and reasoning operations are performed by operations on these diagrams.

== Graph-based knowledge representation and reasoning model ==
Key features of '''GBKR''', the graph-based knowledge representation and reasoning model developed by Chein and Mugnier and the Montpellier group {{harv|Chein|Mugnier|2009}},
can be summarized as follows:

* all kinds of knowledge (ontology, rules, constraints and facts) are labeled graphs, which provide an intuitive and easily understandable means to represent knowledge,
* reasoning mechanisms are based on graph notions, basically the classical notion of graph homomorphism; this allows, in particular, to link basic reasoning problems to other fundamental problems in computer science (problems concerning conjunctive queries in relational databases, constraint satisfaction problem, ...),
* the formalism is logically founded, i.e., it has a semantics in first-order logic and the inference mechanisms are sound and complete with respect to deduction in first-order logic,
* from a computational viewpoint, the graph homomorphism notion was recognized in the 1990s as a central notion, and complexity results and efficient algorithms have been obtained in several domains.

COGITANT and COGUI are tools that implement the '''GBKR''' model. COGITANT is a library of C++ classes that implement most of the GBKR notions and reasoning mechanisms. COGUI  is a graphical user interface dedicated to the construction of a GBKR knowledge base (it integrates COGITANT and, among numerous functionalities, it contains a translator from GBKR to RDF/S and conversely).

== Sentence generalization and generalization diagrams ==
Sentence [[generalization]] and generalization diagrams can be defined as a special sort of conceptual graphs which can be constructed automatically from syntactic [[parse tree]]s and support semantic classification task {{harv|Galitsky et al|2010}}. Similarity measure between syntactic parse trees can be done as a generalization operation on the lists of sub-trees of these trees. The diagrams are representation of mapping between the [[syntax]] generalization level and [[semantics]] generalization level (anti-unification of [[logic forms]]). Generalization diagrams are intended to be more accurate semantic representation than conventional conceptual graphs for individual sentences because only syntactic commonalities are represented at semantic level.

==See also==
*[[Resource Description Framework]] (RDF)
*[[SPARQL]] (Graph Query Language)
*[[Semantic network]]
*[[Knowledge representation]]
*[[Chunking (psychology)]]
*[[Concept map]]
*[[Conceptual schema]]
*[[Diagrammatic reasoning]]

==References==
* {{cite book |last=Chein |first=Michel |last2=Mugnier |first2=Marie-Laure |year=2009 |title=Graph-based Knowledge Representation: Computational Foundations of Conceptual Graphs |publisher=Springer |url=http://www.lirmm.fr/gbkrbook/ |isbn=978-1-84800-285-2 |ref=harv | doi=10.1007/978-1-84800-286-9}}
* {{cite journal |last=Dau |first=F. |year=2003 |title=The Logic System of Concept Graphs with Negation and Its Relationship to Predicate Logic |journal=[[Lecture Notes in Computer Science]] |volume=2892 |publisher=Springer |isbn= |ref=harv }}
* {{cite journal |last=Sowa |authorlink = John Sowa |first=John F. |date=July 1976 |title=Conceptual Graphs for a Data Base Interface |journal=IBM Journal of Research and Development |volume=20 |issue=4 |pages=336–357 |url=http://www.research.ibm.com/journal/rd/204/ibmrd2004E.pdf |ref=harv |doi=10.1147/rd.204.0336}}
* {{cite book |last=Sowa |first=John F. |year=1984 |title=Conceptual Structures:  Information Processing in Mind and Machine |location=Reading, MA |publisher=Addison-Wesley |isbn=978-0-201-14472-7 |ref=harv }}
* {{cite journal | last=Galitsky | first=Boris | last2=Dobrocsi |first2=Gabor | last3=de la Rosa |first3=Josep Lluis | last4=Kuznetsov |first4=Sergei O. |year=2010 |title=From Generalization of Syntactic Parse Trees to Conceptual Graphs |journal=Lecture Notes in Computer Science |volume=6208 |publisher=Springer |isbn= |url=http://dl.acm.org/citation.cfm?id=1881190|ref=harv }}
* {{cite journal|title=Conceptual graphs for the analysis and generation of sentences
|first1=Paola |last1=Velardi |first2=Maria Teresa |last2=Pazienza |first3=Mario |last3=De' Giovanetti |journal=IBM Journal of Research and Development |volume=32 |number=2 |date=March 1988 |pages=251–267 |publisher=IBM Corp. Riverton, NJ, USA |doi=10.1147/rd.322.0251}}

==External links==
* [http://conceptualstructures.org Conceptual Structures Home Page]. (Old site:  [http://conceptualgraphs.org Conceptual Graphs Home Page])
* [http://www.informatik.uni-trier.de/~ley/db/conf/iccs/index.html Yearly international conferences (ICCS)]
* [http://www.jfsowa.com/cg/index.htm Conceptual Graphs on John F. Sowa's Website]

[[Category:Knowledge representation]]
[[Category:Diagrams]]
[[Category:Application-specific graphs]]
<=====doc_Id=====>:731
<=====title=====>:
Simple Knowledge Organization System
<=====text=====>:
'''Simple Knowledge Organization System''' ('''SKOS''') is a [[W3C recommendation]] designed for representation of [[Thesaurus (information retrieval)|thesauri]], [[classification scheme]]s, [[Taxonomy (general)|taxonomies]], [[Authority control|subject-heading systems]], or any other type of structured [[controlled vocabulary]]. SKOS is part of the [[Semantic Web]] family of standards built upon [[Resource Description Framework|RDF]] and [[RDF Schema|RDFS]], and its main objective is to enable easy publication and use of such vocabularies as [[linked data]].

== History ==

=== DESIRE II project (1997&ndash;2000) ===

The most direct ancestor to SKOS was the RDF Thesaurus work undertaken in the second phase of the EU DESIRE project <ref name="Desire Project">{{Citation |publication-date=August 7, 2000 |title=Desire: Development of a European Service for Information on Research and Education |publisher=Desire Consortium |url=http://www.desire.org/ |archiveurl=https://web.archive.org/web/20110725230823/http://www.desire.org/ |archivedate=July 25, 2011 }}</ref>{{Citation needed|reason=The Desire Project reference does not appear to directly address the SKOS ancestry statement made here.|date=August 2012}}.  Motivated by the need to improve the user interface and usability of multi-service browsing and searching,<ref name="Desire Deliverable D.36b">{{Citation |title=Desire: Research Deliverables: D3.1 |publisher=Desire Consortium |url=http://www.desire.org/docs/research/deliverables/D3.6/d36b.html |archiveurl=https://web.archive.org/web/20080509135041/http://www.desire.org/html/research/deliverables/D3.6/#d36b |archivedate=May 9, 2008 }}</ref> a basic RDF vocabulary for Thesauri was [http://www.desire.org/results/discovery/rdfthesschema.html produced]. As noted later in the [http://www.w3.org/2001/sw/Europe/plan/workpackages/live/esw-wp-8.html SWAD-Europe workplan], the DESIRE work was adopted and further developed in the SOSIG and LIMBER projects. A version of the DESIRE/SOSIG implementation was described in W3C's QL'98 workshop, motivating early work on RDF rule and query languages: [http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF].<ref>[http://www.w3.org/TandS/QL/QL98/pp/queryservice.html A Query and Inference Service for RDF]</ref>

=== LIMBER (1999&ndash;2001) ===

SKOS built upon the output of the Language Independent Metadata Browsing of European Resources (LIMBER) project funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. In the LIMBER project [[CCLRC]] further developed an [[Resource Description Framework|RDF]]  thesaurus interchange format<ref>[http://journals.tdl.org/jodi/article/viewArticle/34/35 Miller, K. & Matthews, B. (2001). Having the right connections: the LIMBER Project. Journal of Digital Information, 1 (8), 5 February. ]</ref> which was demonstrated on the European Language Social Science Thesaurus ([http://www.cessda.org/results.html?query=elsst ELSST]) at the [[UK Data Archive]] as a multilingual version of the English language Humanities and Social Science Electronic Thesaurus (HASSET) which was planned to be used by the Council of European Social Science Data Archives [http://www.cessda.org/ CESSDA].

=== SWAD-Europe (2002&ndash;2004) ===

SKOS as a distinct initiative began in the SWAD-Europe project, bringing together partners from both DESIRE, SOSIG (ILRT) and LIMBER (CCLRC) who had worked with earlier versions of the schema. It was developed in the Thesaurus Activity Work Package, in the Semantic Web Advanced Development for Europe (SWAD-Europe) project.<ref>[http://www.w3.org/2001/sw/Europe/ SWAD-Europe]</ref> SWAD-Europe was funded by the [[European Community]], and part of the [[Information Society Technologies]] programme. The project was designed to support W3C's Semantic Web Activity through research, demonstrators and outreach efforts conducted by the five project partners, [[ERCIM]], the [http://www.ilrt.bris.ac.uk/ ILRT] at [[Bristol University]], [[HP Labs]], [[CCLRC]] and Stilo.<ref>[http://www.stilo.com Stilo Home Page]</ref>
The first release of SKOS Core and SKOS Mapping were published at the end of 2003, along with other deliverables on RDF encoding of multilingual thesauri<ref>[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.3.html SWAD-Europe Deliverable 8.3 : RDF Encoding of Multilingual Thesauri]</ref> and thesaurus mapping.<ref>[http://www.w3c.rl.ac.uk/SWAD/deliverables/8.4.html SWAD-Europe Deliverable 8.4 : Inter-Thesaurus Mapping]</ref>

=== Semantic web activity (2004&ndash;2005) ===

Following the termination of SWAD-Europe, SKOS effort was supported by the W3C Semantic Web Activity<ref>[http://www.w3.org/2001/sw/ W3C Semantic Web Activity]</ref> in the framework of the Best Practice and Deployment Working Group.<ref>[http://www.w3.org/2004/03/thes-tf/mission W3C Semantic Web Best Practice and Deployment Working Group : Porting Thesauri Task Force]</ref> During this period, focus was put both on consolidation of SKOS Core, and development of practical guidelines for porting and publishing thesauri for the Semantic Web.

=== Development as W3C Recommendation (2006&ndash;2009)===

The SKOS main published documents — the SKOS Core Guide,<ref>[http://www.w3.org/TR/swbp-skos-core-guide SKOS Core Guide] W3C Working Draft 2 November 2005</ref> the SKOS Core Vocabulary Specification,<ref>[http://www.w3.org/TR/swbp-skos-core-spec SKOS Core Vocabulary Specification] W3C Working Draft 2 November 2005</ref> and the Quick Guide to Publishing a Thesaurus on the Semantic Web<ref>[http://www.w3.org/TR/swbp-thesaurus-pubguide Quick Guide to Publishing a Thesaurus on the Semantic Web] W3C Working Draft 17 May 2005</ref> — were developed through the W3C Working Draft process. Principal editors of SKOS were Alistair Miles,<ref>[http://purl.org/net/aliman Alistair Miles Home Page]</ref> initially Dan Brickley,<ref>[http://danbri.org/ Dan Brickley Home Page]</ref> and Sean Bechhofer.<ref>[http://www.cs.man.ac.uk/~seanb/#me Sean Bechhofer Home Page]</ref>

The Semantic Web Deployment Working Group,<ref>[http://www.w3.org/2006/07/SWD/ W3C Semantic Web Deployment Working Group]</ref> chartered for two years (May 2006 - April 2008), has put in its charter to push SKOS forward on the [[W3C Recommendation]] track. The roadmap projects SKOS as a Candidate Recommendation by the end of 2007, and as a Proposed Recommendation in the first quarter of 2008. The main issues to solve are determining its precise scope of use, and its articulation with other RDF languages and standards used in libraries (such as [[Dublin Core]]).<ref>[http://isegserv.itd.rl.ac.uk/public/skos/press/dc2006/camera-ready-paper.pdf SKOS: Requirements for Standardization]. The paper by Alistair Miles presented in October 2006 at the International Conference on Dublin Core and Metadata Applications.</ref><ref>[http://purl.org/net/retrieval Retrieval and the Semantic Web, incorporating a Theory of Retrieval Using Structured Vocabularies]. Dissertation on the theory of retrieval using structured vocabularies by Alistair Miles.</ref>

=== Formal release (2009) ===
On August 18, 2009, [[W3C]] released the new standard that builds a bridge between the world of knowledge organization systems - including thesauri, classifications, subject headings, taxonomies, and [[folksonomy|folksonomies]] - and the [[linked data]] community, bringing benefits to both. Libraries, museums, newspapers, government portals, enterprises, social networking applications, and other communities that manage large collections of books, historical artifacts, news reports, business glossaries, blog entries, and other items can now use SKOS<ref>[http://www.w3.org/TR/2009/REC-skos-reference-20090818/ Simple Knowledge Organization System (SKOS)]</ref> to leverage the power of linked data.

=== Historical view of components ===

SKOS was originally designed as a modular and extensible family of languages, organized as SKOS Core, SKOS Mapping, and SKOS Extensions, and a Metamodel. The entire specification is now complete within the namespace [http://www.w3.org/2004/02/skos/core# http://www.w3.org/2004/02/skos/core#].

== Overview ==

In addition to the reference itself, the [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/ SKOS Primer] (a W3C Working Group Note) summarizes the Simple Knowledge Organization System.

The SKOS<ref>[http://www.w3.org/TR/skos-reference SKOS Reference]</ref> defines the classes and properties sufficient to represent the common features found in a standard thesaurus. It is based on a concept-centric view of the vocabulary, where primitive objects are not terms, but abstract notions represented by terms. Each SKOS concept is defined as an [[web resource|RDF resource]]. Each concept can have RDF properties attached, including:
* one or more preferred [[index term]]s (at most one in each natural language)
* alternative terms or [[synonym]]s
* definitions and notes, with specification of their language

Concepts can be organized in [[hierarchy|hierarchies]] using broader-narrower relationships, or linked by non-hierarchical (associative) relationships.
Concepts can be gathered in concept schemes, to provide consistent and structured sets of concepts, representing whole or part of a controlled vocabulary.

=== Element categories ===

The principal element categories of SKOS are concepts, labels, notations, semantic relations, mapping properties, and collections. The associated concepts are listed in the table below.

{| border="1" class="wikitable"
|+ SKOS Vocabulary Organized by Theme
! Concepts
! Labels & Notation
! Documentation
! Semantic Relations
! Mapping Properties
! Collections
|-
| Concept || prefLabel  || note || broader || broadMatch || Collection 
|-
| ConceptScheme || altLabel  || changeNote || narrower || narrowMatch || orderedCollection 
|-
| inScheme || hiddenLabel || definition || related || relatedMatch || member
|-
| hasTopConcept || notation  || editorialNote || broaderTransitive || closeMatch || memberList
|-
| topConceptOf ||   || example || narrowerTransitive || exactMatch ||  
|-
|   ||   || historyNote || semanticRelation || mappingRelation || 
|-
|   ||   || scopeNote ||  ||  || 
|-

|}

=== Concepts ===

The SKOS vocabulary is based on concepts. Concepts are the units of thought—ideas, meanings, or objects and events (instances or categories)—which underlie many knowledge organization systems. As such, concepts exist in the mind as abstract entities which are independent of the terms used to label them. In SKOS, a <code>Concept</code> (based on the OWL <code>Class</code>) is used to represent items in a knowledge organization system (terms, ideas, meanings, etc.) or such a system's conceptual or organizational structure.

A <code>ConceptScheme</code> is analogous to a vocabulary, thesaurus, or other way of organizing concepts. SKOS does not constrain a concept to be within a particular scheme, nor does it provide any way to declare a complete scheme—there is no way to say the scheme consists only of certain members. A topConcept is (one of) the upper concept(s) in a hierarchical scheme.

=== Labels and notations ===

Each SKOS <code>label</code> is a string of [[Unicode]] characters, optionally with language tags, that are associated with a concept. The <code>prefLabel</code> is the preferred human-readable string (maximum one per language tag), while <code>altLabel</code> can be used for alternative strings, and <code>hiddenLabel</code> can be used for strings that are useful to associate, but not meant for humans to read.

A SKOS <code>notation</code> is similar to a label, but the literal string has a datatype, like integer, float, or date; the datatype can even be made up (see [http://www.w3.org/TR/skos-reference/#L2613 6.5.1 Notations, Typed Literals and Datatypes] in the SKOS Reference). The notation is useful for classification codes and other strings not recognizable as words.

=== Documentation ===

The Documentation or Note properties provide basic information about SKOS concepts. All the concepts are considered a type of <code>skos:note</code>; they just provide more specific kinds of information. The property <code>definition</code>, for example, should contain a full description of the subject resource.  More specific note types can be defined in a SKOS extension, if desired. A query for <code>&lt;A&gt; skos:note ?</code> will obtain all the notes about &lt;A&gt;, including definitions, examples, and scope, history and change, and editorial documentation.

Any of these SKOS Documentation properties can refer to several object types: a literal (e.g., a string); a resource node that has its own properties; or a reference to another document, for example using a URI. This enables the documentation to have its own [[metadata]], like creator and creation date.

Specific guidance on SKOS documentation properties can be found in the SKOS Primer [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818/#secdocumentation Documentary Notes].

=== Semantic relations ===

SKOS semantic relations are intended to provide ways to declare relationships between concepts within a concept scheme. While there are no restrictions precluding their use with two concepts from separate schemes, this is discouraged because it is likely to overstate what can be known about the two schemes, and perhaps link them inappropriately.

The property <code>related</code> simply makes an association relationship between two concepts; no hierarchy or generality relation is implied. The properties <code>broader</code> and <code>narrower</code> are used to assert a direct hierarchical link between two concepts. The meaning may be unexpected; the relation <code>&lt;A&gt; broader &lt;B&gt;</code> means that A has a broader concept called B—hence that B is broader than A. Narrower follows in the same pattern.

While the casual reader might expect broader and narrower to be transitive properties, SKOS does not declare them as such. Rather, the properties <code>broaderTransitive</code> and <code>narrowerTransitive</code> are defined as transitive super-properties of broader and narrower. These super-properties are (by convention) not used in declarative SKOS statements. Instead, when a broader or narrower relation is used in a triple, the corresponding transitive super-property also holds; and transitive relations can be inferred (and queried) using these super-properties.

=== Mapping ===

SKOS mapping properties are intended to express matching (exact or fuzzy) of concepts from one concept scheme to another, and by convention are used only to connect concepts from different schemes. The concepts <code>relatedMatch</code>, <code>broadMatch</code>, and <code>narrowMatch</code> are a convenience, with the same meaning as the semantic properties <code>related</code>, <code>broader</code>, and <code>narrower</code>. (See previous section regarding the meanings of broader and narrower.)

The property relatedMatch makes a simple associative relationship between two concepts. When concepts are so closely related that they can generally be used interchangeably, <code>exactMatch</code> is the appropriate property (exactMatch relations are transitive, unlike any of the other Match relations). The <code>closeMatch</code> property that indicates concepts that only sometimes can be used interchangeably, and so it is not a transitive property.

=== Concept collections ===

The concept collections (<code>Collection</code>, <code>orderedCollection</code>) are labeled and/or ordered (<code>orderedCollection</code>) groups of SKOS concepts. Collections can be nested, and can have defined URIs or not (which is known as a blank node). Neither a SKOS <code>Concept</code> nor a <code>ConceptScheme</code> may be a Collection, nor vice versa; and SKOS semantic relations can only be used with a Concept (not a Collection). The items in a Collection can not be connected to other SKOS Concepts through the Collection node; individual relations must be defined to each Concept in the Collection.

== Community and participation ==

All development work is carried out via the mailing list which is a completely open and publicly archived<ref>[http://lists.w3.org/Archives/Public/public-esw-thes/ public-esw-thes@w3.org online archive]. Archives of mailing list used for SKOS development.</ref> mailing list devoted to discussion of issues relating to knowledge organisation systems, information retrieval and the Semantic Web. Anyone may participate informally in the development of SKOS by joining the discussions on public-esw-thes@w3.org - informal participation is warmly welcomed. Anyone who works for a [http://www.w3.org/Consortium/join W3C member] organisation may formally participate in the development process by joining the [http://www.w3.org/2006/07/SWD/ Semantic Web Deployment Working Group] - this entitles individuals to edit specifications and to vote on publication decisions.

== Applications ==

*Some important vocabularies have been migrated into SKOS format and are available in the public domain, including [[EuroVoc]], [[AGROVOC]] and [[GEMET]]. [[Library of Congress Subject Headings]] (LCSH) also support the SKOS format.<ref>[http://id.loc.gov/authorities/about.html About the Library of Congress Authorities]</ref>
*SKOS has been used as the language for the thesauri used in the [[SWED Environmental Directory]]<ref>[http://www.swed.org.uk/swed Semantic Web Environmental Directory]</ref> developed in the SWAD-Europe project framework.
*A way to convert thesauri to SKOS,<ref>[http://thesauri.cs.vu.nl/eswc06/ A Method to Convert Thesauri to SKOS]</ref> with examples including the [[Medical Subject Headings|MeSH]] thesaurus, has been outlined by the [[Vrije Universiteit Amsterdam]].
*Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS has been developed by [[IBM]].<ref>[http://www-128.ibm.com/developerworks/xml/library/x-dita10/ Subject classification using [[Darwin Information Typing Architecture|DITA]] and SKOS] by IBM developerWorks.</ref>
*SKOS is used to represent geographical feature types in the [[GeoNames]] ontology.

== Tools ==
* [https://github.com/eScienceCenter/ThesauRex ThesauRex] is an open-source, web-based SKOS editor. It is limited to broader/narrower relations among concepts and offers tree-based interaction and with thesauri and drag&drop creation of new thesauri based on a master thesaurus.
* Mondeca's [http://www.mondeca.com/Products/ITM Intelligent Topic Manager] (ITM) is a full-featured SKOS-compliant solution for managing taxonomies, thesauri, and other controlled vocabularies.
*[http://pactols.frantiq.fr/opentheso/ Opentheso] is an open source web-based thesaurus management system compliant with ISO 25964:2011 and ISO 25964-2:2012 standards (Information and Documentation. Thesauri and Interoperability with other vocabularies). It offers SKOS and csv exports and imports, REST and SOAP web services and manages persistent identifiers (ARK). It has been developed at the French National Center for Scientific Research since 2007. It is currently used by the French archaeological libraries network [http://www.frantiq.fr Frantiq] and by research teams  and by the Hospices Civils de Lyon as a collaborative thesaurus management tool. It can be dowloaded on [https://github.com/frantiq/opentheso github]. 
* [http://openskos.org OpenSKOS] is a web service-based approach to publication, management and use of vocabulary data that can be mapped to SKOS. Its source code is available on [https://github.com/CatchPlus/OpenSKOS GitHub]. It includes [[CRUD]] like [[RESTful]] operations on SKOS concepts and a web-based editor for searching and editing concepts. It was developed by [http://picturae.com Picturae] and funded by the Dutch heritage fond [http://www.catchplus.nl/ CATCHPlus].
* TemaTres Vocabulary Server<ref>[http://www.vocabularyserver.com/ TemaTres] is a Web tool to manage formal and linguistic representations of knowledge.</ref> is an open source web-based vocabulary server for managing controlled vocabularies, taxonomies and thesauruses. [http://sourceforge.net/projects/tematres Tematres] provides complete export of vocabularies into SKOS-core in addition to Zthes, TopicMaps, MADS, Dublin Core,VDEX, BS 8723, SiteMap, SQL and text.
* ThManager<ref>[http://thmanager.sourceforge.net/ ThManager] an Open Source Tool for creating and visualizing SKOS RDF vocabularies.</ref> is a [[Java (programming language)|Java]] [[Open-source software|open-source]] application for creating and visualizing SKOS vocabularies.
* The W3C provides an experimental on-line validation service.<ref>[http://www.w3.org/2004/02/skos/core/validation SKOS Core Validation Service]</ref>
* SKOS files can also be imported and edited in RDF-OWL editors such as [[Protege (software)|Protégé]] and [[SWOOP]] developed by Maryland Information and Network Dynamics Lab Semantic Web Agents Project [[Mindswap]].<ref>[http://www.mindswap.org/2004/SWOOP/ SWOOP] A Hypermedia-based Featherweight OWL Ontology Editor, developed by [[Mindswap]] - Maryland Information and Network Dynamics Lab Semantic Web Agents Project</ref>
* SKOS synonyms can be transformed from [[WordNet]] RDF format using an [[XSLT]] style sheet; see [http://www.w3.org/TR/wordnet-rdf W3C RDF]
* PoolParty<ref>[http://www.poolparty.biz/ PoolParty] is a thesaurus management system and a SKOS editor for the Semantic Web.</ref> is a commercial-quality thesaurus management system and a SKOS editor for the Semantic Web including text analysis functionalities and [[Linked Data]] capabilities.
* qSKOS<ref>[https://github.com/cmader/qSKOS/ qSKOS] is an open-source tool for SKOS vocabulary quality assessment.</ref> is an open-source tool for performing quality assessment of SKOS vocabularies by checking against a quality issue catalog.
* SKOSEd<ref>[http://code.google.com/p/skoseditor/ SKOSEd] SKOS plugin for Protege 4</ref> is an open source plug-in for the Protégé 4<ref>[http://www.co-ode.org/downloads/protege-x/ Protégé 4] Protégé 4 OWL editor</ref> [[Web Ontology Language|OWL]] ontology editor that supports authoring SKOS vocabularies. SKOSEd has an accompanying SKOS API<ref>[http://skosapi.sourceforge.net/ SKOS Java API] Java API for SKOS</ref> written in Java that can be used to build SKOS-based applications.
* Model Futures SKOS Exporter<ref>[http://www.modelfutures.com/software Model Futures Excel SKOS Exporter]</ref> for [[Microsoft Excel]] allows simple vocabularies to be developed as indented Excel spreadsheets and exported as SKOS RDF. BETA version.
* Lexaurus<ref>[http://www.vocman.com/ Lexaurus] is an enterprise thesaurus management system and multi-format editor.</ref> is an enterprise thesaurus management system and multi-format editor. Its extensive API includes full revision management. SKOS is one of its many supported formats.
* TopBraid Enterprise Vocabulary Net (EVN) <ref>[http://www.topquadrant.com/solutions/ent_vocab_net.html TopBraid EVN]</ref> is a web-based solution for simplified development and management of interconnected controlled vocabularies. It supports collaboration on defining and linking enterprise vocabularies, taxonomies, thesauri and ontologies used for information integration, customization and search.
* [http://www.dataharmony.com/products/thesaurus_master.html Thesaurus Master], for creating, developing, and maintaining taxonomies and thesauri, is part of Access Innovations' [http://www.dataharmony.com/ Data Harmony] knowledge management software line. It offers SKOS-compliant export.
* [http://www.cognitum.eu/semantics/FluentEditor/ Fluent Editor 2014] - an ontology editor which allows to work and edit directly OWL annotations and SKOS. Annotations will processed also for referenced ontologies as well as imported/exported to OWL/RDF and can be processed on the server.
* [https://trial.smartlogic.com/S4Trials/ Smartlogic Semaphore Ontology Editor] - a SKOS and SKOS-XL based ontology editor which allows creating models based strictly on the SKOS standards.

== Data ==
There are publicly available SKOS data sources.
* SKOS Datasets wiki<ref>[http://www.w3.org/2001/sw/wiki/SKOS/Datasets SKOS/Datasets]</ref> The W3C recommends using this list of publicly available SKOS data sources. Most data found in this wiki can be used for commercial and research applications.

== Relationships with other standards ==

=== Metamodel ===
The SKOS metamodel is broadly compatible with the data model of [[ISO 25964-1]] - Thesauri for Information Retrieval. This data model can be viewed and downloaded from the website for [[ISO 25964]].<ref name="niso.org">[http://www.niso.org/schemas/iso25964 ''ISO 25964 – the international standard for thesauri and interoperability with other vocabularies'']</ref>
[[File:Skos metamodel.png|thumb|alt=Alt text|Semantic model of the information elements of SKOS]]

=== SKOS and thesaurus standards ===
SKOS development has involved experts from both RDF and library community, and SKOS intends to allow easy migration of thesauri defined by standards such as [[NISO]] Z39.19 - 2005<ref>[http://www.niso.org/standards/ NISO Standards] Z39.19 - 2005 : Guidelines for the Construction, Format, and Management of Monolingual Controlled Vocabularies</ref> or [[ISO 25964]].<ref name="niso.org"/>

=== SKOS and other semantic web standards ===
SKOS is intended to provide a way to make a legacy of concept schemes available to Semantic Web applications, simpler than the more complex ontology language, [[Web Ontology Language|OWL]]. OWL is intended to express complex conceptual structures, which can be used to generate rich metadata and support inference tools. However, constructing useful web ontologies is demanding in terms of expertise, effort, and cost. In many cases, this type of effort might be superfluous or unsuited to requirements, and SKOS might be a better choice. The extensibility of RDF makes possible further incorporation or extension of SKOS vocabularies into more complex vocabularies, including OWL ontologies.

== See also ==
* [[Glossary]]
* [[Knowledge representation]]
* [[Metadata registry]]

== References ==
{{Reflist|2}}

==External links==
* [http://www.w3.org/TR/skos-reference/ SKOS Simple Knowledge Organization System Reference]
* [http://www.w3.org/2004/02/skos/ W3C SKOS Home Page]
* [http://www.w3.org/TR/2009/NOTE-skos-primer-20090818 W3C Simple Knowledge Organization System Primer]
* [http://www.idealliance.org/proceedings/xtech05/papers/03-04-01/ Presentation of SKOS at XTech 2005 Conference]
* [http://www.w3.org/News/2009#item35 W3C Invites Implementations of SKOS (Simple Knowledge Organization System) Reference; Primer Also Published]
* [http://demo.semantic-web.at:8080/SkosServices/index SKOS Validator and Zthes Converter]

{{Semantic Web}}
{{W3C standards}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:School of Computer Science, University of Manchester]]
<=====doc_Id=====>:734
<=====title=====>:
Korean decimal classification
<=====text=====>:
{{Unreferenced stub|date=December 2009}}
The '''Korean decimal classification''' ('''KDC''') is a system of [[library classification]] used in [[South Korea]]. The main classes are the same as in the Dewey Decimal Classification but these are in a different order: Natural sciences 400; Technology and engineering 500; Arts 600; Language 700.

==Main classes==
* 000 General
* 100 Philosophy
* 200 Religion
* 300 Social sciences
* 400 Natural sciences
* 500 Technology and engineering
* 600 Arts
* 700 Language
* 800 Literature
* 900 History

[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Classification systems]]
[[Category:Libraries in North Korea]]
[[Category:Libraries in South Korea]]

{{Library classification systems}}
{{Library-stub}}
<=====doc_Id=====>:737
<=====title=====>:
Agent Communications Language
<=====text=====>:
{{Refimprove|date=September 2014}}
'''Agent Communication Language''' ('''ACL'''), proposed by the [[Foundation for Intelligent Physical Agents]] (FIPA), is a proposed standard language for [[Software agent|agent]] communications.  [[Knowledge Query and Manipulation Language]] (KQML) is another proposed standard.

The most popular ACLs are:
* FIPA-ACL <ref>{{cite journal|last=Poslad|first=Stefan|title=Specifying Protocols for Multi-agent System Interaction|year=2007|journal=ACM Transactions on Autonomous and Adaptive Systems|volume=4|issue=4|doi=10.1145/1293731.1293735}}</ref> (by the [[Foundation for Intelligent Physical Agents]], a standardization consortium)
* [[KQML]] <ref>{{cite journal|last=Finin|first=Tim|author2= Richard Fritzson, Don McKay and Robin McEntire |title=KQML as an agent communication language|year=1994|conference=Proceedings of the third international conference on Information and knowledge management, CIKM '94 |pages= 456–463}}</ref> (Knowledge Query and Manipulation Language)

Both rely on [[speech act]] theory developed by [[John Searle|Searle]] in the 1960s <ref>{{cite book|last= Searle|first=J.R.|year=1969|title=Speech Acts|publisher=Cambridge University Press, Cambridge, UK}}</ref> and enhanced by [[Terry Winograd|Winograd]] and [[Fernando Flores|Flores]] in the 1970s. They define a set of [[Performative utterance|performatives]], also called Communicative Acts, and their meaning (e.g. ask-one). The content of the performative is not standardized, but varies from system to system.

To make agents understand each other they have to not only speak the same language, but also have a common [[Ontology (computer science)|ontology]]. An ontology is a part of the agent's knowledge base that describes what kind of things an agent can deal with and how they are related to each other.

Examples of frameworks that implement a standard agent communication language (FIPA-ACL) include FIPA-OS <ref>{{cite conference|last=Poslad|first=Stefan|author2=Philip Buckle and Robert Hadingham|title=The FIPA-OS agent platform: Open Source for Open Standards|year=2000|conference=Proceedings of 5th International Conference on the Practical Application Of Intelligent Agents And Multi-Agent Technology (PAAM)|pages=355–368}}</ref><ref>{{cite journal|last=Poslad|first= S|author2=Buckle P, Hadingham R.G|title=Open Source, Standards and Scaleable Agencies|journal=Lecture Notes in Computer Science|year= 2001|volume=1887|pages=296–303|DOI=10.1007/3-540-47772-1_30}}</ref>
and [[Java Agent Development Framework|Jade]]. <ref>{{cite conference|last=Bellifeminee|first=Fabio|author2=Agostino Poggi and Giovanni Rimassa|title=JADE: a FIPA2000 compliant agent development environment|year=2001|conference=Proceedings of the fifth international conference on Autonomous agents|pages=216–217}}</ref>

==References==
{{reflist}}

[[Category:Formal languages]]
[[Category:Knowledge representation]]
[[Category:Multi-agent systems]]


{{Measurement-stub}}
{{computing-stub}}
<=====doc_Id=====>:740
<=====title=====>:
F-logic
<=====text=====>:
'''F-logic''' ([[frame (data structure)|frame]] [[Logic programming|logic]]) is a [[knowledge representation]] and [[ontology language]]. F-logic combines the advantages of conceptual modeling with object-oriented, frame-based languages and offers a declarative, compact and simple syntax, as well as the well-defined semantics of a logic-based language. 
Features include, among others, object identity, complex objects, [[inheritance (computer science)|inheritance]], [[polymorphism (computer science)|polymorphism]], query methods, [[encapsulation (computer science)|encapsulation]]. F-logic stands in the same relationship to [[object-oriented programming]] as classical [[predicate calculus]] stands to [[relational database]] programming.

F-logic was developed by [[Michael Kifer]] at [[Stony Brook University]] and [[Georg Lausen]] at the [[University of Mannheim]]. F-logic was originally developed for deductive databases, but is now most frequently used for semantic technologies, especially the [[Semantic Web]]. F-logic is considered as one of the formalisms for [[Ontology (information science)|ontologies]], but [[description logic]] (DL) is more popular and accepted, as is the DL-based [[Web Ontology Language|OWL]].

A development environment for F-logic was developed in the NeOn project and is also used in a range of applications for information integration, [[question answering]] and [[semantic search]]. Prior to the version 4 of Protégé ontology editor, F-Logic is supported as one of the two kinds of ontology.

The frame syntax of the [[Rule Interchange Format]] Basic Logic Dialect (RIF BLD) standardized by the [[World Wide Web Consortium]] is based on F-logic; RIF BLD however does not include [[non-monotonic reasoning]] features of F-logic.<ref name="Krötzsch2010">{{cite book|author=M. Krötzsch|title=Description Logic Rules|url=https://books.google.com/books?id=Z8h7AgAAQBAJ&pg=PA10|year=October 2010|publisher=IOS Press|isbn=978-1-61499-342-1|page=10}}</ref>

In contrast to [[description logic]] based ontology formalism the semantics of F-logic are normally that of a [[closed world assumption]] as opposed to DL's [[open world assumption]]. Also, F-logic is generally [[Undecidable problem|undecidable]]{{Citation needed|date=May 2014}}, whereas 
the [[SHOIN|SHOIN description logic]] that [[Web Ontology Language|OWL DL]] is based on is decidable. However it is possible to represent more expressive statements in F-logic than are possible with description logics.

The most comprehensive description of F-logic appears in.<ref>M. Kifer, G. Lausen, J. Wu (1995). ''Foundations of Object-Oriented and Frame-Based Languages]'', Journal of ACM, May 1995. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3586 PDF]</ref> The preliminary paper <ref>M. Kifer and G. Lausen (1989). ''F-logic: a higher-order language for reasoning about objects, inheritance, and scheme'', ACM SIGMOD Conference, 1989. [http://dl.acm.org/citation.cfm?id=66939 PDF]</ref> <ref>M. Kifer and G. Lausen (1997). ''F-logic: a higher-order language for reasoning about objects, inheritance, and scheme'', re-issued 1997. [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.7149 PDF]</ref> has won the 1999 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]]. A follow-up paper <ref>M. Kifer, W. Kim, Y. Sagiv (1992). ''Querying object-oriented databases'', ACM SIGMOD Conference, 1992. [http://dl.acm.org/citation.cfm?doid=130283.130342 PDF]</ref> has won the 2002 [http://www.sigmod.org/sigmod-awards/sigmod-awards#time Test of Time Award] from [[SIGMOD|ACM SIGMOD]].

== F-logic syntax ==

Classes and individuals may be defined in F-logic as follows
 man::person.
 woman::person.
 brad:man.
 angelina:woman.
This states, that "men and women are persons" and that "Brad is a man", and "Angelina is a woman".

Statements about classes and individuals may be made as follows
 person[hasSon=&gt;man].
 brad[hasSon-&gt;&gt;{maddox,pax}].
 married(brad,angelina).
This defines that "the son of a person is a man", "Maddox and Pax are the sons of Brad" and "Brad and Angelina are married". Note that <code>-&gt;&gt;</code> is used for sets of values.

In addition it is possible to represent axioms in F-logic in the following manner
 man(X) &lt;- person(X) AND NOT woman(X).
 FORALL X, Y &lt;- X:person[hasFather->Y] &lt;- Y:man[hasSon -> X].
These mean "X is a man if X is a person but not a woman" and "if X is the son of Y then Y is the father of X".

The [[Flora-2]] system introduced a number of changes to the syntax of F-logic, making it more suitable for a knowledge representation and reasoning system as opposed to just a theoretical logic. In particular, variables became prefixed with a ?-mark, the distinction between functional and multi-valued properties was dropped and replaced by cardinality constraints, plus other important changes.

==F-logic based Languages==
* [[Flora-2]] is an extension of F-logic with [[HiLog]], [[Transaction logic]], and [[defeasible reasoning]].
* [http://pathlp.sourceforge.net/ PathLP] is a full logic programming language based on F-logic.
* [http://dbis.informatik.uni-freiburg.de/index.php?project=Florid FLORID] is a C++ — based implementation
* [http://www.wsmo.org/wsml/ Web Services Modeling Language (WSML)]
* [http://www.daml.org/services/swsl/ Semantic Web Services Language (SWSL)]
* [[ObjectLogic]] language is based on F-logic; [[OntoStudio]] is an ObjectLogic implementation by [[semafora systems GmbH]] (former [[Ontoprise GmbH]]).

== References ==
{{reflist}}

[[Category:Knowledge representation]]
[[Category:Semantic Web]]
[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]
<=====doc_Id=====>:743
<=====title=====>:
Is-a
<=====text=====>:
In [[knowledge representation]], [[object-oriented programming]] and [[Object-oriented design|design]] (see [[object oriented]] [[program architecture]]), '''is-a''' ('''is_a''' or '''is a''') is a [[wikt:subsume|subsumption]]<ref>See [[Liskov substitution principle]].</ref> relationship between [[abstractions]] (e.g. [[type (disambiguation)|types]], [[class (disambiguation)|classes]]), where one [[Class (computer programming)|class]] ''A'' is a [[subclass (disambiguation)|subclass]] <!-- This deliberately links to the disambiguation page --> of another class ''B'' (and so ''B'' is a [[superclass (disambiguation)|superclass]] <!--This deliberately links to the disambiguation page--> of ''A'').
In other words, type A is a subtype of type B when A’s [[Formal specification|speciﬁcation]] implies B’s speciﬁcation. That is, any object (or class) that satisﬁes A’s speciﬁcation also satisﬁes B’s speciﬁcation, because B’s speciﬁcation is weaker.<ref>{{cite web|title=Subtypes and Subclasses|url=http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-170-laboratory-in-software-engineering-fall-2005/lecture-notes/lec14.pdf|publisher=MIT OCW|accessdate=2 October 2012}}</ref>

The ''is-a'' relationship is to be contrasted with the ''[[has-a]]'' (''has_a'' or ''has a'') relationship between types (classes); confusing the relations ''has-a'' and ''is-a'' is a common error when designing a model (e.g., a [[computer program]]) of the real-world relationship between an object and its subordinate. The ''is-a'' relationship may also be contrasted with the ''[[Typeof|instance-of]]'' relationship between objects (instances) and types (classes): see "[[type-token distinction]]" and "[[type-token relations]]."<ref>[[Type–token relations]]</ref> 

To summarize the relations, we have

* [[hyperonym]]-[[hyponym]] (supertype-subtype) relations between types (classes) defining a taxonomic hierarchy, where
** for a [[Inheritance (object-oriented programming)|subsumption]] relation: a hyponym (subtype, subclass) has a ''type-of'' (''is-a'') relationship with its hypernym (supertype, superclass);
* [[holonym]]-[[meronym]] (whole/entity/container-part/constituent/member) relations between types (classes) defining a possessive hierarchy, where 
** for an [[Aggregation (object-oriented programming)|aggregation]] (i.e. without ownership) relation: 
*** a holonym (whole) has a ''has-a'' relationship with its meronym (part),
** for a [[Composition (object-oriented programming)|composition]] (i.e. with ownership) relation: 
*** a meronym (constituent) has a ''[[part-of]]'' relationship with its holonym (entity),
** for a [[Object composition#Containment|containment]]<ref>See also [[Containment (computer programming)]].</ref> relation:
*** a meronym (member) has a ''[[member-of]]'' relationship with its holonym ([[Container (abstract data type)|container]]);
* concept-object (type-token) relations between types (classes) and objects (instances), where
** a token (object) has an ''[[Instance (computer science)|instance-of]]'' relationship with its type (class).

==Examples of subtyping==

[[Subtype polymorphism|Subtyping]] enables a given type to be substituted for another type or abstraction. Subtyping is said to establish an '''is-a''' relationship between the subtype and some existing abstraction, either implicitly or explicitly, depending on language support. The relationship can be expressed explicitly via inheritance in languages that support inheritance as a subtyping mechanism.

===C++===
The following C++ code establishes an explicit inheritance relationship between classes '''B''' and '''A''', where '''B''' is both a subclass and a subtype of '''A''', and can be used as an '''A''' wherever a '''B''' is specified (via a reference, a pointer or the object itself).

<source lang=cpp>class A 
{ public:
   void DoSomethingALike() const {}
};

class B : public A 
{ public:
   void DoSomethingBLike() const {}
};

void UseAnA(A const& some_A)
{
   some_A.DoSomethingALike();
}

void SomeFunc()
{
   B b;
   UseAnA(b); // b can be substituted for an A.
}
</source><ref name="Mitchell2002">
{{cite book
 | last=Mitchell
 | first=John
 | authorlink=John C. Mitchell
 | title=Concepts in programming language
 | year=2002
 | publisher=Cambridge University Press
 | location=Cambridge, UK
 | isbn=0-521-78098-5
 | page=287
 | chapter=10 "Concepts in object-oriented languages"}}
</ref>

===Python===
The following python code establishes an explicit inheritance relationship between classes '''B''' and '''A''', where '''B''' is both a subclass and a subtype of '''A''', and can be used as an '''A''' wherever a '''B''' is required.

<source lang=python>class A:
    def doSomethingALike(self):
        pass

class B(A):
    def doSomethingBLike(self):
        pass

def useAnA(some_A):
    some_A.doSomethingALike()

def someFunc():
    b = B();
    useAnA(b)  # b can be substituted for an A.
</source>

The following example, type(a) is a "regular" type, and type(type(a)) is a metatype. While as distributed all types have the same metatype (PyType_Type, which is also its own metatype), this is not a requirement. The type of classic classes, known as types.ClassType, can also be considered a distinct metatype.<ref>{{cite web|last=Guido van Rossum|title=Subtyping Built-in Types|url=https://www.python.org/dev/peps/pep-0253/|accessdate=2 October 2012}}</ref>

<source lang=python>
>>> a = 0
>>> type(a)
<type 'int'>
>>> type(type(a))
<type 'type'>
>>> type(type(type(a)))
<type 'type'>
>>> type(type(type(type(a))))
<type 'type'>
</source>

===Java===

In Java, '''is-a''' relation between the type parameters of one class or interface and the type parameters of another are determined by the extends and [[Interface (Java)|implements]] clauses.

Using the Collections classes, ArrayList<E> implements List<E>, and List<E> extends Collection<E>. So ArrayList<String> is a subtype of List<String>, which is a subtype of Collection<String>. The subtyping relationship is preserved between the types automatically. When we define an interface, PayloadList, that associates an optional value of generic type P with each element. Its declaration might look like:

<source lang=java>
interface PayloadList<E, P> extends List<E> {
    void setPayload(int index, P val);
    ...
}
</source>

The following parameterizations of PayloadList are subtypes of List<String>:

<source lang=java>
PayloadList<String, String>
PayloadList<String, Integer>
PayloadList<String, Exception>
</source>

==Liskov substitution principle==
{{main|Liskov substitution principle}}
Liskov substitution principle explains a property, ''"If for each object o1 of type S there is an object o2 of type T such that for all programs P deﬁned in terms of T, the behavior of P is unchanged when o1 is substituted for o2 then S is a subtype of T,"''.<ref>{{cite book|last=Liskov|first=Barbara|title=Data Abstraction and Hierarchy|date=May 1988|publisher=SIGPLAN Notices}}</ref> Following example shows a violation of LSP.

<source lang=cpp>void DrawShape(const Shape& s)
{
  if (typeid(s) == typeid(Square))
    DrawSquare(static_cast<Square&>(s));
  else if (typeid(s) == typeid(Circle))
    DrawCircle(static_cast<Circle&>(s));
}</source>
Obviously, the DrawShape function is badly formatted. It has to know about every derivative classes of Shape class. Also, it should be changed whenever new subclass of Shape are created. In [[Object-oriented design|Object Oriented Design]], many view the structure of this as anathema.

Here is a more subtle example of violation of LSP

<source lang=cpp>
class Rectangle
{
  public:
    void   SetWidth(double w)  { itsWidth = w; }
    void   SetHeight(double h) { itsHeight = h; }
    double GetHeight() const   { return itsHeight; }
    double GetWidth() const    { return itsWidth; }
  private:
    double itsWidth;
    double itsHeight;
};
</source>
This works well but when it comes to Square class, which inherits Rectangle class, it violates LSP even though the '''is-a''' relationship holds between Rectangle and Square. Because square is rectangular. The following example overrides two functions, Setwidth and SetHeight, to fix the problem. But fixing the code implies that the design is faulty.

<source lang=cpp>
public class Square : Rectangle
{
  public:
    virtual void SetWidth(double w);
    virtual void SetHeight(double h);
};
void Square::SetWidth(double w)
{
    Rectangle::SetWidth(w);
    Rectangle::SetHeight(w);
}
void Square::SetHeight(double h)
{
    Rectangle::SetHeight(h);
    Rectangle::SetWidth(h);
}
</source>

The following example, function g just works for Rectangle class but not for Square, and so the open-closed principle has been violated.

<source lang=cpp>
void g(Rectangle& r)
{
  r.SetWidth(5);
  r.SetHeight(4);
  assert(r.GetWidth() * r.GetHeight()) == 20);
}
</source>
<ref>{{cite web|title=The Liskov Substitution Principle|url=http://www.objectmentor.com/resources/articles/lsp.pdf|publisher=Robert C. Martin, 1996|accessdate=2 October 2012}}</ref>

== See also ==

* [[Inheritance (object-oriented programming)]]
* [[Liskov substitution principle]] (in [[object-oriented programming]])
* [[Subsumption (disambiguation)|Subsumption]]<!--This deliberately links to the disambiguation page-->
* Is-a
** [[Hypernymy]] (and [[supertype]])
** [[Hyponymy]] (and [[subtype]])
* [[Has-a]]
** [[Holonymy]]
** [[Meronymy]]

==Notes==
{{reflist|30em}}

==References==
* [[Ronald J. Brachman]]; [http://dblp.uni-trier.de/rec/bibtex/journals/computer/Brachman83 What IS-A is and isn't. An Analysis of Taxonomic Links in Semantic Networks]. IEEE Computer, 16 (10); October 1983
* Jean-Luc Hainaut, Jean-Marc Hick, Vincent Englebert, Jean Henrard, Didier Roland: [http://www.informatik.uni-trier.de/~ley/db/conf/er/HainautHEHR96.html Understanding Implementations of IS-A Relations]. ER 1996: 42-57

[[Category:Object-oriented programming]]
[[Category:Knowledge representation]]
[[Category:Abstraction]]
[[Category:Articles with example Java code]]
<=====doc_Id=====>:746
<=====title=====>:
Microformat
<=====text=====>:
{{About||the photographic miniaturization of documents|Microform|details of microformats used on Wikipedia|:Wikipedia:Microformats}}
A '''microformat''' (sometimes abbreviated '''μF''') is a [[World Wide Web]]-based approach to semantic markup which uses [[HTML]]/[[XHTML]] tags supported for other purposes to convey additional [[metadata]]<ref>{{cite web |url=http://microformats.org/wiki/existing-classes |work=Microformats.org |title=Class Names Across All Microformats |date=2007-09-23 |accessdate=2008-09-06}}</ref> and other attributes in web pages and other contexts that support (X)HTML, such as [[RSS]]. This approach allows [[software agent|software]] to process information intended for end-users (such as [[Address book|contact information]], [[Geographic coordinate system|geographic coordinates]], calendar events, and similar information) automatically.

Although the content of web pages has been capable of some "automated processing" since the inception of the web, such processing is difficult because the [[markup language|markup tags]] used to display information on the web do not describe what the information means.<ref name="Wharton000">{{cite web |title=What’s the Next Big Thing on the Web? It May Be a Small, Simple Thing -- Microformats|work=Knowledge@Wharton |publisher=[[Wharton School of the University of Pennsylvania]] |date=2005-07-27 |url=http://knowledge.wharton.upenn.edu/index.cfm?fa=printArticle&ID=1247}}</ref> Microformats can bridge this gap by attaching [[semantics]], and thereby obviate other, more complicated, methods of automated processing, such as [[natural language processing]] or [[screen scraping]]. The use, adoption and processing of microformats enables data items to be indexed, searched for, saved or cross-referenced, so that information can be reused or combined.<ref name="Wharton000"/>

{{As of | 2013}} microformats allow the encoding and extraction of event details, contact information, social relationships and similar information. Established microformats such as [[hCard]] are published on the web more than alternatives like schema ([[Microdata (HTML)|microdata]]) and [[RDFa]].<ref>{{cite web|url=http://webdatacommons.org/structureddata/index.html#toc2
 |date=2013 |work= section 3.1, "Extraction Results from the November 2013 Common Crawl Corpus" |accessdate=2015-02-21 |title=Web Data Commons – RDFa, Microdata, and Microformat Data Sets}}</ref>{{failed verification|date=November 2016}}

== Background ==
Microformats emerged around 2005<ref>The ''microformats'' is a community-standard maintained by its Wiki, and [http://microformats.org/wiki/index.php?title=Main_Page&dir=prev&action=history the Wiki arrived ~2005].</ref> as part of a grassroots movement to make recognizable data items (such as events, contact details or geographical locations) capable of automated processing by software, as well as directly readable by end-users.<ref name="Wharton000"/><ref>In this context, the definition of "end-user" includes a person reading a web page on a computer screen or mobile device, or an [[assistive technology]] such as a [[screen reader]].</ref> Link-based microformats emerged first. These include vote links that express opinions of the linked page, which search engines can tally into instant polls.<ref name="Khare000">{{cite journal |title=Microformats: The Next (Small) Thing on the Semantic Web? |first=Rohit |last=Khare |journal=[[IEEE Internet Computing]] |volume=10 |issue=1 |pages=68–75 |date=January–February 2006 |publisher=[[IEEE Computer Society]] |url=http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/ic/&toc=comp/mags/ic/2006/01/w1toc.xml&DOI=10.1109/MIC.2006.13 |doi=10.1109/MIC.2006.13 |accessdate=2008-09-06}}
</ref>

[[CommerceNet]], a nonprofit organization that promotes [[e-commerce]] on the Internet, has helped sponsor and promote the technology and support the microformats community in various ways.<ref name="Khare000"/> CommerceNet also helped co-found the Microformats.org community site.<ref name="Khare000"/>

Neither CommerceNet nor Microformats.org operates as a [[standards body]]. The microformats community functions through an open [[wiki]], a mailing list, and an Internet relay chat ([[Internet Relay Chat|IRC]]) channel.<ref name="Khare000"/> Most of the existing microformats originated at the Microformats.org wiki and the associated mailing list{{citation needed|date=October 2012}} by a process of gathering examples of web-publishing behaviour, then codifying it. Some other microformats (such as [[nofollow|rel=nofollow]] and [[unAPI]]) have been proposed, or developed, elsewhere.

== Technical overview ==

XHTML and HTML standards allow for the embedding and encoding of semantics within the [[HTML element|attributes of markup tags]]. Microformats take advantage of these standards by indicating the presence of metadata using the following attributes:

; <code>class</code>
: [[Class (computer programming)|Classname]]

; <code>rel</code>
: relationship, description of the target address in an anchor-element (<code><a href=... rel=...>...</a></code>)

; <code>rev</code>
: reverse relationship, description of the referenced document (in one case, otherwise deprecated in microformats<ref name="uF-rel-faq">{{cite web |url=http://microformats.org/wiki/rel-faq |title="rel" attribute frequently asked questions |work=Microformats.org |date=2008-08-06 |accessdate=2008-09-06}}</ref>)

For example, in the text "The birds roosted at <span class="geo"><span class="latitude">52.48</span>, <span class="longitude">-1.89</span></span>" is a pair of numbers which may be understood, from their context, to be a set of [[geographic coordinate system|geographic coordinates]]. With wrapping in [[Span and div|spans]] (or other HTML elements) with specific class names (in this case <code>geo</code>, <code>latitude</code> and <code>longitude</code>, all part of the [[Geo (microformat)|geo microformat]] specification):

 <syntaxhighlight lang="xml">The birds roosted at
   <span class="geo">
     <span class="latitude">52.48</span>,
     <span class="longitude">-1.89</span>
   </span>
 </syntaxhighlight>

software agents can recognize exactly what each value represents and can then perform a variety of tasks such as indexing, locating it on a map and exporting it to a [[GPS]] device.

=== Examples ===
In this example, the contact information is presented as follows:

 <syntaxhighlight lang="xml">
 <ul>
   <li>Joe Doe</li>
   <li>The Example Company</li>
   <li>604-555-1234</li>
   <li><a href="http://example.com/">http://example.com/</a></li>
 </ul>
 </syntaxhighlight>

With hCard microformat markup, that becomes:

 <syntaxhighlight lang="xml">
 <ul class="vcard">
   <li class="fn">Joe Doe</li>
   <li class="org">The Example Company</li>
   <li class="tel">604-555-1234</li>
   <li><a class="url" href="http://example.com/">http://example.com/</a></li>
 </ul>
 </syntaxhighlight>

Here, the formatted name (<code>fn</code>), organisation (<code>org</code>), telephone number (<code>tel</code>) and [[Uniform Resource Locator|web address]] (<code>url</code>) have been identified using specific class names and the whole thing is wrapped in <code>class="vcard"</code>, which indicates that the other classes form an hCard (short for "HTML [[vCard]]") and are not merely coincidentally named. Other, optional, hCard classes also exist. Software, such as browser plug-ins, can now extract the information, and transfer it to other applications, such as an address book.

<div class="noprint"> <!-- ensures that the following "Live" example dies not carry over to printed mirrors -->

<!-- Note "noprint" div started in previous section
-->=== In-context examples ===
For annotated examples of microformats on live pages, see [[HCard#Live example]] and [[Geo (microformat)#Usage]].
</div>

== Specific microformats ==
Several microformats have been developed to enable semantic markup of particular types of information. However, only hCard and hCalendar have been ratified, the others remaining as drafts:

* [[hAtom]] (superseded by [[h-entry]] and [[h-feed]]) – for marking up [[Atom (standard)|Atom]] feeds from within standard HTML
* [[hCalendar]] – for events
* [[hCard]] – for contact information; includes:
** adr – for postal addresses
** [[geo (microformat)|geo]] – for geographical coordinates ([[latitude]], [[longitude]])
* hMedia - for audio/video content<ref>[http://microformats.org/wiki/hmedia hMedia · Microformats Wiki<!-- Bot generated title -->]</ref><ref>[http://sixrevisions.com/web-development/ultimate-guide-to-microformats-reference-and-examples/ Ultimate Guide to Microformats: Reference and Examples<!-- Bot generated title -->]</ref>
* hAudio – for audio content
* [[hNews]] -  for news content
* [[hProduct]] – for products
* [[hRecipe]] - for recipes and foodstuffs.
* [[hResume]] – for resumes or [[curriculum vitae|CVs]]
* [[hReview]] – for reviews
* rel-[[directory (file systems)|directory]] – for distributed directory creation and inclusion<ref>[http://microformats.org/wiki/rel-directory rel-directory · Microformats Wiki<!-- Bot generated title -->]</ref>
* rel-enclosure – for multimedia attachments to web pages<ref>[http://microformats.org/wiki/rel-enclosure rel="enclosure" · Microformats Wiki<!-- Bot generated title -->]</ref>
* rel-license – specification of copyright license<ref>[http://microformats.org/wiki/rel-license rel="license" · Microformats Wiki<!-- Bot generated title -->]</ref>
* rel-[[nofollow]], an attempt to discourage third-party content spam (e.g. [[spam in blogs]])
* rel-[[tag (metadata)|tag]] – for decentralized tagging ([[Folksonomy]])<ref>[http://microformats.org/wiki/rel-tag rel="tag" · Microformats Wiki<!-- Bot generated title -->]</ref>
* [[xFolk]] – for tagged links
* [[XHTML Friends Network]] (XFN) – for social relationships
* [[XOXO (microformat)|XOXO]] – for lists and outlines

== Uses ==
Using microformats within HTML code provides additional formatting and semantic data that applications can use. For example, applications such as [[web crawler]]s can collect data about on-line resources, or desktop applications such as e-mail clients or scheduling software can compile details. The use of microformats can also facilitate "mash ups" such as exporting all of the geographical locations on a web page into (for example) [[Google Maps]] to visualize them spatially.

Several browser extensions, such as [[Operator (extension)|Operator]] for [[Firefox]] and Oomph for [[Internet Explorer]], provide the ability to detect microformats within an HTML document. When hCard or hCalendar are involved, such browser extensions allow microformats to be exported into formats compatible with contact management and calendar utilities, such as [[Microsoft Outlook]]. When dealing with geographical coordinates, they allow the location to be sent to applications such as [[Google Maps]]. [[Yahoo! query language|Yahoo! Query Language]] can be used to extract microformats from web pages.<ref>{{cite web|url=http://developer.yahoo.net/blog/archives/2009/01/wikipedia_w_yql.html|title=Retrieving and displaying data from Wikipedia with YQL|last=Heilman|first=Chris|date=2009-01-19|work=Yahoo Developer Network|publisher=Yahoo|accessdate=2009-01-19}}</ref> On 12 May 2009 [[Google search|Google]] announced that they would be parsing the hCard, hReview and hProduct microformats, and using them to populate search result pages.<ref name="Rich-Snippets">{{cite web|url=http://googlewebmastercentral.blogspot.com/2009/05/introducing-rich-snippets.html|title=Introducing Rich Snippets|last=Goel|first=Kavi|author2=Ramanathan V. Guha |author3=Othar Hansson |date=2009-05-12|work=Google Webmaster Central Blog|publisher=Google|accessdate=2009-05-25}}</ref> They have since extended this to use hCalendar for events<ref name="Google-recipes">{{cite web|url=http://googlewebmastercentral.blogspot.com/2010/04/better-recipes-on-web-introducing.html|title=Better recipes on the web: Introducing recipe rich snippets|last=Gong|first=Jun|author2=Kosuke Suzuki |author3=Yu Watanabe |date=2010-04-13|publisher=Google|accessdate=17 March 2011}}</ref> and hRecipe for cookery recipes.<ref name="Google-recipes" /> Similarly, microformats are also processed by [[Bing (search engine)|Bing]]<ref name="Bing">{{cite web|url=http://www.bing.com/community/site_blogs/b/search/archive/2011/06/02/bing-google-and-yahoo-unite-to-build-the-web-of-objects.aspx|title=Bing Introducing Schema.org: Bing, Google and Yahoo Unite to Build the Web of Objects - Search Blog - Site Blogs - Bing Community|date=2011-06-02|work=[[Bing (search engine)|Bing]]|accessdate=2 June 2011}}</ref> and [[Yahoo!]].<ref name="YSearch">{{cite web|url=http://www.ysearchblog.com/2011/06/02/introducing-schema-org-a-collaboration-on-structured-data|title=Introducing schema.org: A Collaboration on Structured Data|date=2011-06-02|accessdate=2 June 2011}}</ref> Together, these are the world's top three search engines.<ref>{{cite web |url=http://gs.statcounter.com/#search_engine-ww-monthly-201010-201012 |title=Top 5 Search Engines from Oct to Dec 10 &#124; StatCounter Global Stats |author= |work= |publisher=StatCounter |accessdate=17 January 2011}}</ref>

[[Microsoft]] said they needed to incorporate Microformats into upcoming projects,<ref>{{cite web|url=http://microformats.org/blog/2006/03/20/bill-gates-at-mix06-we-need-microformats |title=Bill Gates at Mix06 – "We need microformats" |date=2006-03-20 |quote=We need microformats and to get people to agree on them. It is going to bootstrap exchanging data on the Web… …we need them for things like contact cards, events, directions… |accessdate=2008-09-06}}</ref> as did other software companies.

Alex Faaborg summarizes the arguments for putting the responsibility for microformat user interfaces in the web browser rather than making more complicated HTML:<ref>[http://blog.mozilla.com/faaborg/2007/02/04/microformats-part-4-the-user-interface-of-microformat-detection/ Microformats – Part 4: The User Interface of Microformat Detection « Alex Faaborg<!-- Bot generated title -->]</ref>
* Only the web browser knows what applications are accessible to the user and what the user's preferences are
* It lowers the barrier to entry for web site developers if they only need to do the markup and not handle "appearance" or "action" issues
* Retains backwards compatibility with web browsers that don't support microformats
* The web browser presents a single point of entry from the web to the user's computer, which simplifies security issues

== Evaluation ==
Various commentators have offered review and discussion on the design principles and practical aspects of microformats. Microformats have been compared to other approaches that seek to serve the same or similar purpose.<ref name="criticism000">{{cite web |title=Criticism |work=Microformats.org |url=http://microformats.org/wiki?title=criticism&oldid=18478 |date=2007-03-24 |accessdate=2007-08-15}}</ref> From time to time, there is criticism of one, or all, microformats.<ref name="criticism000"/> The spread and use of microformats has been advocated.<ref name="advocacy000">{{cite web |title=Advocacy |work=Microformats.org |url=http://microformats.org/wiki/advocacy |date=2008-08-27 |accessdate=2007-08-15}}</ref><ref name="spread000">{{cite web |title=Spread Microformats |work=Microformats.org |url=http://microformats.org/wiki/spread-microformats |date=2008-08-29 |accessdate= 2007-08-15}} This includes community resources for marketing microformats such as buttons, banners, wallpaper / desktop screens, logo graphics, etc.</ref> [[Opera Software]] CTO and [[Cascading Style Sheets|CSS]] creator [[Håkon Wium Lie]] said in 2005 "We will also see a bunch of microformats being developed, and that’s how the [[Semantic Web|semantic web]] will be built, I believe."<ref name="advocacy001">{{cite web |title=Interview with Håkon Wium Lie |url=http://www.molly.com/2005/03/31/interview-with-hkon-wium-lie/ |first=Molly E. |last=Holzschlag |authorlink=Molly Holzschlag |date=2005-03-31 |work=Molly.com |accessdate=2007-11-18}}</ref> However, in August 2008 Toby Inkster, author of the "Swignition" (formerly "Cognition") microformat parsing service, pointed out that no new microformat specifications had been published since 2005.<ref name="threeyears">{{cite web |title=More than three years |url=http://microformats.org/discuss/mail/microformats-discuss/2008-August/012402.html |work=Microformats.org |first=Toby A. |last=Inkster |date=2008-04-22 |accessdate=2008-08-24}}</ref>

=== Design principles ===
Computer scientist and entrepreneur, [[Rohit Khare]] stated that ''reduce, reuse, and recycle'' is "shorthand for several design principles" that motivated the development and practices behind microformats.<ref name="Khare000"/>{{rp|71–72}} These aspects can be summarized as follows:

*Reduce: favor the simplest solutions and focus attention on specific problems;
*Reuse: work from experience and favor examples of current practice;
*Recycle: encourage modularity and the ability to embed, valid XHTML can be reused in blog posts, RSS feeds, and anywhere else you can access the web.<ref name="Khare000"/>

=== Accessibility ===
Because some microformats make use of title attribute of HTML's {{tag|abbr|open}} element to conceal [[machine-readable data]] (particularly date-times and geographical coordinates) in the "[http://microformats.org/wiki/abbr-design-pattern abbr design pattern]", the plain text content of the element is inaccessible to [[screen reader]]s that expand abbreviations.<ref name="ATF">{{cite web |url=http://www.webstandards.org/2007/04/27/haccessibility/ | title=hAccessibility | first=James |last=Craig |publisher= [[Web Standards Project]] |date=2007-04-27 |accessdate=2007-08-16}}</ref> In June 2008 the [[BBC]] announced that it would be dropping use of microformats using the <code>abbr</code> design pattern because of accessibility concerns.<ref name="BBCabbr">{{cite web |url=http://www.bbc.co.uk/blogs/radiolabs/2008/06/removing_microformats_from_bbc.shtml |title=Removing Microformats from bbc.co.uk/programmes |first=Michael |last=Smethurst |publisher= [[BBC]] |date=2008-06-23 |accessdate=2008-08-24}}</ref>

=== Comparison with alternative approaches ===
Microformats are not the only solution for providing "more intelligent data" on the web; alternative approaches are used and are under development. For example, the use of [[XML]] markup and standards of the Semantic Web are cited as alternative approaches.<ref name="Khare000"/> Some contrast these with microformats in that they do not necessarily coincide with the design principles of "reduce, reuse, and recycle", at least not to the same extent.<ref name="Khare000"/>

One advocate of microformats, [[Tantek Çelik]], characterized a problem with alternative approaches: {{cquote|Here's a new language we want you to learn, and now you need to output these additional files on your server. It's a hassle. (Microformats) lower the barrier to entry.<ref name="Wharton000"/>}}

For some applications the use of other approaches may be valid. If the type of data to be described does not map to an existing microformat, [[RDFa]] can embed arbitrary vocabularies into HTML, such as for example domain-specific scientific data such as zoological or chemical data for which there is no microformat. Standards such as W3C's [[GRDDL]] allow microformats to be converted into data compatible with the Semantic Web.<ref name="King007">{{cite web |title=W3C GRDDL Recommendation Bridges HTML/Microformats and the Semantic Web |work=XML Coverpages |date=2007-09-13 |url=http://xml.coverpages.org/ni2007-09-13-a.html |publisher=[[OASIS (organization)|OASIS]] |accessdate=2007-11-23}}</ref>

Another advocate of microformats, Ryan King, put the compatibility of microformats with other approaches this way: {{cquote|Microformats provide an easy way for many people to contribute semantic data to the web. With GRDDL all of that data is made available for RDF Semantic Web tools. Microformats and GRDDL can work together to build a better web.<ref name="King007"/>}}

== See also ==
*[[COinS]]
*[[Embedded RDF]]
*[[Intelligent agent]]s
*[[RDFa Lite]]
*[[JSON-LD]]
*[[S5 (file format)]]
*[[Schema.org]]
*[[Simple HTML Ontology Extensions]]
*[[XMDP]]

== Notes ==
{{Reflist|2}}

== References ==
{{Refbegin|2}}
*{{cite book |last=Allsopp |first=John | title=Microformats: Empowering Your Markup for Web 2.0 |date=March 2007 |publisher=[[Apress|Friends of ED]] |isbn=978-1-59059-814-6 |page=368}}
*{{cite book |last=Orchard |first=Leslie M |title=Hacking RSS and Atom |date=September 2005 |publisher=[[John Wiley & Sons]] |isbn=978-0-7645-9758-9 |page=602}}
*{{cite book |last=Robbins |first=Jennifer Niederst |authorlink=Jennifer Niederst Robbins |first2=Tantek|last2=Çelik|authorlink2=Tantek Çelik|first3=Derek|last3=Featherstone|first4=Aaron|last4=Gustafson|title=Web Design In A Nutshell |edition=Third |date=February 2006 |publisher=[[O'Reilly Media]] |isbn=978-0-596-00987-8 |page=826}}

{{Refend}}

== Further reading ==
* {{cite book |last= Suda |first= Brian |title= Using Microformats |date=September 2006 |publisher= [[O'Reilly Media]] |isbn=978-0-596-52821-8 |page=45}}
* Ahmet Soylu, Patrick De Causmaecker, Fridolin Wild [http://www.rintonpress.com/journals/jmmonline.html#v6n1  Ubiquitous Web for Ubiquitous Environments: The Role of Embedded Semantics], article in Journal of Mobile Multimedia, Vol. 6, No.1, pp.&nbsp;26–48, (2010). [https://lirias.kuleuven.be/bitstream/123456789/243944/2/JMM_soylu_et_al_2010.pdf PDF]

== External links ==
{{Commons category|Microformat screenshots}}
* [http://microformats.org/ microformats.org]
* [http://www.digital-web.com/articles/microformats_primer/ Microformats Primer]
* [http://microformatique.com/optimus/ Optimus] microformats parser and validator
* [http://blog.mozilla.com/faaborg/2006/12/11/microformats-part-0-introduction A four-part discussion of Microformats, UI issues, and possible presentation in Firefox 3 by Alex Faaborg of Mozilla]

{{Semantic Web}}
{{Use dmy dates|date=January 2011}}

[[Category:Microformats| ]]
[[Category:Knowledge representation]]
[[Category:Semantic HTML]]
[[Category:Semantic Web]]
[[Category:Web design]]
[[Category:Web development]]
<=====doc_Id=====>:749
<=====title=====>:
Category:Grouping
<=====text=====>:
[[Category:Knowledge representation]]
<=====doc_Id=====>:752
<=====title=====>:
Knowledge space
<=====text=====>:
{{about|knowledge spaces in mathematical psychology|the concept studied by philosopher Pierre Lévy|Knowledge space (philosophy)}}

In [[mathematical psychology]], a '''knowledge space''' is a [[antimatroid|combinatorial structure]] describing the possible states of knowledge of a human learner.<ref>{{citation|title=Knowledge Spaces|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|publisher=Springer-Verlag|year=1999|isbn = 3-540-64501-2}}.</ref>
To form a knowledge space, one models a domain of knowledge as a [[set (mathematics)|set]] of concepts, and a feasible state of knowledge as a [[subset]] of that set containing the concepts known or knowable by some individual. Typically, not all subsets are feasible, due to prerequisite relations among the concepts. The knowledge space is the family of all the feasible subsets. Knowledge spaces were introduced in 1985 by [[Jean-Paul Doignon]] and [[Jean-Claude Falmagne]]<ref>{{citation|last1=Doignon|first1=J.-P.|last2=Falmagne|first2=J.-Cl.|author2-link=Jean-Claude Falmagne|year=1985|title=Spaces for the assessment of knowledge|journal=International Journal of Man-Machine Studies|volume=23|issue=2|pages=175–196|doi=10.1016/S0020-7373(85)80031-6}}.</ref> and have since been studied by many other researchers.<ref>{{citation|title=Knowledge Spaces. Applications in Education|last1=Falmagne|first1=J.-Cl.|author1-link=Jean-Claude Falmagne|last2=Albert|first2=D.|last3=Doble|first3=C.|last4=Eppstein|first4=D.|author4-link=David Eppstein|last5=Hu|first5=X.|publisher=Springer|year=2013}}.</ref><ref>A [http://kst.hockemeyer.at/kst-bib.html bibliography on knowledge spaces] maintained by Cord Hockemeyer contains over 400 publications on the subject.</ref> They also form the basis for two computerized tutoring systems, [http://wundt.kfunigraz.ac.at/rath/ RATH] and [[ALEKS]].<ref>[http://wundt.uni-graz.at/MathPsych/cda/overview_sokrates.htm Introduction to Knowledge Spaces: Theory and Applications], Christof Körner, Gudrun Wesiak, and Cord Hockemeyer, 1999 and 2001.</ref>

It is possible to interpret a knowledge space as a special form of a restricted [[latent class model]].<ref>{{citation|title=About the connection between knowledge structures and latent class models |last1=Schrepp |first1=M. |journal=Methodology|volume=1|issue=3|pages=92–102|year=2005|doi=10.1027/1614-2241.1.3.92}}.</ref>

==Definitions==
Some basic definitions used in the knowledge space approach -
*A tuple <math>(Q, K)</math> consisting of a non-empty set <math>Q</math> and a set <math>K</math> of subsets from <math>Q</math> is called a ''knowledge structure'' if <math>K</math> contains the empty set and <math>Q</math>.
*A knowledge structure is called a ''knowledge space'' if it is closed under union, i.e. if <math>S, T \in Q</math> implies <math>S\cup T \in Q</math>.
*A knowledge space is called a ''quasi-ordinal knowledge space'' if it is in addition closed under intersection, i.e. if <math>S, T \in Q</math> implies <math>S\cap T \in Q</math>. Closure under both unions and intersections gives (''Q'',∪,∩) the structure of a [[distributive lattice]]; [[Birkhoff's representation theorem]] for distributive lattices shows that there is a one-to-one correspondence between the set of all [[preorder|quasiorders]] on Q and the set of all quasi-ordinal knowledge spaces on Q. I.e., each quasi-ordinal knowledge space can be represented by a quasi-order and vice versa.

An important subclass of knowledge spaces, the ''well-graded knowledge spaces'' or ''learning spaces'', can be defined as satisfying two additional mathematical axioms:
# If <math>S</math> and <math>T</math> are both feasible subsets of concepts, then <math>S\cup T</math> is also feasible. In educational terms: if it is possible for someone to know all the concepts in ''S'', and someone else to know all the concepts in ''T'', then we can posit the potential existence of a third person who combines the knowledge of both people.
# If <math>S</math> is a nonempty feasible subset of concepts, then there is some concept ''x'' in ''S'' such that <math>S\setminus\{x\}</math> is also feasible. In educational terms: any feasible state of knowledge can be reached by learning one concept at a time, for a finite set of concepts to be learned.
A set family satisfying these two axioms forms a [[mathematical structure]] known as an [[antimatroid]].

==Construction of knowledge spaces==
In practice, there exist several methods to construct knowledge spaces. The most frequently used method is querying experts. There exist several querying algorithms that allow one or several experts to construct a knowledge space by answering a sequence of simple questions.<ref>{{citation|title=Extracting human expertise for constructing knowledge spaces: An algorithm |last1=Koppen |first1=M. |journal=Journal of Mathematical Psychology|volume=37|pages=1–20 |year=1993 |doi=10.1006/jmps.1993.1001}}.</ref><ref>{{citation|title=How to build a knowledge space by querying an expert |last1=Koppen |first1=M. |last2=Doignon |first2=J.-P. |journal=Journal of Mathematical Psychology|volume=34|issue=3|pages=311–331 |year=1990 |doi=10.1016/0022-2496(90)90035-8}}.</ref><ref>{{citation|title=A simulation study concerning the effect of errors on the establishment of knowledge spaces by querying experts |last1=Schrepp |first1=M. |last2=Held |first2=T. |journal=Journal of Mathematical Psychology|volume=39|issue=4|pages=376–382 |year=1995|doi=10.1006/jmps.1995.1035}}</ref>

Another method is to construct the knowledge space by explorative data analysis (for example by [[Item tree analysis]]) from data.<ref>{{citation|title=Extracting knowledge structures from observed data |last1=Schrepp |first1=M. |journal=[[British journal of mathematical and statistical psychology]]|volume= 52|issue=2 |pages=213–224 |year=1999|doi=10.1348/000711099159071}}</ref><ref>{{citation|title=A method for the analysis of hierarchical dependencies between items of a questionnaire |last1=Schrepp |first1=M. |journal= Methods of Psychological Research Online|volume=19|pages=43–79  |year=2003|url=http://www.dgps.de/fachgruppen/methoden/mpr-online/issue19/art3/mpr106_04.pdf }}</ref>
A third method is to derive the knowledge space from an analysis of the problem solving processes in the corresponding domain.<ref>{{citation|title=Knowledge Spaces: Theories, Empirical Research, Applications|last1=Albert|first1=D.|last2=Lukas|first2=J.|publisher=Lawrence Erlbaum Associates, Mahwah, NJ|year=1999}}</ref>

==References==

{{reflist}}

[[Category:Cognition]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:755
<=====title=====>:
Yale shooting problem
<=====text=====>:
The '''Yale shooting problem''' is a conundrum or scenario in formal situational [[logic]] on which early logical solutions to the [[frame problem]] fail. The name of this problem derives from its inventors, [[Steve Hanks]] and [[Drew McDermott]], working at [[Yale University]] when they proposed it. In this scenario, Fred (later identified as a [[turkey (bird)|turkey]]) is initially alive and a gun is initially unloaded. Loading the gun, waiting for a moment, and then shooting the gun at Fred is expected to kill Fred. However, if [[inertia]] is formalized in logic by minimizing the changes in this situation, then it cannot be uniquely proved that Fred is dead after loading, waiting, and shooting. In one solution, Fred indeed dies; in another (also logically correct) solution, the gun becomes mysteriously unloaded and Fred survives.

Technically, this scenario is described by two [[fluent (artificial intelligence)|fluents]] (a fluent is a condition that can change [[truth value]] over time): <math>alive</math> and <math>loaded</math>. Initially, the first condition is true and the second is false. Then, the gun is loaded, some time passes, and the gun is fired. Such problems can be formalized in logic by considering four time points <math>0</math>, <math>1</math>, <math>2</math>, and <math>3</math>, and turning every fluent such as <math>alive</math> into a predicate <math>alive(t)</math> depending on time. A direct formalization of the statement of the Yale shooting problem in logic is the following one:

: <math>alive(0)</math>
: <math>\neg loaded(0)</math>
: <math>true \rightarrow loaded(1)</math>
: <math>loaded(2) \rightarrow \neg alive(3)</math>

The first two formulae represent the initial state. The third formula formalizes the effect of loading the gun at time <math>0</math>. The fourth formula formalizes the effect of shooting at Fred at time <math>2</math>. This is a simplified formalization in which action names are neglected and the effects of actions are directly specified for the time points in which the actions are executed. See [[situation calculus]] for details.

The formulae above, while being direct formalizations of the known facts, do not suffice to correctly characterize the domain. Indeed, <math>\neg alive(1)</math> is consistent with all these formulae, although there is no reason to believe that Fred dies before the gun has been shot. The problem is that the formulae above only include the effects of actions, but do not specify that all fluents not changed by the actions remain the same. In other words, a formula <math>alive(0) \equiv alive(1)</math> must be added to formalize the implicit assumption that loading the gun ''only'' changes the value of <math>loaded</math> and not the value of <math>alive</math>. The necessity of a large number of formulae stating the obvious fact that conditions do not change unless an action changes them is known as the [[frame problem]].

An early solution to the frame problem was based on minimizing the changes. In other words, the scenario is formalized by the formulae above (that specify only the effects of actions) and by the assumption that the changes in the fluents over time are as minimal as possible. The rationale is that the formulae above enforce all effect of actions to take place, while minimization should restrict the changes to exactly those due to the actions.

In the Yale shooting scenario, one possible evaluation of the fluents in which the changes are minimized is the following one.

{| cellpadding="5"
| <math>alive(0)</math>
| <math>alive(1)</math> 
| <math>alive(2)</math> 
| <math>\neg alive(3)</math>
|-
| <math>\neg loaded(0)</math>
| <math>loaded(1)</math>
| <math>loaded(2)</math>
| <math>loaded(3)</math>
|}

This is the expected solution. It contains two fluent changes: <math>loaded</math> becomes true at time 1 and <math>alive</math> becomes false at time 3. The following evaluation also satisfies all formulae above.

{| cellpadding="5"
| <math>alive(0)</math>
| <math>alive(1)</math> 
| <math>alive(2)</math> 
| <math>alive(3)</math>
|-
| <math>\neg loaded(0)</math>
| <math>loaded(1)</math>
| <math>\neg loaded(2)</math>
| <math>\neg loaded(3)</math>
|}

In this evaluation, there are still two changes only: <math>loaded</math> becomes true at time 1 and false at time 2. As a result, this evaluation is considered a valid description of the evolution of the state, although there is no valid reason to explain <math>loaded</math> being false at time 2. The fact that minimization of changes leads to wrong solution is the motivation for the introduction of the Yale shooting problem.

While the Yale shooting problem has been considered a severe obstacle to the use of logic for formalizing dynamical scenarios, solutions to it are known since the late 1980s. One solution involves the use of [[predicate completion]] in the specification of actions: according to this solution, the fact that shooting causes Fred to die is formalized by the preconditions: ''alive'' and ''loaded'', and the effect is that ''alive'' changes value (since ''alive'' was true before, this corresponds to ''alive'' becoming false). By turning this implication into an ''if and only if'' statement, the effects of shooting are correctly formalized. (Predicate completion is more complicated when there is more than one implication involved.)

A solution proposed by [[Erik Sandewall]] was to include a new condition of occlusion, which formalizes the “permission to change” for a fluent. The effect of an action that might change a fluent is therefore that the fluent has the new value, and that the occlusion is made (temporarily) true. What is minimized is not the set of changes, but the set of occlusions being true. Another constraint specifying that no fluent changes unless occlusion is true completes this solution.

The Yale shooting scenario is also correctly formalized by the [[Ray Reiter|Reiter]] version of the [[situation calculus]], the [[fluent calculus]], and the [[action description language]]s.

In 2005, the 1985 paper in which the Yale shooting scenario was first described received the [[AAAI Classic Paper award]]. In spite of being a solved problem, that example is still sometimes mentioned in recent research papers, where it is used as an illustrative example (e.g., for explaining the syntax of a new logic for reasoning about actions), rather than being presented as a problem.

==See also==

* [[Circumscription (logic)]]
* [[Frame problem]]
* [[Situation calculus]]

==References==

* M. Gelfond and V. Lifschitz (1993). Representing action and change by logic programs. ''Journal of Logic Programming'', 17:301–322.
* S. Hanks and D. McDermott (1987). Nonmonotonic logic and temporal projection. ''Artificial Intelligence'', 33(3):379–412.
* J. McCarthy (1986). Applications of circumscription to formalizing common-sense knowledge. ''Artificial Intelligence'', 28:89–116.
* T. Mitchell and H. Levesque (2006). The 2005 AAAI Classic Paper awards. "AI Magazine", 26(4):98–99.
* R. Reiter (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression. In Vladimir Lifschitz, editor, ''Artificial Intelligence and Mathematical Theory of Computation: Papers in Honor of John McCarthy'', pages 359–380. Academic Press, New York.
* E. Sandewall (1994). ''Features and Fluents''. Oxford University Press.

[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:1987 introductions]]
<=====doc_Id=====>:758
<=====title=====>:
Defeasible reasoning
<=====text=====>:
{{No footnotes|date=April 2010}}
In [[logic]], '''defeasible reasoning''' is a kind of [[reasoning]] that is rationally compelling though not [[deductive reasoning|deductively valid]].<ref>{{cite web | url=http://plato.stanford.edu/entries/reasoning-defeasible | title="Defeasbile Reasoning," ''Stanford Encyclopedia of Philosophy | accessdate=1 July 2016 }}</ref> The distinction between defeasibility and indefeasibility may be seen in the context of this joke:

:During a train trip through the countryside, an engineer, a physicist, and a mathematician observe a flock of sheep. The engineer remarks, "I see that the sheep in this region are white." The physicist offers a correction, "''Some'' sheep in this region are white." And the mathematician responds, "In this region there exist sheep that are white on at least one side."

The engineer in this story has reasoned defeasibly; since [[engineering]] is a highly practical discipline, it is receptive to generalizations. In particular, engineers cannot and need not defer decisions until they have acquired perfect and complete knowledge. But [[mathematics|mathematical reasoning]], having different goals, inclines one to account for even the rare and special cases, and thus typically leads to a stance that is indefeasible.

Defeasible reasoning is a particular kind of non-demonstrative reasoning, where the reasoning does not produce a full, complete, or final demonstration of a claim, i.e., where fallibility and corrigibility of a conclusion are acknowledged. In other words defeasible reasoning produces a [[Wiktionary:contingent|contingent]] statement or claim.  Other kinds of non-demonstrative reasoning are [[probabilistic reasoning]], [[inductive reasoning]], [[statistical]] reasoning, [[abductive reasoning]], and [[paraconsistent]] reasoning.  Defeasible reasoning is also a kind of [[ampliative]] reasoning because its conclusions reach beyond the pure meanings of the premises.

The differences between these kinds of reasoning correspond to differences about the conditional that each kind of reasoning uses, and on what premise (or on what authority) the conditional is adopted:
* ''[[deductive reasoning|Deductive]]'' (from meaning postulate, axiom, or contingent assertion): if ''p'' then ''q'' (i.e., ''q'' or ''not-p'')
* ''Defeasible'' (from authority): if ''p'' then (defeasibly) ''q''
* ''[[Probabilistic logic|Probabilistic]]'' (from combinatorics and indifference): if ''p'' then (probably) ''q''
* ''[[Statistics|Statistical]]'' (from data and presumption):  the frequency of ''q''s among ''p''s is high (or inference from a model fit to data); hence, (in the right context) if ''p'' then (probably) ''q''
* ''[[inductive reasoning|Inductive]]'' (theory formation; from data, coherence, simplicity, and confirmation): (inducibly) "if ''p'' then ''q''"; hence, if ''p'' then (deducibly-but-revisably) ''q''
* ''[[abductive reasoning|Abductive]]'' (from data and theory):  ''p'' and ''q'' are correlated, and ''q'' is sufficient for ''p''; hence, if ''p'' then (abducibly) ''q'' as cause

Defeasible reasoning finds its fullest expression in [[jurisprudence]], [[ethics]] and [[moral philosophy]], [[epistemology]], [[pragmatics]] and conversational [[Convention (norm)|conventions]] in [[linguistics]], [[Constructivist epistemology|constructivist]] [[Decision theory|decision theories]], and in [[knowledge representation]] and [[planning]] in [[artificial intelligence]].  It is also closely identified with [[prima facie]] (presumptive) reasoning (i.e., reasoning on the "face" of evidence), and [[ceteris paribus]] (default) reasoning (i.e., reasoning, all things "being equal").

== History ==

Though [[Aristotle]] differentiated the forms of reasoning that are valid for [[logic]] and [[philosophy]] from the more general ones that are used in everyday life (see [[dialectics]] and [[rhetoric]]), 20th century philosophers mainly concentrated on deductive reasoning. At the end of the 19th century, logic texts would typically survey both demonstrative and non-demonstrative reasoning, often giving more space to the latter. However, after the blossoming of [[mathematical logic]] at the hands of [[Bertrand Russell]], [[Alfred North Whitehead]] and [[Willard van Orman Quine]], latter-20th century logic texts paid little attention to the non-deductive modes of inference.

There are several notable exceptions. [[John Maynard Keynes]] wrote his dissertation on non-demonstrative reasoning, and influenced the thinking of [[Ludwig Wittgenstein]] on this subject. Wittgenstein, in turn, had many admirers, including the [[positivist]] legal scholar [[H.L.A. Hart]] and the [[speech act]] linguist [[John L. Austin]], [[Stephen Toulmin]] in rhetoric ([[Chaim Perelman]] too), the moral theorists [[W.D. Ross]] and [[C.L. Stevenson]], and the [[vagueness]] epistemologist/ontologist [[Friedrich Waismann]].

The etymology of ''defeasible'' usually refers to Middle English law of contracts, where a condition of defeasance is a clause that can invalidate or annul a contract or deed. Though ''defeat'', ''dominate'', ''defer'', ''defy'', ''deprecate'' and ''derogate'' are often used in the same contexts as ''defeasible,'' the verbs ''annul'' and ''invalidate'' (and ''nullify,'' ''overturn,'' ''rescind,'' ''vacate,'' ''repeal,'' ''debar'', ''void'', ''cancel'', ''countermand'', ''preempt'', etc.) are more properly correlated with the concept of defeasibility than those words beginning with the letter ''d''. Many dictionaries do contain the verb, ''to defease'' with past participle, ''defeased.''

Philosophers in moral theory and rhetoric had taken defeasibility largely for granted when American epistemologists rediscovered Wittgenstein's thinking on the subject: John Ladd, [[Roderick Chisholm]], [[Roderick Firth]], [[Ernest Sosa]], [[Robert Nozick]], and [[John L. Pollock]] all began writing with new conviction about how ''appearance as red'' was only a defeasible reason for believing something to be red.  More importantly Wittgenstein's orientation toward [[language-games]] (and away from [[semantics]]) emboldened these epistemologists to manage rather than to expurgate ''prima facie'' logical inconsistency.

At the same time (in the mid-1960s), two more students of Hart and Austin at Oxford, [[Brian Barry]] and [[David Gauthier]], were applying defeasible reasoning to political argument and practical reasoning (of action), respectively. [[Joel Feinberg]] and [[Joseph Raz]] were beginning to produce equally mature works in ethics and jurisprudence informed by defeasibility.  

By far the most significant works on defeasibility by the mid-1970s were in epistemology, where [[John L. Pollock|John Pollock]]'s 1974 ''Knowledge and Justification'' popularized his terminology of ''undercutting'' and ''rebutting'' (which mirrored the analysis of Toulmin). Pollock's work was significant precisely because it brought defeasibility so close to philosophical logicians. The failure of logicians to dismiss defeasibility in epistemology (as Cambridge's logicians had done to Hart decades earlier) landed defeasible reasoning in the philosophical mainstream.  

Defeasibility had always been closely related to argument, rhetoric, and law, except in epistemology, where the chains of reasons, and the origin of reasons, were not often discussed. [[Nicholas Rescher]]'s ''Dialectics'' is an example of how difficult it was for philosophers to contemplate more complex systems of defeasible reasoning. This was in part because proponents of [[informal logic]] became the keepers of argument and rhetoric while insisting that formalism was anathema to argument.

About this time, researchers in [[artificial intelligence]] became interested in [[non-monotonic reasoning]] and its [[semantics]]. With philosophers such as Pollock and Donald Nute (e.g., [[defeasible logic]]), dozens of computer scientists and logicians produced complex systems of defeasible reasoning between 1980 and 2000. No single system of defeasible reasoning would emerge in the same way that Quine's system of logic became a de facto standard. Nevertheless, the 100-year headstart on non-demonstrative logical calculi, due to [[George Boole]], [[Charles Sanders Peirce]], and [[Gottlob Frege]] was being closed: both demonstrative and non-demonstrative reasoning now have formal calculi.

There are related (and slightly competing) systems of reasoning that are newer than systems of defeasible reasoning, e.g., [[belief revision]] and [[dynamic logic (modal logic)|dynamic logic]]. The dialogue logics of [[Charles Leonard Hamblin|Charles Hamblin]] and Jim Mackenzie, and their colleagues, can also be tied closely to defeasible reasoning. Belief revision is a non-constructive specification of the desiderata with which, or constraints according to which, epistemic change takes place. Dynamic logic is related mainly because, like paraconsistent logic, the reordering of premises can change the set of justified conclusions. Dialogue logics introduce an adversary, but are like belief revision theories in their adherence to deductively consistent states of belief.

==Political and judicial use==
Many political philosophers have been fond of the word ''indefeasible'' when referring to rights, e.g., that were ''inalienable,'' ''divine,'' or ''indubitable.''  For example, in the 1776 [[Virginia Declaration of Rights]], "community hath an indubitable, inalienable, and indefeasible right to reform, alter or abolish government..." (also attributed to [[James Madison]]); and [[John Adams]], "The people have a right, an indisputable, unalienable, indefeasible, divine right to that most dreaded and envied kind of knowledge – I mean of the character and conduct of their rulers."
Also, [[Lord Aberdeen]]:  "indefeasible right inherent in the British Crown" and [[Gouverneur Morris]]:  "the Basis of our own Constitution is the indefeasible Right of the People."  Scholarship about [[Abraham Lincoln]] often cites these passages in the justification of secession.  Philosophers who use the word ''defeasible'' have historically had different world views from those who use the word ''indefeasible'' (and this distinction has often been mirrored by Oxford and Cambridge zeitgeist); hence it is rare to find authors who use both words.

In judicial opinions, the use of ''defeasible'' is commonplace.  There is however disagreement among legal logicians whether ''defeasible reasoning'' is central, e.g., in the consideration of ''open texture'', [[precedent]], [[wikt:exception|exceptions]], and ''rationales'', or whether it applies only to explicit defeasance clauses.  [[H.L.A. Hart]] in ''[[The Concept of Law]]'' gives two famous examples of defeasibility:  "No vehicles in the park" (except during parades); and "Offer, acceptance, and memorandum produce a contract" (except when the contract is illegal, the parties are minors, inebriated, or incapacitated, etc.).

== Specificity ==

One of the main disputes among those who produce systems of defeasible reasoning is the status of a ''rule of specificity.''  In its simplest form, it is the same rule as subclass [[inheritance (computer science)|inheritance]] preempting class inheritance:  

  (R1) if ''r'' then (defeasibly) ''q''                  e.g., if bird, then can fly
  (R2) if ''p'' then (defeasibly) ''not-q''              e.g., if penguin, then cannot fly
  (O1) if ''p'' then (deductively) ''r''                 e.g., if penguin, then bird
  (M1) arguably, p                               e.g., arguably, penguin
  (M2) R2 is a more specific reason than R1      e.g., R2 is better than R1
  (M3) therefore, arguably, not-q                e.g., therefore, arguably, not-flies

Approximately half of the systems of defeasible reasoning discussed today adopt a rule of specificity, while half expect that such ''preference'' rules be written explicitly by whoever provides the defeasible reasons.  For example, Rescher's dialectical system uses specificity, as do early systems of multiple inheritance (e.g., [[David Touretzky]]) and the early argument systems of Donald Nute and of [[Guillermo Simari]] and [[Ronald Loui]].  Defeasible reasoning accounts of  precedent ([[stare decisis]] and [[case-based reasoning]]) also make use of specificity (e.g., [[Joseph Raz]] and the work of Kevin D. Ashley and Edwina Rissland).  Meanwhile, the argument systems of Henry Prakken and Giovanni Sartor, of Bart Verheij and Jaap Hage, and the system of Phan Minh Dung do not adopt such a rule.

== Nature of defeasibility ==

There is a distinct difference between those who theorize about defeasible reasoning as if it were a system of confirmational revision (with affinities to [[belief revision]]), and those who theorize about defeasibility as if it were the result of further (non-empirical) investigation.  There are at least three kinds of further non-empirical investigation:  progress in a lexical/syntactic process, progress in a computational process, and progress in an adversary or legal proceeding.  

'''''Defeasibility as corrigibility:'''''  Here, a person learns something new that annuls a prior inference.  In this case, defeasible reasoning provides a constructive mechanism for belief revision, like a [[truth maintenance system]] as envisioned by Jon Doyle.

'''''Defeasibility as shorthand for preconditions:'''''  Here, the author of a set of rules or legislative code is writing rules with exceptions.  Sometimes a set of defeasible rules can be rewritten, with more cogency, with explicit (local) pre-conditions instead of (non-local) competing rules.  Many non-monotonic systems with [[fixed point (mathematics)|fixed-point]] or [[preferential]] semantics fit this view.  However, sometimes the rules govern a process of argument (the last view on this list), so that they cannot be re-compiled into a set of deductive rules lest they lose their force in situations with incomplete knowledge or incomplete derivation of preconditions.  

'''''Defeasibility as an [[anytime algorithm]]:'''''  Here, it is assumed that calculating arguments takes time, and at any given time, based on a subset of the potentially constructible arguments, a conclusion is defeasibly justified.  [[Isaac Levi]] has protested against this kind of defeasibility, but it is well-suited to the heuristic projects of, for example, [[Herbert A. Simon]].  On this view, the ''best move so far'' in a chess-playing program's analysis at a particular depth is a defeasibly justified conclusion.  This interpretation works with either the prior or the next semantical view.

'''''Defeasibility as a means of controlling an investigative or social process:'''''  Here, justification is the result of the right kind of procedure (e.g., a fair and efficient hearing), and defeasible reasoning provides impetus for pro and con responses to each other.  Defeasibility has to do with the alternation of verdict as locutions are made and cases presented, not the changing of a mind with respect to new (empirical) discovery.  Under this view, defeasible reasoning and defeasible argumentation refer to the same phenomenon.

==See also==
* [[Defeasible estate]]
* [[Indefeasible rights of use]]
* [[Argument (logic)]]
* [[Prima facie]]
* [[Practical reasoning]]
* [[Pragmatics]]
* [[Non-monotonic reasoning]]

== References ==
{{Reflist}}
* [http://www.springerlink.com/index/UQ708JX7XG823H5F.pdf Defeasible logic], Donald Nute, Lecture Notes in Computer Science, Springer, 2003.
* [http://portal.acm.org/citation.cfm?id=371581 Logical models of argument], Carlos Chesnevar, et al., ACM Computing Surveys 32:4, 2000.
* [https://books.google.com/books?hl=en&lr=&id=bQHce6eNhDIC&oi=fnd&pg=PA219&dq=prakken&ots=h7xemV-dM1&sig=E6Ar7mAiBU0raO-rlNmzq8-8HG4 Logics for defeasible argumentation], Henry Prakken and Gerard Vreeswijk, in Handbook of Philosophical Logic, [[Dov M. Gabbay]], [[Franz Guenthner]], eds., Kluwer, 2002.
* [https://books.google.com/books?hl=en&lr=&id=DN5ERAAxUSYC&oi=fnd&pg=PR9&dq=rescher&ots=vRu4s0Ely-&sig=__Dvw746CkFNdCaAdNLCGImTbFU Dialectics], [[Nicholas Rescher]], SUNY Press, 1977.
* [http://linkinghub.elsevier.com/retrieve/pii/S0364021387800174 Defeasible reasoning], John Pollock, Cognitive Science, 1987.
* [https://scholar.google.com/scholar?hl=en&lr=&cites=7198700474843277547 Knowledge and Justification], John Pollock, Princeton University Press, 1974.
* [http://webdigg.net/Defeasible/Defeasible-reasoning/ Abstract argumentation systems], Gerard Vreeswijk, Artificial Intelligence, 1997.
* [http://portal.acm.org/citation.cfm?id=222099 Hart's critics on defeasible concepts and ascriptivism], [[Ronald Loui]], Proc. 5th Intl. Conf. on AI and Law, 1995.
* [https://scholar.google.com/scholar?hl=en&lr=&cites=7525164436422571935 Political argument], [[Brian Barry]], Routledge & Kegan Paul, 1970.
* [https://scholar.google.com/scholar?hl=en&lr=&cites=8944770465668267468 The uses of argument], [[Stephen Toulmin]], Cambridge University Press, 1958.
* [http://portal.acm.org/citation.cfm?id=981352&dl= Discourse relations and defeasible knowledge], Alex Lascarides and Nicholas Asher, Proc. of the 29th Meeting of the Assn. for Comp. Ling., 1991.
* [http://journals.cambridge.org/action/displayAbstract;jsessionid=E68F5CAC6B0001D1ABEFD7C8C24F919F.tomcat1?fromPage=online&aid=191503 Defeasible logic programming: an argumentative approach], Alejandro Garcia and [[Guillermo Simari]], Theory and Practice of Logic Programming 4:95–138, 2004. 
* [http://portal.acm.org/citation.cfm?id=180954.180957 Philosophical foundations of deontic logic and the logic of defeasible conditionals], Carlos Alchourron, in Deontic logic in computer science: normative system specification, J. Meyer, R. Wieringa, eds., Wiley, 1994.
* [http://www.sciencedirect.com/science?_ob=ArticleURL&_udi=B6TYF-47YRKSD-7C&_user=10&_coverDate=02%2F29%2F1992&_rdoc=1&_fmt=high&_orig=browse&_origin=browse&_zone=rslt_list_item&_srch=doc-info%28%23toc%235617%231992%23999469997%23391734%23FLP%23display%23Volume%29&_cdi=5617&_sort=d&_docanchor=&_ct=16&_acct=C000050221&_version=1&_urlVersion=0&_userid=10&md5=0735cebb41ce81bdfe8e260dbef2c71d&searchtype=a A Mathematical Treatment of Defeasible Reasoning and its Implementation.] [[Guillermo Simari]], [[Ronald Loui]], Artificial Intelligence Journal, 53(2–3): 125–157 (1992).

== External links ==
* [http://plato.stanford.edu/entries/reasoning-defeasible/ Article on Defeasible Reasoning] in the [[Stanford Encyclopedia of Philosophy]]
* [http://william-king.www.drexel.edu/top/prin/txt/Intro/Eco112c.html An example of defeasible reasoning in action]

[[Category:Epistemology]]
[[Category:Logic]]
[[Category:Logic programming]]
[[Category:Knowledge representation]]
[[Category:Reasoning]]
<=====doc_Id=====>:761
<=====title=====>:
Keyword AAA
<=====text=====>:
{{notability|date=August 2016}}
{{Multiple issues|
{{Orphan|date=April 2016}}
{{no footnotes|date=December 2011}}
}}

'''Keyword AAA''' is a [[thesaurus]] created by the [[State Records Authority of New South Wales]]. It is often used to [[categorize|categorise]] documents in a [[document management system]]. The thesaurus is often implemented in terms of [[ISO 2788]].

==External links==
* [http://www.records.nsw.gov.au/recordkeeping/resources/keyword-products/keyword-aaa Keyword AAA Overview]
* [https://www.records.nsw.gov.au/recordkeeping/advice/records-classification/developing-and-implementing-a-keyword-thesaurus Developing and implementing a keyword thesaurus]

[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
<=====doc_Id=====>:764
<=====title=====>:
Semantic analysis (knowledge representation)
<=====text=====>:
{{Cleanup|date=November 2008}}
{{context|date=February 2014}}
{{Vague|date=February 2009}}
'''Semantic analysis''' is a method for eliciting and representing [[knowledge]] about [[organisation]]s.{{Vague|date=February 2009}}<ref>[[Liu Kecheng]], (2000) Semiotics in [[information systems engineering]], Cambridge University Press.</ref> 

Initially the problem must be defined by domain experts and passed to the project analyst(s). The next step is the generation of candidate affordances. This step will generate a list of semantic units that may be included in the schema. The candidate grouping follows where some of the semantic units that will appear in the schema are placed in simple groups. Finally the groups will be integrated together into an [[Ontology_(information_science)|ontology]] chart. 

Semantic analysis always starts from the problem definition which if not clear, require the analyst to employ relevant [[literature]], [[interview]]s with the [[Stakeholder (corporate)|stakeholders]] and other techniques towards collecting supplementary [[information]]. All assumptions made must be genuine and not limiting the system. 

== See also ==
* [[Semantic analysis (machine learning)]]
* [[Ontology chart]]

==References==
{{reflist}}

[[Category:Knowledge representation]]

{{Library-stub}}
<=====doc_Id=====>:767
<=====title=====>:
Unique name assumption
<=====text=====>:
The '''unique name assumption''' is a simplifying assumption made in some [[ontology (computer science)|ontology]] languages and [[description logic]]s. In logics with the unique name assumption, different names always refer to different entities in the world.<ref>{{Cite AIMA |edition=2 |pages=333}}</ref>

The standard ontology language [[Web Ontology Language|OWL]] does not make this assumption, but provides explicit constructs to express whether two names denote the same or distinct entities.<ref>{{cite conference |first1=Jiao |last1=Tao |first2=Evren |last2=Sirin |first3=Jie |last3=Bao |first4=Deborah L. |last4=McGuinness |title=Integrity constraints in OWL |conference=Proc. AAAI |year=2010 |url=http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1931}}</ref><ref>[http://www.w3.org/TR/owl-ref/ OWL Web Ontology Language Reference]</ref>
* <code>owl:sameAs</code> is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to the same individual or entity.
* <code>owl:differentFrom</code> is the OWL property that asserts that two given names or identifiers (e.g., URIs) refer to different individuals or entities.

==See also==
* [[Closed-world assumption]]
* [[Coreference]]

==References==
{{reflist}}

[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]

{{logic-stub}}
<=====doc_Id=====>:770
<=====title=====>:
Vivification
<=====text=====>:
'''Vivification''' is an operation on a [[description logic]] knowledge base to improve performance of a [[semantic reasoner]].  Vivification replaces a [[Logical disjunction|disjunction]] of concepts <math>C_1 \sqcup C_2 \ldots \sqcup C_n</math> by the ''[[least common subsumer]]'' of the concepts <math>C_1,C_2,\ldots C_n</math>.

The goal of this operation is to improve the performance of the reasoner by replacing a complex set of concepts with a single concept which subsumes the original concepts. 

For example, consider the example given in (Cohen 92):  Suppose we have the concept <math>\textrm{PIANIST(Jill)} \vee \textrm{ORGANIST(Jill)}</math>.  This concept can be vivified into a simpler concept <math>\textrm{KEYBOARD-PLAYER(Jill)}</math>.  This summarization leads to an approximation that may not be exactly equivalent to the original.

== An approximation ==

[[Knowledge base]] vivification is not necessarily exact. If the reasoner is operating under the [[open world assumption]] we may get surprising results.  In the previous example, if we replace the disjunction with the vivified concept,  we will arrive at a surprising results.  

First, we find that the reasoner will no longer classify Jill as either a pianist or an organist.  Even though <math>\textrm{ORGANIST}</math> and <math>\textrm{PIANIST}</math> are the only two sub-classes, under the OWA we can no longer classify Jill as playing one or the other.  The reason is that there may be another keyboard instrument (e.g. a harpsichord) that Jill plays but which does not have a specific subclass.   

== References ==
# Cohen, W.W., Borgida, A., Hirsh, H., Computing Least Common Subsumers in Description Logics, In: Proc. AAAI-92, AAAI Press/The MIT Press, 1992, pages 754--760. [http://citeseer.ist.psu.edu/cohen92computing.html CiteSeer]
# Baader, F., Kusters, R., Wolter F., Extensions to Description Logics. In F. Baader, D. Calvanese, D. McGuinness, D. Nardi, and P.F. Patel-Schneider, editors, The Description Logic Handbook: Theory, Implementation, and Applications. Cambridge University Press, 2003. http://citeseer.ist.psu.edu/baader03basic.html
{{Wikt|vivification}}

[[Category:Knowledge representation]]
<=====doc_Id=====>:773
<=====title=====>:
Ronald J. Brachman
<=====text=====>:
{{Infobox scientist
| name              = Ronald Jay Brachman
| image             = <!--(filename only)-->
| image_size        = 
| alt               = 
| caption           = 
| birth_date        = {{Birth year and age|1949}} 
| birth_place       = 
| death_date        = <!-- {{Death date and age|YYYY|MM|DD|YYYY|MM|DD}} (death date then birth date) -->
| death_place       = 
| resting_place             = 
| resting_place_coordinates = <!-- {{Coord|LAT|LONG|type:landmark|display=inline,title}} -->
| residence         = 
| citizenship       = 
| nationality       = 
| fields            = 
| workplaces        = [[Harvard University]]<br>[[Yahoo! Research]]<br>[[AT&T Corporation]]<br>[[DARPA]]
| alma_mater        = [[Harvard University]]<br>[[Princeton University]]
| thesis_title      = A structural paradigm for representing knowledge
| thesis_url        = https://books.google.com/books?id=ThS-HAAACAAJ
| thesis_year       = 1977
| doctoral_advisor  = William A. Woods
| academic_advisors = 
| doctoral_students = 
| notable_students  = 
| known_for         = 
| author_abbrev_bot = 
| author_abbrev_zoo = 
| influences        = 
| influenced        = 
| awards            = 
| signature         = <!--(filename only)-->
| signature_alt     = 
| website           = {{URL|www.brachman.org}}<br>{{URL|research.yahoo.com/Ron_Brachman}}
| footnotes         = 
| spouse            = 
}}'''Ronald Jay "Ron" Brachman''' (born 1949) is the director of the Jacobs Technion-Cornell Institute at [[Cornell Tech]].<ref>{{Cite web|url=http://tech.cornell.edu/news/ron-brachman-joins-the-jacobs-technion-cornell-institute-at-cornell-tech-as|title=Ron Brachman Joins the Jacobs Technion-Cornell Institute at Cornell Tech as the New Director|website=Cornell Tech|access-date=2016-05-25}}</ref> Previously, he was the Chief Scientist of Yahoo! and head of [[Yahoo! Labs]].  Prior to that, he was the Associate Head of Yahoo! Labs and Head of Worldwide Labs and Research Operations.

==Education==
Brachman earned his [[Bachelor of Engineering|B.S.E.E.]] degree from [[Princeton University]], and his [[Master of Science|S.M.]] and [[Doctor of Philosophy|Ph.D.]] degrees from [[Harvard University]].

==Career==
Prior to working at Yahoo!, Brachman worked at [[DARPA]] as the Director of the [[Information Processing Techniques Office]] (IPTO), one of DARPA's eight offices at the time.  While at IPTO, he helped develop [[DARPA]]'s Cognitive Systems research efforts. Before that, he worked at [[AT&T Corporation|AT&T]] [[Bell Labs|Bell Laboratories]] ([[Murray Hill, New Jersey]]) as the Head of the [[Artificial Intelligence]] Principles Research Department (2004) and Director of the Software and Systems Research Laboratory.  When AT&T split with Lucent in 1996, he became Communications Services Research Vice President and was one of the founders of [[AT&T Labs]].

He is considered by some to be the godfather{{citation needed|date=August 2012}} of [[Description Logic]], the logic-based [[knowledge representation]] [[Semantics (computer science)|formalism]] underlying the [[Web Ontology Language]] OWL.]

==Publications==
He is the co-author with [[Hector Levesque]] of a popular book on [[knowledge representation and reasoning]]<ref>{{cite book |author1=Reiter, Ray |author2=Brachman, Ronald J. |author3=Levesque, Hector J. |title=Knowledge representation |publisher=MIT Press |location=Cambridge, Mass |year=1992 |pages= |isbn=0-262-52168-7 |oclc= |doi= |accessdate=}}</ref><ref>{{cite book |author1=Levesque, Hector J. |author2=Brachman, Ronald J. |title=Knowledge representation and reasoning |publisher=Elsevier/Morgan Kaufmann |location=Amsterdam |year=2004 |pages= |isbn=1-55860-932-6 |oclc= |doi= |accessdate=}}</ref> and many scientific papers.<ref name="microsoft">{{AcademicSearch|9029466}}</ref><ref name="dblp">{{DBLP|name=Ronald J. Brachman}}</ref><ref>Ronald J. Brachman (1983) "What IS-A is and isn't. An Analysis of Taxonomic Links in [[Semantic network|Semantic Networks]]"; ''IEEE Computer'', 16 (10); October.</ref>

==References==
{{reflist}}

== External links ==
* [http://www.cc.gatech.edu/events/dr-ronald-brachman-yahoo-research-distinguished-guest-lecture External biography]

{{DEFAULTSORT:Brachman, Ronald J.}}
[[Category:Living people]]
[[Category:Artificial intelligence researchers]]
[[Category:Knowledge representation]]
[[Category:Harvard University alumni]]
[[Category:Princeton University alumni]]
[[Category:Fellow Members of the IEEE]]
[[Category:Fellows of the Association for the Advancement of Artificial Intelligence]]
[[Category:Yahoo! employees]]
[[Category:1959 births]]


{{compu-bio-stub}}
<=====doc_Id=====>:776
<=====title=====>:
ITools Resourceome
<=====text=====>:
{{lowercase title}}
[[Image:Biositemap iTools NCBC.png|thumb|right|300px| NCBC iTools]]
'''iTools'''<ref>{{cite journal|vauthors=Dinov ID, Rubin D, Lorensen W, Dugan J, Ma J, Murphy S, Kirschner B, Bug W, Sherman M, Floratos A, Kennedy D, Jagadish HV, Schmidt J, Athey B, Califano A, Musen M, Altman R, Kikinis R, Kohane I, Delp S, Parker DS, Toga AW | title=iTools: A Framework for Classification, Categorization and Integration of Computational Biology Resources | journal=PLoS ONE |volume=3|issue=5|pages= e2265| doi=10.1371/journal.pone.0002265 | year=2008 |url=http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0002265 | pmid=18509477 | pmc=2386255}}</ref> is a distributed infrastructure for managing, discovery, comparison and integration of computational biology resources. iTools employs [[Biositemap]] technology to retrieve and service meta-data about diverse bioinformatics data services, tools, and web-services. iTools is developed by the [[National Centers for Biomedical Computing]] as part of the [http://nihroadmap.nih.gov/ NIH Road Map Initiative].

==See also==
* [[Biositemaps]]

==References==
<references/>

== External links ==
* [http://iTools.ccb.ucla.edu Interactive iTools Server]

[[Category:Knowledge representation]]
[[Category:Bioinformatics]]
<=====doc_Id=====>:779
<=====title=====>:
Category:Knowledge bases
<=====text=====>:
A '''[[Knowledge base]]''' is a special kind of [[database]] for [[knowledge management]]. It provides the means for the computerized collection, organization, and [[retrieval]]{{dn|date=August 2016}} of [[knowledge]].  It is also used for specified information and as a [[personal knowledge base]]

[[Category:Online databases]]
[[Category:Semantic Web]]
[[Category:Knowledge representation]]
[[Category:Types of databases]]
<=====doc_Id=====>:782
<=====title=====>:
User modeling
<=====text=====>:
'''User modeling''' is the subdivision of [[human–computer interaction]] which describes the
process of building up and modifying a conceptual understanding of the user. The main goal of user modeling is customization and [[Adaptation (computer science)|adaptation of systems]] to the user's specific needs. The system needs to "say the 'right' thing at the 'right' time in the 'right' way".<ref name=Fischer>{{Citation
  | last1 = Fischer | first1 = Gerhard
  | title = User Modeling in Human-Computer Interaction
  | journal = User Modeling and User-Adapted Interaction 11
  | pages = 65–68
  | year = 2001 }}</ref> To do so it needs an internal representation of the user. Another common purpose is modeling specific kinds of users, including modeling of their skills and declarative knowledge, for use in automatic software-tests.<ref name=JohnsonTaatgen> {{Citation
  | last1 = Johnson | first1 = Addie
  | last2=Taatgen | first2 = Niels
  | chapter = User Modeling
  | title = Handbook of human factors in Web design
  | pages = 424–439
  | publisher = Lawrence Erlbaum Associates
  | year = 2005 }}</ref> User-models can thus serve as a cheaper alternative to [[user testing]].

== Background ==

A user model is the collection and categorization of personal data associated with a specific user. Therefore, it is the basis for any adaptive changes to the system's behavior. Which data is included in the model depends on the purpose of the application. It can include personal information such as users' names and ages, their interests, their skills and knowledge, their goals and plans, their preferences and their dislikes or data about their behavior and their interactions with the system.

There are different design patterns for user models, though often a mixture of them is used.<ref name=JohnsonTaatgen /><ref>{{Citation
  | last1 = Hothi | first1 = Jatinder
  | last2=Hall | first2 = Wendy
  | title = An Evaluation of Adapted Hypermedia Techniques Using Static User Modelling
  | journal = Proceedings of the 2nd Workshop on Adaptive Hypertext and Hypermedia
  | place = Southampton University, Electronics and Computer Science University Road, Southampton, Hampshire, UK
  | year = June 1998
  | url = http://wwwis.win.tue.nl/ah98/Hothi/Hothi.html }}</ref>
* '''Static user models'''
:Static user models are the most basic kinds of user models. Once the main data is gathered they are normally not changed again, they are static. Shifts in users' preferences are not registered and no learning algorithms are used to alter the model.
* '''Dynamic user models'''
:Dynamic user models allow a more up to date representation of users. Changes in their interests, their learning progress or interactions with the system are noticed and influence the user models. The models can thus be updated and take the current needs and goals of the users into account.
* '''Stereotype based user models '''
:Stereotype based user models are based on [[Demographics|demographic statistics]]. Based on the gathered information users are [[Classification_in_machine_learning|classified]] into common stereotypes. The system then adapts to this stereotype. The application therefore can make assumptions about a user even though there might be no data about that specific area, because demographic studies have shown that other users in this stereotype have the same characteristics. Thus, stereotype based user models mainly rely on statistics and do not take into account that personal attributes might not match the stereotype. However, they allow predictions about a user even if there is rather little information about him or her.
* '''Highly adaptive user models'''
:Highly adaptive user models try to represent one particular user and therefore allow a very high adaptivity of the system. In contrast to stereotype based user models they do not rely on demographic statistics but aim to find a specific solution for each user. Although users can take great benefit from this high adaptivity, this kind of model needs to gather a lot of information first.

== Data gathering ==

Information about users can be gathered in several ways. There are three main methods:

* '''Asking for specific facts while (first) interacting with the system'''<ref name=JohnsonTaatgen />
:Mostly this kind of data gathering is linked with the registration process. While registering users are asked for specific facts, their likes and dislikes and their needs. Often the given answers can be altered afterwards.
* '''Learning users' preferences by observing and interpreting their interactions with the system'''<ref name=JohnsonTaatgen />
:In this case users are not asked directly for their personal data and preferences, but this information is derived from their behavior while interacting with the system. The ways they choose to accomplish a tasks, the combination of things they takes interest in, these observations allow inferences about a specific user. The application dynamically learns from observing these interactions. Different [[machine learning]] algorithms may be used to accomplish this task.
* '''A hybrid approach which asks for explicit feedback and alters the user model by adaptive learning'''<ref name=Montaner>{{Citation
  | last = Montaner | first = Miguel
  | last2 = López | first2 = Beatriz
  | last3 = De La Rosa | first3 = Josep Lluís
  | title = A Taxonomy of Recommender Agents on the Internet,
  | journal = Artif. Intell. Rev.
  | volume = 19
  | pages = 285–330
  | year = 2003 }}</ref>
:This approach is a mixture of the ones above. Users have to answer specific questions and give explicit feedback. Furthermore, their interactions with the system are observed and the derived information are used to automatically adjust the user models.

Though the first method is a good way to quickly collect main data it lacks the ability to automatically adapt to shifts in users' interests. It depends on the users' readiness to give information and it is unlikely that they are going to edit their answers once the registration process is finished. Therefore, there is a high likelihood that the user models are not up to date. However, this first method allows the users to have full control over the collected data about them. It is in their decision which information they are willing to provide. This possibility is missing in the second method. Adaptive changes in a system that learns users' preferences and needs only by interpreting their behavior might appear a bit opaque to the users, because they cannot fully understand and reconstruct why the system behaves the way it does.<ref name=Montaner /> Moreover, the system is forced to collect a certain amount of data before it is able to predict the users' needs with the required accuracy. Therefore, it takes a certain learning time before a user can benefit from adaptive changes. However, afterwards these automatically adjusted user models allow a quite accurate adaptivity of the system. The hybrid approach tries to combine the advantages of both methods. Through collecting data by directly asking its users it gathers a first stock of information which can be used for adaptive changes. By learning from the users' interactions it can adjust the user models and reach more accuracy. Yet, the designer of the system has to decide, which of these information should have which amount of influence and what to do with learned data that contradicts some of the information given by a user.

== System adaptation ==

Once a system has gathered information about a user it can evaluate that data by preset analytical algorithm and then start to adapt to the user's needs. These adaptations may concern every aspect of the system's behavior and depend on the system's purpose. Information and functions can be presented according to the user's interests, knowledge or goals by displaying only relevant features, hiding information the user does not need, making proposals what to do next and so on. One has to distinguish between [[Adaptation (computer science)#Adaptivity_and_adaptability|adaptive and adaptable systems]].<ref name=Fischer /> In an adaptable system the user can manually change the system's appearance, behavior or functionality by actively selecting the corresponding options. Afterwards the system will stick to these choices. In an [[adaptive system]] a dynamic adaption to the user is automatically performed by the system itself, based on the built user model. Thus, an adaptive system needs ways to interpret information about the user in order to make these adaptations. One way to accomplish this task is implementing rule-based filtering. In this case a set of IF... THEN... rules is established that covers the [[knowledge base]] of the system.<ref name=JohnsonTaatgen /> The IF-conditions can check for specific user-information and if they match the THEN-branch is performed which is responsible for the adaptive changes. Another approach is based on [[collaborative filtering]].<ref name=JohnsonTaatgen /><ref name=Montaner /> In this case information about a user is compared to that of other users of the same systems. Thus, if characteristics of the current user match those of another, the system can make assumptions about the current user by presuming that he or she is likely to have similar characteristics in areas where the model of the current user is lacking data. Based on these assumption the system then can perform adaptive changes.

== Usages ==

* [[Adaptive hypermedia]]: In an adaptive hypermedia system the displayed content and the offered hyperlinks are chosen on basis of users' specific characteristics, taking their goals, interests, knowledge and abilities into account. Thus, an adaptive hypermedia system aims to reduce the "lost in hyperspace" syndrome by presenting only relevant information.
* [[Adaptive educational hypermedia]]: Being a subdivision of adaptive hypermedia the main focus of adaptive educational hypermedia lies on education, displaying content and hyperlinks corresponding to the user's knowledge on the field of study.
* [[Intelligent tutoring system]]: Unlike adaptive educational hypermedia systems intelligent tutoring systems are stand-alone systems. Their aim is to help students in a specific field of study. To do so, they build up a user model where they store information about abilities, knowledge and needs of the user. The system can now adapt to this user by presenting appropriate exercises and examples and offering hints and help where the user is most likely to need them. 
* [[Expert systems]]: Expert systems are computer systems that emulate the decision-making ability of a human expert in order to help the user solving a problem in a specific area. Step by step they ask questions to identify the current problem and to find a solution. User models can be used to adapt to the current user's knowledge, differentiating between experts and novices. The system can assume, that experienced users are able to understand and answer more complex questions than someone who is new to the topic. Therefore, it can adjust the used vocabulary and the type of question which are presented to the user, thus reducing the steps needed to find a solution.
* [[Recommender system]]: The basic idea of recommender systems is to present a selection of items to the user which best fit his or her needs. This selection can be based on items the user has bookmarked, rated, bought, recently viewed, etc. Recommender systems are often used in [[e-commerce]] but may also cover areas like social networks, websites, news, etc.
* [[Usability testing|User-Simulation]]: Since user modeling allows the system to hold an internal representation of a specific user, different types of users can be simulated by artificially modeling them. Common types are "experts" or "novices" on the scope of the system or the usage of the system. Based on these characteristics user tests can be simulated.

== Standards==
A certain number of representation formats and standards are available for representing the users in computer systems,<ref>Nabeth Thierry (2005), [http://www.fidis.net/resources/fidis-deliverables/identity-of-identity/#c1753: Models], FIDIS Deliverable, October 2005. </ref> such as:
* [[IMS-LIP]] (IMS &ndash; Learner Information Packaging, used in [[e-learning]]) 
* [[HR-XML Standards|HR-XML]] (used in [[human resource management]])
* [[JXDM]] (Justice with the Global Justice Extensible Markup)
* [[Europass]] (the Europass online CV)

== See also ==

* [[Personalization]]
* [[Cognitive model]]
* [[User profile]]
* [[Identity management]]

== References ==
<references/>

== External references ==
* [http://www.umuai.org/ User Modeling and User-Adapted Interaction (UMUAI)] The Journal of Personalization Research 
* [http://www.cs.cmu.edu/~bej/cogtool/ CogTool Project at CMU]
* [http://www.iit.demokritos.gr/um2007/ UserModeling conference 2007]

[[Category:Knowledge representation]]
<=====doc_Id=====>:785
<=====title=====>:
Pretext
<=====text=====>:
A '''pretext''' (adj: '''pretextual''') is an excuse to do something or say something that is not accurate. Pretexts may be based on a half-truth or developed in the context of a misleading fabrication. Pretexts have been used to conceal the true purpose or rationale behind actions and words.

In [[Law of the United States|US law]], a pretext usually describes false reasons that hide the true intentions or motivations for a legal action. If a party can establish a [[prima facie]] case for the proffered evidence, the opposing party must prove that these reasons were "pretextual" or false. This can be accomplished by directly demonstrating that the motivations behind the presentation of evidence is false, or indirectly by evidence that the motivations are not "credible".<ref name=uslegal>{{cite web|title=Pretext Law & Legal Definition|url=http://definitions.uslegal.com/p/pretext/|publisher=uslegal.com|accessdate=13 March 2013}}</ref> In
''Griffith v. Schnitzer'', an employment discrimination case, a jury award was reversed by a [[Court of Appeals]] because the evidence was not sufficient that the defendant's reasons were "pretextual". That is, the defendant's evidence was either undisputed, or the plaintiff's  was "irrelevant subjective assessments and opinions".<ref>[http://www.omwlaw.com/wp-content/uploads/2013/01/Defining-Pretext-In-Discrimination-Cases.pdf Defining "pretext" in discrimination cases] by Karen Sutherland (2013)</ref>

A "pretextual" arrest by law enforcement officers is one carried out for illegal purposes such as to conduct an unjustified [[search and seizure]].<ref>[http://assets.wne.edu/161/8_note_Criminal.pdf Criminal law - Pretextual arrests and alternatives to the objective tests] by Robert D. Snook</ref><ref name=oday>{{cite web|last=O'Day|first=Kathleen M.|title=Pretextual traffic stops: injustice for minority drivers|url=http://academic.udayton.edu/race/03justice/s98oday.htm|publisher=The University of Dayton School of Law|accessdate=13 March 2013}}</ref>

[[File:Marbleboot.jpg|thumb|right|140px|[[Marble Boat]] on [[Kunming Lake]] near Beijing.]]
As one example of pretext, in the 1880s, the Chinese government raised money on the pretext of modernizing the Chinese navy. Instead, these funds were diverted to repair a ship-shaped, two-story pavilion which had been originally constructed for [[Empress Xiaoshengxian|the mother]] of the [[Qianlong Emperor]]. This pretext and the Marble Barge are famously linked with [[Empress Dowager Cixi]]. This architectural [[folly]], known today as the [[Marble Boat]] (''Shifang''), is "moored" on Lake Kunming in what the empress renamed the "Garden for Cultivating Harmony" (''Yiheyuan'').<ref>Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&pg=PA155&dq=marble+barge&client=firefox-a#PPA155,M1  ''The Last Empress,'' pp. 155-156;]</ref>

Another example of pretext was demonstrated in the speeches of the Roman orator [[Cato the Elder]] (234-149 BC). For Cato, every public speech became a pretext for a comment about Carthage. The Roman statesman had come to believe that the prosperity of ancient Carthage represented an eventual and inevitable danger to Rome. In the Senate, Cato famously ended every speech by proclaiming his opinion that [[Carthage]] had to be destroyed (''[[Carthago delenda est]]''). This oft-repeated phrase was the ultimate conclusion of all logical argument in every oration, regardless of the subject of the speech. This pattern persisted until his death in 149, which was the year in which the Third Punic War began. In other words, any subject became a pretext for reminding his fellow senators of the dangers Carthage represented.<ref>Hooper, William Davis ''et al.'' (1934). [http://penelope.uchicago.edu/Thayer/E/Roman/Texts/Cato/De_Agricultura/Introduction*.html   "Introduction,"] in Cato's ''De Agricultura'' (online version of Loeb edition).</ref>

==Uses in warfare==

[[File:Hokoji-Bell-M1767.jpg|thumb|right|140px|Temple bell at [[Hōkō-ji (Kyoto)|Hōkō-ji]].]][[File:Hokoji-BellDetail-M1767.jpg|thumb|right|140px|Inscription on bell at Hokoji in Kyoto]]
The early years of Japan's [[Tokugawa shogunate]] were unsettled, with warring factions battling for power. The causes for the fighting were in part pretextural, but the outcome brought diminished armed conflicts after the [[Siege of Osaka]] in 1614-1615.

* '''1614''' (''Keichō 19''): The Shogun vanquished Hideyori and set fire to [[Osaka Castle]], and then he returned for the winter to [[Edo]].<ref name="t410">Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&pg=PP9&dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]</ref>
* '''August 24, 1614''' (''Keichō 19, 19th day of the 7th month''): A new bronze bell for the Hōkō-ji was cast successfully [http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=4771][http://oldphoto.lb.nagasaki-u.ac.jp/en/target.php?id=3093]; but despite the dedication ceremony planning, Ieyasu forbade any further actions concerning the great bell:
::"[T]he tablet over the Daibutsu-den and the bell bore the inscription ''"Kokka ankō"'' (meaning "the country and the house, peace and tranquility"), and at this [[Tokugawa Ieyasu]] affect to take umbrage, alleging that it was intended as a curse on him for the character 安 (''an,'' "peace") was placed between the two characters composing his own name 家康 (''"ka-kō",'' "house tranquility") [suggesting subtly perhaps that peace could only be attained by Ieyasu's dismemberment?] ... This incident of the inscription was, of course, a mere pretext, but Ieyasu realized that he could not enjoy the power he had usurped as long as Hideyori lived, and consequently, although the latter more than once Hideyori dispatched his vassal Katagiri Kastumoto to Ieyasu's residence ([[Sunpu Castle]]) with profuse apologies, Ieyasu refused to be placated."<ref>Ponsonby-Fane, Richard. (1956). ''Kyoto, the Old Capital of Japan,'' p. 292; Titsingh, [https://books.google.com/books?id=18oNAAAAIAAJ&pg=PP9&dq=nipon+o+dai+itsi+ran#PRA1-PA410,M1  p. 410.]</ref>
* '''October 18, 1614''' (''Keichō 19, 25th day of the 10th month''): A strong earthquake shook Kyoto.<ref name="t410"/>
* '''1615''' (''Keichō 20''): Osaka Summer Battle begins.

The next two-and-a-half centuries of Japanese history were comparatively peaceful under the successors of Tokugawa Ieyasu and the [[bakufu]] government he established.

===United States===
*During the War of 1812, US President [[James Madison]] was often accused of using impressment of American sailors by the [[Royal Navy]] as a pretext to invade [[Canada]].

{{main|Pearl Harbor advance-knowledge debate}}
*Some have argued that United States President [[Franklin D. Roosevelt]] used the [[attack on Pearl Harbor]] by Japanese forces on December 7, 1941 as a pretext to enter [[World War II]].<ref>Bernstein, Richard. [http://www.nytimes.com/1999/12/15/books/books-of-the-times-on-dec-7-did-we-know-we-knew.html "On Dec. 7, Did We Know We Knew?"] ''New York Times.'' December 15, 1999.</ref> American soldiers and supplies had been assisting British and Soviet operations for almost a year by this point, and the United States had thus "chosen a side", but due to the political climate in the States at the time and some campaign promises made by Roosevelt that he would not send American boys to fight in foreign wars. Roosevelt could not declare war for fear of public backlash. The attack on Pearl Harbor united the American people's resolve against the Axis powers and created the bellicose atmosphere in which to declare war.
* Critics have accused United States President [[George W. Bush]] of using the [[September 11th, 2001 attacks]] and faulty intelligence about the existence of [[weapons of mass destruction]] as a pretext for the [[Iraq war|war in Iraq]].<ref>Borger, Julian. (2006). [https://www.theguardian.com/world/2006/sep/07/usa.books  "Book says CIA tried to provoke Saddam to war,"] ''The Guardian'' (London). 7 September 2006.</ref>

==Social engineering==
{{main|Social engineering (security)}}
A type of [[Social engineering (security)|social engineering]] called [[Social engineering (security)#Pretexting|pretexting]] uses a pretext to elicit information fraudulently  from a target. The pretext in this case includes research into the identity of a certain authorized person or personality type in order to establish legitimacy in the mind of the target.<ref>[[Federal Trade Commission]] (FTC):  [http://www.ftc.gov/bcp/edu/pubs/consumer/credit/cre10.shtm  "Pretexting: Your Personal Information Revealed."] February 2006.</ref>

==See also==
{{wiktionary}}

* [[Plausible deniability]]
* [[Proximate cause]]
* [[Causes of the Franco-Prussian War]]

==Notes==
{{reflist|2}}

==References==
* [[James Bamford|Bamford]], James. (2004). [https://books.google.com/books?id=VuOxAAAACAAJ&dq=Pretext+for+War:+9/11,+Iraq,+and+the+Abuse+of+America%27s+Intelligence+Agencies&client=firefox-a  ''Pretext for War: 9/11, Iraq, and the Abuse of America's Intelligence Agencies.'']  New York: [[Doubleday Books]]. ISBN 978-0-385-50672-4; [http://www.worldcat.org/oclc/55068034?referer=di&ht=edition OCLC 55068034]
* [[Cato the Elder|Cato]], Marcus Porcius. [https://books.google.com/books?id=D2mxAAAAIAAJ&q=Hooper+and+De+Agricultura&dq=Hooper+and+De+Agricultura&lr=&client=firefox-a&pgis=1 ''On Agriculture'' (''De agricultura'')] trans,   William Davis Hooper and Harrison Boyd Ash. Cambridge: [[Harvard University Press]]. [http://www.worldcat.org/oclc/230499252 OCLC  230499252]
* [[Michael Isikoff|Isikoff]], Michael and [[David Corn]]. 2006. [https://books.google.com/books?id=Sa14AAAAMAAJ&q=hubris&dq=hubris&pgis=1  ''Hubris: The Inside Story of Spin, Scandal, and the Selling of the Iraq War'']  New York: [[Crown Publishers]]. ISBN 978-0-307-34681-0
* Min, Anchee. (2007). [https://books.google.com/books?id=_H8TYkS84E4C&client=firefox-a  ''The Last Empress.''] New York: [[Houghton Mifflin Harcourt]]. ISBN 978-0-618-53146-2
* [[Richard Ponsonby-Fane|Ponsonby-Fane]], Richard Arthur Brabazon. (1956). ''Kyoto, the Old Capital of Japan,'' Kyoto: Ponsonby Memorial Society.
* [[Robert Stinnett|Stinnett]] Robert B. (2001). [https://books.google.com/books?id=Q2UKN5daNHYC ''Day of Deceit: The Truth about FDR and Pearl Harbor''] New York: [[Simon & Schuster]]. ISBN 978-0-7432-0129-2
* [[Isaac Titsingh|Titsingh]], Isaac. (1834). [Siyun-sai Rin-siyo/[[Hayashi Gahō]], 1652], ''[[Nipon o daï itsi ran]]; ou, [https://books.google.com/books?id=18oNAAAAIAAJ&dq=nipon+o+dai+itsi+ran  Annales des empereurs du Japon.'']  Paris: [[Royal Asiatic Society|Oriental Translation Fund of Great Britain and Ireland]].

[[Category:Propaganda techniques]]
[[Category:Knowledge representation]]
[[Category:Cognition]]
[[Category:Attack on Pearl Harbor]]
[[Category:Social engineering (computer security)]]
<=====doc_Id=====>:788
<=====title=====>:
Framing (social sciences)
<=====text=====>:
{{globalize|date=July 2010}}

In the [[social sciences]], '''framing''' comprises a set of concepts and theoretical perspectives on how individuals, groups, and societies, organize, perceive, and communicate about [[reality]]. Framing involves [[social construction]] of a [[social phenomenon]] – by [[mass media]] sources, political or social movements, political leaders, or other actors and organizations. Participation in a language community necessarily influences an individual's ''[[perception]]'' of the meanings attributed to words or phrases. Politically, the language communities of [[advertising]], [[religion]], and mass media are highly contested, whereas framing in less-sharply defended [[speech community|language communities]] might evolve imperceptibly and organically over [[cultural]] time frames, with fewer overt modes of disputation.  

Framing itself can be framed in one of two ways, depending on whether one chooses to emphasise processes of [[cognition|thought]] or processes of interpersonal [[communication]]. ''Frames in thought'' consist of the mental representations, interpretations, and simplifications of reality. ''Frames in communication'' consist of the communication of frames between different actors.<ref name="Druckman2001">{{cite journal | last1 = Druckman | first1 = J.N. | year = 2001 | title = The Implications of Framing Effects for Citizen Competence | url = | journal = Political Behavior | volume = 23 | issue = 3| pages = 225–256 | doi=10.1023/A:1015006907312}}</ref>

One can view framing in communication as positive or negative – depending on the audience and what kind of information is being presented. Framing might also be understood as being either ''equivalence frames'', which represent logically equivalent alternatives portrayed in different ways (see [[framing effect (psychology)|framing effect]]) or as ''emphasis frames'', which simplify reality by focusing on a subset of relevant aspects of a situation or issue.<ref name="Druckman2001" /> In the case of  "equivalence frames", the information being presented is based on the same facts, but the "frame" in which it is presented changes, thus creating a reference-dependent perception.

The effects of framing can be seen in many journalism applications. With the same information being used as a base, the "frame" surrounding the issue can change the reader's perception without having to alter the actual facts. In the context of politics or mass-media communication, a frame defines the packaging of an element of [[rhetoric]] in such a way as to encourage certain interpretations and to discourage others.  For political purposes, framing often presents facts in such a way that implicates a problem that is in need of a solution. Members of political parties attempt to frame issues in a way that makes a solution favoring their own political leaning appear as the most appropriate course of action for the situation at hand.<ref name="van der Pas">{{cite journal|last=van der Pas|first=D.|title=Making Hay While the Sun Shines: Do Parties Only Respond to Media Attention When The Framing is Right?|journal=Journal of Press/Politics|year=2014|volume=19|issue=1|pages=42–65|doi=10.1177/1940161213508207}}<!--|accessdate=6 March 2014--></ref>

In [[social theory]], framing is a [[Schema (psychology)|schema]] of [[interpretation (logic)|interpretation]], a collection of [[Anecdotal evidence|anecdotes]] and [[stereotype]]s, that individuals rely on to understand and respond to events.<ref name="Goffman1974">
Goffman, E. (1974). Frame analysis: An easy on the organization of experience. Cambridge, MA: Harvard University Press. 
</ref> In other words, people build a series of mental "filters" through biological and cultural influences. They then use these filters to make sense of the world. The choices they then make are influenced by their creation of a frame.

Framing is also a key component of [[sociology]], the study of social interaction among humans.  Framing is an integral part of conveying and processing data on a daily basis.  Successful framing techniques can be used to reduce the ambiguity of intangible topics by contextualizing the information in such a way that recipients can connect to what they already know.

== Explanation ==
When one seeks to explain an event, the understanding often depends on the frame referred to. If a friend rapidly closes and opens an eye, we will respond very differently depending on whether we attribute this to a purely "physical" frame (they blinked) or to a social frame (they winked).

Though the former might result from a speck of dust (resulting in an involuntary and not particularly meaningful reaction), the latter would imply a voluntary and meaningful action (to convey humor to an accomplice, for example). Observers will read events seen as purely physical or within a frame of "nature" differently from those seen as occurring with social frames. But we do not look at an event and then "apply" a frame to it. Rather, individuals constantly project into the world around them the interpretive frames that allow them to make sense of it; we only shift frames (or realize that we have habitually applied a frame) when incongruity calls for a frame-shift. In other words, we only become aware of the frames that we always already use when something forces us to replace one frame with another.<ref>
This example borrowed from Clifford Geertz: ''Local Knowledge: Further Essays in Interpretive Anthropology'' (1983), Basic Books 2000 paperback: ISBN 0-465-04162-0
</ref><ref>
Goffman offers the example of the woman bidding on a mirror at an auction who first examines the frame and surface for imperfections, and then "checks" herself in the mirror and adjusts her hat. See Goffman, Erving. ''Frame Analysis: An essay on the organization of experience''. Boston: Northeastern University Press, 1986. ISBN 0-930350-91-X, page 39. In each case the mirror represents more than simply a physical object.
</ref>

Framing is so effective because it is a heuristic, or mental shortcut that may not always yield desired results; and is seen as a 'rule of thumb'. According to Susan T. Fiske and Shelley E. Taylor, human beings are by nature "cognitive misers", meaning they prefer to do as little thinking as possible.<ref>Fiske, S. T., & Taylor, S. E. (1991). Social cognition (2nd ed.). New York: McGraw-Hill</ref> Frames provide people a quick and easy way to process information. Hence, people will use the previously mentioned mental filters (a series of which is called a schema) to make sense of incoming messages. This gives the sender and framer of the information enormous power to use these schemas to influence how the receivers will interpret the message.<ref name="EntmanRobertTree">Entman,Robert "Tree Beard". Framing: Toward Clarification of a Fractured Paradigm. Journal of Communication; Autumn 1993, 43, 4, p.51</ref>

Though some consider framing to be synonymous with [[Agenda-setting theory|agenda setting]], other scholars state that there is a distinction. According to an article written by Donald H. Weaver, framing selects certain aspects of an issue and makes them more prominent in order to elicit certain interpretations and evaluations of the issue, whereas agenda setting introduces the issue topic to increase its salience and accessibility.<ref>{{Cite journal|last=Weaver|first=David H.|title=Thoughts on Agenda Setting, Framing, and Priming|journal=Journal of Communication|volume=57}}</ref>

==Framing effect in communication research==
In the field of communication, framing defines how news media coverage shapes [[mass opinion]]. [[Richard Vatz|Richard E. Vatz's]] discourse on creation of rhetorical meaning relates directly to framing, although he references it little.  To be specific, framing effects refer to behavioral or attitudinal strategies and/or outcomes that are due to how a given piece of information is being framed in [[public discourse]]. Today, many volumes of the major communication journals contain papers on media frames and framing effects.<ref>Scheufele, D. A. & Iyengar, S. (forthcoming). The state of framing research: A call for new directions. In K. kENSKI, & K. H. Jamieson (Eds.), The Oxford Handbook of political communication theories. New York: Oxford University Press.</ref> Approaches used in such papers can be broadly classified into two groups: studies of framing as the dependent variable and studies of framing as the independent variable.<ref>Tewksbury & Scheufele (2009). News framing theory and research, In J. Bryant, & M. B. Oliver (Eds.) Media effects: Advances in theory and research, New York: Routledge.</ref> The former usually deals with ''frame building'' (i.e. how frames create societal discourse about an issue and how different frames are adopted by journalists) and latter concerns ''frame setting'' (i.e. how media framing influences an audience).

===Frame building===
Frame building is related to at least three areas: journalist norms, political actors, and cultural situations. It assumes that several media frames compete to set one frame regarding an issue, and one frame finally gains influence because it resonates with [[popular culture]], fits with media practices, or is heavily sponsored by [[elite]]s. 
First, in terms of practices of news production, there are at least five aspects of news work that may influence how journalists frame a certain issue: larger societal norms and values, organizational pressures and constraints, external pressures from [[interest group]]s and other [[policy maker]]s, professional routines, and ideological or political orientations of journalists. The second potential influence on frame building comes from elites, including interest groups, government bureaucracies, and other political or corporate actors. Empirical studies show that these influences of elites seem to be strongest for issues in which journalists and various players in the policy arena can find shared narratives. Finally, cultural contexts of a society are also able to establish frame. Goffman<ref name="Goffman1974"/> assumes that the meaning of a frame has implicit cultural roots. This context dependency of media frame has been described as 'cultural resonance'<ref>Gamson, W. A. & Modigliani, A. (1987) The changing culture of affirmative action. Research in Political Sociology, 3, 137-177</ref> or 'narrative fidelity'.<ref name="SnowBenford1988">Snow, D. A., & Benford, R. D. (1988). Ideology, frame resonance, and participant mobilization. In B. Klandermans, H. Kriesi, & S. Tarrow (Eds.), International social movement research. Vol 1, From structure on action: Comparing social movement research across cultures (pp. 197-217). Greenwich, CT: JAI Press.</ref>

===Frame setting===
When people are exposed to a novel news frame, they will accept the constructs made applicable to an issue, but they are significantly more likely to do so when they have existing schema for those constructs. This is called the applicability effect. That is, when new frames invite people to apply their existing schema to an issue, the implication of that application depends, in part, on what is in that schema. Therefore, generally, the more the audiences know about issues, the more effective are frames.

There are a number of levels and types of framing effects that have been examined. For example, scholars have focused on attitudinal and behavioral changes, the degrees of perceived importance of the issue, voting decisions, and opinion formations. Others are interested in psychological processes other than applicability. For instance, Iyengar<ref>Iyengar, S. (1991). Is anyone responsible? How television frames political issues. Chicago: University of Chicago Press.</ref> suggested that news about social problems can influence attributions of causal and treatment responsibility, an effect observed in both cognitive responses and evaluations of political leaders, or other scholars looked at the framing effects on receivers' evaluative processing style and the complexity of audience members' thoughts about issues.

==In mass communication research==
News media frame all news items by emphasizing specific values, facts, and other considerations, and endowing them with greater apparent applicability for making related judgments.  News media promotes particular definitions, interpretations, evaluations and recommendations.<ref name=Entman1993>{{cite journal|last=Entman|first=R.M.|title=Framing: Toward clarification of a fractured paradigm|journal=Journal of Communication|year=1993|volume=43|issue=4|pages=51–58|doi=10.1111/j.1460-2466.1993.tb01304.x}}</ref><ref name=NelsonClawsonOxley1997>{{cite journal|last=Nelson|first=T.E.|author2=Clawson, R.A. |author3=Oxley, Z.M. |title=Media framing of a civil liberties conflict and its effect on tolerance|journal=American Political Science Review|year=1997|volume=91|issue=3|pages=567–583|doi=10.2307/2952075}}</ref>

===Foundations in mass communication research===

Anthropologist [[Gregory Bateson]] first articulated the concept of framing in his 1972 book ''[[Steps to an Ecology of Mind]]''.  A frame, Bateson wrote, is "a spatial and temporal bounding of a set of interactive messages."<ref name=Bateson1972>{{cite book|last=Bateson|first=G.|title=Steps to an Ecology of Mind|year=1972|publisher=Ballantine Books|location=New York}}</ref>

====Sociological roots of media framing research====

Media framing research has both sociological and psychological roots.  Sociological framing focuses on "the words, images, phrases, and presentation styles" that communicators use when relaying information to recipients.<ref name="Druckman2001" /> Research on frames in sociologically driven media research generally examines the influence of "social norms and values, organizational pressures and constraints, pressures of interest groups, journalistic routines, and ideological or political orientations of journalists" on the existence of frames in media content.<ref name=Scheufele2000>{{cite journal|last=Scheufele|first=D.A.|title=Agenda-setting, priming, and framing revisited: Another look at cognitive effects of political communication|journal=Mass Communication & Society|year=2000|volume=3|issue=2&3|pages=297–316|doi=10.1207/S15327825MCS0323_07}}</ref>

[[Todd Gitlin]], in his analysis of how the news media trivialized the student [[New Left]] movement during the 1960s, was among the first to examine media frames from a sociological perspective.  Frames, Gitlin wrote, are "persistent patterns of cognition, interpretations, and presentation, of selection [and] emphasis ... [that are] largely unspoken and unacknowledged ... [and] organize the world for both journalists [and] for those of us who read their reports."<ref name=Gitlin1980>{{cite book|last=Gitlin|first=T.|title=The Whole World is Watching: Mass Media in the Making and Unmaking of the New Left|year=1980|publisher=University of California Press|location=Berkeley, CA}}</ref>

====Psychological roots of media framing research====

Research on frames in psychologically driven media research generally examines the effects of media frames on those who receive them.  For example, Iyengar explored the impact of episodic and thematic news frames on viewers' attributions of responsibility for political issues including crime, terrorism, poverty, unemployment, and racial inequality.<ref name=Iyengar1991>{{cite book|last=Iyengar|first=S.|title=Is Anyone Responsible? How Television Frames Political Issues|year=1991|publisher=University of Chicago Press|location=Chicago}}</ref> According to Iyengar, an episodic news frame "takes the form of a case study or event-oriented report and depicts public issues in terms of concrete instances," while a thematic news frame "places public issues in some more general abstract context ... directed at general outcomes or conditions."<ref name=Entman1993 /><ref name=Iyengar1991 /> Iyengar found that the majority of television news coverage of poverty, for example, was episodic.<ref name=Iyengar1991 />  In fact, in a content analysis of six years of television news, Iyengar found that the typical news viewer would have been twice as likely to encounter episodic rather than thematic television news about poverty.<ref name=Iyengar1991 />  Further, experimental results indicate participants who watched episodic news coverage of poverty were more than twice as likely as those who watched thematic news coverage of poverty to attribute responsibility of poverty to the poor themselves rather than society.<ref name=Iyengar1991 />  Given the predominance of episodic framing of poverty, Iyengar argues that television news shifts responsibility of poverty from government and society to the poor themselves.<ref name=Iyengar1991 />  After examining content analysis and experimental data on poverty and other political issues, Iyengar concludes that episodic news frames divert citizens' attributions of political responsibility away from society and political elites, making them less likely to support government efforts to address those issue and obscuring the connections between those issues and their elected officials' actions or lack thereof.<ref name=Iyengar1991 />

===Clarifying and distinguishing a "fractured paradigm"===

Perhaps because of their use across the social sciences, frames have been defined and used in many disparate ways.  Entman called framing "a scattered conceptualization" and "a fractured paradigm" that "is often defined casually, with much left to an assumed tacit understanding of the reader."<ref name=Entman1993 /> In an effort to provide more conceptual clarity, Entman suggested that frames "select some aspects of a perceived reality and make them more salient in a communicating text, in such a way as to promote a particular problem definition, causal interpretation, moral evaluation, and/or treatment recommendation for the item described."<ref name=Entman1993 />

Entman's<ref name=Entman1993 />  conceptualization of framing, which suggests frames work by elevating particular pieces of information in salience, is in line with much early research on the psychological underpinnings of framing effects (see also Iyengar,<ref name=Iyengar1991 />  who argues that accessibility is the primary psychological explanation for the existence of framing effects).  Wyer and Srull<ref name=WyerSrull1984 />  explain the construct of accessibility thus:
# People store related pieces of information in "referent bins" in their long-term memory.<ref name=WyerSrull1984>{{cite book|last=Wyer, Jr.|first=R.S.|title=Social Cognition: The Ontario Symposium|year=1984|publisher=Lawrence Erlbaum|location=Hillsdale, NJ|author2=Srull, T.K.|editor=E.T. Higgins |editor2=N.A. Kuiper |editor3=M.P Zanna (Eds.)|chapter=Category Accessibility: Some theoretic and empirical issues concerning the processing of social stimulus information}}</ref> 
# People organize "referent bins" such that more frequently and recently used pieces of information are stored at the top of the bins and are therefore more accessible.<ref name=WyerSrull1984 /> 
# Because people tend to retrieve only a small portion of information from long-term memory when making judgments, they tend to retrieve the most accessible pieces of information to use for making those judgments.<ref name=WyerSrull1984 />

The argument supporting accessibility as the psychological process underlying framing can therefore be summarized thus: Because people rely heavily on news media for public affairs information, the most accessible information about public affairs often comes from the public affairs news they consume.  The argument supporting accessibility as the psychological process underlying framing has also been cited as support in the debate over whether framing should be subsumed by [[agenda-setting theory]] as part of the second level of agenda setting.  McCombs and other agenda-setting scholars generally agree that framing should be incorporated, along with [[Priming (media)|priming]], under the umbrella of agenda setting as a complex model of media effects linking media production, content, and audience effects.<ref name=Kosicki1993>{{cite journal|last=Kosicki|first=G.M.|title=Problems and opportunities in Agenda-setting research|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=100–127|doi=10.1111/j.1460-2466.1993.tb01265.x}}</ref><ref name=McCombsShaw1993>{{cite journal|last=McCombs|first=M.E.|author2=Shaw, D.L.|title=The evolution of agenda-setting research: Twenty-five years in the marketplace of ideas|journal=Journal of Communication|year=1993|volume=43|issue=2|pages=58–67|doi=10.1111/j.1460-2466.1993.tb01262.x}}</ref><ref name=McCombsLlamasLopez-EscobarRey1997 />  Indeed, McCombs, Llamas, Lopez-Escobar, and Rey justified their attempt to combine framing and agenda-setting research on the assumption of parsimony.<ref name=McCombsLlamasLopez-EscobarRey1997>{{cite journal|last=McCombs|first=M.F.|author2=Llamas, J.P. |author3=Lopez-Escobar, E. |author4=Rey, F. |title=Candidate images in Spanish elections: Second-level agenda-setting effects|journal=Journalism & Mass Communication Quarterly|year=1997|volume=74|pages=703–717|doi=10.1177/107769909707400404|issue=4}}</ref>

Scheufele, however, argues that, unlike agenda setting and priming, framing does not rely primarily on accessibility, making it inappropriate to combine framing with agenda setting and priming for the sake of parsimony.<ref name=Scheufele2000 /> Empirical evidence seems to vindicate Scheufele's claim.  For example, Nelson, Clawson, and Oxley empirically demonstrated that applicability, rather than their salience, is key.<ref name="NelsonClawsonOxley1997" /> By operationalizing accessibility as the response latency of respondent answers where more accessible information results in faster response times, Nelson, Clawson, and Oxley demonstrated that accessibility accounted for only a minor proportion of the variance in framing effects while applicability accounted for the major proportion of variance.<ref name="NelsonClawsonOxley1997" /> Therefore, according to Nelson and colleagues, "frames influence opinions by stressing specific values, facts, and other considerations, endowing them with greater apparent relevance to the issue than they might appear to have under an alternative frame."<ref name="NelsonClawsonOxley1997" />

In other words, while early research suggested that by highlighting particular aspects of issues, frames make certain considerations more accessible and therefore more likely to be used in the judgment process,<ref name=Entman1993 /><ref name=Iyengar1991 />  more recent research suggests that frames work by making particular considerations more applicable and therefore more relevant to the judgment process.<ref name="NelsonClawsonOxley1997" /><ref name=Scheufele2000 />

===Equivalency versus emphasis: two types of frames in media research===

Chong and Druckman suggest framing research has mainly focused on two types of frames: equivalency and emphasis frames.<ref name=ChongDruckman2007>{{cite journal|last=Chong|first=D.|author2=Druckman, J.N. |title=Framing theory|journal=Annual Review of Political Science|year=2007|volume=10|pages=103–126|doi=10.1146/annurev.polisci.10.072805.103054}}</ref>  Equivalency frames offer "different, but logically equivalent phrases," which cause individuals to alter their preferences.<ref name="Druckman2001" /> Equivalency frames are often worded in terms of "gains" versus "losses."  For example, Kahneman and Tversky asked participants to choose between two "gain-framed" policy responses to a hypothetical disease outbreak expected to kill 600 people.<ref name=KahnemanTversky1984>{{cite journal |last=Kahneman |first=D.|author2=Tversky, A.|title=Choices, values, and frames|journal=American Psychologist |year=1984|volume=39|issue=4|pages=341–350 |doi=10.1037/0003-066X.39.4.341}}</ref>  Response A would save 200 people while Response B had a one-third probability of saving everyone, but a two-thirds probability of saving no one.  Participants overwhelmingly chose Response A, which they perceived as the less risky option. Kahneman and Tversky asked other participants to choose between two equivalent "loss-framed" policy responses to the same disease outbreak.  In this condition, Response A would kill 400 people while Response B had a one-third probability of killing no one but a two-thirds probability of killing everyone.  Although these options are  mathematically identical to those given in the "gain-framed" condition, participants overwhelmingly chose Response B, the risky option.  Kahneman and Tversky, then, demonstrated that when phrased in terms of potential gains, people tend to choose what they perceive as the less risky option (i.e., the sure gain).  Conversely, when faced with a potential loss, people tend to choose the riskier option.<ref name=KahnemanTversky1984 />

Unlike equivalency frames, emphasis frames offer "qualitatively different yet potentially relevant considerations" which individuals use to make judgments.<ref name=ChongDruckman2007 /> For example, Nelson, Clawson, and Oxley exposed participants to a news story that presented the [[Ku Klux Klan]]'s plan to hold a rally.<ref name="NelsonClawsonOxley1997" />  Participants in one condition read a news story that framed the issue in terms of public safety concerns while participants in the other condition read a news story that framed the issue in terms of free speech considerations.  Participants exposed to the public safety condition considered public safety applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed lower tolerance of the Klan's right to hold a rally.<ref name="NelsonClawsonOxley1997" />  Participants exposed to the free speech condition, however, considered free speech applicable for deciding whether the Klan should be allowed to hold a rally and, as expected, expressed greater tolerance of the Klan's right to hold a rally.<ref name="NelsonClawsonOxley1997" />

==Framing effect in psychology and economics==
[[File:Daniel KAHNEMAN.jpg|thumb|180px|[[Daniel Kahneman]]]]
{{Main|Framing effect (psychology)}}
[[Amos Tversky]] and [[Daniel Kahneman]] have shown that framing can affect the outcome (i.e. the choices one makes) of choice problems, to the extent that several of the classic axioms of [[rational choice]] do not hold.<ref name="TverskyKahneman1981">{{cite journal | last1 = Tversky | first1 = Amos | last2 = Kahneman | first2 = Daniel | year = 1981 | title = The Framing of Decisions and the Psychology of Choice | url = | journal = Science | volume = 211 | issue = 4481| pages = 453–458 | doi = 10.1126/science.7455683 | pmid = 7455683 }}</ref> This led to the development of [[prospect theory]] as an alternative to rational choice theory.<ref>Econport. "Decision-Making Under Uncertainty - Advanced Topics: An Introduction to Prospect Theory". (EconPort is an economics digital library specializing in content that emphasizes the use of experiments in teaching and research.) [http://www.econport.org/econport/request?page=man_ru_advanced_prospect]</ref>

The context or framing of problems adopted by decision-makers results in part from extrinsic manipulation of the decision-options offered, as well as from forces intrinsic to decision-makers, e.g., their norms, habits, and unique [[temperament]].

===Experimental demonstration===
Tversky and Kahneman (1981) demonstrated systematic [[preference reversal|reversals of preference]] when the same problem is presented in different ways, for example in the Asian disease problem. Participants were asked to "imagine that the U.S. is preparing for the outbreak of an unusual Asian disease, which is expected to kill 600 people. Two alternative programs to combat the disease have been proposed. Assume the exact scientific estimate of the consequences of the programs are as follows."

The first group of participants was presented with a choice between programs:
In a group of 600 people,
* Program A: "200 people will be saved"
* Program B: "there is a 1/3 probability that 600 people will be saved, and a 2/3 probability that no people will be saved"

72 percent of participants preferred program A (the remainder, 28%, opting for program B).

The second group of participants was presented with the choice between the following:
In a group of 600 people,
* Program C: "400 people will die"
* Program D: "there is a 1/3 probability that nobody will die, and a 2/3 probability that 600 people will die"

In this decision frame, 78% preferred program D, with the remaining 22% opting for program C.

Programs A and C are identical, as are programs B and D. The change in the decision frame between the two groups of participants produced a preference reversal: when the programs were presented in terms of lives saved, the participants preferred the secure program, A (= C). When the programs were presented in terms of expected deaths, participants chose the gamble D (= B).<ref>{{Cite journal
| last = Entman
| first = R. M.
| year = 1993
| contribution = Framing: Toward Clarification of a Fractured Paradigm
| periodical = [[Journal of Communication]]
| volume = 43
| issue = 4
| pages = 51–58 [pp. 53–54] |doi=10.1111/j.1460-2466.1993.tb01304.x
| postscript = <!--None-->
}}</ref>

===Absolute and relative influences===
Framing effects arise because one can frequently frame a decision using multiple [[scenario]]s, wherein one may express benefits either as a relative risk reduction (RRR), or as absolute risk reduction (ARR). Extrinsic control over the cognitive distinctions (between [[risk tolerance]] and [[Incentive|reward anticipation]]) adopted by decision makers can occur through altering the presentation of [[relative risk]]s and [[Three degrees of comparison|absolute]] benefits.

People generally prefer the absolute certainty inherent in a positive framing-effect, which offers an assurance of gains. When decision-options appear framed as a ''likely gain'', risk-averse choices predominate.

A shift toward risk-seeking behavior occurs when a decision-maker frames decisions in negative terms, or adopts a negative framing effect.

In [[Decision-making|medical decision making]], [[framing bias]] is best avoided by using absolute measures of efficacy.<ref name="pmid21792695">{{cite journal|vauthors=Perneger TV, Agoritsas T | title=Doctors and Patients' Susceptibility to Framing Bias: A Randomized Trial | journal=J Gen Intern Med | year= 2011 | volume= 26| issue= 12| pages= 1411–7| pmid=21792695 | doi=10.1007/s11606-011-1810-x | pmc= 3235613| url= }}</ref>

===Frame-manipulation research===
Researchers have found<ref name="TverskyKahneman1981" /> that framing decision-problems in a positive light generally results in less-risky choices; with negative framing of problems, riskier choices tend to result. According to [[behavioral economics|behavioral economist]]s{{Citation needed|date=November 2007}}:

*positive framing effects (associated with [[risk aversion]]) result from presentation of options as sure (or absolute) gains
*negative framing effects (associated with a preference shift toward choosing riskier options) result from options presented as the relative likelihood of losses

Researchers have found{{Citation needed|date=October 2007}} that framing-manipulation invariably affects subjects, but to varying degrees. Individuals proved risk averse when presented with value-increasing options; but when faced with value decreasing contingencies, they tended towards increased risk-taking. Researchers {{Who|date=September 2008}} found that variations in decision-framing achieved by manipulating the options to represent either a gain or as a loss altered the risk-aversion preferences of decision-makers.

In one study, 57% of the subjects chose a medication when presented with benefits in relative terms, whereas only 14.7% chose a medication whose benefit appeared in absolute terms. Further questioning of the patients suggested that, because the subjects ignored the underlying risk of disease, they perceived benefits as greater when expressed in relative terms.<ref>[http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&db=PubMed&list_uids=8271086&dopt=Abstract The framing effect of relative and absolute risk. [J Gen Intern Med. 1993&#93; - PubMed Result<!-- Bot generated title -->]</ref>

===Theoretical models===
Researchers have proposed<ref>Chong, D. and Druckman, J. N. (2007): Framing Theory, Annual Review of Political Science, vol. 10</ref><ref>Price, V., Tewksburg, D. and Powers, E. (1997): Switching Trains of Thought: The Impact of News Frames on Readers' Cognitive Responses, Communication Research, Vol. 24 No. 5 s. 481 - 506</ref> various models explaining the '''framing effect''':

*cognitive theories, such as the [[fuzzy-trace theory]], attempt to explain the framing-effect by determining the amount of cognitive processing effort devoted to determining the value of potential gains and losses.
*[[prospect theory]] explains the framing-effect in functional terms, determined by preferences for differing perceived values, based on the assumption that people give a greater weighting to losses than to equivalent gains.
*[[motivation]]al theories explain the framing-effect in terms of [[hedonic]] forces affecting individuals, such as fears and wishes—based on the notion that negative emotions evoked by potential losses usually out-weigh the emotions evoked by hypothetical gains.
*cognitive [[Cost-benefit analysis|cost-benefit]] trade-off theory defines choice as a compromise between desires, either as a preference for a correct decision or a preference for minimized cognitive effort. This model, which dovetails elements of cognitive and motivational theories, postulates that calculating the value of a sure gain takes much less cognitive effort than that required to select a risky gain.

===Neuroimaging===
Cognitive [[neuroscientist]]s have linked the framing-effect to neural activity in the [[amygdala]], and have identified another brain-region, the orbital and medial [[prefrontal cortex]] (OMPFC), that appears to moderate the role of [[emotion]] on decisions. Using [[functional magnetic resonance imaging]] (fMRI) to monitor brain-activity during a financial decision-making task, they observed greater activity in the OMPFC of those research subjects less susceptible to the framing-effect.<ref>{{cite journal | last1 = De Martino | first1 = B. | last2 = Kumaran | first2 = D. | last3 = Seymour | first3 = B. | last4 = Dolan | first4 = R. J. | year = 2006 | title = Frames, biases, and rational decision-making in the human brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684–687 | doi = 10.1126/science.1128356 | pmid = 16888142 | pmc = 2631940 }}</ref>

==Framing theory and frame analysis in sociology==
Framing theory and frame analysis provide a broad theoretical approach that analysts have used in [[communication studies]], [[journalism|news]] (Johnson-Cartee, 1995), politics, and [[social movement]]s (among other applications).

According to some sociologists, the "social construction of collective action frames" involves "public discourse, that is, the interface of media discourse and interpersonal interaction; persuasive communication during mobilization campaigns by movement organizations, their opponents and countermovement organizations; and consciousness raising during episodes of collective action."<ref>
Bert Klandermans. 1997. ''The Social Psychology of Protest''. Oxford: Blackwell, page 45
</ref>

===History===
[[diction|Word-selection]] or diction has been a component of [[rhetoric]] since time immemorial. But most commentators attribute the concept of framing to the work of [[Erving Goffman]] on [[frame analysis]] and point especially to his 1974 book, ''Frame analysis: An essay on the organization of experience''. Goffman used the idea of frames to label "schemata of interpretation" that allow individuals or groups "to locate, perceive, identify, and label" events and occurrences, thus rendering meaning, organizing experiences, and guiding actions.<ref>
Erving Goffman (1974). ''Frame Analysis: An essay on the organization of experience''. Cambridge: Harvard University Press, 1974, page 21.
</ref>
Goffman's framing concept evolved out of his 1959 work, ''[[The Presentation of Self in Everyday Life]]'', a commentary on the [[management]] of [[Impression management|impression]]s. These works arguably depend on [[Kenneth Boulding]]'s concept of image.<ref>
Kenneth Boulding: ''The Image: Knowledge in Life and Society'', University of Michigan Press, 1956)
</ref>

===Social movements===
Sociologists have utilized framing to explain the process of [[social movement]]s.<ref name="SnowBenford1988" />
Movements act as carriers of beliefs and ideologies (compare [[meme]]s). In addition, they operate as part of the process of constructing meaning for participants and opposers (Snow & Benford, 1988). Sociologists deem the mobilization of mass-movements "successful" when the frames projected align with the frames of participants to produce resonance between the two parties. Researchers of framing speak of this process as ''frame re-alignment''.

===Frame-alignment===
Snow and Benford (1988) regard frame-alignment as an important element in social mobilization or movement. They argue that when individual frames become linked in congruency and complementariness, "frame alignment" occurs,<ref name="Snowetal1986">
Snow, D. A., Rochford, E. B., Worden, S. K., & Benford, R. D. (1986). Frame alignment processes, micromobilization, and movement participation. American Sociological Review, 51, page 464
</ref>
producing "frame resonance", a catalyst in the process of a group making the transition from one frame to another (although not all framing efforts prove successful). The conditions that affect or constrain framing efforts include the following:

*"The robustness, completeness, and thoroughness of the framing effort". Snow and Benford (1988) identify three core framing-tasks, and state that the degree to which framers attend to these tasks will determine participant mobilization. They characterize the three tasks as the following:
*#diagnostic framing for the identification of a problem and assignment of blame
*#prognostic framing to suggest solutions, strategies, and tactics to a problem
*#motivational framing that serves as a call to arms or rationale for action
*The relationship between the proposed frame and the larger [[belief system|belief-system]]; centrality: the frame cannot be of low hierarchical significance and salience within the larger belief system. Its range and interrelatedness, if the framer links the frame to only one core belief or value that, in itself, has a limited range within the larger belief system, the frame has a high degree of being discounted.
*Relevance of the frame to the realities of the participants; a frame must seem relevant to participants and must also inform them. Empirical credibility or testability can constrain relevancy: it relates to participant experience, and has narrative fidelity, meaning that it fits in with existing cultural myths and narrations.
*[[Protest cycle|Cycles of protest]] (Tarrow 1983a; 1983b); the point at which the frame emerges on the timeline of the current era and existing preoccupations with social change. Previous frames may affect efforts to impose a new frame.

Snow and Benford (1988) propose that once someone has constructed proper frames as described above, large-scale changes in society such as those necessary for social movement can be achieved through frame-alignment.

====Types====
Frame-alignment comes in four forms: frame bridging, frame amplification, frame extension and frame transformation.

#''Frame bridging'' involves the "linkage of two or more ideologically congruent but structurally unconnected frames regarding a particular issue or problem" (Snow et al., 1986, p.&nbsp;467). It involves the linkage of a movement to "unmobilized [''[[sic]]''] sentiment pools or public opinion preference clusters" (p.&nbsp;467) of people who share similar views or grievances but who lack an organizational base.
#''Frame amplification'' refers to "the clarification and invigoration of an interpretive frame that bears on a particular issue, problem, or set of events" (Snow et al., 1986, p.&nbsp;469). This interpretive frame usually involves the invigorating of values or beliefs.
#''Frame extensions'' represent a movement's effort to incorporate participants by extending the boundaries of the proposed frame to include or encompass the views, interests, or sentiments of targeted groups (Snow et al., 1986, p.&nbsp;472).
#''Frame transformation'' becomes necessary when the proposed frames "may not resonate with, and on occasion may even appear antithetical to, conventional lifestyles or rituals and extant interpretive frames" (Snow et al., 1986, p.&nbsp;473).

When this happens, the securing of participants and support requires new values, new meanings and understandings. Goffman (1974, p.&nbsp;43–44) calls this "keying", where "activities, events, and biographies that are already meaningful from the standpoint of some primary framework, in terms of another framework" (Snow et al., 1986, p.&nbsp;474) such that they are seen differently. Two types of frame transformation exist:

#Domain-specific transformations, such as the attempt to alter the status of groups of people, and
#Global interpretive frame-transformation, where the scope of change seems quite radical—as in a change of [[world view|world-views]], total conversions of thought, or uprooting of everything familiar (for example: moving from [[communism]] to [[market capitalism]], or vice versa; [[religious conversion]], etc.).

==Frame analysis as rhetorical criticism==
Although the idea of language-framing had been explored earlier by [[Kenneth Burke]] (terministic screens), political communication researcher [[Jim A. Kuypers]] first published work advancing [[frame analysis]] (framing analysis) as a rhetorical perspective in 1997. His approach begins inductively by looking for themes that persist across time in a text (for Kuypers, primarily news narratives on an issue or event) and then determining how those themes are framed. Kuypers's work begins with the assumption that frames are powerful rhetorical entities that "induce us to filter our perceptions of the world in particular ways, essentially making some aspects of our multi-dimensional reality more noticeable than other aspects. They operate by making some information more salient than other information...."<ref>
Jim A. Kuypers, "Framing Analysis" in ''Rhetorical Criticism: Perspectives in Action'', edited by J.A. Kuypers,  Lexington Press, 2009. p. 181.</ref>

In his 2009 essay "Framing Analysis" in ''Rhetorical Criticism: Perspectives in Action''<ref>
''Rhetorical Criticism: Perspectives in Action''</ref> and his 2010 essay "Framing Analysis as a Rhetorical Process",<ref>Kuypers, Jim A. "Framing Analysis as a Rhetorical Process," Doing News Framing Analysis.  Paul D'Angelo and  Jim A. Kuypers, eds. (New York: Routeledge, 2010).</ref> Kuypers offers a detailed conception for doing framing analysis from a rhetorical perspective. According to Kuypers, "Framing is a process whereby communicators, consciously or unconsciously, act to construct a point of view that encourages the facts of a given situation to be interpreted by others in a particular manner. Frames operate in four key ways: they define problems, diagnose causes, make moral judgments, and suggest remedies. Frames are often found within a narrative account of an issue or event, and are generally the central organizing idea."<ref>Jim A. Kuypers, ''Bush's War: Media Bias and Justifications for War in a Terrorist Age'', Rowman & Littlefield Publishers, Inc., 2009.</ref> Kuypers's work is based on the premise that framing is a rhetorical process and as such it is best examined from a rhetorical point of view. Curing the problem is not rhetorical and best left to the observer.

==Rhetorical framing in politics==

===Semiotic analysis of 2016 Republican primaries===

Framing is used to construct, refine, and deliver messages. Framing in politics is essential to getting your message across to the masses. Frames are mental structures that shape the way we view the world (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004).<ref name=Lakoff>In Don't Think of an Elephant! Know Your Values and Frame the Debate, by George Lakoff, 144. Chelsea Green Publishing, 2004.</ref> Reframing is used particularly well by both conservatives and liberals in the political arena, so well that they have news anchors and commentators discussing the ideas, supplied phrases and framing (Lakoff, Don't Think of an Elephant! Know Your Values and Frame the Debate 2004).<ref name=Lakoff/> 

The neoconservatives in the Bush Administration and the Pentagon viewed the 9/11 attack as an opportunity to go to war in the Middle East and finally take out Saddam Hussain. The Bush administration sold the war by convincing the nation that Iraq had WMDs and collected supportive evidence that they had Secretary of State Colin Powell present at the United Nations. The War on Terror was the label assigned by the Bush administration to its national security policy, launched in response to the attacks of 9/11 (Lewis 2009).<ref name="Lewis 2009">Lewis, Stephen D. Reese and Seth C. "Framing the War on Terror The internalization of policy in the US press." Journalism, 2009: 777–797.</ref> The cultural construction and political rationale supporting this slogan represent a powerful organizing principle that has become a widely accepted framing, laying the groundwork for the invasion of Iraq (Lewis 2009).<ref name="Lewis 2009" /> 

The challenge of political violence has grown with new means of global coordination and access to weapons of mass destruction. The Bush administration's response to this threat, following the now iconic policy reference point of 11 September 2001, has had far-ranging implications for national security strategy, relations with the world community, and civil liberties (Lewis 2009).<ref name="Lewis 2009" /> Labeled the 'War on Terror', the policy was framed within a phrase now part of the popular lexicon, becoming a natural and instinctive shorthand. More than phrases though, frames are 'organizing principles that are socially shared and persistent over time, that work symbolically to meaningfully structure the social world' (Lewis 2009).<ref name="Lewis 2009" /> As a particularly powerful organizing principle, the War on Terror created a supportive political climate for what has been called the biggest US foreign policy blunder in modern times: the invasion of Iraq. Thus, in the scope and consequences of its policy-shaping impact, the War on Terror may be the most important frame in recent memory. (Lewis 2009)

In the now well-known evolution of the administration's policy, influential neoconservatives within the administration had advocated regime change in Iraq for some time, but the events of 9/11 gave them a compelling way to fast-track their ideas and justify a new policy of preemptive war, fist in Afghanistan and then in Iraq. The National Strategy for Combating Terrorism defined the attacks of 9/11 as 'acts of war against the United States of America and its allies, and against the very idea of civilized society'. It identified the enemy as terrorism, an 'evil' threatening our 'freedoms and our way of life. The related National Security Strategy of the United States of America clearly divides 'us' from 'them', linking terrorism to rogue states that 'hate the United States and everything for which it stands (Lewis 2009).<ref name="Lewis 2009" /> Presenting himself as God's agent, Bush's Manichean struggle pitted the USA and its leader against the evildoers (Lewis 2009).<ref name="Lewis 2009" /> 

This argument is being played out in the 2016 Republican primaries, especially by Donald Trump. Trump has portrayed the Syrian refugees as foot soldiers for ISIS, coming to America to kill us in our main streets. Trump's rhetoric appears to be working; many middle class Americans are consuming his rhetoric.{{citation needed|date=June 2016}} The Americans that are supporting Trump and the Republicans in general, many of them are working class and the Republican agenda although it appears to be in their favor it is not. Framing their message to say one thing and mean something completely different is what the conservatives have become masters at. The 2016 Republican primary has been a knock down fight since it started in August 2015. Donald Trump has approached this contest as if Vince McMahon were the promoter and the rest of the field are a bunch of jobbers (persons who are paid to lose). Trump was inducted into the World Wrestling Entertainment (WWE) Hall of Fame in 2003. Even his attacks on Megan Kelly from FOX News are straight out of the WWE's playbook. Roland Barthes analyzed wrestling and boxing in his book ''Mythologies''.
<blockquote>''This public knows very well the distinction between wrestling and boxing; it knows that boxing is a Jansenist sport, based on a demonstration of excellence. One can bet on the outcome of a boxing-match: with wrestling, it would make no sense. A boxing- match is a story which is constructed before the eyes of the spectator; in wrestling, on the contrary, it is each moment which is intelligible, not the passage of time... The logical conclusion of the contest does not interest the wrestling-fan, while on the contrary a boxing-match always implies a science of the future. In other words, wrestling is a sum of spectacles, of which no single one is a function: each moment imposes the total knowledge of a passion which rises erect and alone, without ever extending to the crowning moment of a result.'' (Legum 2015)<ref>{{cite web|author=Legum, Judd|title=This French Philosopher Is The Only One Who Can Explain The Donald Trump Phenomenon|work=thinkprogress.org|date=September 14, 2015|url=http://thinkprogress.org/politics/2015/09/14/3701084/donald-trump/|accessdate=April 23, 2016}}</ref></blockquote>

==Applications==

===Finance===
Preference reversals and other associated phenomena are of wider relevance within behavioural economics, as they contradict the predictions of [[rational choice]], the basis of traditional economics. Framing biases affecting investing, lending, borrowing decisions make one of the themes of [[behavioral finance]].

===Law===
[[Edward Zelinsky]] has shown that framing effects can explain some observed behaviors of legislators.<ref>[[Edward Zelinsky|Zelinsky, Edward A.]]. 2005. Do Tax Expenditures Create Framing Effects? Volunteer Firefighters, Property Tax Exemptions, and the Paradox of Tax Expenditure Analysis. ''Virginia Tax Review'' 24. [http://www.allbusiness.com/accounting/3584666-1.html]</ref>

===Media===
The role framing plays in the effects of media presentation has been widely discussed, with the central notion that associated perceptions of factual information can vary based upon the presentation of the information.

====News media examples====
In ''Bush's War: Media Bias and Justifications for War in a Terrorist Age,''<ref>Jim A. Kuypers, ''Bush's War: Media Bias and Justifications for War in a Terrorist Age'' (Lanham, MD: Rowman and Littlefield, 2006),</ref>[[Jim A. Kuypers]] examined the differences in framing of the war on terror between the Bush administration and the U.S. Mainstream News between 2001 and 2005.  Kuypers looked for common themes between presidential speeches and press reporting of those speeches, and then determined how the president and the press had framed those themes.  By using a rhetorical version of framing analysis, Kuypers determined that the U.S. news media advanced frames counter to those used by the Bush administration:

{{quote| the press actively contested the framing of the War on Terror as early as eight weeks following 9/11. This finding stands apart from a collection of communication literature suggesting the press supported the President or was insufficiently critical of the President's efforts after 9/11. To the contrary, when taking into consideration how themes are framed, [Kuypers] found that the news media framed its response in such a way that it could be viewed as supporting the idea of some action against terrorism, while concommitantly opposing the initiatives of the President.  The news media may well relay what the president says, but it does not necessarily follow that it is framed in the same manner; thus, an echo of the theme, but not of the frame.  The present study demonstrates, as seen in Table One [below], that shortly after 9/11 the news media was beginning to actively counter the Bush administration and beginning to leave out information important to understanding the Bush Administration's conception of the War on Terror.  In sum, eight weeks after 9/11, the news media was moving beyond reporting political opposition to the President—a very necessary and invaluable press function—and was instead actively choosing themes, and framing those themes, in such a way that the President's focus was opposed, misrepresented, or ignored.<ref>Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, The American Press, and the Initial Framing of the War on Terror after 9/11," ''The George W. Bush Presidency: A Rhetorical Perspective,'' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 89-112.</ref>}}

Table One: Comparison of President and News Media Themes and Frames 8 Weeks after 9/11<ref>Jim A. Kuypers, Stephen D. Cooper, Matthew T. Althouse, "George W. Bush, "The American Press, and the Initial Framing of the War on Terror after 9/11," ''The George W. Bush Presidency: A Rhetorical Perspective,'' Robert E. Denton, ed. (Lanham, MD: Lexington Books, 2012), 105.</ref>

{| class="wikitable"
|-
! Themes !! President's Frame !! Press Frame
|-
| Good v. Evil || Struggle of good and evil || Not mentioned
|-
| Civilization v. Barbarism || Struggle of civilization v. barbarism || Not mentioned
|-
| Nature of Enemy ||Evil, implacable, murderers || Deadly, indiscriminant

Bush Administration
|-
| Nature of War || Domestic/global/enduring

War
 || Domestic/global/longstanding

War or police action
|-
| Similarity to Prior Wars || Different Kind of War || WWII or Vietnam?
|-
| Patience || Not mentioned || Some, but running out
|-
| International Effort || Stated || Minimally reported
|-

|}

In 1991 Robert M. Entman published findings<ref>Entman, R. M. (1991), Symposium Framing U.S. Coverage of International News: Contrasts in Narratives of the KAL and Iran Air Incidents. Journal of Communication, 41: 6–27. {{DOI|10.1111/j.1460-2466.1991.tb02328.x}}</ref> surrounding the differences in media coverage between [[Korean Air Lines Flight 007]] and [[Iran Air Flight 655]]. After evaluating various levels of media coverage, based on both amount of airtime and pages devoted to similar events, Entman concluded that the frames the events were presented in by the media were drastically different:

{{quote| By de-emphasizing the agency and the victims and by the choice of graphics and adjectives, the news stories about the U.S. downing of an Iranian plane called it a technical problem, while the Soviet downing of a Korean jet was portrayed as a moral outrage… [T]he contrasting news frames employed by several important U.S. media outlets in covering these two tragic misapplications of military force. For the first, the frame emphasized the moral bankruptcy and guilt of the perpetrating nation, for the second, the frame de-emphasized the guilt and focused on the complex problems of operating military high technology. }}

Differences in coverage amongst various media outlets:

{| class="wikitable"
|-
! Amounts of Media coverage dedicated to each event !! Korean Air !! Iran Air
|-
| Time Magazine and Newsweek || 51 pages || 20 pages
|-
| CBS || 303 minutes || 204 minutes
|-
| New York Times || 286 stories || 102 stories
|}

In 1988 Irwin Levin and Gary Gaeth did a study on the effects of framing attribute information on consumers before and after consuming a product (1988). In this study they found that in a study on beef. People who ate beef labeled as 75% lean rated it more favorably than people whose beef was labelled 25% fat.

===Politics===

Linguist and rhetoric scholar [[George Lakoff]] argues that, in order to persuade a political audience of one side of and argument or another, the facts must be presented through a rhetorical frame.  It is argued that, without the frame, the facts of an argument become lost on an audience, making the argument less effective.  The rhetoric of politics uses framing to present the facts surrounding an issue in a way that creates the appearance of a problem at hand that requires a solution.  Politicians using framing to make their own solution to an exigence appear to be the most appropriate compared to that of the opposition.<ref name="van der Pas" />  Counter-arguments become less effective in persuading an audience once one side has framed an argument, because it is argued that the opposition then has the additional burden of arguing the frame of the issue in addition to the issue itself.

Framing a political issue, a political party or a political opponent is a [[strategy|strategic]] goal in [[politics]], particularly in the [[United States of America]]. Both the [[Democratic Party (United States)|Democratic]] and [[Republican Party (United States)|Republican]] political parties compete to successfully harness its power of persuasion. According to the ''[[New York Times]]'':

{{quote|Even before the [[United States presidential election, 2004|election]], a new political word had begun to take hold of the party, beginning on the [[West Coast of the United States|West Coast]] and spreading like a virus all the way to the inner offices of the [[United States Capitol|Capitol]]. That word was 'framing.' Exactly what it means to 'frame' issues seems to depend on which Democrat you are talking to, but everyone agrees that it has to do with choosing the language to define a debate and, more important, with fitting individual issues into the contexts of broader story lines.|<ref name="framingwars">
[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html?pagewanted=1&ei=5070&en=e3e686efd4fa97c5&ex=1183608000 The Framing Wars. ''[[New York Times]]'' 17 July 2005]</ref>}}

Because framing has the ability to alter the public's perception, politicians engage in battles to determine how issues are framed. Hence, the way the issues are framed in the media reflects who is winning the battle. For instance, according to Robert Entman, professor of Communication at George Washington University, in the build-up to the Gulf War the conservatives were successful in making the debate whether to attack sooner or later, with no mention of the possibility of not attacking. Since the media picked up on this and also framed the debate in this fashion, the conservatives won.<ref name="EntmanRobertTree" />

One particular example of [[George Lakoff|Lakoff's]] work that attained some degree of fame was his advice to rename<ref>[[Walter Olson]], [http://www.overlawyered.com/2005/07/some_framing_advice.html Overlawyered weblog], 2005-07-18</ref> [[trial lawyer]]s (unpopular in the United States) as "public protection attorneys". Though Americans have not generally adopted this suggestion, the [[Association of Trial Lawyers of America]] did rename themselves the "American Association of Justice", in what the [[Chamber of Commerce]] called an effort to hide their identity.<ref>[[Al Kamen]], [http://www.washingtonpost.com/wp-dyn/content/article/2007/01/16/AR2007011601429_pf.html "Forget Cash -- Lobbyists Should Set Support for Lawmakers in Stone"], ''[[Washington Post]]'', 2007-01-17</ref>

The ''[[New York Times]]'' depicted similar intensity among Republicans:

{{quote|In one recent memo, titled 'The 14 Words Never to Use,' [[Frank Luntz|[Frank] Luntz]] urged conservatives to restrict themselves to phrases from what he calls ... the 'New American Lexicon.' Thus, a smart Republican, in Luntz's view, never advocates '[[oil drilling|drilling for oil]]'; he prefers 'exploring for energy.' He should never criticize the 'government,' which cleans our streets and pays our firemen; he should attack '[[Washington, D.C.|Washington]],' with its ceaseless thirst for taxes and regulations. 'We should never use the word [[outsourcing]],' Luntz wrote, 'because we will then be asked to defend or end the practice of allowing companies to ship American jobs overseas.'|<ref name="framingwars"/>}}

From a political perspective, framing has widespread consequences. For example, the concept of framing links with that of [[agenda setting theory|agenda-setting]]: by consistently invoking a particular frame, the framing party may effectively control discussion and perception of the issue. [[Sheldon Rampton]] and [[John Stauber]] in ''[[Trust Us, We're Experts]]'' illustrate how [[Public Relations|public-relations]] (PR) firms often use language to help frame a given issue, structuring the questions that then subsequently emerge. For example, one firm advises clients to use "bridging language" that uses a strategy of answering questions with specific terms or ideas in order to shift the discourse from an uncomfortable topic to a more comfortable one.<ref>
Rampton, Sheldon and Stauber, John. ''Trust Us, We're Experts!'' Putnam Publishing, New York, NY, 2002. Page 64.</ref>
Practitioners of this strategy might attempt to draw attention away from one frame in order to focus on another. As Lakoff notes, "On the day that [[George W. Bush]] took office, the words "tax relief" started coming out of the White House."<ref name="Lakoff2004">{{Cite book|last=Lakoff|first=George|title=Don't think of an elephant!: know your values and frame the debate|year=2004|publisher=Chelsea Green Publishing|isbn=978-1-931498-71-5|page=56}}</ref>
By refocusing the structure away from one frame ("tax burden" or "tax responsibilities"), individuals can set the agenda of the questions asked in the future.

[[Cognitive linguistics|Cognitive linguists]] point to an example of framing in the phrase "[[tax cut|tax relief]]". In this frame, use of the concept "relief" entails a concept of (without mentioning the benefits resulting from) taxes putting strain on the citizen:

{{quote|The current tax code is full of inequities. Many single moms face higher marginal tax rates than the wealthy. Couples frequently face a higher tax burden after they marry. The majority of Americans cannot deduct their charitable donations. Family farms and businesses are sold to pay the death tax. And the owners of the most successful small businesses share nearly half of their income with the government. President Bush's tax cut will greatly reduce these inequities. It is a fair plan that is designed to provide tax relief to everyone who pays income taxes.|<ref>[http://georgewbush-whitehouse.archives.gov/news/reports/taxplan.html The President's Agenda for Tax Relief] retrieved 3 July 2007.</ref>}}

Alternative frames may emphasize the concept of taxes as a source of infrastructural support to businesses:

{{quote|The truth is that the wealthy have received more from America than most Americans—not just wealth but the infrastructure that has allowed them to amass their wealth: banks, the Federal Reserve, the stock market, the Securities and Exchange Commission, the legal system, federally sponsored research, patents, tax supports, the military protection of foreign investments, and much much more. American taxpayers support the infrastructure of wealth accumulation. It is only fair that those who benefit most should pay their fair share.|<ref>[http://www.cognitivepolicyworks.com/resource-center/planning-tools/framing-tutorials/simple-framing/ Cognitive Policy Works/Rockridge Institute: Simple Framing]</ref>}}

Frames can limit debate by setting the vocabulary and [[metaphor]]s through which participants can comprehend and discuss an issue. They form a part not just of political discourse, but of [[cognition]]. In addition to generating new frames, politically oriented framing research aims to increase public awareness of the connection between framing and reasoning.

====Examples====
*The initial response of the [[George W. Bush administration|Bush administration]] to the [[September 11, 2001 attacks|assault of September 11, 2001]] was to frame the acts of [[Counterterrorism|terror]] as [[crime]]. This framing was replaced within hours by a war metaphor, yielding the "[[War on Terrorism|War on Terror]]". The difference between these two framings is in the implied response. Crime connotes bringing criminals to justice, putting them on trial and sentencing them, whereas as [[war]] implies enemy territory, military action and war powers for government.<ref name="Lakoff2004" /><ref>{{Cite journal|last=Zhang|first=Juyan |title=Beyond anti-terrorism: Metaphors as message strategy of post-September-11 U.S. public diplomacy |journal=Public Relations Review|year=2007|volume=33|issue=1|pages=31–39|doi=10.1016/j.pubrev.2006.11.006}}</ref>
*The term "escalation" to describe an increase in American troop-levels in [[Iraq]] in 2007 implied that the United States deliberately increased the scope of conflict in a provocative manner and possibly implies that U.S. strategy entails a long-term military presence in Iraq, whereas [[Iraq War troop surge of 2007|"surge"]] framing implies a powerful but brief, transitory increase in intensity.<ref>[http://www.alternet.org/waroniraq/48059/ "It's Escalation, Stupid." ''Alternet''] retrieved 3 July 2007</ref>
*The "bad apple" frame, as in the proverb "one bad [[apple]] spoils the barrel". This frame implies that removing one underachieving or corrupt official from an [[institution]] will solve a given problem; an opposing frame presents the same problem as systematic or structural to the institution itself—a source of infectious and spreading rot.<ref>[http://www.huffingtonpost.com/bruce-budner/the-rumsfeld-dilemma-dem_b_29550.html "The Rumsfeld Dilemma: Demand an Exit Strategy, Not a Facelift"] by Bruce Budner, in ''The Huffington Post'' 15 September 2006</ref>
*The "[[taxpayers]] money" frame, rather than [[government spending|public or government funds]], which implies that individual taxpayers have a claim or right to set [[government policy]] based upon their payment of tax rather than their status as [[citizen]]s or [[voters]] and that taxpayers have a right to control public funds that are the shared property of all citizens and also privileges individual self-interest above group interest.{{Citation needed|date=April 2009}}
*The "collective property" frame, which implies that property owned by individuals is really owned by a collective in which those individuals are members. This collective can be a territorial one, such as a nation, or an abstract one that does not map to a specific territory.
*Program-names that may describe only the intended effects of a program but may also imply their effectiveness. These include the following:
**"[[Foreign aid]]"<ref>[http://hij.sagepub.com/cgi/content/abstract/12/2/120 "Is It All in a Word? The Effect of Issue Framing on Public Support for U.S. Spending on HIV/AIDS in Developing Countries."] by Sara Bleich. Retrieved 2007-07-03
</ref> (which implies that spending money will aid foreigners, rather than harm them)
**"[[Social security]]" (which implies that the program can be relied on to provide security for a society)
**"[[Stabilisation policy]]" (which implies that a policy will have a stabilizing effect).
* Based on [[opinion polling]] and [[focus group]]s, [[ecoAmerica]], a nonprofit environmental marketing and messaging firm, has advanced the position that [[global warming]] is an ineffective framing due to its identification as a leftist advocacy issue. The organization has suggested to government officials and environmental groups that alternate formulations of the issues would be more effective.<ref>[http://www.nytimes.com/2009/05/02/us/politics/02enviro.html "Seeking to Save the Planet, With a Thesaurus"] article by John M. Broder in ''[[The New York Times]]'' May 1, 2009</ref>
*In her 2009 book ''Frames of War'', [[Judith Butler]] argues that the justification within liberal-democracies for war, and atrocities committed in the course of war, (referring specifically to the current war in Iraq and to [[Abu Ghraib torture and prisoner abuse|Abu Ghraib]] and [[Guantanamo Bay detention camp|Guantanamo Bay]]) entails a framing of the (especially Muslim) 'other' as pre-modern/primitive and ultimately not human in the same way as citizens within the liberal order.<ref>Butler, J. (2009), ''Frames of War'', London: Verso.</ref>

==See also==
{{div col|3}}
*[[Anecdotal value]]
*[[Alternative facts]]
*[[Argumentation theory]]
*[[Bias]]
*[[Choice architecture]]
*[[Code word (figure of speech)]]
*[[Communication theory]]
*[[Connotation]]
*[[Cultural bias]]
*[[Decision making]]
*[[Definition of the situation]]
*[[Demagoguery]]
*[[Domain of discourse]]
*[[Echo chamber (media)]]
*[[Fallacy of many questions]]
*[[Figure of speech]]
*[[Filter bubble]]
*[[Freedom of speech]]
*[[Freedom of the press|Free press]]
*[[Idea networking]]
*[[Language and thought]]
*[[Meme]]
*[[Newspeak]]
<!--* [[Political frame]] - self ref after merge/redirect-->
*[[Power word]]
*[[Overton window]]
*[[Political correctness]]
*[[Rhetorical device]]
*[[Semantics]]
*[[Semantic domain]]
*[[Social heuristics]]
*[[Sophism]]
*[[Spin doctor]]
*[[Stovepiping]]
*''[[Thought Reform (book)]]''
*[[Trope (linguistics)|Trope]]
*[[Unspeak]] (book)
*[[Virtue word]]
{{div col end}}

==References==
{{reflist|colwidth=30em}}
Levin, Irwin P., and Gary J. Gaeth. "How Consumers Are Affected By The Framing Of Attribute Information Before And After Consuming The Product." Journal of Consumer Research 15.3 (1988): 374. Print.

==Further reading==
*[[Bernard Baars|Baars, B]]. ''A cognitive theory of consciousness'', NY: [[Cambridge University Press]] 1988, ISBN 0-521-30133-5.
*[[Kenneth E. Boulding|Boulding, Kenneth E.]] (1956). The Image: Knowledge in Life and Society. Michigan University Press.
* {{cite journal | last1 = Carruthers | first1 = P. | authorlink = Peter Carruthers (philosopher) | year = 2003 | title = On Fodor's Problem | url = | journal = Mind and Language | volume = 18 | issue = 5| pages = 502–523 | doi = 10.1111/1468-0017.00240 }}
*Clark, A. (1997), Being There: Putting Brain, Body, and World Together Again, Cambridge, MA: MIT Press.
*Cutting, Hunter and Makani Themba Nixon (2006). Talking the Walk: A Communications Guide for Racial Justice: AK Press
*[[Daniel Dennett|Dennett, D.]] (1978), Brainstorms, Cambridge, MA: MIT Press.
*Fairhurst, Gail T. and Sarr, Robert A. 1996. ''The Art of Framing: Managing the Language of Leadership.'' USA: Jossey-Bass, Inc.
*Feldman, Jeffrey. (2007), ''Framing the Debate: Famous Presidential Speeches and How Progressives Can Use Them to Control the Conversation (and Win Elections)''. Brooklyn, NY: Ig Publishing.
*[[Jerry Fodor|Fodor, J.A.]] (1983), The Modularity of Mind, Cambridge, MA: MIT Press.
*Fodor, J.A. (1987), "Modules, Frames, Fridgeons, Sleeping Dogs, and the Music of the Spheres", in Pylyshyn (1987).
*Fodor, J.A. (2000), The Mind Doesn't Work That Way, Cambridge, MA: MIT Press.
*Ford, K.M. & Hayes, P.J. (eds.) (1991), Reasoning Agents in a Dynamic World: The Frame Problem, New York: JAI Press.
*[[Erving Goffman|Goffman, Erving]]. 1974. ''Frame Analysis: An Essay on the Organization of Experience.'' London: Harper and Row.
*Goffman, E. (1974). Frame Analysis. Cambridge: Harvard University Press.
*Goffman, E. (1959). Presentation of Self in Everyday Life. New York: Doubleday.
*Goodman, N. (1954), Fact, Fiction, and Forecast, Cambridge, MA: Harvard University Press.
*{{cite journal | last1 = Hanks | first1 = S. | last2 = McDermott | first2 = D. | year = 1987 | title = Nonmonotonic Logic and Temporal Projection | url = | journal = Artificial Intelligence | volume = 33 | issue = 3| pages = 379–412 | doi = 10.1016/0004-3702(87)90043-9 }}
*Haselager, W.F.G. (1997). Cognitive science and folk psychology: the right frame of mind. London: Sage
*{{cite journal | last1 = Haselager | first1 = W.F.G. | last2 = Van Rappard | first2 = J.F.H. | year = 1998 | title = Connectionism, Systematicity, and the Frame Problem | url = | journal = Minds and Machines | volume = 8 | issue = 2| pages = 161–179 | doi = 10.1023/A:1008281603611 }}
*Hayes, P.J. (1991), "Artificial Intelligence Meets David Hume: A Reply to Fetzer", in Ford & Hayes (1991).
*Heal, J. (1996), "Simulation, Theory, and Content", in Theories of Theories of Mind, eds. P. Carruthers & P. Smith, Cambridge: Cambridge University Press, pp.&nbsp;75–89.
*Johnson-Cartee, K. (2005). News narrative and news framing: Constructing political reality. Lanham, MD: Rowman & Littlefield.
*[[Diana Kendall|Kendall, Diana]], ''Sociology In Our Times'', Thomson Wadsworth, 2005, ISBN 0-534-64629-8 [https://books.google.com/books?vid=ISBN0534646298&id=kzU-gtx2VfoC&pg=PA531&lpg=PA531&dq=%22Resource+Mobilization%22&sig=NgTePMtdl2stO7V2FofPqeZuP5I&hl=en Google Print, p.531]
*Klandermans, Bert. 1997. ''The Social Psychology of Protest.'' Oxford: Blackwell.
*[[George Lakoff|Lakoff, G.]] & Johnson, M. (1980), Metaphors We Live By, Chicago: University of Chicago Press.
*Leites, N. & Wolf, C., Jr. (1970). Rebellion and authority. Chicago: Markham Publishing Company.
*{{cite journal | last1 = Martino | first1 = De | year = 2006 | title = Frames, Biases, and Rational Decision-Making in the Human Brain | url = | journal = Science | volume = 313 | issue = 5787| pages = 684–687 | doi = 10.1126/science.1128356 | pmid = 16888142 | last2 = Kumaran | first2 = D | last3 = Seymour | first3 = B | last4 = Dolan | first4 = RJ | pmc = 2631940 }}
*McAdam, D., McCarthy, J., & Zald, M. (1996). Introduction: Opportunities, Mobilizing Structures, and Framing Processes—Toward a Synthetic, Comparative Perspective on Social Movements. In D. McAdam, J. McCarthy & M. Zald (Eds.), Comparative Perspectives on Social Movements; Political Opportunities, Mobilizing Structures, and Cultural Framings (pp.&nbsp;1–20). New York: Cambridge University Press.
*McCarthy, J. (1986), "Applications of Circumscription to Formalizing Common Sense Knowledge", [[Artificial Intelligence (journal)|Artificial Intelligence]], vol. 26(3), pp.&nbsp;89–116.
*McCarthy, J. & Hayes, P.J. (1969), "Some Philosophical Problems from the Standpoint of Artificial Intelligence", in Machine Intelligence 4, ed. D.Michie and B.Meltzer, Edinburgh: Edinburgh University Press, pp.&nbsp;463–502.
*McDermott, D. (1987), "We've Been Framed: Or Why AI Is Innocent of the Frame Problem", in Pylyshyn (1987).
*Mithen, S. (1987), ''The Prehistory of the Mind'', London: Thames & Hudson.
*{{cite journal | last1 = Nelson | first1 = T. E. | last2 = Oxley | first2 = Z. M. | last3 = Clawson | first3 = R. A. | year = 1997 | title = Toward a psychology of framing effects | url = | journal = Political Behavior | volume = 19 | issue = 3| pages = 221–246 | doi = 10.1023/A:1024834831093 }}
*{{cite journal | last1 = Pan | first1 = Z. | last2 = Kosicki | first2 = G. M. | year = 1993 | title = Framing analysis: An approach to news discourse | url = | journal = Political Communication | volume = 10 | issue = 1| pages = 55–75 | doi = 10.1080/10584609.1993.9962963 }}
*Pan. Z. & Kosicki, G. M. (2001). Framing as a strategic action in public deliberation. In S. D. Reese, O. H. Gandy, Jr., & A. E. Grant (Eds.), Framing public life: Perspectives on media and our understanding of the social world, (pp.&nbsp;35–66). Mahwah, NJ: Lawrence Erlbaum Associates.
*Pan, Z. & Kosicki, G. M. (2005). Framing and the understanding of citizenship. In S. Dunwoody, L. B. Becker, D. McLeod, & G. M. Kosicki (Eds.), Evolution of key mass communication concepts, (pp.&nbsp;165–204). New York: Hampton Press.
*[[Zenon Pylyshyn|Pylyshyn, Zenon W.]] (ed.) (1987), The Robot's Dilemma: The Frame Problem in Artificial Intelligence, Norwood, NJ: Ablex.
*Stephen D. Reese, Oscar H. Gandy and August E. Grant. (2001). [https://books.google.com/books?id=I0BlAAAAMAAJ&q=journalist+subject:%22Reporters+and+reporting%22&dq=journalist+subject:%22Reporters+and+reporting%22&lr=&client=firefox-a&pgis=1 ''Framing Public Life: Perspectives on Media and Our Understanding of the Social World.''] Maywah, New Jersey: Lawrence Erlbaum. ISBN 978-0-8058-3653-0
*Russell, S. & Wefald, E. (1991), Do the Right Thing: Studies in Limited Rationality, Cambridge, MA: MIT Press.
*{{cite journal | last1 = Scheufele | first1 =  DA| authorlink = Scheufele | last2 = Dietram | first2 = A.  | year = 1999 | title = Framing as a theory of media effects | url = | journal = [[Journal of Communication]] | volume = 49 | issue = 1| pages = 103–122 | doi = 10.1111/j.1460-2466.1999.tb02784.x }}
*Shanahan, Murray P. (1997), ''Solving the Frame Problem: A Mathematical Investigation of the Common Sense Law of Inertia'', Cambridge, MA: MIT Press. ISBN 0-262-19384-1
*Shanahan, Murray P. (2003), "The Frame Problem", in ''The Macmillan Encyclopedia of Cognitive Science'', ed. L.Nadel, Macmillan, pp.&nbsp;144–150.
*[[Herbert A. Simon|Simon, Herbert]] (1957), ''Models of Man, Social and Rational: Mathematical Essays on Rational Human Behavior in a Social Setting'', New York: John Wiley. {{OCLC|165735}}
*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Benford | first2 = R. D. | year = 1988 | title = Ideology, frame resonance, and participant mobilization | url = | journal = International Social Movement Research | volume = 1 | issue = | pages = 197–217 }}
*{{cite journal | last1 = Snow | first1 = D. A. | last2 = Rochford | first2 = E. B. | last3 = Worden | first3 = S. K. | last4 = Benford | first4 = R. D. | year = 1986 | title = Frame alignment processes, micromobilization, and movement participation | url = | journal = American Sociological Review | volume = 51 | issue = 4| pages = 464–481 | doi = 10.2307/2095581 }}
*{{cite journal | last1 = Sperber | first1 = D. | last2 = Wilson | first2 = D. | year = 1996 | title = Fodor's Frame Problem and Relevance Theory | url = | journal = Behavioral and Brain Sciences | volume = 19 | issue = 3| pages = 530–532 | doi = 10.1017/S0140525X00082030 }}
*[[Sidney Tarrow|Tarrow, S.]] (1983a). "Struggling to Reform: social Movements and policy change during cycles of protest". Western Societies Paper No. 15. Ithaca, NY: Cornell University.
*Tarrow, S. (1983b). "Resource mobilization and cycles of protest: Theoretical reflections and comparative illustrations". Paper presented at the Annual Meeting of the [[American Sociological Association]], Detroit, August 31–September 4.
*Triandafyllidou, A. and Fotiou, A. (1998), [http://www.socresonline.org.uk/3/1/2.html "Sustainability and Modernity in the European Union: A Frame Theory Approach to Policy-Making"], Sociological Research Online, vol. 3, no. 1.
*[[Charles Tilly|Tilly, C.]], Tilly, L., & Tilly, R. (1975). ''The rebellious century, 1830–1930''. Cambridge, MA: Cambridge University Press.
*Turner, R. H., & Killian, L. M. (1972). ''Collective Behavior''. Englewood Cliffs, NJ: Prentice-Hall.
* "Rational Choice and the Framing of Decisions", A.Tversky, D.Kahneman, ''Journal of Business'', 1986, vol.59, no.4, pt.2.
*{{cite journal | last1 = Wilkerson | first1 = W.S. | year = 2001 | title = Simulation, Theory, and the Frame Problem | url = | journal = [[Philosophical Psychology (journal)|Philosophical Psychology]] | volume = 14 | issue = 2| pages = 141–153 | doi = 10.1080/09515080120051535 }}
*[[Charles Arthur Willard|Willard, Charles Arthur]]. ''Liberalism and the Social Grounds of Knowledge'' Chicago: University of Chicago Press, 199

==External links==
*[http://www.nytimes.com/2005/07/17/magazine/17DEMOCRATS.html The Framing Wars. ''New York Times'' 17 July 2005]
*Curry, Tom. 2005. [http://www.msnbc.msn.com/id/7640262/page/2/ "Frist chills talk of judges deal (Page 2)."] "The question in the poll was not '''framed''' as a matter of whether nominee ought to get an up-or-down vote. And that '''framing''' of the issue, Republican strategists believe, is the most advantageous one..."; [[MSNBC]]
*[http://www.ccbi.cmu.edu/reprints/Gonzalez_JOEP2005-decision-making.pdf CMU.edu (pdf)] - 'The Framing effect and risky decision: Examining cognitive functions with fMRI', C. Gonzalez, et al., ''[[Journal of Economic Psychology]]'' (2005)
*[http://hbswk.hbs.edu/item/5488.html HBS.edu] - 'Fixing Price Tag Confusion'(interview), Sean Silverthorne (December 11, 2006)
*[http://www.msnbc.msn.com/id/14170927/ "'Framing effect' influences decisions: Emotions play a role in decision-making when information is too complex"], Charles Q. Choi, [[MSNBC]] (August 3, 2006)

{{Media culture}}
{{Media manipulation}}
{{Propaganda}}
{{World view}}

{{DEFAULTSORT:Framing (Social Sciences)}}
[[Category:Cognitive biases]]
[[Category:Framing (social sciences)| ]]
[[Category:Knowledge representation]]
[[Category:Propaganda techniques]]
[[Category:Prospect theory]]
[[Category:Social constructionism]]
<=====doc_Id=====>:791
<=====title=====>:
Reification (computer science)
<=====text=====>:
{{Other uses|Reification (disambiguation)}}

'''Reification''' is the process by which an abstract idea about a [[computer program]] is turned into an explicit [[data model]] or other object created in a [[programming language]]. A computable/addressable object — a resource — is created in a system as a proxy for a non computable/addressable object. By means of reification, something that was previously implicit, unexpressed, and possibly inexpressible is explicitly formulated and made available to conceptual (logical or computational) manipulation. Informally, reification is often referred to as "making something a [[first-class citizen]]" within the scope of a particular system. Some aspect of a system can be reified at ''language design time'', which is related to [[Reflection (computer science)|reflection]] in programming languages. It can be applied as a stepwise refinement at ''system design time''. Reification is one of the most frequently used techniques of [[conceptual analysis]] and [[knowledge representation]].

== Reification and reflective programming languages ==
In the context of [[programming language]]s, reification is the process by which a user program or any aspect of a programming language that was implicit in the translated program and the run-time system, are  expressed in the language itself. This process makes it available to the program, which can inspect all these aspects as ordinary [[data]]. In [[Reflection (computer science)|reflective languages]], reification data is causally connected to the related reified aspect such that a modification to one of them affects the other. Therefore, the reification data is always a faithful representation of the related reified aspect. Reification data is often said to be made a [[first class object]]. Reification, at least partially, has been experienced in many languages to date: in early [[Lisp (programming language)|Lisp dialects]] and in current [[Prolog| Prolog dialects]], programs have been treated as data, although the causal connection has often been left to the responsibility of the programmer. In [[Smalltalk]]-80, the compiler from the source text to bytecode has been part of the run-time system since the very first implementations of the language.<ref>J. Malenfant, M. Jacques and F.-N. Demers, [http://www2.parc.com/csl/groups/sda/projects/reflection96/docs/malenfant/ref96/ref96.html A Tutorial on Behavioral Reflection and its Implementation]</ref>

* The [[C (programming language)|C programming language]] reifies the low-level detail of [[memory address]]es.{{paragraph break}}Many programming language designs encapsulate the details of memory allocation in the compiler and the run-time system. In the design of the C programming language, the memory address is reified and is available for direct manipulation by other language constructs. For example, the following code may be used when implementing a memory-mapped device driver. The buffer pointer is a proxy for the memory address 0xB800000.{{paragraph break}}<source lang="c">
 char* buffer = (char*) 0xB800000;
 buffer[0] = 10; 
</source>
* [[Functional programming languages]] based on [[lambda-calculus]] reify the concept of a procedure abstraction and procedure application in the form of the [[Lambda calculus#Lambda calculus and programming languages|Lambda expression]].
* The [[Scheme (programming language)|Scheme]] programming language reifies [[continuations]] (approximately, the call stack).
* In [[C Sharp (programming language)|C#]], reification is used to make [[parametric polymorphism]] implemented as generics as a first-class feature of the language.
* In the [[Java (programming language)|Java]] programming language, there exist "reifiable types" that are "completely available at run time" (i.e. their information is not erased during compilation).<ref>[http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.7 The Java Language Specification, section 4.7], Java SE 7 Edition</ref>
* [[REBOL]] reifies code as data and vice versa.
* Many languages, such as [[Lisp (programming language)|Lisp]], [[JavaScript]], and [[Curl (programming language)|Curl]], provide an [[eval|<code>eval</code> or <code>evaluate</code> procedure]] that effectively reifies the language interpreter.
* The [[Logtalk]] framework for [[Prolog]] offers a means to explore reification in the context of [[logic programming]].
* [[Smalltalk]] and [[Actor model|Actor languages]] permit the reification of blocks and [[message passing|messages]],<ref>{{cite web|url=http://c2.com/cgi/wiki?SmalltalkBlocksAndClosures |title=Smalltalk Blocks And Closures |publisher=C2.com |date=2009-10-15 |accessdate=2010-10-09}}</ref> which are equivalent of lambda expressions in Lisp, and [[thisContext]] which is a reification of the current executing block.
* [[Homoiconicity|Homoiconic languages]] reify the syntax of the language itself in the form of an [[abstract syntax tree]], typically together with <code>eval</code>.

== Data reification vs. data refinement ==
Data reification ([[stepwise refinement]]) involves finding a more concrete representation of the [[abstract data type]]s used in a [[formal specification]].

Data reification is the terminology of the [[Vienna Development Method]] (VDM) that most other people would call data refinement. An example is taking a step towards an implementation by replacing a data representation without a counterpart in the intended implementation language, such as sets, by one that does have a counterpart (such as maps with fixed domains that can be implemented by arrays), or at least one that is closer to having a counterpart, such as sequences. The VDM community prefers the word "reification" over "refinement", as the process has more to do with concretising an idea than with refining it.<ref>[https://www.cs.tcd.ie/FME/original/FAQ/vdm/part13.html Formal Methods Europe, Frequently Asked Questions, part 13].</ref>

For similar usages, see [[Reification (linguistics)]].

== Reification in conceptual modeling ==
Reification is widely used in [[Conceptual model (computer science)|conceptual modeling]].<ref>Antoni Olivé, Conceptual Modeling of Information Systems, Springer Verlag, 2007.</ref> Reifying a relationship means viewing it as an entity. The purpose of reifying a relationship is to make it explicit, when additional information needs to be added to it. Consider the relationship type ''<code>IsMemberOf(member:Person, Committee)</code>''. An instance of ''<code>IsMemberOf</code>'' is a relationship that represents the fact that a person is a member of a committee. The figure below shows an example population of ''<code>IsMemberOf</code>'' relationship in tabular form. Person ''P1'' is a member of committees ''C1'' and ''C2''. Person ''P2'' is a member of committee ''C1'' only. [[File:reification example1.png|500px|thumb|Example population of <code>IsMemberOf</code> relationship in tabular form. Person P1 is a member of committees C1 and C2. Person P2 is a member of committee C1 only.]]

The same fact, however, could also be viewed as an entity. Viewing a relationship as an entity, one can say that the entity reifies the relationship. This is called reification of a relationship. Like any other entity, it must be an instance of an entity type. In the present example, the entity type has been named <code>Membership</code>. For each instance of ''<code>IsMemberOf</code>'', there is one and only one instance of ''<code>Membership</code>'', and vice versa. Now, it becomes possible to add more information to the original relationship. As an example, we can express the fact that "person p1 was nominated to be the member of committee c1 by person p2". Reified relationship ''<code>Membership</code>'' can be used as the source of a new relationship ''<code>IsNominatedBy(Membership, Person)</code>''.

For related usages see [[Reification (knowledge representation)]].

== Reification in Unified Modeling Language (UML) ==
[[File:reification example2.png|400px|thumb|The UML [[class diagram]] for the Membership example.]] [[Unified Modeling Language|UML]] provides an ''association class'' construct for defining reified relationship types. The association class is a single model element that is both a kind of association and a kind of a class.<ref>''Unified Modeling Language, UML superstructure'', Object Management Group, 2007-11-02.</ref> The association and the entity type that reifies are both the same model element. Note that attributes cannot be reified.

== Reification on Semantic Web ==

=== RDF and OWL ===
In [[Semantic Web]] languages, such as [[Resource Description Framework]] (RDF) and [[Web Ontology Language]] (OWL), a statement is a binary relation. It is used to link two individuals or an individual and a value. Applications sometimes need to describe other RDF statements, for instance, to record information like when statements were made, or who made them, which is sometimes called "[[provenance]]" information. As an example, we may want to represent properties of a relation, such as our certainty about it, severity or strength of a relation, relevance of a relation, and so on.

The example from the conceptual modeling section describes a particular person with <code>URIref person:p1</code>, who is a member of the <code>committee:c1</code>. The RDF triple from that description is
<source lang="sparql">
  person:p1   committee:isMemberOf   committee:c1 .
</source>
Consider to store two further facts: (i) to record who nominated this particular person to this committee (a statement about the membership itself), and (ii) to record who added the fact to the database (a statement about the statement).

The first case is a case of classical reification like above in UML: reify the membership and store its attributes and roles etc.:

<source lang="sparql">
 committee:Membership        rdf:type              owl:Class .
 committee:membership12345   rdf:type              committee:Membership .
 committee:membership12345   committee:ofPerson    person:p1 .
 committee:membership12345   committee:inCommittee committee:c1 .
 person:p2                   committee:nominated   committee:membership12345 .  
</source>

Additionally, RDF provides a built-in vocabulary intended for describing RDF statements. A description of a statement using this vocabulary is called a reification of the statement. The RDF reification vocabulary consists of the type <code>rdf:Statement</code>, and the properties <code>rdf:subject</code>, <code>rdf:predicate</code>, and <code>rdf:object</code>.<ref name="rdf">{{cite web|url=http://www.w3.org/TR/2004/REC-rdf-primer-20040210/#reification |title=RDF Primer |publisher=W3.org |date= |accessdate=2010-10-09}}</ref>

Using the reification vocabulary, a reification of the statement about the person's membership would be given by assigning the statement a URIref such as <code>committee:membership12345</code> so that describing statements can be written as follows:
<source lang="sparql">
 committee:membership12345Stat   rdf:type        rdf:Statement .
 committee:membership12345Stat   rdf:subject     person:p1 .
 committee:membership12345Stat   rdf:predicate   committee:isMemberOf . 
 committee:membership12345Stat   rdf:object      committee:c1 .
</source>
These statements say that the resource identified by the <code>URIref committee:membership12345Stat</code> is an RDF statement, that the subject of the statement refers to the resource identified by <code>person:p1</code>, the predicate of the statement refers to the resource identified by <code>committee:isMemberOf</code>, and the object of the statement refers to the resource <code>committee:c1</code>. Assuming that the original statement is actually identified by <code>committee:membership12345</code>, it should be clear by comparing the original statement with the reification that the reification actually does describe it. The conventional use of the RDF reification vocabulary always involves describing a statement using four statements in this pattern. Therefore, they are sometimes referred to as the "reification quad".<ref name="rdf"/>

Using reification according to this convention, we could record the fact that <code>person:p3</code> added the statement to the
database by
<source lang="sparql">
  person:p3    committee:addedToDatabase    committee:membership12345Stat .
</source>
It is important to note that in the conventional use of reification, the subject of the reification triples is assumed to identify a particular instance  of a triple in a particular RDF document, rather than some arbitrary triple having the same subject, predicate, and object. This particular convention is used because reification is intended for expressing properties such as dates of composition and source information, as in the examples given already, and these properties need to be applied to specific instances of triples. 
Note that the described triple <code>(subject predicate object)</code> itself is not implied by such a reification quad (and it is not necessary that it actually exists in the database). This allows also to use this mechanism to express which triples do ''not'' hold.

The power of the reification vocabulary in RDF is restricted by the lack of a built-in means for assigning URIrefs to statements, so in order to express "provenance" information of this kind in RDF, one has to use some mechanism (outside of RDF) to assign URIs to individual RDF statements, then make further statements about those individual statements, using their URIs to identify them.<ref name="rdf"/>

=== Reification in Topic Maps ===
In an [[Topic Maps|XML Topic Map]] (XTM), only a topic can have a name or play a role in an association. One may use an association to make an assertion about a topic, but one cannot directly make assertions about that assertion. However, it is possible to create a topic that reifies a non-topic construct in a map, thus enabling the association to be named and treated as a topic itself.<ref>[http://www.techquila.com/practical_intro.html Practical Introduction into Topic Maps].</ref>

=== Reification and n-ary relations ===
In Semantic Web languages, such as RDF and OWL, a property is a binary relation used to link two individuals or an individual and a value. However, in some cases, the natural and convenient way to represent certain concepts is to use relations to link an individual to more than just one individual or value. These relations are called [[n-ary relations]]. Examples are representing relations among multiple individuals, such as a committee, a person who is a committee member and another person who has nominated the first person to become the committee member, or a buyer, a seller, and an object that was bought when describing a purchase of a book.

A more general approach to reification is to create an explicit new class and n new properties to represent an n-ary relation, making an instance of the relation linking the n individuals an instance of this class. This approach can also be used to represent provenance information and other properties for an individual relation instance.<ref>{{cite web|url=http://www.w3.org/TR/swbp-n-aryRelations/ |title=W3C Defining N-ary relations on Semantic Web |publisher=W3.org |date= |accessdate=2010-10-09}}</ref>
<source lang="turtle">
 :p1
      a       :Person ;
      :has_membership _:membership_12345 .
 _:membership_12345
      a       :Membership ;
      :committee :c1;
      :nominated_by :p2 .
</source>

=== Reification vs. quotation ===
It is also important to note that the reification described here is not the same as "quotation" found in other languages. Instead, the reification describes the relationship between a particular instance of a triple and the resources the triple refers to. The reification can be read intuitively as saying "this RDF triple talks about these things", rather than (as in quotation) "this RDF triple has this form." For instance, in the reification example used in this section, the triple:
<source lang="sparql">
  committee:membership12345   rdf:subject   person:p1 .
</source>
describing the <code>rdf:subject</code> of the original statement says that the subject of the statement is the resource (the person) identified by the URIref <code>person:p1</code>. It does not state that the subject of the statement is the URIref itself (i.e., a string beginning with certain characters), as quotation would.

==See also==
{{Wiktionary|reification}}
* [[Denotational semantics]]
* [[Formal semantics of programming languages]]
* [[Meta-circular evaluator]]
* [[Metamodeling]]
* [[Metaobject]]
* [[Metaprogramming]]
* [[Normalization by evaluation]]
* [[Operational semantics]]
* [[Reflection (computer science)]]
* [[Resource Description Framework]]
* [[Self-interpreter]]
* [[Topic Maps]]

==References==
{{reflist}}

<!--Interwikies-->

{{DEFAULTSORT:Reification (Computer Science)}}
<!--Categories-->
[[Category:Object-oriented programming]]
[[Category:Formal methods terminology]]
[[Category:Knowledge representation]]

[[de:Reifikation#Informatik]]
[[fr:Réification]]
<=====doc_Id=====>:794
<=====title=====>:
Category:Knowledge representation languages
<=====text=====>:
== See also ==

* [[:Category:Constraint programming languages]]
* [[:Category:Domain-specific programming languages]]

[[Category:Knowledge representation]]
[[Category:Engineered languages]]
[[Category:Markup languages]]
<=====doc_Id=====>:797
<=====title=====>:
Reason maintenance
<=====text=====>:
{{more footnotes|date=September 2009}}

'''Reason maintenance'''<ref name="insNouts">Doyle, J.: The ins and outs of reason maintenance.</ref><ref name="originalTR">Doyle, J.: Truth maintenance systems for problem solving. Tech. Rep. AI-TR-419,
Dep. of Electrical Engineering and Computer Science of MIT (1978)</ref> is a [[knowledge representation]] approach to efficient handling of inferred information that is explicitly stored. Reason maintenance distinguishes between base facts, which can be [[Defeasible reasoning|defeated]], and derived facts. As such it differs from [[belief revision]] which, in its basic form, assumes that all facts are equally important. Reason maintenance was originally developed as a technique for implementing problem solvers.<ref name="originalTR"/> It encompasses a variety of techniques that share a common architecture:<ref name="mcAllesterInterface">McAllester, D.A.: Truth maintenance. AAAI90 (1990)</ref> two components - a reasoner and a reason maintenance system - communicate with each other via an interface. The reasoner uses the reason maintenance system to record its inferences and justifications of ("reasons" for) the inferences. The reasoner also informs the reason maintenance system which are the currently valid base facts (assumptions). The reason maintenance system uses the information to compute the truth value of the stored derived facts and to restore consistency if an inconsistency is derived.

A '''truth maintenance system''', or '''TMS''', is a [[knowledge representation]] method for representing both beliefs and their dependencies and an algorithm called the "truth maintenance algorithm" that manipulates and maintains the dependencies. The name ''truth maintenance'' is due to the ability of these systems to restore consistency.   

It is also termed as a belief revision system, a truth maintenance system maintains consistency between old believed knowledge and current believed knowledge in the knowledge base (KB) through revision. If the current believed statements contradict the knowledge in the KB, then the KB is updated with the new knowledge. It may happen that the same data will again come into existence, and the previous knowledge will be required in the KB. If the previous data is not present, it is required for new inference. But if the previous knowledge was in the KB, then no retracing of the same knowledge was needed. Hence the use of TMS to avoid such retracing; it keeps track of the contradictory data with the help of a dependency record. This record reflects the retractions and additions which makes the inference engine (IE) aware of its current belief set.

Each statement having at least one valid justification is made a part of the current belief set. When a contradiction is found, the statement(s) responsible for the contradiction are identified and an appropriate is retraced. This process is called dependency-directed backtracking.

The TMS algorithm maintains the records in the form of a dependency network. The nodes in the network are one of the entries in the KB (a premise, antecedent, or inference rule etc.) Each arc of the network represent the inference steps from which the node was derived.

A premise is a fundamental belief which is assumed to be always true. They do not need justifications. Considering premises are base from which justifications for all other nodes will be stated.

There are two types of justification for each node. They are:

# Support List [SL]
# Conditional Proof (CP)

Many kinds of truth maintenance systems exist.   Two major types are single-context and multi-context truth maintenance.   
In single context systems, consistency is maintained among all facts in memory (database) and relates to the notion of consistency found in [[classical logic]]. Multi-context systems support [[paraconsistency]] by allowing consistency to be  relevant to a subset of facts in memory (a context) according to the history of logical inference.  This is achieved by tagging each fact or deduction with its logical history. Multi-agent truth maintenance systems perform truth maintenance across multiple memories, often located on different machines. de Kleer's assumption-based truth maintenance system (ATMS, 1986) was utilized in systems based upon [[AI winter#The fall of expert systems|KEE]] on the [[Lisp Machine]]. The first multi-agent TMS was created by Mason and Johnson. It was a multi-context system.  Bridgeland and Huhns created the first single-context multi-agent system.

==See also==
* [[Knowledge representation]]
* [[Artificial intelligence]]
* [[Belief revision]]
* [[Knowledge acquisition]]

==References==
<references />

==Other references==
* Bridgeland, D. M. & Huhns, M. N.,  Distributed Truth Maintenance. Proceedings of. AAAI–90: Eighth National Conference on Artificial Intelligence, 1990.
* J. de Kleer (1986). An assumption-based TMS. ''Artificial Intelligence'', 28:127–162.
* J. Doyle. A Truth Maintenance System. AI. Vol. 12. No 3, pp.&nbsp;251–272. 1979.
* U. Junker and K. Konolige (1990). Computing the extensions of autoepistemic and default logics with a truth maintenance system. In ''Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI'90)'', pages 278–283. [[MIT Press]].
* Mason, C. and Johnson, R. DATMS: A Framework for Assumption Based Reasoning, in Distributed Artificial Intelligence, Vol. 2, [[Morgan Kaufmann Publishers]], Inc., 1989.
* D. A. McAllester. A three valued maintenance system. [[Massachusetts Institute of Technology]], Artificial Intelligence Laboratory. AI Memo 473. 1978.
* G. M. Provan (1988). A complexity analysis of assumption-based truth maintenance systems. In B. Smith and G. Kelleher, editors, ''Reason Maintenance Systems and their Applications'', pages 98–113. Ellis Horwood, New York.
* G. M. Provan (1990). The computational complexity of multiple-context truth maintenance systems. In ''Proceedings of the Ninth European Conference on Artificial Intelligence (ECAI'90)'', pages 522–527.
* R. Reiter and J. de Kleer (1987). Foundations of assumption-based truth maintenance systems: Preliminary report. In ''Proceedings of the Sixth National Conference on Artificial Intelligence (AAAI'87)'', pages 183–188. [http://www2.parc.com/spl/members/dekleer/Publications/Foundations%20of%20Assumption-Based%20Truth%20Maintenance%20Systems.pdf  PDF]

==External links==
* [http://scholar.google.com/scholar?q=Truth+maintenance+system&ie=UTF-8&oe=UTF-8&hl=en&btnG=Search Google Scholar on TMSs]
* [http://plato.stanford.edu/entries/logic-ai/#3.2.1 Belief Revision and TMSs] at [[Stanford Encyclopedia of Philosophy]]

[[Category:Belief revision]]
[[Category:Knowledge representation]]
[[Category:Information systems]]
<=====doc_Id=====>:800
<=====title=====>:
Linguistic value
<=====text=====>:
:''For the similar but unrelated term in linguistics see '' [[Linguistic variable]]

In [[artificial intelligence]], [[operations research]], and related fields, a '''Linguistic value''', (for some authors '''Linguistic variable''') is a [[natural language]] term which is derived using quantitative or qualitative reasoning such as with probability and statistics or [[fuzzy sets and systems]].<ref>''Fuzzy Logic for Business and Industry'' Earl Cox, Charles River Media, pp188,214,302,306,352  1995 ISBN 1-886801-01-0</ref><ref>''The Fuzzy Systems Handbook, Second Edition'' Earl Cox, Academic Press, 1999 ISBN 0-12-194455-7 Ch 6 Fuzzy Reasoning, &sect; 1 The Role of Linguistic Variables</ref>
<ref>''On the Modeling of Linguistic Information using Random Sets'' Hung T. Nguyen p. 242 in Readings in Fuzzy Sets for Intelligent Systems. Morgan Kaufmann 1993. Dubois, Prade, and Yager eds. </ref>
<ref>''Fuzzy Sets And The Social Nature of Truth'' J. Goguen. CS UCLA p. 49-67 in Advances in Fuzzy Sets and Systems, North Holland, 1979. &sect; 2.3 ''Linguistic Truth Values''. ISBN 0-444-85372-3</ref>

==Example of Linguistic Value==

For example, if a shuttle heat shield is deemed of having a linguistic value of a "very low" percentage of damage in re-entry, based upon knowledge from experts in the field, that probability would be given a value of say, 5%.  From there on out, if it were to be used in an equation, the variable of percentage of damage will be at 5% if it deemed very low percentage.

==References==
{{Reflist}}

{{DEFAULTSORT:Linguistic Value}}
[[Category:Knowledge representation]]


{{AI-stub}}
<=====doc_Id=====>:803
<=====title=====>:
Completeness (knowledge bases)
<=====text=====>:
A [[knowledge base]] KB is '''complete''' ''if'' there is no formular α such that KB ⊭ α and KB ⊭ ¬α.

Example of knowledge base with incomplete knowledge:

KB := { A ∨ B }

Then we have KB ⊭ A and KB ⊭ ¬A.

In some cases, you can make a [[Consistency (knowledge bases)|consistent knowledge base]] complete with the [[closed world assumption]] - that is, adding all not-entailed literals as negations to the knowledge base. In the above example though, this would not work because it would make the knowledge base inconsistent:

KB' = { A ∨ B, ¬A, ¬B }

In the case you have KB := { P(a), Q(a), Q(b) }, you have KB ⊭ P(b) and KB ⊭ ¬P(b), so with the closed world assumption you would get KB' = { P(a), ¬P(b), Q(a), Q(b) } where you have KB' ⊨ ¬P(b).

See also:

* [[Vivid knowledge]]

{{computable knowledge}}

[[Category:Knowledge representation]]
{{logic-stub}}
{{database-stub}}
<=====doc_Id=====>:806
<=====title=====>:
OntoCAPE
<=====text=====>:
'''OntoCAPE''' is a large-scale [[ontology (computer science)|ontology]] for the [[Domain knowledge|domain]] of [[Computer-Aided Process Engineering]] (CAPE). It can be downloaded free of charge from the [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&L=1 OntoCAPE Homepage].

OntoCAPE is partitioned into 62 sub-ontologies, which can be used individually or as an integrated suite. 
The sub-ontologies are organized across different [[abstraction layer]]s, which separate general knowledge from knowledge about particular domains and applications.

* The upper layers have the character of an [[Upper ontology (information science)|upper ontology]], covering general topics such  as mereotopology, systems theory, quantities and units.
* The lower layers conceptualize the domain of chemical process engineering, covering domain-specific topics such as materials, chemical reactions, or unit operations.

==Further reading==
* Marquardt et al. (2010). [http://www.springer.com/chemistry/book/978-3-642-04654-4 ''OntoCAPE: A Re-Usable Ontology for Chemical Process Engineering'']. Springer-Verlag, Berlin Heidelberg.

== External links ==
* [http://www.avt.rwth-aachen.de/AVT/index.php?id=730&L=1 OntoCAPE Homepage]

[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
<=====doc_Id=====>:809
<=====title=====>:
Polythematic Structured Subject Heading System
<=====text=====>:
[[Image:PSH logo kratke.gif|thumb|right|PSH logo]]
'''Polythematic Structured Subject Heading System''' (abbreviated as '''PSH''' from the [[Czech language|Czech]] ''Polytematický Strukturovaný Heslář'') is a bilingual Czech–English [[controlled vocabulary]] of [[Index term|subject headings]] developed and maintained by the National Technical Library (the former State Technical Library) in [[Prague]]. It was designed for describing and searching information resources according to their subject. PSH contains more than 13,900 terms, which cover the main fields of human knowledge.

[[Image:Lod-datasets 2010-09-22 colored.png|thumb|The Linking Open Data cloud diagram]]
Thanks to its release in [[Simple Knowledge Organization System|SKOS]], PSH can be used not only for describing [[document]]s in a [[library]], but also for [[Web indexing|indexing web pages]]. Everyone can use PSH for free. PSH is a part of the Linking Open Data cloud diagram (LOD cloud diagaram). The image of the LOD cloud diagram shows datasets that have been published in [[Linked Data]] format, by contributors to the [http://esw.w3.org/topic/SweoIG/TaskForces/CommunityProjects/LinkingOpenData Linking Open Data] community project and other individuals and organisations.

== History and development ==
The PSH preparation project started in 1993, supported by several grants from the Czech Ministry of Culture and Czech Ministry of Education, Youth and Sport. Since 1995, PSH has been used for indexing the State Technical Library’s documents. Starting 1997,<ref>KLOUČKOVÁ, Zdenka. Polytematický strukturovaný heslář Státní technické knihovny. Čtenář. 1997, vol. 49, no. 4, p. 128-129. ISSN 0011-2321.</ref> PSH has been distributed to other libraries and companies, originally as a commercial, paid product; since 2009<ref>MYNARZ, Jindřich; KAMRÁDKOVÁ, Kateřina; KOŽUCHOVÁ, Kristýna. [http://www.techlib.cz/files/download/id/649/psh-cc.pdf Polythematic Structured Subject Heading System & Creative Commons]. In Seminář ke zpřístupňování šedé literatury [online]. 2008– [retrieved 2010-05-28]. Praha : Státní technická knihovna, 2008.</ref> for free. In 2000, the State Technical Library received a grant from the Ministry of Culture to translate PSH into English. The next milestone in its development was its releasing in the [[Simple Knowledge Organization System|SKOS]] format, in 2009.<ref name="mynarz">MYNARZ, Jindřich; KOŽUCHOVÁ, Kristýna; KAMRÁDKOVÁ, Kateřina. [http://www.ikaros.cz/node/5591 Novinky z oblasti Polytematického strukturovaného hesláře]. Ikaros [online]. 2009, vol. 13, no. 7 [retrieved 2010-05-28]. URN-NBN:cz-ik5591. ISSN 1212-5075.</ref>

The vast majority of new subject headings is suggested and approved by the indexing experts from the National Technical Library. However, the users and public can also make suggestions, using an online form, which are then assessed by the experts. The main decisions about the development and the future of PSH are done by the Committee for Coordination of Polythematic Structured Subject Heading System. The Committee consists of specialists from the National Technical Library and cooperating institutions, and representatives from the libraries and companies which use PSH. The Committee meets once a year in the National Technical Library; in the meantime, the members communicate using an [[electronic mailing list]].<ref name="mynarz" />

== Browsing PSH ==
[http://psh.ntkcz.cz/skos/ PSH Browser] was released in June 2009. It serves for browsing the PSH system and its distribution in SKOS format. This tool navigates users through PSH from general to specific terms. Users can also use the Search field. [http://pshmanager.techlib.cz/ PSH manager] tool was released in 2012. It serves as an indexing tool especially to catalogers. Catalogers can easy orient in its clear structure. All the terms in PSH manager contain link to the catalogue of NTK. There can be also viewed the record in MARC21 format.

== Autoindexing ==
In 2012 was released beta version of autoindexing application. It is accessible on [http://invenio.ntkcz.cz/indexer/ Autoindexing]. Users enter chosen text into indexing field and activate indexing. In few seconds the terms describing content are displayed.

== PSH structure ==
PSH is a [[tree structure]] with 44 thematic sections. Subject headings are included in a hierarchy of six (or seven) levels according to their [[Semantics|semantic]] content and specificity. There are hierarchical, associative ("see also") and [[equivalence relation|equivalence]] ("see") relations in PSH. Hierarchical relations are represented by broader and narrower terms (e.g. ''physical diagnostic methods'' is broader term to ''electrocardiography'', and on the other hand, ''electrocardiography'' is narrower term to ''physical diagnostic methods''). Equivalence relations link subject headings with their nonpreferred versions (e.g. ''electrocardiography'' and ''ECG''). Moreover, associative relations are used to link related subject headings from different parts of PSH, regardless their affiliation to a section, (e.g. ''electrocardiography'': see also ''cardiology''). Every subject heading belongs to just one section, which has its own two-character abbreviation, assigned to every subject heading of the section. This enables users to recognize affiliation of subject headings from lower levels to the thematic sections. The 44 thematic sections have following [[Tree (data structure)|root nodes]]: 
{{Col-begin}}
{{Col-break}}
* agriculture
* anthropology
* architecture and town planning
* art
* astronomy
* biology
* chemistry
* civil engineering
* communications
* computer technology
* consumer industry
* economic sciences
* electronics
* electrotechnics
* food industry
{{Col-break}}
* generalities
* geography
* geology
* geophysics
* health services
* history
* informatics
* information science
* law
* linguistics
* literature
* mathematics
* mechanical engineering
* metallurgy
* military affairs
{{Col-break}}
* mining engineering
* pedagogy
* philosophy
* physics
* politology
* power engineering
* psychology
* religion
* science and technology
* sociology
* sport
* theory of systems
* transport
* water management
{{Col-end}}

== PSH formats ==
The main format for storage, maintenance and sharing PSH is the [[MARC standards|MARC 21 Format for Authority Data]], which is implemented in [[integrated library system|library automated systems]]. PSH is also available in [[Simple Knowledge Organization System|SKOS]], using [[RDF/XML]] syntax, which is a version suitable for web distribution. Single headings can be accessed on the PSH website through [[Uniform Resource Identifier|URI]] links. Alternatively, the whole vocabulary can be downloaded in one file. It is possible to display tags from PSH ([[metadata]] snippets – [[Dublin Core]] and CommonTag), which can be embedded in an HTML document to provide its semantic description in a machine-readable way.

== New subject headings ==
New subject headings are primarily obtained through the log analysis in the National Technical Library's on-line catalogue of documents, which are the terms used by end-users when searching various documents. Google Analytics service is now used for gaining search queries used by users. Within the data analysis, users queries are divided into seven categories that contain the title of the document, person, subject, action, institution, geographical terms and others. Then the candidates for new preferred terms and non-preferred terms are identified in the subject category.

Users can suggest preferred or non-preferred terms through the [https://www.techlib.cz/en/82958-tech-subject-headings#tab_heading web form] or via e-mail psh(@)techlib.cz.

== PSH & Creative Commons ==
PSH/SKOS has been available under the Creative Commons License CC BY 3.0 CZ (Attribution-ShareAlike 3.0 Czech Republic)since 2011. Users are free to copy, distribute, display and perform the work and make derivative works, but they must give the original author credit and if they alter, transform, or build upon this work, they have to distribute the resulting work only under a licence identical to this one. Users can download all data in one [https://www.techlib.cz/en/82958-tech-subject-headings#tab_documentation zip file], which is continuously updated.

== See also ==
*[[Thesaurus]]
*[[Library of Congress Subject Headings]]
*[[Information retrieval]]
*[[Semantic Web]]

== References ==
{{Reflist}}

== External links ==
* [https://www.techlib.cz/en/82958-tech-subject-headings/ PSH official web page]

[[Category:Index (publishing)]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:812
<=====title=====>:
E-services
<=====text=====>:
The concept of '''e-service''' (short for electronic service) represents one prominent application of utilizing the use of [[Information and communication technology|information and communication technologies]] (ICTs) in different areas. However, providing an exact definition of e-service is hard to come by as researchers have been using different definitions to describe e-service. Despite these different definitions, it can be argued that they all agree about the role of technology in facilitating the delivery of services which make them more of electronic services.

It seems compelling to adopt Rowley (2006)<ref name=autogenerated1>Rowley, J. (2006) An analysis of the e-service literature: towards a research agenda. Internet Research, 16 (3), 339-359</ref> approach who defines e-services as: “…deeds, efforts or performances whose delivery is mediated by information technology. Such e-service includes the service element of e-tailing, customer support, and service delivery”. This definition reflect three main components- service provider, service receiver and the channels of service delivery (i.e., technology). For example, as concerned to public e-service, public agencies are the service provider and citizens as well as businesses are the service receiver. The channel of service delivery is the third requirement of e-service. Internet is the main channel of e-service delivery while other classic channels (e.g. telephone, call center, public kiosk, mobile phone, television) are also considered.

==Definitions and origin of the term e-service==
Since its conceptual inception in the late 1980s in Europe{{Citation needed|date=October 2012}} and formal introduction in 1993 by the US Government,<ref>Alasem, A. (2009). An Overview of e-Government Metadata Standards and Initiatives based on Dublin Core. Electronic Journal of e-Government, 7(1), 1 – 10</ref> the term ‘[[E-Government]]’ has now become one of the recognized research domains especially in the context of public policy and now has been rapidly gaining strategic importance in public sector modernization.<ref>Wimmer, M., Codagnone, C. and Janssen, M. (2008) “Future of e-Government Research: 13 research themes identified in the eGovRTD2020 project’. Proceeding of the 41st Hawaii International Conference on System Sciences, USA</ref> E-service is one of the branches of this domain and its attention has also been creeping up among the practitioners and researchers.<ref>Lӧfstedt, U. (2005) ‘Assessment of current research and some proposals for future direction’, International Journal of Public IS</ref>

E-service (or eservice) is a highly generic term, usually referring to ‘The provision of services via the Internet (the prefix 'e' standing for ‘electronic’, as it does in many other usages), thus e-Service may also include e-Commerce, although it may also include non-commercial services (online), which is usually provided by the government.’ (Irma Buntantan & G. David Garson, 2004: 169-170; Muhammad Rais & Nazariah, 2003: 59, 70-71).

'E-Service constitutes the online services available on the Internet, whereby a valid transaction of buying and selling (procurement) is possible, as opposed to the traditional websites, whereby only descriptive information are available, and no online transaction is made possible.' (Jeong, 2007).<ref>Jeong Chun Hai @Ibrahim. (2007). ''Fundamental of Development Administration.'' Selangor: Scholar Press. ISBN 978-967-5-04508-0</ref>

==Importance of E-service ==
Lu (2001)<ref>Lu, J. (2001). Measuring cost/benefits of e-business applications and customer satisfaction”, Proceedings of the 2nd International Web Conference, 29–30 November, Perth, Australia, 139-47</ref> identifies a number of benefits for e-services, some of these are:

* Accessing a greater customer base
* Broadening market reach
* Lowering of entry barrier to new markets and cost of acquiring new customers
* Alternative communication channel to customers
* Increasing services to customers
* Enhancing perceived company image
* Gaining competitive advantages
* Potential for increasing [[Customer knowledge]]

==Importance and advantages of E-shopping==
*E-shops are open 24 hours a day.
*There is no need to travel to the malls or wait at the checkout counters.
*There is usually a wide selection of goods and services.
*It is easy to compare prices and quality by using the E-shopping tool.
*Price reduction and discounts are electronically conveyed.

==E-service domain==
The term ‘e-service’ has many applications and can be found in many disciplines. The two dominant application areas of e-services are

E-business (or e-commerce): e-services mostly provided by businesses or [NGO|non-government organizations] (NGOs) (private sector).

E-government: e-services provided by government to citizens or business (public sector is the supply side). The use and description of the e-service in this page will be limited to the context of e-government only where of the e-service is usually associated with prefix “public”: Public e-services. In some cases, we will have to describe aspects that are related to both fields like some conferences or journals which cover the concept of “e-Service” in both domains of e-government and [[e-business]].

==Architecture==
Depending on the types of services, there are certain functionalities required in the certain layers of e-service architectural framework, these include but are not limited to – Data layer (data sources), processing layers (customer service systems, management systems, data warehouse systems, integrated customer content systems), exchange layer ([[Enterprise application integration|Enterprise Application Integration]]– EAI), interaction layer ( integrating e-services), and presentation layer (customer interface through which the web pages and e-services are linked).

==E-service quality==
Measuring [[service quality]] and service excellence are important in a competitive organizational environment. The [[SERVQUAL]]- service quality model is one of the widely used tools for measuring quality of the service on various aspects. The five attributes of this model are: reliability, responsiveness, assurance, tangibles, and empathy. The following table summarizes some major of these:

{| class="sortable wikitable"
|-
!'''SERVQUAL<ref>Jiang, J.J.; Klein, G. and Crampton, S.M. (2000). A note on SERVQUAL reliability and validity in information system service quality measurement. Decision Sciences. Atlanta: Summer 2000. Vol. 31, Iss. 3; p. 725</ref>'''
!'''Kaynama & Black (2000)'''<ref>Kaynama, S. A., and Black, C. I.  (2000). A Proposal to assess the Service Quality of Online Travel Agencies: An Exploratory Study. Journal of Professional Services Marketing (21:1), 63-88</ref>
!'''Zeithaml (2002)'''<ref>Zeithaml, V. A. (2002). Service Excellence in Electronic Channels. Managing Service Quality (12:3), 2002, 135-138</ref>
!'''Janda et al. (2002)'''<ref>Janda, S., Trocchia, P. J., and Gwinner, K. (2002). Consumer perceptions of Internet Retail Service Quality. International Journal of Service Industry Management (13:5),  412-431</ref>
!'''Alawattegama & Wattegama (2008)'''<ref>Alawattegama, L. and Wattegama, C. (2008). Benchmarking Asia Pacific National Telecom Regulatory Authority Websites. LIRNEasia</ref>
|-
|| Reliability      ||Content      ||Access      ||Access      || Factual information
|-
||Responsiveness      ||Access      ||Ease of navigation      ||Security     || Business information
|-
||Assurance      ||Navigation      ||Efficiency     ||Sensation        || General information
|-
||Tangibles      ||Design      ||Flexibility      ||Information/content      || Consumer‐ related information
|-
||Empathy      ||Response      ||Reliability ||     ||
|-
||     ||Background      ||Personalization      ||     ||
|-
||     ||Personalization     ||Security/privacy      ||     ||
|-
||     ||     ||Responsiveness      ||     ||
|-
||     ||     ||Assurance/trust     ||     ||
|-
||     ||     ||Site aesthetics      ||     ||
|-
||     ||    ||Price knowledge      ||     ||
|}

The [[LIRNEasia]] study on benchmarking national telecom regulator websites focuses on content than on accessibility and ease of use, unlike the other studies mentioned here. Websites are increasingly important portals to government agencies, especially in the context of [[information society]] reforms. Stakeholders, including businesses, investors and even the general public, are interested in information produced by these agencies, and websites can help to increase their transparency and accountability. The quality of its website also demonstrates how advanced a regulatory agency is.

==E-service cost factor==
Some major cost factors are (Lu, 2001):<ref>Lu, J. (2001). Measuring cost/benefits of e-business applications and customer
satisfaction”, Proceedings of the 2nd International Web Conference, 29–30 November, Perth, Australia, 139-47</ref>

* Expense of setting up applications
* Maintaining applications
* Internet connection
* Hardware/software
* Security concerns
* legal issues
* Training; and
* Rapid technology changes

==Practical examples of e-services in the Developing World==
Information technology is a powerful tool for accelerating economic development. Developing countries have focused on the development of ICT during the last two decades and as a result, it has been recognized that ICT is critical to economy and is as a catalyst of economic development. So, in recent years there seems to have been efforts for providing various e-services in many developing countries since ICT is believed to offer considerable potential for the sustainable development of e-Government and as a result, e-Services.<ref name=autogenerated4>Ndou,V.(2004)E-Government for developing countries: Opportunities and Challenges, EJISDC 18, 1, 1-24</ref>

Many government agencies in developed countries have taken progressive steps toward the web and ICT use, adding coherence to all local activities on the Internet, widening local access and skills, opening up interactive services for local debates, and increasing the participation of citizens on promotion and management of the territory(Graham and Aurigi, 1997).<ref>Graham, S. and Aurigi, A. (1997) Virtual Cities, Social Polarisation, and the Crisis in Urban Public Space, Journal of Urban Technology, 4, 1, 19-52</ref>

But the potential for e-government in developing countries remains largely unexploited, even though. ICT is believed to offer considerable potential for the sustainable development of e-government. Different human, organizational and technological factors,
issues and problems pertain in these countries, requiring focused studies and appropriate approaches. ICT, in general, is referred to as an “enabler”, but on the other hand it should also be regarded as a challenge and a peril in itself. The organizations, public or private,which ignore the potential value and use of ICT may suffer pivotal competitive disadvantages. Nevertheless, some e-government initiatives have flourished in developing countries too, e.g. Brazil, India, Chile, etc.<ref name=autogenerated4 /> What the experience in these countries shows, is that governments in the developing world can effectively exploit and appropriate the benefits of ICT, but e-government success entails the accommodation of certain unique conditions,needs and obstacles. The adaptive challenges of e-government go far beyond technology, they call for organizational structures and skills, new forms of leadership, transformation of public-private partnerships (Allen et al., 2001).<ref>Allen, A.B., Juillet, L., Paquet, G. and Roy, J. (2001) E-Governance and Government Online in Canada: Partnerships, People and Prospects, Government Information Quarterly,18, 93-104.)</ref>

Following are a few examples regarding e-services in some developing countries:

===E-services in Rwanda===
Only a decade after emerging from the fastest genocide of the 20th Century, Rwanda, a small country in Eastern Central Africa,
has become one of the continent’s leaders in, and model on, bridging the digital divide through e-government. Rwanda has undergone a rapid turnaround from one of the most technologically deficient countries only a decade ago to a country
where legislative business is conducted online and wireless access to the Internet is available anywhere in the country. This is
puzzling when viewed against the limited progress made in other comparable developing countries, especially those located in the
same region, sub-Saharan Africa, where the structural and institutional constraints to e-government diffusion are similar.<ref>Mawangi, W.(2006) The social relations of e-government diffusion in developing countries: the case of Rwanda, Proceedings of the 2006 international conference on Digital government research, May 21–24, 2006, San Diego, California</ref>

===E-services in South Africa===
In South Africa, there continues to be high expectations of government in respect to improved delivery of service and of closer consultation with citizens. Such expectations are not unique to this country, and in this regard there is a need for governments to recognise that the implementation of e-government systems and e-services affords them the opportunity to enhance service delivery and good governance.<ref name=autogenerated3>van Brakel, P.A.(2009) Proceedings of the 11th Annual Conference on World Wide Web Applications, Port Elizabeth, 2–4 September</ref> The implementation of e-Government has been widely acclaimed in that it provides new impetus to deliver services quickly and efficiently (Evans & Yen, 2006:208).<ref>Evans, D. & Yen, D. C. 2006. e-Government: evolving relationship of citizens and government, domestic, and international development. Government Information Quarterly, 23(2): 207-235.)</ref> In recognition of these benefits, various arms of the South African government have embarked on a number of e-government programmes for example the [http://www.gov.za/  Batho Pele portal], [[Sars efiling|SARS e-filing]], the [http://www.enatis.com/  e-Natis system], electronic processing of grant applications from remote sites, and a large number of departmental information websites. Also a number of well publicised e-government ventures such as the latter, analysts and researchers consider the state of e-government in South Africa to be at rudimentary stages. There are various factors
which collectively contribute to such an assessment. Amongst these, key factors relate to a lack of a clear strategy to facilitate uptake and adoption of e-government services as well as evaluation frameworks to assess expectations of citizens who are one of the primary user groups of these services.<ref name=autogenerated3 />

===E-services in Malaysia===
E-Services is one of the pilot projects under the Electronic Government Flagship within the Multimedia Super Corridor ([http://www.mscmalaysia.my/  MSC]) initiative. With E-Services, one can now conduct transactions with Government agencies, such as the Road Transport Department (RTD) and private utility companies such as Tenaga Nasional Berhad ([http://www.tnb.com.my  TNB]) and Telekom Malaysia Berhad ([http://www.tm.net.my  TM]) through various convenient channels such as the eServices kiosks and internet. No more queuing, traffic jams or bureaucratic hassles and one can now conduct transaction at one’s own convenience. Also, Electronic Labour Exchange ([http://www.elx.gov.my  ELX])is one stop-centre for labor market information, as supervised by the Ministry of Human Resource ([http://www.mohr.gov.my  MOHR]), to enable employers and job seekers to communicate on the same platform.

[http://www.esyariah.gov.my/  e-Syariah] is the seventh project under the Electronic Government flagship application of the Multimedia Super Corridor (MSC). A case management system that integrates the processes related to management of cases for the Syariah Courts.

==Challenges to e-services in the Developing World==
The future of e-service is bright but some challenges remain. There are some challenges in e-service, as Sheth & Sharma (2007)<ref>Sheth., J.N. , Sharma, A., (2007). E-Services: A framework for growth.  Journal of Value Chain Management, 1(1/2)</ref> identify, are:

* Low penetration of ICT especially in the developing countries;
* [[Internet fraud|Fraud]] on the internet space which is estimated around 2.8billion USD
* [[Internet privacy|Privacy]] due the emergence of various types of spyware and security holes, and
* intrusive characteristics of the service (e.g. mobile phones based) as customers may not like to be contacted with the service providers at any time and at any place.

The first challenge and primary obstacle to the e-service platform will be penetration of the internet. In some developing countries, the access to the internet is limited and speeds are also limited. In these cases firms and customers will continue to use traditional platforms. The second issue of concern is fraud on the internet. It is anticipated that the fraud on the e-commerce internet space costs $2.8 billion. Possibility of fraud will continue to reduce the utilization of the internet. The third issue is of privacy. Due to both spyware and security holes in operating systems, there is concern that the transactions that consumers undertake have privacy limitations.  For example, by stealthily following online activities, firms can develop fairly accurate descriptions of customer profiles.  Possibility of privacy violations will reduce the utilizations of the internet. The final issue is that e-service can also become intrusive as they reduce time and location barriers of other forms of contract. For example, firms can contact people through mobile devices at any time and at any place. Customers do not take like the intrusive behavior and may not use the e-service platform. (Heiner and lyer, 2007)<ref>Heiner and lyer (2007) E-Service opportunities and Threats, Journal of value chain management, 1, 11.</ref>

==Major e-service keywords==
A considerable amount of research efforts already exists on the subject matter exploring different aspects of e-service and e-service delivery ; one worth noting effort is Rowley’s study (2006)<ref name=autogenerated1 /> who did a review study on the e-service literature. The key finding of his study is that there is need to explore dimensions of e-service delivery not focusing only on service quality “In order to understand e-service experiences it is necessary to go beyond studies of e-service quality dimensions and to also take into account the inherent characteristics of e-service delivery and the factors that differentiate one service experience from another.”

Some of the major keywords of e-service as found in the e-government research are as follows:

===Acceptance===
User acceptance of technology is defined according to Morris (1996, referred by Wu 2005, p.&nbsp;1)<ref>Wu, Philip F. (2009). User Acceptance of Emergency Alert Technology: A Case Study. Proceedings of the 6th International ISCRAM Conference – Gothenburg, Sweden</ref> as “the demonstrable willingness within a user group to employ information technology for the tasks it is designed to support”. This definition can be brought into the context of e-service where acceptance can be defined as the users’ willingness to use e-service or the willingness to decide when and how to use the e-service.

===Accessibility===
Users’ ability to access to the e-service is important theme in the previous literature. For example, Huang (2003)<ref>Huang, C.J. (2003). Usability of E-Government Web Sites for People with Disabilities, In Proceedings of the 36th Hawaii International Conference on System Sciences (HICSS’03), IEEE Computer Society, 2003</ref> finds that most of the websites in general fail to serve users with disabilities. Recommendation to improve accessibility is evident in previous literature including Jaeger (2006)<ref>Jaeger, P.T. Assessing Section 508 compliance on federal e-government Web sites: A multi-method, user-centered evaluation of accessibility for persons with disabilities. Government Information Quarterly 23 (2006) 169–190</ref> who suggests the following to improve e-services’ accessibility like: design for accessibility from the outset of website development, Involve users with disabilities in the testing of the site …Focus on the benefits of an accessible Web site to all users.

===Administrative literacy===
According to Grönlund et al. (2007),<ref>Grönlund, Å., Hatakka, M. and Ask, A. (2007) ‘ Inclusion in the E-Service Society – Investigating Administrative Literacy Requirements for Using E-Services’. 6th International Conference (EGOV 2007, Regensburg, Germany), 4656</ref> for a simple e-service, the needs for knowledge and skills, content and procedures are considerably less. However, in complicated services there are needed to change some prevailed skills, such as replacing verbal skills with skill in searching for information online.

===Benchmarking===
This theme is concerned with establishing standards for measuring e-services or the best practices within the field. This theme also includes the international benchmarking of e-government services (UN reports, EU reports); much critic has been targeting these reports being incomprehensive and useless. According to Bannister (2007)<ref>Bannister F. (2007). The curse of the benchmark: an assessment of the validity and value of e-government comparisons, International Review of Administrative Sciences, 73 (2), 171-188</ref> “… benchmarks are not a reliable tool for measuring real e-government progress. Furthermore, if they are poorly designed, they risk distorting government policies as countries may chase the benchmark rather than looking at real local and national needs”

===Digital divide===
[[Digital divide]] is considered one of the main barriers to implementing e-services; some people do not have means to access the e-services and some others do not know how to use the technology (or the e-service). According to Helbig et al. (2009),<ref>Helbig, N; Gil-García, J ; Ferro, E (2009). Understanding the complexity of electronic government: Implications from the digital divide literature. Government Information Quarterly, 26(2009), 89–97</ref> “we suggest E-Government and the digital divide should be seen as complementary social phenomena (i.e., demand and supply). Moreover, a serious e-government digital divide is that services mostly used by social elites."

===E-readiness===
Most of the reports and the established criteria focus on assessing the services in terms of infrastructure and public policies ignoring the citizen participation or [[e-readiness]]. According to by Shalini (2009),<ref>Shalini, R. (2009). Are Mauritians ready for e-Government services?. Government Information Quarterly 26 (2009) 536–539</ref> “the results of the research project reveal that a high [http://www.eiu.com/site_info.asp?info_name=eiu_2007_e_readiness_rankings&rf=0|e-readiness index] may be only indicating that a country is e-ready in terms of ICT infrastructure and info-structure, institutions, policies, and political commitment, but it is a very poor measure of the e-readiness of citizens. To summarize the findings, it can be said that Mauritius is ready but the Mauritians are not”

``E-readiness, as the Economist Intelligence Unit defines, is the measure of a country’s ability to leverage digital channels for communication, commerce and government in order to further economic and social development. Implied in this measure is the extent to which the usage of communications devices and Internet services creates efficiencies for business and citizens, and the extent to which this usage is leveraged in the development of information and communications technology (ICT) industries. In general terms, the definition of e-readiness is relative,for instance depending on a country in question's priorities and perspective.<ref>GeoSINC International (2002). E-Readiness Guide. Available at http://www.apdip.net/documents/evaluation/e-readiness/geosinc01042002.pdf</ref>

===Efficiency===
As opposed to effectiveness, efficiency is focused on the internal competence within the government departments when delivering e-services. There is a complaint that researchers focus more on effectiveness “There is an emerging trend seemingly moving away from the efficiency target and focusing on users and governance outcome. While the latter is worthwhile, efficiency must still remain a key priority for eGovernment given the budget constraints compounded in the future by the costs of an ageing population. Moreover, efficiency gains are those that can be most likely proven empirically through robust methodologies”<ref>Codagnone, C.  Undheim T.A (2008).  Benchmarking eGovernment: tools, theory, and practice. European Journal of ePractice. Nº 4 • August 2008</ref>

===Security===
Security is the most important challenge that faces the implementation of e-services because without a guarantee of privacy and security citizens will not be willing to take up e-government services. These security concerns, such as hacker attacks and the theft of credit card information, make governments hesitant to provide public online services.  According to the GAO report<ref>GAO.(2002). E-Government: Proposal addresses Critical Challenges. U.S General Accounting Office, Govt of the USA</ref> of 2002 “security concerns present one of the toughest challenges to extending the reach of e-government.The rash of hacker attacks, Web page defacing, and credit card information being posted on electronic bulletin boards can make many federal agency officials—as well as the general public—reluctant to conduct sensitive government transactions involving personal or financial data over the Internet.” By and Large,  Security is one of the major challenges that faces the implementation and development of electronic services. people want to be assured that they are safe when they are conducting online services and that their information will remain secure and confidential

===Stakeholders===
Axelsson et al. (2009)<ref>Axelsson, K, Melin, f, Lindgren, I, (2009) DEVELOPING PUBLIC E-SERVICES FOR SEVERAL STAKEHOLDERS – A MULTIFACETED VIEW OF THE NEEDS FOR AN E-SERVICE. 17th European Conference on Information Systems</ref> argue that the stakeholder concept-which was originally used in private firms-, can be used in public setting and in the context of e-government. According to them, several scholars have discussed the use of the [[stakeholder theory]] in public settings.<ref>Scholl, H. J. (2001). Applying stakeholder theory to e-government: Benefits and Limits. Kluwer Academic Publishers, Massachusetts</ref> The stakeholder theory suggests that need to focus on all the involved stakeholder s when designing the e-service; not only on the government and citizens.

===Usability===
Compared to Accessibility, There is sufficient literature that addresses the issue of usability; researchers have developed different models and methods to measure the usability and effectiveness of eGovernment websites. However, But still there is call to improve these measures and make it more compressive<ref>Kaylor, C.,  Deshazo, R. & Eck, D. V. (2001) "Gauging e-government: A report on implementing services among American cities". Government Information Quarterly (GIQ), 18(4), 293 - 307</ref>

``The word usability has cropped up a few times already in this unit. In the context of biometric identification, usability referred to the smoothness of enrollment and other tasks associated with setting up an identification system. A system that produced few false matches during enrollment of applicants was described as usable. Another meaning of usability is related to the ease of use of an interface. Although this meaning of the term is often used in the context of computer interfaces, there is no reason to confine it to computers.<ref>[http://openlearn.open.ac.uk/mod/resource/view.php?id=211245 Open Learning - OpenLearn - Open University]</ref>´´

==Social, cultural and ethical implications of e-services==
The perceived effectiveness of e-service can be influenced by public’s view of the social and cultural implications of e-technologies and e-service.

Impacts on individuals’ rights and privacy – as more and more companies and government agencies use technology to collect, store, and make accessible data on individuals, privacy concerns have grown. Some companies monitor their employees' computer usage patterns in order to assess individual or workgroup performance.<ref>Asgarkhani, M. (2002). Strategic Management of Information systems and Technology in an e-World”, Proceedings of the 21st IT Conference, Sri Lanka, pp103-111.</ref> Technological advancements are also making it much easier for businesses, government and other individuals to obtain a great deal of information about an individual without their knowledge. There is a growing concern<ref name=autogenerated2>Asgarkhani, M. (2002b) “e-Governance in Asia Pacific”, Proceedings of the International Conference on Governance in Asia, Hong Kong.</ref> that access to a wide range of information can be dangerous within politically corrupt government agencies.

Impact on Jobs and Workplaces - in the early days of computers, management scientists anticipated that computers would replace human decision-makers. However, despite significant technological advances, this prediction is no longer a mainstream concern. At the current time, one of the concerns associated with computer usage in any organization (including governments) is the health risk – such as injuries related to working continuously on a computer keyboard. Government agencies are expected to work with regulatory groups in order to avoid these problems.

Potential Impacts on Society – despite some economic benefits of ICT to individuals, there is evidence that the computer literacy and access gap between the haves and have-nots may be increasing. Education and information access are more than ever the keys to economic prosperity, yet access by individuals in different countries is not equal - this social inequity has become known as the digital divide.

Impact on Social Interaction – advancements in ICT and e-Technology solutions have enabled many government functions to become automated and information to be made available online. This is a concern to those who place a high value on social interaction.

Information Security - technological advancements allow government agencies to collect, store and make data available online to individuals and organizations. Citizens and businesses expect to be allowed to access data in a flexible manner (at any time and from any location). Meeting these expectations comes at a price to government agencies where it concerns managing information – more specifically, ease of access; data integrity and accuracy; capacity planning to ensure the timely delivery of data to remote (possibly mobile) sites; and managing the security of corporate and public information.<ref name=autogenerated2 />

==E-service awards==
The benefits of e-services in advancing businesses efficiency and in promoting good governance are huge; recognizing the importance of these benefits has resulted in number of international awards that are dedicated to recognize the best designed e-services. In the section, we will provide description of some international awards

===Best online e-service in Europe===
[http://www.epractice.eu/en/awards|The European eGovernment Awards program] started 2003 to recognize the best online public service in Europe. The aim of Awards is to encourage the deployment of e-services and to bring the attention to best practices in the field. The winners of the [http://ec.europa.eu/idabc/en/document/7842|4th European eGovernment Awards] were announced in the award ceremony that took place at the [http://www.egov2009.se/ 5th Ministerial eGovernment Conference] on 19 November 2009 (Sweden); the winners in their respective categories are:

* Category 1. eGovernment supporting the Single Market: EU-OPA, the European Order for Payment Application ({{flag|Austria}} and {{flag|Germany}})
* Category 2a. eGovernment empowering citizens: Genvej ({{flag|Denmark}})
* Category 2b. eGovernment empowering businesses: MEPA, the Public Administration eMarketplace ({{flag|Italy}})
* Category 3. eGovernment enabling administrative efficiency and effectiveness: Licensing of Hunters via the “Multibanco” ATM Network ({{flag|Portugal}})
* Public prize: SMS Information System ({{flag|Turkey}})

===Other awards===

[http://www.ita.gov.om/HMAward/ Sultan Qaboos Award for excellence in eGovernance] {{flag|Oman}}(Started 2009) The award has five categories: Best eContent, Best eService, Best eProject, eEconomy, eReadiness.

[http://www.egovawards.bh/AboutEn.aspx?Id=1|Bahrain eGovernment Excellence Awards] {{flag|Bahrain}}(Started 2007) The program has three categories: Government Awards: Best eContent, Best eService, Best eProject, eEconomy, eEducation, eMaturity Business Awards: Best ICT solution Provider, eEconomy,eEducation Citizen Awards: Best eContent, eCitizen.

[http://www.e-servicesphils.com/esp2010/ Philippines e-Service Awards] {{flag|Philippines}}(Started 2001) Categories: Outstanding Client Application of the Year, Outstanding Customer Application of the year, Groundbreaking Technology of the Year, Most Progressive Homegrown Company of the Year.

==Major journals focusing on e-services==
There are some journals particularly interested for “e-Service “. Some of these are:
* [http://www.igi-global.com/journal/international-journal-services-mobile-applications/1114  International Journal of E-services and Mobile Applications]
* [http://eservicejournal.org/ eService Journal]
* [http://www.palgrave-journals.com/ejis/index.html  European Journal of Information Systems]
* [http://www.misq.org/  MIS Quarterly]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/505553/description#description  Information & Management]
* [http://www.wiley.com/bw/journal.asp?ref=1350-1917&site=1  Information Systems Journal]
* [http://www.igi-global.com/Bookstore/TitleDetails.aspx?TitleId=1091  International Journal of Electronic Government]
* [http://www.ejeg.com/  Electronic Journal of e-Government]
* [http://www.gvsu.edu/business/ijec/  International Journal of Electronic Commerce]
* [http://www.emeraldinsight.com/Insight/viewContainer.do?containerType=Journal&containerId=11229  Internet Research]
* [http://www.palgrave-journals.com/jit/index.html  Journal Information Technology]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/525447/description#description  Journal of Strategic Information Systems]
* [http://aisel.aisnet.org/jais/  Journal of the Association for Information Systems]
* [http://www.elsevier.com/wps/find/journaldescription.cws_home/620202/description#description  Government Information Quarterly]
* [http://www.wiley.com/bw/journal.asp?ref=0033-3352  Public Administration Review]

==Major conferences focusing on e-services==

Major conferences considering e-service as one of the themes are:
* [http://ec.europa.eu/information_society/newsroom/cf/itemdetail.cfm?item_id=5649&utm_campaign=isp&utm_medium=rss&utm_source=newsroom&utm_content=tpa-8  eServices in European Civil Registration conference]
* [http://www.iist.unu.edu/I3E/IFIP  Conference on e-Business, e-Services, and e-Society]
* [http://www.eafricaconference.org/  International ICST Conference on e-service]
* [http://www.e-servicesphils.com/esp2010/  E-service Global Sourcing Conference & Exhibition]
* [http://www.hicss.hawaii.edu/hicss_43/minitracks/eg-sin.htm  Annual Hawaii International Conference on Systems Sciences]
* [http://www.egov-conference.org/egov-2011-preview  Electronic Government Conference (EGOV)]
* [http://www.dexa.org/  International Conference on Electronic Government and the Information Systems Perspective (EGOVIS)]
* [http://www.icegov.org/  International Conference on Theory and Practice of Electronic Governance ( ICEGOV)]

==See also==
* [[Electronic services delivery]]
* [[Customer knowledge]]

==References==
{{Reflist}}

==External links==
* [http://www.e-govwatch.org.nz/criteria/e-services_delivery.html  E-services delivery]
* [http://www.computerworld.com/s/article/9005371/Report_Card_The_Best_E_Government_Sites  The Best E-Government Sites]
* [http://egov.infodev.org/en/Section.78.html#citizen-or-business-centric-portals  The World Bank (InfoDev) e-Government toolkit]

[[Category:Digital divide]]
[[Category:E-commerce]]
[[Category:Information technology]]
[[Category:Knowledge representation]]
[[Category:Open government]]
[[Category:Technology in society]]
<=====doc_Id=====>:815
<=====title=====>:
Knowledge value chain
<=====text=====>:
A '''[[knowledge value]] chain''' is a sequence of intellectual tasks by which [[knowledge workers]] build their employer's unique competitive advantage <ref>Carlucci, D., Marr, B. and Schiuma, G. (2004) 'The knowledge value chain: how intellectual capital impacts on business performance', ''Int J. Technology Management'', Vol. 27, Nos. 6/7, pp. 575&ndash;690  [http://www.som.cranfield.ac.uk/som/dinamic-content/research/cbp/2004,%20Knowledge%20Value%20Chain%20(IJTM%2027,%206-7,%20Carlucci,%20Marr,%20Schiuma).pdf (pdf)]</ref> and/or social and environmental benefit. As an example, the components of a research and development project form a knowledge value chain.

Productivity improvements in a knowledge value chain may come from [[knowledge integration]] in its original sense of data systems consolidation. Improvements also flow from the knowledge integration that occurs when [[knowledge management]] techniques are applied to the continuous improvement of a business process or processes.<ref>[http://www.edgeltd.com/performance-consultants-services/edge_service.php?service=3 Canada Edge Performance Consultants] - official page</ref>

The term first started coming into common use around 1999, appearing in management-related talks and papers.<ref>[http://www.aurorawdc.com/kmworld99.htm 1999 KMWorld conference program], listing Powell's talk on "The Knowledge Value Chain - Aligning       Knowledge Workers with Competitive Strategy"</ref><ref>[http://www.ingentaconnect.com/content/mcb/026/2000/00000019/00000009/art00003 "Knowledge value chain"],''The Journal of Management Development'',            Volume 19, Number 9, 2000, pp. 783&ndash;794(12)</ref><ref>Tim Powell, "Knowledge Value Chain", May 2001, Proceeding of 22nd National Online Meeting, Information Today ([http://www.knowledgeagency.com/pdf_center/Knowledge_Value_Chain.pdf pdf)]</ref> It was registered as a trademark in 2004 by TW Powell Co., a [[Manhattan]] company.<ref>[http://www.knowledgeagency.com TW Powell Co. website]</ref><ref>U.S. Trademark, December 2004. 2,912,705</ref>

'''Knowledge value chain processes'''
*Knowledge acquisition
*Knowledge storage
*Knowledge dissemination
*Knowledge application

==References==
{{reflist}}

{{DEFAULTSORT:Knowledge Value Chain}}
[[Category:Knowledge representation]]
<=====doc_Id=====>:818
<=====title=====>:
Darwin Core Archive
<=====text=====>:
{{Orphan|date=January 2011}}

'''Darwin Core Archive''' (DwC-A) is a [[biodiversity informatics]] data standard that makes use of the [[Darwin Core]] terms to produce a single, self-contained dataset for species occurrence or checklist data. Essentially it is a set of text (CSV) files with a simple descriptor (meta.xml) to inform others how your files are organized. The format is defined in the Darwin Core Text Guidelines.<ref name="dwc-text">[http://rs.tdwg.org/dwc/terms/guides/text/index.htm Darwin Core Text Guidelines]</ref> It is the preferred format for publishing data to the [[GBIF]] network. 

__TOC__

==Darwin Core==
The Darwin Core standard has been used to mobilise the vast majority of specimen occurrence and observational records within the GBIF network.<ref name="gbif-dwca">[http://www.gbif.org/resources/2552 GBIF Darwin Core Archive, How-to Guide]</ref> The [[Darwin Core]] standard was originally conceived to facilitate the discovery, retrieval, and integration of information about modern biological specimens, their spatio-temporal occurrence, and their supporting evidence housed in collections (physical or digital).

The Darwin Core today is broader in scope. It aims to provide a stable, standard reference for sharing information on biological diversity. As a glossary of terms, the Darwin Core provides stable semantic definitions with the goal of being maximally reusable in a variety of contexts. This means that Darwin Core may still be used in the same way it has historically been used, but may also serve as the basis for building more complex exchange formats, while still ensuring interoperability through a common set of terms.

==Archive Format==
{{unreferenced section|date=December 2010}}
The central idea of an archive is that its data files are logically arranged in a star-like manner, with one core data file surrounded by any number of ’extensions’. Each extension record (or ‘extension file row’) points to a record in the core file; in this way, many extension records can exist for each single core record.

Details about recommended extensions can be found in their respective subsections and will be extensively documented in the GBIF registry, which will catalogue all available extensions.

Sharing entire datasets instead of using pageable web services like DiGIR and TAPIR allows much simpler and more efficient data transfer. For example, retrieving 260,000 records via TAPIR takes about nine hours, issuing 1,300 http requests to transfer 500 MB of XML-formatted data. The exact same dataset, encoded as DwC-A and zipped, becomes a 3 MB file. Therefore, GBIF highly recommends compressing an archive using ZIP or GZIP when generating a DwC-A. 

An archive requires stable identifiers for core records, but not for extensions. For any kind of shared data it is therefore necessary to have some sort of local record identifiers. It’s good practice to maintain – with the original data – identifiers that are stable over time and are not being reused after the record is deleted. If you can, please provide globally unique identifiers instead of local ones.

===Archive Descriptor===
To be completed.

<!--
===Data Files===
To be completed.
-->

===Dataset Metadata===
A Darwin Core Archive should contain a file containing metadata describing the whole dataset. The [[Ecological Metadata Language]] (EML) is the most common format for this, but simple Dublin Core files are being used too.

==References==
{{reflist}}

==External links==
* [http://rs.tdwg.org/dwc/terms/index.htm Darwin Core Quick Reference Guide]
* [[Biodiversity Information Standards]] (TDWG)
* [[Global Biodiversity Information Facility]] (GBIF)
* [[Biodiversity informatics]]

[[Category:Bioinformatics]]
[[Category:Knowledge representation]]
[[Category:Interoperability]]
<=====doc_Id=====>:821
<=====title=====>:
General Architecture for Text Engineering
<=====text=====>:
{{Infobox software
| name = GATE
| screenshot = [[Image:GATE5 main window.png|250px]]
| caption = GATE Developer v5 main window
| developer = [http://gate.ac.uk/people GATE research team], [http://www.dcs.shef.ac.uk/ Dept. Computer Science, University of Sheffield]
| released = {{start date and age |1995}}
| frequently_updated = yes<!-- Release version update? Don't edit this page, just click on the version number! -->
| programming language = [[Java (programming language)|Java]]
| operating system = [[Cross-platform]]
| language = English
| genre = [[Text mining]] [[Information Extraction]]
| license = [[LGPL]]
| website = {{url|http://gate.ac.uk}}
}}
'''General Architecture for Text Engineering''' or '''GATE''' is a [[Java (programming language)|Java]] suite of tools originally developed at the [[University of Sheffield]] beginning in 1995 and now used worldwide by a wide community of scientists, companies, teachers and students for many [[natural language processing]] tasks, including [[information extraction]] in many languages.<ref>Languages mentioned on http://gate.ac.uk/gate/plugins/ include Arabic, Bulgarian, Cebuano, Chinese, French, German, Hindi, Italian, Romanian and Russian.</ref>

GATE has been compared to [[NLTK]], [[R (programming language)|R]] and [[RapidMiner]].<ref>{{cite web|url=http://www.b-eye-network.com/view/9516|title=Open Source Text Analytics by Seth Grimes - BeyeNETWORK|publisher=|accessdate=17 December 2016}}</ref> As well as being widely used in its own right, it forms the basis of the KIM semantic platform.<ref>{{cite journal|url=https://www.cambridge.org/core/journals/natural-language-engineering/article/div-classtitlekim-a-semantic-platform-for-information-extraction-and-retrievaldiv/7249CC61F5AB25CBC7AAE182509DFEDE|title=KIM – a semantic platform for information extraction and retrieval|first1=Borislav|last1=Popov|first2=Atanas|last2=Kiryakov|first3=Damyan|last3=Ognyanoff|first4=Dimitar|last4=Manov|first5=Angel|last5=Kirilov|date=1 September 2004|publisher=|volume=10|issue=3-4|pages=375–392|accessdate=17 December 2016|via=Cambridge Core|doi=10.1017/S135132490400347X}}</ref>

GATE community and research has been involved in several European research projects including [[Transitioning Applications to Ontologies|TAO]], [[SEKT]], NeOn, Media-Campaign, Musing, [[Service-Finder]], LIRICS and [[KnowledgeWeb Project|KnowledgeWeb]], as well as many other projects.

As of May 28, 2011, 881 people are on the gate-users mailing list at SourceForge.net, and 111,932 downloads from [[SourceForge]] are recorded since the project moved to SourceForge in 2005.<ref>{{cite web|url=http://sourceforge.net/projects/gate/|title=GATE|publisher=|accessdate=17 December 2016}}</ref> The paper "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications"<ref>[http://gate.ac.uk/sale/acl02/acl-main.pdf "GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications", by Cunningham H., Maynard D., Bontcheva K. and Tablan V. (In proc. of the 40th Anniversary Meeting of the Association for Computational Linguistics, 2002)]</ref> has received over 800 citations in the seven years since publication (according to Google Scholar). Books covering the use of GATE, in addition to the GATE User Guide,<ref>{{cite web|url=http://gate.ac.uk/userguide/|title=GATE.ac.uk  - sale/tao/split.html|publisher=|accessdate=17 December 2016}}</ref> include "Building Search Applications: Lucene, LingPipe, and Gate", by Manu Konchady,<ref>Konchady, Manu. [https://books.google.com/books?id=mcM-OAAACAAJ&dq=Building+Search+Applications:+Lucene,+LingPipe,+and+Gate&hl=en&ei=avbDTczPJITqrQfk1IXQBA&sa=X&oi=book_result&ct=result&resnum=1&ved=0CDEQ6AEwAA Building Search Applications: Lucene, LingPipe, and Gate]. Mustru Publishing. 2008.</ref> and "Introduction to Linguistic Annotation and Text Analytics", by Graham Wilcock.<ref>{{cite web|url=https://books.google.com/books?id=TDQJb1UgVywC&dq=Introduction%20to%20Linguistic%20Annotation%20and%20Text%20Analytics&printsec=frontcover&source=bl&ots=bAF26ZQSTx&sig=TbxZ_-3tRy3IeDBKFofeVN6bAIc&hl=en&ei=vc0gS7PlLo-64QaSgqnfCQ&sa=X&oi=book_result&ct=result&resnum=2&ved=0CBcQ6AEwAQ#v=onepage&q=&f=false|title=Introduction to Linguistic Annotation and Text Analytics|first=Graham|last=Wilcock|date=1 January 2009|publisher=Morgan & Claypool Publishers|accessdate=17 December 2016|via=Google Books}}</ref>

== Features ==

GATE includes an [[information extraction]] system called '''ANNIE''' ('''A Nearly-New Information Extraction System''') which is a set of modules comprising a [[Lexical analysis|tokenizer]], a [[gazetteer]], a [[Sentence boundary disambiguation|sentence splitter]], a [[Part-of-speech tagging|part of speech tagger]], a [[Named entity recognition|named entities]] transducer and a [[coreference]] tagger. ANNIE can be used as-is to provide basic [[information extraction]] functionality, or provide a starting point for more specific tasks.

Languages currently handled in GATE include [[English language|English]], [[Mandarin Chinese|Chinese]], [[Arabic]], [[Bulgarian language|Bulgarian]], [[French language|French]], [[German language|German]], [[Hindi]], [[Italian language|Italian]], [[Cebuano language|Cebuano]], [[Romanian language|Romanian]], [[Russian language|Russian]], [[Danish language|Danish]].

Plugins are included for [[machine learning]] with [[Weka (machine learning)|Weka]], RASP, MAXENT, SVM Light, as well as a [[LIBSVM]] integration and an in-house [[perceptron]] implementation, for managing [[Ontology (information science)|ontologies]] like [[WordNet]], for querying [[search engines]] like [[Google]] or [[Yahoo]], for [[part of speech tagging]] with [[Brill tagger|Brill]] or TreeTagger, and many more. Many external plugins are also available, for handling e.g. [[Twitter|tweets]].<ref>{{cite web|url=https://gate.ac.uk/wiki/twitie.html|title=GATE.ac.uk  - wiki/twitie.html|publisher=|accessdate=17 December 2016}}</ref>

GATE accepts input in various formats, such as [[Text file|TXT]], [[HTML]], [[XML]], [[DOC (computing)|Doc]], [[PDF]] documents, and [[Serialization|Java Serial]], [[PostgreSQL]], [[Lucene]], [[Oracle database|Oracle]] Databases with help of [[RDBMS]] storage over [[JDBC]].

[[JAPE (linguistics)|JAPE]] transducers are used within GATE to manipulate annotations on text. Documentation is provided in the GATE User Guide.<ref>{{cite web|url=http://gate.ac.uk/userguide/chap:jape|title=GATE.ac.uk  - sale/tao/splitch8.html|publisher=|accessdate=17 December 2016}}</ref> A tutorial has also been written by Press Association Images.<ref>{{cite web|url=http://realizingsemanticweb.blogspot.com/2009/07/jape-grammar-tutorial.html|title=Realizing Semantic Web: JAPE grammar tutorial|first=Dhavalkumar|last=Thakker|date=17 July 2009|publisher=|accessdate=17 December 2016}}</ref>

== GATE Developer ==

[[Image:GATE5 main window.png|thumb|400px|GATE 5 main window.]]

The screenshot shows the document viewer used to display a document and its annotations. In pink are <A> hyperlink annotations from an [[Hypertext Markup Language|HTML]] file. The right list is the annotation sets list, and the bottom table is the annotation list. In the center is the annotation editor window.

== GATE Mímir ==
<!-- re-written to remove any lingering copyright worries -->
 Generate vast quantities of information including; natural language text, semantic annotations, and ontological information. Sometimes the data itself is the end product of an application but often the information would be more useful if it could be efficiently searched. GATE Mimir provides support for indexing and searching the linguistic and semantic information generated by such applications and allows for querying the information using arbitrary combinations of text, structural information, and [[SPARQL]].

==See also==
{{Portal|Free software}}
* [[Unstructured Information Management Architecture]] (UIMA)
* [[OpenNLP]]
* [[List of natural language processing toolkits]]
* [[Pheme (project)|Pheme]], a major EU project managed by the GATE group on early detection of false information in social media

==References==
<references/>

{{DEFAULTSORT:General Architecture For Text Engineering}}
[[Category:Data mining and machine learning software]]
[[Category:Free computer libraries]]
[[Category:Free science software]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:Free integrated development environments]]
[[Category:Knowledge representation]]
[[Category:Natural language processing toolkits]]
[[Category:Ontology editors]]
<=====doc_Id=====>:824
<=====title=====>:
Ishikawa diagram
<=====text=====>:
{{Cleanup|date=March 2012}}
{{Infobox quality tool
| image =     Cause and effect diagram for defect XXX.svg
| category =  One of the [[Seven Basic Tools of Quality]]
| describer = [[Kaoru Ishikawa]]
| purpose =   To break down (in successive layers of detail) root causes that potentially contribute to a particular effect
}}
'''Ishikawa diagrams''' (also called '''fishbone diagrams''', '''herringbone diagrams''', '''cause-and-effect diagrams''', or '''Fishikawa''') are [[causal diagram]]s created by [[Kaoru Ishikawa]] (1968) that show the [[cause]]s of a specific [[wikt:event|event]].<ref>{{cite book | last = Ishikawa |first = Kaoru | title= Guide to Quality Control | year = 1968 | publisher = JUSE | location = Tokyo
}}</ref><ref>{{cite book | last = Ishikawa | first = Kaoru | title = Guide to Quality Control | publisher = Asian Productivity Organization | year =  1976 | isbn = 92-833-1036-5}}</ref> Common uses of the Ishikawa diagram are [[product design]] and quality defect prevention to identify potential factors causing an overall effect. Each cause or reason for imperfection is a source of variation. Causes are usually grouped into major categories to identify these sources of variation. The categories typically include
*People: Anyone involved with the process
*Methods: How the process is performed and the specific requirements for doing it, such as policies, procedures, rules, regulations and laws
*Machines: Any equipment, computers, tools, etc. required to accomplish the job
*Materials: Raw materials, parts, pens, paper, etc. used to produce the final product
*Measurements: Data generated from the process that are used to evaluate its quality
*Environment: The conditions, such as location, time, temperature, and culture in which the process operates

==Overview==
[[File:Ishikawa Fishbone Diagram.svg|280px|thumb|left|Ishikawa diagram, in fishbone shape, showing factors of Equipment, Process, People, Materials, Environment and Management, all affecting the overall problem. Smaller arrows connect the sub-causes to major causes.]]
Ishikawa diagrams were popularized in the 1960s by [[Kaoru Ishikawa]],<ref>{{cite book |year=2001 |title=Infusion Therapy in Clinical Practice |first=Judy |last=Hankins |pages=42}}</ref> who pioneered quality management processes in the [[Kawasaki Heavy Industries|Kawasaki]] shipyards, and in the process became one of the founding fathers of modern management.

The basic concept was first used in the 1920s, and is considered one of the [[Seven Basic Tools of Quality|seven basic tools]] of [[quality control]].<ref>{{cite web | url = http://www.asq.org/learn-about-quality/seven-basic-quality-tools/overview/overview.html |first=Nancy R. | last=Tague | title = Seven Basic Quality Tools | year = 2004 | work = The Quality Toolbox | publisher = American Society for Quality | location = Milwaukee, Wisconsin | page = 15 | accessdate = 2010-02-05}}</ref> It is known as a fishbone diagram because of its shape, similar to the side view of a fish skeleton.

[[Mazda]] Motors famously used an Ishikawa diagram in the development of the [[Miata]] sports car, where the required result was "Jinba Ittai" (Horse and Rider as One — jap. 人馬一体). The main causes included such aspects as "touch" and "braking" with the lesser causes including highly granular factors such as "50/50 weight distribution" and "able to rest elbow on top of driver's door". Every factor identified in the diagram was included in the final design.{{citation needed|date=November 2015}}

==Causes==
Causes in the diagram are often categorized, such as to the 5 M's, described below. Cause-and-effect diagrams can reveal key relationships among various variables, and the possible causes provide additional insight into process behavior.

Causes can be derived from brainstorming sessions. These groups can then be labeled as categories of the fishbone.  They will typically be one of the traditional categories mentioned above but may be something unique to the application in a specific case.  Causes can be traced back to root causes with the [[5 Whys]] technique.

Typical categories are

===The 5 Ms (used in manufacturing industry)===
*Machine (technology)
*Method (process)
*Material (Includes Raw Material, Consumables and Information.)
*Man Power (physical work)/Mind Power (brain work): [[Kaizen]]s, Suggestions
*Measurement (Inspection)
The original 5 Ms used by the Toyota Production System have been expanded by some to include the following and are referred to as the 8 Ms. However, this is not globally recognized. It has been suggested to return to the roots of the tools and to keep the teaching simple while recognizing the original intent; most programs do not address the 8Ms.
*Milieu/Mother Nature(Environment)
*Management/Money Power
*Maintenance

"Milieu" is also used as the 6th M by industries for investigations taking the environment into account.

===The 8 Ps (used in marketing industry)===
*Product/Service
*Price 
*Place
*Promotion
*People/personnel
*Process
*Physical Evidence
*Packaging
The 8 Ps are primarily used in service marketing.

===The 5 Ss (used in service industry)===
*Surroundings
*Suppliers
*Systems
*Standard documentation skills
*Scope of work

==See also==
{{Portal|Thinking}}
* [[Seven Basic Tools of Quality]]
* [[Five whys]]

== References ==

=== Citations ===
{{Reflist|30em}}

=== Sources ===
* Ishikawa, Kaoru (1990); (Translator: J. H. Loftus); ''Introduction to Quality Control''; 448 p; ISBN 4-906224-61-X {{OCLC|61341428}}
* Dale, Barrie G. et al. (2007); ''Managing Quality 5th ed''; ISBN 978-1-4051-4279-3 {{OCLC|288977828}}

==External links==
{{Commons category|Ishikawa diagrams}}

{{DEFAULTSORT:Ishikawa Diagram}}
[[Category:Causal diagrams]]
[[Category:Causality]]
[[Category:Knowledge representation]]
[[Category:Quality control tools]]
<=====doc_Id=====>:827
<=====title=====>:
Issue trees
<=====text=====>:
{{Orphan|date=August 2012}}

'''An issue tree''', also called "logic tree" or "issue map", is a graphical breakdown of a question that dissects it into its different components vertically and that progresses into details as it reads to the right.<ref>[https://global.oup.com/academic/product/strategic-thinking-in-complex-problem-solving-9780190463908?q=chevallier&lang=en&cc=us  Chevallier, Arnaud (2016). Strategic Thinking in Complex Problem Solving. Oxford, UK, Oxford University Press. p.47]</ref>

Issue trees are useful in [[problem solving]] to identify the root causes of a problem as well as to identify its potential solutions. They also provide a reference point to see how each piece fits into the whole picture of a problem.<ref>http://webarchive.nationalarchives.gov.uk/20060213205515/http://strategy.gov.uk/downloads/survivalguide/downloads/ssg_v2.1.pdf</ref>

There are two types of issue trees: diagnostic ones and solution ones.

Diagnostic trees breakdown a "why" key question, identifying all the possible root causes for the problem.
Solution tree breakdown a "how" key question, identifying all the possible alternatives to fix the problem.

To be effective, an issue tree needs to obey four basic rules:<ref>http://powerful-problem-solving.com/build-logic-trees</ref>
# Consistently answer a “why” or a “how” question
# Progress from the key question to the analysis as it moves to the right
# Have branches that are mutually exclusive and collectively exhaustive ([[MECE]])
# Use an insightful breakdown

The requirement for issue trees to be collectively exhaustive implies that [[divergent thinking]] is a critical skill.

A profitability tree is an example of an issue tree. It looks at different ways in which a company can increase its profitability. Starting from the key question on the right, it breaks it down between revenues and costs, and break these down into further details.
[[File:An issue tree showing how a company can increase profitability.png|thumb|An issue tree showing how a company can increase profitability]]

==References==
<!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using <ref></ref> tags which will then appear here automatically -->
{{Reflist}}

[[Category:Articles created via the Article Wizard]]
[[Category:Knowledge representation]]
[[Category:Problem solving methods]]


{{logic-stub}}
<=====doc_Id=====>:830
<=====title=====>:
Concept map
<=====text=====>:
{{for|concept maps in [[generic programming]]|Concept (generic programming)}}
[[File:Electricity Concept Map.gif|thumb|An Electricity Concept Map, an example of a concept map]]
{{InfoMaps}}
A '''concept map''' or '''conceptual diagram''' is a [[diagram]] that depicts suggested relationships between [[concept]]s.<ref>Peter J. Hager,Nancy C. Corbin. ''Designing & Delivering: Scientific, Technical, and Managerial Presentations,'' 1997, . 163.</ref> It is a graphical tool that [[instructional designer]]s, [[engineer]]s, [[Technical communication|technical writers]], and others use to organize and structure [[knowledge]].

A concept map typically represents ideas and information as boxes or circles, which it connects with labeled arrows in a downward-branching hierarchical structure. The relationship between concepts can be articulated in linking phrases such as ''causes'', ''requires'', or ''contributes to''.<ref name=theory>[[Joseph D. Novak]] & Alberto J. Cañas (2006). [http://cmap.ihmc.us/Publications/ResearchPapers/TheoryCmaps/TheoryUnderlyingConceptMaps.htm "The Theory Underlying Concept Maps and How To Construct and Use Them"], [[Institute for Human and Machine Cognition]]. Accessed 24 Nov 2008.</ref>

The technique for [[Visualization (graphic)|visualizing]] these relationships among different concepts is called ''concept mapping''. Concept maps define the [[Ontology (information science)|ontology]] of computer systems, for example with the [[object-role modeling]] or  [[Unified Modeling Language]] formalism.

== Overview ==
A concept map is a way of representing relationships between [[idea]]s, [[image]]s, or [[word]]s in the same way that a [[sentence diagram]] represents the grammar of a sentence, a road map represents the locations of highways and towns, and a [[circuit diagram]] represents the workings of an electrical appliance. In a concept map, each word or phrase connects to another, and links back to the original idea, word, or phrase. Concept maps are a way to develop logical thinking and study skills by revealing connections and helping students see how individual ideas form a larger whole. An example of the use of concept maps is provided in the context of learning about types of fuel.{{clarify|why is this noteworthy?|date=November 2016}}<ref name="CMP">[http://www.energyeducation.tx.gov/pdf/223_inv.pdf CONCEPT MAPPING FUELS]. Accessed 24 Nov 2008.</ref>

Concept maps were developed{{whom|date=November 2016}} to enhance meaningful learning in the sciences{{fact|date=November 2016}}. A well-made concept map grows within a ''context frame'' defined by an explicit "focus question", while a [[mind map]] often has only branches radiating out from a central picture. Some research evidence suggests that the brain stores knowledge as productions (situation-response conditionals) that act on [[declarative memory]] content, which is also referred to as chunks or propositions.<ref>Anderson, J. R., & Lebiere, C. (1998). The atomic components of thought. Mahwah, NJ: Erlbaum.</ref><ref>Anderson, J. R., Byrne, M. D., Douglass, S., Lebiere, C., & Qin, Y. (2004). An Integrated Theory of the Mind. Psychological Review, 111(4), 1036&ndash;1050.</ref> Because concept maps are constructed to reflect organization of the declarative memory system, they facilitate sense-making and meaningful learning on the part of individuals who make concept maps and those who use them.

== Differences from other visualizations ==

'''[[Topic map]]s:'''  Concept maps are rather similar to topic maps in that both allow to concepts or topics via [[graph (data structure)|graphs]]. Among the various schema and techniques for visualizing ideas, processes, and organizations, concept mapping, as developed by [[Joseph D. Novak|Joseph Novak]] is unique in its philosophical basis, which "makes concepts, and propositions composed of concepts, the central elements in the structure of knowledge and construction of meaning."<ref>Novak, J.D. & Gowin, D.B. (1996). Learning How To Learn, Cambridge University Press: New York, p. 7.</ref>

'''[[mind mapping|Mind maps]]:'''  Both concept maps and topic maps can be contrasted with [[mind mapping]], which is often restricted to radial hierarchies and [[tree structure]]s. Another contrast between concept mapping and mind mapping is the speed and spontaneity when a mind map is created. A mind map reflects what you think about a single topic, which can focus group brainstorming. A concept map can be a map, a system view, of a real (abstract) system or set of concepts. Concept maps are more free form, as multiple hubs and clusters can be created, unlike mind maps, which fix on a single two centered approach.

== History ==
The technique of concept mapping was developed by [[Joseph D. Novak]] and his research team at [[Cornell University]] in the 1970s as a means of representing the emerging science knowledge of students.<ref>{{cite web|url=http://www.ihmc.us/users/user.php?UserID=jnovak|title=Joseph D. Novak|publisher=Institute for Human and Machine Cognition (IHMC)|accessdate=2008-04-06}}</ref> It has subsequently been used as a tool to increase meaningful learning in the sciences and other subjects as well as to represent the expert knowledge of individuals and teams in education, government and business. Concept maps have their origin  in the learning movement called [[constructivism (learning theory)|constructivism]]. In particular, constructivists hold that learners actively construct knowledge.

Novak's work is based on the cognitive theories of [[David Ausubel]], who stressed the importance of prior knowledge in being able to learn (or ''assimilate'') new concepts: "The most important single factor influencing learning is what the learner already knows. Ascertain this and teach accordingly."<ref>Ausubel, D. (1968) Educational Psychology: A Cognitive View. Holt, Rinehart & Winston, New York.</ref> Novak taught students as young as six years old to make concept maps to represent their response to focus questions such as "What is water?" "What causes the seasons?" In his book ''Learning How to Learn'', Novak states that a "meaningful learning involves the assimilation of new concepts and propositions into existing cognitive structures."

Various attempts have been made to conceptualize the process of creating concept maps. Ray McAleese, in a series of articles, has suggested that mapping is a process of ''off-loading''. In this 1998 paper, McAleese draws on the work of Sowa<ref>Sowa, J.F., 1983. ''Conceptual structures: information processing in mind and machine'', Addison-Wesley.</ref> and a paper by Sweller & Chandler.<ref>Sweller, J. & Chandler, P., 1991. Evidence for Cognitive Load Theory. ''Cognition and Instruction'', 8(4), p.351-362.</ref> In essence, McAleese suggests that the process of making knowledge explicit, using ''nodes'' and ''relationships'', allows the individual to become aware of what they know and as a result to be able to modify what they know.<ref>McAleese,R (1998) '''The Knowledge Arena''' as an Extension to the Concept Map: Reflection in Action, ''Interactive Learning Environments'', '''6,3,p.251-272'''.</ref> Maria Birbili applies that same idea to helping young children learn to think about what they know.<ref>Birbili, M. (2006) [http://ecrp.uiuc.edu/v8n2/birbili.html "Mapping Knowledge: Concept Maps in Early Childhood Education"], ''Early Childhood Research & Practice'', ''8(2)'', Fall 2006</ref> The concept of the ''knowledge arena'' is suggestive of a virtual space where learners may explore what they know and what they do not know.

==Use==
[[Image:Conceptmap.png|thumb|450px|Example concept map created using the IHMC CmapTools computer program.]]
Concept maps are used to stimulate the generation of ideas, and are believed to aid [[creativity]].<ref name="theory"/> Concept mapping is also sometimes used for [[brain-storming]]. Although they are often personalized and idiosyncratic, concept maps can be used to communicate complex ideas.

Formalized concept maps are used in [[software design]], where a common usage is [[Unified Modeling Language]] diagramming amongst similar conventions and development methodologies.

Concept mapping can also be seen as a first step in [[ontology (computer science)|ontology]]-building, and can also be used flexibly to represent formal argument.

Concept maps are widely used in education and business. Uses include:
*[[Note taking]] and summarizing gleaning key concepts, their relationships and hierarchy from documents and source materials
*New knowledge creation: e.g., transforming [[tacit knowledge]] into an organizational resource, mapping team knowledge
*Institutional knowledge preservation (retention), e.g., eliciting and mapping expert knowledge of employees prior to retirement
*Collaborative knowledge modeling and the transfer of expert knowledge
*Facilitating the creation of shared vision and shared understanding within a team or organization
*Instructional design: concept maps used as [[David Ausubel|Ausubelian]] "advance organizers" that provide an initial conceptual frame for subsequent information and learning.
*Training: concept maps used as [[David Ausubel|Ausubelian]] "advanced organizers" to represent the training context and its relationship to their jobs, to the organization's strategic objectives, to training goals.
*Communicating complex ideas and arguments
*Examining the symmetry of complex ideas and arguments and associated terminology
*Detailing the entire structure of an idea, [[train of thought]], or line of argument (with the specific goal of exposing faults, errors, or gaps in one's own reasoning) for the scrutiny of others.
*Enhancing [[metacognition]] (learning to learn, and thinking about knowledge)
*Improving language ability
*Assessing learner understanding of learning objectives, concepts, and the relationship among those concepts
*Lexicon development

==See also==
{{list|date=November 2016}}
{{colbegin}}
* [[Argument map]]
* [[Cognitive map]]
* [[Conceptual graphs]]
* [[Conceptual framework]]
* [[Idea networking]]
* [[Knowledge visualization]]
* [[List of concept- and mind-mapping software]]
* [[Mental model]]
* [[Mind map]]
* [[Radial tree]]
* [[Entity-relationship model]]
* [[Nomological network]]
* [[Semantic web]]
* [[Topic Maps]]
* [[Educational psychology]]
* [[Educational technology]]
* [[Morphological analysis (problem-solving)|Morphological analysis]]
* [[Wicked problem]]
* [[Object role modeling]]
* [[Personal knowledge base]]
* [[Semantic network]]
* [[Olog]]
* [[Pathfinder network]]
* [[Sensemaking]]
{{colend}}

==References==
{{reflist|2}}

== Further reading ==
* {{cite book |last= Novak |first= J.D. |title= Learning, Creating, and Using Knowledge: Concept Maps as Facilitative Tools in Schools and Corporations |publisher= Routledge |edition= 2nd |date= 2009 |isbn= 9780415991858 }}
<!-- |publisher= Lawrence Erlbaum Associates |location= Mahwah |date= 1998 |edition= 1st -->
* {{cite book |last1= Novak |first1= J.D. |last2= Gowin |first2= D.B. |title= Learning How to Learn |publisher= Cambridge University Press |location= Cambridge |date= 1984 |isbn= 9780521319263 }}

== External links ==
{{commons|Concept map}}
{{wikiversity|Concept mapping}}
* [http://www.mind-mapping.org/images/walt-disney-business-map.png Example of a concept map from 1957] by Walt Disney.

{{Mindmaps}}

{{DEFAULTSORT:Concept Map}}
[[Category:Concepts]]
[[Category:Constructivism (psychological school)]]
[[Category:Diagrams]]
[[Category:Educational technology]]
[[Category:Graph drawing]]
[[Category:Knowledge representation]]
[[Category:Note-taking]]
[[Category:Visual thinking]]
<=====doc_Id=====>:833
<=====title=====>:
Category:Lexical databases
<=====text=====>:
{{catmain|Lexical database}}

[[Category:Translation databases]]
[[Category:Computational linguistics]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:836
<=====title=====>:
Pinakes
<=====text=====>:
{{italic title}}
{{hatnote|"Pinakes" may be plural of [[pinax]], a votive tablet that served as a votive object deposited in a sanctuary or burial chamber.}}
[[Image:Ancientlibraryalex.jpg|thumb|Imaginary depiction of the [[Library of Alexandria]]]]
The '''''Pinakes''''' ({{lang-grc|Πίνακες}} "tables", plural of {{lang|grc|[[wikt:πίναξ|πίναξ]]}}) was a [[bibliography|bibliographic]] work composed by [[Callimachus]] (310/305–240 BCE) that is popularly considered to be the first [[library catalog]]; its contents were based upon the holdings of the [[Library of Alexandria]] during Callimachus' tenure there during the third century BCE.<ref>N. Krevans 2002: 173</ref>

==History==

The Library of Alexandria had been founded by [[Ptolemy I Soter]] about 306 BCE. The first recorded librarian was [[Zenodotus]] of Ephesus. During Zenodotus' tenure, Callimachus, who was never the head librarian, compiled the ''Pinakes'', thus becoming the first bibliographer and the scholar who organized the library by authors and subjects about 245 BCE.<ref>Neil Hopkinson, ''A Hellenistic Anthology'' (CUP, 1988) 83.</ref><ref name ="alexandria3">{{cite web|url= http://www.greekplanet.com.au/forum/lofiversion/index.php/t486.html|title=
Greek Inventions|accessdate= 2008-09-19}}</ref> His work was 120 volumes long.<ref>Hopkinson</ref>

[[Apollonius of Rhodes]] was the successor to Zenodotus. [[Eratosthenes]] of Cyrene succeeded Apollonius in 235 BCE and compiled his ''tetagmenos epi teis megaleis bibliothekeis'', the "scheme of the great bookshelves." In 195 BCE [[Aristophanes of Byzantium]] was the librarian and updated the ''Pinakes'',<ref>Pfeiffer, R. ''History of Classical Scholarship from the Beginnings to the End of the Hellenistic Age'' (OUP, 1968) 133.</ref> although it is also possible that his work was not a supplement of Callimachus' ''Pinakes'' themselves, but an independent polemic against, or commentary upon, their contents.<ref>Slater, W.J. "Grammarians on Handwashing", ''Phoenix'' 43 (1989) 100&ndash;11, at 102.</ref>

==Description==

The collection at the Library of Alexandria contained nearly 500,000 [[papyrus]] scrolls, which were grouped together by subject matter and stored in bins.<ref>P.J. Parson, "Libraries", in the ''Oxford Classical Dictionary'', 3rd ed. (OUP, 1996) describes the evidence for the size of the library's holdings thus: "The first Ptolemies (see Ptolemy (1) ) collected ambitiously and systematically; the Alexandrian Library (see ALEXANDRIA (1) ) became legend, and *Callimachus (3)'s ''Pinakes'' made its content accessible. There were rivals at *Pella, *Antioch (1) (where *Euphorion (2) was librarian), and especially *Pergamum. Holdings were substantial: if the figures can be trusted, Pergamum held at least 200,000 rolls (Plut. ''Ant.'' 58. 9), the main library at Alexandria nearly 500,000 (*Tzetzes, ''Prolegomena de comoedia'' 11a. 2. 10–11 Koster)&mdash;the equivalent, perhaps, of 100,000 modern books."</ref> Each bin carried a label with painted tablets hung above the stored papyri. ''Pinakes'' was named after these tablets and are a set of index lists. The bins gave bibliographical information for every roll.<ref>Phillips, Heather A., [http://unllib.unl.edu/LPP/phillips.htm "The Great Library of Alexandria?". Library Philosophy and Practice, August 2010]</ref> A typical entry started with a title and also provided the author's name, birthplace, father's name, any teachers trained under, and educational background. It contained a brief biography of the author and a list of the author's publications. The entry had the first line of the work, a summary of its contents, the name of the author, and information about the origin of the roll.<ref name ="alexandria4">{{cite web|url= http://www.greece.org/hec01/www/arts-culture/alexandria/library/library11.htm|title= The Pinakes|accessdate= 2010-05-29}}</ref>

Callimachus' system divided works into six [[genres]] and five sections of prose: rhetoric, law, epic, tragedy, comedy, lyric poetry, history, medicine, mathematics, natural science and miscellanies. Each category was alphabetized by author.

Callimachus composed two other works that were referred as ''pinakes'' and were probably somewhat similar in format to the ''Pinakes'' (of which they "may or may not be subsections"<ref>Nita Krevans, "Callimachus and the Pedestrian Muse," in M.A. harder et al., eds., ''Callimachus II'' (Hellenistica Groningana 7), 2002, p. [https://books.google.com/books?id=CL4A5I3K-KsC&lpg=PA173&dq=callimachus%20democritus%20catalog&pg=PA173#v=onepage&q&f=false 173] n. 1.</ref>), but were concerned with individual topics. These are listed by the ''[[Suda]]'' as: ''A Chronological Pinax and Description of [[Theatre director#The director in theatre history|Didaskaloi]] from the Beginning'' and ''Pinax of the Vocabulary and Treatises of [[Democritus]]''.<ref>[http://www.stoa.org/sol-entries/kappa/227 ''Suda'' On Line]</ref>

==Later bibliographic ''pinakes''==
The term ''pinax'' was used for bibliographic catalogs beyond Callimachus. For example, [[Ptolemy-el-Garib]]'s catalog of [[Aristotle]]'s writings comes to us with the title ''Pinax (catalog) of Aristotle's writings''.<ref>[[Ingemar Düring]], ''Aristotle in the Ancient Biographical Tradition'' (Göteborg 1957), p. 221.</ref>

==Legacy==
The ''Pinakes'' proved indispensable to librarians for centuries. They became a model to use all over the [[Mediterranean]]. Their later influence can be traced to medieval times, even to the Arabic counterpart of the tenth century: [[Ibn al-Nadim]]'s ''Al-Fihrist'' ("Index"). Variations on this system were used in libraries until the late 1800s when [[Melvil Dewey]] developed the [[Dewey Decimal Classification]] in 1876, which is still in use today.<ref name ="alexandria4"/>

==Notes==
{{reflist|35em}}

==Bibliography==

===Texts and translations===
* The evidence concerning the Pinakes is collected by [[Rudolf Pfeiffer]] (ed.), ''Callimachus, vol. I: Fragmenta'', Oxford: Clarendon Press 1949, frr. 429-456 (with reference to the most important literature).
* Witty, F. J. "The Pinakes of Callimachus", ''Library Quarterly'' 28:1/4 (1958), 132&ndash;36.
* Witty, F. J. "The Other Pinakes and Reference Works of Callimachus", ''Library Quarterly'' 43:3 (1973), 237&ndash;44.

===Studies===
* [[Roger S. Bagnall|Bagnall, R. S.]] [http://archive.nyu.edu/bitstream/2451/28263/2/D172-Alexandria%20Library%20of%20Dreams.pdf "Alexandria: Library of Dreams"], ''Proceedings of the American Philosophical Society'' 46 (2002) 348&ndash;62.
* Blum, R. ''Kallimachos. The Alexandrian Library and the Origins of Bibliography'', trans. H.H. Wellisch (U. Wisconsin, 1991). ISBN 978-0-299-13170-8.
* Krevans, N. [https://books.google.com/books?id=CL4A5I3K-KsC&lpg=PA173&dq=callimachus%20democritus%20catalog&pg=PA173#v=onepage&q&f=false "Callimachus and the Pedestrian Muse"], in: A. Harder et al. (eds.) ''Callimachus II'', Hellenistic Groningana 6 (Groningen, 2002) 173&ndash;84.
* West, M. L. "The Sayings of Democritus", ''Classical Review'' (1969) 142.

{{coord missing|Egypt}}

{{Callimachus}}

[[Category:Defunct libraries]]
[[Category:Libraries in Egypt]]
[[Category:3rd-century BC books]]
[[Category:History of museums]]
[[Category:Ptolemaic Alexandria]]
[[Category:Library cataloging and classification]]
[[Category:Knowledge representation]]
[[Category:Bibliographies]]
<=====doc_Id=====>:839
<=====title=====>:
Document classification
<=====text=====>:
'''Document classification''' or '''document categorization''' is a problem in [[library science]], [[information science]] and [[computer science]]. The task is to assign a [[document]] to one or more [[Class (philosophy)|classes]] or [[Categorization|categories]]. This may be done "manually" (or "intellectually") or [[algorithmically]]. The intellectual classification of documents has mostly been the province of library science, while the algorithmic classification of documents is mainly in information science and computer science. The problems are overlapping, however, and there is therefore interdisciplinary research on document classification.

The documents to be classified may be texts, images, music, etc. Each kind of document possesses its special classification problems. When not otherwise specified, text classification is implied.

Documents may be classified according to their [[Subject (documents)|subjects]] or according to other attributes (such as document type, author, printing year etc.). In the rest of this article only subject classification is considered. There are two main philosophies of subject classification of documents: the content-based approach and the request-based approach.

=="Content-based" versus "request-based" classification==
'''Content-based classification''' is classification in which the weight given to particular subjects in a document determines the class to which the document is assigned. It is, for example, a common rule for classification in libraries, that at least 20% of the content of a book should be about the class to which the book is assigned.<ref>Library of Congress (2008). The subject headings manual. Washington, DC.: Library of Congress, Policy and Standards Division. (Sheet H 180: "Assign headings only for topics that comprise at least 20% of the work.")</ref> In automatic classification it could be the number of times given words appears in a document.

'''Request-oriented classification''' (or -indexing) is classification in which the anticipated request from users is influencing how documents are being classified. The classifier asks himself: “Under which descriptors should this entity be found?” and “think of all the possible queries and decide for which ones the entity at hand is relevant” (Soergel, 1985, p.&nbsp;230<ref>Soergel, Dagobert (1985). Organizing information: Principles of data base and retrieval systems. Orlando, FL: Academic Press.</ref>).

Request-oriented classification may be classification that is targeted towards a particular audience or user group. For example, a library or a database for feminist studies may classify/index documents differently when compared to a historical library.  It is probably better, however, to understand request-oriented classification as ''policy-based classification'': The classification is done according to some ideals and reflects the purpose of the library or database doing the classification. In this way it is not necessarily a kind of classification or indexing based on user studies. Only if empirical data about use or users are applied should request-oriented classification be regarded as a user-based approach.

==Classification versus indexing==
Sometimes a distinction is made between assigning documents to classes ("classification") versus assigning [[Subject (documents)|subjects]] to documents ("[[subject indexing]]") but as [[Frederick Wilfrid Lancaster]] has argued, this distinction is not fruitful. "These terminological distinctions,” he writes, “are quite meaningless and only serve to cause confusion” (Lancaster, 2003, p.&nbsp;21<ref>Lancaster, F. W. (2003). Indexing and abstracting in theory and practice. Library Association, London.</ref>). The view that this distinction is purely superficial is also supported by the fact that a classification system may be transformed into a [[thesaurus]] and vice versa (cf., Aitchison, 1986,<ref>Aitchison, J. (1986). "A classification as a source for thesaurus: The Bibliographic Classification of H. E. Bliss as a source of thesaurus terms and structure." Journal of Documentation, Vol. 42 No. 3, pp. 160-181.</ref> 2004;<ref>Aitchison, J. (2004). "Thesauri from BC2: Problems and possibilities revealed in an experimental thesaurus derived from the Bliss Music schedule." Bliss Classification Bulletin, Vol. 46, pp. 20-26.</ref> Broughton, 2008;<ref>Broughton, V. (2008). "A faceted classification as the basis of a faceted terminology: Conversion of a classified structure to thesaurus format in the Bliss Bibliographic Classification (2nd Ed.)." Axiomathes, Vol. 18 No.2, pp. 193-210.</ref> Riesthuis & Bliedung, 1991<ref>Riesthuis, G. J. A., & Bliedung, St. (1991). "Thesaurification of the UDC." Tools for knowledge organization and the human interface, Vol. 2, pp. 109-117. Index Verlag, Frankfurt.</ref>). Therefore, is the act of labeling a document (say by assigning a term from a [[controlled vocabulary]] to a document) at the same time to assign that document to the class of documents indexed by that term (all documents indexed or classified as X belong to the same class of documents).

==Automatic document classification (ADC)==
Automatic document classification tasks can be divided into three sorts: '''supervised document classification''' where some external mechanism (such as human feedback) provides information on the correct classification for documents, '''unsupervised document classification''' (also known as [[document clustering]]), where the classification must be done entirely without reference to external information, and '''semi-supervised document classification''',<ref>
Rossi, R. G., Lopes, A. d. A., and Rezende, S. O. (2016). Optimization and label propagation in bipartite heterogeneous networks to improve transductive classification of texts.
Information Processing & Management, 52(2):217–257.
</ref> where parts of the documents are labeled by the external mechanism. There are several software products under various license models available.<ref>[http://www.ling.ohio-state.edu/~kbaker/Automatic_Interactive_Document_Classification.pdf An Interactive Automatic Document Classification Prototype]</ref><ref>[https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An Interactive Automatic Document Classification Prototype] {{wayback|url=https://seer.lcc.ufmg.br/index.php/jidm/article/download/43/41An |date=20150424122349 }}</ref><ref>[http://www.artsyltech.com/da_classification.htmlAutomatic Document Classification - Artsyl]{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}</ref><ref>[http://www.abbyy.com/ocr_sdk_windows/what_is_new/classification/ ABBYY FineReader Engine 11 for Windows]</ref>

== Techniques ==
Automatic document classification techniques include:
* [[Expectation maximization]] (EM)
* [[Naive Bayes classifier]]
* [[tf–idf]]
* [[Instantaneously trained neural networks]]
* [[Latent semantic indexing]]
* [[Support vector machines]] (SVM)
* [[Artificial neural network]]
* [[k-nearest neighbor algorithm|K-nearest neighbour algorithms]]
* [[Decision tree learning|Decision trees]] such as [[ID3 algorithm|ID3]] or [[C4.5 algorithm|C4.5]]
* [[Concept Mining]]
* [[Rough set]]-based classifier
* [[Soft set]]-based classifier
* [[Multiple-instance learning]]
* [[Natural language processing]] approaches

== Applications ==
Classification techniques have been applied to
* [[spam filter]]ing, a process which tries to discern [[E-mail spam]] messages from legitimate emails
* email [[routing]], sending an email sent to a general address to a specific address or mailbox depending on topic<ref>Stephan Busemann, Sven Schmeier and Roman G. Arens (2000). Message classification in the call center. In Sergei Nirenburg, Douglas Appelt, Fabio Ciravegna and Robert Dale, eds., Proc. 6th Applied Natural Language Processing Conf. (ANLP'00), pp. 158-165, ACL.</ref>
* [[language identification]], automatically determining the language of a text
* genre classification, automatically determining the genre of a text<ref>{{Citation|last = Santini| first = Marina | last2 = Rosso| first2 = Mark| title = Testing a Genre-Enabled Application: A Preliminary Assessment| url = http://www.bcs.org/upload/pdf/ewic_fd08_paper7.pdf| series = BCS IRSG Symposium: Future Directions in Information Access| place = London, UK | pages= 54–63| year = 2008 }}</ref>
* [[Readability|readability assessment]], automatically determining the degree of readability of a text, either to find suitable materials for different age groups or reader types or as part of a larger [[text simplification]] system
* [[sentiment analysis]], determining the attitude of a speaker or a writer with respect to some topic or the overall contextual polarity of a document.
* Article triage, selecting articles that are relevant for manual literature curation, for example as is being done as the first step to generate manually curated annotation databases in biology.<ref>{{Cite journal
 | pmid = 18834495
| year = 2008
| author1 = Krallinger
| first1 = M
| title = Overview of the protein-protein interaction annotation extraction task of Bio ''Creative'' II
| journal = Genome Biology
| volume = 9 Suppl 2
| pages = S4
| last2 = Leitner
| first2 = F
| last3 = Rodriguez-Penagos
| first3 = C
| last4 = Valencia
| first4 = A
| doi = 10.1186/gb-2008-9-s2-s4
| pmc = 2559988
}}</ref>

== See also ==
{{colbegin}}
* [[Categorization]]
* [[Classification (disambiguation)]]
* [[Compound term processing]]
* [[Concept-based image indexing]]
* [[Content-based image retrieval]]
* [[Document]]
* [[Supervised learning]], [[unsupervised learning]]
* [[Document retrieval]]
* [[Document clustering]]
* [[Information retrieval]]
* [[Knowledge organization]]
* [[Knowledge Organization System]]
* [[Library classification]]
* [[Machine learning]]
* [[Native Language Identification]]
* [[String metrics]]
* [[Subject (documents)]]
* [[Subject indexing]]
* [[Text mining]], [[web mining]], [[concept mining]]
{{colend}}

== Further reading ==
* Fabrizio Sebastiani. [http://arxiv.org/pdf/cs.ir/0110053 Machine learning in automated text categorization]. ACM Computing Surveys, 34(1):1–47, 2002.
* Stefan Büttcher, Charles L. A. Clarke, and Gordon V. Cormack. [http://www.ir.uwaterloo.ca/book/ Information Retrieval: Implementing and Evaluating Search Engines]. MIT Press, 2010.

==References==
{{Reflist}}

== External links ==
* [http://isp.imm.dtu.dk/thor/projects/multimedia/textmining/node11.html Introduction to document classification]
* [http://www.cs.technion.ac.il/~gabr/resources/atc/atcbib.html Bibliography on Automated Text Categorization]
* [http://liinwww.ira.uka.de/bibliography/Ai/query-classification.html Bibliography on Query Classification]
* [http://www.gabormelli.com/RKB/Text_Classification_Task Text Classification] analysis page
* [http://www.nltk.org/book/ch06.html Learning to Classify Text - Chap. 6 of the book Natural Language Processing with Python] (available online)
* [http://techtc.cs.technion.ac.il TechTC - Technion Repository of Text Categorization Datasets]
* [http://www.daviddlewis.com/resources/testcollections/ David D. Lewis's Datasets]
* [http://www.biocreative.org/tasks/biocreative-iii/ppi/ BioCreative III ACT (article classification task) dataset]

[[Category:Information science]]
[[Category:Natural language processing]]
[[Category:Knowledge representation]]
[[Category:Data mining]]
[[Category:Machine learning]]
<=====doc_Id=====>:842
<=====title=====>:
Brand page
<=====text=====>:
A '''brand page''' (also known as a '''page''' or '''fan page'''), in online social networking parlance, is a profile on a social networking website which is considered distinct from an actual [[user profile]] in that it is created and managed by at least one other registered user as a representation of a non-personal [[online identity]]. This feature is most used to represent the brands of organizations associated with, properties owned by, or general interests favored by a user of the hosting network.

While also being potentially manageable by more than one registered user, pages are distinguished from [[Group (online social networking)|groups]] in that pages are usually designed for the managers to direct messages and posts to subscribing users (akin to a [[newsletter]] or [[blog]]) and promote a brand, while groups are usually and historically formed for discussion purposes.

==History==
Prior to 2007, only a few websites made use of non-personal profile pages. [[Last.fm]], established in 2002, used its music recommendation service to automatically generate "artist pages" which serve as portals for biographies, events and artist-related playlists. This approach, however, is not explicitly controlled by artists or music groups because of the automatic nature of artist pages; pages, for example, could be created from erroneous misspellings and miscredits of works which are accepted as-is by the Audioscrobbler recommendation service used by Last.fm. Furthermore, Last.fm has never advertised itself as a social networking service, despite accruing myriad social features since 2002.

The most high-profile usage of this model is [[Facebook]]'s Pages (formerly known as "Fan Page" until 2010) feature, launched in 2007; one could "be a fan of" a page until April 2010, when the parlance was replaced with "Like".<ref>{{cite web|url=http://www.allfacebook.com/2010/04/facebooks-become-a-fan-officially-switches-to-like/ | title=Facebook’s "Become A Fan" Officially Switches To "Like" | author=[[Nick O'Neill]]|publisher = AllFacebook.com|date = April 19, 2010 <!-- 4:37 PM --> }}</ref> [[Foursquare]], a location-oriented social networking site, launched its "Brands" feature allowing for the creation of specialized brand pages in January 2010 (with [[Intel]] being the first user), but they did not become "self-serve" (controllable by individuals employed by page brand owners) until August 2011.<ref>{{cite web|url = http://blog.foursquare.com/2011/08/02/pages-are-now-self-serve-a-new-home-for-brands-and-organizations-on-foursquare/|title = Pages are now self-serve! A new home for brands and organizations on foursquare.|date = Aug 2, 2011|publisher = Foursquare}}</ref> [[LinkedIn]], an enterprise-oriented social networking service, launched "Company Pages" in November 2010.<ref>{{cite web|url = http://blog.linkedin.com/2010/11/01/linkedin-company-pages/|title = Recommend your favorite products and services on LinkedIn Company Pages|author = Ryan Roslansky|publisher = LinkedIn|date = November 1, 2010}}</ref> [[Google+]], the current social networking service operated by [[Google]], launched its own "Pages" feature in October 2011.<ref>{{cite web|url = http://googleblog.blogspot.com/2011/11/google-pages-connect-with-all-things.html|title = Google+ Pages: connect with all the things you care about|publisher = Google|date = 11-07-2011 <!-- 10:01:00 AM --> }}</ref> On November 19th, 2012, [[Amazon.com|Amazon]] announced Amazon Pages giving brands self-service control over their presence on the site.<ref>{{cite web|url = http://techcrunch.com/2012/11/20/amazon-offers-amazon-pages-for-brands-to-customize-with-their-own-urls-and-amazon-posts-for-social-media-marketing/|title = Amazon Offers ‘Amazon Pages’ For Brands To Customize With Their Own URLs, And ‘Amazon Posts’ For Social Media Marketing|author = TechCrunch|date = November 20, 2012}}</ref> On 8 December, [[Twitter]] announced that it would roll out "brand pages" as part of a major user interface redesign in 2012.<ref>{{cite web|url = http://advertising.twitter.com/2011/12/let-your-brand-take-flight-on-twitter.html|title = Let your brand take flight on Twitter with enhanced profile pages|publisher = Twitter Advertising Blog|author = TwitterAds|date = December 8, 2011}}</ref><ref>{{cite web|url = http://adage.com/article/digital/twitter-joins-facebook-google-launches-brand-pages-marketers/231448/|title = Twitter Joins Facebook, Google, Launches 'Brand Pages' for Marketers|author = Cotton Delo|date = December 8, 2011}}</ref>

==Features==
Increasingly, brand pages make use of the following features: 
* Header banners
* The ability to post blogs or replies on the brand page in the name of the brand page
* The ability to administer multiple pages
* Photos
* Video
* Maps (including physical location of the page)
* Subscribers
* Other apps

Twitter made use of header banners in their launch of brand pages, and Facebook made use of "cover photos" in their re-design of brand pages in March 2011.

==Uses==
Organizations and brands regularly make use of pages in order to syndicate news and upcoming events, especially off-site blog posts, to subscribing users. Page subscription numbers can also be used as a metric of trust or interest in the associated brand.

Interests can also be indexed as pages, and are often the basis for the formation of mass social movements (i.e., the [[Arab Spring]], [[Occupy Wall Street]]).

===Newsroom accounts===
Pages are also used as newsroom accounts.

A '''[[newsroom]] account''' refers to any microblogging or social networking account branded by or owned by a publishing or broadcasting organization which is dedicated solely to syndicating content from a particular category of content as published on the original website of the organization. Such accounts have come into increased usage by news organizations as means by which:
# A news organization's presence on a social networking or microblogging website is increased
# A news organization can specialize content syndication to selective users who wish to subscribe

News organizations who make use of multiple newsroom accounts typically allow for either online editors or multiple employed authors to edit and update the syndications of newsroom content. Such accounts are typically marked by graphic icons which make use of the brand symbol combined with distinct colors assigned to each account.

Examples of newsroom accounts and pages include the Facebook pages for both ''[[The Guardian]]'' and the newspaper's Technology newsroom.

==Impact==

===Pseudonyms===
The usage of pseudonyms on social networking services, long considered a preserve of user privacy, has been partially affected by the promotion of pseudonyms, as social networking services have encouraged users to create pages for pseudonyms and implemented legal name requirements for user profile registration (i.e., New York resident Stefani Germanotta keeping a separate personal user profile under her legal name while maintaining a fan page under her stage name and pseudonym [[Lady Gaga]]).

===Interest-based connections===
As pages can be created to represent interests, the number of attempts to create vertical social networking services (i.e., [[Ning (website)|Ning]]) has leveled off in the 2010s. [[Social network advertising]] can also be targeted to users based upon their page subscriptions.

==See also==
* [[Fansite]]
* [[Landing page]]

==References==
{{reflist}}

{{Online social networking}}
{{Microblogging}}

{{DEFAULTSORT:Page (online social networking)}}
[[Category:Software features]]
[[Category:Knowledge representation]]
[[Category:Identity management]]
<=====doc_Id=====>:845
<=====title=====>:
Dublin Core
<=====text=====>:
{{Use dmy dates|date=July 2012}}
The '''Dublin Core Schema''' is a small set of vocabulary terms that can be used to describe web resources (video, images, web pages, etc.), as well as physical resources such as books or CDs, and objects like artworks.<ref>{{cite web|url=http://dublincore.org/documents/dcmi-type-vocabulary/index.shtml |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}</ref> The full set of Dublin Core metadata terms can be found on the Dublin Core Metadata Initiative (DCMI) website.<ref>{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |accessdate=5 April 2013}}</ref> The original set of 15 classic<ref>{{cite web|url=http://dublincore.org/specifications/ |title=DCMI Specifications |publisher=Dublincore.org |date=14 December 2009 |accessdate=5 April 2013}}</ref> metadata terms, known as the Dublin Core Metadata Element Set<ref name="DCMES">{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |accessdate=5 April 2013}}</ref> are endorsed in the following standards documents:

* IETF RFC 5013<ref>[http://www.ietf.org/rfc/rfc5013.txt The Dublin Core Metadata Element Set], Dublin Core Metadata Initiative, August 2007</ref>
* ISO Standard 15836-2009<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_ics/catalogue_detail_ics.htm?csnumber=52142 |title=ISO 15836:2009 - Information and documentation - The Dublin Core metadata element set |publisher=Iso.org |date=18 February 2009 |accessdate=5 April 2013}}</ref>
* NISO Standard Z39.85<ref>{{cite web|url=http://www.niso.org/kst/reports/standards?step=2&gid=None&project_key=9b7bffcd2daeca6198b4ee5a848f9beec2f600e5 |title=NISO Standards - National Information Standards Organization |publisher=Niso.org |date=22 May 2007 |accessdate=5 April 2013}}</ref>

Dublin Core Metadata may be used for multiple purposes, from simple resource description, to combining metadata vocabularies of different [[Metadata#Metadata standards|metadata standards]], to providing interoperability for metadata vocabularies in the [[Linked Data]] cloud and [[Semantic Web]] implementations.

== Background ==
"Dublin" refers to [[Dublin, Ohio]], USA where the schema originated during the 1995 invitational OCLC/NCSA Metadata Workshop,<ref>[http://dublincore.org/workshops/dc1/ OCLC/NCSA Metadata Workshop]</ref> hosted by the [[Online Computer Library Center]] (OCLC), a library consortium based in Dublin, and the [[National Center for Supercomputing Applications]] (NCSA).  "Core" refers to the metadata terms as "broad and generic being usable for describing a wide range of resources".<ref name="DCMES"/> The semantics of Dublin Core were established and are maintained by an international, cross-disciplinary group of professionals from [[librarianship]], [[computer science]], [[text encoding]], [[museum]]s, and other related fields of scholarship and practice.

Starting in 2000, the Dublin Core community focused on "[[application profile]]s" &ndash; the idea that metadata records would use Dublin Core together with other specialized vocabularies to meet particular implementation requirements. During that time, the World Wide Web Consortium's work on a generic data model for metadata, the [[Resource Description Framework]] (RDF), was maturing. As part of an extended set of DCMI Metadata Terms, Dublin Core became one of the most popular vocabularies for use with RDF, more recently in the context of the Linked Data movement.<ref>{{cite web|title=DCMI Metadata Basics|publisher=dublincore.org/metadata-basics/}}</ref>

The '''Dublin Core Metadata Initiative''' (DCMI)<ref>{{cite web|url=http://dublincore.org/ |title=DCMI Home: Dublin Core® Metadata Initiative (DCMI) |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref> provides an open forum for the development of interoperable online [[metadata standards]] for a broad range of purposes and of business models. DCMI's activities include consensus-driven working groups, global conferences and workshops, standards liaison, and educational efforts to promote widespread acceptance of metadata standards and practices. In 2008, DCMI separated from OCLC and incorporated as an independent entity.<ref>{{cite web | title=OCLC Research and the Dublin Core Metadata Initiative | url=http://www.oclc.org/research/activities/past/orprojects/dublincore/default.htm | accessdate=21 April 2010}}</ref>

Currently, any and all changes that are made to the Dublin Core standard, are reviewed by a DCMI Usage Board within the context of a DCMI Namespace Policy (DCMI-NAMESPACE). This policy describes how terms are assigned and also sets limits on the amount of editorial changes allowed to the labels, definitions, and usage comments.<ref>{{cite web|url=http://dublincore.org/documents/dces/ |title=Dublin Core Metadata Element Set, Version 1.1 |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref>

== Levels of the standard ==
The Dublin Core standard originally includes two levels: Simple and Qualified. '''Simple Dublin Core''' comprised 15 elements; '''Qualified Dublin Core''' included three additional elements (Audience, Provenance and RightsHolder), as well as a group of element refinements (also called qualifiers) that could refine the semantics of the elements in ways that may be useful in resource discovery.

Since 2012 the two have been incorporated into the DCMI Metadata Terms as a single set of terms using the [[Resource Description Framework]] (RDF).<ref name="dublincore1">{{cite web|url=http://dublincore.org/documents/dcmi-terms/ |title=DCMI Metadata Terms |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref> The full set of elements is found under the namespace http://purl.org/dc/terms/. Because the definition of the terms often contains domains and ranges, which may not be compatible with the pre-RDF definitions used for the original 15 Dublin Core elements, there is a separate namespace for the original 15 elements as previously defined: http://purl.org/dc/elements/1.1/.<ref>[http://dublincore.org/documents/dces/ Dublin Core Metadata Element Set, version 1.1]</ref>

=== Dublin Core Metadata Element Set Version 1.1===
The original '''Dublin Core Metadata Element Set''' consists of 15 metadata elements:<ref name="DCMES"/>
# Title
# Creator
# Subject
# Description
# Publisher
# Contributor
# Date
# Type
# Format
# Identifier
# Source
# Language
# Relation
# Coverage
# Rights

Each Dublin Core element is optional and may be repeated. The DCMI has established standard ways to refine elements and encourage the use of encoding and vocabulary schemes. There is no prescribed order in Dublin Core for presenting or using the elements. The Dublin Core became ISO 15836 standard in 2006 and is used as a base-level data element set for the description of learning resources in the [[ISO/IEC 19788]]-2 Metadata for learning resources (MLR) &ndash; Part 2: Dublin Core elements, prepared by the [[ISO/IEC JTC1/SC36|ISO/IEC JTC1 SC36]].

Full information on element definitions and term relationships can be found in the Dublin Core Metadata Registry.<ref name="registry">[http://dcmi.kc.tsukuba.ac.jp/dcregistry/ Dublin Core Metadata Registry]</ref>

==== Example of code ====
: {{code|2=html4strict|1=<meta name="DC.Format" content="video/mpeg; 10 minutes">}}
: {{code|2=html4strict|1=<meta name="DC.Language" content="en" >}}
: {{code|2=html4strict|1=<meta name="DC.Publisher" content="publisher-name" >}}
: {{code|2=html4strict|1=<meta name="DC.Title" content="HYP" >}}

==== An example of use [and mention] of D.C. (by [[WebCite]]) ====

At the web page which serves as the "archive" form for [[WebCite]],<ref name="WebCite_archive_form_(web_page)">{{cite web
| url          = http://webcitation.org/archive
| title        = WebCite® archive form
| quote        = Metadata (optional)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These are Dublin Core elements. [...]
| publisher    = [[WebCite]]
}}
</ref> it says, in part: "Metadata (optional)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These are Dublin Core elements. [...]".

=== Qualified Dublin Core (deprecated in 2012<ref>{{cite web|url=http://dublincore.org/documents/2000/07/11/dcmes-qualifiers/ |title=Dublin Core Qualifiers |publisher=Dublincore.org |date= |accessdate=2015-12-04}}</ref>)===
Subsequent to the specification of the original 15 elements, an ongoing process to develop exemplary terms extending or refining the Dublin Core Metadata Element Set (DCMES) was begun. The additional terms were identified, generally in working groups of the Dublin Core Metadata Initiative, and judged by the DCMI Usage Board to be in conformance with principles of good practice for the qualification of Dublin Core metadata elements.

Elements refinements make the meaning of an element narrower or more specific. A refined element shares the meaning of the unqualified element, but with a more restricted scope. The guiding principle for the qualification of Dublin Core elements, colloquially known as the ''Dumb-Down Principle'',<ref>[http://dublincore.org/workshops/dc8/dcgrammar/tsld008.html Dumb-Down Principle for qualifiers]</ref> states that an application that does not understand a specific element refinement term should be able to ignore the qualifier and treat the metadata value as if it were an unqualified (broader) element. While this may result in some loss of specificity, the remaining element value (without the qualifier) should continue to be generally correct and useful for discovery.

In addition to element refinements, Qualified Dublin Core includes a set of recommended encoding schemes, designed to aid in the interpretation of an element value. These schemes include controlled vocabularies and formal notations or parsing rules. A value expressed using an encoding scheme may thus be a token selected from a controlled vocabulary (for example, a term from a classification system or set of subject headings) or a string formatted in accordance with a formal notation, for example, "2000-12-31" as the ISO standard expression of a date. If an encoding scheme is not understood by an application, the value may still be useful to '''human reader'''.

'''Audience, Provenance''' and '''RightsHolder''' are elements, but not part of the Simple Dublin Core 15 elements. Use Audience, Provenance and RightsHolder only when using Qualified Dublin Core.
DCMI also maintains a small, general vocabulary recommended for use within the element Type. This vocabulary currently consists of 12 terms.<ref name="registry"/>

=== DCMI Metadata Terms ===
The Dublin Core Metadata Initiative (DCMI) Metadata Terms is the current set of the Dublin Core vocabulary.<ref name="dublincore1"/> This set includes the fifteen terms of the Dublin Core Metadata Element Set (in ''italic''), as well as the qualified terms. Each term has a unique URI in the namespace http://purl.org/dc/terms, and all are defined as [[Resource Description Framework|RDF]] properties.

{{columns-list|4|
*abstract
*accessRights
*accrualMethod
*accrualPeriodicity
*accrualPolicy
*alternative
*audience
*available
*bibliographicCitation
*conformsTo
*''contributor''
*''coverage''
*created
*''creator''
*''date''
*dateAccepted
*dateCopyrighted
*dateSubmitted
*''description''
*educationLevel
*extent
*''format''
*hasFormat
*hasPart
*hasVersion
*''identifier''
*instructionalMethod
*isFormatOf
*isPartOf
*isReferencedBy
*isReplacedBy
*isRequiredBy
*issued
*isVersionOf
*''language''
*license
*mediator
*medium
*modified
*provenance
*''publisher''
*references
*''relation''
*replaces
*requires
*''rights''
*rightsHolder
*''source''
*spatial
*''subject''
*tableOfContents
*temporal
*''title''
*''type''
*valid
}}

== Syntax ==
Syntax choices for Dublin Core metadata depends on a number of variables, and "one size fits all" prescriptions rarely apply. When considering an appropriate syntax, it is important to note that Dublin Core concepts and semantics are designed to be syntax independent and are equally applicable in a variety of contexts, as long as the metadata is in a form suitable for interpretation both by machines and by human beings.

The '''Dublin Core Abstract Model'''<ref>[http://dublincore.org/documents/abstract-model/ Dublin Core Abstract Model]</ref> provides a reference model against which particular Dublin Core encoding guidelines can be compared, independent of any particular encoding syntax. Such a reference model allows implementers to gain a better understanding of the kinds of descriptions they are trying to encode and facilitates the development of better mappings and translations between different syntax.

== Some applications ==
One [[Document Type Definition]] based on Dublin Core is the [http://www.ibiblio.org/osrt/omf/ Open Source Metadata Framework] (OMF) specification. OMF is in turn used by [[Rarian]] (superseding [[ScrollKeeper]]), which is used by the [[GNOME]] desktop and [[KDE]] help browsers and the ScrollServer documentation server. [[PBCore]] is also based on Dublin Core. The [[Zope]] [[Zope Content Management Framework|CMF's]] Metadata products, used by the [[Plone (content management system)|Plone]], [[ERP5]], the Nuxeo CPS [[Content management system]]s, [[SimpleDL]], and [[FedoraCommons]] also implement Dublin Core. The [[EPUB]] [[e-book]] format uses Dublin Core metadata in the [[OPF (file format)|OPF file]].<ref>{{cite web|url=http://www.idpf.org/epub/20/spec/OPF_2.0_latest.htm#Section2.2|title=Open Packaging Format (OPF) 2.0.1 – 2.2: Publication Metadata|publisher=[[International Digital Publishing Forum]]|accessdate=12 September 2013}}</ref> [[eXo Platform]] also implements Dublin Core.

DCMI also maintains a list of projects using Dublin Core<ref>{{cite web|url=http://dublincore.org/projects/|title=DCMI Projects - Alphabetical|publisher=DCMI|accessdate=15 March 2013}}</ref> on its website.

== See also ==
* [[Metadata registry]]
* [[Metadata Object Description Schema]]
* [[Wikiversity:Digital Libraries/Metadata|Metadata from Wikiversity]]
* [[Semantic Web]]
* [[Ontology (information science)]]
* [[Open Archives Initiative]]
* [[Controlled vocabulary]]
* [[Interoperability]]
* [[Asset Description Metadata Schema]] ([http://www.w3.org/TR/vocab-adms/ ADMS]), a metadata standard maintained by the [[World Wide Web Consortium]] for describing semantic standards. Implemented on Joinup.<ref>{{cite web|url=https://joinup.ec.europa.eu/catalogue/all?filters=bs_current_version:true{{!}}Joinup |title=Joinup &#124; Joinup |publisher=Joinup.ec.europa.eu |date=2015-10-22 |accessdate=2015-12-04}}</ref> 
* [[Metadata Encoding and Transmission Standard]] (METS), maintained by the [[Library of Congress]] for the [[Digital Library Federation]]
* [[Preservation Metadata: Implementation Strategies]] (PREMIS)

=== Related software ===
* [[Dublin Core Meta Toolkit]] (Conversion of Access, MySQL, or CSV data to DublinCore metadata)
* [[Fedora (software)|Fedora]] repository architecture and Project (An open-source software system capable of implementing [[OAI-PMH]] (and thus Dublin Core).
* [[Omeka]], A free, open-source, unqualified Dublin-Core compliant web-publishing system for digital archives.
* The [http://archiviststoolkit.org/ Archivist's Toolkit] is a self-described as an "Archival Data Management system" able to work with the Dublin Core format. It will soon be merged with [[Archon (software)|Archon]], which is ambiguous as to its OAI support.
* [[ICA-AtoM]], a web-based archival description/publication software that can serve as an OAI-PMH repository and uses OAI-PMH as the main language for remote data exchange

== References ==
{{Reflist|2}}

== Further reading ==
* {{cite book |title= Organising Knowledge in a Global Society |last= Harvey  |first= Ross |authorlink= |author2=Philip Hider |year= 2004 |publisher= Charles Sturt University |location= Wagga Wagga NSW |isbn= 1-876938-66-8 |page= |pages= |url= |accessdate=}}
* [https://www.inf.unibz.it/courses/images/stories/2005_2006/Digital_Libraries/dini-less-5-6.ppt "Lecture slides about Dublin Core"], by Luca Dini, lecturer at the [[Free University of Bolzano]]

== External links ==
* [http://dublincore.org/ Dublin Core Metadata Initiative]
* [http://wiki.dublincore.org/index.php/User_Guide Dublin Core usage guide]
* [http://xml.coverpages.org/ni2005-03-21-a.html Dublin Core Metadata Initiative Publishes DCMI Abstract Model] (''Cover Pages'', March 2005)
* [http://www.loc.gov/standards/mods/v3/mods-userguide-3-0.html Metadata Object Description Schema (MODS)]
*[http://www.dublincoregenerator.com/ The Dublin Core Generator: A tool for generating Dublin Core code]
*[http://library.kr.ua/dc/dceditunie.html The Dublin Core Generator-Editor: Free tool for extracting-editing Dublin Core HTML code]

{{Semantic Web}}
{{Authority control}}

[[Category:Archival science]]
[[Category:Bibliography file formats]]
[[Category:Digital libraries]]
[[Category:Information management]]
[[Category:Interoperability]]
[[Category:ISO standards]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Metadata standards]]
[[Category:Museology]]
[[Category:Records management]]
[[Category:Reference models]]
[[Category:Semantic Web]]
<=====doc_Id=====>:848
<=====title=====>:
GermaNet
<=====text=====>:
{{primary sources|date=November 2011}}
'''GermaNet''' is a lexical-semantic net for the [[German language]] that relates [[noun]]s, [[verb]]s, and [[adjective]]s semantically by grouping lexical units that express the same concept into ''[[synset]]s'' and by defining [[semantic]] relations between these synsets.<ref name="Storjohann2010">{{cite book|author=Petra Storjohann|title=Lexical-semantic relations: theoretical and practical perspectives|url=https://books.google.com/books?id=OYBWObJ547AC&pg=PA165|accessdate=16 November 2011|date=23 June 2010|publisher=John Benjamins Publishing Company|isbn=978-90-272-3138-3|pages=165–}}</ref> GermaNet has much in common with the English [[WordNet]] and can be viewed as an on-line [[thesaurus]] or a light-weight [[ontology (information science)|ontology]]. GermaNet has been developed and maintained within various projects at the research group for General and Computational Linguistics, [[University of Tübingen]] since 1997. It has been integrated into the [[EuroWordNet]], a multilingual lexical-semantic database.<ref name="homepage">[http://www.sfs.uni-tuebingen.de/lsd/index.shtml GermaNet homepage]</ref>

==Database==

===Contents===
GermaNet  partitions the lexical space into a set of concepts that are interlinked by semantic relations. A semantic concept is modeled by a ''[[synset]]''. A synset is a set of words (called lexical units) where all the words are taken to have (almost) the same meaning. Thus a synset is a set-representation of the semantic relation of synonymy, which means that it consists of a list of lexical units and a definition (paraphrase). The lexical units in turn have frames (which specify syntactic valence) and examples of their use.<ref name="GernEdiT">V. Henrich, E. Hinrichs. 2010. [http://www.lrec-conf.org/proceedings/lrec2010/pdf/264_Paper.pdf GernEdiT - The GermaNet Editing Tool]. In: ''Proceedings of the Seventh Conference on International Language Resources and Evaluation''.</ref>
Just as in WordNet, for each word category the semantic space is divided into a number of [[semantic field]]s closely related to major nodes in the semantic network: ''Ort'', or "location", ''Körper'', or "body", etc.<ref name="homepage"/>

The following is an up-to-date statistics of GermaNet's version 6.0 contents (release April 2011):
 
*Number of synsets: 69594
**Of which adjectives: 5991
**Of which nouns: 53753
**Of which verbs: 9850
*Number of lexical units: 93407
**Of which adjectives: 8582
**Of which nouns: 71844
**Of which verbs: 12981 <ref name="homepage"/>

===Format===
All GermaNet data is stored in a relational [[PostgreSQL]] 5 database. The database model follows the internal structure of GermaNet: there are tables to store synsets, lexical units, conceptual and lexical relations, etc.<ref name="GernEdiT"/> The distribution format of all GermaNet data is [[XML]]. The two types of files, one for synsets and the other for relations, represent all data that is available in the GermaNet database.

==Interfaces==
There are several [[Application Programming Interface]]s (API) available for [[Java (programming language)|Java]]<ref name="api">[http://www.sfs.uni-tuebingen.de/lsd/tools.shtml GermaNet APIs in Java]</ref> and for [[Perl]]. These APIs are distributed freely and provide easy access to all information in various versions of GermaNet.

==Licenses==
GermaNet 6.0 (released April 2011) can be distributed under one of the following types of [[software license agreement|license agreements]]: ''Academic Research Agreement'', ''Research and Development Agreement'', or ''Commercial Agreement''. GermaNet is free for academic use.

==Applications==
GermaNet has been used for a variety of applications, including semantic analysis, shallow recognition of implicit document structure, compound analysis;<ref>Manuela Kunze and Dietmar Rösner. 2004. Issues in Exploiting GermaNet as a Resource in Real Applications.</ref> for analyzing selectional preferences,<ref>Sabine Schulte im Walde, 2004. GermaNet Synsets as Selectional Preferences in Semantic Verb Clustering.</ref> for word sense disambiguation,<ref>Saito et al., 2002. Evaluation of GermanNet: Problems Using GermaNet for Automatic Word Sense Disambiguation.</ref> etc.
== See also==
* [[Hyponym]]
* [[Is-a]]
* [[Machine-readable dictionary]]
* [[Ontology (information science)]]
* [[Semantic network]]
* [[Semantic Web]]
* [[Synonym Ring]]
* [[Taxonomy (general)|Taxonomy]]
* [[ThoughtTreasure]]
* [[UBY-LMF]]
* [[Word sense disambiguation]]

==References==
{{Reflist}}

{{Authority control}}
[[Category:German language]]
[[Category:Thesauri]]
[[Category:Lexical databases]]
[[Category:Knowledge representation]]
[[Category:Computational linguistics]]
[[Category:Online dictionaries]]
<=====doc_Id=====>:851
<=====title=====>:
Template:InfoMaps
<=====text=====>:
{{Sidebar
|name     = InfoMaps
|topimage = [[Image:Screen_Shot_2012-07-19_at_5.56.57_PM.png |165px|Part of "School of Athens" by Raphael (Raffaelo Sanzio, 1483-1520)]]
|title    = [[Information mapping]]
|bodyclass = hlist
|titleclass= navbox-title
|headingstyle = background:transparent;

|heading1 = Topics & fields
|content1style = padding-bottom:0.9em;
|content1 = 
* [[Business decision mapping]]  
* [[Cognitive map]]  
* [[Data visualization]]  
* [[Decision tree]] 
* [[Educational psychology]] 
* [[Educational technology]] 
* [[Graphic communication]] 
* [[Information design]]  
* [[Information graphics]]  
* [[Interactive visualization]]  
* [[Knowledge visualization]] 
* [[Mental model]]  
* [[Morphological analysis (problem-solving)|Morphological analysis]]  
* [[Visual analytics]]  
* [[Visual language]]

|heading2 = Tree-like approaches
|content2style = padding-bottom:0.9em;
|content2 = 
* [[Cladistics]]  
* [[Argument map]] 
* [[Cognitive map]]
* [[Concept lattice]] 
* [[Concept map]]ping 
* [[Conceptual graph]] 
* [[Dendrogram]]  
* [[Graph drawing]]  
* [[Hyperbolic tree]]  
* [[Layered graph drawing]]  
* [[Mental model]]  
* [[Mind map]]ping 
* [[Object-role modeling]] 
* [[Organizational chart]]  
* [[Radial tree]] 
* [[Semantic network]] 
* [[Sociogram]]  
* [[Timeline]]   
* [[Topic Maps]]  
* [[Tree structure]]   

|heading3 =See also
|content3style = padding-bottom:0.9em;
|content3 =
* [[Diagrammatic reasoning]]
* [[Entity-relationship model]]
* [[Geovisualization]]  
* [[List of concept- and mind-mapping software]]  
* [[Olog]]  
* [[Semantic web]]  
* [[Treemapping]]  
* [[Wicked problem]]  

|tnavbarstyle = border-top:1px solid #aaa;
}}<noinclude>
[[Category:Knowledge representation]]
</noinclude>
<=====doc_Id=====>:854
<=====title=====>:
Composite Capability/Preference Profiles
<=====text=====>:
'''Composite Capability/Preference Profiles''' ('''CC/PP''') is a specification for defining capabilities and preferences (also known as 'delivery context') of [[user agents]]. CC/PP is a [[vocabulary extension]] of the [[Resource Description Framework]] (RDF). Delivery context can be used to guide the process of tailoring content for a [[user agent]].

The CC/PP specification is maintained by the [[World Wide Web Consortium|W3C]]'s [[UWAWG|Ubiquitous Web Applications Working Group (UWAWG)]] Working Group.

== History ==
* Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0 became a W3C recommendation on 15 January 2004.
* A "Last-Call Working-Draft" of CC/PP 2.0 was issued in April 2007

== See also ==
* [[Resource Description Framework|Resource Description Framework (RDF)]]
* [[UAProf|User Agent Profile (UAProf)]]
* [[WURFL|Wireless Universal Resource File (WURFL)]]

== External links ==
* [http://www.w3.org/Mobile/CCPP/ W3C CC/PP Information Page]
* [http://www.w3.org/TR/CCPP-struct-vocab2/ Newest version of CC/PP: Structure and Vocabularies]
* [http://www.w3.org/TR/2004/REC-CCPP-struct-vocab-20040115/ Composite Capability/Preference Profiles (CC/PP): Structure and Vocabularies 1.0]
* [http://www.w3.org/2001/di/ W3C Device Independence working group]
* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite/ CC/PP: Structure and Vocabularies Test Suite]
* [http://www.w3.org/2003/07/ccpp-SV-PR/test-suite-20030827/implementation-report.html CC/PP: Structure and Vocabularies Implementation Report]
* [http://www.w3.org/Consortium/Offices/Presentations/CCPP/ CC/PP presentation]
* [http://java.sun.com/j2ee/ccpp/ Sun J2EE CC/PP Processing Tools]

{{DEFAULTSORT:Composite Capability Preference Profiles}}
[[Category:Knowledge representation]]
[[Category:World Wide Web Consortium standards]]
<=====doc_Id=====>:857
<=====title=====>:
Spatial–temporal reasoning
<=====text=====>:
{{about|spatial-temporal reasoning in information technology|spatial-temporal reasoning in psychology|Spatial visualization ability}}
{{technical|date=October 2012}}
'''Spatial–temporal reasoning''' is an area of [[Artificial Intelligence|artificial intelligence]] which draws from the fields of [[computer science]], [[cognitive science]], and [[cognitive psychology]]. The theoretic goal—on the cognitive side—involves representing and reasoning spatial-temporal knowledge in mind. The applied goal—on the computing side—involves developing high-level control systems of robots for navigating and understanding time and space. 

== Influence from cognitive psychology ==
A convergent result in cognitive psychology is that the connection relation is the first spatial relation that human babies acquire, followed by understanding orientation relations and distance relations. Internal relations among the three kinds of spatial relations can be computationally and systematically explained within the theory of cognitive prism as follows: (1) the connection relation is primitive; (2) an orientation relation is a distance comparison relation: you being in front of me can be interpreted as you are nearer to my front side than my other sides; (3) a distance relation is connection relations using a third object: you being one meter away from me can be interpreted as an object with the maximum extension of one meter can be connected with you and me simultaneously. 

== Fragmentary representations of temporal calculi ==
Without addressing internal relations among spatial relations, AI researchers contributed many fragmentary representations. Examples of temporal calculi include [[Allen's interval algebra]], and Vilain's & Kautz's [[point algebra]]. The most prominent spatial calculi are [[Mereotopology|mereotopological calculi]], [[Andrew U. Frank|Frank]]'s [[cardinal direction calculus]], Freksa's double cross calculus, Egenhofer and Franzosa's [[9-intersection calculus|4- and 9-intersection calculi]], Ligozat's [[flip-flop calculus]], various [[region connection calculus|region connection calculi]] (RCC), and the [[Oriented Point Relation Algebra]]. Recently, spatio-temporal calculi have been designed that combine spatial and temporal information. For example, the [[spatiotemporal constraint calculus]] (STCC) by Gerevini and Nebel combines Allen's interval algebra with RCC-8. Moreover, the [[qualitative trajectory calculus]] (QTC) allows for reasoning about moving objects.

== Quantitative abstraction ==
An emphasis in the literature has been on [[Qualitative reasoning|qualitative]] spatial-temporal reasoning which is based on qualitative abstractions of temporal and spatial aspects of the common-sense background knowledge on which our human perspective of physical reality is based.  Methodologically, qualitative [[Constraint satisfaction|constraint]] calculi restrict the vocabulary of rich mathematical theories dealing with temporal or spatial entities such that specific aspects of these theories can be treated within [[Decidability (logic)|decidable]] fragments with simple qualitative (non-[[Metric (mathematics)|metric]]) languages. Contrary to mathematical or physical theories about space and time, qualitative constraint calculi allow for rather inexpensive reasoning about entities located in space and time.  For this reason, the limited expressiveness of qualitative representation formalism calculi is a benefit if such reasoning tasks need to be integrated in applications.  For example, some of these calculi may be implemented for handling spatial [[Geographic information system|GIS]] queries efficiently and some may be used for navigating, and communicating with, a mobile [[robot]].

== Relation algebra ==
Most of these calculi can be formalized as abstract [[relation algebra]]s, such that reasoning can be carried out at a symbolic level. For computing solutions of a [[constraint network]], the [[Local consistency#Path_consistency|path-consistency algorithm]] is an important tool.

== Software ==
* [http://www.sfbtr8.spatial-cognition.de/de/projekte/reasoning/r4-logospace/research-tools/gqr/ GQR], constraint network solver for calculi like RCC-5, RCC-8, Allen's interval algebra, point algebra, cardinal direction calculus, etc.

== See also ==
*[[Cerebral cortex]]
*[[Diagrammatic reasoning]]
*[[Temporal logic]]
*[[Visual thinking]]
*[[Spatial ability]]

== Notes ==
{{reflist}}

==References==
*J. Renz, B. Nebel, [http://users.rsise.anu.edu.au/~jrenz/papers/renz-nebel-los.pdf Qualitative Spatial Reasoning using Constraint Calculi], in: M. Aiello, I. Pratt-Hartmann, J. van Benthem (eds.): Handbook of Spatial Logics, Springer 2007.
*T. Dong: [http://www.jstor.org/stable/41217909?seq=1#page_scan_tab_contents A COMMENT ON RCC: FROM RCC TO RCC⁺⁺]. Journal of Philosophical Logic, Vol 34, No. 2, pp. 319--352
*M. Vilain, H. Kautz, P. van Beek, [http://www.cs.rochester.edu/~kautz/papers/vilain-kautz-book.pdf Constraint propagation algorithms for temporal reasoning: A Revised Report], 1987.
*T. Dong. [http://www.springer.com/de/book/9783642240577 Recognizing Variable Environment -- The Theory of Cognitive Prism]. Studies in Computational Intelligence, Vol. 388, Springer-Verlag, Berlin Heidelberg, 2012.

{{DEFAULTSORT:Spatial-temporal reasoning}}
[[Category:Cognitive science]]
[[Category:Knowledge representation]]
[[Category:Educational psychology]]
[[Category:Logical calculi]]
[[Category:Reasoning]]
<=====doc_Id=====>:860
<=====title=====>:
Chow–Liu tree
<=====text=====>:
[[File:Chow-liu.png|thumb|400 px|A first-order dependency tree representing the product on the left.]]

In probability theory and statistics '''Chow–Liu tree''' is an efficient method for constructing a second-[[Orders of approximation|order]] product approximation of a [[joint probability distribution]], first described in a paper by {{Harvtxt|Chow|Liu|1968}}.  The goals of such a decomposition, as with such [[Bayesian networks]] in general, may be either [[data compression]] or [[inference]].

==The Chow–Liu representation==
The Chow–Liu method describes a [[joint probability distribution]] <math>P(X_{1},X_{2},\ldots,X_{n})</math> as a product of second-order conditional and marginal distributions.  For example, the six-dimensional distribution <math>P(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})</math> might be approximated as

:<math>
P^{\prime
}(X_{1},X_{2},X_{3},X_{4},X_{5},X_{6})=P(X_{6}|X_{5})P(X_{5}|X_{2})P(X_{4}|X_{2})P(X_{3}|X_{2})P(X_{2}|X_{1})P(X_{1})
</math>

where each new term in the product introduces just one new variable, and the product can be represented as a first-order dependency tree, as shown in the figure.  The Chow–Liu algorithm (below) determines which conditional probabilities are to be used in the product approximation.   In general, unless there are no third-order or higher-order interactions, the Chow–Liu approximation is indeed an ''approximation'', and cannot capture the complete structure of the original distribution.  {{Harvtxt|Pearl|1988}} provides a modern analysis of the Chow–Liu tree as a [[Bayesian network]].

==The Chow–Liu algorithm==
Chow and Liu show how to select second-order terms for the product approximation so that, among all such second-order approximations (first-order dependency trees), the constructed approximation <math>P^{\prime}</math> has the minimum [[Kullback–Leibler distance]] to the actual distribution <math>P</math>, and is thus the ''closest'' approximation in the classical [[information theory|information-theoretic]] sense. The Kullback–Leibler distance between a second-order product approximation and the actual distribution is shown to be

:<math>
D(P\parallel P^{\prime })=-\sum I(X_{i};X_{j(i)})+\sum
H(X_{i})-H(X_{1},X_{2},\ldots ,X_{n})
</math>

where <math>I(X_{i};X_{j(i)})</math> is the [[mutual information]] between variable <math>X_{i}</math> and its parent <math>X_{j(i)}</math> and <math>H(X_{1},X_{2},\ldots ,X_{n})</math> is the [[joint entropy]] of variable set <math>\{X_{1},X_{2},\ldots ,X_{n}\}</math>.   Since the terms <math>\sum H(X_{i})</math> and  <math>H(X_{1},X_{2},\ldots ,X_{n})</math> are independent of the dependency ordering in the tree, only the sum of the pairwise [[mutual information]]s, <math>\sum I(X_{i};X_{j(i)})</math>, determines the quality of the approximation. Thus, if every branch (edge) on the tree is given a weight corresponding to the mutual information between the variables at its vertices, then the tree which provides the optimal second-order approximation to the target distribution is just the ''maximum-weight tree''. The equation above also highlights the role of the dependencies in the approximation: When no dependencies exist, and the first term in the equation is absent, we have only an approximation based on first-order marginals, and the distance between the approximation and the true distribution is due to the redundancies that are not accounted for when the variables are treated as independent. As we specify second-order dependencies, we begin to capture some of that structure and reduce the distance between the two distributions.

Chow and Liu provide a simple algorithm for constructing the optimal tree; at each stage of the procedure the algorithm simply adds the maximum [[mutual information]] pair to the tree.  See the original paper, {{Harvtxt|Chow|Liu|1968}}, for full details. A more efficient tree construction algorithm for the common case of sparse data was outlined in {{Harvtxt|Meilă|1999}}.

Chow and Wagner proved in a later paper {{Harvtxt|Chow|Wagner|1973}} that the learning of the Chow–Liu tree is consistent given samples (or observations) drawn i.i.d. from a tree-structured distribution. In other words, the probability of learning an incorrect tree decays to zero as the number of samples tends to infinity. The main idea in the proof is the continuity of the mutual information in the pairwise marginal distribution. Recently, the exponential rate of convergence of the error probability was provided.<ref name="Tan">A Large-Deviation Analysis for the Maximum-Likelihood Learning of Tree Structures. V. Y. F. Tan, A. Anandkumar, L. Tong and A. Willsky. In the International symposium on information theory (ISIT), July 2009.</ref>

==Variations on Chow–Liu trees==
The obvious problem which occurs when the actual distribution is not in fact a second-order dependency tree can still in some cases be addressed by fusing or aggregating together densely connected subsets of variables to obtain a "large-node" Chow–Liu tree {{Harv|Huang|King|2002}}, or by extending the idea of greedy maximum branch weight selection to non-tree (multiple parent) structures {{Harv|Williamson|2000}}. (Similar techniques of variable substitution and construction are common in the [[Bayes network]] literature, e.g., for dealing with loops.  See {{Harvtxt|Pearl|1988}}.)

Generalizations of the Chow–Liu tree are the so-called [[t-cherry junction trees]]. It is proved that the t-cherry junction trees provide a better or at least as good approximation for a  discrete multivariate probability distribution as the Chow–Liu tree gives.
For the third order t-cherry junction tree see {{Harv|Kovács|Szántai|2010}}, for the ''k''th-order t-cherry junction tree see {{Harv|Szántai|Kovács|2010}}. The second order t-cherry junction tree is in fact the Chow–Liu tree.

==See also==
*[[Bayesian network]]
*[[Knowledge representation]]

==Notes==
{{reflist}}

==References==
{{refbegin|2}}
*{{Citation
 | last=Chow | first=C. K. | last2=Liu | first2=C.N.
 | title=Approximating discrete probability distributions with dependence trees
 | journal=IEEE Transactions on Information Theory
 | volume=IT-14  | issue=3 | year=1968 | pages=462–467 | url= | doi=10.1109/tit.1968.1054142}}.
*{{Citation
 | last=Huang | first=Kaizhu  | last2=King | first2=Irwin
 | last3=Lyu | first3=Michael R.  | year= 2002
 | chapter=Constructing a large node Chow–Liu tree based on frequent itemsets
 |editor1=Wang, Lipo |editor2=Rajapakse, Jagath C. |editor3=Fukushima, Kunihiko |editor4=Lee, Soo-Young |editor5=Yao, Xin | title=Proceedings of the 9th International Conference on Neural Information Processing ({ICONIP}'02)
 | place=[[Singapore]] | url= | accessdate= | pages=498–502}}.
*{{Citation
 | last=Pearl | first=Judea
 | title=Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference
 | publisher=[[Morgan Kaufmann]] | place=[[San Mateo, CA]] | year=1988}}
*{{Citation
 | last=Williamson | first=Jon | year= 2000
 | chapter=Approximating discrete probability distributions with Bayesian networks
 | title=Proceedings of the International Conference on Artificial Intelligence in Science and Technology
 | place=[[Tasmania]] | accessdate= | pages=16–20}}.
*{{Citation
 | last=Meilă | first=Marina | year= 1999
 | chapter=An Accelerated Chow and Liu Algorithm: Fitting Tree Distributions to High-Dimensional Sparse Data
 | title=Proceedings of the Sixteenth International Conference on Machine Learning
 | publisher=Morgan Kaufmann | accessdate= | pages=249–257}}.
*{{Citation
 | last=Chow | first=C. K. | last2=Wagner | first2=T.
 | title=Consistency of an estimate of tree-dependent probability distribution
 | journal=IEEE Transactions on Information Theory
 | volume=IT-19  | issue=3 | year=1973 | pages=369–371 | doi=10.1109/tit.1973.1055013}}.
*{{Citation
 | last=Kovács | first=E. | last2=Szántai | first2=T.
 | title=On the approximation of a discrete multivariate probability distribution using the new concept of t-cherry junction tree
 | journal=Lecture Notes in Economics and Mathematical Systems
 | volume=633, Part 1 | year=2010 | pages= 39–56 | doi=10.1007/978-3-642-03735-1_3}}.
*{{Citation
 | last=Szántai | first=T. | last2=Kovács | first2=E.
 | title=Hypergraphs as a mean of discovering the dependence structure of a discrete multivariate probability distribution
 | journal=Annals of Operations Research | year=2010 | pages= }}.
{{refend}}

{{DEFAULTSORT:Chow-Liu tree}}
[[Category:Knowledge representation]]
<=====doc_Id=====>:863
<=====title=====>:
Designated Community
<=====text=====>:
In information and archival communities, a '''Designated Community''' is an identified group of potential consumers who should be able to understand a particular set of information. These consumers may consist of multiple communities, are designated by the archive, and may change over time.<ref>{{cite web|title=Reference Model for an Open Archival Information System (ISO 14721:2012)|url=http://public.ccsds.org/publications/archive/650x0m2.pdf}}</ref>


==References==
{{reflist}}
{{library-stub}}

[[Category:Archival science]]
[[Category:Knowledge representation]]
[[Category:Digital libraries]]
<=====doc_Id=====>:866
<=====title=====>:
Semantic reasoner
<=====text=====>:
{{Redirect|Reasoner}}
A '''semantic reasoner''', '''reasoning engine''', '''rules engine''', or simply a '''reasoner''', is a piece of software able to infer [[logical consequence]]s from a set of asserted facts or [[axioms]]. The notion of a semantic reasoner generalizes that of an [[inference engine]], by providing a richer set of mechanisms to work with. The [[inference rules]] are commonly specified by means of an [[ontology language]], and often a [[description logic]] language.  Many reasoners use [[first-order predicate logic]] to perform reasoning; [[inference]] commonly proceeds by [[forward chaining]] and [[backward chaining]]. There are also examples of probabilistic reasoners, including Pei Wang's [[non-axiomatic reasoning system]],<ref name=Wang>{{cite web|last1=Wang|first1=Pei|title=Grounded on Experience Semantics for intelligence, Tech report 96|url=http://www.cogsci.indiana.edu/pub/wang.semantics.ps|website=http://www.cogsci.indiana.edu/|publisher=CRCC|accessdate=13 April 2015}}</ref> and [[probabilistic logic network]]s.<ref name=Goertzel2008>{{cite book|last1=Goertzel|first1=Ben|last2=Iklé|first2=Matthew|last3=Goertzel|first3=Izabela Freire|last4=Heljakka|first4=Ari|title=Probabilistic Logic Networks: A Comprehensive Framework for Uncertain Inference|date=2008|publisher=Springer Science & Business Media|isbn=9780387768724|page=42}}</ref>

==List of semantic reasoners==

Existing semantic reasoners and related software:

===Commercial software===
* Bossam (software), an RETE-based rule engine with native supports for reasoning over [[Web Ontology Language|OWL]] ontologies, SWRL rules, and RuleML rules.
* RacerPro

===Free to use (Closed Source)===
* [[Cyc]] inference engine, a forward and backward chaining inference engine with numerous specialized modules for high-order logic. ([http://research.cyc.com/] ResearchCyc) ([http://opencyc.org/] OpenCyc)
* [[KAON2]] is an infrastructure for managing [[OWL-DL]], [[Semantic Web Rule Language|SWRL]], and [[F-Logic]] ontologies.
* [[ Internet Business Logic (software)]]—A reasoner designed for end-user app authors. Automatically generates and runs complex networked SQL queries. Explains the results in English at the end-user level.

===Free software (open source)===
* [[Cwm (software)|Cwm]], a forward-chaining reasoner used for querying, checking, transforming and filtering information. Its core language is RDF, extended to include rules, and it uses RDF/XML or N3 serializations as required. ([http://www.w3.org/2000/10/swap/doc/cwm.html CWM],  W3C software license)
* [[Drools]], a forward-chaining inference-based rules engine which uses an enhanced implementation of the [[Rete algorithm]]. ([http://www.jboss.org/drools/ Drools], Apache license 2.0)
* [http://owl.cs.manchester.ac.uk/tools/fact/ FaCT++ Reasoner], a tableaux-based reasoner for expressive Description Logics (DL), covering OWL and OWL 2 but lacking support for key constraints and some datatypes. Written in C++. (LGPL)
* [[Flora-2]], an object-oriented, rule-based knowledge-representation and reasoning system. ([http://flora.sourceforge.net Flora-2], Apache 2.0)
* [https://gndf.io/ Gandalf], open-source decision rules engine on PHP (GPL).
* [[Prova]], a semantic-web rule engine which supports data integration via SPARQL queries and type systems (RDFS, OWL ontologies as type system). ([http://prova.ws Prova], GNU GPL v2, commercial option available)
* [https://github.com/stardog-union/pellet Pellet], OWL 2 DL reasoner (AGPL, commercial option available)
* [http://www.hermit-reasoner.com/ HermiT], OWL 2 DL reasoner (LGPL)
* [https://github.com/liveontologies/elk-reasoner ELK], OWL 2 EL reasoner (Apache 2)
* [https://lat.inf.tu-dresden.de/systems/cel CEL], OWL 2 EL reasoner (Apache 2)
* [https://github.com/julianmendez/jcel jcel], OWL 2 EL reasoner (LGPL / Apache 2)
* [https://github.com/ha-mo-we/Racer RACER], OWL 2 DL reasoner (BSD-3)
* [[Jena (framework)]], an open-source semantic-web framework for Java which includes a number of different semantic-reasoning modules. ([http://jena.apache.org/ Apache Jena], Apache License 2.0)
* [[RDFSharp]], an open source semantic web framework for .NET which includes a semantic extension implementing RDFS/OWL-DL/custom rule-based reasoning. ([http://rdfsharp.codeplex.com/ RDFSharp], Apache License 2.0)

=== Applications that contain reasoners ===
* [[Apache Marmotta]] includes a rule-based reasoner in its KiWi [[triple store]].
* [http://techinvestlab.ru/dot15926Editor dot15926 Editor]—Ontology management framework initially designed for engineering ontology standard [[ISO 15926]]. Allows [[Python (programming language)|Python]] rule scripting and pattern-based data analysis. Supports extensions.

==See also==
{{portal|Software}}
* [[Business rules engine]]
* [[Expert systems]]
* [[Doxastic logic]]
* [[Method of analytic tableaux]]
*[[Logic Programming]]

==References==
{{reflist}}

==External links==
* [https://www.w3.org/2001/sw/wiki/OWL/Implementations OWL 2 Reasoners listed on W3C SW Working Group homepage]
* [http://www.w3.org/TR/rdf-sparql-query/ SPARQL Query Language for RDF]
* [http://www.inf.unibz.it/~franconi/dl/course/ Introduction to Description Logics DL course] by Enrico Franconi, Faculty of Computer Science, [[Free University of Bolzano]], Italy
* [http://trimc-nlp.blogspot.com/2013/04/owl-properties.html ''Inference using OWL 2.0 Semantics''] by Craig Trim (IBM).
* Marko Luther, Thorsten Liebig, Sebastian Böhm, Olaf Noppens: [http://dx.doi.org/10.1007/978-3-642-02121-3_9 Who the Heck Is the Father of Bob?]. ESWC 2009: 66-80
* Jurgen Bock, Peter Haase, Qiu Ji, Raphael Volz. [http://www.aifb.uni-karlsruhe.de/WBS/pha/publications/owlbenchmark_07_2007.pdf Benchmarking OWL Reasoners]. In ARea2008 - Workshop on Advancing Reasoning on the Web: Scalability and Commonsense (June 2008)
* Tom Gardiner, Ian Horrocks, Dmitry Tsarkov. [http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/Vol-189/submission_23.pdf Automated Benchmarking of Description Logic Reasoners]. Description Logics Workshop 2006
* [http://www2009.org/proceedings/pdf/p601.pdf OpenRuleBench] Senlin Liang, Paul Fodor, Hui Wan, Michael Kifer. OpenRuleBench: An Analysis of the Performance of Rule Engines. 2009.  Latest benchmarks at [http://rulebench.projects.semwebcentral.org/ OpenRuleBench website].

{{Semantic Web}}
{{Computable knowledge}}

{{DEFAULTSORT:Semantic Reasoner}}
[[Category:Rule engines| ]]
[[Category:Knowledge representation]]
[[Category:Knowledge engineering]]
[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Reasoning]]
<=====doc_Id=====>:869
<=====title=====>:
Transaction logic
<=====text=====>:
'''Transaction Logic''' is an extension of [[predicate logic]] that accounts in a clean and declarative way for the phenomenon of state changes in [[logic program]]s and [[database]]s. This extension adds connectives specifically designed for combining simple actions into complex transactions and for providing control over their execution. The logic has a natural [[model theory]] and a sound and complete [[proof theory]]. Transaction Logic has a [[Horn clause]] subset, which has a procedural as well as a declarative semantics. The important features of the logic include hypothetical and committed updates, dynamic constraints on transaction execution, non-determinism, and bulk updates.  In this way, Transaction Logic is able to declaratively capture a number of non-logical phenomena, including [[procedural knowledge]] in [[artificial intelligence]], [[active database]]s, and methods with side effects in [[object database]]s.

Transaction Logic was originally proposed in <ref name="tr-iclp1993">A.J. Bonner and M. Kifer (1993), ''Transaction Logic Programming'', International Conference on Logic Programming (ICLP), 1993.</ref> by [http://www.cs.toronto.edu/~bonner/ Anthony Bonner] and [http://www.cs.stonybrook.edu/~kifer/ Michael Kifer] and later described in more detail in <ref>A.J. Bonner and M. Kifer (1994), ''An Overview of Transaction Logic'', Theoretical Computer Science, 133:2, 1994.</ref> and.<ref>A.J. Bonner and M. Kifer (1998), [http://www.cs.sunysb.edu/~kifer/TechReports/tr-chomicki.pdf ''Logic Programming for Database Transactions''] in Logics for Databases and Information Systems, J. Chomicki and G. Saake (eds.), Kluwer Academic Publ., 1998.</ref> The most comprehensive description appears in.<ref>A.J. Bonner and M. Kifer (1995), [http://www.cs.sunysb.edu/~kifer/TechReports/transaction-logic.pdf ''Transaction Logic Programming (or A Logic of Declarative and Procedural Knowledge)'']. Technical Report CSRI-323, November 1995, Computer Science Research Institute, University of Toronto.</ref>

In later years, Transaction Logic was extended in various ways, including [[concurrency]]{{dn|date=November 2016}},<ref name="concurrentTR">A.J. Bonner and M. Kifer (1996), [http://www.cs.sunysb.edu/~kifer/TechReports/concurrent-trans-logic.pdf ''Concurrency and communication in Transaction Logic''], Joint Intl. Conference and Symposium on Logic Programming, Bonn, Germany, September 1996</ref> [[defeasible reasoning]],<ref>P. Fodor and M. Kifer (2011), [http://drops.dagstuhl.de/opus/volltexte/2011/3159/ ''Transaction Logic with Defaults and Argumentation Theories'']. In Technical communications of the 27th International Conference on Logic Programming (ICLP), July 2011.</ref> partially defined actions,<ref>M. Rezk and M. Kifer (2012), [http://link.springer.com/article/10.1007%2Fs13740-012-0007-8 ''Transaction Logic with Partially Defined Actions'']. Journal on Data Semantics, August 2012, vol. 1, no. 2, Springer.</ref> and other features.<ref>H. Davulcu, M. Kifer and I.V. Ramakrishnan (2004), [http://www.www2004.org/proceedings/docs/2p144.pdf CTR-S: A Logic for Specifying Contracts in Semantic Web Services'']. Proceedings of the 13-th World Wide Web Conference (WWW2004), May 2004.</ref><ref>P. Fodor and M. Kifer (2010), [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.184.6968 ''Tabling for Transaction Logic'']. In Proceedings of the 12th international ACM SIGPLAN symposium on Principles and practice of declarative programming (PPDP), July 2010.</ref>

In 2013, the original paper on Transaction Logic <ref name="tr-iclp1993"/> has won the 20-year Test of Time Award as the most influential paper from the proceedings of [http://www.informatik.uni-trier.de/~ley/db/conf/iclp/iclp93.html ICLP 1993 conference] in the preceding 20 years.{{citation needed|date=February 2014}}

== Examples ==

Graph coloring. Here <tt>tinsert</tt> denotes the elementary update operation of ''transactional insert''. The connective ⊗ is called ''serial conjunction''.
 colorNode <-  // color one node correctly
     node(N) ⊗ &neg; colored(N,_) ⊗ color(C)
     ⊗ ¬(adjacent(N,N2) ∧ colored(N2,C))
     ⊗ tinsert(colored(N,C)).
 colorGraph <- ¬uncoloredNodesLeft.
 colorGraph <- colorNode ⊗ colorGraph.

Pyramid stacking. The elementary update <tt>tdelete</tt> represents the ''transactional delete'' operation.
 stack(N,X) <- N>0 ⊗ move(Y,X) ⊗ stack(N-1,Y).
 stack(0,X).
 move(X,Y) <- pickup(X) ⊗ putdown(X,Y).
 pickup(X) <- clear(X) ⊗ on(X,Y) ⊗
              ⊗ tdelete(on(X,Y)) ⊗ tinsert(clear(Y)).
 putdown(X,Y) <-  wider(Y,X) ⊗ clear(Y) 
                  ⊗ tinsert(on(X,Y)) ⊗ tdelete(clear(Y)).

Hypothetical execution. Here <tt>&lt;&gt;</tt> is the modal operator of possibility: If both <tt>action1</tt> and <tt>action2</tt> are possible, execute <tt>action1</tt>. Otherwise, if only <tt>action2</tt> is possible, then execute it.
  execute <- <>action1 ⊗ <>action2 ⊗ action1.
  execute <- ¬<>action1 ⊗ <>action2 ⊗ action2.

Dining philosophers. Here | is the logical connective of parallel conjunction of Concurrent Transaction Logic.<ref name="concurrentTR"/>
 diningPhilosophers <- phil(1) | phil(2) | phil(3) | phil(4).

== Implementations ==

A number of implementations of Transaction Logic exist. The original implementation is available [http://www.cs.toronto.edu/~bonner/transaction-logic.html here]. An implementation of Concurrent Transaction Logic is available [http://www.cs.toronto.edu/~bonner/ctr/index.html here]. Transaction Logic enhanced with [[tabling]] is available [http://flora.sourceforge.net/tr-interpreter-suite.tar.gz here]. An implementation of Transaction Logic has also been incorporated as part of the [[Flora-2]] knowledge representation and reasoning system. All these implementations are [[open source]].

Additional papers on Transaction Logic can be found on the [http://flora.sourceforge.net Flora-2 Web site].

== References ==
{{Reflist}}

[[Category:Logic programming languages]]
[[Category:Declarative programming languages]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:872
<=====title=====>:
ERIL
<=====text=====>:
{{COI|date=June 2016}}
{{Orphan|date=April 2014}}

[[File:A simple example ERIL diagram.png|thumb|right|An example ERIL diagram with 3 classes and 3 one-to-many relationships.]]

'''ERIL''' ('''Entity-Relationship and Inheritance Language''') is a [[visual language]] for representing the data structure of a computer system.
As its name suggests, ERIL is based on [[Entity–relationship model|entity-relationship]] diagrams and [[class diagram]]s.
ERIL combines the [[Relational data model|relational]] and [[Object-oriented programming|object-oriented]] approaches to [[data model]]ing.

== Overview ==
ERIL can be seen as a set of guidelines aimed at improving the readability of structure diagrams.
These guidelines were borrowed from [[DRAKON]], a variant of [[flowchart]]s created within the Russian space program.
ERIL itself was developed by Stepan Mitkin.

The ERIL guidelines for drawing diagrams:
* Lines must be straight, either strictly vertical or horizontal.
* Vertical lines mean ownership ([[Object composition|composition]]).
* Horizontal lines mean peer relationships ([[Object composition#Aggregation|aggregation]]).
* Line intersections are not allowed.
* It is not recommended to fit the whole data model on a single diagram. Draw many simple diagrams instead.
* The same class (table) can appear several times on the same diagram.    
* Use the following standard symbols to indicate the type of the relationship.
** One-to-one: a simple line.
** One-to-many, two-way: a line with a "paw".
** One-to-many, one-way: an arrow.
** Many-to-many: a line with two "paws".    
* Do not lump together inheritance and data relationships.<ref>[http://drakon-editor.sourceforge.net/eril.html ERIL: a Visual Language for Data Modelling]</ref>

== Indexes ==
A class (table) in ERIL can have several indexes.
Each index in ERIL can include one or more fields, similar to indexes in [[relational database]]s.
ERIL indexes are logical. They can optionally be implemented by real data structures.

== Links ==
Links between classes (tables) in ERIL are implemented by the so-called "link" fields.
Link fields can be of different types according to the link type:
* reference;
* collection of references.
    
Example: there is a one-to-many link between ''Documents'' and ''Lines''. One ''Document'' can have many ''Lines''. Then the ''Document.Lines'' field is a collection of references to the lines that belong to the document. ''Line.Document'' is a reference to the document that contains the line.

Link fields are also logical. They may or may not be implemented physically in the system.

== Usage ==

ERIL is supposed to model any kind of data regardless of the storage. 
The same ERIL diagram can represent data stored in a [[relational database]], in a [[Nosql|NoSQL]] database, [[Xml|XML]] file or in the memory.

ERIL diagrams serve two purposes.
The primary purpose is to explain the data structure of an existing or future system or component.
The secondary purpose is to automatically generate source code from the model.
Code that can be generated includes specialized collection classes, hash and comparison functions, data retrieval and modification procedures, [[Data definition language|SQL data-definition]] code, etc. Code generated from ERIL diagrams can ensure referential and uniqueness [[data integrity]].
Serialization code of different kinds can also be automatically generated.
In some ways ERIL can be compared to [[object-relational mapping]] frameworks.

== See also ==
* [[Model-driven engineering]]
* [[Unified Modeling Language|UML]]
* [[Entity–relationship model]]
* [[Flowchart]]s
* [[Class diagram]]
* [[DRAKON]]

== Notes ==
{{Reflist}}


[[Category:Architecture description language]]
[[Category:Data modeling languages]]
[[Category:Data modeling diagrams]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Specification languages]]
[[Category:Software modeling language]]
<=====doc_Id=====>:875
<=====title=====>:
KL-ONE
<=====text=====>:
{{distinguish | KL1 }}

'''KL-ONE''' (pronounced "kay ell won") is a well known [[knowledge representation]] system in the tradition of [[semantic networks]] and [[Frame (Artificial intelligence) | frames]]; that is, it is a [[frame language]]. The system is an attempt to overcome semantic indistinctness in semantic network representations and to explicitly represent conceptual information as a structured inheritance network.<ref>{{Cite journal | last1 = Woods | first1 = W. A. | authorlink1 = William Aaron Woods| last2 = Schmolze | first2 = J. G. | doi = 10.1016/0898-1221(92)90139-9 | title = The KL-ONE family | journal = Computers & Mathematics with Applications | volume = 23 | issue = 2–5 | pages = 133 | year = 1992 | pmid =  | pmc = }}</ref><ref>{{Cite journal | last1 = Brachman | first1 = R. J. | authorlink1 = Ronald J. Brachman| last2 = Schmolze | first2 = J. G. | doi = 10.1207/s15516709cog0902_1 | title = An Overview of the KL-ONE Knowledge Representation System | journal = Cognitive Science | volume = 9 | issue = 2 | pages = 171 | year = 1985 | pmid =  | pmc = }}</ref><ref>{{cite book | last = D.A. Duce | first = G.A. Ringland  | title = Approaches to Knowledge Representation, An Introduction | year = 1988 | publisher = Research Studies Press, Ltd. | isbn = 0-86380-064-5 }}</ref>

There is a whole family of KL-ONE-like systems. One of the innovations that KL-ONE initiated was the use of a [[deductive classifier]], an automated reasoning engine that can validate a frame ontology and deduce new information about the ontology based on the initial information provided by a domain expert. 

Frames in KL-ONE are called [[concepts]]. These form hierarchies using subsume-relations; in the KL-ONE terminology a [[superclass (computer science)|super class]] is said to subsume its [[Subclass (computer science) | subclasses]]. 
[[Multiple inheritance]] is allowed. Actually a concept is said to be well-formed only if it inherits from more than one other concept. All concepts, except the top concept (usually THING), must have at least one super class. 

In KL-ONE descriptions are separated into two basic classes of concepts: primitive and defined. Primitives are domain concepts that are not fully defined. This means that given all the properties of a concept, this is not sufficient to classify it. They may also be viewed as incomplete definitions. Using the same view, defined concepts are complete definitions. Given the properties of a concept, these are [[necessary and sufficient]] conditions to classify the concept.

The slot-concept is called roles and the values of the roles are role-fillers. There are several different types of roles to be used in different situations. The most common and important role type is the generic RoleSet that captures the fact that the role may be filled with more than one filler.

==See also==
* [[Ontology language]]

==References==
{{reflist}}


{{FOLDOC}}

[[Category:Artificial intelligence]]
[[Category:Knowledge representation]]
[[Category:Ontology languages]]
<=====doc_Id=====>:878
<=====title=====>:
Open-world assumption
<=====text=====>:
In a [[Mathematical logic|formal system of logic]] used for [[knowledge representation]], the '''open-world assumption''' is the assumption that the [[truth value]] of a [[statement (logic)|statement]] may be true irrespective of whether or not it is ''known'' to be true. It is the opposite of the [[closed-world assumption]], which holds that any statement that is true is also known to be true.

The open-world assumption (OWA) codifies the informal notion that in general no single agent or observer has complete knowledge, and therefore cannot make the closed-world assumption. The OWA limits the kinds of inference and deductions an agent can make to those that follow from statements that are known to the agent to be true. In contrast, the closed world assumption allows an agent to infer, from its lack of knowledge of a statement being true, anything that [[Logical consequence|follows from]] that statement being false.

Heuristically, the open-world assumption applies when we represent knowledge within a system as we discover it, and where we cannot guarantee that we have discovered or will discover complete information. In the OWA, statements about knowledge that are not included in or inferred from the knowledge explicitly recorded in the system may be considered unknown, rather than wrong or false. 

[[Semantic Web]] languages such as [[Web Ontology Language|OWL]] make the open-world assumption. The absence of a particular statement within the web means, in principle, that the statement has not been made explicitly yet, irrespective of whether it would be true or not, and irrespective of whether we believe that it would be true or not. In essence, from the absence of a statement alone, a deductive reasoner cannot (and must not) infer that the statement is false.

Many [[procedural programming language]]s and [[database]]s make the closed-world assumption. For example, if a typical airline database does not contain a seat assignment for a traveler, it is assumed that the traveler has not checked in. The closed-world assumption typically applies when a system has complete control over information; this is the case with many database applications where the [[database transaction]] system acts as a central broker and arbiter of concurrent requests by multiple independent clients (e.g., airline booking agents). There are, however, many databases with incomplete information: for example, one cannot assume that because there is no mention on a patient's history of a particular allergy, that the patient does not suffer from that allergy.

'''Example'''
  Statement: "Mary" "is a citizen of" "France"

  Question: Is Paul a citizen of France?

  "Closed world" (for example SQL) answer: No.
  "Open world" answer: Unknown.

Under OWA, failure to derive a fact does not imply the opposite. For example, assume we only know that Mary is a citizen of France. From this information we can neither conclude that Paul is not a citizen of France, nor that he is. Therefore, we admit the fact that our knowledge of the world is incomplete. The open-world assumption is closely related to the [[Monotonicity of entailment|monotonic]] nature of [[first-order logic]]: adding new information never falsifies a previous conclusion. Namely, if we subsequently learn that Paul is also a citizen of France, this does not change any earlier positive or negative conclusions.

The language of logic programs with [[Stable_model_semantics#Strong_negation|strong negation]] allows us to postulate the closed-world assumption for some predicates and leave the other predicates in the realm of the open-world assumption.

==See also==
*[[Closed-world assumption]]

==References==
*{{cite book |last1=Russell |first1=Stuart J. |authorlink1=Stuart J. Russell |last2=Norvig |first2=Peter |authorlink2=Peter Norvig |title=Artificial Intelligence: A Modern Approach |year=2010 |publisher=Prentice Hall |location=Upper Saddle River |isbn=9780136042594 |url=http://www.pearsonhighered.com/educator/product/Artificial-Intelligence-A-Modern-Approach/9780136042594.page |edition=3rd}}

{{DEFAULTSORT:Open-world assumption}}
[[Category:Logic programming]]
[[Category:Knowledge representation]]
<=====doc_Id=====>:881
<=====title=====>:
Frame (artificial intelligence)
<=====text=====>:
'''Frames''' were proposed by [[Marvin Minsky]] in his 1974 article "A Framework for Representing Knowledge." A frame is an [[artificial intelligence]] [[data structure]] used to divide [[knowledge]] into substructures by representing "[[stereotype]]d situations." Frames are the primary data structure used in artificial intelligence [[frame language]]s. 

Frames are also an extensive part of [[knowledge representation and reasoning]] schemes. Frames were originally derived from semantic networks and are therefore part of structure based knowledge representations. According to Russell and Norvig's "Artificial Intelligence, A Modern Approach," structural representations assemble "...facts about particular object and even types and arrange the types into a large taxonomic hierarchy analogous to a biological taxonomy."

== Frame structure ==

The frame contains information on how to use the frame, what to expect next, and what to do when these expectations are not met. Some information in the frame is generally unchanged while other information, stored in "terminals,"{{clarify|date=January 2010|reason=Which type of 'terminal' is meant?}} usually change. Different frames may share the same terminals.

Each piece of information about a particular frame is held in a slot. The information can contain:

* Facts or Data
** Values (called facets)
* Procedures (also called procedural attachments)
** IF-NEEDED : deferred evaluation
** IF-ADDED : updates linked information
* Default Values
** For Data
** For Procedures
* Other Frames or Subframes

== Features and advantages ==

A frame's terminals are already filled with default values, which is based on how the human mind works. For example, when a person is told "a boy kicks a ball," most people will visualize a particular ball (such as a familiar [[soccer ball]]) rather than imagining some abstract ball with no attributes.

One particular strength of frame based knowledge representations is that, unlike semantic networks, they allow for exceptions in particular
instances. This gives frames an amount of flexibility that allow representations of real world phenomena to be reflected more accurately.

Like [[semantic networks]], frames can be queried using spreading activation. Following the rules of inheritance, any value given to a slot that is inherited by subframes will be updated (IF-ADDED) to the corresponding slots in the subframes and any new instances of a particular frame will feature that new value as the default.

Because frames are structurally based, it is possible to generate a semantic network given a set of frames even though it lacks explicit arcs. The reference to Minsky's teacher [[Noam Chomsky]] and his [[generative grammar]] of 1950 is generally missing in Minsky's publications. However, the semantic strength is originated by that concept. 

The simplified structures of frames allow for easy analogical reasoning, a much prized feature in any intelligent agent. The procedural attachments provided by frames also allow a degree of flexibility that makes for a more realistic representation and gives a natural affordance for programming applications.

== Example ==

Worth noticing here is the easy analogical reasoning (comparison) that can be done between a boy and a monkey just by having similarly named slots.

Also notice that Alex, an instance of a boy, inherits default values like "Sex" from the more general parent object Boy,
but the boy may also have different instance values in the form of exceptions such as the number of legs.

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| ALEX  || _ || (This Frame)
|-
| NAME || Alex || (key value)
|-
| ISA || Boy || (parent frame)
|-
| SEX || Male || (inheritance value)
|-
| AGE || IF-NEEDED: Subtract(current,BIRTHDATE); || (procedural attachment)
|-
| HOME || 100 Main St. || (instance value)
|-
| BIRTHDATE || 8/4/2000 || (instance value)
|-
| FAVORITE_FOOD || Spaghetti || (instance value)
|-
| CLIMBS || Trees || (instance value)
|-
| BODY_TYPE || Wiry || (instance value)
|-
| NUM_LEGS || 1 || (exception)
|}

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| BOY  || _ || (This Frame)
|-
| ISA || Person || (parent frame)
|-
| SEX || Male || (instance value)
|-
| AGE || Under 12 yrs. || (procedural attachment - sets constraint)
|-
| HOME || A Place || (frame)
|-
| NUM_LEGS || Default = 2 || (default, inherited from Person frame)
|}

{| class="wikitable"
|-
! Slot !! Value !! Type
|-
| MONKEY  || _ || (This Frame)
|-
| ISA || Primate || (parent frame)
|-
| SEX || OneOf(Male,Female) || (procedural attachment)
|-
| AGE || an integer || (procedural attachment - sets constraint)
|-
| HABITAT || Default = Jungle || (default)
|-
| FAVORITE_FOOD || Default = Bananas || (default)
|-
| CLIMBS || Trees || _ 
|-
| BODY_TYPE || Default = Wiry || (default)
|-
| NUM_LEGS || Default = 2 || (default)
|}

== See also ==
* [[Frame language]]
* [[Frame problem]]

== References ==
Russell, Stuart J.; Norvig, Peter (2010), Artificial Intelligence: A Modern Approach (2nd ed.), Upper Saddle River, New Jersey: Prentice Hall, ISBN 0-13-604259-7, http://aima.cs.berkeley.edu/ , chpt. 1

== External links ==
* [http://web.media.mit.edu/~minsky/papers/Frames/frames.html Minsky's "A Framework for Representing Knowledge"]
* [http://aima.cs.berkeley.edu/ Artificial Intelligence: A Modern Approach Website]

[[Category:Knowledge representation]]
[[Category:History of artificial intelligence]]
<=====doc_Id=====>:884
<=====title=====>:
Flex expert system
<=====text=====>:
{{multiple issues|
{{COI|date=September 2014}}
{{notability|Products|date=September 2014}}
{{third-party|date=September 2014}}
}}
Flex is a hybrid expert system toolkit developed by [[Logic Programming Associates|LPA]] which incorporates [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures.

Flex supports both forwards and backward chaining, and they can be interleaved.

Flex provides its own English-like Knowledge Specification Language, KSL, which helps ensure that knowledge-bases are readable by domain experts. Flex KSL can now be generated automatically for certain classes of problems from [[VisiRule]].

Flex is implemented in, and has access to, [[Prolog]]. As opposed to most expert system shells, which tend to be constrained, Flex is an open toolkit.

Flex has proved very popular in education and was licensed to the Open University as part of T396: 'Artificial intelligence for technology'.

Much of the material for this course is described by Prof Adrian Hopgood in his book: Intelligent Systems for Engineers and Scientists, Third Edition, and on his web-site.<ref name = "AI Toolkit">{{citation |url=http://www.adrianhopgood.com/aitoolkit/aitoolkit.shtml | title=AI toolkit: support site for Intelligent Systems for Engineers and Scientists, Third Edition by Adrian Hopgood}}</ref>

There is also a Flex tutorial on the LPA web-site.<ref name = "Flex Tutorial">{{citation |url=http://www.lpa.co.uk/ftp/5000/flx_tut.pdf | title=Flex Tutorial by Clive Spenser on LPA web-site}}</ref>

Flex has been used to power AllerGenius, an expert system specifically developed by leading allergologists to help interpret the results of modern in vitro allergy tests such as the ImmunoCAP ISAC. These tests can typically measure specific antibodies to more than 100 allergen components from more than 50 pre-selected allergen sources and require a lot of expert interpretation.<ref name = "Allergenius">{{citation |url=http://www.allergenius.it/new/index.php/en/general-conceps/2-non-categorizzato/139-the-structure-of-the-exper-system | title=Allergenius web site}}</ref>

==External links==
*[http://www.generation5.org/content/2001/prg04.asp "Introduction to Flex/KSL (Part I)", James Mathhews, Generation5]
*[http://www.lpa.co.uk/flx_det.htm Flex Technical Details], LPA
*[http://www.lpa.co.uk/wfs_dem.htm WebFlex demos], LPA
*[http://www.intbis.com/intbis_pages/train.php Flex Training], IbIS
*[http://dl.acm.org/citation.cfm?id=297981 "A flex-based expert system for sewage treatment works support", Dixon et al, PCAI Magazine]
*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]
*[http://4c.ucc.ie/web/upload/publications/inProc/KCCP-2007%20Dokas-Nordlander-Wallace.pdf "Fuzzy Fault Tree Representation and Maintenance based on Frames and Constraint Technologies: A Case Study", Dokas, Nordlander and Wallace]
*[https://repositorium.sdum.uminho.pt/bitstream/1822/8868/1/A%20Knowledge-Based%20System%20for%20Spinning%20Management.pdf “A Knowledge-Based System for Spinning Management”]
*[http://iraj.in/up_proc/pdf/86-140412387293-96.pdf A NOVEL APPROACH FOR EXPERT SYSTEM AIDED DATACENTER DESIGN]
*[http://www.cscjournals.org/csc/manuscript/Journals/IJAE/volume1/Issue2/IJAE-10.pdf “An Expert System using A Decision Logic Charting Approach for Indian Legal Domain With specific reference to Transfer of Property Act”], N B Bilgi, Dr. R V Kulkarni & C. Spenser
*[http://www.ijser.org/researchpaper%5CAN-EXPERT-SYSTEM-FOR-SEISMIC-DATA-INTERPRETATION.pdf “An expert system for Seismic data interpretation using visual and analytical tools”], Neelu Jyothi Ahuja and Parag Diwan
*[http://pubcouncil.kuniv.edu.kw/jer/files/19Nov2012102247An%20expert%20system%20machinability%20data%20bank%20%28ESMDB%29%20approach..pdf “An expert system machinability data bank”]
*[http://www.iis.sinica.edu.tw/APEC02/Program/chingyeh.pdf “Development of an Ontology-Based Portal for Digital Archive Services”], Ching-Long Yeh
*[http://orbit.dtu.dk/fedora/objects/orbit:88354/datastreams/file_7703263/content “A development process meta-model for Web based expert systems: the Web engineering point of view”], Ioannis M. Dokas and Alexandre Alapetite
*[http://www.pacis-net.org/file/1997/75.pdf “Behavioural issues in Information Systems Design”], Mike McGrath
*[http://www.slaai.lk/proc/2006/chatura.pdf “Artificial Intelligence Approach to Effective Career Guidance”], Chathra Hendahewa et al
*[http://www.icsd.aegean.gr/kkemalis/pubs/SETN_CAMES.pdf “DYNAMIC ACCESS CONTROL MANAGEMENT USING EXPERT SYSTEM TECHNOLOGY”], Prof. G. Pangalos et al
*[http://www.waojournal.org/content/7/1/15 “Allergenius, an expert system for the interpretation of allergen microarray results”], Giovanni Melioli, Clive Spenser et al

== References ==
{{Reflist}}

[[Category:Expert systems]]
[[Category:Rule engines]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}
<=====doc_Id=====>:887
<=====title=====>:
Logic Programming Associates
<=====text=====>:
{{multiple issues|
{{notability|Companies|date=January 2012}}
{{more footnotes|date=April 2013}}
}}

{{Infobox company
|name   = Logic Programming Associates Ltd
|type   = [[Private company|Private]]
|foundation = 1981
|directors= Diane Reeve <br />Clive Spenser <br />Brian Steel 
|location = London SW18 3SX
|area_served = UK, United States, [[Europe, the Middle East and Africa|EMEA]]
|industry             = [[Computer software]] 
|products = [[VisiRule]], [[Flex expert system|Flex expert system toolkit]], [[Flint toolkit]], LPA Prolog for Windows
|website = [http://www.lpa.co.uk www.lpa.co.uk]}}

'''Logic Programming Associates''' ('''LPA''') is a company specializing in [[logic programming]] and [[artificial intelligence]] software. LPA was founded in 1980 and is widely known for its range of [[Prolog]] compilers and more recently for [[VisiRule]].

LPA was established to exploit research at Imperial College, London into [[logic programming]] carried out under the supervision of [[Robert Kowalski|Prof Robert Kowalski]]. One of the first implementations made available by LPA was micro-PROLOG<ref name = "Prolog implementations">{{citation |url=http://www.berghel.com/publications/micropro/micropro_ncc87.pdf | title= Microcomputer PROLOG implementations | accessdate=2013-04-29}}</ref> which ran on popular 8-bit home computers such as the [[Sinclair Spectrum]]<ref name = "micro-PROLOG for Sinclair Spectrum">{{citation |url=http://www.worldofspectrum.org/infoseekid.cgi?id=0008429 | title= micro-PROLOG for Sinclair Spectrum | accessdate=2013-04-29}}</ref> and [[Apple II]]. This was followed by micro-PROLOG Professional one of the first Prolog implementations for MS-DOS.

As well as continue with Prolog compiler technology development, LPA has a track record of creating innovative associated tools and products to address specific challenges and opportunities.

In 1989, LPA developed the [[Flex expert system|Flex expert system toolkit]], which incorporated [[Frame language|frame-based]] reasoning with inheritance, [[rule-based programming]] and data-driven procedures. Flex has its own English-like Knowledge Specification Language (KSL) which means that knowledge and rules are defined in an easy-to-read and understand way.

In 1992, LPA helped set up the Prolog Vendors Group,<ref name = "PVG launched">{{citation |url=http://iospress.metapress.com/content/c1p1351212770518/fulltext.pdf | title=Prolog Vendors Group Launched | accessdate=2013-04-29}}</ref> a not-for-profit organization whose aim was to help promote Prolog by making people aware of its usage in industry.

In 2000, LPA helped set up [[Business Integrity]], now a leading supplier of document assembly and contract creation software solutions for the legal market.

LPA's core product is LPA Prolog for Windows,<ref name = "WIN-PROLOG">{{citation |url=http://www.lpa.co.uk/win.htm | title= LPA Prolog for Windows | accessdate=2013-04-29}}</ref> a compiler and development system for the Microsoft Windows platform. The current LPA software range comprises an integrated AI toolset which covers various aspects of [[Artificial Intelligence]] including Logic Programming, [[Expert Systems]], [[Knowledge-based Systems]], Data Mining, Agents and [[Case-based reasoning]] etc.

In 2004, LPA launched [[VisiRule]] <ref name = "VisiRule">{{citation |url=http://www.lpa.co.uk/vsr.htm | title= LPA VisiRule | accessdate=2013-04-29}}</ref> a graphical tool for developing knowledge-based and decision support systems. VisiRule has been used in various sectors, to build [[legal expert systems]], machine diagnostic programs, medical and financial advice systems, etc.

==Customers==
For many years, LPA has worked closely with [[Valdis Krebs]], an American-Latvian researcher, author, and consultant in the field of social and organizational network analysis. Valdis is the founder and chief scientist of Orgnet, and the creator of the popular Inflow <ref name = "InFlow">[http://www.orgnet.com/inflow3.html InFlow]</ref> software package.

==External links==
*[http://www.lpa.co.uk/ind_pro.htm LPA home page]
*[http://www.lpa.co.uk/abo_lpa.htm About LPA]
*[[:es:Micro-PROLOG|Micro-PROLOG (in Spanish)]]
*[http://www.teamethno-online.org.uk/Issue2/Rouchy.pdf Aspects of PROLOG History]
*[http://www.lpa.co.uk/vrs_dem.htm VisiRule demos]
*[http://dssresources.com/news/83.php VisiRule: a new graphical business rules tool from LPA]
*[http://dl.acm.org/citation.cfm?id=297981 A flex-based expert system for sewage treatment works support]
*[http://www.lamsade.dauphine.fr/~tsoukias/papers/esse.pdf ESSE: An Expert System for Software Evaluation]

== References ==
{{Reflist}}

[[Category:Information technology organisations]]
[[Category:Software companies of the United Kingdom]]
[[Category:Expert systems]]
[[Category:Knowledge engineering]]
[[Category:Knowledge representation]]


{{compu-ai-stub}}
<=====doc_Id=====>:890
<=====title=====>:
Argument map
<=====text=====>:
[[File:Argument Map.png|thumb|A schematic argument map showing a contention (or conclusion), supporting arguments and objections, and an inference objection.]]
In [[informal logic]] and [[philosophy]], an '''argument map''' or '''argument diagram''' is a visual representation of the structure of an [[argument]]. An argument map typically includes the key components of the argument, traditionally called the ''[[Logical consequence|conclusion]]'' and the ''[[premise]]s'', also called ''[[Main contention|contention]]'' and ''[[Reason (argument)|reason]]s''.<ref>{{harvnb|Freeman|1991|pp=49–90}}</ref> Argument maps can also show [[co-premise]]s, [[Objection (argument)|objection]]s, [[counterargument]]s, [[rebuttal]]s, and [[Lemma (logic)|lemma]]s. There are different styles of argument map but they are often functionally equivalent and represent an argument's individual claims and the relationships between them.

Argument maps are commonly used in the context of teaching and applying [[critical thinking]].<ref>For example: {{harvnb|Davies|2012}}; {{harvnb|Facione|2013|p=86}}; {{harvnb|Fisher|2004}}; {{harvnb|Kelley|2014|p=73}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}; {{harvnb|Walton|2013|p=10}}</ref> The purpose of mapping is to uncover the logical structure of arguments, identify unstated assumptions, evaluate the support an argument offers for a conclusion, and aid understanding of debates. Argument maps are often designed to support deliberation of issues, ideas and arguments in [[wicked problem]]s.<ref>For example: {{harvnb|Culmsee|Awati|2013}}; {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Metcalfe|Sastrowardoyo|2013}}; Ricky Ohl, [//books.google.com/books?id=loa4BAAAQBAJ&pg=PA360 "Computer supported argument visualisation: modelling in consultative democracy around wicked problems"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=361–380}}</ref>

An argument map is not to be confused with a [[concept map]] or a [[mind map]], which are less strict in relating claims.

==Key features of an argument map==
A number of different kinds of argument map have been proposed but the most common, which Chris Reed and Glenn Rowe called the ''standard diagram'',<ref name="ReedRowe64">{{harvnb|Reed|Rowe|2007|p=64}}</ref> consists of a [[tree structure]] with each of the reasons leading to the conclusion. There is no consensus as to whether the conclusion should be at the top of the tree with the reasons leading up to it or whether it should be at the bottom with the reasons leading down to it.<ref name="ReedRowe64"/> Another variation diagrams an argument from left to right.<ref>For example: {{harvnb|Walton|2013|pp=18–20}}</ref>

According to [[Doug Walton]] and colleagues, an argument map has two basic components: "One component is a set of circled numbers arrayed as points. Each number represents a proposition (premise or conclusion) in the argument being diagrammed. The other component is a set of lines or arrows joining the points. Each line (arrow) represents an inference. The whole network of points and lines represents a kind of overview of the reasoning in the given argument..."<ref>{{harvnb|Reed|Walton|Macagno|2007|p=2}}</ref> With the introduction of software for producing argument maps, it has become common for argument maps to consist of boxes containing the actual propositions rather than numbers referencing those propositions.

There is disagreement on the terminology to be used when describing argument maps,<ref>{{harvnb|Freeman|1991|pp=49–90}}; {{harvnb|Reed|Rowe|2007}}</ref> but the ''standard diagram'' contains the following structures:

'''Dependent premises''' or '''co-premises''', where at least one of the joined premises requires another premise before it can give support to the conclusion: An argument with this structure has been called a ''linked'' argument.<ref>{{harvnb|Harrell|2010|p=19}}</ref>

[[File:Dependent premises.jpg|thumb|centre|100px|Statements 1 and 2 are dependent premises or co-premises]]

'''Independent premises''', where the premise can support the conclusion on its own: Although independent premises may jointly make the conclusion more convincing, this is to be distinguished from situations where a premise gives no support unless it is joined to another premise. Where several premises or groups of premises lead to a final conclusion the argument might be described as ''convergent''. This is distinguished from a ''divergent'' argument where a single premise might be used to support two separate conclusions.<ref>{{harvnb|Freeman|1991|pp=91–110}}; {{harvnb|Harrell|2010|p=20}}</ref>

[[File:Independent premises.jpg|thumb|centre|150px|Statements 2, 3, 4 are independent premises]]

'''Intermediate conclusions''' or '''sub-conclusions''', where a claim is supported by another claim that is used in turn to support some further claim, i.e. the final conclusion or another intermediate conclusion: In the following diagram, statement '''4''' is an intermediate conclusion in that it is a conclusion in relation to statement '''5''' but is a premise in relation to the final conclusion, i.e. statement '''1'''. An argument with this structure is sometimes called a ''complex'' argument. If there is a single chain of claims containing at least one intermediate conclusion, the argument is sometimes described as a ''serial'' argument or a ''chain'' argument.<ref>{{harvnb|Beardsley|1950|pp=18–19}}; {{harvnb|Reed|Walton|Macagno|2007|pp=3–8}}; {{harvnb|Harrell|2010|pp=19–21}}</ref>

[[File:Intermediate conclusion.jpg|thumb|centre|150px|Statement 4 is an intermediate conclusion or sub-conclusion]]

Each of these structures can be represented by the equivalent "box and line" approach to argument maps. In the following diagram, the ''contention'' is shown at the top, and the boxes linked to it represent supporting ''reasons'', which comprise one or more ''premises''. The green arrow indicates that the two ''reasons'' support the ''contention'':

[[File:A box and line diagram.png|thumb|center|A box and line diagram]]

Argument maps can also represent counterarguments. In the following diagram, the two ''objections'' weaken the ''contention'', while the ''reasons'' support the ''premise'' of the objection:

[[File:A sample argument using objections.png|thumb|center|A sample argument using objections]]

==Representing an argument as an argument map==
A written text can be transformed into an argument map by following a sequence of steps. [[Monroe Beardsley]]'s 1950 book ''Practical Logic'' recommended the following procedure:<ref name="Beardsley">{{harvnb|Beardsley|1950}}</ref>
#Separate statements by brackets and number them.
#Put circles around the logical indicators.
#Supply, in parenthesis, any logical indicators that are left out.
#Set out the statements in a diagram in which arrows show the relationships between statements.

[[File:Diagram using Beardsley's procedure.jpg|thumb|right|100px|A diagram of the example from Beardsley's ''Practical Logic'']]

Beardsley gave the first example of a text being analysed in this way:

:Though <span style="color:red;">① [</span>people who talk about the "social significance" of the arts don’t like to admit it<span style="color:red;">]</span>, <span style="color:red;">② [</span>music and painting are bound to suffer when they are turned into mere vehicles for propaganda<span style="color:red;">]</span>. <span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;">For</span> <span style="color:red;">③ [</span>propaganda appeals to the crudest and most vulgar feelings<span style="color:red;">]</span>: <span style="color:red;">(for)</span> <span style="color:red;">④ [</span>look at the academic monstrosities produced by the official Nazi painters<span style="color:red;">]</span>. What is more important, <span style="color:red;">⑤ [</span>art must be an end in itself for the artist<span style="color:red;">]</span>, <span style="width:100%; border:solid 2px blue; padding-left:5px; padding-right:5px; border-radius:25px;">because</span> <span style="color:red;">⑥ [</span>the artist can do the best work only in an atmosphere of complete freedom<span style="color:red;">]</span>.

Beardsley said that the conclusion in this example is statement ②. Statement ④ needs to be rewritten as a declarative sentence, e.g. "Academic monstrosities [were] produced by the official Nazi painters." Statement ① points out that the conclusion isn't accepted by everyone, but statement ① is omitted from the diagram because it doesn't support the conclusion. Beardsley said that the logical relation between statement ③ and statement ④ is unclear, but he proposed to diagram statement ④ as supporting statement ③.

[[File:Using Harrell's procedure.jpg|thumb|right|200px|A box and line diagram of Beardsley's example, produced using Harrell's procedure]]

More recently, philosophy professor Maralee Harrell recommended the following procedure:<ref>{{harvnb|Harrell|2010|p=28}}</ref>
#Identify all the claims being made by the author.
#Rewrite them as independent statements, eliminating non-essential words.
#Identify which statements are premises, sub-conclusions, and the main conclusion.
#Provide missing, implied conclusions and implied premises. (This is optional depending on the purpose of the argument map.)
#Put the statements into boxes and draw a line between any boxes that are linked.
#Indicate support from premise(s) to (sub)conclusion with arrows.

Argument maps are useful not only for representing and analyzing existing writings, but also for thinking through issues as part of a [[Problem structuring methods|problem-structuring process]] or [[writing process]]. The use of such argument analysis for thinking through issues has been called "reflective argumentation".<ref>For example: {{harvnb|Hoffmann|Borenstein|2013}}; {{harvnb|Hoffmann|2015}}</ref>

==History==

===The philosophical origins and tradition of argument mapping===
[[File:Whatley.png|thumb|From Whately's Elements of Logic p467, 1852 edition]]
In the ''Elements of Logic'', which was published in 1826 and issued in many subsequent editions,<ref>{{harvnb|Whately|1834}} (first published 1826)</ref> Archbishop [[Richard Whately]] gave probably the first form of an argument map, introducing it with the suggestion that "many students probably will find it a very clear and convenient mode of exhibiting the logical analysis of the course of argument, to draw it out in the form of a Tree, or Logical Division".

However, the technique did not become widely used, possibly because for complex arguments, it involved much writing and rewriting of the premises.

[[File:Wigmore chart.png|thumb|Wigmore evidence chart, from 1905]]
Legal philosopher and theorist [[John Henry Wigmore]] produced maps of legal arguments using numbered premises in the early 20th century,<ref>{{harvnb|Wigmore|1913}}</ref> based in part on the ideas of 19th century philosopher [[Henry Sidgwick]] who used lines to indicate relations between terms.<ref>{{harvnb|Goodwin|2000}}</ref>

===Anglophone argument diagramming in the 20th century===
Dealing with the failure of [[Formal system|formal]] reduction of informal argumentation, English speaking [[argumentation theory]] developed diagrammatic approaches to informal reasoning over a period of fifty years.

[[Monroe Beardsley]] proposed a form of argument diagram in 1950.<ref name="Beardsley"/> His method of marking up an argument and representing its components with linked numbers became a standard and is still widely used. He also introduced terminology that is still current describing ''convergent'', ''divergent'' and ''serial'' arguments.

[[File:Toulmindiag.png|thumb|A Toulmin argument diagram, redrawn from his 1959 ''Uses of Argument'']]
[[File:Toulmingeneral.png|thumb|A generalised Toulmin diagram]]
[[Stephen Toulmin]], in his groundbreaking and influential 1958 book ''The Uses of Argument'',<ref>{{harvnb|Toulmin|2003}} (first published 1958)</ref> identified several elements to an argument which have been generalized. The Toulmin diagram is widely used in educational critical teaching.<ref name="Simon06">{{harvnb|Simon|Erduran|Osborne|2006}}</ref><ref>{{harvnb|Böttcher|Meisert|2011}}; {{harvnb|Macagno|Konstantinidou|2013}}</ref> Whilst Toulmin eventually had a significant impact on the development of [[informal logic]] he had little initial impact and the Beardsley approach to diagramming arguments along with its later developments became the standard approach in this field. Toulmin introduced something that was missing from Beardsley's approach. In Beardsley, "arrows link reasons and conclusions (but) no support is given to the implication itself between them. There is no theory, in other words, of inference distinguished from logical deduction, the passage is always deemed not controversial and not subject to support and evaluation".<ref>{{harvnb|Reed|Walton|Macagno|2007|p=8}}</ref> Toulmin introduced the concept of ''warrant'' which "can be considered as representing the reasons behind the inference, the backing that authorizes the link".<ref>{{harvnb|Reed|Walton|Macagno|2007|p=9}}</ref>

Beardsley's approach was refined by Stephen N. Thomas, whose 1973 book ''Practical Reasoning In Natural Language''<ref>{{harvnb|Thomas|1997}} (first published 1973)</ref> introduced the term ''linked'' to describe arguments where the premises necessarily worked together to support the conclusion.<ref name="SnoeckHenkemans">{{harvnb|Snoeck Henkemans|2000|p=453}}</ref> However, the actual distinction between dependent and independent premises had been made prior to this.<ref name="SnoeckHenkemans"/> The introduction of the linked structure made it possible for argument maps to represent missing or "hidden" premises. In addition, Thomas suggested showing reasons both ''for'' and ''against'' a conclusion with the reasons ''against'' being represented by dotted arrows. Thomas introduced the term ''argument diagram'' and defined ''basic reasons'' as those that were not supported by any others in the argument and the ''final conclusion'' as that which was not used to support any further conclusion.

[[File:Scrivendiag.png|thumb|Scriven's argument diagram. The explicit premise 1 is conjoined with additional unstated premises a and b to imply 2.]]
[[Michael Scriven]] further developed the Beardsley-Thomas approach in his 1976 book ''Reasoning''.<ref>{{harvnb|Scriven|1976}}</ref> Whereas Beardsley had said "At first, write out the statements...after a little practice, refer to the statements by number alone"<ref>{{harvnb|Beardsley|1950|p=21}}</ref> Scriven advocated clarifying the meaning of the statements, listing them and then using a tree diagram with numbers to display the structure. Missing premises (unstated assumptions) were to be included and indicated with an alphabetical letter instead of a number to mark them off from the explicit statements. Scriven introduced counterarguments in his diagrams, which Toulmin had defined as rebuttal.<ref>{{harvnb|Reed|Walton|Macagno|2007|pp=10–11}}</ref> This also enabled the diagramming of "balance of consideration" arguments.<ref>{{harvnb|van Eemeren|Grootendorst|Snoeck Henkemans|Blair|1996|p=175}}</ref>

In the 1990s, [[Tim van Gelder]] and colleagues developed a series of computer software applications that permitted the premises to be fully stated and edited in the diagram, rather than in a legend.<ref>{{harvnb|van Gelder|2007}}</ref> Van Gelder's first program, Reason!Able, was superseded by two subsequent programs, bCisive and Rationale.<ref>{{harvnb|Berg|van Gelder|Patterson|Teppema|2009}}</ref>

Throughout the 1990s and 2000s, many other software applications were developed for argument visualization. By 2013, more than 60 such software systems existed.<ref>{{harvnb|Walton|2013|p=11}}</ref> One of the differences between these software systems is whether collaboration is supported.<ref name="Scheuer10">{{harvnb|Scheuer|Loll|Pinkwart|McLaren|2010}}</ref> Single-user argumentation systems include Convince Me, iLogos, LARGO, Athena, [[Araucaria (software)|Araucaria]], and Carneades; small group argumentation systems include Digalo, QuestMap, [[Compendium (software)|Compendium]], Belvedere, and AcademicTalk; community argumentation systems include [[Debategraph]] and [[Collaboratorium]].<ref name="Scheuer10"/> For more software examples, see: {{section link||External links}}.

In 1998 a series of large-scale argument maps released by [[Robert E. Horn]] stimulated widespread interest in argument mapping.<ref>{{harvnb|Holmes|1999}}; {{harvnb|Horn|1998}} and Robert E. Horn, [http://www.stanford.edu/~rhorn/a/topic/arg/artclCmptrSpArgmttn.pdf "Infrastructure for navigating interdisciplinary debates: critical decisions for representing argumentation"], in {{harvnb|Kirschner|Buckingham Shum|Carr|2003|pp=165–184}}</ref>

==Applications==
Argument maps have been applied in many areas, but foremost in educational, academic and business settings, including [[design rationale]].<ref name="Applications">{{harvnb|Kirschner|Buckingham Shum|Carr|2003}}; {{harvnb|Okada|Buckingham Shum|Sherborne|2014}}</ref> Argument maps are also used in [[forensic science]],<ref>For example: {{harvnb|Bex|2011}}</ref> [[law]], and [[artificial intelligence]].<ref>For example: {{harvnb|Verheij|2005}}; {{harvnb|Reed|Walton|Macagno|2007}}; {{harvnb|Walton|2013}}</ref> It has also been proposed that argument mapping has a great potential to improve how we understand and execute democracy, in reference to the ongoing evolution of [[e-democracy]].<ref>{{harvnb|Hilbert|2009}}</ref>

===Difficulties with the philosophical tradition===
It has traditionally been hard to separate teaching critical thinking from the philosophical tradition of teaching [[logic]] and method, and most critical thinking textbooks have been written by philosophers. [[Informal logic]] textbooks are replete with philosophical examples, but it is unclear whether the approach in such textbooks transfers to non-philosophy students.<ref name="Simon06" /> There appears to be little statistical effect after such classes. Argument mapping, however, has a measurable effect according to many studies.<ref>{{harvnb|Twardy|2004}}; {{harvnb|Álvarez Ortiz|2007}}; {{harvnb|Harrell|2008}}; Yanna Rider and Neil Thomason, [//books.google.com/books?id=loa4BAAAQBAJ&pg=PA112 "Cognitive and pedagogical benefits of argument mapping: LAMP guides the way to better thinking"], in {{harvnb|Okada|Buckingham Shum|Sherborne|2014|pp=113–134}}; {{harvnb|Dwyer|2011}}; {{harvnb|Davies|2012}}</ref> For example, instruction in argument mapping has been shown to improve the critical thinking skills of business students.<ref>{{harvnb|Carrington|Chen|Davies|Kaur|2011}}; {{harvnb|Kunsch|Schnarr|van Tyle|2014}}</ref>

===Evidence that argument mapping improves critical thinking ability===
There is empirical evidence that the skills developed in argument-mapping-based critical thinking courses substantially transfer to critical thinking done without argument maps. Alvarez's meta-analysis found that such critical thinking courses produced gains of around 0.70 SD, about twice as much as standard critical-thinking courses.<ref>{{harvnb|Álvarez Ortiz|2007|pp=69–70 ''et seq''}}</ref> The tests used in the reviewed studies were standard critical-thinking tests.

===How argument mapping helps with critical thinking===
The use of argument mapping has occurred within a number of disciplines, such as philosophy, management reporting, military and intelligence analysis, and public debates.<ref name="Applications"/>

''Logical structure'': Argument maps display an argument's logical structure more clearly than does the standard linear way of presenting arguments.

''Critical thinking concepts'': In learning to argument map, students master such key critical thinking concepts as "reason", "objection", "premise", "conclusion", "inference", "rebuttal", "unstated assumption", "co-premise", "strength of evidence", "logical structure", "independent evidence", etc. Mastering such concepts is not just a matter of memorizing their definitions or even being able to apply them correctly; it is also understanding why the distinctions these words mark are important and using that understanding to guide one's reasoning.

''Visualization'': Humans are highly visual and argument mapping may provide students with a basic set of visual schemas with which to understand argument structures.

''More careful reading and listening'': Learning to argument map teaches people to read and listen more carefully, and highlights for them the key questions "What is the logical structure of this argument?" and "How does this sentence fit into the larger structure?" In-depth cognitive processing is thus more likely.

''More careful writing and speaking'': Argument mapping helps people to state their reasoning and evidence more precisely, because the reasoning and evidence must fit explicitly into the map's logical structure.

''Literal and intended meaning'': Often, many statements in an argument do not precisely assert what the author meant. Learning to argument map enhances the complex skill of distinguishing literal from intended meaning.

''Externalization'': Writing something down and reviewing what one has written often helps reveal gaps and clarify one's thinking. Because the logical structure of argument maps is clearer than that of linear prose, the benefits of mapping will exceed those of ordinary writing.

''Anticipating replies'': Important to critical thinking is anticipating objections and considering the plausibility of different rebuttals. Mapping develops this anticipation skill, and so improves analysis.

==Standards==

===Argument Interchange Format===
The Argument Interchange Format, AIF, is an international effort to develop a representational mechanism for exchanging argument resources between research groups, tools, and domains using a semantically rich language.<ref>See the [http://www.arg.dundee.ac.uk/people/chris/publications/2006/aif_final.pdf AIF original draft description] (2006) and the [http://www.argdf.org/source/ArgDF_Ontology.rdfs full AIF-RDF ontology specifications] in [[RDFS]] format.</ref> AIF-RDF is the extended ontology represented in the [[Resource Description Framework]] Schema (RDFS) semantic language. Though AIF is still something of a moving target, it is settling down.<ref>{{harvnb|Bex|Modgil|Prakken|Reed|2013}}</ref>

===Legal Knowledge Interchange Format===
The Legal Knowledge Interchange Format (LKIF),<ref>{{harvnb|Boer|Winkels|Vitali|2008}}</ref> developed in the European ESTRELLA project,<ref>{{cite web |title=Estrella project website |url=http://www.estrellaproject.org/ |website=estrellaproject.org |archiveurl=https://web.archive.org/web/20160212103522/http://www.estrellaproject.org/ |archivedate=2016-02-12 |accessdate=2016-02-24}}</ref> is an XML schema for rules and arguments, designed with the goal of becoming a standard for representing and interchanging policy, legislation and cases, including their justificatory arguments, in the legal domain. LKIF builds on and uses the [[Web Ontology Language]] (OWL) for representing concepts and includes a reusable basic ontology of legal concepts.

== See also ==
{{Commons category|Argument maps}}
* [[Flow (policy debate)]]
* [[Informal fallacy]]
* [[Information graphics]]
* [[Natural deduction]], a logical system with argument map-like notation

== Notes ==
{{Reflist|colwidth=20em}}

== References ==
* {{cite thesis |type=M.A. thesis |last=Álvarez Ortiz |first=Claudia María |title=Does philosophy improve critical thinking skills? |url=http://images.austhink.com/pdf/Claudia-Alvarez-thesis.pdf |year=2007 |publisher=Department of Philosophy, [[University of Melbourne]] |oclc=271475715 |ref=harv}}
* {{cite book |last=Beardsley |first=Monroe C. |authorlink=Monroe Beardsley |date=1950 |title=Practical logic |location=New York |publisher=Prentice-Hall |oclc=4318971 |ref=harv}}
* {{cite book |last1=Berg |first1=Timo ter |last2=van Gelder |first2=Tim |authorlink2=Tim van Gelder |last3=Patterson |first3=Fiona |last4=Teppema |first4=Sytske |year=2009 |title=Critical thinking: reasoning and communicating with Rationale |location=Amsterdam |publisher=Pearson Education Benelux |isbn=9043018015 |oclc=301884530 |ref=harv}}
* {{cite book |last=Bex |first=Floris J. |date=2011 |title=Arguments, stories and criminal evidence: a formal hybrid theory |series=Law and philosophy library |volume=92 |location=Dordrecht; New York |publisher=Springer |isbn=9789400701397 |oclc=663950184 |doi=10.1007/978-94-007-0140-3 |ref=harv}}
* {{cite journal |last1=Bex |first1=Floris J. |last2=Modgil |first2=Sanjay |last3=Prakken |first3=Henry |last4=Reed |first4=Chris |title=On logical specifications of the Argument Interchange Format |journal=[[Journal of Logic and Computation]] |volume=23 |issue=5 |pages=951–989 |year=2013 |doi=10.1093/logcom/exs033 |url=http://www.arg.dundee.ac.uk/people/chris/publications/2013/AIF-ASPIC-JLC-Final.pdf |ref=harv}}
* {{cite book |last1=Boer |first1=Alexander |last2=Winkels |first2=Radboud |last3=Vitali |first3=Fabio |date=2008 |chapter=MetaLex XML and the Legal Knowledge Interchange Format |editor1-last=Casanovas |editor1-first=Pompeu |editor2-last=Sartor |editor2-first=Giovanni |editor3-last=Casellas |editor3-first=Núria |editor4-last=Rubino |editor4-first=Rossella |title=Computable models of the law: languages, dialogues, games, ontologies |series=Lecture notes in computer science |volume=4884 |location=Berlin; New York |publisher=Springer |pages=21–41 |isbn=9783540855682 |oclc=244765580 |doi=10.1007/978-3-540-85569-9_2 |chapterurl=https://www.researchgate.net/profile/Radboud_Winkels/publication/225708550_MetaLex_XML_and_the_Legal_Knowledge_Interchange_Format/links/02bfe510239cc79c8d000000.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Böttcher |first1=Florian |last2=Meisert |first2=Anke |title=Argumentation in science education: a model-based framework |journal=Science & Education |volume=20 |issue=2 |pages=103–140 |date=February 2011 |doi=10.1007/s11191-010-9304-5 |ref=harv}}
* {{cite journal |last1=Carrington |first1=Michal |last2=Chen |first2=Richard |last3=Davies |first3=Martin |last4=Kaur |first4=Jagjit |last5=Neville |first5=Benjamin |title=The effectiveness of a single intervention of computer‐aided argument mapping in a marketing and a financial accounting subject |journal=Higher Education Research & Development |volume=30 |issue=3 |pages=387–403 |date=June 2011 |doi=10.1080/07294360.2011.559197 |url=https://www.researchgate.net/profile/Martin_Davies/publication/232947753_The_effectiveness_of_a_single_intervention_of_computeraided_argument_mapping_in_a_marketing_and_a_financial_accounting_subject/links/0deec51d94fec63bdc000000.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite book |last1=Culmsee |first1=Paul |last2=Awati |first2=Kailash |date=2013 |origyear=2011 |chapter=Chapter 7: Visualising reasoning, and Chapter 8: Argumentation-based rationale |title=The heretic's guide to best practices: the reality of managing complex problems in organisations |location=Bloomington, IN |publisher=iUniverse, Inc. |pages=153–211 |isbn=9781462058549 |oclc=767703320 |chapterurl=https://books.google.com/books?id=Gb2uuT1zrAAC&pg=PA153 |ref=harv}}
* {{cite journal |last=Davies |first=Martin |title=Computer-aided argument mapping as a tool for teaching critical thinking |journal=[[International Journal of Learning and Media]] |volume=4 |issue=3-4 |pages=79–84 |date=Summer 2012 |doi=10.1162/IJLM_a_00106 |url=http://www.mitpressjournals.org/doi/full/10.1162/IJLM_a_00106 |ref=harv}}
* {{cite thesis |type=Ph.D. thesis |last=Dwyer |first=Christopher Peter |title=The evaluation of argument mapping as a learning tool |url=http://aran.library.nuigalway.ie/xmlui/bitstream/handle/10379/2617/C.Dwyer-PhD%20Thesis%20Psychology.pdf |year=2011 |publisher=School of Psychology, National University of Ireland, Galway |oclc=812818648 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Grootendorst |first2=Rob |authorlink2=Rob Grootendorst |last3=Snoeck Henkemans |first3=A. Francisca |last4=Blair |first4=J. Anthony |last5=Johnson |first5=Ralph H. |authorlink5=Ralph Johnson (philosopher) |last6=Krabbe |first6=Erik C. W. |last7=Plantin |first7=Christian |last8=Walton |first8=Douglas N. |authorlink8=Doug Walton |last9=Willard |first9=Charles A. |authorlink9=Charles Arthur Willard |last10=Woods |first10=John |authorlink10=John Woods (logician) |date=1996 |title=Fundamentals of argumentation theory: a handbook of historical backgrounds and contemporary developments |location=Mahwah, NJ |publisher=[[Lawrence Erlbaum Associates]] |isbn=0805818618 |oclc=33970847 |doi=10.4324/9780203811306 |ref=harv}}
* {{cite book |last=Facione |first=Peter A. |title=THINK critically |year=2013 |origyear=2011 |edition=2nd |location=Boston |publisher=Pearson |isbn=0205490980 |oclc=770694200 |ref=harv}}
* {{cite book |last=Fisher |first=Alec |title=The logic of real arguments |url=https://books.google.com/books?id=Q2a_RtDZUGgC |year=2004 |origyear=1988 |edition=2nd |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521654815 |oclc=54400059 |doi=10.1017/CBO9780511818455 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Freeman |first=James B. |title=Dialectics and the macrostructure of arguments: a theory of argument structure |url=https://books.google.com/books?id=ScvV9riTOGsC |year=1991 |location=Berlin; New York |publisher=Foris Publications |isbn=3110133903 |oclc=24429943 |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last=van Gelder |first=Tim |authorlink=Tim van Gelder |title=The rationale for Rationale |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=23–42 |year=2007 |doi=10.1093/lpr/mgm032 |url=http://sites.google.com/site/timvangelder/publications-1/therationaleforrationale/TheRationaleforRationale.pdf |ref=harv}}
* {{cite journal |last=Goodwin |first=Jean |title=Wigmore's chart method |journal=Informal Logic |volume=20 |issue=3 |pages=223–243 |year=2000 |url=http://windsor.scholarsportal.info/ojs/leddy/index.php/informal_logic/article/view/2278 |ref=harv}}
* {{cite journal |last=Harrell |first=Maralee |title=No computer program required: even pencil-and-paper argument mapping improves critical-thinking skills |journal=[[Teaching Philosophy]] |volume=31 |issue=4 |pages=351–374 |date=December 2008 |doi=10.5840/teachphil200831437 |url=http://www.hss.cmu.edu/philosophy/harrell/HarrellTeachingPhilosophy2008.pdf |ref=harv}}
* {{cite web |last=Harrell |first=Maralee |title=Creating argument diagrams |date=August 2010 |url=http://www.academia.edu/772321/Creating_Argument_Diagrams |website=[[academia.edu]] |ref=harv}}
* {{cite journal |last=Hilbert |first=Martin |title=The maturing concept of e-democracy: from e-voting and online consultations to democratic value out of jumbled online chatter |journal=[[Journal of Information Technology and Politics]] |date=April 2009 |volume=6 |issue=2 |pages=87–110 |doi=10.1080/19331680802715242 |url=http://www.martinhilbert.net/e-democracyHilbertJITP.pdf |ref=harv}}
* {{cite journal |last=Hoffmann |first=Michael H. G. |date=November 2015 |title=Reflective argumentation: a cognitive function of arguing |journal=Argumentation |doi=10.1007/s10503-015-9388-9 |ref=harv}}
* {{cite journal |last1=Hoffmann |first1=Michael H. G. |last2=Borenstein |first2=Jason |date=February 2013 |title=Understanding ill-structured engineering ethics problems through a collaborative learning and argument visualization approach |journal=Science and Engineering Ethics |volume=20 |issue=1 |pages=261–276 |doi=10.1007/s11948-013-9430-y |pmid=23420467 |url=http://works.bepress.com/michael_hoffmann/39/ |ref=harv}}
* {{cite news |last=Holmes |first=Bob |title=Beyond words |url=http://www.newscientist.com/article/mg16321944.700-beyond-words.html |date=10 July 1999 |newspaper=New Scientist |issue=2194 |archiveurl=https://web.archive.org/web/20080928062226/http://www.newscientist.com/article/mg16321944.700-beyond-words.html |archivedate=28 September 2008 |ref=harv}}
* {{cite book |last=Horn |first=Robert E. |authorlink=Robert E. Horn |title=Visual language: global communication for the 21st century |year=1998 |location=Bainbridge Island, WA |publisher=MacroVU, Inc. |isbn=189263709X |oclc=41138655 |ref=harv}}
* {{cite book |last=Kelley |first=David |authorlink=David Kelley |title=The art of reasoning: an introduction to logic and critical thinking |year=2014 |origyear=1988 |edition=4th |location=New York |publisher=W. W. Norton & Company |isbn=0393930785 |oclc=849801096 |ref=harv}}
* {{cite book |editor1-last=Kirschner |editor1-first=Paul Arthur |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Carr |editor3-first=Chad S |title=Visualizing argumentation: software tools for collaborative and educational sense-making |year=2003 |series=Computer supported cooperative work |location=New York |publisher=Springer |isbn=1852336641 |oclc=50676911 |doi=10.1007/978-1-4471-0037-9 |url=https://books.google.com/books?id=dNijwv-my_kC |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Kunsch |first1=David W. |last2=Schnarr |first2=Karin |last3=van Tyle |first3=Russell |title=The use of argument mapping to enhance critical thinking skills in business education |journal=Journal of Education for Business |volume=89 |issue=8 |pages=403–410 |date=November 2014 |doi=10.1080/08832323.2014.925416 |url= |ref=harv}}
* {{cite journal |last1=Macagno |first1=Fabrizio |last2=Konstantinidou |first2=Aikaterini |title=What students' arguments can tell us: using argumentation schemes in science education |journal=Argumentation |volume=27 |issue=3 |pages=225–243 |date=August 2013 |doi=10.1007/s10503-012-9284-5 |url=http://ssrn.com/abstract=2185945 |ref=harv}}
* {{cite journal |last1=Metcalfe |first1=Mike |last2=Sastrowardoyo |first2=Saras |date=November 2013 |title=Complex project conceptualisation and argument mapping |journal=International Journal of Project Management |volume=31 |issue=8 |pages=1129–1138 |doi=10.1016/j.ijproman.2013.01.004 |ref=harv}}
* {{cite book |editor1-last=Okada |editor1-first=Alexandra |editor2-last=Buckingham Shum |editor2-first=Simon J |editor3-last=Sherborne |editor3-first=Tony |year=2014 |origyear=2008 |title=Knowledge cartography: software tools and mapping techniques |edition=2nd |series=Advanced information and knowledge processing |location=New York |publisher=Springer |isbn=9781447164692 |oclc=890438015 |doi=10.1007/978-1-4471-6470-8 |url=https://books.google.com/books?id=loa4BAAAQBAJ |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Reed |first1=Chris |last2=Rowe |first2=Glenn |title=A pluralist approach to argument diagramming |journal=Law, Probability and Risk |volume=6 |issue=1-4 |pages=59–85 |year=2007 |doi=10.1093/lpr/mgm030 |url=http://lpr.oxfordjournals.org/content/6/1-4/59.full.pdf |ref=harv |accessdate=2016-02-24}}
* {{cite journal |last1=Reed |first1=Chris |last2=Walton |first2=Douglas |authorlink2=Doug Walton |last3=Macagno |first3=Fabrizio |title=Argument diagramming in logic, law and artificial intelligence |journal=The Knowledge Engineering Review |volume=22 |issue=1 |pages=1–22 |date=March 2007 |doi=10.1017/S0269888907001051 |url=https://www.academia.edu/2992281/Argument_Diagramming_in_Logic_Artificial_Intelligence_and_Law |ref=harv}}
* {{cite journal |last1=Scheuer |first1=Oliver |last2=Loll |first2=Frank |last3=Pinkwart |first3=Niels |last4=McLaren |first4=Bruce M. |title=Computer-supported argumentation: a review of the state of the art |journal=International Journal of Computer-Supported Collaborative Learning |volume=5 |issue=1 |pages=43–102 |year=2010 |doi=10.1007/s11412-009-9080-x |url=http://www.oliver-scheuer.info/publications/ScheuerEtAl-EdArgSystemsReview-IJCSCL-2010.pdf |ref=harv}}
* {{cite book |last=Scriven |first=Michael |authorlink=Michael Scriven |title=Reasoning |year=1976 |location=New York |publisher=McGraw-Hill |isbn=0070558825 |oclc=2800373 |ref=harv}}
* {{cite journal |last1=Simon |first1=Shirley |last2=Erduran |first2=Sibel |last3=Osborne |first3=Jonathan |title=Learning to teach argumentation: research and development in the science classroom |journal=International Journal of Science Education |volume=28 |issue=2-3 |pages=235–260 |year=2006 |doi=10.1080/09500690500336957 |url=http://cset.stanford.edu/sites/default/files/files/documents/publications/Simon-Learning%20to%20Teach%20Argumentation.pdf |ref=harv}}
* {{cite journal |last=Snoeck Henkemans |first=A. Francisca |date=November 2000 |title=State-of-the-art: the structure of argumentation |journal=Argumentation |volume=14 |issue=4 |pages=447–473 |doi=10.1023/A:1007800305762 |ref=harv}}
* {{cite book |last=Thomas |first=Stephen N. |date=1997 |origyear=1973 |title=Practical reasoning in natural language |edition=4th |location=Upper Saddle River, NJ |publisher=[[Prentice-Hall]] |isbn=0136782698 |oclc=34745923 |ref=harv}}
* {{cite book |last=Toulmin |first=Stephen E. |authorlink=Stephen Toulmin |title=The uses of argument |url=https://books.google.com/books?id=8UYgegaB1S0C |year=2003 |origyear=1958 |edition=Updated |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=0521534836 |oclc=57253830 |doi=10.1017/CBO9780511840005 |accessdate=2016-02-24 }}
* {{cite journal |last=Twardy |first=Charles R. |title=Argument maps improve critical thinking |journal=[[Teaching Philosophy]] |volume=27 |issue=2 |pages=95–116 |date=June 2004 |doi=10.5840/teachphil200427213 |url=http://cogprints.org/3008/1/reasonpaper.pdf |ref=harv}}
* {{cite book |last=Verheij |first=Bart |title=Virtual arguments: on the design of argument assistants for lawyers and other arguers |year=2005 |location=The Hague |publisher=T.M.C. Asser Press |series=Information technology & law series |volume=6 |isbn=9789067041904 |oclc=59617214 |ref=harv}}
* {{cite book |last=Walton |first=Douglas N. |authorlink=Doug Walton |title=Methods of argumentation |url=https://books.google.com/books?id=vaU0AAAAQBAJ |year=2013 |location=Cambridge; New York |publisher=[[Cambridge University Press]] |isbn=1107677335 |oclc=830523850 |doi=10.1017/CBO9781139600187 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Whately |first=Richard |authorlink=Richard Whately |title=Elements of logic: comprising the substance of the article in the Encyclopædia metropolitana: with additions, &c. |url=https://books.google.com/books?id=5mgAAAAAMAAJ |year=1834 |origyear=1826 |edition=5th |location=London |publisher=B. Fellowes |oclc=1739330 |ref=harv |accessdate=2016-02-24}}
* {{cite book |last=Wigmore |first=John Henry |authorlink=John Henry Wigmore |title=The principles of judicial proof: as given by logic, psychology, and general experience, and illustrated in judicial trials |url=https://books.google.com/books?id=4ho-AAAAIAAJ |year=1913 |location=Boston |publisher=Little Brown |oclc=1938596 |ref=harv |accessdate=2016-02-24}}

== Further reading ==
* {{cite book |last1=van Eemeren |first1=Frans H. |authorlink1=Frans H. van Eemeren |last2=Garssen |first2=Bart |last3=Krabbe |first3=Erik C. W. |last4=Snoeck Henkemans |first4=A. Francisca |last5=Verheij |first5=Bart |last6=Wagemans |first6=Jean H. M. |date=2014 |title=Handbook of argumentation theory |location=New York |publisher=Springer |isbn=9789048194728 |oclc=871004444 |doi=10.1007/978-90-481-9473-5 |ref=harv}}
* {{cite book|last1=Facione |first1=Peter A. |last2=Facione |first2=Noreen C. |title=Thinking and reasoning in human decision making: the method of argument and heuristic analysis |year=2007 |location=Milbrae, CA |publisher=California Academic Press |isbn=1891557580 |oclc=182039452 |ref=harv}}
* {{cite web |last=van Gelder |first=Tim |authorlink=Tim van Gelder |url=http://timvangelder.com/2009/02/17/what-is-argument-mapping/ |title=What is argument mapping? |publisher=timvangelder.com |date=17 February 2009 |accessdate=12 January 2015 |ref=harv}}
* {{cite book |last=van Gelder |first=Tim |authorlink=Tim van Gelder |date=2015 |chapter=Using argument mapping to improve critical thinking skills |editor1-last=Davies |editor1-first=Martin |editor2-last=Barnett |editor2-first=Ronald |title=The Palgrave handbook of critical thinking in higher education |location=New York |publisher=[[Palgrave Macmillan]] |pages=183–192 |isbn=9781137378033 |oclc=894935460 |doi=10.1057/9781137378057_12 |ref=harv}}
* {{cite journal |last=Harrell |first=Maralee |title=Using argument diagramming software in the classroom |journal=[[Teaching Philosophy]] |volume=28 |issue=2 |pages=163–177 |date=June 2005 |doi=10.5840/teachphil200528222 |url=http://www.hss.cmu.edu/philosophy/harrell/ArgumentDiagramsInClassroom.pdf |ref=harv}}
* {{cite journal |last1=Schneider |first1=Jodi |last2=Groza |first2=Tudor |last3=Passant |first3=Alexandre |date=April 2013 |title=A review of argumentation for the social semantic web |journal=Semantic Web |volume=4 |issue=2 |pages=159–218 |url=http://semantic-web-journal.org/sites/default/files/swj138_0.pdf |ref=harv}}

== External links ==

===Argument mapping software===
*[http://araucaria.computing.dundee.ac.uk/ Araucaria] (open source, cross platform/Java)
*[http://sourceforge.net/projects/argumentative/ Argumentative] (open source, Windows); supports single-user, graphical argumentation
*[http://www.argunet.org/editor/ Argunet] (open source, cross platform)
*[http://compendiuminstitute.net/ Compendium] (open source, cross platform/Java)
*[http://www.phil.cmu.edu/projects/argument_mapping/ iLogos] (cross platform/Java)
*[http://ova.arg-tech.org/ OVA] (Web based, Online Visualisation of Argument)
*[http://www.cs.ie.niigata-u.ac.jp/Research/PIRIKA/PIRIKA.html PIRIKA (PIlot for the RIght Knowledge and Argument)] (open source, Linux, Windows)
*[http://www.vangeldermonk.com/reasoningapp/ The Reasoning PowerPoint App] ([[PowerPoint]] add-in, Windows 2007 or later)

===Online, collaborative software===
*[http://agora.gatech.edu/ AGORA-net] (user interface in English, German, Spanish, Chinese, and Russian)
*[http://arguman.org/ Arguman] (Open source, English, Turkish, and Chinese)
*[http://www.bcisiveonline.com/ bCisiveOnline]
*[http://carneades.github.io/ Carneades] (open source, argument (re)construction, evaluation, mapping and interchange)
*[https://code.google.com/p/collam/ Collam] ([[JavaScript]] library for visualizing argument maps)
*[http://www.debategraph.org/ Debategraph]
*[http://www.truthmapping.com/ TruthMapping]

[[Category:Argument mapping]]
[[Category:Arguments]]
[[Category:Critical thinking]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:Logic]]
[[Category:Problem structuring methods]]
<=====doc_Id=====>:893
<=====title=====>:
Class (knowledge representation)
<=====text=====>:
{{Main|Ontology components#Classes|l1 = Classes on the ontology component page}}

In  [[Knowledge representation and reasoning|knowledge representation]], a '''class''' is a collection of individuals or objects.<ref name="DLs">{{cite proceedings|url=http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=110B072B7265573684AB8D4F0D6B2306?doi=10.1.1.177.2787&rep=rep1&type=pdf|title=Description Logics: Foundations for Class-based Knowledge Representation|author1=Diego Calvanese|author2=Giuseppe De Giacomo|author3=Maurizio Lenzerini|conference=[[Logic in Computer Science]]|year=2002}}</ref> A class can be defined either by [[Extensional definition|extension]], or by [[Intensional definition|intension]], using what is called in some ontology languages like [[Web Ontology Language|OWL]]. If we follow the [[Type–token distinction]], the ontology is divided into individuals, who are real worlds objects, or events, and types, or classes, who are sets of real world objects. Class expressions or definitions gives the properties that the individuals must fulfill to be members of the class. Individuals that fulfill the property are called [[Instance (computer science)|Instances]].

== Relationships ==

=== Instantiation ===

The instantiation [[relation (mathematics)|relationship]] is a relation between objects and classes. We say that an object O, say ''Harry the eagle'' is an instance of a class, say ''Eagle''. ''Harry the eagle'' has all the properties that we can attribute to an eagle, for example his parents were eagles, he's a bird, he's a meat eater and so on. It's a special kind of [[is a]] relationship. It's noted [[Concept assertion]] (<math> : </math>) in [[Description logic]]s, a family of logic based on classes, class assertion <ref name="owlclass">{{cite web|url=http://www.w3.org/TR/owl2-syntax/#Class_Assertions|title=owl2 syntax}}</ref>

=== Subsumption ===
Classes can [[is a|subsume]] each other. We say usually that if <code>A</code> and <code>B</code> are classes, and all <code>A</code> instances are also <code>B</code> instances, then B subsumes A, or A is a subclass of B, for example in the OWL Language it's called subclassof.<ref name="owlclass"/>

==References==
<references/>

== See also ==
* [[Metaclass (Semantic Web)]]
* [[Ontology (information science)|Ontology]]
* [[Ontology components]]
* [[Description logic]]

[[Category:Knowledge representation]]
[[Category:Semantic Web Ontology]]


{{Information-science-stub}}
<=====doc_Id=====>:896
<=====title=====>:
Information Coding Classification
<=====text=====>:
The '''Information Coding Classification''' ('''ICC''') is a [[classification system]] covering almost all extant 6500 knowledge fields ([[knowledge domain]]s). Its [[conceptualization]] goes beyond the scope of the well known  library classification systems, such as [[Dewey Decimal Classification]] (DCC), [[Universal Decimal Classification]] (UDC), and [[Library of Congress Classification]] (LCC), by extending also to [[knowledge system]]s that so far have not afforded to  classify  [[literature]].  ICC actually presents a flexible universal ordering system for both literature and other kinds of [[information]], set out as knowledge fields. From a methodological point of view, ICC differs from the above-mentioned systems along the following three lines:
# Its main classes are not based on [[discipline]]s but on nine live stages of development, so-called [[ontic]]al levels.
# It breaks them roughly down into hierarchical steps by further nine [[Categorization|categories]] which makes decimal number coding possible.
# The contents of a knowledge field is earmarked via a digital position scheme, which makes the first hierarchical step refer to the nine ontical levels  (object areas as subject categories), and the second hierarchical step refer to nine functionally ordered form categories.
Respective knowledge fields permit to step down by the same principle to a third and  forth level, and even further to a fifth and sixth level. Finally, knowledge field subdivisions will have to conform to said digital position scheme.
Hence, for a given knowledge field identical codes will mark identical categories under respective numbers of the coding system. This  [[mnemotechnics|mnemotechnical]]  aspect of the  system helps memorizing and straightaway retrieving the whereabouts of respective [[interdisciplinary]] and [[transdisciplinary]] fields.

The first two hierarchical levels may be regarded as a top- or [[upper ontology]] for ontologies and other applications.

The terms of the first three hierarchical levels were set out in German and English in ''Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft'',<ref name="WEAAZ2014">{{citation/core|Surname1=[[Ingetraut Dahlberg]]|EditorSurname1=Deutsche Sektion der Internationalen Gesellschaft für Wissensorganisation e.V. (ISKO)|Periodical=Textbooks for Knowledge Organization|Title=Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft |Volume=Vol.3|Publisher=Ergon Verlag|PublicationPlace=Würzburg|Year=2014|At=pp.&nbsp;1-175|ISBN=978-3-95650-065-7|Date=  2014|language=German|AccessDate=2015-07-16}}</ref> on pp.&nbsp;82 to 100. It was published in 2014 and available so far only in German. In the meantime, also the French terms of the knowlwdge fields have been collected.
Competence for maintenance and further development rests with the German Chapter of the  
[[International Society for Knowledge Organization]] (ISKO) e.V.

== Historical development ==
At the end of 1970, Prof. Alwin Diemer, Univ.of Düsseldorf proposed to [[Ingetraut Dahlberg]] to undertake a philosophical [[dissertation]] on ''The universal classification system of knowledge, its ontological, epistemological, and information theoretical foundations''. Diemer had in mind an innovating ontological approach for such a system based on the whole spectrum of kinds of being and complying with [[epistemological]] requirements. The third requirement had already been taken up somehow in the Indian [[Colon Classification]], yet it still called for explanations and additions. In 1974, the dissertation was published in German entitled ''Grundlagen universaler Wissensordnung''.<ref name="GuWo">{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Deutsche Gesellschaft für Dokumentation e.V.|Title=Grundlagen universaler Wissensordnung. Probleme und Möglichkeiten eines universalen Klassifikationssystems des Wissens. : im Antiquariat noch erhältlich sonst als Print on Demand bei deGruyter|Publisher=Verlag Dokumentation|PublicationPlace=Pullach bei München|Year=1974|ISBN=978-3111412672|Date=  1974|language=German|AccessDate=2015-05-13}}</ref> It started with conceptual clarifications, and why and how the term „universal“ was linked to knowledge, including knowledge fields, such as commodity science, artefacts,  statistics, patents, standardization, communication, utility services et al. In chapter 3, six universal classification systems (DDC, UDC, LCC, BC, CC and BBK) were presented,  analyzed and compared.

While preparing the dissertation, Dahlberg started with elaborating the new universal system by first gleaning a lot of extant designations of knowledge fields from whatever available reference works. This was funded by the German Documentation Society (DGD) (1971-2) under the title of ''Order system of knowledge fields''. In addition, the [[syllabus]]es of German universities and polytechniques were explored for relevant terms and documented (1975).  Thereafter, it seemed necessary to add definitions from special dictionaries and encyclopediae; it soon appeared that the 12.500 terms included numerous synonyms, so that the whole collection boiled down to about 6.500 concept designations (Project Logstruktur, supported by the German Science Foundation (DFG) 1976-78).

The outcome of this work <ref name="GuWo" /> was the formulation of 30 theses which ended up in 12 principles for the new system, published 40 years later under.<ref name="WEAAZ2014" /> These principles refer not only to theoretical foundations but also to structure and other organizational aspects of the whole array of knowledge fields. In 1974, the digital position scheme for field subdivision had already been developed  to allow for classifying classification literature in the bibliographical section of the first issue of the Journal International Classification. In 1977, the entire ICC  was ready for presentation at a seminar in Bangalore, India.<ref>{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Sarada Ranganathan Endowment for Library Science|Title=Ontical Structures and Universal Classification |PublicationPlace=Bangalore|Year=1978|Date=  1978|language=German|AccessDate=2014-10-06}}</ref> A publication of the first three hierarchical levels appeared however only in 1982.<ref name="ICC-PSAP">{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=International Classification|Title=ICC – Information Coding Classification. Principles, structure and application possibilities |Volume=2|Year=1982|At=pp.&nbsp;98-103|Date=  1982|language=German}}</ref> It was applied to the bibliography of classification systems and [[thesauri]] in vol.1 of the International Classification and Indexing Bibliography;<ref>{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=International Classification and Indexing Bibliography (ICIB 1): Classification systems and thesauri 1950-1982 |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1982|ISSN=0943-7444|Date=  1982|language=German|AccessDate=2015-05-13}}</ref> it has been updated.<ref name="WEAAZ2014" />

== Governing principles of ICC ==
These were published in full length in the book ''Wissensorganisation. Entwicklung, Aufgabe, Anwendung, Zukunft''<ref name="WEAAZ2014" /> and the  article ''Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches'',<ref name="ICC2010">{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Marlies Ockenfeld|Periodical=Information, Wissenschaft & Praxis|Title=Information Coding Classification. Geschichtliches, Prinzipien, Inhaltliches |Volume=61, Heft 8|Publisher=De Gruyter|Year=2010|At=pp.&nbsp;449-454|ISSN=1619-4292|Date=  2010|language=German|AccessDate=2015-07-16}}</ref> hence it suffices to just mention their topics with some necessary additions.

* Principle 1: Concept theoretical approaches. Concepts are the contents of ICC, they are understood as being units of knowledge. The „birth“ of a concept. Where do the characteristics, the knowledge elements come from? How do conceptual relations arise?
* Principle 2: The four kinds of concept relations and their applications.
* Principle 3: Decimal numbers form the ICC codes as its universal language.
* Principle 4: The nine ontical levels of ICC. They were grouped under three captions: Prolegomena (1-3), life sciences (4-6) and human output (7-9):
:# Structure and form
:# Matter and energy
:# Cosmos and earth
:# Biosphere
:# Anthroposphere
:# Sociosphere
:# Material products (economics and technology)
:# Intellectual products (knowledge and information)
:# Spiritual products (products of mind and culture)

* Principle 5: Knowledge fields are structured by categories, based on the Aristotelian form-categories, under a digital position scheme, a kind of scaling rule for subdividing a given field as follows:
:# General area: problems, theories, principles (axiom and structure)
:# Object area: objects, kinds, parts, properties of objects
:# Activity area: methods, processes, activities
:# Field properties or first characterization   
:# Persons or secondary characterization
:# Societies or tertiary characterization
:# Influences from outside
:# Applications of the field to other fields
:# Field information and synthesizing tasks
:The digital position scheme, called Systematifier, has  also been used for structuring the entire system via the categories figuring on the upper zero level.

An example of its application is the structure of the classification system for knowledge organization literature [http://www.isko.org/scheme.php Gliederung der Klassifikationsliteratur]. (A simplified version with an additional introduction is given in,<ref name="WEAAZ2014" /> p.&nbsp;71)

* Principle 6: The ontical levels outlined under principle 4 conform to the „integrative level theory“ which means that every level is integrated in the following one. In addition, each knowledge area presumes the following one.
* Principle 7: The combination potential of knowledge fields (interdisciplinarity and transdisciplinarity)is determined by the digital position scheme. (Examples are given in,<ref name="WEAAZ2014" /> p.&nbsp;103-4)
* Principle 8: The categories of the zero-level are general concepts, their possible subdivisions could once be used for classificatory statements. (These subdivisions still need elaboration)
* Principle 9 and 10: These relate to the combination potential of classificatory statements with space and time concepts. (Still to be elaborated)
* Principle 11: The system's mnemotechnical aspect relies on the fixed system position codes and on the 3x3 form- and subject-categories.
* Principle 12: The combination potential of system position 1, 8 and 9 make ICC to a self-networking system which complies with the present scientific development.

== ICC in Matrix Form ==

The first two levels of ICC can be represented by following matrix.

[[File:ICC as a Matrix.png|maxi|840px|ICC as a Matrix with the first two hierarchical levels]]

The first hierarchical level of the 9 subject categories results from the first vertical array under codes 1-9.  The second hierarchical level of subject categories is structured by the 9 functionally ordered form categories, listed in the first horizontal line under codes 01-09. Some exceptions are mentioned in principle 7.

== Research work with ICC ==

=== Exploration of automatic classification with ICC ===
For classifying web documents as conceived by Jens Hartmann, University of Karlsruhe, Prof.Walter Koch, University of Graz, has explored in his Institute for Applied Information Technology Research Society (AIT) the application of ICC to automatically classifying metadata of some 350.000 documents. This was facilitated by data generated within the framework of an EU-supported project [http://www.europeana-local.at "EuropeanaLocal"] . For this exploration, three ICC hierarchical levels have been used for some 5000 terms. The result is described in the report of Christoph Mak.<ref>{{citation/core|Surname1=Christian Mak|Periodical=Bericht des Instituts "Angewandte Informationstechnik Forschungsgesellschaft mbh" (AIT)|Title=Kategorisierung des Datenbestandes der EuropeanaLocal-Österreich anhand der ICC |PublicationPlace=Graz|Year=2011|Date=  2011|language=German}}</ref> Prof.Koch regarded a classification degree of almost 50% as a good result, considering that only a shortened version of ICC had been used. In order to reach a better result one would have needed 1–2 years. Also an index of all terms with their codes could be achieved under these explorations.

=== Data Linkage with ICC ===
Motivated by the work of an Italian research Group in Trento on ''Revising the Wordnet Domains Hierarchy: semantics, coverage and balancing'',<ref name="FGinTrnto">{{citation/core|Surname1=Luisa Bentivogli|Surname2=Pamela Forner|Surname3=Bernardo Magnini|Surname4=Emanuele Pianta|Periodical=Proceedings of COLING 2004 Workshop on "Multilingual Linguistic Resources|Title=[http://multiwordnet.fbk.eu/paper/coling04-ws-WDH.pdf Revising WordNet Domains Hierarchy: Semantics, Coverage, and Balancing] |PublicationPlace=Geneva, Switzerland|Year=2004|At=pp.&nbsp;101-108|Date=  2004|language=German}}</ref> by which the DDC codes were used, Prof. Ernesto DeLuca et al. showed in a study that for such case the use of ICC could lead to essentially better results. This was shown in two contributions: ''Including knowledge domains from the ICC into the Multilingual Lexical Linked Data Cloud (LLD)'' <ref name=" IKDfICC">{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Knowledge Organization in the 21st Century. Between Historical Patterns and Future Prospects. Proc.13th Int. ISKO Conf.|Title=Including Knowledge Domains from the ICC into the Multilingual Lexical Linked Data Cloud |PublicationPlace=Krakau, Polen|Year=2014|At=pp.&nbsp;258-365|Date=  2014|language=German}}</ref> and ''Die Multilingual Lexical Linked Data Cloud: Eine mögliche Zugangsoptimierung?'',<ref name="MLLDC">{{citation/core|Surname1=Ernesto William DeLuca et al|Periodical=Information, Wissenschaft & Praxis|Title=Die Multilingual Lexical Linked Data Cloud: Eine mögliche Zugangsoptimierung? |Volume=65, Heft 4-5|Publisher=De Gruyter|Year=2014|At=pp.&nbsp;279-287|ISSN=1619-4292|Date=  2014|language=German}}</ref> in which the LLD was used in a meta-model which contains all ressources with the possibility of retrieval and navigation of data from different aspects. By this, the existing work about many thousand knowledge fields (of  ICC) can be combined with the Multilingual Lexical Linked Data Cloud, based on RDF/OWL representation of EuroWordNet and similar integrated lexical ressources (MultiWordNet, MEMODATA and the Hamburg Metapher BD).

=== Semantic Web structuring with ICC ===
In October 2013, the computer scientist Hermann Bense, Dortmund, explored the possibilities for structuring the Semanic Web with ICC codes. He developed two approaches for a pictorial presentation of knowledge fields with their possible subdivisions. A graphic representation of those knowledge fields pertaining to the first two levels can be found under [http://www.ontology4.us/english/Ontologies/Science%2520Ontology/index.html Ontology4]. The inclusion of the third hierarchical level has been envisaged as the next step.

== Some potential applications of ICC in its present form ==
# Possibility to roughly structure documents, especially bibliographies and reference works.
# Structuring personal repertories, e.g. a [[Who’s Who]] in ''Who’s Who in Classification and Indexing''<ref name="whoiswho">{{citation/core|EditorSurname1=Ingetraut Dahlberg|Title=Who's Who in Classification and Indexing |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1983|Date=  1983|language=German}}</ref>
# Supporting the recollection of statistics by knowledge fields, e.g. also concerning university professors, statistics of academies, of institutions, of teachers in special education
# Publishing houses could take up ICC codes for their products to help later sorting by knowledge fields.
# As a standard classification ICC may be used in many cases, especially in industry,  [[knowledge management]] and [[knowledge engineering]].
# With the definition of all its terms a lexicon of knowledge fields could be published. This could also be used for such lexica in other languages.<ref name="lexikon">{{citation/core|Surname1=Ingetraut Dahlberg|Periodical=Knowledge Organisation 39, No.2|Title=A systematic new lexicon of all knowledge fields based on the Information Coding Classification. |Year=2012|At=pp.&nbsp;142-150|Date=  2012|language=German}}</ref>
# As an example, ICC could be used to compare ongoing scientific activities on a European or world-wide scale.
# ICC can also be an appropriate tool for switching between extant universal classification systems.<ref name="switch">{{citation/core|Surname1=Ingetraut Dahlberg|EditorSurname1=Green, R.|Periodical=Knowledge organization & change. Proc.4th Int.ISKO Conf., Washington,D.C.|Title=Library catalogs and the Internet. Switching for future subject access. |Publisher=INDEKS Verlag|PublicationPlace=Frankfurt|Year=1996|At=pp.&nbsp;155-165|Date=  1996|language=German}}</ref>
# ICC can also be a suitable „hang-up system“ for special classification systems, e.g. for special terminological concept systems.
# ICC in its three hierarchies and corresponding explanations might also be used in higher education to supply the youngsters with an overview of knowledge fields and an understanding of the relationships in the whole of human knowledge.
# Similar to the [[Unified Medical Language System]] (UMLS) for medicine such a Unified System of Knowledge Fields could be held available in many languages and thus reach a global understanding of knowledge fields.
# The alphabetical index to all knowledge field concepts could be used for comparisons with other such indexes to help in finding the missing fields in the different universal classification systems.   
 
== References ==
{{Reflist|30em}}

{{Authority control}}

[[Category:Classification systems]]
[[Category:Knowledge representation]]
[[Category:Library cataloging and classification]]
[[Category:Ontology]]
[[Category:Data coding framework]]
<=====doc_Id=====>:899
<=====title=====>:
Unified Modeling Language
<=====text=====>:
{{Use American English|date=January 2012}}
[[File:UML logo.gif|thumb|UML logo]]
The '''Unified Modeling Language''' ('''UML''') is a general-purpose, developmental,  [[modeling language]] in the field of [[software engineering]], that is intended to provide a standard way to visualize the design of a system.<ref name=":1" />

UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design developed by [[Grady Booch]], [[Ivar Jacobson]] and [[James Rumbaugh]] at [[Rational Software]] in 1994–1995, with further development led by them through 1996.<ref name=":1" />

In 1997 UML was adopted as a standard by the [[Object Management Group]] (OMG), and has been managed by this organization ever since. In 2005 UML was also published by the [[International Organization for Standardization]] (ISO) as an approved ISO standard.<ref>{{cite web|url=http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=32620 |title=ISO/IEC 19501:2005 - Information technology - Open Distributed Processing - Unified Modeling Language (UML) Version 1.4.2 |publisher=Iso.org |date=2005-04-01 |accessdate=2015-05-07}}</ref> Since then it has been periodically revised to cover the latest revision of UML.<ref>{{cite web|url=http://www.iso.org/iso/iso_catalogue/catalogue_tc/catalogue_detail.htm?csnumber=32624 |title=ISO/IEC 19505-1:2012 - Information technology - Object Management Group Unified Modeling Language (OMG UML) - Part 1: Infrastructure |publisher=Iso.org |date=2012-04-20 |accessdate=2014-04-10}}</ref>

== History ==

[[File:OO Modeling languages history.jpg|thumb|320px|History of object-oriented methods and notation]]

=== Before UML 1.x ===

UML has been evolving since the second half of the 1990s and has its roots in the object-oriented methods developed in the late 1980s and early 1990s. The timeline (see image) shows the highlights of the history of object-oriented modeling methods and notation.

It is originally based on the notations of the [[Booch method]], the [[object-modeling technique]] (OMT) and [[object-oriented software engineering]] (OOSE), which it has integrated into a single language.<ref name=":0" />

[[Rational Software Corporation]] hired [[James Rumbaugh]] from [[General Electric]] in 1994 and after that the company became the source for two of the most popular object-oriented modeling approaches of the day:<ref>Andreas Zendler (1997) ''Advanced Concepts, Life Cycle Models and Tools for Objeckt-Oriented Software Development''. p.122</ref> Rumbaugh's [[object-modeling technique]] (OMT) and [[Grady Booch]]'s method. They were soon assisted in their efforts by [[Ivar Jacobson]], the creator of the [[object-oriented software engineering]] (OOSE) method, who joined them at Rational in 1995.<ref name=":1">{{cite book
 | title = Unified Modeling Language User Guide, The
 | publisher = Addison-Wesley
 | edition = 2
 | year = 2005
 | page = 496
 | url = http://www.informit.com/store/unified-modeling-language-user-guide-9780321267979
 | isbn = 0321267974
}}
, See the sample content, look for history</ref>

=== UML 1.x ===

Under the technical leadership of those three (Rumbaugh, Jacobson and Booch), a consortium called the [[UML Partners]] was organized in 1996 to complete the ''Unified Modeling Language (UML)'' specification, and propose it to the Object Management Group (OMG) for standardisation. The partnership also contained additional interested parties (for example [[Hewlett-Packard|HP]], [[Digital Equipment Corporation|DEC]], [[IBM]] and [[Microsoft]]). The UML Partners' UML 1.0 draft was proposed to the OMG in January 1997 by the consortium. During the same month the UML Partners formed a group, designed to define the exact meaning of language constructs, chaired by [[Cris Kobryn]] and administered by Ed Eykholt, to finalize the specification and integrate it with other standardization efforts. The result of this work, UML 1.1, was submitted to the OMG in August 1997 and adopted by the OMG in November 1997.<ref name=":1" /><ref>{{cite web|url=http://www.omg.org/cgi-bin/doc?ad/97-08-11 |title=UML Specification version 1.1 (OMG document ad/97-08-11) |publisher=Omg.org |accessdate=2011-09-22}}</ref>

After the first release a task force was formed<ref name=":1" /> to improve the language, which released several minor revisions, 1.3, 1.4, and 1.5.<ref>{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2014-04-10}}</ref>

The standards it produced (as well as the original standard) have been noted as being ambiguous and inconsistent.<ref>Génova et alia 2004 "Open Issues in Industrial Use Case Modeling"</ref><ref>{{cite web|url=http://www.uml-forum.com/docs/papers/CACM_Jan02_p107_Kobryn.pdf |title=Will UML 2.0 Be Agile or Awkward? |format=PDF |accessdate=2011-09-22}}</ref>

=== UML 2.x ===

UML 2.0 major revision replaced version 1.5 in 2005, which was developed with an enlarged consortium to improve the language further to reflect new experience on usage of its features.<ref>{{cite web|url=http://www.omg.org/spec/UML/2.0/ |title=UML 2.0 |publisher=Omg.org |accessdate=2011-09-22}}</ref>

Although UML 2.1 was never released as a formal specification, versions 2.1.1 and 2.1.2 appeared in 2007, followed by UML 2.2 in February 2009. UML 2.3 was formally released in May 2010.<ref name="spec">{{cite web|url=http://www.omg.org/spec/UML/ |title=UML |publisher=Omg.org |accessdate=2011-09-22}}</ref> UML 2.4.1 was formally released in August 2011.<ref name="spec"/> UML 2.5 was released in October 2012 as an "In process" version and was officially released in June 2015.<ref name="spec"/>

There are four parts to the UML 2.x specification:

# The Superstructure that defines the notation and semantics for diagrams and their model elements
# The Infrastructure that defines the core metamodel on which the Superstructure is based
# The [[Object Constraint Language]] (OCL) for defining rules for model elements
# The UML Diagram Interchange that defines how UML 2 diagram layouts are exchanged

The current versions of these standards follow: UML Superstructure version 2.4.1, UML Infrastructure version 2.4.1, OCL version 2.3.1, and UML Diagram Interchange version 1.0.<ref name="Versions">{{cite web|author=OMG|title=OMG Formal Specifications (Modeling and Metadata paragraph)|url=http://www.omg.org/spec/#M&M|accessdate = 2016-02-12}}</ref> It continues to be updated and improved by the revision task force, who resolve any issues with the language.<ref>{{cite web|url=http://www.omg.org/issues/uml2-rtf.open.html |title=Issues for UML 2.6 Revision task Force mailing list |publisher=Omg.org |accessdate=2014-04-10}}</ref>

== Design ==

UML offers a way to visualize a system's architectural blueprints in a diagram (see image), including elements such as:<ref name=":0">{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Superstructure/PDF |title=OMG Unified Modeling Language (OMG UML), Superstructure. Version 2.4.1 |publisher=Object Management Group |accessdate=9 April 2014}}</ref>

* any [[Activity (UML)|activities]] (jobs);
* individual [[Component (UML)|components]] of the system;
** and how they can interact with other [[Component-based software engineering|software components]];
* how the system will run;
* how entities interact with others (components and interfaces);
* external [[user interface]].

Although originally intended for object-oriented design documentation, UML has been extended to a larger set of design documentation (as listed above),<ref>Satish Mishra (1997). [http://www2.informatik.hu-berlin.de/~hs/Lehre/2004-WS_SWQS/20050107_Ex_UML.ppt "Visual Modeling & Unified Modeling Language (UML): Introduction to UML"]. Rational Software Corporation. Accessed 9 November 2008.</ref> and been found useful in many contexts.<ref name="UML, Success Stories">{{cite web|url=http://www.uml.org/uml_success_stories/index.htm|title=UML, Success Stories|accessdate=9 April 2014}}</ref>

=== Software development methods ===

UML is not a development method by itself;<ref>John Hunt (2000). ''The Unified Process for Practitioners: Object-oriented Design, UML and Java''. Springer, 2000. ISBN 1-85233-275-1. p.5.door</ref> however, it was designed to be compatible with the leading object-oriented software development methods of its time, for example [[Object-modeling technique|OMT]], [[Booch method]], [[Objectory]] and especially [[Rational Unified Process|RUP]] that it was originally intended to be used with when work began at Rational Software.

=== Modeling ===

It is important to distinguish between the UML model and the set of diagrams of a system. A diagram is a partial graphic representation of a system's model. The set of diagrams need not completely cover the model and deleting a diagram does not change the model. The model may also contain documentation that drives the model elements and diagrams (such as written use cases).

UML diagrams represent two different views of a system model:<ref>Jon Holt Institution of Electrical Engineers (2004). ''UML for Systems Engineering: Watching the Wheels'' IET, 2004, ISBN 0-86341-354-4. p.58</ref>

* Static (or ''structural'') view: emphasizes the static structure of the system using objects, attributes, operations and relationships. It includes [[class diagram]]s and [[composite structure diagram]]s.
* Dynamic (or ''behavioral'') view: emphasizes the dynamic behavior of the system by showing collaborations among objects and changes to the internal states of objects. This view includes [[sequence diagram]]s, [[activity diagram]]s and [[UML state machine|state machine diagrams]].

UML models can be exchanged among UML tools by using the [[XML Metadata Interchange]] (XMI) format.

== Diagrams ==
{{UML diagram types}}

UML 2 has many types of diagrams, which are divided into two categories.<ref name=":0" /> Some types represent ''structural'' information, and the rest represent general types of ''behavior'', including a few that represent different aspects of ''interactions''. These diagrams can be categorized hierarchically as shown in the following class diagram:<ref name=":0" />

[[File:UML diagrams overview.svg|center|600px|Hierarchy of UML 2.2 Diagrams, shown as a [[class diagram]]]]

These diagrams may all contain comments or notes explaining usage, constraint, or intent.

=== Structure diagrams ===

[[Structure diagram]]s emphasize the things that must be present in the system being modeled. Since structure diagrams represent the structure, they are used extensively in documenting the [[software architecture]] of software systems. For example, the [[component diagram]] describes how a software system is split up into components and shows the dependencies among these components.

<gallery class="center">
Policy Admin Component Diagram.PNG|[[Component diagram]]
BankAccount1.svg|[[Class diagram]]
</gallery>

=== Behavior diagrams ===

Behavior diagrams emphasize what must happen in the system being modeled. Since behavior diagrams illustrate the behavior of a system, they are used extensively to describe the functionality of software systems. As an example, the [[activity diagram]] describes the business and operational step-by-step activities of the components in a system.

<gallery class="center">
Activity conducting.svg|[[Activity diagram]]
UML Use Case diagram.svg|[[Use case diagram]]
</gallery>

==== Interaction diagrams ====

Interaction diagrams, a subset of behavior diagrams, emphasize the flow of control and data among the things in the system being modeled. For example, the [[sequence diagram]] shows how objects communicate with each other in terms of a sequence of messages.

<gallery class="center">
CheckEmail.svg|[[Sequence diagram]]
UML Communication diagram.svg|[[Communication diagram]]
</gallery>

== Meta modeling ==
{{Main article|Meta-Object Facility}}

[[File:M0-m3.png|thumb|320px|Illustration of the Meta-Object Facility]]

The Object Management Group (OMG) has developed a [[metamodeling]] architecture to define the UML, called the [[Meta-Object Facility]].<ref>Iman Poernomo (2006) "[http://calcium.dcs.kcl.ac.uk/1259/1/acm-paper.pdf The Meta-Object Facility Typed]" in: ''Proceeding SAC '06 Proceedings of the 2006 ACM symposium on Applied computing''. pp. 1845-1849</ref> MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.

The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.<ref>{{cite web|url=http://www.omg.org/spec/UML/2.4.1/Infrastructure/PDF/ |title=UML 2.4.1 Infrastructure |publisher=Omg.org |date=2011-08-05 |accessdate=2014-04-10}}</ref>

The meta-model can be extended using a mechanism called [[stereotype (UML)|stereotyping]]. This has been criticised as being insufficient/untenable by [[Brian Henderson-Sellers]] and Cesar Gonzalez-Perez in "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0".<ref name="UsesAbusesStereotype">B. Henderson-Sellers; C. Gonzalez-Perez (2006). "Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0". in: ''Model Driven Engineering Languages and Systems''. Springer Berlin / Heidelberg.</ref>

== Adoption ==

UML has been found useful in many design contexts.<ref name="UML, Success Stories"/><ref>{{Cite web|url = http://www.drdobbs.com/architecture-and-design/uml-25-do-you-even-care/240163702?queryText=uml|title = UML 2.5: Do you even care?}} "UML truly is ubiquitous"</ref>

It has been treated, at times, as a design [[no silver bullet|silver bullet]], which has led to problems in its usage. Misuse of it includes excessive usage of it (design every little part of the system's [[Programming code|code]] with it, which is unnecessary) and assuming that anyone can design anything with it (even those who haven't [[Programmer|programmed]]).<ref>{{Cite web|url = http://queue.acm.org/detail.cfm?id=984495|title = Death by UML Fever}}</ref>

It is seen to be a large language, with many [[Syntax (programming languages)|constructs]] in it. Some (including [[Ivar Jacobson|Jacobson]]) feel that there are too many and that this hinders the learning (and therefore usage) of it.<ref>{{Cite web|url = http://www.infoq.com/interviews/Ivar_Jacobson|title = Ivar Jacobson on UML, MDA, and the future of methodologies}}</ref>

== Criticisms ==
{{Criticism section|date=December 2010}}

Common criticisms of UML from industry include:<ref name="petre">{{Cite conference| quote=The majority of those interviewed simply do not use UML, and those who do use it tend to do so selectively and often informally|conference=35th International Conference on Software Engineering 18–26 May 2013 |url=http://oro.open.ac.uk/35805/8/UML%20in%20practice%208.pdf|title=UML in practice|first=Marian|last=Petre| date=2013|pages=722–731}}</ref>

* not useful: "[does] not offer them advantages over their current, evolved practices and representations"
* too complex, particularly for communication with clients: "unnecessarily complex" and "The best reason not to use UML is that it is not ‘readable’ for all stakeholders. How much is UML worth if a business user (the customer) can not understand the result of your modelling effort?"
* need to keep UML and code in sync, as with documentation generally

=== Critique of UML 1.x ===

; Cardinality notation: As with database Chen, Bachman, and ISO [[ER diagram]]s, class models are specified to use "look-across" [[Cardinality (data modeling)|cardinalities]], even though several authors ([[Merise]],<ref>Hubert Tardieu, Arnold Rochfeld and René Colletti La methode MERISE: Principes et outils (Paperback - 1983)</ref> Elmasri & Navathe<ref>Elmasri, Ramez, B. Shamkant, Navathe, Fundamentals of Database Systems, third ed., Addison-Wesley, Menlo Park, CA, USA, 2000.</ref> amongst others<ref>[https://books.google.com/books?id=odZK99osY1EC&pg=PA52&img=1&pgis=1&dq=genova&sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&edge=0 ER 2004 : 23rd International Conference on Conceptual Modeling, Shanghai, China, 8-12 November 2004] {{webarchive |url=https://web.archive.org/web/20130527133330/https://books.google.com/books?id=odZK99osY1EC&pg=PA52&img=1&pgis=1&dq=genova&sig=ACfU3U3tDC_q8WOMqUJW4EZCa5YQywoYLw&edge=0 |date=27 May 2013 }}</ref>) prefer same-side or "look-here" for roles and both minimum and maximum cardinalities. Recent researchers (Feinerer,<ref>{{cite web|url=http://publik.tuwien.ac.at/files/pub-inf_4582.pdf |title=A Formal Treatment of UML Class Diagrams as an Efficient Method for Configuration Management 2007 |format=PDF |accessdate=2011-09-22}}</ref> Dullea et. alia<ref>{{cite web|url=http://www.ischool.drexel.edu/faculty/song/publications/p_DKE_03_Validity.pdf |title=James Dullea, Il-Yeol Song, Ioanna Lamprou - An analysis of structural validity in entity-relationship modeling 2002 |format=PDF |accessdate=2011-09-22}}</ref>) have shown that the "look-across" technique used by UML and ER diagrams is less effective and less coherent when applied to ''n''-ary relationships of order strictly greater than 2.

: Feinerer says: "Problems arise if we operate under the look-across semantics as used for UML associations. Hartmann<ref>{{cite web|url=http://crpit.com/confpapers/CRPITV17Hartmann.pdf |title="Reasoning about participation constraints and Chen's constraints" S Hartmann - 2003 |format=PDF |accessdate=2013-08-17}}</ref> investigates this situation and shows how and why different transformations fail.", and: "As we will see on the next few pages, the look-across interpretation introduces several difficulties which prevent the extension of simple mechanisms from binary to ''n''-ary associations."

== See also ==
{{Portal|Software}}

* [[Object Oriented Role Analysis and Modeling]]
* [[Model-based testing]]
* [[Model-driven engineering]]
* [[Applications of UML]]
* [[List of Unified Modeling Language tools]]

== References ==
{{FOLDOC}}

{{reflist|colwidth=30em}}

== Further reading ==

* {{cite book

 | first= Scott William
 | last = Ambler
 | year = 2004
 | url = http://www.ambysoft.com/books/theObjectPrimer.html
 | title = The Object Primer: Agile Model Driven Development with UML 2
 | publisher = Cambridge University Press
 | isbn=0-521-54018-6
}}

* {{cite book

 | first= Michael Jesse
 | last = Chonoles
 | author2=James A. Schardt
 | year = 2003
 | title = UML 2 for Dummies
 | publisher = Wiley Publishing
 | isbn=0-7645-2614-6
}}

* {{cite book

 | first = Martin
 | last = Fowler
 | authorlink = Martin Fowler
 | title = UML Distilled: A Brief Guide to the Standard Object Modeling Language
 | edition = 3rd
 | publisher = Addison-Wesley
 | isbn = 0-321-19368-7
}}

* {{cite book

 | first= Ivar
 | last = Jacobson |author2=Grady Booch |author3=James Rumbaugh
 | authorlink = Ivar Jacobson
 | year = 1998
 | title = The Unified Software Development Process
 | publisher = Addison Wesley Longman
 | isbn=0-201-57169-2
}}

* {{cite book

 | first = Robert Cecil
 | last = Martin
 | authorlink = Robert Cecil Martin
 | year = 2003
 | title = UML for Java Programmers
 | publisher = Prentice Hall
 | isbn = 0-13-142848-9
}}

* {{cite web

 | author = Noran, Ovidiu S.
 | url = http://www.cit.gu.edu.au/~noran/Docs/UMLvsIDEF.pdf
 | title = Business Modelling: UML vs. IDEF
 | format = PDF
 | accessdate = 2005-12-28
}}

* {{cite web

 | author = Horst Kargl
 | url = http://umlnotation.sparxsystems.eu/
 | title = Interactive UML Metamodel with additional Examples
 }}

* {{cite book

 | first = Magnus
 | last = Penker
 | author2=Hans-Erik Eriksson
 | author-link2= Hans-Erik Eriksson
 | year = 2000
 | title = Business Modeling with UML
 | publisher = John Wiley & Sons
 | isbn = 0-471-29551-5
}}

== External links ==
{{Commons}}

{{Wikiversity|UML}}

* {{Official website}}

{{UML}}

{{Software engineering}}

{{ISO standards}}

{{Use dmy dates|date=July 2011}}

{{Authority control}}

[[Category:Architecture description language]]
[[Category:Data modeling languages]]
[[Category:Data modeling diagrams]]
[[Category:Diagrams]]
[[Category:Knowledge representation]]
[[Category:ISO standards]]
[[Category:Specification languages]]
[[Category:Unified Modeling Language| ]]
[[Category:Software modeling language]]
<=====doc_Id=====>:902
<=====title=====>:
Mind map
<=====text=====>:
{{about|the visual diagram|the geographical concept|Mental mapping}}
[[File:Tennis-mindmap.png|thumb|upright=1.8|A mind map about the sport of [[tennis]]]]

A '''mind map''' is a  [[diagram]] used to visually organize information. A mind map is hierarchical and shows relationships among pieces of the whole.<ref>Carolyn H. Hopper, Practicing College Learning Strategies, 7th Edition, ISBN 9781305109599, Ch. 7</ref> It is often created around a single concept, drawn as an image in the center of a blank page, to which associated representations of ideas such as images, words and parts of words are added. Major ideas are connected directly to the central concept, and other ideas branch out from those.

Mind maps can be drawn by hand, either as "rough notes" during a lecture, meeting or planning session, for example, or as higher quality pictures when more time is available. Mind maps are considered to be a type of [[spider diagram]].<ref>{{cite web|url=http://dictionary.cambridge.org/dictionary/british/mind-map?q=mind+map |title=Mind Map noun - definition in the British English Dictionary & Thesaurus - Cambridge Dictionaries Online |publisher=Dictionary.cambridge.org |accessdate=2013-07-10}}</ref> A similar concept in the 1970s was "idea [[sunburst chart|sun bursting]]".<ref>{{cite web|url=http://www.mind-mapping.org/mindmapping-learning-study-memory/who-invented-mind-mapping.html |title=Who invented mind mapping |publisher=Mind-mapping.org |accessdate=2013-07-10}}</ref>

== Origins ==

Although the term "mind map" was first popularized by British [[popular psychology]] author and television personality [[Tony Buzan]], the use of diagrams that visually "map" information using branching and [[Radial tree|radial maps]] traces back centuries. These pictorial methods record knowledge and model systems, and have a long history in learning, [[brainstorming]], [[memory]], [[visual thinking]], and [[problem solving]] by educators, engineers, psychologists, and others. Some of the earliest examples of such graphical records were developed by [[Porphyry of Tyros]], a noted thinker of the 3rd century, as he graphically visualized the concept [[Categories (Aristotle)|categories of Aristotle]]. Philosopher [[Ramon Llull]] (1235–1315) also used such techniques.

The [[semantic network]] was developed in the late 1950s as a theory to understand human learning and developed further by [[Allan M. Collins]] and [[M. Ross Quillian]] during the early 1960s. Mind maps are similar in radial structure to [[concept map]]s, developed by learning experts in the 1970s, but differ in that the former are simplified by focusing around a single central key concept.

== Popularisation of the term "mind map" ==

Buzan's specific approach, and the introduction of the term "mind map" arose during a 1974 BBC TV series he hosted, called ''Use Your Head''.<ref>{{cite web|url=http://www.mind-mapping.org/blog/mapping-history/roots-of-visual-mapping/ |title=Roots of visual mapping - The mind-mapping.org Blog |publisher=Mind-mapping.org |date=2004-05-23 |accessdate=2013-07-10}}</ref><ref>Buzan, Tony 1974. Use your head. London: BBC Books.</ref> In this show, and companion book series, Buzan promoted his conception of radial tree, diagramming key words in a colorful, radiant, tree-like structure.<ref>[http://www.knowledgeboard.com/item/2980 Buzan claims mind mapping his invention in interview.] ''KnowledgeBoard'' retrieved Jan. 2010.</ref>

Buzan says the idea was inspired by [[Alfred Korzybski]]'s [[general semantics]] as popularized in science fiction novels, such as those of [[Robert A. Heinlein]] and [[A. E. van Vogt]]. He argues that while "traditional" outlines force readers to scan left to right and top to bottom, readers actually tend to scan the entire page in a non-linear fashion. Buzan's treatment also uses then-popular assumptions about the functions of [[cerebral hemispheres]] in order to explain the claimed increased effectiveness of mind mapping over other forms of note making.

==Mind map guidelines==
Buzan suggests the following guidelines for creating mind maps:

# Start in the center with an image of the topic, using at least 3 colors.
# Use images, symbols, codes, and dimensions throughout your mind map.
# Select key words and print using upper or lower case letters.
# Each word/image is best alone and sitting on its own line.
# The lines should be connected, starting from the central image. The lines become thinner as they radiate out from the center.
# Make the lines the same length as the word/image they support.
# Use multiple colors throughout the mind map, for visual stimulation and also for encoding or grouping.
# Develop your own personal style of mind mapping.
# Use emphasis and show associations in your mind map.
# Keep the mind map clear by using radial hierarchy or outlines to embrace your branches.

== Uses ==

[[File:Mindmap.gif|thumb|Rough mindmap notes taken during a course session]]
As with other diagramming tools, mind maps can be used to [[generation|generate]], [[creative visualization|visualize]], [[structure]], and [[taxonomic classification|classify]] ideas, and as an aid to [[study skills|studying]]<ref>'Mind maps as active learning tools', by Willis, CL. Journal of computing sciences in colleges. {{ISSN|1937-4771}}. 2006. Volume:
21 Issue: 4</ref> and [[organization|organizing]] information, [[problem solving|solving problems]], [[decision making|making decisions]], and writing.

Mind maps have many applications in personal, family, [[education]]al, and [[business]] situations, including [[notetaking]], brainstorming (wherein ideas are inserted into the map radially around the center node, without the implicit prioritization that comes from hierarchy or sequential arrangements, and wherein grouping and organizing is reserved for later stages), summarizing, as a [[mnemonic technique]], or to sort out a complicated idea. Mind maps are also promoted as a way to collaborate in color pen creativity sessions.

In addition to these direct use cases, data retrieved from mind maps can be used to enhance several other applications; for instance [[expert system|expert search systems]], [[search engine]]s and search and tag query recommender.<ref name=Beel2009>{{Cite journal| first=Jöran | last=Beel | first2=Bela| last2=Gipp | first3=Jan-Olaf |last3= Stiller | contribution=Information Retrieval On Mind Maps - What Could It Be Good For? | contribution-url=http://www.sciplore.org/publications_en.php | title=Proceedings of the 5th International Conference on Collaborative Computing: Networking, Applications and Worksharing (CollaborateCom'09) | year=2009 | publisher=IEEE | place=Washington | postscript=. -->}}</ref> To do so, mind maps can be analysed with classic methods of [[information retrieval]] to classify a mind map's author or documents that are linked from within the mind map.<ref name=Beel2009 />

==Differences from other visualizations==

* '''Concept maps''' - Mind maps differ from [[concept maps]] in that mind maps focus on ''only'' one word or idea, whereas concept maps connect multiple words or ideas. Also, concept maps typically have text labels on their connecting lines/arms. Mind maps are based on radial hierarchies and [[tree structure]]s denoting relationships with a central governing concept, whereas concept maps are based on connections between concepts in more diverse patterns.  However, either can be part of a larger [[personal knowledge base]] system.
* '''Modelling graphs''' - There is no rigorous right or wrong with mind maps, relying on the arbitrariness of [[mnemonic]] systems. A [[UML diagram]] or a [[semantic network]] has structured elements modelling relationships, with lines connecting objects to indicate relationship. This is generally done in black and white with a clear and agreed iconography. Mind maps serve a different purpose: they help with memory and organization. Mind maps are collections of words structured by the mental context of the author with visual mnemonics, and, through the use of colour, icons and visual links, are informal and necessary to the proper functioning of the mind map.

==Research==

'''Effectiveness''' - Cunningham (2005) conducted a user study in which 80% of the students thought "mindmapping helped them understand concepts and ideas in science".<ref name="Cunningham05">{{cite thesis| type=Ph.D.| author={G}lennis {E}dge {C}unningham| title=Mindmapping: Its Effects on Student Achievement in High School Biology| year=2005| publisher=The University of Texas at Austin| accessdate=1 November 2013}}</ref> Other studies also report positive effects through the use of mind maps.<ref name="Holland2004">{{cite journal| author={B}rian {H}olland, {L}ynda {H}olland, {J}enny {D}avies| title=An investigation into the concept of mind mapping and the use of mind mapping software to support and improve student academic performance| year=2004| accessdate=1 November 2013}}</ref><ref name="Antoni2006">{{cite journal| author=D'Antoni, A.V., Zipp, G.P.| title=Applications of the Mind Map Learning Technique in Chiropractic Education: A Pilot Study and Literature| year=2006| accessdate=1 November 2013}}</ref> Farrand, Hussain, and Hennessy (2002) found that [[spider diagram]]s (similar to concept maps) had limited, but significant, impact on memory recall in undergraduate students (a 10% increase over baseline for a 600-word text only) as compared to preferred study methods (a 6% increase over baseline).<ref name= Farrand2002>{{cite journal |author=Farrand, P. |author2=Hussain, F. |author3=Hennessy, E.  |year=2002 |title=The efficacy of the mind map study technique |journal=Medical Education |volume=36 |issue=5 |pages=426–431 |url=http://www3.interscience.wiley.com/journal/118952400/abstract |accessdate=2009-02-16 |doi=10.1046/j.1365-2923.2002.01205.x |pmid=12028392}}</ref> This improvement was only robust after a week for those in the diagram group and there was a significant decrease in motivation compared to the subjects' preferred methods of note taking. A meta study about [[concept map]]ping concluded that concept mapping is more effective than "reading text passages, attending lectures, and participating in class discussions".<ref name="Nesbit06">{{cite journal| author={N}esbit, {J}.{C}., {A}desope, {O}.{O}.| title=Learning with concept and knowledge maps: A meta-analysis| journal=Review of Educational Research| year=2006| volume=76| number=3| pages=413| publisher=Sage Publications| doi=10.3102/00346543076003413}}</ref> The same study also concluded that concept mapping is slightly more effective "than other constructive activities such as writing summaries and outlines". In addition, they concluded that low-ability students may benefit more from mind mapping than high-ability students.

'''Features of Mind Maps''' - Beel & Langer (2011) conducted a comprehensive analysis of the content of mind maps.<ref name="Beel2011d">{{cite book| author={J}oeran {B}eel, {S}tefan {L}anger| chapter=An Exploratory Analysis of Mind Maps| title=Proceedings of the 11th ACM Symposium on Document Engineering (DocEng'11)| year=2011| publisher=ACM| url=http://docear.org/papers/An%20Exploratory%20Analysis%20of%20Mind%20Maps%20--%20preprint.pdf | accessdate=1 November 2013}}</ref> They analysed 19,379 mind maps from 11,179 users of the mind mapping applications [[SciPlore MindMapping]] (now [[Docear]]) and [[MindMeister]]. Results include that average users create only a few mind maps (mean=2.7), average mind maps are rather small (31 nodes) with each node containing about 3 words (median). However, there were exceptions. One user created more than 200 mind maps, the largest mind map consisted of more than 50,000 nodes and the largest node contained ~7500 words. The study also showed that between different mind mapping applications ([[Docear]] vs [[MindMeister]]) significant differences exist related to how users create mind maps.

'''Automatic Creating of Mind Maps''' - There have been some attempts to create mind maps automatically. Brucks & Schommer created mind maps automatically from full-text streams.<ref name="Brucks2008">{{cite journal| author={C}laudine {B}rucks, {C}hristoph {S}chommer| title=Assembling Actor-based Mind-Maps from Text Stream| journal=CoRR| year=2008| volume=abs/0810.4616| accessdate=1 November 2013}}</ref> Rothenberger et al. extracted the main story of a text and presented it as mind map.<ref name="Rothenberger2008">{{cite journal| author=Rothenberger, T, Oez, S, Tahirovic, E, Schommer, Christoph| title=Figuring out Actors in Text Streams: Using Collocations to establish Incremental Mind-maps| journal= | arxiv=0803.2856| year=2008}}</ref> And there is a patent about automatically creating sub-topics in mind maps.<ref name="Plotkin09">{{cite journal|year=2009|title=Software tool for creating outlines and mind maps that generates subtopics automatically|journal=USPTO Application: 20090119584|volume=|author={R}obert {P}lotkin|accessdate=1 November 2013}}</ref>

'''Pen and Paper vs Computer''' - There are two studies that analyze whether electronic mind mapping or pen based mind mapping is more effective.<ref name="Mahler09">{{cite book| author={M}ahler, {T}., {W}eber, {M}.| chapter=Dimian-Direct Manipulation and Interaction in Pen Based Mind Mapping| title=Proceedings of the 17th World Congress on Ergonomics, IEA 2009| year=2009| accessdate=1 November 2013}}</ref><ref name="Shih09">{{cite journal| author={S}hih, {P}.{C}., {N}guyen, {D}.{H}., {H}irano, {S}.{H}. and {R}edmiles, {D}.{F}., {H}ayes, {G}.{R}.| title=Groupmind: supporting idea generation through a collaborative mind-mapping tool| year=2009| pages=139–148| accessdate=1 November 2013}}</ref>

==Tools==
[[List of concept- and mind-mapping software|Mind-mapping software]] can be used to organize large amounts of information, combining spatial organization, dynamic hierarchical structuring and node folding. Software packages can extend the concept of mind-mapping by allowing individuals to map more than thoughts and ideas with information on their computers and the Internet, like spreadsheets, documents, Internet sites and images.<ref>{{cite web|url=http://www.imdevin.com/top-10-totally-free-mind-mapping-software-tools/|title=Top 10 Totally Free Mind Mapping Software Tools|last=Santos|first=Devin|date=15 February 2013|publisher=IMDevin|accessdate=10 July 2013}}</ref> It has been suggested that mind-mapping can improve learning/study efficiency up to 15% over conventional [[note-taking]].<ref>
{{cite journal
  | last = Farrand
  | first = Paul |author2=Hussain, Fearzana |author3=Hennessy, Enid
  | title = The efficacy of the 'mind map' study technique
  | journal = Medical Education
  | volume = 36
  | issue = 5
  | pages = 426–431
  | date = May 2002
  | doi = 10.1046/j.1365-2923.2002.01205.x
  | pmid = 12028392
}}</ref>

==See also==
{{Portal|Education}}
* [[Brainstorming]]
* [[Graph (discrete mathematics)]]
* [[Idea]]
* [[List of concept mapping and mind mapping software]]
* [[Mental literacy]]
* [[Personal wiki]]

; Related diagrams
* [[Argument map]]
* [[Cognitive map]]
* [[Concept map]]
* [[Nodal organizational structure]]
* [[Radial tree]]
* [[Rhizome (philosophy)]]
* [[Semantic network]]
* [[Social map]]
* [[Spider mapping]]
* [[Tree structure]]

==References==
{{reflist|30em}}

==Further reading==
* {{cite journal |last= Novak |first= J.D. |date= 1993 |title= How do we learn our lesson?: Taking students through the process |journal= [[The Science Teacher]] |volume= 60 |number= 3 |pages= 50–55 |issn= 0036-8555 }}

==External links==
*{{Commons category-inline|Mind maps}}

{{Mindmaps}}

{{Authority control}}

{{DEFAULTSORT:Mind Map}}
[[Category:Articles with inconsistent citation formats]]
[[Category:Knowledge representation]]
[[Category:Games of mental skill]]
[[Category:Creativity]]
[[Category:Design]]
[[Category:Educational technology]]
[[Category:Diagrams]]
[[Category:Thought]]
[[Category:Note-taking]]
[[Category:Reading (process)]]
[[Category:Zoomable user interfaces]]
[[Category:Educational devices]]
<=====doc_Id=====>:905
<=====title=====>:
UMBEL
<=====text=====>:
{{for|the part of a plant|Umbel}}
{{Paid contributions|date=December 2015}}
{{Infobox software
 |name = UMBEL
 |logo =
 |screenshot =
 |caption = Upper Mapping and Binding Exchange Layer
 |developer =  Structured Dynamics
 |released = 16 July 2008
 |latest_release_version =  UMBEL 1.50
 |latest_release_date = 10 May 2016<ref>{{cite web |url=http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/
 |title=New, Major Upgrade of UMBEL Released: version 1.50 |publisher=UMBEL Web site |date=11 May 2016}}</ref>
 |operating_system =
 |genre = {{Flatlist|
* [[Ontology (information science)|Ontology]]
* [[Semantic Web]]
* [[Linked Data]]
* [[Artificial intelligence]]
}}
 |programming language = {{Flatlist|
* [[Web Ontology Language|OWL 2]]
* [[SKOS]]
}}
 |license = [[Creative Commons licenses|Creative Commons Attribution 3.0]]
 |website = {{URL|umbel.org}}
}}
'''UMBEL''' ('''U'''pper '''M'''apping and '''B'''inding '''E'''xchange '''L'''ayer) is a logically organized knowledge [[Graph (discrete mathematics)|graph]] of 34,000 concepts and entity types that can be used in [[information science]] for relating information from disparate sources to one another. The [[Symbol grounding problem|grounding]] of this information occurs by common reference to the permanent [[Uniform Resource Identifier|URIs]] for the UMBEL concepts; the connections within the UMBEL [[Upper ontology (information science)|upper ontology]] enable concepts from sources at different levels of abstraction or specificity to be logically related. Since UMBEL is an [[open source]] extract of the [[Cyc#OpenCyc|OpenCyc]] [[knowledge base]], it can also take advantage of the [[Inference engine|reasoning capabilities]] within [[Cyc]].

UMBEL has two means to promote the [[semantic interoperability]] of information:.<ref>{{cite web
 | url = http://techwiki.umbel.org/index.php/UMBEL_Specification
 | title = UMBEL (Upper Mapping and Binding Exchange Layer)
 | website = UMBEL Web Site
 | access-date = 8 February 2016
}}</ref> It is:

* An [[ontology (information science)|ontology]] of about 35,000 reference concepts, designed to provide common [[Data mapping|mapping]] points for relating different ontologies or [[Conceptual schema|schema]] to one another, and
* A [[Controlled vocabulary|vocabulary]] for aiding that ontology mapping, including expressions of likelihood relationships distinct from exact identity or equivalence. This vocabulary is also designed for interoperable [[Ontology (information science)#Domain ontologies and upper ontologies|domain ontologies]].

[[File:LOD Cloud 2014.svg|thumb|400px|Diagram showing [[Linking Open Data]] datasets.  UMBEL is near the hub, below and to the right of the central DBpedia.]]

UMBEL is written in the [[Semantic Web]] languages of [[SKOS]] and [[Web Ontology Language|OWL 2]]. It is a [[Class (set theory)|class]] structure used in [[Linked Data]], along with OpenCyc, [[Yago (database)|YAGO]], and the [[DBpedia]] ontology. Besides data integration, UMBEL has been used to aid concept search,<ref>{{cite book
|last1= Sah
|first1= M
|last2= Wade
|first2= V
|year= 2012
|chapter= A novel concept-based search for the web of data using UMBEL and a fuzzy retrieval model
|chapter-url= https://www.researchgate.net/profile/Melike_Sah/publication/262218017_A_novel_concept-based_search_for_the_web_of_data_using_UMBEL_and_a_fuzzy_retrieval_model/links/555d9c5808ae8c0cab2ad795.pdf
|title= The Semantic Web: Research and Applications
|publisher= Springer
|location= Berlin Heidelberg
|publication-date= 27 May 2012
|pages= 103–118
}}</ref><ref>{{cite book
|last1= Sah
|first1= M
|last2= Wade
|first2= V
|year= 2013
|chapter= Personalized concept-based search and exploration on the web of data using results categorization
|chapter-url= http://eswc-conferences.org/sites/default/files/papers2013/sah.pdf
|title= The Semantic Web: Semantics and Big Data
|publisher= Springer
|location= Berlin Heidelberg
|publication-date= 26 May 2013
|pages= 532–547
}}</ref> concept definitions,<ref>{{cite book
|last1= Ballatore
|first1= Andrea
|editor1-first= Harlan
|editor1-last= Onsrud
|editor2-first= Werner
|editor2-last= Kuhn
|year= 2016
|chapter= Prolegomena for an Ontology of Place
|chapter-url= http://eprints.cdlib.org/uc/item/0rw1n045.pdf
|title= Advancing Geographic Information Systems
|publisher= GSDI Association Press
|location= Needham, MA
|pages= 91–103
}}</ref> [[Ranking (information retrieval)|query ranking]],<ref>{{cite journal
| last       = Stecher
| first      = R
| last2      = Costache
| first2     = S
| last3      = Niederée
| first3     = C
| last4      = Nejdl
| first4     = W
| date       = 7 Jun 2010
| title      = Query ranking in information integration
| url        = https://www.researchgate.net/profile/Stefania_Costache2/publication/220921101_Query_Ranking_in_Information_Integration/links/00b7d516d17814201b000000.pdf
| journal    = Advanced Information Systems Engineering
| publisher  = Springer Berlin Heidelberg
| pages      = 230–235
}}</ref> ontology integration,<ref>{{cite book
|last1= Damova
|first1= M
|last2= et
|first2= al.
|year= 2012
|editor1-first= Maria Teresa
|editor1-last= Pazienza
|chapter= Creation and Integration of Reference Ontologies for Efﬁcient LOD Management
|url= http://www.igi-global.com/book/semi-automatic-ontology-development/58294
|title= Semi-Automatic Ontology Development: Processes and Resources
|publisher= IGI Global
|pages= 162–199
}}</ref> and ontology consistency checking.<ref>{{cite journal
| last       = Sheng
| first      = Z
| last2      = Wang
| first2     = X
| last3      = Shi
| first3     = H
| last4      = Feng
| first4     = Z
| date       = 26 Oct 2012
| title      = Checking and handling inconsistency of DBpedia
| url        = http://link.springer.com/chapter/10.1007/978-3-642-33469-6_60
| journal    = Web Information Systems and Mining
| publisher  = Springer Berlin Heidelberg
| pages      = 480–488
}}</ref> It has also been used to build large ontologies <ref>{{cite journal
| last       = Yablonsky
| first      = S
| date       = Jun 2009
| title      = Semantic Web framework for development of very large ontologies
| url        = http://www.scielo.org.mx/pdf/poli/n39/n39a4.pdf
| journal    = Polibits
| issue      = 39
| pages      = 19–26
}}</ref> and for online [[question answering]] systems.<ref>{{cite journal
| last       = Bishop
| first      = B
| last2      = et
| first2     = al.
| date       = Jan 2011
| title      = Factforge: A fast track to the web of data
| url        = http://www.semantic-web-journal.net/sites/default/files/swj77_2.pdf
| journal    = Semantic Web
| volume     = 2
| issue      = 2
| pages      = 157–166
}}</ref>

Including OpenCyc, UMBEL has about 65,000 formal mappings to [[DBpedia]], PROTON, [[GeoNames]], and [[schema.org]], and provides linkages to more than 2 million [[Wikipedia]] pages (English version). All of its reference concepts and mappings are organized under a hierarchy of 31 different "super types",<ref>See [http://techwiki.umbel.org/index.php/UMBEL_-_Annex_G Annex G] in the UMBEL specifications.</ref> which are mostly disjoint from one another. Each of these "super types" has its own typology of entity classes to provide flexible tie-ins for external content. 90% of UMBEL is contained in these entity classes.

UMBEL was first released in July 2008. Version 1.00 was released in February 2011.<ref>See the [http://umbel.org/resources/news/finally-umbel-v-100] release announcement.</ref> Its current release is version 1.50.<ref>See http://umbel.org/resources/news/new-major-upgrade-of-umbel-released-version-1.50/</ref>

==See also==
* [[Cyc]]
* [[DBpedia]]

==Notes==
{{Reflist}}

==External links==
* [http://umbel.org/ Main page for UMBEL]
* [http://techwiki.umbel.org/index.php/UMBEL_Specification UMBEL specification], and its accompanying [http://umbel.org/specifications/annexes Annexes A - L, Z]

{{Semantic Web}}
{{Use dmy dates|date=October 2012}}

{{DEFAULTSORT:Umbel}}
[[Category:Knowledge representation]]
[[Category:Ontology (information science)]]
[[Category:Semantic Web]]
[[Category:Knowledge bases]]
<=====doc_Id=====>:908
<=====title=====>:
Pattern language
<=====text=====>:
{{About|the structured design approach by architect Christopher Alexander}}
A '''pattern language''' is a method of describing good design practices or patterns of useful organization within a field of expertise. The term was coined by architect [[Christopher Alexander]] and popularized by his 1977 book ''[[A Pattern Language]]''.

A pattern language can also be an attempt to express the deeper wisdom of what brings aliveness within a particular field of human endeavor, through a set of interconnected patterns. Aliveness is one placeholder term for "the quality that has no name": a sense of wholeness, spirit, or grace, that while of varying form, is precise and empirically verifiable.{{cn|date=March 2016}} Some advocates{{who|date=March 2016}} of this design approach claim that ordinary people can use it to successfully solve very large, complex design problems.

==What is a pattern?==
{{See also|Design pattern}}
When a designer designs something – whether a house, computer program, or lamp – they must make many decisions about how to solve problems. A single problem is documented with its typical place (the [[syntax]]), and use (the [[grammar]]) with the most common and recognized good solution seen in the wild, like the examples seen in [[dictionary|dictionaries]]. Each such entry is a single [[design pattern]]. Each pattern has a name, a descriptive entry, and some cross-references, much like a dictionary entry. A documented pattern should explain why that solution is good in the pattern's contexts.

Elemental or universal ''patterns'' such as "door" or "partnership" are versatile ideals of design, either as found in experience or for use as components in practice, explicitly described as holistic resolutions of the forces in recurrent contexts and circumstances, whether in architecture, medicine, software development or governance, etc. Patterns might be invented or found and studied, such as the naturally occurring patterns of design that characterize human environments.<ref>Henshaw, J. [http://www.synapse9.com/pub/2015_PURPLSOC-JLHfinalpub.pdf Guiding Patterns of Naturally Occurring Design: Elements. PURPLSOC 2015 proceedings, July 3-5 2015 Krems, Austria] PURPLSOC meeting on the many open scientific questions, e.g. regarding the theoretical background of patterns and the practical implementation of pattern methods in research and teaching.</ref>

Like all languages, a pattern language has [[vocabulary]], [[syntax]], and [[grammar]] – but a pattern language applies to some complex activity other than communication. In pattern languages for design, the parts break down in this way:
* The language description – the ''vocabulary'' – is a collection of named, described solutions to problems in a field of interest. These are called ''design patterns''. So, for example, the language for architecture describes items like: settlements, buildings, rooms, windows, latches, etc.
* Each solution includes ''syntax'', a description that shows where the solution fits in a larger, more comprehensive or more abstract design. This automatically links the solution into a web of other needed solutions. For example, rooms have ways to get light, and ways to get people in and out.
* The solution includes ''grammar'' that describes how the solution solves a problem or produces a benefit. So, if the benefit is unneeded, the solution is not used. Perhaps that part of the design can be left empty to save money or other resources; if people do not need to wait to enter a room, a simple doorway can replace a waiting room.
* In the language description, grammar and syntax cross index (often with a literal alphabetic index of pattern names) to other named solutions, so the designer can quickly think from one solution to related, needed solutions, and document them in a logical way. In Christopher Alexander's book ''A Pattern Language'', the patterns are in decreasing order by size, with a separate alphabetic index.
* The web of relationships in the index of the language provides many paths through the design process.

This simplifies the design work because designers can start the process from any part of the problem they understand and work toward the unknown parts. At the same time, if the pattern language has worked well for many projects, there is reason to believe that even a designer who does not completely understand the design problem at first will complete the design process, and the result will be usable. For example, skiers coming inside must shed snow and store equipment. The messy snow and boot cleaners should stay outside. The equipment needs care, so the racks should be inside.

==Many patterns form a language==
Just as [[words]] must have [[Grammar|grammatical]] and [[Semantics|semantic]] relationships to each other in order to make a spoken [[language]] useful, design patterns must be related to each other in position and utility order to form a pattern language. Christopher Alexander's work describes a process of decomposition, in which the designer has a problem (perhaps a commercial assignment), selects a solution, then discovers new, smaller problems resulting from the larger solution. Occasionally, the smaller problems have no solution, and a different larger solution must be selected. Eventually all of the remaining design problems are small enough or routine enough to be solved by improvisation by the builders, and the "design" is done.

The actual organizational structure ([[Hierarchy|hierarchical]], [[Iterative method|iterative]], etc.) is left to the discretion of the designer, depending on the problem. This explicitly lets a designer explore a design, starting from some small part. When this happens, it's common for a designer to realize that the problem is actually part of a larger solution. At this point, the design almost always becomes a better design.

In the language, therefore, each pattern has to indicate its relationships to other patterns and to the language as a whole. This gives the designer using the language a great deal of guidance about the related problems that must be solved.

The most difficult part of having an outside expert apply a pattern language is in fact to get a reliable, complete list of the problems to be solved. Of course, the people most familiar with the problems are the people that need a design. So, Alexander famously advocated on-site improvisation by concerned, empowered users,<ref>A Pattern Language, ibid</ref><ref>Alexander, Christopher, The Oregon Project</ref> as a powerful way to form very workable large-scale initial solutions, maximizing the utility of a design, and minimizing the design rework. The desire to empower users of architecture was, in fact, what led Alexander to undertake a pattern language project for architecture in the first place.

==Design problems in a context==
An important aspect of design patterns is to identify and document the key ideas that make a good system different from a poor system (that may be a house, a computer program or an object of daily use), and to assist in the design of future systems. The idea expressed in a pattern should be general enough to be applied in very different systems within its context, but still specific enough to give constructive guidance.

The range of situations in which the problems and solutions addressed in a pattern apply is called its context. An important part in each pattern is to describe this context. Examples can further illustrate how the pattern applies to very different situation.

For instance, Alexander's pattern "A PLACE TO WAIT" addresses bus stops in the same way as waiting rooms in a surgery, while still proposing helpful and constructive solutions. The [[Design Patterns|"Gang-of-Four" book ''Design Patterns'']] by Gamma et al. proposes solutions that are independent of the programming language, and the program's application domain.

Still, the problems and solutions described in a pattern can vary in their level of abstraction and generality on the one side, and specificity on the other side. In the end this depends on the author's preferences. However, even a very abstract pattern will usually contain examples that are, by nature, absolutely concrete and specific.

Patterns can also vary in how far they are proven in the real world. Alexander gives each pattern a rating by zero, one or two stars, indicating how well they are proven in real-world examples. It is generally claimed that all patterns need at least some existing real-world examples. It is, however, conceivable to document yet unimplemented ideas in a pattern-like format.

The patterns in Alexander's book also vary in their level of scale – some describing how to build a town or neighbourhood, others dealing with individual buildings and the interior of rooms. Alexander sees the low-scale artifacts as constructive elements of the large-scale world, so they can be connected to a [[#Aggregation in an associative network (pattern language)|hierarchic network]].

===Balancing of forces===
A pattern must characterize the problems that it is meant to solve, the context or situation where these problems arise, and the conditions under which the proposed solutions can be recommended.

Often these problems arise from a conflict of different interests or "forces". A pattern emerges as a dialogue that will then help to balance the forces and finally make a decision.

For instance, there could be a pattern suggesting a wireless telephone. The forces would be the need to communicate, and the need to get other things done at the same time (cooking, inspecting the bookshelf). A very specific pattern would be just "WIRELESS TELEPHONE". More general patterns would be "WIRELESS DEVICE" or "SECONDARY ACTIVITY", suggesting that a secondary activity (such as talking on the phone, or inspecting the pockets of your jeans) should not interfere with other activities.

Though quite unspecific in its context, the forces in the "SECONDARY ACTIVITY" pattern are very similar to those in "WIRELESS TELEPHONE". Thus, the competing forces can be seen as part of the essence of a design concept expressed in a pattern.

===Patterns contain their own rationale===
Usually a pattern contains a rationale referring to some given values. For Christopher Alexander, it is most important to think about the people who will come in contact with a piece of architecture. One of his key values is making these people feel more alive. He talks about the "quality without a name" (QWAN).

More generally, we could say that a good system should be accepted, welcomed and happily embraced as an enrichment of daily life by those who are meant to use it, or – even better – by all people it affects. For instance, when discussing a street café, Alexander discusses the possible desires of a guest, but also mentions people who just walk by.

The same thinking can be applied to technical devices such as telephones and cars, to social structures like a team working on a project, or to the user interface of a computer program. The qualities of a software system, for instance, could be rated by observing whether users spend their time enjoying or struggling with the system.

By focusing on the impacts on human life, we can identify patterns that are independent from changing technology, and thus find "timeless quality" (Alexander).

==Generic structure and layout==
Usually the author of a pattern language or collection chooses a generic structure for all the patterns it contains, breaking each into generic sections like context, problem statement, solution etc.

Christopher Alexander's patterns, for instance, each consist of a short name, a rating (up to two '*' symbols), a sensitizing picture, the context description, the problem statement, a longer part of text with examples and explanations, a solution statement, a sketch and further references. This structure and layout is sometimes referred to as the "Alexandrian form".

Alexander uses a special text layout to mark the different sections of his patterns. For instance, the problem statement and the solution statement are printed in bold font, the latter is always preceded by the "Therefore:" keyword. Some authors instead use explicit labels, which creates some degree of redundancy.

===Meaningful names===
When design is done by a team, pattern names will form a vocabulary they can share. This makes it necessary for pattern names to be easy to remember and highly descriptive. Some examples from Alexander's works are WINDOW PLACE (helps define where windows should go in a room) and A PLACE TO WAIT (helps define the characteristics of bus stops and hospital waiting rooms, for example).

==Aggregation in an associative network (pattern language)==
A pattern language, as conceived by Alexander, contains links from one pattern to another, so when trying to apply one pattern in a project, a designer is pushed to other patterns that are considered helpful in its context.

In Alexander's book, such links are collected in the "references" part, and echoed in the linked pattern's "context" part – thus the overall structure is a directed graph. A pattern that is linked to in the "references" usually addresses a problem of lower scale, that is suggested as a part of the higher-scale problem. For instance, the "PUBLIC OUTDOOR ROOM" pattern has a reference to "STAIR SEATS".

Even without the pattern description, these links, along with meaningful names, carry a message: When building a place outside where people can spend time ("PUBLIC OUTDOOR ROOM"), consider to surround it by stairs where people can sit ("STAIR SEATS"). If you are planning an office ("WORKSHOPS AND OFFICES"), consider to arrange workspaces in small groups ("SMALL WORKING GROUPS"). Alexander argues that the connections in the network can be considered even more meaningful than the text of the patterns themselves.

The links in Alexander's book clearly result in a hierarchic network. Alexander draws a parallel to the hierarchy of a grammar – that is one argument for him to speak of a pattern ''language''.

The idea of linking is generally accepted among pattern authors, though the semantic rationale behind the links may vary. Some authors, however, like Gamma et al. in ''[[Design Patterns]]'', make only little use of pattern linking – possibly because it did not make that much sense for their collection of patterns. In such a case we would speak of a ''pattern catalogue'' rather than a ''pattern language''.<ref name="dearden">{{cite journal | author = Andy Dearden, Janet Finlay | title = Pattern Languages in HCI: A critical review | date = January 2006 | journal = Human Computer Interaction | volume = 21 | issue = 1 }}</ref>

===Usage===
Alexander encouraged people who used his system to expand his language with patterns of their own. In order to enable this, his books do not focus strictly on architecture or civil engineering; he also explains the general method of pattern languages. The original concept for the book ''A Pattern Language'' was that it would be published in the form of a 3-ring binder, so that pages could easily be added later; this proved impractical in publishing.<ref>Portland Urban Architecture Research Laboratory
Symposium 2009, presentation by 4 of 6 original authors of ''A Pattern Language''.</ref>  The pattern language approach has been used to document expertise in diverse fields. Some examples are [[Design pattern (architecture)|architectural patterns]], [[Design pattern (computer science)|computer science patterns]], [[interaction design pattern]]s, [[pedagogical patterns]],  social action patterns, and group facilitation patterns. The pattern language approach has also been recommended as a way to promote [[civic intelligence]] by helping to coordinate actions for diverse people and communities who are working together on significant shared problems (see <ref>Schuler, D. [http://publicsphereproject.org/sites/default/files/Critical%20Enablers%20of%20Civic%20Intelligence.reduced.pdf Choosing Success: Pattern Languages as Critical Enablers of Civic Intelligence]; PUARL Conference, Portland, OR. 2009</ref> for additional discussion of motivation and rationale as well as examples and experiments).  Alexander's specifications for using pattern languages as well as creating new ones remain influential, and his books are referenced for style by experts in unrelated fields.

It is important to note that notations such as [[Unified Modeling Language|UML]] or the [[flowchart]] symbol collection are not pattern languages. They could more closely be compared to an alphabet: their symbols could be used to document a pattern language, but they are not a language by themselves. A [[recipe]] or other sequential set of steps to be followed, with only one correct path from start to finish, is also not a pattern language. However, the process of designing a new recipe might benefit from the use of a pattern language.

===Simple example of a pattern===
*''Name'': ChocolateChipRatio
*''Context'': You are baking chocolate chip cookies in small batches for family and friends
*''Consider these patterns first'': SugarRatio, FlourRatio, EggRatio
*''Problem'': Determine the optimum ratio of chocolate chips to cookie dough
*''Solution'': Observe that most people consider chocolate to be the best part of the chocolate chip cookie. Also observe that too much chocolate may prevent the cookie from holding together, decreasing its appeal. Since you are cooking in small batches, cost is not a consideration. Therefore, use the maximum amount of chocolate chips that results in a really sturdy cookie.
*''Consider next'': NutRatio or CookingTime or FreezingMethod

==Origin==
[[Christopher Alexander]], an architect and author, coined the term pattern language.<ref>{{Cite book | publisher = [[Oxford University Press]], USA | isbn = 0-19-501919-9 | last = Alexander | first = Christopher | title = A Pattern Language: Towns, Buildings, Construction | year = 1977 | page = 1216}}</ref> He used it to refer to common problems of the [[design]] and [[construction]] of buildings and towns and how they should be solved. The solutions proposed in the book include suggestions ranging from how cities and towns should be structured to where windows should be placed in a room.

The framework and philosophy of the "pattern language" approach was initially popularized in the book ''[[A Pattern Language]]'' that was written by Christopher Alexander and five colleagues at the Center for Environmental Structure in Berkeley, California in the late 1970s. While ''A Pattern Language'' contains 253 "patterns" from the first pattern, "Independent Regions" (the most general) to the last, "Things from Your Life", Alexander's book ''[[The Timeless Way of Building]]'' goes into more depth about the motivation and purpose of the work. The following definitions of "pattern" and "pattern language" are paraphrased from ''A Pattern Language'':

"A ''pattern'' is a careful description of a perennial solution to a recurring problem within a building context, describing one of the configurations that brings life to a building.

Each pattern describes a problem that occurs over and over again in our environment, and then describes the core solution to that problem, in such a way that you can use the solution a million times over, without ever doing it the same way twice."

A ''pattern language'' is a network of patterns that call upon one another. Patterns help us remember insights and knowledge about design and can be used in combination to create solutions.

== Application domains ==
Christopher Alexander's idea has been adopted in other disciplines, often much more heavily than the original [[Pattern (architecture)|application of patterns to architecture]] as depicted the book ''[[A Pattern Language]]''. Recent examples include [[software design pattern]]s in software engineering and, more generally, [[Architectural pattern (computer science)|architectural patterns in computer science]], as well as [[interaction design pattern]]s. [[Pedagogical patterns]] are used to document good practices in teaching. The book ''Liberating Voices: A Pattern Language for Communication Revolution'', containing [http://www.publicsphereproject.org/patterns/lv 136 patterns] for using information and communication to promote sustainability, democracy and positive social change, was published in 2008. The deck "Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings" was published in 2011. Recently, patterns were also introduced into [[systems architecture]] design.<ref>{{cite web|last=Hein|first=Andreas|title=Adopting Patterns for Space Mission and Space Systems Architecting|url=http://www.academia.edu/2110976/A.M._Hein_Adopting_Patterns_for_Space_Mission_and._Space_Systems_Architecting_|work=5 th International Workshop on System & Concurrent Engineering for Space ApplicationsSECESA 2012|accessdate=2 March 2013}}</ref><ref>{{cite web|last=Hein|first=Andreas|title=Project Icarus: Stakeholder Scenarios for an Interstellar Exploration Program|url=http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM|work=Journal of the British Interplanetary Society, 64, 224-233, 2011|accessdate=2 March 2013}}</ref><ref>{{cite web|last=Cloutier|first=Robert|title=The Concept of Reference Architectures|url=http://www.calimar.com/TheConceptOfReferenceArchitectures.pdf|work=Systems Engineering Vol. 13, No. 1, 2010|accessdate=2 March 2013}}</ref>  [[Chess]] [[Chess strategy|strategy]] and [[Chess tactics|tactics]] involve many patterns from [[Chess opening|opening]] to [[checkmate]].

==See also==
* [[Feng shui]]
* [[Method engineering]]
* [[Rule of thumb]]
* [[Typology (urban planning and architecture)]]

==References==
{{Reflist}}

==Further reading==
* Christopher Alexander, Sara Ishikawa, Murray Silverstein (1974). 'A Collection of Patterns which Generate Multi-Service Centres' in Declan and Margrit Kennedy (eds.): ''The Inner City.'' Architects Year Book 14, Elek, London. ISBN 0 236 15431 1.
* Alexander, C. (1977). ''[[A Pattern Language: Towns, Buildings, Construction]]''. USA: [[Oxford University Press]]. ISBN 978-0-19-501919-3.
* Alexander, C. (1979). ''The Timeless Way of Building''. USA: Oxford University Press. ISBN 978-0-19-502402-9.
* Schuler, D. (2008). ''Liberating Voices: A Pattern Language for Communication Revolution''. USA: [[MIT Press]]. ISBN 978-0-262-69366-0.
* Leitner, Helmut (2015): ''Pattern Theory: Introduction and Perspectives on the Tracks of Christopher Alexander''. ISBN 1505637430.

==External links==

===About patterns in general===
* [http://www.c2.com/cgi/wiki?TipsForWritingPatternLanguages Tips For Writing Pattern Languages], by [[Ward Cunningham]]
* [http://www.gardenvisit.com/landscape/architecture/3.1-patternlanguage.htm Essay on the pattern language as it relates to urban design]
* [http://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM Use of patterns for scenario development for large scale aerospace projects]
* [http://torgronsund.wordpress.com/2010/01/06/lean-startup-business-model-pattern/ Lean Startup Business Model Pattern]
* [http://www.informit.com/articles/printerfriendly.aspx?p=30084 What Is a Quality Use Case?] from the book ''Patterns for Effective Use Cases''
* [http://groupworksdeck.org/what-we-mean-by-pattern Characteristics of group facilitation patterns]

===Online pattern collections===
* [http://www.patternlanguage.com/ patternlanguage.com], by the Center for Environmental Structure
* [http://www.fusedgrid.ca/ Fused Grid] – A Contemporary Urban Pattern "a collection and synthesis of neighbourhood patterns"
* [http://www.reliableprosperity.net ReliableProsperity.net] – Patterns for building a "restorative, socially just, and reliably prosperous society"
* [http://www.hcipatterns.org/ hcipatterns.org] – Patterns for HCI
* [http://www.c2.com/cgi/wiki?PatternIndex The Portland Pattern Repository]
* [http://developer.yahoo.com/ypatterns Yahoo! Design Pattern Library]
* [http://groupworksdeck.org Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings] – A pattern language of group process
* [http://liveingreatness.com/core-protocols/ The Core Protocols] – A set of team communication patterns
* [http://www.publicsphereproject.org/patterns/lv Liberating Voices! Pattern Language Project] — Short versions of patterns available in [http://www.publicsphereproject.org/patterns_arabic Arabic], [http://www.publicsphereproject.org/patterns_chinese Chinese], and [http://www.publicsphereproject.org/patterns_spanish Spanish]

{{DEFAULTSORT:Pattern Language}}

[[Category:Architectural theory]]
[[Category:Cybernetics]]
[[Category:Design]]
[[Category:Knowledge representation]]
[[Category:Linguistics]]

[[fi:Suunnittelumalli]]
<=====doc_Id=====>:911
<=====title=====>:
Tree (data structure)
<=====text=====>:
{{hatnote|Not to be confused with [[trie]], a specific type of tree data structure.}}
{{Refimprove|date=August 2010}}
[[File:binary tree.svg|right|192|thumb|A simple unordered tree; in this diagram, the node labeled 7 has two children, labeled 2 and 6, and one parent, labeled 2. The root node, at the top, has no parent.]]
In [[computer science]], a '''tree''' is a widely used [[abstract data type]] (ADT)—or [[data structure]] implementing this ADT—that simulates a hierarchical [[tree structure]], with a root value and [[subtrees]] of children with a parent node, represented as a set of linked [[Vertex (graph theory)|nodes]].

A tree data structure can be defined [[Recursion|recursively]] (locally) as a collection of [[node (computer science)|nodes]] (starting at a root node), where each node is a data structure consisting of a value, together with a list of references to nodes (the "children"), with the constraints that no reference is duplicated, and none points to the root.

Alternatively, a tree can be defined abstractly as a whole (globally) as an [[ordered tree]], with a value assigned to each node. Both these perspectives are useful: while a tree can be analyzed mathematically as a whole, when actually represented as a data structure it is usually represented and worked with separately by node (rather than as a list of nodes and an [[adjacency list]] of edges between nodes, as one may represent a [[#Digraphs|digraph]], for instance). For example, looking at a tree as a whole, one can talk about "the parent node" of a given node, but in general as a data structure a given node only contains the list of its children, but does not contain a reference to its parent (if any).

==Definition==
{| style="float:right"
| [[File:Directed graph, disjoint.svg|thumb|x100px|{{color|#800000|Not a tree}}: two non-[[Connectivity (graph theory)#Definitions of components, cuts and connectivity|connected]] parts, A→B and C→D→E. There is more than one root.]]
|}
{| style="float:right"
| [[File:Directed graph with branching SVG.svg|thumb|x100px|{{color|#800000|Not a tree}}: undirected cycle 1-2-4-3. 4 has more than one parent (inbound edge).]]
|}
{| style="float:right"
| [[File:Directed graph, cyclic.svg|thumb|x100px|{{color|#800000|Not a tree}}: cycle B→C→E→D→B. B has more than one parent (inbound edge).]]
|}
{| style="float:right"
| [[File:Graph single node.svg|thumb|x50px|{{color|#800000|Not a tree}}: cycle A→A. A is the root but it also has a parent.]]
|}
{| style="float:right"
| [[File:Directed Graph Edge.svg|thumb|x50px|Each linear list is trivially {{color|#008000|a tree}}]]
|}

A tree is a (possibly non-linear) data structure made up of nodes or vertices and edges without having any cycle. The tree with no nodes is called the '''null''' or '''empty''' tree. A tree that is not empty consists of a root node and potentially many levels of additional nodes that form a hierarchy.

==Terminology used in Trees==
{{term|Root}} {{defn|The top node in a tree.}}
{{term|Child}} {{defn|A node directly connected to another node when moving away from the Root.}}
{{term|Parent}} {{defn|The converse notion of a ''child''.}}
{{term|Siblings}} {{defn| A group of nodes with the same parent.}}
{{term|Descendant}} {{defn|A node reachable by repeated proceeding from parent to child.}}
{{term|Ancestor}} {{defn|A node reachable by repeated proceeding from child to parent.}}
{{term|Leaf}} {{term|(less commonly called External node)|multi=y}} {{defn|A node with no children.}}
{{term|Branch}} {{term|Internal node|multi=y}} {{defn|A node with at least one child.}}
{{term|Degree}} {{defn|The number of sub trees of a node.}}
{{term|Edge}} {{defn|The connection between one node and another.}}
{{term|Path}} {{defn|A sequence of nodes and edges connecting a node with a descendant.}}
{{term|Level}} {{defn|The level of a node is defined by 1 + (the number of connections between the node and the root).}}
{{term|Height of node}} {{defn|The height of a node is the number of edges on the longest path between that node and a leaf.}}
{{term|Height of tree}} {{defn|The height of a tree is the height of its root node.}}
{{term|Depth}} {{defn|The depth of a node is the number of edges from the tree's root node to the node.}}
{{term|Forest}} {{defn|A forest is a set of n ≥ 0 disjoint trees.}}

===Data type vs. data structure===
There is a distinction between a tree as an abstract data type and as a concrete data structure, analogous to the distinction between a [[List (abstract data type)|list]] and a [[linked list]].
As a data type, a tree has a value and children, and the children are themselves trees; the value and children of the tree are interpreted as the value of the root node and the subtrees of the children of the root node. To allow finite trees, one must either allow the list of children to be empty (in which case trees can be required to be non-empty, an "empty tree" instead being represented by a forest of zero trees), or allow trees to be empty, in which case the list of children can be of fixed size ([[branching factor]], especially 2 or "binary"), if desired.

As a data structure, a linked tree is a group of [[Node (computer science)|nodes]], where each node has a value and a list of [[Reference (computer science)|references]] to other nodes (its children). This data structure actually defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} because it may have loops or several references to the same node, just as a linked list may have a loop. Thus there is also the requirement that no two references point to the same node (that each node has at most a single parent, and in fact exactly one parent, except for the root), and a tree that violates this is "corrupt".

Due to the use of ''references'' to trees in the linked tree data structure, trees are often discussed implicitly assuming that they are being represented by references to the root node, as this is often how they are actually implemented. For example, rather than an empty tree, one may have a null reference: a tree is always non-empty, but a reference to a tree may be null.

===Recursive===

Recursively, as a data type a tree is defined as a value (of some data type, possibly empty), together with a list of trees (possibly an empty list), the subtrees of its children; symbolically:
 t: v <nowiki>[t[1], ..., t[k]]</nowiki>
(A tree ''t'' consists of a value ''v'' and a list of other trees.)

More elegantly, via [[mutual recursion]], of which a tree is one of the most basic examples, a tree can be defined in terms of a forest (a list of trees), where a tree consists of a value and a forest (the subtrees of its children):
 f: <nowiki>[t[1], ..., t[k]]</nowiki>
 t: v f

Note that this definition is in terms of values, and is appropriate in [[functional language]]s (it assumes [[Referential transparency (computer science)|referential transparency]]); different trees have no connections, as they are simply lists of values.

As a data structure, a tree is defined as a node (the root), which itself consists of a value (of some data type, possibly empty), together with a list of references to other nodes (list possibly empty, references possibly null); symbolically:
 n: v <nowiki>[&amp;n[1], ..., &amp;n[k]]</nowiki>
(A node ''n'' consists of a value ''v'' and a list of references to other nodes.)

This data structure defines a directed graph,{{efn|Properly, a rooted, ordered directed graph.}} and for it to be a tree one must add a condition on its global structure (its topology), namely that at most one reference can point to any given node (a node has at most a single parent), and no node in the tree point to the root. In fact, every node (other than the root) must have exactly one parent, and the root must have no parents.

Indeed, given a list of nodes, and for each node a list of references to its children, one cannot tell if this structure is a tree or not without analyzing its global structure and  that it is in fact topologically a tree, as defined below.

===Type theory===
As an [[Abstract data type|ADT]], the abstract tree type ''T'' with values of some type ''E'' is defined, using the  abstract forest type ''F'' (list of trees), by the functions:
:value: ''T'' → ''E''
:children: ''T'' → ''F''
:nil: () → ''F''
:node: ''E'' × ''F'' → ''T''
with the axioms:
:value(node(''e'', ''f'')) = ''e''
:children(node(''e'', ''f'')) = ''f''
In terms of [[type theory]], a tree is an [[Recursive data type|inductive type]] defined by the constructors ''nil'' (empty forest) and ''node'' (tree with root node with given value and children).

===Mathematical===
Viewed as a whole, a tree data structure is an [[ordered tree]], generally with values attached to each node. Concretely, it is (if required to be non-empty):
* A [[rooted tree]] with the "away from root" direction (a more narrow term is an "[[Arborescence (graph theory)|arborescence]]"), meaning:
** A [[directed graph]],
** whose underlying [[undirected graph]] is a [[tree (graph theory)|tree]] (any two vertices are connected by exactly one simple path),
** with a distinguished root (one vertex is designated as the root),
** which determines the direction on the edges (arrows point away from the root; given an edge, the node that the edge points from is called the ''parent'' and the node that the edge points to is called the ''child''),
together with:
* an ordering on the child nodes of a given node, and
* a value (of some data type) at each node.
Often trees have a fixed (more properly, bounded) [[branching factor]] ([[outdegree]]), particularly always having two child nodes (possibly empty, hence ''at most'' two ''non-empty'' child nodes), hence a "binary tree".

Allowing empty trees makes some definitions simpler, some more complicated: a rooted tree must be non-empty, hence if empty trees are allowed the above definition instead becomes "an empty tree, or a rooted tree such that ...". On the other hand, empty trees simplify defining fixed branching factor: with empty trees allowed, a binary tree is a tree such that every node has exactly two children, each of which is a tree (possibly empty).The complete sets of operations on tree must include fork operation.

==Terminology==
A '''[[node (computer science)|node]]''' is a structure which may contain a value or condition, or represent a separate data structure (which could be a tree of its own). Each node in a tree has zero or more '''child nodes''', which are below it in the tree (by convention, trees are drawn growing downwards). A node that has a child is called the child's '''parent node''' (or ''ancestor node'', or [[Superior (hierarchy)|superior]]). A node has at most one parent.

An '''internal node''' (also known as an '''inner node''', '''inode''' for short, or '''branch node''') is any node of a tree that has child nodes. Similarly, an '''external node''' (also known as an '''outer node''', '''leaf node''', or '''terminal node''') is any node that does not have child nodes.

The topmost node in a tree is called the '''root node'''. Depending on definition, a tree may be required to have a root node (in which case all trees are non-empty), or may be allowed to be empty, in which case it does not necessarily have a root node. Being the topmost node, the root node will not have a parent. It is the node at which algorithms on the tree begin, since as a data structure, one can only pass from parents to children. Note that some algorithms (such as post-order depth-first search) begin at the root, but first visit leaf nodes (access the value of leaf nodes), only visit the root last (i.e., they first access the children of the root, but only access the ''value'' of the root last). All other nodes can be reached from it by following '''edges''' or '''links'''. (In the formal definition, each such path is also unique.) In diagrams, the root node is conventionally drawn at the top. In some trees, such as [[heap (data structure)|heaps]], the root node has special properties. Every node in a tree can be seen as the root node of the subtree rooted at that node.

The '''height''' of a node is the length of the longest downward path to a leaf from that node. The height of the root is the height of the tree. The '''depth''' of a node is the length of the path to its root (i.e., its ''root path''). This is commonly needed in the manipulation of the various self-balancing trees, [[AVL Trees]] in particular. The root node has depth zero, leaf nodes have height zero, and a tree with only a single node (hence both a root and leaf) has depth and height zero. Conventionally, an empty tree (tree with no nodes, if such are allowed) has depth and height −1.

A '''subtree''' of a tree ''T'' is a tree consisting of a node in ''T'' and all of its descendants in ''T''.{{efn|This is different from the formal definition of subtree used in graph theory, which is a subgraph that forms a tree – it need not include all descendants. For example, the root node by itself is a subtree in the graph theory sense, but not in the data structure sense (unless there are no descendants).}}<ref>{{MathWorld|id=Subtree|title=Subtree}}</ref> Nodes thus correspond to subtrees (each node corresponds to the subtree of itself and all its descendants) – the subtree corresponding to the root node is the entire tree, and each node is the root node of the subtree it determines; the subtree corresponding to any other node is called a '''proper subtree''' (by analogy to a [[proper subset]]).

==Drawing Trees==
Trees are often drawn in the plane. Ordered trees can be represented essentially uniquely in the plane, and are hence called ''plane trees,'' as follows: if one fixes a conventional order (say, counterclockwise), and arranges the child nodes in that order (first incoming parent edge, then first child edge, etc.), this yields an embedding of the tree in the plane, unique up to [[ambient isotopy]]. Conversely, such an embedding determines an ordering of the child nodes.

If one places the root at the top (parents above children, as in a [[family tree]]) and places all nodes that are a given distance from the root (in terms of number of edges: the "level" of a tree) on a given horizontal line, one obtains a standard drawing of the tree. Given a binary tree, the first child is on the left (the "left node"), and the second child is on the right (the "right node").

==Representations==
There are many different ways to represent trees; common representations represent the nodes as [[Dynamic memory allocation|dynamically allocated]] records with pointers to their children, their parents, or both, or as items in an [[Array data structure|array]], with relationships between them determined by their positions in the array (e.g., [[binary heap]]).

Indeed, a binary tree can be implemented as a list of lists (a list where the values are lists): the head of a list (the value of the first term) is the left child (subtree), while the tail (the list of second and subsequent terms) is the right child (subtree). This can be modified to allow values as well, as in Lisp [[S-expression]]s, where the head (value of first term) is the value of the node, the head of the tail (value of second term) is the left child, and the tail of the tail (list of third and subsequent terms) is the right child.

In general a node in a tree will not have pointers to its parents, but this information can be included (expanding the data structure to also include a pointer to the parent) or stored separately. Alternatively, upward links can be included in the child node data, as in a [[threaded binary tree]].

==Generalizations==

===Digraphs===
If edges (to child nodes) are thought of as references, then a tree is a special case of a digraph, and the tree data structure can be generalized to represent [[directed graph]]s by removing the constraints that a node may have at most one parent, and that no cycles are allowed. Edges are still abstractly considered as pairs of nodes, however, the terms ''parent'' and ''child'' are usually replaced by different terminology (for example, ''source'' and ''target''). Different [[graph (data structure)#Representations|implementation strategies]] exist: a digraph can be represented by the same local data structure as a tree (node with value and list of children), assuming that "list of children" is a list of references, or globally by such structures as [[adjacency list]]s.

In [[graph theory]], a [[tree (graph theory)|tree]] is a connected acyclic [[Graph (data structure)|graph]]; unless stated otherwise, in graph theory trees and graphs are assumed undirected. There is no one-to-one correspondence between such trees and trees as data structure. We can take an arbitrary undirected tree, arbitrarily pick one of its [[vertex (graph theory)|vertices]] as the ''root'', make all its edges directed by making them point away from the root node – producing an [[Arborescence (graph theory)|arborescence]] – and assign an order to all the nodes. The result corresponds to a tree data structure. Picking a different root or different ordering produces a different one.

Given a node in a tree, its children define an ordered forest (the union of subtrees given by all the children, or equivalently taking the subtree given by the node itself and erasing the root). Just as subtrees are natural for recursion (as in a depth-first search), forests are natural for [[corecursion]] (as in a breadth-first search).

Via [[mutual recursion]], a forest can be defined as a list of trees (represented by root nodes), where a node (of a tree) consists of a value and a forest (its children):
 f: <nowiki>[n[1], ..., n[k]]</nowiki>
 n: v f

==Traversal methods==
{{Main article|Tree traversal}}
Stepping through the items of a tree, by means of the connections between parents and children, is called '''walking the tree''', and the action is a '''walk''' of the tree. Often, an operation might be performed when a pointer arrives at a particular node. A walk in which each parent node is traversed before its children is called a '''pre-order''' walk; a walk in which the children are traversed before their respective parents are traversed is called a '''post-order''' walk; a walk in which a node's left subtree, then the node itself, and finally its right subtree are traversed is called an '''in-order''' traversal. (This last scenario, referring to exactly two subtrees, a left subtree and a right subtree, assumes specifically a [[binary tree]].)
A '''level-order''' walk effectively performs a [[breadth-first search]] over the entirety of a tree; nodes are traversed level by level, where the root node is visited first, followed by its direct child nodes and their siblings, followed by its grandchild nodes and their siblings, etc., until all nodes in the tree have been traversed.

==Common operations==
* Enumerating all the items
* Enumerating a section of a tree
* Searching for an item
* Adding a new item at a certain position on the tree
* Deleting an item
* [[Pruning (algorithm)|Pruning]]: Removing a whole section of a tree
* [[Grafting (algorithm)|Grafting]]: Adding a whole section to a tree
* Finding the root for any node

==Common uses==
* Representing [[hierarchical]] data
* Storing data in a way that makes it efficiently [[search algorithm|searchable]] (see [[binary search tree]] and [[tree traversal]])
* Representing [[sorting algorithm|sorted lists]] of data
* As a workflow for [[Digital compositing|compositing]] digital images for [[visual effects]]
* [[Routing]] algorithms

==See also==
* [[Tree structure]]
* [[Tree (graph theory)]]
* [[Tree (set theory)]]
* [[Hierarchy (mathematics)]]
* [[Dialog tree]]
* [[Single inheritance]]
* [[Generative grammar]]
* [[Hierarchical clustering]]
* [[Binary space partition tree]]
* [[Recursion]]

===Other trees===
* [[Trie]]
* [[DSW algorithm]]
* [[Enfilade (Xanadu)|Enfilade]]
* [[Left child-right sibling binary tree]]
* [[Hierarchical temporal memory]]

==Notes==
{{notelist}}

==References==
{{Reflist}}
{{refbegin}}
* [[Donald Knuth]]. ''[[The Art of Computer Programming]]: Fundamental Algorithms'', Third Edition. Addison-Wesley, 1997. ISBN 0-201-89683-4 . Section 2.3: Trees, pp.&nbsp;308–423.
* [[Thomas H. Cormen]], [[Charles E. Leiserson]], [[Ronald L. Rivest]], and [[Clifford Stein]]. ''[[Introduction to Algorithms]]'', Second Edition. MIT Press and McGraw-Hill, 2001. ISBN 0-262-03293-7 . Section 10.4: Representing rooted trees, pp.&nbsp;214–217. Chapters 12–14 (Binary Search Trees, Red-Black Trees, Augmenting Data Structures), pp.&nbsp;253–320.
{{refend}}

==External links==
{{Commons category|Tree structures}}
* [http://www.community-of-knowledge.de/beitrag/data-trees-as-a-means-of-presenting-complex-data-analysis/ Data Trees as a Means of Presenting Complex Data Analysis] by Sally Knipe
* [https://xlinux.nist.gov/dads/HTML/tree.html Description] from the [[Dictionary of Algorithms and Data Structures]]
* [http://www.ipub.com/data.tree data.tree] implementation of a tree data structure in the R programming language
* [http://wormweb.org/celllineage WormWeb.org: Interactive Visualization of the ''C. elegans'' Cell Tree] – Visualize the entire cell lineage tree of the nematode ''C. elegans'' (javascript)
* [http://www.allisons.org/ll/AlgDS/Tree/ ''Binary Trees'' by L. Allison]

{{CS-Trees}}
{{Data structures}}

{{DEFAULTSORT:Tree (Data Structure)}}
[[Category:Data types]]
[[Category:Trees (data structures)| ]]
[[Category:Knowledge representation]]

[[de:Baum (Graphentheorie)]]
<=====doc_Id=====>:914
<=====title=====>:
Library system
<=====text=====>:
[[File:Boston Public Library Reading Room.jpg|thumb|Reading Room at [[Boston Public Library, McKim Building|McKim Building]] in 2013]]Library system is a central organization created to manage and coordinate operations and services in or between different centers, buildings or [[Library branch|libraries branches]] and libraries patrons. It uses a [[Library classification]] to organize their volumes and nowadays also uses a [[Integrated library system]], an [[enterprise resource planning]] system for a [[library]], used to track items owned, orders made, bills paid, and patrons who have borrowed.<ref>Adamson, Veronica, ''et al.'' (2008). {{cite web|url= http://www.jisc.ac.uk/media/documents/programmes/resourcediscovery/lmsstudy.pdf |title=''JISC & SCONUL Library Management Systems Study'' }}&nbsp;{{small|(1&nbsp;MB)}}. Sheffield, UK: Sero Consulting. p. 51. Retrieved on 21 January 2009. "... a Library Management System (LMS or ILS 'Integrated Library System' in US parlance)."
Some useful library automation software are: KOHA ,Grennstone .LIBsis, and granthlaya.</ref> Many counties, states or Universities have developed their own libraries systems, among them can be named [[Los Angeles Public Library|Los Angeles Public Library System]],<ref>{{Cite web|url=http://www.lapl.org/about-lapl/press/2013-library-facts|title=Los Angeles Public Library Facts 2013 (for fiscal year 2012-13) {{!}} Los Angeles Public Library|website=www.lapl.org|access-date=2016-03-06}}</ref> [[Harvard Library|Harvard Library System]],.<ref name="AR2013">{{cite web|title=Harvard Library Annual Report FY 2013 |url=http://library.harvard.edu/annual-report-fy-2013 |date=2013 |website=Harvard Library |author=Harvard University |accessdate=17 March 2015}}</ref>

Most of [[County|counties]] of every country have their own '''library system'''s that usually have between 10 to 30 libraries on every city of their counties, some of them are; [[London Public Library]] on [[Canada]] with 16 library branches, [[Helsinki Metropolitan Area Libraries]], in [[Finland]], with 63 libraries,<ref>{{cite web| url=http://www.iii.com/news/pr_display.php?id=559 | title=Helsinki Metropolitan Area Libraries (Finland) Upgrades to Sierra Services Platform | publisher=Innovative | type= Press release | date=5 February 2013 | accessdate=1 August 2014 }}</ref> and some countries, like Venezuela has only one library system for the whole country as is [[National Library of Venezuela]] with 685 branches.  In the United States can be named [[Boston Public Library|Boston Public Library System]], [[New York Public Library|New York Public Library System]], [[District of Columbia Public Library|District of Columbia Public Library System]], among others.

==See also==
* [[Integrated library system]]
* [[Library classification]]
* [[Library branch]]
* [[List of the largest libraries in the United States]]

==References==
{{reflist}}

[[Category:Public libraries]]
[[Category:Private libraries]]
[[Category:Libraries]]
[[Category:Culture]]
[[Category:Knowledge representation]]


{{Library-stub}}
<=====doc_Id=====>:917
<=====title=====>:
Hallin's spheres
<=====text=====>:
'''Hallin's spheres''' is a theory of media objectivity posited by journalism historian [[Daniel C. Hallin]] in his book ''The Uncensored War'' to explain the coverage of the Vietnam war.<ref name="DH">{{cite book|last=Hallin|first=Daniel|title=The Uncensored War: The Media and Vietnam.|year=1986|publisher=Oxford University press|location=New York|pages=116–118|isbn=978-0-19-503814-9}}</ref> Hallin divides the world of political discourse into three concentric spheres: consensus, legitimate controversy, and deviance. In the sphere of consensus, journalists assume everyone agrees. The sphere of legitimate controversy includes the standard political debates, and journalists are expected to remain neutral. The sphere of deviance falls outside the bounds of legitimate debate, and journalists can ignore it. These boundaries shift, as public opinion shifts.<ref>[http://www.cjr.org/analysis/trump_inspires_murrow_moment_for_journalism.php For journalists covering Trump, a Murrow moment], By David Mindich, Columbia Journalism Review, July 15, 2016</ref>

Hallin's spheres, which deals with the media, are similar to the [[Overton window]], which deals with public opinion generally, and posits a sliding scale of public opinion on any given issue ranging from conventional wisdom to unacceptable. 

Hallin used the concept of [[Framing (social sciences)|framing]] to describe the presentation and reception of issues in public. For example, framing the use of drugs as criminal activity can encourage the public to consider that behavior anti-social. Hallin also used the concept of an [[opinion corridor]], in which the range of public opinion narrows, and opinion outside that corridor moves from legitimate controversy into deviance. 

== Description ==

=== Sphere of consensus ===
This sphere contains those topics on which there is widespread agreement, or at least the perception thereof. Within the sphere of consensus, 'journalists feel free to invoke a generalized "we" and to take for granted shared values and shared assumptions'<ref>Schudson 2002, p. 40</ref> Example include such things as free speech, the abolition of slavery, or human rights.  For topic in this sphere "journalists do not feel compelled to present an opposing view point or to remain disinterested observers."<ref name="DH"/>

=== Sphere of legitimate controversy ===
For topics in this sphere rational and informed people hold differing views. These topics are therefore the most important to cover, and also ones upon which journalists are obliged to remain disinterested reporters, rather than advocating for or against a particular view.<ref>Hallin, 1986, p. 116;</ref> Schudson notes that Hallin, in his influential study of the US media during the Vietnam War, argues that journalism's commitment to objectivity has always been compartmentalized. That is, within a certain sphere—the sphere of legitimate controversy—journalists seek conscientiously to be balanced and objective.<ref>Schudson, M (2002) 'What's unusual about covering politics as usual', in Zelizer, B., & Allan, S. (Eds.). Journalism after September 11. London: Routledge, p. 40</ref>

=== Sphere of deviance ===
Topics in this sphere are rejected by journalists as being unworthy of general consideration.  Such views are perceived as being either unfounded, taboo, or of such minor consequence that they are not news worthy.  Hallin argues that in the sphere of deviance, 'journalists also depart from standard norms of objective reporting and feel authorized to treat as marginal, laughable, dangerous, or ridiculous individuals and groups who fall far outside a range of variation taken as legitimate.'<ref>Schudson 2002, 40</ref> For example, a person claiming that aliens are manipulating college basketball scores might have difficulty finding media coverage for such a claim.<ref>Hallin, 1986, p. 117</ref>

== Uses of the terms ==
Craig Watkins (2001, pp.&nbsp;92–4) makes use of the Hallin's spheres in a paper examining ABC, CBS, and NBC television network television news coverage of the "Million Man March", a demonstration that took place in Washington, DC on October 16, 1995. Watkins analyzes the dominant framing practices-problem definition, rhetorical devices, use of sources, and images-employed by journalists to make sense of this particular expression of political protest. He argues that Hallins three spheres are a way for media framing practices to develop specific reportorial contexts, each sphere develops its own distinct style of news reporting resources by different rhetorical tropes and discourses.<ref>Watkins, S. C. (2001). Framing protest: News media frames of the Million Man March. Critical Studies in Media Communication, 18(1), 83-101.</ref>

Piers Robinson (2001, p.&nbsp;536) uses the concept in relation to debate that have emerged over the extent to which the mass media serves elite interests or, alternatively, plays a powerful role in shaping political outcomes. His articles reviews Hallin's spheres as an example of media-state relations, that highlights theoretical and empirical shortcomings in the 'manufacturing consent' thesis (Chomsky McChesney).<ref>Herman, E. S., & Chomsky, N. (2010). Manufacturing consent: The political economy of the mass media. Random House.</ref> Robinson argues that a more nuanced and bi-directional understanding is needed of the direction of influence between media and the state that builds upon, rather than rejecting, existing theoretical accounts.<ref>Robinson, P. (2001). Theorizing the Influence of Media on World Politics Models of Media Influence on Foreign Policy. European Journal of Communication, 16(4), 523-544.</ref>

Hallin's theory assumed a relatively homogenized media environment, where most producers were trying to reach most consumers. A more fractured media landscape can challenge this assumption.<ref>{{cite web|title=Does NPR Have A Liberal Bias?|url=http://www.onthemedia.org/2012/sep/14/|work=On The Media from NPR|publisher=WNYC|accessdate=11 February 2013}}</ref> because different audiences may place topics in different spheres, a concept related to the [[filter bubble]], which posits that many members of the public choose to limit their media consumption to the areas of consensus and deviance that they personally prefer.

==See also==
* [[Ambit claim]]
* [[Argument to moderation]]
* [[Creeping normality]]
* [[Cultural hegemony]]
* [[Door-in-the-face technique]]
* [[Political suicide]]
* [[Slippery slope]]
* [[Spiral of silence]]
* [[Third rail of politics]]

== References ==
{{reflist}}

==External links==
* [http://archive.pressthink.org/2009/01/12/atomization.html Audience Atomization Overcome: Why the Internet Weakens the Authority of the Press], Rosen, Jay. PressThink.org, January 12, 2009
* [http://wnymedia.net/smith/2009/03/the-sphere-of-deviance/ The Sphere of Deviance] Smith, Christopher. WNYMedia.net, 2009.
* [http://www.onthemedia.org/2012/sep/14/ Does NPR have a Liberal Bias?], On The Media from NPR. Retrieved 11 February 2013

{{Media culture}}
{{Media manipulation}}
{{Propaganda}}

[[Category:Framing (social sciences)]]
[[Category:Knowledge representation]]
[[Category:Propaganda techniques]]
[[Category:Journalism]]
